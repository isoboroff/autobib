Discovering Experts across Multiple Domains
Aditya Pal
IBM Almaden Research
San Jose, CA, USA

apal@us.ibm.com

ABSTRACT

of the expertise of its authors, which need not be the case.
Second, it uses a weighted sum of document relevance and
popularity to compute users’ expertise. This aggregation
strategy can bias the model towards authors that have large
number of less relevant documents.
Our expertise framework systematically addresses the shortcomings of prior work. It first analyses documents on various
aspects, such as language, questions, topics, indicator of expertise with a goal of extracting key features from the expertise documents of an author. We then propose IR techniques
(such as a proximity based document relevance computation, concept filtering based) beyond a basic Lucene search
to retrieve highly relevant documents. Finally, we propose a
sophisticated rank aggregation framework that takes the relevant documents as input and computes the expertise score
of their authors. Evaluation over a large real World dataset
shows the improved performance of our approach in comparison to prior methods.

Researchers have focused on finding experts in individual
domains, such as emails, forums, question answering, blogs,
and microblogs. In this paper, we propose an algorithm for
finding experts across these different domains. To do this,
we propose an expertise framework that aims at extracting
key expertise features and building an unified scoring model
based on SVM ranking algorithm. We evaluate our model on
a real World dataset and show that it is significantly better
than the prior state-of-art.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval

General Terms
Algorithms, Experimentation, Theory

2.

Keywords
Expertise finding; Social media; Enterprise search

1.

RELATED WORK

Expertise discovery is a heavily researched area (see [1]
for a comprehensive overview). Dom et al. [4] considered
an email network for expertise identification. They evaluated several graph based algorithms and found PageRank
algorithm to outperform HITS. Zhang et al. [11] presented
an expertise discovery algorithm to rank experts in a question answering website by examining the number of questions answered by the user along with the number of persons
that were answered by the user. They showed that a simple model could outperform classical expert finding models
like PageRank and HITS in the QA domain. Java et al. [7]
modeled the spread of influence in the blogosphere in order
to select an influential set of bloggers such that they would
maximize the spread of information in the blogosphere. Pal
et al. [9] proposed a feature-based algorithm for finding topical authorities in microblogs. They proposed features to capture topical interest and prominence for users and showed
that their model out-performs graph based models.
There is little work on expert finding across different webbased data sources, with a notable exception of [5]. Guy
et al. [5] proposed a Lucene based model to index multidomain documents and combined document relevance, and
popularity to compute user expertise. While somewhat similar to this prior work, our approach differs in important
ways. First, we incorporate a number of components and
algorithms with the goal of extracting key features (topics,
language) from documents that are indicative of users’ expertise. Then we systematically improve the retrieval engine

INTRODUCTION

The existing efforts on expertise discovery have targeted
a specific data source for finding experts, such as emails [4],
question answering [11], blogs [7] and microblogs [9]. There
is little prior work (except [5]) that models expertise by combining data from these web sources - primarily because of
challenges in gathering such data and disambiguating users
across website (e.g. finding Twitterers on Facebook) can be
impossible. Prior work [5] considered data within an Enterprise that allows its employees to publish documents in
several data sources internally. They proposed an expertise
model that indexes all the content in Lucene and uses the
relevant documents retrieved through Lucene to compute
users’ expertise. There are several shortcomings of this approach. First it assumes that all documents are reflective
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
SIGIR’15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767774.

923

Employees
20,000

Blogs
38,675

Wikis
87,301

Forum Posts
44,794

Microblogs
160,484

Algorithm 1 DocSense(Du )
Require: keywords, γ = 0.7, α, β
for d ∈ Du do
Mark d if
|d[: 1] ∩ keywords| > 0
d has P, |d ∩ P | ≥ γ|d| or |d| < 20
maxd0 ∈Du \d {|d ∩ d0 |} ≥ γ|d|
end for
0 ← {d ∈ D : d is not marked}
Du
u
0 do
for d ∈ Du
0 : JS
Mark d if |{d0 ∈ Du
d,d0 ≥ β} \ d| ≤ α
end for
0
return {d ∈ Du : d is not marked}

Table 1: Dataset statistics.

Figure 1: Expertise framework.

DATASET

We crawled online participation data of employees within
IBM. We selected 20,000 employees randomly and crawled
their complete online activity which includes publishing blogs,
microblogs, wikis, forum posts. Additionally we crawled the
profiles of the employees which includes their job title, role
and self-reported expertise and past experience. Table 1
presents the summary statistics of the crawled dataset.

4.

4.1.1

Document Features

Table 2 lists the document features. Most of these features
are common across the different document types. Since we
treat an user’s information such as role, bio, etc as a user
document, so some of the content features are relevant to it.
We carried out language analysis using Linguistic Inquiry
and Word count (LIWC) tool. LIWC provides scores on 8090 features for the input text. We used JS divergence to
compute the similarity between a post and its replies.

EXPERTISE FRAMEWORK

Fig. 1 presents an overview of our expertise framework.
It consists of several core components that are geared towards extracting key features, relevance engine that aims at
picking the relevant documents to a query, and a ranking
component to select top experts.

4.1

{non-topical}

cators for users’ core expertise. For filtering such documents,
we propose DocSense algorithm that takes documents authored by a user as input and filters them on 4 criteria: 1)
non-relevance: document contains specific keywords (summary, announcement, call for) in its opening paragraph, 2)
url summary: document summarizing a url, 3) duplicate:
document matching other documents of the author (useful
for filtering financial reports, reminders, etc.), and 4) nontopical : document not pertinent to user’s core interest area.

through key modules such as, proximity based document relevance computation and rank aggregation.

3.

{non-relevant}
{url summary}
{duplicate}

Type
Content

Core Components

We describe the core components that are used for extracting key features from user documents.
Language Detection: Our dataset consists of documents written in several languages. However the technical
jargon (such as products, tools) are shared across all languages. Ignoring this multi-lingual aspect of the documents
could retrieve experts versed in different languages - thereby
confusing the end users. We use ngram based classifier [3]
to select only the documents written in English as it is the
most predominant language of our dataset (80%).
Topic Estimation: We use the Latent Dirichlet Allocation algorithm (LDA) [2] to estimate the documents’ topics.
To estimate the number of topics for LDA, we use Bayesian
Information Criteria (BIC). We use Jensen-Shannon divergence [8] to compute the similarity between the documents.
Question Modeler: Users employ ingenious methods
to ask questions in different data sources, e.g. a user can
create a wiki page with a question and the users can answer
by editing the wiki. We trained a linear SVM classifier over
the ngrams of a set of labeled documents (question or not
a question). Only the first four sentences of a document
are considered for feature extraction as beyond that several
documents have questions that do not merit any answer, e.g.
if you have further questions, please contact the author.
Document Sensing: A crucial component for expertise
discovery is identifying documents that reflect expertise of
their authors. E.g. talk announcements, webpage summary,
scribe notes, financial reports, can provide misleading indi-

Social
Processed
Reply

Features
document length, publish date, #referenced url,
similarity with referenced urls’ content, presence of smiles and greetings, #referenced entities,
#hashtags, #technical jargons, topic distribution,
language analysis
#reply, #view, #recommendation, #share, #vote
is a question?, document marking (DocSense)
processed features of parent, similarity with parent doc., responsiveness (how soon reply was
posted), relative content features (compared to
other replies on the same parent)

Table 2: List of document features.

4.2

Retrieval Engine

We use Apache Lucene to index the documents. Separate indexes are maintained for each domain; ensuring that
the retrieval engine doesn’t get biased towards any specific
data source (such as microblogs, which typically have shorter
length). It also provides a flexibility of picking same number
of documents per source irrespective of their relevance. We
propose several components besides Lucene that are aimed
towards improving the relevance of the retrieved documents.
Query Expansion: Before a query is passed to Lucene,
we first use the probabilistic query expansion model [10] to
expand it by adding relevant terms. This is done so that all
the topically-relevant documents are retrieved. The query
expansion model depends on a word-word similarity thesaurus which we build using the JS divergence over the

924

Algorithm 2 DocRel(d – document, Q – expanded query)

are estimated using the observed Ds . We get,
sP
P
s 2
s
s
d∈D s (d(i) − µi )
d∈D s d(i)
σ
=
µi =
i
|Ds |
|Ds | − 1

Require: α ∈ [0.5, 1), β ∈ (1, ∞)
R? ← 0; n ← 0
for i ← 1 to |d| and w ← d[i] do
continue if @W ∈ Q, s.t. w ∈ W {w not a query keyword}
P [W [1]] ←ni
{Set/Update
w’s root’s position}
o
R ← max 0, 1 −

i+1−min(P )−|P |
10

Weights over individual features in Rds is computed through
a Gaussian CDF function which is a monotonically increasing function and hence well suited to our scoring problem
as we prefer a higher value for all features. If a lower value
is preferred, then negation can be applied to that feature
before applying the above formula.
In order to learn the weights wi on different features, we
consider a training dataset with R̄ds as the true score of the
document. Following optimization is used to get optimal w.
(
)
X s
s 2
s 1
s 2
arg min
(R̄d − ln Rd ) + λ1 ||w ||1 + λ2 ||w ||2
(2)
s

{current relevance}

if |P | < |Q|
l then m
|P |
|P |
R ← R |Q| − 12 log( 12 + |Q| ) {partial match penalty}
end if
if n < |P | or {n = |P | and rel? < rel} then
R? ← R; n ← |P |
{Update global best relevance}
end if
end for
return R?

w

4.3

We get expertise score for each user per data source. To
combine these scores, we use SVM rank aggregation algorithm [6]. Let Eu = [Euwiki . . . ]T be the vector of expertise
score of the user u. Let (u, v) ∈ R be the ranked set of the
users, indicating that user u has higher expertise than user
v. The rank aggregation problem can be written as:X
1 T
a a+λ
ξu,v
minimize :
2
subject to :
(4)
∀(u, v) ∈ R :
∀(u, v) ∈ R :

Expertise Model

=


n(d)
Y Z


i=1

d(i)

−∞

N (x; µsi , σis )

dx


wis 

aT (Eu − Ev ) ≥ 1 − ξu,v
ξu,v ≥ 0

The constraints are geared towards finding a ranking function that is consistent with the ranked set R. ξ indicates
soft-margin penalties to the separation constraints, without
which the minimization problem might be infeasible (in case
of non-linearly separability). The optimal weights a? are
used to sort the users in decreasing order of aT Eu value.

5.

Let the set of relevant documents provided by the retrieval
engine be D = {Dwiki , Dmicroblogs , . . .}. Let d ∈ Ds indicate document d’s features (Table 2). First we estimate the
relative expertise score (Rds ) of the documents given all other
relevant document from the same source.
Rds

d∈Ds

The above optimization problem is quadratic in ws and
hence can be computed using ElasticNet regression. It leads
to shrinking in the values of w as well as induces sparsity.
Another advantage is that amongst a block of highly correlated features - only few features have non-zero weights.
Once we have relative expertise of documents, we can compute users’ expertise per data source. Let u(D) ⊆ D indicate
the documents in D authored by u. Then we compute the
source specific expertise (Eus ) of a user as follows.
P
s
d∈u(D s ) Rd · DocRel(d, q)
s
(3)
Eu =
|u(Ds )|

words’ topical distribution. The model ensures that the
terms that bind closely with the query keywords are selected. The expanded query is then used to retrieve 1000
documents per data source.
Document Relevance: Lucene’s relevance scores are
based on tf-idf normalized weights of the query terms in
the documents. This scheme is not ideal for two reasons:
(1) restriction on the length of documents differ for different data sources, which could bias this relevance, and (2)
it does not consider the proximity of the query keywords
within the documents - altering the context of the documents altogether. E.g. a document containing machine and
learning far apart, might pertain to systems domain and
not artificial intelligence. We propose DocRel algorithm
that takes into consideration the minimum distance between
query keywords (vector of vector due to query expansion) in
the documents to compute the document relevance. The algorithm also takes care of the scenario where not all query
keywords are present in the document.
Concept Filtering: A user query partitions the documents into relevant and non-relevant bucket. The relevant
documents can be further partitioned into concepts buckets.
E.g. a query machine learning might draw documents from
concepts, such as finance, math and statistics, computer science, software engineering. The idea here is to eliminate
concepts that are not well represented by examining the underlying topic distribution of the document. We use Gaussian Mixture Model (GMM) over the document topics for
partitioning them into concept space and discard document
that belong to low frequency clusters.

RESULTS AND EVALUATION

We compared our model with several baseline models:
B1: This model uses the prior state-of-art approach [5]
for discovering experts across multiple domains.
B2: This model adapts B1 to use the several components
proposed in our framework. The final expertise of users is
aggregated the same way as suggested by B1.
B3: This model uses the document retrieval technique
proposed by our expertise framework. Then users are selected with probability proportional to the relevance of the
documents published by them.
All the models were trained using a small labeled dataset
of the experts for two hand picked queries. In order to evaluate our approach, we conducted a user study in which results

(1)



where n(d) is the number of features in d, d(i) indicates the
ith component of d, N is a Gaussian distribution and wis is
the weight on the ith feature. The model parameters µs , σ s

925

Component
Language Classification
Question Classification
Document Sensing
Query Expansion
Document Relevance
Concept Filtering

from our model were compared to those from three baseline
models. We selected a set of 10 popular queries (such as Distributed Scrum, Commercial Finance, Topic Modeling) and
picked top 10 authors per model per query. Then 40 authors
per query (usually less due to some commonality) are shown
to five coders in a random order. The coders were shown authors’ profile and the relevant documents published by them
and they were asked to rate on a 5-point Likert scale, the
expertise of the authors on the query. Inter rater agreement
between the coders was 0.6, indicating moderate agreement.

5.1

Table 6: Drop in DCG by removing a component.
Recall that the authors retrieved by the full model are
coded for expertise. We consider the rank of these authors
and their ordering when computing the DCG of the model.
In this case, we consider the DCG of the full model and
consider the drop in DCG for the partial models.
Table 6 shows the drop in average DCG received by our
model without a specified component. We note that a component can affect the model performance in several ways.
E.g. if the language analysis component is turned off, then
several non-English documents would be retrieved at the
cost of relevant English documents. These documents could
reduce the score of top experts, thereby reducing their ranks
and giving prominence to sub-par authors. We observe that
the document sensing is in particular the most effective component of the model, without which the model would retrieve
authors with 17% less ratings. We note that document sensing, question classification and document sensing contribute
substantially towards improving the model performance.

Model Performance

Table 3 shows the average survey rating received by the
top 10 authors. We note that the authors discovered by
our model received 37% higher rating than the prior work
(B1). It also performed 10% better than B2 indicating the
effectiveness of our expertise framework in comparison to a
simple aggregation technique. All improvements were statistically significant using one-sided t-test with 95% CI.
Our
3.40

B1
2.48

B2
3.10

B3
2.84

Table 3: Average ratings of the models.
Table 4 shows the discounted cumulative gain (DCG)1
of the four models. We note that the DCG of our model
is statistically significantly better for all queries, indicating
that it surfaced top experts at a higher position in the ranked
list, in comparison to the baselines.
Our
24.51

B1
17.49

B2
21.14

6.

B3
19.85

Model Performance using Best Rating

In practice, an end-user is looking for connecting with one
expert and hence they would be satisfied if the model recommends at least one expert that matches their requirements.
To compare the models in this practical setting, we consider
the ratings given to the best-rated author per model by each
coder (Table 5). We observe that authors of our model received heist ratings – indicating that an end-user would be
most satisfied with our model. Additionally we see that the
best author is surfaced within top 2 of the ranked list, which
makes it easier for an end-user to locate the top expert.
Our
a
r
4.2 1.8

B1
a
3.8

B2
r
1.9

a
4.1

References
[1] K. Balog, Y. Fang, M. de Rijke, P. Serdyukov, and L. Si.
Expertise retrieval. FTIR, 6(2-3):127–256, 2012.
[2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet
allocation. JMLR, 2003.
[3] W. B. Cavnar and J. M. Trenkle. N-gram-based text categorization. In SDAIR, 1994.
[4] B. Dom, I. Eiron, A. Cozzi, and Y. Zhang. Graph-based
ranking algorithms for e-mail expertise analysis. In DMKD,
2003.
[5] I. Guy, U. Avraham, D. Carmel, S. Ur, M. Jacovi, and I. Ronen. Mining expertise and interests from social media. In
WWW, 2013.
[6] T. Joachims. Optimizing search engines using clickthrough
data. In KDD, 2002.
[7] A. Kale, A. Karandikar, P. Kolari, A. Java, T. Finin, and
A. Joshi. Modeling trust and influence in the blogosphere
using link polarity. In ICWSM, 2007.
[8] J. Lin. Divergence measures based on the shannon entropy.
IEEE Transactions on Information Theory, 2006.
[9] A. Pal and S. Counts. Identifying topical authorities in microblogs. In WSDM, 2011.
[10] Y. Qiu and H.-P. Frei. Concept based query expansion. In
SIGIR, 1993.
[11] J. Zhang, M. S. Ackerman, and L. A. Adamic. Expertise
networks in online communities: structure and algorithms.
In WWW, 2007.

B3
r
1.9

a
3.9

r
2.1

Table 5: (a) Best ratings and (r) rank of best author.

5.3

CONCLUSION

This paper presents a framework for expertise finding that
aggregates content from several data sources. We introduced
several novel components pivotal for building an effective
expertise search engine, such as language classifier, question
classifier, concept filtering scheme, and expertise rank aggregation. Our experiments show improved performance in
comparison to models that ignores these aspects.

Table 4: Discounted cumulative gain of the models.

5.2

DCG loss (%)
6.9
10.0
17.1
4.0
9.7
2.1

Model Component Analysis

The previous results show that our expertise framework
improves over the baselines, which is primarily due to the
various components proposed in this paper. Here we estimate the exact contribution of the different components
towards model performance.
1
DCG works in our case as we are retrieving equal number
of experts from each model

926

