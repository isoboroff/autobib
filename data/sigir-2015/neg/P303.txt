WEMAREC: Accurate and Scalable Recommendation
through Weighted and Ensemble Matrix Approximation
Chao Chen§,∗, Dongsheng Li† , Yingying Zhao§ , Qin Lv‡ , Li Shang‡
§

Tongji University, Shanghai, 201804, P.R. China
IBM Research - China, Shanghai, 201203, P.R. China
‡
University of Colorado Boulder, Boulder, CO 80309, USA
†

{chench.resch, yyzhao.tj}@gmail.com, ldsli@cn.ibm.com, {qin.lv, li.shang}@colorado.edu
ABSTRACT

1.

Matrix approximation is one of the most eﬀective methods for collaborative ﬁltering-based recommender systems.
However, the high computation complexity of matrix factorization on large datasets limits its scalability. Prior solutions have adopted co-clustering methods to partition a
large matrix into a set of smaller submatrices, which can
then be processed in parallel to improve scalability. The
drawback is that the recommendation accuracy is lower as
the submatrices only contain subsets of the user-item rating
information.
This paper presents WEMAREC, a weighted and ensemble matrix approximation method for accurate and scalable recommendation. It builds upon the intuition that
(sub)matrices containing more frequent samples of certain
user/item/rating tend to make more reliable rating predictions for these speciﬁc user/item/rating. WEMAREC consists of two important components: (1) a weighting strategy
that is computed based on the rating distribution in each
submatrix and applied to approximate a single matrix containing those submatrices; and (2) an ensemble strategy that
leverages user-speciﬁc and item-speciﬁc rating distributions
to combine the approximation matrices of multiple sets of
co-clustering results. Evaluations using real-world datasets
demonstrate that WEMAREC outperforms state-of-the-art
matrix approximation methods in recommendation accuracy
(0.5–11.9% on the MovieLens dataset and 2.2–13.1% on the
Netﬂix dataset) with 3–10X improvement on scalability.

Collaborative ﬁltering (CF), which predicts users’ item
ratings based on the ratings of other users with similar taste,
has been shown to preform well in many recommender systems [1]. Among existing CF solutions, matrix approximation has become increasingly popular. It formulates the recommendation problem as missing entry prediction using existing entries in a user-item rating matrix, i.e., attempting
to predict the missing entries in a partially observed matrix.
Given m users and n items, the user-item rating matrix
M ∈ Rm×n is typically of low-rank, then M can be approximated by a r-rank matrix M̂ = U V T , where U ∈ Rm×r
is the set of user features, V ∈ Rn×r is the set of item features, and r  min(m, n). Then, the rating of the i-th
user on the j-th item can be predicted by the inner product
Ui VjT . Using matrix approximation, the user/item feature
vectors are reduced to lower dimensions, which helps to address the “data sparsity” issue, a challenge to memory-based
CF methods [1, 24]. Recent studies have shown that matrix
approximation based CF methods outperform many other
CF solutions [10, 17, 22, 28].
However, existing matrix approximation based CF methods exhibit poor scalability due to the high computation
complexity of matrix factorization on large user-item rating datasets [10, 22, 16, 11, 28]. Recent work adopted coclustering methods [8, 29, 26] to partition the large user-item
rating matrix into a set of smaller submatrices, which can
then be processed in parallel to improve system scalability.
However, this usually leads to lower recommendation accuracy. Co-clustering tries to ﬁnd coherent submatrices each of
which contains a subset of users who share similar interests
on a subset of items. In the ideal case, recommendations
based on such submatrices can be as accurate as recommendations based on the original large matrix while requiring
much less computation overhead. However, the submatrices
obtained by co-clustering methods are not perfect, as a small
fraction of user-item ratings may not follow the distribution
of majority ratings. As a result, recommendation accuracy
on such user-item ratings will degrade, aﬀecting the overall
recommendation accuracy. Our study shows that, for the
MovieLens dataset with 1 million ratings, the recommendation RMSE (root mean square error) increases from 0.8645
to 0.9 when the co-clustering setting varies from 1 × 1 to
5 × 5. Therefore, a better matrix approximation solution
that achieves both high accuracy and high scalability for
recommendation is needed.
In this work, we have developed WEMAREC, a weighted
and ensemble matrix approximation method for accurate
and scalable CF-based recommendation. The intuition is

Categories and Subject Descriptors
H.3.4 [Systems and Software]: User proﬁles and alert
services

Keywords
R ecommendation; matrix approximation; weighted; ensemble
∗
Chao Chen and Dongsheng Li contributed equally to this
work.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
SIGIR’15, August 09 - 13, 2015, Santiago, Chile.
© 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767718 .

303

INTRODUCTION

2.

0.90
RMSE

0.89

160
120

0.88

80

0.87

40

0.86
1×1

0
2×2
3×3
4×4
#RowCluster × #ColumnCluster

5×5

Figure 1: The tradeoﬀ between recommendation
accuracy and runtime eﬃciency when varying the
number of co-clusters.
The max norm is denoted as:
||M ||∞ = max{|Mij |}.

2.2

Existing Low-Rank Matrix Approximation

Two methods have been used for low-rank matrix approximation M̂ of M , i.e., SVD and compressed sensing [5, 6].
The SVD method is based on minimizing the sum-squared
distance — Frobenius norm:
M̂ = arg min  I ⊗ (M − X) F

s.t. rank(X) = r,

(1)

X

where each Iij is the indicator function that equals to 1 if
Mij is observed and equals to 0 otherwise. The compressed
sensing method is based on minimizing the nuclear norm:
M̂ = arg min  X ∗
X

s.t.  I ⊗ (M − X) F < .

(2)

As shown in [22], the problem deﬁned in (1) is a diﬃcult
non-convex optimization problem and an iterative method
may converge to a local minimum. In contrast to SVD, the
problem deﬁned in (2) is convex and can be casted as a
semi-deﬁnite program [6].

PROBLEM FORMULATION

This section provides necessary background for the matrix approximation problem. It then presents case studies
to motivate the challenge faced by existing matrix approximation methods. The case studies presented in this section
are conducted on the MovieLens (1M) dataset and the standard singular value decomposition (SVD) algorithm is used
in matrix approximation.

2.1

200

Recommendation Error
Execution Time

Time (s)

that, (sub)matrices only contain partially-sampled information of user/item/rating; if a submatrix contains more samples for a certain user or item or rating, this submatrix can
probably make more reliable predictions on the speciﬁc user
or item or rating. This applies not only to the submatrices generated from a single co-clustering process, but also
to the multiple sets of submatrices generated using diﬀerent
co-clustering constraints. Since these submatrices contain
diﬀerent user-item rating information, an intelligent combination of these submatrices can yield better recommendation quality while still enjoy the beneﬁt of high scalability
due to parallel processing the co-clustering submatrices.
This work makes the following contributions: (1) identiﬁcation of the unbalanced predication power on diﬀerent
users/items/ratings due to the partial information that is
contained in (sub)matrices; (2) development of a submatrixbased weighting strategy to capture rating-speciﬁc prediction power and combine submatrices into the approximation
of a single user-item rating matrix; (3) development of an
ensemble matrix approximation method that uses diﬀerent
co-clustering constraints to generate and combine multiple
sets of submatrices with diﬀerent rating prediction power for
diﬀerent users and items; and (4) evaluation using two largescale real-world datasets which demonstrates WEMAREC’s
improvement in recommendation accuracy and scalability
over state-of-the-art matrix approximation techniques.
The rest of this paper is organized as follows. Section 2
formulates the problem. Section 3 describes the proposed
WEMAREC method in detail. Section 4 analyzes the error bounds of the proposed method. Section 5 presents the
evaluation results. Section 6 discusses the related work, and
ﬁnally Section 7 concludes this work.

Rating
1
2
3
4
5

Notations and Deﬁnitions

In this paper, upper case letters such as M, U, V denote
matrices. For matrix M ∈ Rm×n , we denote Mi∗ as the
i-th row vector, M∗j as j-th column vector, and Mij as
the entry in the i-th row and j-th column. We denote M
as a submatrix of M , i.e., both the rows and columns in
M are subsets of those in M . A r-rank approximation of
M is denoted as M̂ = U V T , where U ∈ Rm×r , V ∈ Rn×r
and r  min(m, n). In addition, [n] denotes the list of
{1, . . . , n}, Ω denotes the set of observed entries in the useritem rating matrix M , i.e., ∀(i, j) ∈ Ω, Mi,j = 0. Then, |Ω|
is denoted as the total number of observed entries in M .
Three matrix norms are used in this paper. The Frobenius
norm is denoted as:

2
||M ||F :=
Mij
.

Distribution

17.44%
25.39%
35.35%
18.28%
3.50%
Overall result

RMSE
(w/o weighting)
1.2512
0.6750
0.5260
1.1856
2.1477
0.9517

RMSE
(w/ weighting)
1.2533
0.6651
0.5162
1.1793
2.1597
0.9479

Table 1: Rating-speciﬁc RMSE when running SVD
without (w/o) or with (w/) weighting on a submatrix.

2.3

Motivating Examples

Next, we present case studies to demonstrate the accuracy
issue of existing co-clustering based matrix approximation
methods, and provide insights on why user-item rating distribution can be leveraged to improve the recommendation
accuracy.

ij

2.3.1

The nuclear norm is denoted as the sum of singular values:

σk .
||M ||∗ :=

Scalability vs. Accuracy

Co-clustering is an eﬀective method to improve the scalability of matrix approximation based CF methods [8, 29],
because these submatrices can be processed in parallel. In

k

304

Mˆ ui(1)

addition, co-clustering tries to ﬁnd coherent submatrices,
each of which containing a subset of users who share similar
interests on a subset of items. In the ideal case, users’ common interests in each submatrix can be accurately predicted.
Unfortunately, the submatrices obtained by co-clustering are
not perfect. Within each submatrix, a subset of user-item
ratings may not follow the distribution of majority ratings.
As a result, the corresponding recommendation accuracy of
those minority user-item ratings may be poor, which in turn
aﬀects the overall recommendation accuracy.
To evaluate how the recommendation accuracy varies with
co-clustering granularity, we apply Bregman co-clustering [2]
on the MovieLens dataset. As demonstrated in Figure 1, assuming the co-clustering submatrices are processed in parallel, the scalability of matrix approximation increases when
the number of co-clusters increase from 1 × 1 to 5 × 5 (k ×
k). On the other hand, the recommendation error (measured as RMSE) increases from 0.8645 to 0.9 as the number
of co-clusters increases. Therefore, in order to utilize coclustering based scalable matrix approximation methods in
recommender systems, one must address the accuracy issue.

2.3.2

Mˆ ui(2)

Qui(1)

Qui(2)
M ui

Qui( z )
M ui

...

Mˆ ui( z )
z

¦

t 1

Qui(t )

z

¦ Qui( s )

Mˆ ui(t )

s 1

Figure 2: WEMAREC design overview. The original user-item rating matrix M is described by z
low-rank matrices, which are based on z diﬀerent
k × l co-clustering settings. For all pairs (u, i) ∈
[m] × [n], the entry Mui is computed based the corresponding entries in the co-cluster-based submatrices (denoted as shaded regions). The equation
describes how to compute a uniﬁed matrix approximation M̃ from z co-clustering based approximations
{M̂1 , M̂2 , . . . , M̂z }.

Accuracy vs. Rating Distribution

Although the submatrices obtained through co-clustering
are not informative enough to build accurate recommendation models for all users and all items in each submatrix,
they can still be utilized to build “weak” recommendation
models which can accurately predict the common interests
shared by users in the same submatrix. In this study, we
analyze (1) which part of information in each submatrix represents users’ common interests and thus can be utilized to
build “weak” recommendation models and (2) how we can
make these “weak” recommendation models more accurate
when predicting users’ common interests in a submatrix.
Table 1 shows the distribution of diﬀerent ratings in a
submatrix obtained from Bregman co-clustering, as well as
the recommendation quality when applying standard SVD
algorithm on the submatrix. As shown in the third column,
the RMSE varies by the speciﬁc rating and lower RMSEs
(i.e., better recommendation accuracy) are achieved for ratings that occur more frequently in the submatrix, such as 3
and 2 ratings. This makes sense because a learning model
usually does a better job capturing samples that occur more
frequently in the train data. Based on this observation, we
can train “weak” recommendation models from the submatrices, which can at least make accurate recommendations
on users’ common interests, i.e., ratings that occur most frequently in the corresponding submatrix.
Given the biased prediction power of the “weak” models towards ratings that occur more frequently in a submatrix, we could potentially boost the recommendation accuracy by weighting the user-item ratings diﬀerently, assigning
higher (lower) weights to ratings that occur more frequently
(rarely) in the submatrix. The expectation is that we could
obtain more accurate recommendations on the majority of
the ratings and the overall recommendation accuracy can be
improved. The fourth column in Table 1 shows the recommendation quality after adding such weights to diﬀerentiate
the user-item ratings. Clearly, we can see that the “weak”
recommendation model built by SVD with weighting can indeed make more accurate recommendations on the ratings
that occur more frequently (i.e., 2, 3 and 4) than the SVD
method without weighting. More importantly, the overall recommendation quality also increases after weighting,

i.e., RMSE decreases from 0.9517 to 0.9479. These results
demonstrate that rating-speciﬁc weighting has the potential
to boost the accuracy of more frequently-occurring ratings
and enhance the overall accuracy as well.

3.

WEMAREC ALGORITHM DESIGN

In this section, we present the design details of WEMAREC
which can achieve both high recommendation accuracy and
high scalability for matrix approximation based CF methods. As illustrated in Figure 2, WEMAREC consists of three
key steps:
1. Co-clustering and submatrices generation. The
original user-item rating matrix is ﬁrst divided into a set
of submatrices by Bregman co-clustering, so that the scalability issue can be addressed by factoring all submatrices
in parallel. Also, diﬀerent co-clusterings can be obtained
by varying the constraints in Bregman co-clustering, which
naturally oﬀers us the ability to exploit the advantages of different co-clusterings to achieve better recommendation accuracy.
2. Submatrices-based weighting and matrix approximation. A new weighting strategy is proposed, which is
computed based on each individual submatrix and assigns
higher weights to ratings that occur more frequently in a
given submatrix. The weighted submatrices from the same
co-clustering setting are then used to generate a single matrix approximation.
3. Ensemble of multiple matrix approximations. Different co-clustering constraints can lead to diﬀerent submatrices and thus diﬀerent matrix approximations. Since each
“weak” recommendation model can only make accurate recommendations on some of the user-item ratings, an ensemble
strategy is proposed, which utilizes the advantages of different “weak” models to realize a “strong” recommendation
model which can achieve high accuracy.

3.1

Co-clustering & Submatrices Generation

Co-clustering is a popular technique which allows simul-

305

3.2

taneous clustering of both the rows and columns in a given
matrix. By applying co-clustering methods on user-item rating matrix in recommender systems, users and items correspond to a co-cluster (submatrix) are highly correlated, i.e.,
these users will have similar opinions on these items. More
formally, let submatrix M = {Mui | u ∈ U, i ∈ I} denote
the ratings of a subset of users U on a subset of items I. If
we properly choose a set of very similar users U and a set of
very similar items I, then M can be reconstructed by fewer
number of parameters, i.e., lower rank. Such co-clustering
can be beneﬁcial in two aspects: 1) these submatrices can be
approximated simultaneously via parallel computing so that
high scalability can be achieved and 2) each submatrix will
have lower rank than original user-item rating matrix, so
that low-rank matrix approximation can be computed more
eﬃciently for each submatrix.
In order to ﬁnd such coherent submatrices in the useritem rating matrix, we consider to simultaneously partition
all users and items into disjoint user clusters {U1 , . . . , Uk }
and item clusters {I1 , . . . , Il }, and a co-cluster (U , I) corresponds to one desired submatrix. Bregman co-clustering [2]
is adopted in this paper to achieve such partitioning. It
views the co-clustering as a lossy data compression problem, and attempts to obtain as much information as possible about the original matrix with a few number of critical statistics for co-clusters, such as the row and column
averages of each co-cluster. Following common approaches
in Bregman co-clustering, we should ﬁrstly choose a set of
statistics of original matrix that need to be preserved, e.g.,
the average rating of each user or the average rating of each
item, etc., and each statistic can be viewed as a constraint.
We denote C as the constraint set, and six of the most popular non-trivial constraint sets are described as follows:

As described earlier, performing standard matrix approximation on submatrices will not be accurate due to the
fact that each submatrix only holds partial information of
users/items/ratings. Therefore, we can only learn a “weak”
recommendation model from each submatrix, which can make
accurate recommendation on a majority of ratings in the
corresponding submatrix. As demonstrated in the motivating examples (Section 2.3.2), by associating higher weights
to ratings that occur more frequently in a given submatrix,
we could potentially improve not only the recommendation
accuracy for the frequent ratings, but also the overall recommendation accuracy.
Based on the idea above, we propose a new method for
low-rank matrix approximation, which weighs the individual
user-item rating diﬀerently based on the rating distribution
in a submatrix. Speciﬁcally, we compute the probabilistic
distribution of diﬀerent ratings in each submatrix and construct a weighted norm by adding the probabilistic information. Since the weight is a function of the rating distribution,
we can construct the weighting function p(x) : F → R with
Taylor’s formula as follows:
p(x)

where Ir and Ic are the random variables of row and column
indices, which take values over {1, . . . , m} and {1, . . . , n},
respectively. Iˆr and Iˆc are the random variables of row
and column clusters, which take values over {1, . . . , k} and
{1, . . . , l} in a k×l co-clustering. More speciﬁcally, C1 means
that the average values of each row and each column should
be preserved, C2 means the average values of all entries inside each co-cluster should be preserved, and similarly for
the other constraint sets. Besides the constraints, we also
need to select appropriate Bregman divergence to evaluate
a co-clustering, which is deﬁned as follows: for z1 , z2 ∈ R,
dφ (z1 , z2 ) = φ(z1 ) − φ(z2 )− < z1 − z2 , φ (z2 ) >, where
φ is the gradient of diﬀerential function φ. Two popular Bregman divergences are I-divergence and Squared Euclidean distance, deﬁned respectively as follows:
(3)

dφ (z1 , z2 ) = (z1 − z2 )2 , φ(z) = z 2

(4)

= f (Pr[x])
= C0 + C1 Pr[x] + rp
≈ 1 + β0 Pr[x]

(5)
(6)
(7)

Since Pr[x] ∈ [0, 1], the residual term rp , which is super linear to Pr[x], can be omitted. Without loss of generality,
we can assume that C0 = 1 by scaling all the parameters,
then there is only one unknown parameter β0 in the weighting function. The value of β0 should be trained such that
optimal recommendation accuracy can be achieved. However, this is not straightforward, because recommendation
is one-step further after matrix approximation. Therefore,
we choose the optimal β0 by brute force search, in which
we check every β0 value in the linear function p(x). The
sensitivity analysis of β0 is presented in Section 5.2.
After deﬁning the weighting function p(x), we present the
extended SVD and compressed sensing matrix approximation methods here, in which weight Wij is p(Mij ) if the
entry is observed and 0 otherwise. It should be noted that
the standard low-rank matrix approximation methods, such
as SVD, can be regarded as special cases of the proposed
method by setting β0 = 0.

C1 = {{Ir }, {Ic }}, C2 = {{Iˆr , Iˆc }},
C3 = {{Iˆr , Iˆc }, {Ir }}, C4 = {{Iˆr , Iˆc }, {Ic }},
C5 = {{Iˆr , Iˆc }, {Ir }, {Ic }}, C6 = {{Ir , Iˆc }, {Iˆr , Ic }},

dφ (z1 , z2 ) = z1 log(z1 /z2 ) − (z1 − z2 ), φ(z) = z log z

Submatrices-based Weighting and Matrix
Approximation

Extension of SVD :
M̂ = arg min ||W ⊗ (M − X)||F s.t. rank(X) = r. (8)
X

Extension of Compressed Sensing :
M̂ = arg min ||X||∗ s.t. ||W ⊗ (M − X)||F < .

(9)

X

The two optimization problems describe how to estimate
M̂ from observed entries in an original submatrix M. Then,
missing entries in the original submatrix M can be obtained
from M̂, i.e., user ratings on unrated items can be predicted
from M̂ as in other matrix approximation methods.
After applying Bregman co-clustering on the user-item
rating matrix M , a k × l co-clustering (k is the number
of user clusters and l is the number of item clusters) can
be obtained. Then, we can perform the proposed low-rank
matrix approximation on each submatrix. Here, we present
a gradient decent learning algorithm for approximating the

Finally, the row and column clustering will be achieved by
an iterative meta algorithm, in which the row and column
cluster updates can be obtained from the optimal Lagrange
multipliers in parallel. The details of the meta algorithms
for achieving Bregman co-clustering can be found in [7, 8].
Since the meta algorithms are not the contributions of this
paper, details of these algorithms are omitted.

306

user-item rating matrix based on the k × l co-clustering. As
described in Algorithm 1, the weight p(x) for each entry in
each submatrix is ﬁrst computed. Then, the proposed lowrank matrix approximation is achieved by a gradient decent
method with L2 regularization. At last, we combine all the
resulting submatrix approximations, so that the approximated matrix M̂ can be obtained by re-locating each entry
in the k × l submatrices.

sponding user frequently gave such rating to items before
and 2) the corresponding item was frequently rated by such
rating before. More formally, the i-th user (Mi∗ ) and the
j-th item (M∗j ) can be viewed as discrete random variables
over a ﬁnite-ﬁeld F with unique distributions Pr(x; Mi∗ ) and
Pr(x; M∗j ), x ∈ F. Furthermore, the prediction of Mij = x
should be considered more conﬁdent if user i and item j
have (been) rated x many times in M . Therefore, the ensemble weight q(x) : F → R can be regarded as a function
of Pr(x; Mi∗ ) and Pr(x; M∗j ) as follows:

Algorithm 1 Co-clustering-based Matrix Approximation
Input: All co-clustering submatrices M(t) ⊆ M (t ∈ [kl]),
rank r, learning rate v, regularization coeﬃcient λ.
Output: Approximated user-item rating matrix M̂ .
1: for each t ∈ {1, . . . , kl} in parallel do
2:
// Computing weights
3:
Compute the rating distribution on F in M(t) .
4:
for each observed entry (u, i) in M(t) do
5:
Wui = p(x), if Mui = x.
6:
end for
7:
// Updating model
8:
Initialize U (t) ∈ Rm×r , V (t) ∈ Rn×r randomly
9:
while not converged do
10:
for each observed entry (u, i) in M(t) do
(t)
(t)
(t) T
11:
ui = Mui − Ui (Vj )
12:
for each z ∈ {1, . . . , r} do
(t)
(t)
(t)
(t)
13:
Uuz = Uuz + v ∗ ( ui ∗ Viz ∗ Wui − λUuz )
(t)
(t)
(t)
(t)
14:
Viz = Viz + v ∗ ( ui ∗ Uuz ∗ Wui − λViz )
15:
end for
16:
end for
17:
end while
18: end for
19: for each (u, i) ∈ [m] × [n] do
20:
Locate (u, i) in its corresponding submatrix and let
the index of the submatrix be ξ.
(ξ)
(ξ)
21:
M̂ui = Uu (Vi )T
22: end for
23: return M̂

3.3

q(x)

= f (Pr[x; Mi∗ ], Pr[x; M∗j ])
= C0 + C1 Pr[x; Mi∗ ] + C2 Pr[x; M∗j ] + rq
≈ 1 + β1 Pr[x; Mi∗ ] + β2 Pr[x; M∗j ]

(10)
(11)
(12)

Again, by omit the small residual term rq and scaling the
constant term to 1, we can obtain the ﬁnal ensemble weight
as Equation 12. Then, the proposed ensemble method can
be performed as follows:
M̃ui =

z

t=1

(t)

Qui
(t)
M̂ui ,
z
(s)
s=1 Qui

(13)

(t)

where Qui = q(Mui ). Based on Equation 13, we present Algorithm 2 to describe how to predict missing values in useritem rating matrix for recommendation. We can clearly see
that the global matrix approximation M̃ can be eﬃciently
computed because the computation for all entries in M̃ just
requires weighted averaging.
Algorithm 2 WEMAREC Ensemble (u, i)
Input: Resulting matrix approximations M̂ (t) (t ∈ [z])
from z diﬀerent co-clusterings, u and i are the targeted
user and item, respectively.
Output: The predicted rating of user u on item i: M̃ui .
1: // Computing weights
2: for t ∈ [z] do
(t)
(t)
3:
Qui = q(M̂ui )
4: end for
(t)

Q
(t)
5: return M̃ui = zt=1 z ui (s) M̂ui
s=1

Ensemble of Multiple Matrix Approximations

3.4

As described in Section 3.1, a Bregman co-clustering consists of three components: a constraint set C, a Bregman
divergence dφ , and the number of row clusters k and column
clusters l. For convenience, we denote a 3-tuple (C, dφ , k × l)
as a co-clustering. It should be noted that diﬀerent 3-tuples
(C, dφ , k ×l) could lead to diﬀerent matrix approximation results M̂ because each entry in M̂ is generated based on the
corresponding co-cluster. In Section 3.1, we described six
constraint sets and two Bregman divergences, which means
that we can construct 6 × 2 = 12 diﬀerent M̂ s by combining
diﬀerent C and dφ given ﬁxed k and l. Therefore, we propose
an ensemble method to intelligently combine the user-item
rating predictions obtained from multiple matrix approximations based on diﬀerent co-clustering settings. Our goal
is to further enhance the recommendation accuracy.
To recover a global approximation M̃ from z low-rank approximations M̂ (t) (t ∈ [z]), we adopt the weighted mean of
(t)
M̂ij as M̃ij . The weight for the ensemble method should be
determined by the conﬁdences of both users and items. And
a predicted rating from a “weak” recommendation model
should be considered as more important if 1) the corre-

Qui

Running Time Analysis

The proposed weighted and ensemble matrix approximation method (WEMAREC) is faster than many state-of-theart matrix approximation algorithms, although its overall
computational complexity is nearly z times larger than solving a regularized SVD problem. The reasons why the proposed WEMAREC method can run faster are as follow: (1)
Every co-cluster is independent from each other, and matrix approximation on each co-cluster can be computed in
parallel; (2) standard low-rank algorithms have a computation complexity of Ω(rmn) per-iteration, whereas the proposed WEMAREC method signiﬁcantly reduces the computation complexity to Ω(r|U ||I|) per-iteration because user
clusters and item clusters are not overlapping in Bregman
co-clustering; and (3) the users inside each co-cluster are
highly similar, and so are the items. Therefore, lower rank
(r) is required to achieve accurate matrix approximation in
the proposed method than other methods, which further reduces its running time. Besides theoretical analysis, we also
analyze the scalability of the proposed WEMAREC method
in Section 5.

307

4.

ERROR BOUND ANALYSIS

are uniformly distributed in submatrices such that the density of the observed entries in every submatrix is consistent
|Ω|
with each other (i.e., = mn
). Without loss of generality,
we assume n ≥ m, and denote α = max (F) − min (F), where
F is the set of ratings. We start by analyzing the error bound
of the co-clustering-based model M̂ in Proposition 2. Then,
based on Proposition 2, we proceed to derive an error bound
on the global approximation M̃ in Proposition 3.

This section analyzes the generalization error bounds of
the proposed method. We use the root mean squared error
(RMSE), one of the most widely adopted accuracy measures
in recommender systems [1], as the evaluation metric:


n
m 
 1 
D(M̂) = 
(M̂ui − Mui )2
mn u=1 i=1

Proposition 2. If the density of the observed entries is
large enough such that |Ω| ≥ Cμ2 nr log6 n, then with probability of at leat 1−δ, M̂ corresponding to a k×l co-clustering
satisﬁes

(1 + β0 )α
(2 + )
D(M̂ ) ≤ √
(4
(klm) + 2kl),
mn

where M ∈ Fm×n and Pr[max(F) ≥ M̂ui ≥ min(F)] = 1.
Then, the following proposition establishes the error bound
of the proposed weighted matrix approximation method, i.e.,
the RMSE of the weighted low-rank matrix approximation
method on each co-cluster is bounded, so that we can still
ﬁnd optimum submatrix factorization for recommendation
by optimizing the extended optimization problems (Equation 8 and 9).

where δ = (2kln)−3 .
Proof. For every user-item pair (u, i), an observation
Mui is equal to M̂ui +Z where Z is a random variable whose
absolute error is bounded by

Proposition 1. For any M ∈ Fm×n , m, n > 2, δ > 0,
with probability at least 1 − δ over choosing a subset Ω of
entries in M uniformly,

log δ
D(M̂) ≤ DΩ (M̂) +
(max(F) − min(F))2 .
−2|Ω|

W ⊗ (M − M̂ )

Pr[α2 ≥ loss(M̂ui ; Mui ) ≥ 0] = 1
which α = max(F) − min(F). Hence, based on the Hoeﬀding
Inequality, we
 have Pr[D(M̂ ) − DΩ (M̂ ) ≥ ] ≤ e
log δ
α2 , we have
setting  = −2|Ω|

i.e.,
Pr[D(M̂) ≤ DΩ (M̂) +

∞

≤ (1 + β0 )α.

where υ = max (|U |, |I|),γ = min (|U |, |I|). For one k × l co(t)
clustering, there are
 kl diﬀerent submatrices Mt , γ , t ∈
[kl], and obviously t∈[kl] γ (t) ≤ m. Using Cauchy-Schwarz
inequality, we get
 

√
γ (t) ≤ kl
γ (t) ≤ klm.
(15)

. By



Pr[D(M̂) − DΩ (M̂) ≤

≤ (1 + β0 )(M − M̂ )

By applying Theorem 7 in [5] to matrix completion problem
with bounded noise, we get with probability greater than
1 − υ −3 that every co-cluster-based approximation M̂ will
satisfy

γ(2 + )
W ⊗ (M − M̂) F ≤ (1 + β0 )α(4
+ 2) (14)

Proof. Since the entries of Ω are chosen independently
and uniformly, it is reasonable to assume each loss(M̂ui ; Mui )
= (Mui − M̂ui )2 is a random variable and satisﬁes

−2|Ω|2
α2

∞

log δ 2
α ]≥1−δ
−2|Ω|

t∈[kl]

t∈[kl]

Therefore, we can bound the approximation error as follows:


log δ
(max(F) − min(F))2 ] ≥ 1 − δ
−2|Ω|

W ⊗ (M − M̂ )

Therefore, the errors of the new problems are bounded.
Next, we theoretically analyze the generalization error
bounds of the proposed co-clustering based matrix approximation algorithm (Algorithm 1) and the ensemble method
(Algorithm 2). Since the error bound of SVD based low-rank
matrix approximation method has been well analyzed [19,
21], we focus on analyzing the error bound of compressedsensing based method (deﬁned in Equation 9) by using similar analysis techniques as in [5, 6]. As shown in [5, 6],
we can recover a rank r matrix M ∈ Rm×n (n ≥ m) with
probability at least 1 − n−3 , if the number of observed entries is |Ω| ≥ Cμ2 nr log6 n, where C is a constant and μ
is the strong incoherence parameter. However, this result is
not applicable in our case, because the matrix M is approximated by multiple low-rank submatrices. Hence, we develop
a new error bound based on a variant of the aforementioned
conclusion.
The following analysis makes the following assumptions:
(a) every submatrix M is a rank r matrix that satisﬁes the
strong incoherent properties, and (b) the observed entries

F

(a)



(b)



≤

Wt ⊗ (Mt − M̂t ) F

≤ t∈[kl] (1 + β0 )α(4 (2+)
γ (t) + 2)


(c)
≤ (1 + β0 )α(4 2+
(klm) + 2kl)

t∈[kl]

(16)
in which (a) holds due to the triangle inequality of Frobenius
norm; and (b) holds due to (14); and (c) holds due to (15).
Since for all (u, i) pairs, Wui ≥ 1. Then, we have
D(M̂ ) =

M − M̂
√
mn

F

≤

W ⊗ (M − M̂ )
√
mn

F

.

(17)

Combining (16) and (17), we established the error bound of
M̂ as stated above. In order to adjust the conﬁdence level,
we take a union bound of the events W ⊗ (M − M̂) F ≥

+ 2) for each submatrix M(t) , then we
(1 + β0 )α(4 γ(2+)

have
 
 √
√
3
3
υ (t) ≤ 3 kl
υ (t) ≤ 2kln.
t∈[kl]

t∈[kl]

i.e., the inequation√in Proposition 2 holds with probabilities
at least1 − δ (δ = 3 2kln).

308

Proposition 3. If Proposition 2 holds, then with probability of at leat 1 − δ, the M̃ based on z diﬀerent k × l
co-clustering settings satisﬁes:

(1 + β0 )α
(2 + )
(4
D(M̃ ) ≤ √
(klm) + 2kl),
mn

methods of natural language processing, in which user/item
features are estimated by minimizing the sum-squared error.
• BPMF [Salakhutdinov et al., ICML’ 08]: is a Bayesain
extension of probabilistic matrix factorization, in which the
model is trained using Markov chain Monte Carlo methods.
• APG1 [Toh et al., PJO 2010]: views the recommendation
task as a matrix completion problem, and computes the approximation by solving a nuclear norm regularized linear
least squares problem.
• DFC2 [Mackey et al., NIPS’ 11]: divides a large-scale
matrix factorization task into smaller subproblems, and uses
the techniques from randomized matrix approximation to
combine the subproblem solutions.
• LLORMA [Lee et al., ICML’ 13]: relaxes the low-rank assumption, and assumes that the original matrix is described
using multiple low-rank submatrices, which are constructed
using techniques from non-parametric kernel smoothing.

where δ = z(2kln)−3 .
Proof. By Proposition 2, we bound the error of M̃ as
follows:
M̃ − M

(a)
F

≤

(b)

=

(c)

≤

1
z
1
z
1
z





s∈[z]

Q(s) ⊗ (M̃ − M )

s∈[z]

Q(s) ⊗ (M̂ (s) − M )
(s)

F
F

(s)

⊗ (M̂ − M ) F

≤ z1 s (1 + β0 )α(4 2+p
(klm) + 2kl)
p

2+p
= (1 + β0 )α(4
(klm) + 2kl)
p

(d)

s∈[z]

Q



5.2



(s)
where (a) holds because every s∈[z] Qij ≥ s∈[z] 1 = z;
(b) holds due to Equation 13; (c) holds due to the triangle
inequality of Frobenius
√ norm; (d) holds due to Equation 16.
Finally, by dividing mn from both sides, we conclude the
proof. The conﬁdence level is adjusted to z(2kln)−3 using
the union bound property as in Proposition 2.

5.

EXPERIMENTAL RESULTS

This section evaluates the proposed method on real-world
datasets. The ﬁrst study conducts sensitivity analysis. The
proposed method consists of a set of parameters, i.e., rank
of matrices r, and the number of row clusters k and column
clusters l. This study evaluates how the recommendation
accuracy of the proposed method is aﬀected by these parameters. The second study compares the recommendation
accuracy of the proposed method against six state-of-theart matrix approximation based CF methods using PREA
toolkit [13]. The third study evaluates the runtime scalability of the proposed method against standard SVD method.

5.1

Sensitivity Analysis

We ﬁrst show how WEMAREC performs with diﬀerent
combinations of parameters. MovieLens (10M) and Netﬂix
datasets are used in this study with randomly selected 90%
of data as training data and the rest 10% as test data.
Figure 3 presents the eﬀects of the weighted function p(x)
(Eqn. 5) with β0 varying in [0, 2.0] on three artiﬁcially selected datasets (from MovieLens (10M)) with diﬀerent rating distributions. The detailed characteristics of the three
selected datasets are presented in Table 2. As we can see,
the RMSEs on all three datasets ﬁrst decrease as β0 increases from 0, and increases after the optimal accuracies
are achieved. We also observe that optimal β0 s on more uneven datasets are smaller than those on more even datasets.
The reason is that the frequency of diﬀerent ratings are close
on even datasets, so that a greater β0 is required to make
the weights of diﬀerent ratings more diﬀerent. Based on
the above study, we choose β0 = 0.4 for the following experiments because the submatrices generated by Bregman
co-clustering are always uneven.
Figure 4 and 5 analyze the eﬀects of Bregman co-clustering
by changing the rank r and the numbers of row and column
clusters k and l on MovieLens (10M) and Netﬂix dataset.
We can see from the results that recommendation accuracies increase when the rank r increases from 5 to 20 in Figure 4. And the accuracies on the left (I-divergence) and
middle (Euclidean-distance) are worse than those on the
right (combination of these two distances), which indicates
that the combination of diﬀerent approximations M̂ leads
to better recommendation accuracy than both original ones.
Figure 4 also shows that the recommendation accuracy decreases when k and l increase (from 2 × 2 to 3 × 3). This is
due to the fact that each co-cluster based submatrix consists
of less user-item ratings when k and l increase, resulting in
insuﬃcient training data for both model training and prediction. However, the accuracy ﬁrst increases when k and l
increase (from 2 × 2 to 2 × 3) in Figure 5, and then decrease
when k × l = 3 × 3. This implies that larger dataset can be
divided into more clusters to further improve the eﬃciency
with improved accuracy. Based on this study, we choose
rank r = 20 and k × l as 2 × 2 or 3 × 2 in the ensemble
method, which oﬀers the best recommendation accuracy for
WEMAREC.

Experiment Setup

The experimental study uses three real-world datasets that
have been widely used for evaluating recommendation algorithms – 1) MovieLens 1M (106 ratings of 6, 040 users on
3, 706 items); 2) MovieLens 10M (107 rating of 69, 878 users
on 10, 677 items); and 3) Netﬂix (108 rating of 480, 189 users
on 17, 770 items). For each dataset, we split it into train and
test sets randomly by setting the ratio between train set and
test set as 9 : 1. The results are presented by averaging the
results over ﬁve diﬀerent random train-test splits.
We use learning rate v = 0.002 for gradient decent method,
λ = 0.01 for L2 -regularization coeﬃcient,  = 0.0001 for gradient descent convergence threshold, and T = 100 for maximum number of iterations. The proposed method (WEMAREC) is compared against six state-of-the-art matrix
approximation based CF methods, which are described as
follows:
• NMF [Lee et al., NIPS’ 01]: assumes the data and components are non-negative and every entry follows the Poisson
distribution. Then the approximation is achieved by maximizing the log-likelihood.
• Regularized SVD [Paterek et al., KDD’ 07]: is a standard matrix factorization method inspired by the eﬀective

1
2

309

http://www.math.nus.edu.sg/~mattohkc/NNLS.html
http://www.cs.ucla.edu/~ameet/dfc/

High
Median
Low

Rating = 1
0.98%
3.44%
18.33%

Rating = 2
3.14%
9.38%
26.10%

Rating = 3
15.42%
29.25%
35.27%

Rating = 4
40.98%
37.86%
16.88%

Rating = 5
39.49%
20.06%
3.43%

Entropy
1.174590
1.387499
1.445043

0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0
β0

0.888
0.886
0.884
0.882
0.880
0.878
0.876

RMSE

0.798
0.796
0.794
0.792
0.790
0.788
0.786
0.784

RMSE

RMSE

Table 2: Characteristics of three artiﬁcially selected datasets with diﬀerent rating distributions (smaller
entropy means that the dataset is more uneven).

0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0
β0

0.940
0.938
0.936
0.934
0.932
0.930
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0
β0

Figure 3: Eﬀects of weighted function p(x) on the performance of cocluster-based model over high-uneven(left),
median-uneven(middle), low-uneven(right) datasets, with β0 varying in [0, 2.0].
Rank=15

Euclidean-Distance
Rank=20

0.83
Rank=5
0.82
0.81
0.80
0.79
0.78

2×2
3×2
4×2
3×3
#RowCluster × #ColumnCluster

Rank=10

Rank=15

Combination
Rank=20
RMSE

Rank=10

RMSE

RMSE

I-Divergence
0.83
Rank=5
0.82
0.81
0.80
0.79
0.78

0.83
Rank=5
0.82
0.81
0.80
0.79
0.78

2×2
3×2
4×2
3×3
#RowCluster × #ColumnCluster

Rank=10

Rank=15

Rank=20

2×2
3×2
4×2
3×3
#RowCluster × #ColumnCluster

Figure 4: Eﬀects of Bregman co-clustering on the performance of WEMAREC in I-divergence (left),
Euclidean-distance (middle), and Combination of both two above distances (right), with the rank of submatrix varying in {5, 10, 15, 20}, the number of row and column clusters varying in {2 × 2, 3 × 2, 4 × 2, 3 × 3} on
MovieLens (10M).
Rank=15

Rank=20

2×2
3×2
4×2
3×3
#RowCluster × #ColumnCluster

0.86
Rank=5
0.85
0.84
0.83
0.82
0.81

Rank=10

Rank=15

Combination
Rank=20
RMSE

Rank=10

Euclidean-Distance

RMSE

RMSE

I-Divergence
0.86
Rank=5
0.85
0.84
0.83
0.82
0.81

2×2
3×2
4×2
3×3
#RowCluster × #ColumnCluster

0.86
Rank=5
0.85
0.84
0.83
0.82
0.81

Rank=10

Rank=15

Rank=20

2×2
3×2
4×2
3×3
#RowCluster × #ColumnCluster

Figure 5: Eﬀects of Bregman co-clustering on the performance of WEMAREC in I-divergence (left),
Euclidean-distance (middle), and Combination of both two above distances (right), with the rank of submatrix varying in {5, 10, 15, 20}, the number of row and column clusters varying in {2 × 2, 3 × 2, 4 × 2, 3 × 3} on
Netﬂix.
Figure 6 analyzes the eﬀect of ensemble weight function
q(x) (Eqn. 12) by selecting diﬀerent β1 and β2 . And we
can see that the accuracies of the proposed weighted average methods always outperform the simple average method
without weighting (i.e., β1 = 0.0, β2 = 0.0). It seems that
larger β1 will lead to better accuracy, but we also observe
that the RMSE becomes stable when β1 > 40. Therefore,
we adopt β1 = 3.0 and β2 = 40 in the following experiments.

5.3

NMF
RSVD
BPMF
APG
DFC
LLORMA
WEMAREC

MovieLens (10M)
0.8832 ± 0.0007
0.8253 ± 0.0009
0.8195 ± 0.0006
0.8098 ± 0.0005
0.8064 ± 0.0006
0.7851 ± 0.0007
0.7769 ± 0.0004

Netﬂix
0.9396 ± 0.0002
0.8534 ± 0.0001
0.8420 ± 0.0003
0.8476 ± 0.0028
0.8451 ± 0.0005
0.8275 ± 0.0004
0.8142 ± 0.0001

Table 3: RMSE on MovieLens (10M) and Netﬂix
of NMF (r=50) [11], Regularized SVD (r=50) [16],
BPMF(r=30) [18], APG [25], DFC [14], LLORMA
(r=20) [12], WEMAREC (r=20).

Recommendation Accuracy Comparisons

This study evaluates the accuracy of the proposed methods by comparing it with the six state-of-the-art matrix approximation based CF methods summarized in Section 5.1,
i.e., NMF [11], Regularized SVD (RSVD) [16], BPMF [18],
APG [25], DFC [14], LLORMA [12]. Each of the method is
conﬁgured using the same parameters provided by the original paper. For the proposed WEMAREC method, we consider 2∗2∗2 = 8 resulting matrix approximations, which are
constructed by varying C ∈ {C2 , C5 }, dφ ∈ {I-divergence,
Euclidean-distance} and k × l ∈ {2 × 2, 3 × 2}. The MovieLens (10M) and Netﬂix datasets are used in this study.
Table 3 presents the RMSEs of all these matrix approx-

imation based CF methods on the MovieLens (10M) and
Netﬂix datasets. This study shows that the proposed WEMAREC method outperforms all the other six matrix approximation based CF methods on both the datasets. The
reason why the proposed method can further improve the
recommendation accuracy is due to 1) the new low-rank matrix approximation method can build more accurate models
on submatrices because most often appeared ratings (major

310

0.7780

4

0.7776

3

0.7772

2

0.7768

1

0.7764

eﬀectively improve the recommendation system scalability
on large datasets.

6.

0

0.7760
0

5

10

15

20
β1

25

30

35

40

Figure 6: Eﬀects of ensemble weighted function q(x)
on the performance of WEMAREC with diﬀerent β1
and β2 on MovieLens 10M.
1600
1400

Time (s)

1200

SVD
WEMAREC (2×2)
WEMAREC (3×3)
WEMAREC (4×4)
WEMAREC (5×5)

1000
800
600
400
200
0
20

40

60
Rank

80

100

Figure 7: Eﬃciency comparisons of SVD method
and WEMAREC method with diﬀerent sizes of useritem rating matrix and diﬀerent numbers of row
clusters and column clusters.

interests) of users are treated more importantly; and 2) the
ensemble method can eﬀectively take advantage of the highquality recommendation results from diﬀerent co-clusterings
to further improve the recommendation accuracy.

5.4

RELATED WORK

Matrix factorization methods have been widely adopted
in many applications [27], as well as recommendation systems [10]. Billsus et al. [4] initially introduced SVD to collaborative ﬁltering context. Then, Srebro et al. [23] proposed a maximum-margin matrix factorization (MMMF)
method, which can be formulated as a semi-deﬁnite programming problem for achieving matrix approximation based
CF. Rennie et al. [17] investigated a direct gradient-based
optimization method for achieving MMMF based CF, which
can eﬀectively improve the eﬃciency of MMMF method.
Singh et al. [20] introduced a collective matrix factorization
method based on relational learning to generalize existing
matrix factorization methods and yielded new large-scale
optimization algorithms for these problems. Yu et al. [28]
proposed a non-parametric matrix factorization method, to
make matrix approximation based CF methods applicable
on large-scale datasets. Salakhutdinov et al. [15] extended
matrix factorization to probabilistic algorithms by proposing
a Probabilistic Matrix Factorization (PMF) method, which
can scale linearly with the number of observations in the matrix. Based on the above work, a fully Bayesian treatment
of PMF is present By Salakhutdinov et al. [18], which can
train the user-rating matrix in recommender systems using
Markov chain Monte Carlo methods.
In addition to single matrix factorization, ensemble methods have also been investigated in the literature. The Netﬂix Prize winners Bell et al. [3] and Koren et al. [9] utilized
the combination of memory-based and matrix factorization
methods to improve recommendation accuracy. Diﬀerent
from the above work, Mackey et al. [14] introduced a DivideFactor-Combine (DFC) framework, in which the expensive
task of matrix factorization is randomly divided into smaller
subproblems which can be solved in parallel using an arbitrary base matrix factorization algorithms. Lee et al. [12]
proposed a local low-rank matrix approximation (LLORMA)
method, which generalized the DFC method in a way that
a metric structure is used on the original matrix and the
matrix partitions are constructed by kernel smoothing.
The DFC method and LLORMA method share similar
idea with our method in that ensemble methods are adopted
to boost recommendation accuracy. A signiﬁcant diﬀerence
between DFC and LLORMA is the construction of submatrices. In DFC, each submatrix is constructed by random
sampling, while in LLORMA the submatrix is made of nearest neighbors within certain range. Diﬀerent from both of
them, the submatrices in our method are constructed via
partitional co-clustering, so that each submatrix has lowparameter structure with less users and items, i.e., the submatrices in our method are of lower rank. Therefore, matrix
approximation on such submatrices can be performed more
eﬃciently. In addition, a submatrix-based weighting strategy is proposed to capture rating-speciﬁc prediction power
of each submatrix, so that more accurate recommendations
can be generated on more frequent samples in each submatrix. Therefore, the overall recommendation accuracy can
be improved in each submatrix. Finally, the ensemble strategy in the proposed method can leverage both user-speciﬁc
and item-speciﬁc rating distributions to combine the approximation matrices, which considers much more information
compared with simple averaging method in DFC and kernel

β2

5

Scalability Analysis

This study evaluates how the WEMAREC method can
speedup the recommendation process by leveraging parallel computing. The MovieLens (1M) dataset is used in this
study. Leveraging parallel computing, all the submatrices
are processed in parallel, and the execution time of WEMAREC is determined by that of the largest submatrix.
Figure 7 shows the execution time of WEMAREC by
changing the co-clustering settings. The SVD method is
included in this study for comparison purpose. This study
shows that the execution time of SVD and WEMAREC both
increases as the rank increases, and the performance of the
WEMAREC method improves as the sizes of the submatrices decrease. WEMAREC outperforms SVD by 3X – 10X
when the co-clustering setting varies from 2 × 2 to 5 × 5,
and the speedup increases as the number of submatrices
increases. This study demonstrates that WEMAREC can

311

smoothing method in LLORMA. Moreover, the proposed ensemble method is more eﬃcient than the LLORMA method,
because the kernel distance used in LLORMA method is
based on the cosine distances between the rows of factor
matrices, requiring extra singular value decompositions.

7.

[10] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization
techniques for recommender systems. Computer,
42(8):30–37, 2009.
[11] D. D. Lee and H. S. Seung. Algorithms for non-negative
matrix factorization. In Advances in neural information
processing systems, pages 556–562, 2001.
[12] J. Lee, S. Kim, G. Lebanon, and Y. Singer. Local low-rank
matrix approximation. In Proceedings of The 30th
International Conference on Machine Learning, pages
82–90, 2013.
[13] J. Lee, M. Sun, and G. Lebanon. Prea: Personalized
recommendation algorithms toolkit. The Journal of
Machine Learning Research, 13(1):2699–2703, 2012.
[14] L. W. Mackey, M. I. Jordan, and A. Talwalkar.
Divide-and-conquer matrix factorization. In Advances in
Neural Information Processing Systems, pages 1134–1142,
2011.
[15] A. Mnih and R. Salakhutdinov. Probabilistic matrix
factorization. In Advances in neural information processing
systems, pages 1257–1264, 2007.
[16] A. Paterek. Improving regularized singular value
decomposition for collaborative ﬁltering. In Proceedings of
KDD cup and workshop, volume 2007, pages 5–8, 2007.
[17] J. D. Rennie and N. Srebro. Fast maximum margin matrix
factorization for collaborative prediction. In Proceedings of
the 22nd international conference on Machine learning,
pages 713–719. ACM, 2005.
[18] R. Salakhutdinov and A. Mnih. Bayesian probabilistic
matrix factorization using markov chain monte carlo. In
Proceedings of the 25th international conference on
Machine learning, pages 880–887. ACM, 2008.
[19] J. Shawe-Taylor, N. Cristianini, and J. S. Kandola. On the
concentration of spectral properties. In Advances in neural
information processing systems, pages 511–517, 2001.
[20] A. P. Singh and G. J. Gordon. Relational learning via
collective matrix factorization. In Proceedings of the 14th
ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 650–658. ACM, 2008.
[21] N. Srebro, N. Alon, and T. S. Jaakkola. Generalization
error bounds for collaborative prediction with low-rank
matrices. In Advances In Neural Information Processing
Systems, pages 1321–1328, 2004.
[22] N. Srebro, T. Jaakkola, et al. Weighted low-rank
approximations. In Proceedings of the 20th International
Conference on Machine Learning, pages 720–727, 2003.
[23] N. Srebro, J. Rennie, and T. S. Jaakkola. Maximum-margin
matrix factorization. In Advances in neural information
processing systems, pages 1329–1336, 2004.
[24] X. Su and T. M. Khoshgoftaar. A survey of collaborative
ﬁltering techniques. Advances in Artiﬁcial Intelligence,
Jan. 2009. Article ID 421425.
[25] K.-C. Toh and S. Yun. An accelerated proximal gradient
algorithm for nuclear norm regularized linear least squares
problems. Paciﬁc Journal of Optimization, 6(615-640):15,
2010.
[26] B. Xu, J. Bu, C. Chen, and D. Cai. An exploration of
improving collaborative recommender systems via user-item
subgroups. In Proceedings of the 21st International
Conference on World Wide Web, pages 21–30, 2012.
[27] J. Yan, M. Zhu, H. Liu, and Y. Liu. Visual saliency
detection via sparsity pursuit. IEEE Signal Processing
Letters, 17(8):739–742, 2010.
[28] K. Yu, S. Zhu, J. Laﬀerty, and Y. Gong. Fast
nonparametric matrix factorization for large-scale
collaborative ﬁltering. In Proceedings of the 32nd
international ACM SIGIR conference on Research and
development in information retrieval, pages 211–218, 2009.
[29] Y. Zhang, M. Zhang, Y. Liu, and S. Ma. Improve
collaborative ﬁltering through bordered block diagonal
form matrices. In Proceedings of the 36th international
ACM SIGIR conference on Research and development in
information retrieval, pages 313–322. ACM, 2013.

CONCLUSIONS

Matrix approximation methods have shown great success
in recommender systems. However, tradeoﬀ between scalability and accuracy must be made for most existing matrix
approximation based CF methods. In this paper, a weighted
and ensemble matrix approximation method (WEMAREC)
is proposed to improve both recommendation accuracy and
scalability. In WEMAREC, the user-item rating matrix is
partitioned into a set of submatrices, which are then processed in parallel to improve system scalability. To optimize recommendation accuracy, a submatrix-based weighting strategy and an ensemble strategy are proposed. The
weighting strategy improves the accuracy of the majority
ratings of individual submatrices. The ensemble strategy
improves the overall recommendation accuracy by combining multiple sets of co-clustering results based on the userspeciﬁc and item-speciﬁc rating distributions. Experimental
study on MovieLens and Netﬂix datasets demonstrates that
the proposed method can outperform state-of-the-art matrix approximation based CF methods on recommendation
accuracy and scalability.

8.

ACKNOWLEDGMENT

This work was supported in part by the National Natural
Science Foundation of China under Grant No. 61233016,
and the National Science Foundation of USA under Grant
Nos. 1251257, 1334351 and 1442971.

9.

REFERENCES

[1] G. Adomavicius and A. Tuzhilin. Toward the next
generation of recommender systems: A survey of the
state-of-the-art and possible extensions. IEEE Transactions
on Knowledge and Data Engeering, 17(6):734–749, 2005.
[2] A. Banerjee, I. Dhillon, J. Ghosh, S. Merugu, and D. S.
Modha. A generalized maximum entropy approach to
bregman co-clustering and matrix approximation. The
Journal of Machine Learning Research, 8:1919–1986, 2007.
[3] R. Bell, Y. Koren, and C. Volinsky. Modeling relationships
at multiple scales to improve accuracy of large
recommender systems. In Proceedings of the 13th ACM
SIGKDD international conference on Knowledge discovery
and data mining, pages 95–104. ACM, 2007.
[4] D. Billsus and M. J. Pazzani. Learning collaborative
information ﬁlters. In Proceedings of the 15th International
Conference on Machine Learning, pages 46–54, 1998.
[5] E. J. Candès and Y. Plan. Matrix completion with noise.
Proceedings of the IEEE, 98(6):925–936, 2010.
[6] E. J. Candès and T. Tao. The power of convex relaxation:
Near-optimal matrix completion. IEEE Transactions on
Information Theory, 56(5):2053–2080, 2010.
[7] I. S. Dhillon, S. Mallela, and D. S. Modha.
Information-theoretic co-clustering. In Proceedings of the
ninth ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 89–98, 2003.
[8] T. George and S. Merugu. A scalable collaborative ﬁltering
framework based on co-clustering. In The Fifth IEEE Intl.
Conference on Data Mining, pages 625–628. IEEE, 2005.
[9] Y. Koren. Factorization meets the neighborhood: a
multifaceted collaborative ﬁltering model. In Proceedings of
the 14th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 426–434, 2008.

312

