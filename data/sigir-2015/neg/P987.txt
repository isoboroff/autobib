Joint Matrix Factorization and Manifold-Ranking for
Topic-Focused Multi-Document Summarization
∗

Jiwei Tan, Xiaojun Wan and Jianguo Xiao
Institute of Computer Science and Technology, Peking University, Beijing 100871, China
The MOE Key Laboratory of Computational Linguistics, Peking University, Beijing 100871, China

{tanjiwei, wanxiaojun, xiaojianguo}@pku.edu.cn
ABSTRACT

However, the basic manifold-ranking summarization method
constructs the relationships of sentences simply by the bag-ofwords cosine similarity, which cannot faithfully capture the semantic similarity among sentences. We believe a better similarity metric will further improve the performance of the manifold-ranking
based summarization method. In this paper, we propose a Joint
Matrix Factorization and Manifold-Ranking (JMFMR) framework
for topic-focused multi-document summarization, which aims at
learning better sentence similarity scores and better sentence ranking scores simultaneously.
In the following parts of this paper, we first summarize related
works, and then introduce the manifold-ranking framework and the
matrix factorization framework. Then we propose our approach
and the experiments. At last is the conclusion of this paper.

Manifold-ranking has proved to be an effective method for topicfocused multi-document summarization. As basic manifoldranking based summarization method constructs the relationships
between sentences simply by the bag-of-words cosine similarity,
we believe a better similarity metric will further improve the effectiveness of manifold-ranking. In this paper, we propose a joint optimization framework, which integrates the manifold-ranking process with a similarity metric learning process. The joint framework
aims at learning better sentence similarity scores and better sentence ranking scores simultaneously. Experiments on DUC datasets
show the proposed joint method achieves better performance than
the manifold-ranking baselines and several popular methods.

Categories and Subject Descriptors

2.

H.3.1 [Information Storage and Retrieval]: Content Analysis
and Indexing—abstracting methods

Keywords
Multi-document summarization; matrix factorization; manifoldranking

1.

RELATED WORK

There have been quite a lot works on multi-document summarization and in this section we mainly focus on the unsupervised
topic-focused extractive-based summarization task. Some methods treat summarization as a ranking task and apply graph-based
ranking algorithms. [13] propose to use manifold-ranking to make
uniform use of sentence-to-sentence and sentence-to-topic relationships. [12] extend a multi-modality manifold-ranking by considering the within-document sentence relationships and the crossdocument sentence relationships as two modalities. [1] further improve the manifold-ranking based relevance propagation via mutual
reinforcement between sentences and theme clusters.
Matrix factorization technique has been applied to summarization tasks in several works. For example, [8] propose using nonnegative matrix factorization to extract the query relevant sentences.
[9] use nonnegative matrix factorization and K-means clustering
for sentence clustering and extraction. [14] propose using sentencelevel semantic analysis and symmetric nonnegative matrix factorization to factorize the sentence-sentence similarity matrix.

INTRODUCTION

Multi-document summarization (MDS) aims at producing a
summary of the major information from a set of documents. Topicfocused multi-document summarization is a task which requires
the generated summary to be biased to a given topic. As topicfocused MDS can provide a personalized profile, it is useful in
news services, QA systems and search engines. Manifold-ranking
based summarization method [13] has proved useful to deal with
the topic-focused MDS task. It makes uniform use of the sentenceto-sentence relationships and sentence-to-topic relationships in a
manifold-ranking process, and does not need a training procedure
or subtle parameter tuning, which makes it more appropriate for
practical application.

3.

PRELIMINARIES

In this section we first introduce the framework of manifoldranking based multi-document summarization in [13]. Then we
will introduce the matrix factorization technique for latent semantic similarity learning in our approach.

∗Xiaojun Wan is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
SIGIR’15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767765.

3.1

Manifold-Ranking Method

Given a set of data points as χ = {s1 , s2 , . . . , sN }, representing sentences to be ranked including the pseudo-sentence as s1 , let
Γ : χ → R denotes a ranking function which assigns each point
si (1 ≤ i ≤ N ) with a ranking value fi . Γ can be viewed as a vector F = [f1 , . . . , fN ]T . Define a prior vector Y = [y1 , . . . , yN ]T ,
in which y1 = 1 because s1 is a topic description pseudo-sentence

987

4.2

and yi = 0 (2 ≤ i ≤ N ) for all the remaining sentences to be
ranked. The adjacency matrix of the graph can be denoted as
W = {Wij } (1 ≤ i, j ≤ N ), with each element Wij corresponding to the similarity between si and sj (Wii is forced to 0 to avoid
self-reinforcement). Then the manifold-ranking algorithm can be
formulized as a regularization framework of minimizing:
N
X

1
1
ϕ(F) =
Wij √ fi − p fj
D
Djj
ii
i,j=1

2

+µ

N
X
i=1

|fi −yi |2

In the joint framework, we adopt a weighted nonnegative matrix
factorization technique similar to WTMF, to learn sentence similarity. Specifically, we integrate similarity learning and sentence score
learning into one optimization objective, as minimizing Φ:
Φ=

(1)

F
where S = D

3.2

−1/2

WD

= αSF
−1/2

(t)

+ (1 − α) Y

+

(2)

N
X
i=1

|fi −yi | 2

′′

Wij (P·,i ·Q·,j −Xij)2 +λkPk22 +λkQk22

(5)

In the objective function (5), P, Q and F are to be optimized,
and constrained to be nonnegative. µ and λ are free parameters
to control the regularization terms. By optimizing the objective
function, similarity measurements of the sentences will be got from
Q, as sim (si , sj ) = cos (Q·,i , Q·,j ); simultaneously the ranking
scores of sentences will be got from F . We will explain the idea underlying the objective function as follows. We define the following
two sub-objective functions:

(3)

φ1 =

N
X

N

′

Wij √

i,j=1

φ2 =

M X
N
X
i=1 j=1

X
1
1
fi − p
fj 2 +µ |fi −yi |2
Dii
Djj
i=1

′′

Wij (P·,i ·Q·,j −Xij)2 +λkPk22 +λkQk22

(7)

(8)

Sub-objective function (7) corresponds to the objective function
of manifold-ranking. The only difference is that W in the objective
′
function of manifold-ranking is now replaced by W , a function of
an optimization variable Q. Sub-objective function (8) corresponds
to the objective function of matrix factorization, which is similar
to the WTMF framework, and we call it Weighted Textual Nonnegative Matrix Factorization (WTNMF). Besides the nonnegative
constraints, the main difference
between WTNMF and WTMF is
′′
that when Xij 6= 0, Wij is changed from 1 to efj . The change
aims to strengthen the influence of the terms in the important
sentences which have high ranking scores obtained by manifoldranking. This aim is achieved by forcing P·,i · Q·,j closer to Xij
if sentence sj is more important. In the joint objective function,
manifold-ranking is affected by similarity measurements from matrix factorization, and matrix factorization is affected by sentence
importance scores from manifold-ranking. This co-effect will lead
to a different convergence state from separate matrix factorization
and manifold-ranking objective functions, which we believe will
help improve the results.

ωm is a very small weight for penalty. The intuition of a small ωm
when Xij = 0 in WTMF is to diminish the influence where word
wi is missed in the sentence sj , and the weighted matrix factorization is argued to yield better latent vectors than traditional matrix
factorization [3].

4.1

+µ

where Wij = cos (Q·,i , Q·,j ) represents the cosine value of the angle between latent semantic vectors Q·,i and Q·,j , intuitively representing the cosine similarity of sentence si and sj after matrix
factorization. D is a diagonal matrix with its (i, i)-element equals
′
to the sum of the i-th row of W . Y is same as in Section 3.1. X is
same as in WTMF, representing the word-sentence co-occurrence
′′
matrix. W here is defined as:
(
′′
efj , if Xij 6= 0
Wij =
(6)
ωm , if Xij = 0

where λ is a free regularization factor to control the regularization
term, and the weight matrix W defines a weight for each cell in X.
P·,i is a K-dimensional latent semantic vector profile for word wi
and Q·,j is the K-dimensional vector profile that represents sentence sj . Then the similarity of two sentences sj and sj ′ can be
calculated by the cosine similarity of Q·,j and Q·,j ′ . The weight
matrix W is defined as:
(
1,
if Xij 6= 0
Wij =
(4)
ωm , if Xij = 0

4.

2

s.t. Pki ≥ 0, Qkj ≥ 0, fi ≥ 0. (1 ≤ k ≤ K)

and α = 1/ (1 + µ).

Wij (P·,i · Q·,j − Xij )2 + λ kPk22 + λ kQk22

M X
N
X

1
1
fj
fi − p
Dii
Djj

′

We introduce a Weighted Textual Matrix Factorization (WTMF)
model [3] for assessing latent semantic text similarity. Given a M ×
N co-occurrence matrix X, where the row number M corresponds
to M words in the corpus, and the column number N corresponds
to N sentences. Each cell Xij contains the TF-IDF value of word
wi in the sentence sj . The WTMF framework aims at factorizing
matrix X into two matrices such that X ≈ PT Q, where P is a
K × M matrix, and Q is a K × N matrix. K is the dimension
of latent vectors. The main difference between WTMF and LSA
[7] is that WTMF allows for direct weight control on each matrix
cell Xij . The model variables to be solved (vectors in P, Q) are
optimized by minimizing:

i=1 j=1

′

Wij √

i=1 j=1

Matrix Factorization

M X
N
X

N
X

i,j=1

where D is a diagonal matrix with its (i, i)-element equals to the
sum of the i-th row of W. Let F ∗ denotes the solution of minimizing ϕ (F ), and F ∗ can be solved by iterating (2) until convergence:
(t+1)

Joint Learning Objective

OUR PROPOSED METHOD
Overview

We propose a joint approach of matrix factorization based
manifold-ranking, to learn the similarity metric and sentence
ranking scores in a framework simultaneously. The underlying idea is that a better similarity metric will help improve
the manifold-ranking process; meanwhile information from the
manifold-ranking process can also help learn a better similarity
metric. In this section we first introduce the objective function of
our proposed joint approach, and then we discuss how to solve the
minimization problem and the inner weighted nonnegative matrix
factorization problem.

4.3

Approximate Optimization Algorithm

In this section, we discuss how to minimize objective function
(5). Gradient based optimization algorithms can be used, but they

988

5.

are too time-consuming for this task. To solve this problem, we
explore an alternating strategy to find an approximate solution to
the optimization problem. The approximate algorithm optimizes
P, Q and F alternately. When optimizing P and Q, F is treated
as fixed, and in this step P and Q are optimized to convergence, by
minimizing φ2 to convergence. After the optimization of P and Q,
an adjacency matrix for manifold-ranking can be calculated from
Q. Then it is turn to fix P, Q and optimize F to convergence, by
minimizing φ1 with (2) to convergence. Then a new F is achieved
and the alternating procedure can be restarted to optimize P and
Q again. This alternating optimization is repeated several times,
and an approximate solution will be achieved. The entire procedure is shown in Algorithm 1. The underlying idea of the alternating strategy is that in the matrix factorization procedure, a new
similarity metric is learned, and a better similarity metric will help
the manifold-ranking procedure learn better sentence importance
scores; at next step, better sentence importance scores will further
help improve matrix factorization results. After several repeats, an
approximate stable state will be reached and better similarity measurements and ranking scores of sentences will be achieved.

5.1

5.2

′t

Update W and Dt according to Qt+1 .
Minimize φ1 to convergence. Get F t+1 .
t = t + 1.
until convergence or the iteration number reaches a predefined number
Output: F ∗ and Q∗ .

Weighted Nonnegative Matrix Factorization

In this section we discuss how to optimize P, Q in (8). In
WTMF [3] without nonnegative constraints, P, Q are computed
iteratively by the following equations:

(9)

′′

where W̃(i) = diag(Wi,· ) is an N × N diagonal matrix containing
′′
′′
the i-th row of weight matrix W and W̃(j) = diag(W·,j ) is an
′′
M × M diagonal matrix containing the j-th column of W . The
derivation of (9) can be found in [10].
Inspired by the alternating nonnegative least squares algorithm
used in [6], we apply the nonnegative least squares algorithm to
solve (9) by solving the reformed alternating least squares problem
(10). We used a C++ implement of nonnegative least squares tool
called tsnnls 1 in the experiment to solve (10).

P·,i

min

Q·,j

1






T
QW̃(i) QT + λI P·,i − QW̃(i) Xi,·


PW̃(j) PT + λI Q·,j − PW̃(j) X·,j

2
2
2

Results

To prove the effectiveness of the proposed method (JMFMR),
we implemented two baselines of the basic manifold-ranking
method (MR Baseline) [13] and a strong baseline (WTNMF-MR).
WTNMF-MR replaces the bag-of-words similarity in the basic
manifold-ranking method with latent semantic similarity learned
with WTNMF. The main difference between WTNMF-MR and
JMFMR is that WTNMF-MR does not consider the mutual effect of
the manifold-ranking process and matrix factorization process. The
comparison intends to test the effectiveness of the joint model. Parameters of WTNMF-MR are the same with JMFMR. The ROUGE
scores of recall are shown in Table 1. Two-tailed t-tests showed that
the improvements of JMFMR to the two baselines are all statistically significant (p ≪ 0.001).
We also compared our method (JMFMR) with several representative unsupervised MDS methods, including 1) manifoldranking based methods: Multi-modality manifold-ranking (MMMR) [12], Mutually reinforced manifold-ranking (RDRP-AP) [1];
2) matrix factorization based methods: Latent Semantic Analysis
(LSA) and Symmetric Nonnegative Matrix Factorization (SNMF)
[14]; 3) other typical methods: query Latent Dirichlet Allocation (qLDA) and Topic Modeling with Regularization (TMR) [11],
Multi-objective Optimization model (MO-LEX) [5], Document
Summarization based on Data Reconstruction (DSDR-non) [4].
Results of these methods for comparison are directly borrowed
from respective papers as shown in Table 1 and Table 2. As we
cannot get the summary-level scores for each document set, we are
unable to perform t-test between our results with the results directly
copied from published papers. However, we can see the large performance gap between our results and these results.
Tables 1 and 2 show the results over ROUGE recall and Fmeasure scores on the DUC 2006 dataset [2]. The parameters of our
model are tuned on DUC 2005 dataset and set as follows: µ = 0.1,
λ = 0.09 and 6 iterations. We also set K = 100, ωm = 0.0005
which are the same with WTMF [3]. As we can see from Tables 1
and 2, the proposed joint optimization method achieves significant
improvement over the MR Baseline and WTNMF based manifoldranking method. Compared with other unsupervised methods, the
proposed method also achieves better results over most metrics,

Input: X, Y , µ, λ, K and ωm
Initialization: Set t = 0, initialize Pt = Qt = 1, F t = 0
repeat
′′ t
Update W according to F t .
Minimize φ2 to convergence. Get Pt+1 and Qt+1 .

min

Experiment Setup

We conducted the experiments similarly to [13]. For every topic
and its related documents, we first constructed the word-sentence
co-occurrence matrix X, and initialize all elements of P, Q and F
with 1. Y = [1, 0, . . . , 0]T , and parameters µ, λ, K and ωm are
set before the optimization procedure starts.
After the optimization procedure in Algorithm 1 is finished, the
ranking scores and similarity measurements of sentences can be got
from F ∗ and Q∗ , respectively. Then the greedy algorithm in [13] is
applied to remove redundancy between sentences. A final summary
is formed with the highest overall ranking score sentences.

5.3


−1
T
P·,i = QW̃(i) QT + λI
QW̃(i) Xi,·

−1
Q·,j = PW̃(j) PT + λI
PW̃(j) X·,j

Dataset and Evaluation Metric

We conducted experiments on the widely used Document Understanding Conference (DUC) datasets, with DUC 2005 as the development set to determine the parameters and DUC 2006 as the test
set. We used the popular ROUGE toolkit2 for evaluation.

Algorithm 1 Approximate optimization algorithm

4.4

EXPERIMENTS AND EVALUATIONS

(10)

2

s.t. Pki ≥ 0, Qkj ≥ 0. (1 ≤ k ≤ K)

2

http://haydn.isi.edu/ ROUGE/
The entire ROUGE command used is “ROUGE-1.5.5.pl -n 2 -m -x
-2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -a -d -l 250”.

http://www.jasoncantarella.com/wordpress/software/tsnnls/

989

ROUGE-1 ROUGE-2 ROUGE-SU4
0.41482
0.08921
0.14671
0.39615
0.08975
0.13905
0.4030
0.0913
0.1449
0.41176
0.08759
0.14213
0.40211
0.08687
0.14419
0.40908
0.08744
0.14404
0.40552
0.08374
0.14169
0.30217
0.04947
0.09788

ROUGE−1

Figure 1(a): ROUGE−1 vs. µ

Table 1: ROUGE Recall scores on DUC 2006

0.36

ROUGE-1 ROUGE-2 ROUGE-SU4
0.41244
0.0887
0.14585
0.40306
0.08508
0.13997
0.39551
0.08549
0.13981
0.33087
0.05022
0.10226
0.33168
0.06047
———
0.32082
0.05267
0.10408

0

0.001 0.01

0.05

0.06

0

0.001 0.01

0.05

0.06

0.07

0.08 0.09
0.1
0.2
µ
Figure 1(b): ROUGE−1 vs. λ

0.3

0.4

0.5

1

10

0.07 0.08 0.09
0.1
0.2
0.3
λ
Figure 1(c): ROUGE−1 vs. Iteration

0.4

0.5

1

10

4

8

0.4
0.38
0.36
0.34

ROUGE−1

Method
JMFMR
MM-MR
SNMF
LSA
DSDR-non
NIST Baseline

0.4
0.38

0.34

ROUGE−1

Method
JMFMR
RDRP-AP
MO-LEX
TMR
qLDA
WTNMF-MR
MR Baseline
NIST Baseline

0.4
0.38
0.36
0.34

1

2

3

5

6

7

9

10

Iteration

Table 2: ROUGE F-measure scores on DUC 2006

Figure 1: Parameter Sensitivity

which demonstrates the effectiveness of the proposed joint matrix
factorization and manifold-ranking method.

[3] Weiwei Guo and Mona Diab. Modeling sentences in the
latent space. In ACL, pages 864–872. Association for
Computational Linguistics, 2012.
[4] Zhanying He, Chun Chen, Jiajun Bu, Can Wang, Lijun
Zhang, Deng Cai, and Xiaofei He. Document summarization
based on data reconstruction. In AAAI, 2012.
[5] Lei Huang, Yanxiang He, Furu Wei, and Wenjie Li.
Modeling document summarization as multi-objective
optimization. In IITSI, pages 382–386. IEEE, 2010.
[6] Yong-Deok Kim and Seungjin Choi. Weighted nonnegative
matrix factorization. In Acoustics, Speech and Signal
Processing, 2009., pages 1541–1544. IEEE, 2009.
[7] Thomas K Landauer, Peter W Foltz, and Darrell Laham. An
introduction to latent semantic analysis. Discourse processes,
25(2-3):259–284, 1998.
[8] Sun Park, Ju-Hong Lee, Chan-Min Ahn, Jun Sik Hong, and
Seok-Ju Chun. Query based summarization using
non-negative matrix factorization. In Knowledge-Based
Intelligent Information and Engineering Systems, pages
84–89. Springer, 2006.
[9] Sun Park, Ju-Hong Lee, Deok-Hwan Kim, and Chan-Min
Ahn. Multi-document summarization based on cluster using
non-negative matrix factorization. In SOFSEM 2007: Theory
and Practice of Computer Science, pages 761–770. Springer,
2007.
[10] Nathan Srebro, Tommi Jaakkola, et al. Weighted low-rank
approximations. In ICML, volume 3, pages 720–727, 2003.
[11] Jie Tang, Limin Yao, and Dewei Chen. Multi-topic based
query-oriented summarization. In SDM, volume 9, pages
1147–1158. SIAM, 2009.
[12] Xiaojun Wan and Jianguo Xiao. Graph-based multi-modality
learning for topic-focused multi-document summarization. In
IJCAI, pages 1586–1591, 2009.
[13] Xiaojun Wan, Jianwu Yang, and Jianguo Xiao.
Manifold-ranking based topic-focused multi-document
summarization. In IJCAI, volume 7, pages 2903–2908, 2007.
[14] Dingding Wang, Tao Li, Shenghuo Zhu, and Chris Ding.
Multi-document summarization via sentence-level semantic
analysis and symmetric matrix factorization. In SIGIR, pages
307–314. ACM, 2008.

5.4

Parameter Sensitivity Study

In this section we explore how the proposed optimization model
is influenced by different parameter values on F-measure scores of
ROUGE-1. When tuning one parameter, we fixed other parameters
at the initial values in the experiments introduced in Section 5.3.
It can be seen from Figure 1(a) that the model gets best results
around µ = 0.1, and the overall results are good when µ ≤ 0.1, and
fall down rapidly when µ is too large. The reason can be got from
the iteration equation (2), that when µ is too large, α = 1/ (1 + µ)
becomes too small, and F is mainly determined by the initial score
vector Y with most elements being 0. In Figure 1(b) the results are
relatively good around λ = 0.05 to λ = 1, showing that the model
is not fiercely influenced by the matrix factorization regularization
parameter λ. In Figure 1(c) the results achieve a relatively stable
state after 4 iterations, so after 6 times of iteration the optimization
procedure can be viewed as convergent.

6.

CONCLUSION AND FUTURE WORK

In this paper, we propose a joint optimization framework to integrate the manifold-ranking process and the similarity metric learning process simultaneously. In the future work, we will explore
more syntactic and semantic information of the documents to further improve the similarity metric learning.

7.

ACKNOWLEDGMENTS

This work was supported by National Hi-Tech Research and
Development Program (863 Program) of China (2015AA015403,
2014AA015102) and National Natural Science Foundation of
China (61170166, 61331011).

8.

REFERENCES

[1] Xiaoyan Cai and Wenjie Li. Mutually reinforced
manifold-ranking based relevance propagation model for
query-focused multi-document summarization. Audio,
Speech, and Language Processing, 20(5):1597–1607, 2012.
[2] Hoa Trang Dang. Overview of duc 2006. In Document
Understanding Conference. New York City, 2006.

990

