A Random Walk Model for Optimization of
Search Impact in Web Frontier Ranking
Giang Tran∗

Ata Turk

L3S Research Center /
University of Hannover
Hannover, Germany

Boston University
Boston, United States

ataturk@bu.edu

gtran@l3s.de
B. Barla Cambazoglu

Wolfgang Nejdl

Yahoo Labs
Barcelona, Spain

L3S Research Center /
University of Hannover
Hannover, Germany

barla@yahoo-inc.com

nejdl@l3s.de

ABSTRACT

1.

Large-scale web search engines need to crawl the Web continuously to discover and download newly created web content. The speed at which the new content is discovered and
the quality of the discovered content can have a big impact
on the coverage and quality of the results provided by the
search engine. In this paper, we propose a search-centric
solution to the problem of prioritizing the pages in the frontier of a crawler for download. Our approach essentially
orders the web pages in the frontier through a random walk
model that takes into account the pages’ potential impact
on user-perceived search quality. In addition, we propose a
link graph enrichment technique that extends this solution.
Finally, we explore a machine learning approach that combines different frontier prioritization approaches. We conduct experiments using two very large, real-life web datasets
to observe various search quality metrics. Comparisons with
several baseline techniques indicate that the proposed approaches have the potential to improve the user-perceived
quality of web search results considerably.

Large-scale search engines need to crawl the web content
continuously to be able to cope with the constant growth and
the content-wise dynamic nature of the Web [25]. In commercial search engines, web pages are downloaded through
two independent crawling processes: refreshing and discovery [5]. The objective of the first process is to selectively
refetch pages whose content was already downloaded and
stored in the web repository of the search engine. This way,
potential modifications in the online content can be detected
and reflected to the index of the search engine, reducing the
staleness of search results presented to the users. The objective of the second process is to discover previously unseen
pages and fetch their content for the first time. This lets the
search engine grow its web frontier1 and increase its coverage of the Web. While discovery may be seen primarily as a
mechanism to increase the recall of the search engine (with
respect to the content available on the Web), it can also
affect the quality of search results presented to the users.
In this work, we are mainly interested in the discovery
process, which essentially requires maintaining a priority
queue for the web pages in the frontier. The download decisions of the crawler are guided by this queue such that
highly ranked (important) web pages in the queue are tried
to be fetched earlier than the rest. Besides the scalability
and efficiency challenges in maintaining a web-scale priority queue [17, 21], the main issue here is to decide how the
pages should be ordered in the queue for download. This
is because, in practice, delaying the download of important
pages may have direct implications on the overall quality of
the search engine’s web repository and, consequently, on the
user experience with the retrieved search results.
The discovery process requires a metric to quantify web
page importance and prioritizing pages accordingly. The
early work on this issue relied on page importance metrics
that exploit the connectivity of a page in the web graph.
A commonly adopted approach was to attribute higher importance to pages that have larger PageRank values (computed over a partially complete web graph). Although the
connectivity-based metrics performed fairly well in model-

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval

Keywords
Web search engine; web crawling; URL prioritization; web
frontier; discovery; frontier ranking; result relevance; random walks.
∗
This work is conducted during the author’s internship at
Yahoo Labs Barcelona.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
SIGIR’15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767737.

INTRODUCTION

1
The web frontier refers to the set of web pages that were discovered by the crawler but whose content is not yet fetched.

153

2.1

ing page importance, they were indirect in the way they
captured the impact of crawling a web page on the search
engine’s result quality. Based on this observation, the latter
work adopted a more direct approach where the discovery
process is guided by the actual impact on search quality. For
example, the technique proposed in [29] prioritized pages
in decreasing order of their likelihood of being retrieved in
search result pages, increasing the utility of web crawling
for many search queries and users. The main difficulty in
search-centric approaches is the need to estimate the impact
of a web page that is not yet exposed to any user in search
results. That is, estimating the impact of a page requires
using only auxiliary signals without exploiting any explicit
user feedback (e.g., clicks or dwell time) about the page.
The main contribution of this work is a hybrid frontier
prioritization approach that combines the two lines of work
mentioned above. In particular, we propose a novel random
walk model that incorporates the inferred search impact of
pages into the standard connectivity-based page importance
computation. We measure the actual search impact of a web
page in terms of two different metrics: the number of times
the page is clicked and the number of times it is viewed
in search results. To infer these two measures for a newly
discovered web page, we exploit the observed click and view
counts of its referring pages which were previously shown
in search results. Moreover, we enhance our random walk
model by a novel teleportation approach which lets us go
beyond the original web graph by connecting pages that have
a good chance of being influential for each other in terms
of their search impact. Finally, we combine the proposed
technique and various baselines under a machine learning
model to show further improvements.
We evaluate the performance of the proposed frontier prioritization approach over two large-scale, real-life web collections. As the ground truth for search quality, we use a
web search query log obtained from Yahoo. Comparisons
with several frontier prioritization approaches taken from
the literature indicate that the proposed approach can lead
to considerable improvement in search quality. In particular, we observe that our approach obtains up to 60%–70% of
the search quality attainable by an ideal web crawler while
outperforming the best baseline by a margin of 5%–10%.
The rest of the paper is organized as follows. We present
some preliminary information about our problem in Section 2. In Section 3, we describe the data and baselines
used in our work. The proposed frontier prioritization approach is presented in Section 4. We report the results of
our experiments in Section 5. In Section 6, we present a
machine learning solution for the problem. In Section 7, we
survey the related work. Section 8 concludes the paper.

2.

Page Importance Measures

As discussed in Section 1, previously proposed page importance metrics can be grouped under two headings as
connectivity-based [1, 23, 27] and search-centric [28, 29]
page importance metrics. Herein, we are only interested
in search-centric page importance metrics as they provide a
more direct measure of the crawled web collection’s utility
for the search engine. For a given web page p, we measure
the search impact in two different ways:
• Click impact: The click impact, IC (p, t), of page p indicates the number of times the page’s link was clicked
in search results during a period of t units of time since
the page was last crawled. This definition is the same
as the measure used in [14], which compares the effect
of various crawling policies on web search effectiveness.
• View impact: The view impact, IV (p, t), of page p indicates the number of times the page’s link was displayed
in the top 10 search results during a period of t units of
time since the page was last crawled. This definition is
similar to (but simpler than) the measure in [29], where
the view count of a page is normalized by taking into
account the rank of the page in search results. In our
work, we prefer not to use this sort of normalization as
we want to have query-independent impact definitions
that can be computed efficiently during crawling.
Without loss of generality, these two impact measures can
be extended to a set of pages. For a set S of web pages, the
click impact is denoted by IC (S, t) and is equal to the sum
of the click impacts of all pages in S. The view impact of S
is denoted by IV (S, t) and is computed similarly.
In the rest of the paper, we use the above-mentioned two
measures although the proposed techniques are not tied to a
particular measure. In our analyses, we focus on the shortterm search impact of the crawled collection (e.g., within a
few days) rather than the long-term impact, which largely
depends on the recrawling policy as addressed in [7, 26, 28].

2.2

Problem Definition

We formally define the frontier prioritization problem in
the expected case as follows.
Definition 1 (Frontier Prioritization Problem).
Given a set D of already crawled URLs, a set F of URLs
in the frontier, a time period t, and a limit ` indicating
the number of new URLs to be added to the collection,2
find a subset S ⊂ F , such that |S| ≤ `,Pand depending
on the impact definition, either EC (S) = p∈S IC (p, t) or
P
EV (S) = p∈S IV (p, t) is maximized, where EC and EV are
two potential objective functions in the expected case.
We note that the described frontier prioritization problem is NP-hard in the worst case and cannot admit the fully
polynomial-time approximation scheme [29]. Hence, we opt
for an expected-case optimization, where the time complexity of the algorithm depends on the time complexity of the
computation of rank distribution.

PRELIMINARIES

Search engines continuously crawl the Web to discover and
download newly created content. It is practically impossible
to crawl all the pages in the Web within a time frame that
will enable meaningful use of the crawled collection. Thus,
crawlers aim to download and maintain a reasonably representative and high-quality snapshot of the Web by (i) keeping important pages in the collection fresh, (ii) expanding
the collection with new important pages, and (iii) removing unimportant pages from the collection. Herein, we focus
on (ii), which requires prioritizing the pages in the crawler’s
frontier according to a page importance metric.

3.

EXPERIMENTAL SETUP

In this section, we first introduce the datasets we used to
evaluate the proposed techniques. We then provide a brief
description of the techniques we used as baselines. Table 1
presents some statistics about the experimental datasets.
2

154

We assume a system crawling the web pages in batches.

Table 1: Dataset statistics
Dataset
Wikipedia
WebCrawl

3.1
3.1.1

total # of pages
D
F
476,805
127,411
74,270,857 23,613,846

# of clicked pages
DC
FC
98,709
34,926
105,495
94,902

# of viewed pages
DV
FV
148,812
45,213
245,092
194,512

Datasets
Wikipedia

4.

IMPACT-AWARE FRONTIER PRIORITIZATION

In this section, we describe the proposed web frontierprioritization approach. The heart of our approach lies in
the propositions that (i) high impact pages in F are more
likely to be discovered by following the outbound links of
high impact pages in D, and (ii) the current web browsing trend is such that a user is more likely to “teleport”
to a related page by going through a search engine instead
of jumping to a random page within a browsing session.
Based on these two propositions, we propose a random walk
model that subsumes the impact of pages into the traditional link-based computation (proposition (i)). We then
enhance the model with a novel teleportation mechanism
(proposition (ii)) by virtually connecting similar pages based
on their link structure. Finally, we elaborate on the convergence property of our model.

4.1

Web Crawl

Impact-Based Linkage Analysis

We argue that high impact pages in D are more likely
to link other high impact pages in F . We assume that,
for each page p ∈ D, we already have the IC (p, ∆t) and/or
IV (p, ∆t) scores. Let DC , DV , DNC , and DNV denote the set
of clicked, viewed, not clicked, and not viewed pages in D,
respectively. Similarly, let FC , FV , FNC , and FNV denote the
set of clicked, viewed, not clicked, and not viewed pages in F ,
respectively. We note that D = DC ∪ DNC = DV ∪ DNV and
F = FC ∪FNC = FV ∪FNV . As an example, based on the click
impact metric, Figs. 1(b) and 1(c) illustrate the partition of
a sample D set into DC and DNC sets and the partition of
a sample F set into FC and FNC sets, respectively.

The WebCrawl dataset is obtained by sampling pages from
a large-scale web crawl performed by Yahoo during June
2014. In this case, set D contained pages that were crawled
before June 15, 2014, while set F contained pages linked by
the pages in set D and crawled before the end of June 2014.
We filtered both sets D and F to ensure that D ∩ F = ∅. As
in the case of the previous dataset, for each page in D and
F , we counted the number of times it was shown in a search
result page and the number of times it was clicked during a
limited time period after the page was crawled.

3.2

frontier impact ratio
|FC |/|F | |FV |/|F |
27.4%
35.5%
0.4%
0.8%

ferring anchor text and a set of search queries issued
in the past [29].
• Q-Hybrid (query hybrid): The impact of a page is computed as a linear combination of a PageRank-based
impact estimate and the Q-Overlap score [29]. As suggested in [29], we use decision tree regression6 to map
the PageRank scores to impact estimates.

Wikipedia is an Internet encyclopedia with 18 billion page
views and nearly 500 million unique visitors each month [9].
With 34 million pages,3 dense linking among its articles,
constant revisions on existing articles, and few article removals, Wikipedia stands as a small-scale representative of
the informative portion of the Web. Wikipedia regularly
provides database dumps of all of its available content.4
Our Wikipedia dataset contains the pages in the English
Wikipedia dumps provided for the January 1, 2013 – April
30, 2014 period.5 In the dumps, each Wikipedia page can
have multiple revisions that are available in the Web in different time periods. Fortunately, the dumps also provide the
revision time of each page. Hence, we can clearly identify
the time period that each revision was online.
In our experiments, we assumed that the revisions of
Wikipedia pages that were online from January 1, 2013 to
December 31, 2013 were already downloaded by the crawling
system, i.e., these revisions constituted set D. We placed in
the frontier (set F ) the revisions that were (i) linked by the
revisions in set D, (ii) online from January 1, 2014 to April
30, 2014, and (iii) not present in D. For each revision in D
and F , we counted the number of times it was shown in a
search result page and the number of times it was clicked,
during a short time period after the revision was created.

3.1.2

# of links
D→D
D→F
1,532,121
347,071
876,871,434 304,709,842

Existing Solutions

4.1.1

As the baseline frontier prioritization techniques, we evaluate the following five approaches:
• Random: Frontier pages are crawled in a random order.
• InDegree: For each page in the frontier, we maintain
an in-degree value (i.e., the total number of links from
the set of downloaded pages). The frontier page with
the highest in-degree value is crawled first.
• PageRank: This is similar to InDegree, but instead of
in-degree values, we use the PageRank scores [27] to
prioritize the pages.
• Q-Overlap (query overlap): The impact of a page is
estimated by the overlap between the set of page’s re-

Frontier Impact Ratio

Given a set X ⊂ D, let the set of pages in F linked from
the pages in X be denoted as L(X → F ). Then, we define
the average frontier click-impact ratio, rc (X → F ), of pages
in X as follows:
P
IC (L(X → F ))
pi ∈L(X→F ) IC (pi )
=
. (1)
rc (X → F ) =
|X|
|X|
Similarly, we define the average frontier view-impact ratio,
rv (X → F ), of pages in X as follows:
P
IV (L(X → F ))
pi ∈L(X→F ) IV (pi )
rv (X → F ) =
=
. (2)
|X|
|X|

3

http://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia.
http://en.wikipedia.org/wiki/Wikipedia:Database_download.
5
http://dumps.wikimedia.org/enwiki/.
4

6

155

We use the implementation in the scikit-learn toolkit [30].

Figure 1: (a) The set of downloaded (D) and frontier (F ) pages. (b) Partition of D into search impacting (clicked pages in
DC ) and non-impacting (not clicked pages in DNC ) page sets. (c) Partition of F into search impacting (clicked pages in FC )
and non-impacting (not clicked pages in FNC ) page sets. We also separate the links from D to F into categories based on
their source and destination (e.g., from DNC to FNC ). (d) After link graph enrichment: A set of pages in D that had no link
to FC before enrichment now have links to FC . The linking structure in D is enriched as well, improving impact propagation.
Table 2: Analysis of how pages in the downloaded set are
linked with search impacting pages in the frontier set
Dataset
Wikipedia

WebCrawl

L(DC → F )
L(DNC → F )
L(DV → F )
L(DNV → F )
L(DC → F )
L(DNC → F )
L(DV → F )
L(DNV → F )

Clicks
0.6M
0.4M
–
–
15.4M
27.3M
–
–

rc
5.76
1.07
–
–
162.17
0.03
–
–

Views
–
–
2.0M
1.0M
–
–
26.8M
50.5M

the web surfing behavior of users. The random surfer model
of PageRank assumes that a user visiting a given page can
either follow one of the outbound links of that page or arbitrarily jumps to another page. Formally, the random walk
process of PageRank can be defined as
X
xt (j) = α
Mij xt−1 (i) + (1 − α)vj ,
(3)

rv
–
–
13.68
3.15
–
–
137.99
0.06

i∈L−
j

where Mij is the transition probability from page pi to pj ,
xt (j) is the importance score of page pj at step t, α is a
damping factor that controls how often the walker jumps to
an arbitrary node, vj is the initial probabilistic importance
score (generally set to 1/n, where n is the number of nodes
in the graph), and L−
i is the set of inbound links of page
pi , respectively. When t is iterated enough, the importance
scores reach a stationary distribution that can be used as a
source of information for ranking pages.
The main drawback of the traditional PageRank process
in Eq. (3) is that it captures only the observed linking characteristics of pages and ignores other sources of information
which can also be indicators of their importance. To overcome this drawback, we introduce an impact factor for every
page and use this to augment the traditional PageRank process to make use of other sources of information. The proposed random walk model (RW) allows the random walker to
take into account multiple sources of information and perform the voting adaptively and more effectively.
To define our model formally, let G = {V, E} denote the
link graph, where vertices in V = D ∪ F with |V| = n
represent pages and edges in E represent the links among
pages. We compute the transition probability for pi → pj as
+
Mij =1/|L+
i |, where Li denotes the set of outbound links of
page pi (obviously, pj ∈ L+
i ). We define the impact factor
Fi of a page pi used in embedding the search-centric impact
of a page as

Intuitively, rc and rv provide a measure of how important
X is in suggesting good candidate pages for crawling. Table 2 shows the frontier impact ratio values for the pages
in DC , DV , DNC , and DNV . We observe that pages in DC
and DV have significantly higher impact ratio values than
the pages in DNC and DNV . In other words, despite the
fact that clicked and viewed pages in the downloaded page
set constitute a small fraction of this set, these pages are
promising sources for discovering new pages (frontier) with
high search impact.

4.2

Random Walk Model

We have observed in Table 2 that the relatively small set
of clicked or viewed pages in D are more likely to directly
link clicked or viewed pages in F . Generally speaking, the
high-impact pages in the frontier are likely to be discovered through the outbound links of the high-impact pages
in the downloaded set. Furthermore, in our analyses, we also
observed that there is a significant number of high-impact
pages in D that do not directly link pages in F , but are at
two- or three-hop distance to the high-impact pages in F .
These observations pointed us in the direction of a random
walk model where the impact of pages can trickle down to
the pages that are reachable via following outbound links.
A random walk on a given graph is a Markov process,
where each node represents a state and a walk transiting
from one state to another state based on a transition probability matrix. One of the well-known random walk algorithms is PageRank [27], and variants of it are used for
frontier prioritization [1] in state-of-the-art web crawlers.7
PageRank determines the importance of pages by modeling
7

Fi =

Ii
γ
× (d+
i ) .
maxj Ij

(4)

Here, the term Ii / maxj Ij represents the normalized searchcentric impact of pi .8 d+
i ∈ [0, 1] is the normalized out-degree
8

We note that, in the implementation, we use Ii =
IC (i, ∆t) +  or Ii = IV (i, ∆t) + , depending on the optimization objective. Ii is smoothed by adding a small  value

https://wiki.apache.org/nutch/NewScoring.

156

+
+
+ γ
of pi , i.e., d+
i = |Li |/maxk |Lk |. The (di ) multiplier is
added to enable a fine-tuning capability for favoring pages
with high out-degrees. As γ gets larger than zero, the model
increases the effect of impactful pages with high out-degree.
Our observation is that the set of pages with non-zero impact
in D are rather small, and among such pages, the ones that
have a high out-degree can be considered as hubs linking
authoritative pages [19]. We set γ = 1 for the Wikipedia
dataset and γ = 0 for the WebCrawl dataset because of this
characteristic.
The impact factor is embedded into Eq. (3) to iteratively
compute the importance score xt (j) of a page pj as
X
xt (j) = α
Fi Mij xt−1 (i) + (1 − α)vj .
(5)

Since α < 1 and Fu ≤ 1, we have (1 − αFu ) > 0 and hence
yu ≤ 0. Similarly, let v = argminj yj . We have yv ≥ 0. As
yv ≤ yu , this implies yu = yv = 0 to satisfy all inequalities.
Consequently, yi = 0 for all i, or y = 0. Thus, I − αPT is
invertible. Equivalently, (I − αMT Λ) is invertible.
Proposition 2. The iteration in Eq. (5) converges to
(1 − α)(I − αMT Λ)−1 v.
Proof. Let P = MT Λ. We can rewrite Eq. (5) in matrix
form as
xt = αMT Λxt−1 + (1 − α)v
= αPxt−1 + (1 − α)v

i∈L−
j

t

= (αP) x0 + (1 − α)(

=

XX

=

X

i

(αP )ik (αP )t−1
kj

k

(αP )t−1
kj

X

=

X

(αP )t−1
kj α

=
≤

X

X

(Fk Mki )

i

k

X

(αP )ik

i

k

(9)

(αP )t−1
kj αFk

k

(αP )t−1
kj α

k

≤ (α)t .
Because α < 1, this column sum converges to zero when
t → ∞. We then derive lim (αP)t x0 = 0. When t → ∞,
t→∞

given Proposition 1, by applying Neumann series, Eq. (8)
becomes:
xt = (αP)t x0 + (1 − α)(I − αP)−1 v.
Hence,
lim xt = (1 − α)(I − αP)−1 v.

t→∞

Convergence proved.

4.3

Random Walk with Graph Enrichment

In the previous section, we have sketched our random walk
model, which presumes that when a random walker is at
some page p, it can either visit the outbound links of p or
teleport to a random page with a certain probability and
these visit probabilities provide some voting impact to the
rank of the candidate pages. In this model, the teleportation
probability to other pages is fixed.
We hypothesize that the user browsing experiences generally start with a query issued to a search engine and by
clicking on a search result, possibly followed by a number
of page visits performed by tracking the outbound links of
the encountered pages. Then, if the search task did not end,
it is followed by another (possibly related/refined) query to
the search engine. In this scenario, teleportation is also generally performed via visits to a search engine and a user is
more likely to “teleport” to a related or similar page instead
of a random page in a search session. Following this hypothesis, in our random walk model, when a page p is visited,

(6)

(Fi Mij yj ).

j

Let u = argmaxj yj . Eq. (6) infers
X
yu ≤ αFu
(Fi Mij yu )
j

(αP )tij

i

j

yu ≤ αFu yu

)v.

t→∞

X

Proof. Let P = MT Λ, we need to prove that I − αP is
invertible. Equivalently, we prove its transpose I−αPT is invertible, which can be proved by showing that (I−αPT )y =
0 only has the trivial solution y = 0.
(I − αPT )y = 0

X

(αP)

We will show that lim xt = (1 −α)(I −αP)−1 v. We have

Proposition 1. (I−αMT Λ) is invertible for all M, Λ, α.

=α

(8)
i−1

i=1

Intuitively, our random walk process can be explained as
follows. When following the outbound links of a node, the
random walker is likely to contribute the voting impact of
the node to its neighbors with a high rate when this node has
a good impact (i.e., more clicks or views in our case). Otherwise, its neighbors will receive its voting impact with a low
rate. Here, we should note that, the importance score computations of the pages in the frontier according to Eq. (5)
can be done in a separate iteration as the outbound links
of those pages are not known. Hence, their contributions to
other pages’ importance scores are practically zero. In other
words, it is possible to compute the accumulated score contribution of the pages in D to pages in F first and reflect
that onto pages in F in one iteration later. For further computational efficiency, it is possible to compute the scores of
pages in D offline and update them incrementally adopting
techniques similar to those defined for incremental PageRank computation [11].
It is worth mentioning that RW converges to a stationary
distribution. Let Λ be the diagonal matrix with diagonal
elements (F1 , F2 , .., Fn ), I be the n × n identity matrix, v
be the transpose of 1 × n uniform stochastic vector, and M
denote the transition matrix for G.

y = αPT y
X
yi = α
Pji yj

t
X

(7)

yu (1 − αFu ) ≤ 0.
(empirically set to 0.001) to its absolute count to avoid random ranking in the case all inbound links to a page have
zero impact value.

157

v1	  

v1	  

v6	  

v2	  

richment, the impact of the set of pages in the frontier that
are linked by DC and DV has increased. Fig. 1(d) demonstrates how the link graph changes after enrichment.
To formally define our random walk model with graph
0
enrichment, let M 0 be an n × n matrix, such that Mij
indicates the virtual transition probability among pages pi
0
and pj based on virtual edges in E 0 . Mij
is computed as
wij
−0
0
Mij = P wik . We also define Lj as the list of inbound
k
links based on virtual edges and Fj0 as the influence impact
of a page pj based on virtual edges. We propose to compute
the impact-based importance score xt (j) of a page pj using
the enriched graph EG as
X
xt (j) = αω
Fi Mij xt−1 (i)

v6	  

v2	  

Enrichment
v5	  

v3	  

3
v5	  

v3	  

1

v4	  

v4	  
v7	  

v7	  

v8	  

v8	  

G

EG

Figure 2: Illustration of graph enrichment technique.

i∈L−
j

Table 3: Analysis on the search impact of pages in F linked
via enriched edges from DC and DV
Dataset
Wikipedia
WebCrawl

EG(DC
EG(DV
EG(DC
EG(DV

→ F)
→ F)
→ F)
→ F)

Clicks
0.6M
–
21.9M
–

rc
6.17
–
207.63
–

Views
–
2.2M
–
36.9M

+ α(1 − ω)

X

0
Fi0 Mij
xt−1 (i)

(10)
+(1 − α)vj .

i∈L−0
j

rv
–
14.67
–
150.59

Here, 0 ≤ ω ≤ 1 is the hyper-parameter to control the proportion of the impact a page distributes to its neighbors
via its original links. When ω = 1, no virtual link is taken
into account in the importance calculation, and when ω = 0
only the virtual links are used. The proposed random walk
process can be interpreted as follows. At every node, the
random walker can either follow the original outbound links
with a probability αω or teleport to a similar node with a
probability α(1 − ω). Otherwise, it teleports to an arbitrary
node with probability 1 − α.
Similar to the RW model, the RW-EG model converges to a
stationary distribution. Let Λ and Λ0 be diagonal matrices
with diagonals (F1 , F2 , .., Fn ) and (F10 , F20 , .., Fn0 ), respectively.

even if the outbound links of p are not followed, pages that
are similar to p should have a higher chance of being visited
next.
The implication of the above discussion for prioritizing
frontier pages is that pages in F that are similar to high
impact pages in D are more likely to be teleported to. There
are a number of ways for performing this special form of
teleportation. Herein, we opted to enhance the link graph
via virtual edges that link similar pages. Since one of the
best information we have about the pages in the frontier is
their in-link structure, we define a similarity metric based on
the in-link structure of pages. We refer to the set of virtual
0
edges as E . Extending the original graph G via virtual edges
0
0
in E forms the enriched graph EG = {V, E ∪ E }. Applying
our random walk model on EG yields a random walk model
with a graph enrichment solution (RW-EG) for the frontier
prioritization problem.
In this enhanced model, we consider pages linked by the
same pages as similar. We create a virtual edge between a
page pi and another page pj if and only if they have at least
one page px that has links to both of them. The weight
wij of the virtual edge between pi and pj is computed as
−
wij = |L−
i ∩ Lj |.
Fig. 2 provides an example for our link graph enrichment
technique. The regular link graph G is shown on the left
while the enriched graph EG is shown on the right. Shaded
vertices in the figure indicate already crawled pages, and
white vertices indicate URLs in the frontier. The darker a
crawled vertex is, the higher the search-impact of the corresponding page. As seen in the figure, two virtual edges
are added to G after the enrichment process. The edge between v5 and v7 is added as both of them have a common
inbound link from v3 . The weight of the (v5 , v7 ) edge is 1
since the number of common link providers of these two vertices is one. The edge (v5 , v6 ) is added as both of them have
common inbound links from v2 , v3 , and v4 . The weight of
(v5 , v6 ) edge is 3 since the number of common link providers
is three.
Table 3 shows the frontier impact ratio values for the pages
in DC and DV after enrichment. We observe that, via en-

Proposition 3 (Invertible). (I − α(wMT Λ + (1 −
ω)M0T Λ0 )) is invertible for all M, M0 , Λ, Λ0 , α, ω.
Proposition 4 (Convergence). The iteration in
Eq. (10) converges to (1 − α)(I − α(ωMT Λ + (1 −
ω)M0T Λ0 ))−1 v.
Proof. Let P = wMT Λ + (1 − ω)M0T Λ0 . The proof is
similar to those of Propositions 1 and 2.

5.
5.1

EXPERIMENTAL RESULTS
Parameter Tuning

We randomly split the frontier of each dataset into training and test sets. The training set is used to train and tune
the parameters of the algorithms, while the test set is used
for evaluation. In both datasets, we use roughly 65% of the
pages for training.
RW-EG is parametrized by ω, which linearly combines the
votes based on original links and the votes based on virtual links, and α, which determines the probability that the
surfer chooses to follow one of the outbound of the current
page. We set, α = 0.85 as suggested by [27] and tune ω
on the training data before applying the model on the test
data. Our tuning strategy is straightforward: We vary ω to
identify the ω value maximizing the objective function. For
higher confidence and to avoid data randomness, we apply
the 0.632+ bootstrapping technique [12] to generate multiple samples from the tuning data and measure the average
performance on these samples while tuning.

158

90%

Impact relative to the ideal

Impact relative to the ideal

70%

60%

50%

40%
number of clicks @ 5%
number of clicks @ 10%
number of views @ 5%
number of views @ 10%

30%

20%

0

0.1

0.2

0.3

0.4

0.5

0.7

0.6

ω

0.8

0.9

80%
70%
60%
50%

30%

1.0

number of clicks @ 1%
number of clicks @ 2%
number of views @ 1%
number of views @ 2%

40%

0

0.1

0.2

0.3

0.4

0.5

0.7

0.6

ω

0.8

0.9

1.0

(b) WebCrawl.

(a) Wikipedia.

Figure 3: Performance with varying ω parameter.
70%

Random
Q-Overlap
InDegree
Q-Hybrid
PageRank
RW
RW-EG

60%
50%
40%

Impact relative to the ideal

Impact relative to the ideal

70%

30%
20%
10%
0

1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

Random
Q-Overlap
InDegree
Q-Hybrid
PageRank
RW
RW-EG

60%
50%
40%
30%
20%
10%
0

1%

2%

3%

4%

5%

6%

7%

8%

Percentage of fetched pages

Percentage of fetched pages

(a) Click impact.

(b) View impact.

9%

10%

Figure 4: Performance comparison on the WebCrawl dataset.

5.2.1

Figs. 3a and 3b show the performance of RW-EG on the
tuning data as we increase the value of ω from 0 to 1 in
steps of 0.1. For each ω value on the x axis, we present the
percentage of search impact of RW-EG with respect to that of
an ideal ranking, which orders the pages in the frontier according to their actual search impacts. We select the highest
ranked 5% and 10% frontier pages according to RW-EG for the
Wikipedia dataset. Similarly, we select the highest ranked
1% and 2% frontier pages according to RW-EG for the WebCrawl dataset. The summed impact of pages selected by
RW-EG are divided against that of the ideal ranking to obtain
the provided figures. We observe that the best combination
for Wikipedia is at ω = 0.1 and ω = 0.2 for the click impact
and view impact metrics, respectively. For WebCrawl, the
best combination is found at ω = 0.9 for the click impact
metric and ω = 0.8 for the view impact metric.

5.2

Performance on the Wikipedia Dataset

Figs. 4a and 4b present the performance of the investigated approaches on the Wikipedia dataset. As seen in the
figures, when the fraction of the downloaded pages exceeds
6% of the frontier, RW-EG provides the best results. However, when the fraction of downloaded pages is below 5%,
Q-Overlap and Q-Hybrid outperforms RW and RW-EG. This
is due to the fact that the query overlap score used in QOverlap and Q-Hybrid is very effective in informative pages
such as those in Wikipedia.
RW and RW-EG provide considerably better performance
compared to PageRank, showing the positive influence of incorporating click and view impact in predicting high-impact
frontier pages. The difference in the performance of RW and
RW-EG indicates the effectiveness of the proposed link graph
enrichment technique. We note that, even though Random
provides the worst ordering among all approaches, it still
obtains approximately 20% of the impact attained by the
ideal ranking, both in click and view impact metrics. This
shows that the Wikipedia dataset, by its nature, contains
high-impact pages.

Performance Analysis

Figs. 4a, 4b, 5a, and 5b present the performance of all approaches on the two datasets and in terms of the two impact
metrics. The performance values are provided as a percentage of the click impact and view impact measures attained
by the ideal ranking (observed within the first week following the crawling). We note that similar results are observed
when the observation period is limited up to a day. For
Wikipedia, similar to [29], we aim to maximize the search
impact while selecting 10% of the frontier. This is a reasonable assumption considering the size of the Wikipedia
frontier. For WebCrawl, we aim to maximize the impact
while selecting up to 2% (approximately 160K pages) of the
frontier. In practice, it may be more effective to crawl a few
hundred thousand pages in a crawling iteration and rerun
the ranking algorithm with additional discovered pages.

5.2.2

Performance on the WebCrawl Dataset

Figs. 5a and 5b show the performance of all approaches
on the WebCrawl dataset. As mentioned before, we evaluate
the search impact up to top 2% of the frontier. Unlike the
Wikipedia dataset, WebCrawl has a highly skewed distribution in terms of impact, with a tiny fraction (0.4%–0.8%) of
pages receiving the majority of clicks and views.
In the figures, we first observe that Random provides nearly
zero impact frontier selection. This is another indicator for
the difference between Wikipedia and WebCrawl. Second, we
observe that Q-Overlap and Q-Hybrid both perform better

159

70%

Random
Q-Overlap
InDegree
Q-Hybrid
PageRank
RW
RW-EG

60%
50%
40%

Impact relative to the ideal

Impact relative to the ideal

70%

30%
20%
10%
0

0.2%

0.4%

0.6%

0.8%

1.0%

1.2%

1.4%

1.6%

1.8%

60%
50%
40%
30%

Random
Q-Overlap
InDegree
Q-Hybrid
PageRank
RW
RW-EG

20%
10%
0

2.0%

0.2%

0.4%

0.6%

0.8%

1.0%

1.2%

1.4%

1.6%

Percentage of fetched pages

Percentage of fetched pages

(a) Click impact.

(b) View impact.

1.8%

2.0%

1.8%

2.0%

Figure 5: Performance comparison on the WebCrawl dataset.
0.12

Normalized CTR per page

Normalized CTR per page

0.40
0.35
0.30
0.25
0.20
Random
Q-Overlap
InDegree
Q-Hybrid
PageRank
RW
RW-EG

0.15
0.10
0.05
0.00

0

1%

2%

3%

4%

5%

6%

7%

8%

9%

0.09

0.06

0.03

0.00

10%

Random
Q-Overlap
InDegree
Q-Hybrid
PageRank
RW
RW-EG

0

0.2%

0.4%

0.6%

0.8%

1.0%

1.2%

1.4%

1.6%

Percentage of fetched pages

Percentage of fetched pages

(a) Wikipedia.

(b) WebCrawl.

Figure 6: Average CTR values for different approaches.
Table 4: Features used in the machine learning model

than PageRank and InDegree in identifying the top fraction
(0.2%) of pages with high click impact. However, as they
download a larger fraction of frontier pages, they perform
rather poorly. As suggested in [29], a very rapid “drying up”
trend is observed for Q-Overlap. WebCrawl possesses a very
low fraction of high-impact pages, and selecting the set of
pages that match previously observed queries in and itself
is not enough in tackling the complex problem of identifying these small number of high-impact frontier pages and
may even lead to the selection of low-impact pages. Finally, Figs. 5a and 5b show that RW and RW-EG are superior
when compared to the other algorithms. This proves the
effectiveness of the proposed approaches that incorporate
the search impact into a random walk model. Once again,
we observe that RW-EG improves over RW by selecting higher
impact pages in the early stages.

5.3

Feature
InDegree
PageRank
Q-Overlap
Q-Hybrid
RW
RW-EG
RW (virtual)

6.

Description
Number of inbound links of the page
PageRank score of the page
Q-Overlap score of the page
Q-Hybrid score of the page
RW scores of the page (click/view-impact)
RW-EG scores of the page (click/view-impact)
RW scores on virtual edges (click/view-impact)

MACHINE-LEARNED APPROACH

The analyses in the previous section show that RW and
RW-EG both perform better than the baseline frontier prioritization approaches. However, the information provided
by the baseline approaches may still be useful. Hence, in
this section, we describe a machine learning approach (ML)
that exploits the ranking scores of all available approaches.
Our aim is to show that by combining approaches that make
use of different information wells (e.g., links, search impact,
query neediness), it is possible to perform better than the
individual approaches.
In ML, for every page in the frontier, we compute the features given in Table 4. Then, a model is learned using the
set of training pages, where the (ground-truth) search impact scores obtained by using the query logs are used as
prediction targets. In practice, there exists a wide range
of machine learning algorithms that can be used to train
our model. In our experiments, we use the decision tree regression technique9 and the online gradient-descent-based
regression technique10 with the Wikipedia and WebCrawl
datasets, respectively.

Analysis of Other Quality Metrics

We investigate the quality of highly ranked frontier pages
in terms of two other quality metrics: click-through rate
(CTR) and PageRank score. Fig. 6 presents the normalized
CTR values per page for the investigated algorithms. As
seen in the figure, for the Wikipedia dataset, Q-Overlap
and Q-Hybrid lead to higher average CTR while, for the
WebCrawl dataset, RW and RW-EG lead to higher average CTR.
Fig. 7 presents the average PageRank scores for each approach. As expected, PageRank performs the best in this
metric. As RW and RW-EG employ random walk models,
they also tend to select pages with relatively high PageRank scores. Similarly, as Q-Hybrid incorporates PageRank,
it performs relatively well in this metric. We observe that
pages selected by Q-Overlap tend to have low PageRank
scores.

9
10

160

We use the implementation in the scikit-learn toolkit [30].
We use the implementation in the Vowpal Wabbit toolkit.

Normalized PageRank per page

Normalized PageRank per page

1.0

Random
Q-Overlap
InDegree
Q-Hybrid
PageRank
RW
RW-EG

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0

0

1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

1.0

Random
Q-Overlap
InDegree
Q-Hybrid
PageRank
RW
RW-EG

0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0

0

0.2%

0.4%

0.6%

0.8%

1.0%

1.2%

1.4%

1.6%

1.8%

2.0%

1.8%

2.0%

Percentage of fetched pages

Percentage of fetched pages

(a) Wikipedia.

(b) WebCrawl.

Figure 7: Average PageRank values for different approaches.
80%

RW
RW-EG
ML

90%

Impact relative to the ideal

Impact relative to the ideal

100%

80%
70%
60%
50%
40%
30%
20%
10%
0

1%

2%

3%

4%

5%

6%

7%

8%

9%

10%

RW
RW-EG
ML

70%
60%
50%
40%
30%
20%
10%
0

Percentage of fetched pages

0.2%

0.4%

0.6%

0.8%

1.0%

1.2%

1.4%

1.6%

Percentage of fetched pages

(a) Wikipedia.

(b) WebCrawl.

Figure 8: Comparison of the ML, RW, and RW-EG approaches in terms of the click-impact metric.
Figs. 8a and 8b compare the performance of ML with those
of RW and RW-EG in terms of the click impact metric, using
the Wikipedia and WebCrawl datasets, respectively. We
do not report the results for the view impact metric as the
findings are similar. In Fig. 8a, we observe that ML outperforms RW and RW-EG for the Wikipedia dataset. This finding
encourages the idea that information coming from different
prioritization approaches, when combined in the right way,
can provide additional benefits. In Fig. 8b, however, we observe that ML does not show the same relative performance
for WebCrawl. We hypothesize that, as WebCrawl is highly
skewed in terms of impact distribution, the task of combining the features in the right way becomes much more difficult. There are techniques to cope with this issue, such as
supervising a random walk model [3] or stratified sampling
via weighted random walks [20].

7.

study of the recrawling problem is available in [31]. The
work in [7] investigated different page refresh policies. The
authors of [7] defined two metrics to estimate the freshness of
a web repository, and the refresh policies proposed therein
were shown to improve the freshness over simpler refresh
policies. The work in [28] proposed guiding the recrawling
process based on the potential impact on the search experience of users. The authors empirically demonstrated that
the same user experience can be attained by allocating much
fewer resources to the recrawling process, enabling the use of
freed resources in the discovery process. In [26], the authors
argued that ephemeral and persistent web content should
be distinguished when deciding which pages should be refreshed. Based on this observation, they designed a recrawling policy that takes the longevity of web pages into account.
This approach was shown to lead to better freshness in the
web repository also reducing the crawling cost.
Frontier prioritization. The discoverability of the content in the Web was investigated in [10]. The early work
in [8] defined different page importance metrics and proposed crawling the Web according to page importance.
The authors showed that importance-based prioritization of
pages can significantly improve the quality of a web repository. In [23], the relatively simple breadth-first crawling
strategy was shown to discover high-quality pages early.
In [13], the authors tried to prioritize the pages in the
crawler’s frontier by exploiting the hierarchical structure
of the Web and showed that this is more efficient than
the PageRank-based prioritization approach proposed in [8].
The work in [16] focused on the discovery of good pages,
specifically focusing on filtering out spam content during
the crawling process. The authors of [29] proposed ordering
the pages in the frontier according to their estimated im-

RELATED WORK

Some recent work investigated passive content discovery
techniques [4, 18], such as those that rely on bookmarking or toolbar data, as complementary techniques to web
crawling. We limit our attention here only to conventional
crawling strategies where the Web is actively crawled by following hyperlinks [25]. We also do not cover the scalability
and efficiency issues encountered in crawling [21]. Interested
reader may refer to [5] for a more detailed coverage of practical issues involved in web crawling. Below we cover the
two main lines of research on crawling: recrawling [7, 26,
28] and frontier prioritization [8, 13, 15, 16, 22, 23, 25, 29].
Recrawling. Evolution of the Web has been investigated
in several research work [2, 24]. These work pointed out
the rapid change in both content and link structure of the
Web, demonstrating the importance of recrawling. An early

161

pact on search results. They showed that their technique
enables crawling of tail web content that is of interest to
some users, not only popular content. In two concurrent
work [14] and [6], the authors investigated the impact of
well-known crawling strategies on web search effectiveness.

8.

[12]

[13]

CONCLUSION

We formalized the frontier prioritization problem as a
search-centric optimization problem, where the objective is
to maximize the impact of the crawled collection on the
result quality of the search engine. To tackle the problem, we presented a novel random walk model that incorporates the inferred search impact of pages into the standard
connectivity-based page importance computation. We also
proposed a link graph enrichment technique that approximates user’s searching and browsing behavior and improves
the voting process in the proposed random walk model. Finally, we combined different solutions for the frontier prioritization problem under a common machine learning model.
To evaluate the proposed approaches, we conducted experiments over two large-scale web collections. The results of
our experiments indicated that the proposed approaches perform considerably better than the state-of-the-art frontier
prioritization approaches in improving search quality.

[14]

[15]

[16]

[17]
[18]

[19]
[20]

Acknowledgement
This work was supported by the ERC Advanced Grant
ALEXANDRIA (339233) and the LEADS project (ICT318809), funded by the European Community.

9.

[21]

REFERENCES

[1] S. Abiteboul, M. Preda, and G. Cobena. Adaptive on-line
page importance computation. In Proc. 12th Int’l Conf.
World Wide Web, pages 280–290, 2003.
[2] E. Adar, J. Teevan, S. T. Dumais, and J. L. Elsas. The
Web changes everything: Understanding the dynamics of
web content. In Proc. 2nd ACM Int’l Conf. Web Search
and Data Mining, pages 282–291, 2009.
[3] L. Backstrom and J. Leskovec. Supervised random walks:
Predicting and recommending links in social networks. In
Proc. 4th ACM Int’l Conf. Web Search and Data Mining,
pages 635–644, 2011.
[4] X. Bai, B. B. Cambazoglu, and F. P. Junqueira.
Discovering URLs through user feedback. In Proc. 20th
ACM Int’l Conf. Information and Knowledge
Management, pages 77–86, 2011.
[5] B. B. Cambazoglu and R. Baeza-Yates. Scalability
challenges in web search engines. In M. Melucci and
R. Baeza-Yates, editors, Advanced Topics in Information
Retrieval, volume 33 of The Information Retrieval Series,
pages 27–50. Springer Berlin Heidelberg, 2011.
[6] B. B. Cambazoglu, V. Plachouras, and R. Baeza-Yates.
Quantifying performance and quality gains in distributed
web search engines. In Proc. 32nd Int’l ACM SIGIR Conf.
Research and Development in Information Retrieval, pages
411–418, 2009.
[7] J. Cho and H. Garcia-Molina. Effective page refresh policies
for web crawlers. ACM Transactions on Database Systems,
28(4):390–426, 2003.
[8] J. Cho, H. Garcia-Molina, and L. Page. Efficient crawling
through URL ordering. Computer Networks and ISDN
Systems, 30(1-7):161–172, 1998.
[9] N. Cohen. Wikipedia vs. the small screen, 2014.
[10] A. Dasgupta, A. Ghosh, R. Kumar, C. Olston, S. Pandey,
and A. Tomkins. The discoverability of the Web. In Proc.
16th Int’l Conf. World Wide Web, pages 421–430, 2007.
[11] P. Desikan, N. Pathak, J. Srivastava, and V. Kumar.
Incremental page rank computation on evolving graphs. In

[22]

[23]

[24]

[25]
[26]

[27]

[28]

[29]

[30]

[31]

162

Special Interest Tracks and Posters of the 14th Int’l Conf.
World Wide Web, pages 1094–1095, 2005.
B. Efron and R. Tibshirani. Improvements on
cross-validation: The .632+ bootstrap method. Journal of
the American Statistical Association, 92(438), 1997.
N. Eiron, K. S. McCurley, and J. A. Tomlin. Ranking the
web frontier. In Proc. 13th Int’l Conf. World Wide Web,
pages 309–318, 2004.
D. Fetterly, N. Craswell, and V. Vinay. The impact of crawl
policy on web search effectiveness. In Proc. 32nd Int’l ACM
SIGIR Conf. Research and Development in Information
Retrieval, pages 580–587, 2009.
Z. Guan, C. Wang, C. Chen, J. Bu, and J. Wang. Guide
focused crawler efficiently and effectively using on-line
topical importance estimation. In Proc. 31st Annual Int’l
ACM SIGIR Conf. Research and Development in
Information Retrieval, pages 757–758, 2008.
Z. Gyöngyi, H. Garcia-Molina, and J. Pedersen. Combating
web spam with Trustrank. In Proc. 38th Int’l Conf. Very
Large Data Bases, pages 576–587, 2004.
A. Heydon and M. Najork. Mercator: A scalable, extensible
web crawler. World Wide Web, 2(4):219–229, 1999.
P. Heymann, G. Koutrika, and H. Garcia-Molina. Can
social bookmarking improve web search? In Proc. 1st Int’l
Conf. Web Search and Data Mining, pages 195–206, 2008.
J. M. Kleinberg. Hubs, authorities, and communities. ACM
Computing Surveys, 31(4es), 1999.
M. Kurant, M. Gjoka, C. T. Butts, and A. Markopoulou.
Walking on a graph with a magnifying glass: Stratified
sampling via weighted random walks. In Proc. ACM
SIGMETRICS Joint Int’l Conf. Measurement and
Modeling of Computer Systems, pages 281–292, 2011.
H.-T. Lee, D. Leonard, X. Wang, and D. Loguinov. IRLbot:
Scaling to 6 billion pages and beyond. In Proc. 17th Int’l
Conf. World Wide Web, pages 427–436, 2008.
F. Menczer, G. Pant, P. Srinivasan, and M. E. Ruiz.
Evaluating topic-driven web crawlers. In Proc. 24th Annual
Int’l ACM SIGIR Conf. Research and Development in
Information Retrieval, pages 241–249, 2001.
M. Najork and J. L. Wiener. Breadth-first crawling yields
high-quality pages. In Proc. 10th Int’l Conf. World Wide
Web, pages 114–118, 2001.
A. Ntoulas, J. Cho, and C. Olston. What’s new on the
Web?: The evolution of the Web from a search engine
perspective. In Proc. 13th Int’l Conf. World Wide Web,
pages 1–12, 2004.
C. Olston and M. Najork. Web crawling. Foundations and
Trends in Information Retrieval, 4(3):175–246, 2010.
C. Olston and S. Pandey. Recrawl scheduling based on
information longevity. In Proc. 17th Int’l Conf. World
Wide Web, pages 437–446, 2008.
L. Page, S. Brin, R. Motwani, and T. Winograd. The
PageRank citation ranking: Bringing order to the web.
Technical Report 1999-66, Stanford InfoLab, 1999.
S. Pandey and C. Olston. User-centric web crawling. In
Proc. 14th Int’l Conf. World Wide Web, pages 401–411,
2005.
S. Pandey and C. Olston. Crawl ordering by search impact.
In Proc. 1st Int’l Conf. Web Search and Data Mining,
pages 3–14, 2008.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay.
Scikit-learn: Machine learning in Python. Journal of
Machine Learning Research, 12:2825–2830, 2011.
J. L. Wolf, M. S. Squillante, P. S. Yu, J. Sethuraman, and
L. Ozsen. Optimal crawling strategies for web search
engines. In Proc. 11th Int’l Conf. World Wide Web, pages
136–147, 2002.

