Retrieval of Relevant Opinion Sentences for New Products
Hyun Duk Kim

ChengXiang Zhai

Twitter Inc.
1355 Market St Suite 900
San Francisco, CA 94103,
USA

Department of Computer
Science
University of Illinois at
Urbana-Champaign
Urbana, IL 61801, USA

Dae Hoon Park

Department of Computer
Science
University of Illinois at
Urbana-Champaign
Urbana, IL 61801, USA

hkim@twitter.com

dpark34@illinois.edu

czhai@cs.illinois.edu
Lifan Guo

TCL Research America
2870 Zanker Road
San Jose, CA 95134, USA

GuoLifan@tcl.com
ABSTRACT

1.

With the rapid development of Internet and E-commerce,
abundant product reviews have been written by consumers
who bought the products. These reviews are very useful
for consumers to optimize their purchasing decisions. However, since the reviews are all written by consumers who
have bought and used a product, there are generally very
few or even no reviews available for a new product or an unpopular product. We study the novel problem of retrieving
relevant opinion sentences from the reviews of other products using specifications of a new or unpopular product as
query. Our key idea is to leverage product specifications
to assess product similarity between the query product and
other products and extract relevant opinion sentences from
the similar products where a consumer may find useful discussions. Then, we provide ranked opinion sentences for
the query product that has no user-generated reviews. We
first propose a popular summarization method and its modified version to solve the problem. Then, we propose our
novel probabilistic methods. Experiment results show that
the proposed methods can effectively retrieve useful opinion
sentences for products that have no reviews.

The role of product reviews has been more and more important. Reevoo, a social commerce solutions provider, surveyed 1,000 consumers on shopping habits and found that
88 percent of them sometimes or always consult customer
reviews before purchase.1 According to the survey, 60 percent of them said that they were more likely to purchase
from a site that has customer reviews on. Also, they considered customer reviews more influential (48%) than advertising (24%) or recommendations from sales assistants (22%).
With the development of Internet and E-commerce, people’s
shopping habits have changed, and we need to take a closer
look at it in order to provide the best shopping environment
to consumers.
Even though product reviews are considered important
to consumers, the majority of the products has only a few
or no reviews. Products that are not released yet or newly
released generally do not have enough reviews. Also, unpopular products in the market lack reviews because they
are not sold and exposed to consumers enough. How can
we help consumers who are interested in buying products
with no reviews? In this paper, we propose methods to automatically retrieve review text for such products based on
reviews of other products. Our key insight is that opinions
on similar products may be applicable to the product that
lacks reviews. For example, if products X and Y have the
same CPU clock rate, then people’s opinion on CPU clock
rate for product X may be applicable to that for product Y
as well. The similarity between products can be computed
based on product specifications which are often available,
where an example of product specifications is shown in Figure 1. %Here is an example of review text we manually
retrieved for a certain product’s specification “Resolution:
12.1 megapixels” from real reviews of products that have
the same resolution.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval—Retrieval models; H.4.0 [Information
Systems Applications]: General

General Terms
Algorithms, Design

Keywords

INTRODUCTION

12.1 MP captures very minute details even at
highest zoom. This 12.1 megapixel megazoom
offers an awesome value as the pictures it produces are on par with some cheap DSLRs. I will
not longer bring my big DSR camera on my vacations. 12MP is too much, I use it with 8MP
- that’s more than plenty. What I like most
about the W200 is my ability to get crystal clear
4000x3000 12.1 meg photos without having to

opinion mining, probabilistic information retrieval
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
SIGIR’15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767748.

1

https://www.reevoo.com/news/half-of-consumers-findsocial-content-useful-when-shopping-online/

393

Feature

2.

Value

RELATED WORKS

Reviews are one of the most popular sources in opinion
analysis. Opinion retrieval and summarization techniques
attracted a lot of attentions because of its usefulness in Web
2.0 environment. There are several surveys which summarize the existing opinion mining work [9, 21, 14]. Compared
to text data in other general retrieval problems, opinionated
articles such as product reviews have some different characteristics. In opinion analysis, analyzing polarities of input
opinions are crucial. Also, majority of the opinion retrieval
works are based on product feature (aspect) analysis. They
first find sub-topics (features) of a target and show positive
and negative opinions for each aspect. By further segmenting the input texts into the smaller units, they showed more
details in a structured way [7, 15, 16, 18, 25, 28, 10]. Meanwhile, product reviews have been also employed to predict
ratings [20, 5] or sales [3] of a product. However, no existing
work addressed the problem of retrieving opinion sentences
for new products yet.
In this paper, we also utilize unique characteristics of
product data: specifications (structured data) as well as reviews (unstructured data). Although product specifications
have been provided in many e-commerce web sites, there
are only a limited number of studies that utilized specifications for product review analysis. Zhou and Chaovalit
[32] performed sentiment classification on reviews using domain ontology database, which may be regarded as product
specifications. Bhattattacharya et al. [2] employed IMDb’s
structured data to categorize documents, and Yu et al. [30]
built an aspect hierarchy using product specifications and
reviews. Wang et al. [29] and Peñalver-Martı́nez et al. [23]
also employed product specifications to summarize product
features. Product reviews and specifications were jointly
modeled using topic models by Duan et al. [4] to improve
product search and by Park et al. [22] to generate augmented specifications with useful information. Park et al.
[22] retrieved review sentences for each (feature, value) pair,
but they did not study their model’s performance on products with no reviews. In addition, their model does not
consider similarity among products or specifications, which
is an important factor for the problem. Likewise, there are a
few studies that employed product specifications, but their
goals are different from ours.
Our work is related to text summarization, which considers centrality of text. Automatic text summarization techniques have been studied for a long time due to the need
of handling large amount of electronic text data [19, 11,
6]. Automatic summarization techniques can be categorized
into two types, extractive summary and abstractive summary. Extractive summarization makes a summary by selecting representative text segments, usually sentences, from
the original documents. Abstractive summarization does
not directly reuse the existing sentences but generates sentences based on text analysis. Our work is similar to extractive summarization in that we select sentences from original
documents but different in that we retrieve sentences for an
entity that does not have any text. Among the previous
work, MEAD [26] is one of the most popular public extractive summarization toolkits, which supports multi-document
summarization in general domain. The goal of MEAD is different from ours in that we want a summary for a specific
product, and also MEAD does not utilize external structured
data (specifications).
Cold start problem in recommendation systems [27], where
no one has rated new items yet, is also related to our problem. However, unlike rating connections between items and
users, each user review carries its unique and complex meaning, which makes the problem more challenging. Moreover,

Figure 1: A part of product specifications in
CNET.com.
spend a couple of thousand dollars on an digital
SLR camera body and then even more cash on
the accessories (e.g. lens).
Even though these sentences are not necessarily coherent
opinions, they are clearly very useful for users to understand
the product feature and get access to relevant discussion of
opinions. Since a user would hardly have a clue about opinions on a new product, such a retrieved review text can be
expected to be useful. As a minimum, it can be very useful
to help users prioritizing what to read in the existing reviews
of other products. Not only from a consumer’s perspective,
but also from a manufacturer’s perspective, such techniques
would be beneficial to collect opinions on its new or unpopular products. From the retrieved opinions, the manufacturers would be able to predict what consumers would think
even before their product release and react to the predicted
feedback in advance.
This paper makes the following contributions:
1. We introduce and study a novel problem of relevant
opinion retrieval for products that do not have reviews
in order to provide useful information to consumers
and manufacturers. To the best of our knowledge, no
previous work has addressed this problem.
2. To solve the problem, we propose a new probabilistic retrieval method, Translation model, Specifications
Generation model, and Review and Specifications Generation model, as well as standard summarization model
MEAD, its modified version MEAD-SIM, and standard ad-hoc retrieval method. Our suggested probabilistic methods are also able to retrieve per-feature
opinions for a query product.
3. We create a new data set for evaluating the new problem and conduct experiments to show that our translation model indeed retrieves useful opinions and outperforms other baseline models. We also provide an
interesting way to evaluate retrieved sentences for new
products.
In order to evaluate the automatically retrieved opinions
for a new or unpopular product, we pretend that the query
product does not have reviews and predict its review text
based on similar products. Then, we compare the predicted
review text with the query product’s actual reviews to evaluate the performance of suggested methods. Experiment
results show that our translation model effectively retrieves
opinions for a product without reviews and it significantly
outperforms baseline methods.

394

function between products as
PF
k=1 wk SIMf (si,k , sj,k )
SIMp (Pi , Pj ) =
PF
k=1 wk

our goal is to provide useful relevant opinions about a product, not recommending a product. XML retrieval [12] that
utilizes structured information of documents is also related
to our work, in that reviews and specifications can be represented as a special XML. However, unlike general XML
retrieval, in this paper, we propose more specialized methods for product reviews using product category and specifications. In addition, because we require the retrieved sentences to be central in reviews, we consider both centrality
and relevance while general retrieval methods focus on relevance only. As far as we know, none of the existing work
tried to solve the same problem as ours.

3.

where wk is a weight for the feature fk , and the weights
{w1 , ..., wF } are assumed identical (wk = 1) in this study,
so the similarity function becomes
PF
k=1 SIMf (si,k , sj,k )
SIMp (Pi , Pj ) =
(2)
F
where SIMf (si,k , sj,k ) is a cosine similarity for feature fk
between Pi and Pj and is defined as

PROBLEM DEFINITION

vi,k · vj,k
qP
2
2
v∈vi,k v
v∈vj,k v

The product data consists of N products {P1 , ..., PN }.
Each product Pi consists of its set of reviews Ri = {r1 , ..., rm }
and its set of specifications Si = {si,1 , ..., si,F }, where a specification si,k is a feature-value pair, (fk , vi,k ), and F is the
number of features. Given a query product Pz , for which Rz
is not available, our goal is to retrieve a sequence of relevant
opinion sentences T in K words for Pz .
Note that our problem setup is a mixture of retrieval and
summarization. On the one hand, it can be regarded as a
ranking problem, similar to retrieval; on the other hand, it
can also be regarded as a summarization problem since we
restrict the total number of words in the retrieved opinions
.
This is a new problem that has not been addressed in
any previous work. The problem is challenging for several
reasons. Retrieved sentences for Pz should conform to its
specifications Sz while we do not know which sentences are
about which specific feature-value pair. In addition, the
retrieved sentences should be central across relevant reviews
so that they reflect central opinions. Despite the challenges,
we try to show that achieving the goal is feasible. In the
next a few sections, we propose multiple methods to solve
the problem.

4.

SIMf (si,k , sj,k ) = qP

(3)

where vi,k and vj,k are phrase vectors in values vi,k and vj,k ,
respectively. Both SIMp (Pi , Pj ) and SIMf (si,k , sj,k ) range
from 0 to 1.
In this paper, we define the phrases as comma-delimited
feature values. SIMf (si,k , sj,k ) is similar to cosine similarity function, which is used often for measuring document
similarity in Information Retrieval (IR), but the difference
is that we use a phrase as a basic unit while a word unit
is usually adopted in IR. We use a phrase as a basic unit
because majority of the words may overlap in two very different feature values. For example, the specification phrases
“Memory Stick Duo”, “Memory Stick PRO-HG Duo”, “Memory Stick PRO Duo”, and “Memory Stick PRO Duo Mark2”
have high word cosine similarities among themselves since
they at least have 3 common words while the performances
of the specifications are very different. Thus, our similarity
function with phrase unit counts a match only if the phrases
are the same.

6.

METHODS

In this section, we suggest multiple methods for relevant
opinion sentences retrieval. We first suggest a standard summarization tool, MEAD [26]. In order to make up for the
MEAD’s weak points, we also suggest modified version of
MEAD. Then, we propose our probabilistic models to solve
the problem.

OVERALL APPROACH

When reviews are not available for a product, a consumer
has no way to obtain opinions on it. In order to help consumers in such situation, we believe that product specifications are the most valuable source to find similar products. We thus leverage product specifications to find similar
products and choose relevant sentences from their user reviews. In this approach, we assume that if products have
similar specifications, the reviews are similar as well. For
example, here is an actual review sentence from the review
of a digital camera that takes a picture at high resolution:
“the best camera I have ever owned, takes unbelievable crisp
sharp photos with it’s 16.1 Megapixels.” It is admitted that
the consumer is very impressed with the feature-value pair,
(“Resolution”, “16.1 Megapixels”), and we can expect that
other digital cameras with the same feature-value pair could
impress their consumers as well. The assumption may not
be valid in some cases, i.e., same specifications may yield
very different user reviews. We thus try to retrieve “central” opinions from similar products so that the retrieved
sentences can become clearly useful.

5.

(1)

6.1

MEAD: Retrieval by Centroid

For our problem, text retrieval based only on query-relevance
is not desirable. The retrieved sentences need to be central in
other reviews in order to obtain central opinions about specifications. For example, if there are more opinions that contains a word “big” than a word “small” for a certain featurevalue pair, it is desired to assign higher score to the sentences
having the word “big”. However, since the query contains
only feature-value pair words, classic information retrieval
approaches are not able to prefer such sentences. Therefore,
we suggest using a method that considers centrality among
sentences.
MEAD [26] is a popular centroid-based summarization
tool for multiple documents, and it was shown to effectively
generate summaries from a large corpus. It provides an autogenerated summary for multiple documents. For a corpus R,
a score of ith sentence t in a document is computed by sum
of centroid and position scores of words, which is defined as

SIMILARITY BETWEEN PRODUCTS

We assume that similar products have similar featurevalue pairs (specifications). In general, there are many ways
to define a similarity function. We are interested in finding how well a basic similarity function will work although
our framework can obviously accommodate any other similarity functions. Therefore, we simply define the similarity

score(t; R) = wc Ct + wo Ot

(4)

where Ct is a sum
P of centroid scores of words in t, which is
defined as Ct = w Cw,t , and Ot is a position score, which
gives higher score to the sentences appearing earlier in a

395

document and defined as Ot = (n−i+1)
· Cmax where n is
n
the number of sentences in the document and Cmax is the
maximum centroid score in the document. Centroid score
of a word, Cw,t , is a TFIDF value in the corpus R, and wc
and wo are weights for Ct and Ot , respectively. Please refer
to [26] for more details.
In order to retrieve sentences that are likely to be relevant to the query product Pz , which has no reviews, we
employ specifications to find products similar to Pz and use
the similarity as a clue for finding relevant sentences. Since
the score formula (4) utilizes only centrality and does not
consider relevance to the query product, we augment it with
product similarity to Pz so that we can find sentences that
are query-relevant and central at the same time. In addition,
MEAD employs position score that is reasonable for news
articles, but it may not be appropriate for reviews; unlike
news articles, it is hard to say that the sentences appearing
earlier in the reviews are more important than those appearing later. Thus, we remove position score term from formula
(4), and we augment it with similarity to query. The new
score function is defined as
score(t, Sy ; R, Sz ) = Ct · SIMp (Sy , Sz )

p(sy,k |t) is high. p(sz,k |sy,k ), proximity of sy,k to sz,k , is
estimated as follows.
p(sz,k |sy,k ) ∝ P

p(sy,k |t) =

Y

p(w|t) =

w∈sy,k

Y

p(w|t)c(w,sy,k )

(9)

w∈U

where U is a vocabulary set in corpus R, and c(w, sy,k ) is a
count of word w in the feature-value pair sy,k . p(w|t) follows
t’s unigram language model [24], and it means a word w’s
likelihood in a sentence t. One of the standard ways to estimate p(w|t) is using maximum likelihood (ML) estimator,
which gives p(w|t) = c(w,t)
, where c(w, t) is the count of w
|t|
in t, and |t| is the number of words in t. Thus, p(sy,k |t),
likelihood of a feature-value pair sy,k in a sentence t, becomes higher if more words in the feature-value pair appear
often in t. To avoid over-fitting and prevent p(sy,k |t) from
being zero, we smooth p(w|t) with Jelinek-Mercer smoothing method [8], which is shown in [31] to work reasonably
well. Using Jelinek-Mercer smoothing, p(w|t) is defined as:

(5)

Probabilistic Retrieval

p(w|t) = (1 − λ)pml (w|t) + λp(w|R)

To solve the problem in a more principled way, we introduce our probabilistic methods. Query likelihood retrieval
model [1], which assumes that a document generates a query,
has been shown to work well for ad-hoc information retrieval.
Similarly, we attempt to generate the query specifications Sz
from a candidate sentence t via several generative scenarios.

6.2.1

(8)

where Distinct(k) is a set of distinct feature-value pairs for
a feature fk . p(sy,k |t) is defined as

where t is a sentence in a review for product Py and SIMp
(Sy , Sz ) is a product similarity between Py and the query
product Pz , which is defined in equation (2).

6.2

SIMf (sz,k , sy,k )
s∈Distinct(k) SIMf (s, sy,k )

(10)

where pml (w|t) and p(w|R) follow a sentence language model
and a corpus language model, respectively, estimated with
ML estimator. To smooth p(w|t), a reference language model
p(w|R) is used so that we can have general word likelihood
that nicely augments pml (w|t). The resulting p(w|t) can be
regarded as weighted average of pml (w|t) and p(w|R).

Specifications Generation Model

The generative story is described as follows. Each sentence t from reviews of its product Py first generates its
specifications Sy . The specifications Sy then generates the
query specifications Sz . Following the dependencies among
variables, the scoring function is defined as

6.2.2

score(t, Sy ; R, Sz ) ∝ p(t, Sy |Sz )
=

p(Sz |Sy )p(Sy |t)p(t)
p(Sz )

(6)

We can interpret p(t, Sy |Sz ) as the probability that t and
Sy satisfy information needs of a user given Sz . p(Sz |Sy )
measures proximity of Sy to Sz . p(Sy |t) measures proximity
of t to Sy , and p(t) is a general preference on t. Since we assume no preference on sentences, we ignore p(t) for ranking.
p(Sz ) is also ignored because it does not affect the ranking
of sentences for Sz . Thus, the formula assigns high score
to a sentence if its specifications Sy match Sz well and the
sentence t matches its specifications Sy well. p(t, Sy |Sz ) is
then defined as

score(t, Ry\t , Sy ; R, Sz ) ∝ p(t, Ry\t , Sy |Sz )
\t

=

F
X

p(sz,k |sy,k )p(sy,k |t)

\t

p(Sz |Sy )p(Sy |t, Ry )p(Ry |t)p(t)
p(Sz )

(11)

∝ p(Sz |Sy )p(Sy |t, Ry\t )p(Ry\t |t)

p(t, Sy |Sz ) ∝ p(Sz |Sy )p(Sy |t)
=

Review and Specifications Generation Model

Specifications Generation model in section 6.2.1 does not
consider centrality among reviews. However, as explained in
section 6.1, centrality as well as query-relevance should be
considered for the task. Here, we assume that a candidate
sentence t of product Py generates the product’s reviews Ry
except itself t. This generation enables us to measure centrality of t among all other sentences in the reviews for Py .
\t
Then, t and Ry jointly generate its specifications Sy , where
\t
Ry is a set of reviews for Py except the sentence t. Intuitively, it makes more sense for Sy to be generated by both t
\t
and Ry than by only t. Sy then generates the query specifications Sz . Following the dependencies, the score function
is defined as

(7)

where p(t) and p(Sz ) are ignored for the same reason as
\t
in section 6.2.1. Now, p(Ry |t), a proximity of t to the re\t
views Ry , is computed to consider centrality of t. Also,
\t
\t
p(Sy |t, Ry ), a proximity of t and Ry to the specifications
Sy , is computed to promote sentences from reviews that
match its specifications well. Thus, a sentence t is preferred if (1) its specifications Sy is similar to Sz , (2) Sy
\t
represent its reviews Ry well, and (3) Ry represents t well.

k=1

where a set of specifications such as Sy is decomposed into
feature-value pairs sy,k . We assume that a k’th feature-value
pair of one specification set generates only the k’th featurevalue pair of another specification set, not other featurevalue pairs. This is to ensure that sentences not related to
a specification sz,k are scored low even if their word score

396

\t

p(t, Ry , Sy |Sz ) can be re-written as
p(t, Ry\t , Sy |Sz )

=

p(Ry\t |t)

F
X

decompose p(t, Sy |Sz ) as follows.
p(t, Sy |Sz )

Y

p(sz,k |sy,k )

p(w|t, Ry\t )



0
c(w,t)+c(w,Ri )
|t|+|Ri |

if w 6∈ t
if w ∈ t

qP

p(sz,k |sy,k )

X

p(sx,k |sy,k )p(Rx |t)

Y

p(w|t, Rx )

w∈sy,k

Px ∈P \z

(17)

(14)

where IDF of word w is defined as
(15)

where |R| is the number of reviews in the whole corpus, and
DF (w) is the number of documents that contain w.

6.2.3

p(sx,k |sy,k )p(sy,k |t, Rx )p(Rx |t)

where proximity between specifications are estimated using
cosine similarity function SIMf as in specifications generation model, and the proximity of t to arbitrary reviews Rx ,
p(Rx |t), is estimated by TFIDF cosine similarity function.
In order to consider the case Py is the same as Px , we define
p(w|t, Rx ) as

(1 − λ)δ(w|t, Rx ) + λp(w|R) if Px 6= Py
p(w|t, Rx ) =
\t
(1 − λ)δ(w|t, Rx ) + λp(w|R) if Px = Py
(18)
Meanwhile, looping over all non-query products is probably too expensive in terms of computational complexity.
We thus choose X translating products P X to reduce the
complexity. Perhaps, the most promising translating products may be those who are similar to the query product Pz .
We want the retrieved sentences to be translated well by
the actual reviews of Pz , which means that those reviews
of products not similar to Pz are not considered important.
Since we assume that products similar to Pz are likely to
have similar reviews, we exploit the similar products’ reviews to approximate Rz , where we measure similarity using
specifications. Therefore, we loop over only X translating
products P X that are most similar to Pz , where similarity
function SIMp is employed to measure similarity between
products. Since Px needs to be similar to Pz , we further
assume that Px generates Pz , which yields proximity of Px
to Pz , p(Pz |Px ), and it is defined as

(13)

c(w, d) · c(w, d0 ) · IDF (w)2
qP
2
0 0
0 2
w∈d (c(w, d) · IDF (w)) ·
w0 ∈d0 (c(w , d ) · IDF (w ))

|R|
1 + DF (w)

X
Px ∈P \z

k=1

w∈d,d0

IDF (w) = log

F
X

=

We ignore w if w is not in t in order to require the retrieved
sentences to contain words in sy,k . The proximity of t to
\t
\t
Ry , p(Ry |t), is estimated by TFIDF cosine similarity func\t
tion SIM (Ry , t), where TFIDF cosine similarity between
documents d and d0 is defined as
SIM (d, d0 ) =
P

p(sz,k |sy,k )

k=1

(12)
\t
\t
where p(w|t, Ry ) is smoothed to (1−λ)δ(w|t, Ry )+λp(w|R),
\t
where δ(w|t, Ry ) is defined as
δ(w|t, Ri ) =

F
X

∝

w∈sy,k

k=1

Translation Model

In Review and Specifications Generation model, we assumed a sentence t of product Py generates its reviews Ry ,
and t and Ry jointly generate their specifications Sy . However, we can also assume that t generates reviews of an arbitrary product because there may be better reviews that can
represent t and generate Sy well. In other words, there may
be a product Px that translates t and generates Pz based on
the translation with a better performance.
The generative story is described as follows. A candidate
sentence t of a product Py generates each review set of all
products, which will be used as translations of t. t and
each of the generated review sets, Rx , jointly generates t’s
specifications Sy , and Sy generates specifications of Rx , Sx ,
and the query specifications Sz . We intend Sy to generate
specifications of the translating product Sx so as to penalize
the translating product if its specifications are not similar
to Sy . Following the generative story, the score function is
defined as

p(Pz |Px ) = P

SIMp (Pz , Px )
SIMp (Pz , Px0 )

(19)

x0 ∈P X

and this product-level similarity is used as a weight of Px in
formula (17).

7.
7.1

EXPERIMENTAL SETUP
Data Set

Since we study a new task that has not been studied before, there is no existing test collection available to use for
evaluation. We thus must solve the challenge of creating a
test set. We address this problem by using products with
known reviews as test cases. We pretend that we do not
know their reviews and use our methods to retrieve sentences
in K words; we then compare these results with the actual
known review of a test product. This allows for evaluating
the task without requiring manual work, and is a reasonable
way to perform evaluation because it would reward a system
that can retrieve review sentences that are very similar to
the actual review sentences of a product.
We now describe how to build our data set in detail.
First, it is required for our problem to obtain reviews and
specifications for products, and this kind of data is available in several web sites such as Amazon.com, BestBuy.com,
and CNET.com. Among them, we chose CNET.com because they have reasonable amount of review data and relatively well-organized specifications. There are several product categories in CNET.com, and we chose digital camera

score(t, Sy ; R, Sz ) ∝ p(t, Sy |Sz )
P
p(Sz |Sy ) Px ∈P \z p(Sx |Sy )p(Sy |t, Rx )p(Rx |t)p(t)
=
p(Sz )
X
∝ p(Sz |Sy )
p(Sx |Sy )p(Sy |t, Rx )p(Rx |t)
Px ∈P \z

(16)
where p(Sz ) and p(t) are ignored for the same reason as
before. As described, the score function contains a loop
over all products (except Pz ), instead of using only t’s review
set Ry , to get the votes from all translating products. The
features in different specifications are paired together, which

397

and MP3 player categories since they are reasonably popular and therefore the experiment results can yield significant
impact. From CNET.com, we crawled product information
for all products that were available on February 22, 2012
in both categories. For each product, we collected its user
reviews and specifications.

editions, in order to ensure that review sentences from the
different version of the same product are not retrieved. For
each of the top products, Pz , all sentences of other products
are regarded as candidate sentences. Pretending Pz does not
have any reviews, we rank those candidate sentences and
generate a text of first K word tokens, and we compare it
with the actual reviews of Pz . We assume that if the generated review text is similar to the actual reviews, it is a good
review text for Pz . The average number of reviews in the top
50 products is 78.5 and 152.2 for digital cameras and mp3
players, respectively. For the probabilistic retrieval models,
we use λ to control the amount of smoothing for language
models, and we empirically set it to 0.5 for both product
categories, which showed the best performance.

Table 1: Statistics of the data for digital camera and
MP3 player categories.
Digital
MP3
Camera
Player
Num. of products
1,153
605
Num. of reviews
12,779
14,159
Num. of sentences
137,599
291,858
Num. of word tokens
754,888 172,5192
Vocabulary size
6442
6959
Num. of features
9
8
Num. of distinct feature values
1,038
384

7.2

We pruned out products that do not contain reviews or
specifications. (We found that about two thirds of the products didn’t have any user reviews.) To preprocess the review text, we performed sentence segmentation, word tokenization, and lemmatization using Stanford CoreNLP [17]
version 1.3.5. We lowered word tokens and removed punctuations. Then, we removed word tokens that appear in
less than five reviews and stopwords. We also preprocessed
specifications data. In general, specifications contain dozens
or hundreds of distinct features, and many of them are not
mentioned in the reviews. Therefore, we choose features
that are considered important by users. In order to choose
such key features, we simply adopt highlighted features provided by CNET.com assuming that they chose the features
based on importance. The highlighted features are listed in
Table 2. We removed feature values that appear in less than
five products. Then, we tokenized the feature and feature
value words, and we lowered the word tokens. The statistics of the reviews and specifications data is shown in Table
1. While digital camera category has more products, more
reviews are written for mp3 player categories. Also, in general, users wrote more texts per review for mp3 players than
digital cameras. The number of highlighted features used for
digital cameras is similar to that for mp3 players while there
are much more distinct feature values for digital cameras.

where r and s are reference and retrieved summaries, respectively, gramn is n-gram text, Countmatch (gramn ) is the
maximum number of n-grams co-occurring in the retrieved
summary and a reference summary. When there are multiple reference summaries are available, they use the following
evaluation formula.

Table 2: CNET.com’s highlighted features for digital camera and MP3 player categories.
Digital Camera
Manufacturer
Product Type
Resolution
Digital Video Format
Image Stabilizer
Lens System – Type
Memory / Storage
– Supported Mem. Cards
Camera Flash
– Camera Flash
Optical Sensor Type

Evaluation Metrics

To evaluate a quality of the length-K retrieved text based
on actual reviews for the query, we face another challenge:
how should we measure the performance? We could consider using standard retrieval measures, but neither NDCG,
nor MAP seems appropriate since we do not have multiple
levels of judgments or even binary judgments. We thus decided to measure the proximity between the retrieved text
and the actual reviews. Regarding the retrieved text as a
summary for the query product, we can view our task as
similar to multiple document summarization, whose goal is
to generate a summary of multiple documents. Thus, we employ ROUGE evaluation method [13], which is a standard
evaluation system for multiple document summarization. In
general, ROUGE evaluates the quality of an automatically
generated summary by comparing it with one or more manually generated reference summaries. Assuming the actual
reviews of the query product are manually generated reference summaries, we can adopt ROUGE to evaluate the
retrieved sentences. Among various ROUGE metrics, we
employ ROUGE-1 and ROUGE-2, which are unigram and
bigram matching metrics, respectively, and have been shown
to perform well for the task. We compute precision, recall,
and F1-score of each metric. For example, recall of ROUGEn is defined as
P
gramn ∈S Countmatch (gramn )
P
ROUGE-n(r, s) =
(20)
gramn ∈S Count(gramn )

MP3 Player
Manufacturer
Product Type
Digital Storage
Flash Memory Installed
Built-in Display – Diagonal Size
Battery / Power – Battery
Digital Player / Recorder
– Supported Digital Audio Standards
Battery / Power
– Mfr. Estimated Battery Life

ROUGE-nmulti = maxi ROUGE-n(ri , s)

(21)

Please note that each of the precision, recall, and F1-score
takes the maximum from the reference summaries. More
details about ROUGE can be found in [13].
However, the problem of ROUGE metrics is that it does
not consider importance of words. All words have different
level of importance; for example a word such as “of” is much
less important than a word “megapixel” since “of” appears
too often in documents and does not carry useful information. If a retrieved text contains many unimportant words,
it may obtain a high score by ROUGE metrics, which is
not desired. Therefore, we also employ TFIDF cosine similarity, which considers word importance by Inverse Document Frequency (IDF). TFIDF cosine similarity function
between two documents is defined in equation (14). While
the formula measures similarity based on bag of words, bigram provides important information about distance among
words, so we adopt bigram-based TFIDF cosine similarity as

In order to evaluate the performance of our methods for
retrieving review sentences for a new or unpopular product,
we perform the following experiment. To choose test products, which will be regarded as products with no reviews, we
selected top 50 qualified products by the number of reviews
in each category in order to obtain statistically reliable gold
standard data. Please note that we did not select (qualify)
products that have their different versions such as colors or

398

well. Similar to ROUGE-nmulti , we take a maximum from
SIM (ri , s) among different reference summaries because we
still evaluate based on multiple reference summaries. For
both ROUGE and SIM metrics, we use retrieved text length
100, 200, and 400, which reflect diverse users’ information
needs.

8.
8.1

supported by the following actual review sentences: “Amazingly sharp lens.” and “It has a much better lens package
than the Rebel and the base 20D kit.” Sentences (2) and (6)
claim the product’s good value, which is again supported
by actual review sentences: “Better value than you think”
and “The camera is also cheaper than the comparable Nikon
and Canon.” The retrieved sentence such as (8) mentions
about ease of use for the camera, and many users actually
complimented the camera on its ease of use, indeed. The
supporting sentences are as follows: “Very easy to use right
out of the box.” and “The controls are very easy to learn
and are, for the most part, very intuitive.” Meanwhile, the
sentence (1) carries inconsistent opinion, which shows negative sentiment on Pentax camera. Nevertheless, in a user’s
perspective, who does not know much about Pentax *ist DS
or other Pentax cameras, the listed information would be
highly informative especially if the camera has no or few
reviews. Although some of the retrieved sentences do not
carry useful information, it is clear that some other retrieved
sentences are indeed useful.
Our probabilistic retrieval models have a capability of retrieving relevant sentences for a specific feature. For each
of the probabilistic models, we can assume that the number of features F is one so that the score functions compute
only for one feature. Table 5 shows top retrieved sentences
for the feature “Lens System – Type” of Pentax *ist DS. As
found in the top sentences for the whole product in Table
3, we can easily find that all the sentences except (2) praise
the lens compatibility of Pentax, indeed. In addition, all
sentences except (1) praises high quality of its lens, which
is coherent with the top sentences for the whole product.
From the sentences, users can learn much about the given
product’s lens such as other consumers’ general sentiment
and specific reasons why they like or dislike its lens.

EXPERIMENT RESULTS
Qualitative Analysis

Table 3: Top ten sentences retrieved for Pentax
*ist DS (Digital Camera) by Translation model with
X=5.
(1) This was my first and my last Pentax .
(2) This pentax is a great value for money , and a nice entry
level dslr , compatible with most Pentax lens .
(3) I have found the Pentax DL to be high quality , with
great features .
(4) Nice job pentax .
(5) I have been a Pentax SLR user for years , beginning
with the SuperProgram , ZX-50 , and ZX-5n .
(6) When I bought it , I was in bankruptcy and the cheaper
Pentax came to me .
(7) Pentax have been making great lenses and cameras for
a long time , and this range is no exception .
(8) Great photos , color , ease of use , compact size ,
compatible with Pentax mount lenses .
(9) I had owned a great 35mm Pentax camera before that
took wonderful pictures , which , after 20 years went caput .
(10) Very smart Pentax .

Table 4: Specifications for Pentax *ist DS. Note that
some feature values are not available.
Feature
Manufacturer
Product Type
Resolution
Digital Video Format
Image Stabilizer
Lens System – Type
Memory / Storage
– Supported Mem. Cards
Camera Flash
– Camera Flash
Optical Sensor Type

Value
Pentax
Digital camera - SLR
6.1 megapixels

Table 5: Top sentences retrieved by Translation
model (X=5) specifically for the feature “Lens System – Type” of Pentax *ist DS.
(1) This pentax is a great value for money , and a nice entry
level dslr , compatible with most Pentax lens .
(2) Pentax have been making great lenses and cameras for
a long time , and this range is no exception .
(3) Great photos , color , ease of use , compact size ,
compatible with Pentax mount lenses .
(4) The kit lens is better than what ships with some
competitors , and the camera is compatible with most
older Pentax lenses , making it possible to save hundreds
by buying used lenses rather than having to sink money
into new digital lenses .
(5) Compatibility with older Pentax lenses is a real bonus
too , as these are usually of very high quality and can be
picked up at good prices second-hand .

3 x x Zoom lens - 18 mm - 55 mm
- F/3.5-5.6 DA Pentax KAF
SD Memory Card
Pop-up flash
CCD

In order to see the usefulness of the sentences retrieved
by our novel Translation model, we show the top retrieved
sentences for query products and compare them with the
actual review sentences for the query products. Table 3
lists top retrieved sentences for a product in each category,
where the sentences are ordered by their scores, and the
specifications of the product is listed in Table 4.We set the
number of translating products to five, which is reasonable
if we consider the computational cost of the model.
For the digital camera Pentax *ist DS, several top retrieved sentences such as (2), and (8) mention about its
compatibility with Pentax lenses. Surprisingly, there were
several reviews for Pentax *ist DS that praise its lens compatibility, and here are two actual examples from review
sentences: “Plus the DS is backwards compatible with all
old Pentax lenses, which have a well-deserved reputation
among photographers.” and “I can use my pile of old (and
very old) Pentax lenses including the m42 lenses.” Also,
the retrieved sentences such as (7), (8), (9), and possibly
(3) mention about Pentax’s great picture quality, which is

Manually finding relevant opinions for a query product or
its specific feature is extremely time-consuming for users;
they need to find similar products by manually comparing specifications and extract relevant and central sentences
from all the reviews of the similar products, which may take
too much time. Here, we verified the automatically retrieved
sentences can be indeed useful for users. In the next section,
we quantitatively compare our Translation model with other
suggested methods.

8.2

Quantitative Evaluation

To retrieve review sentences that are likely to be written for a new or unpopular product, we employ several
methods. In order to see the effectiveness of a standard
ad-hoc retrieval method, we employ query likelihood (QL)
language model approach [24], and we define the score func-

399

@ K for Digital Camera
COS1@400 COS2@100
0.147
0.0185
0.141
0.0258
0.158
0.0271
0.206
0.0230
0.231
0.0210
0.333†‡
0.0736†‡
(+111%)
(+172%)
0.118
0.0147
0.091
0.0123
0.145
0.0206
0.208
0.0225
0.253
0.0261
0.316†‡
0.0458†‡
(+118%)
(+104%)

400

and MP3 Player categories.
COS2@200 COS2@400
0.0215
0.0257
0.0204
0.0184
0.0226
0.0223
0.0270
0.0291
0.0244
0.0298
0.0743†‡
0.0794†‡
(+229%)
(+256%)
0.0159
0.0173
0.0128
0.0117
0.0197
0.0178
0.0274
0.0294
0.0270
0.0327
0.0567†‡
0.0649†‡
(+188%)
(+265%)

1

2

5

10

20

50

0.08
0.07
0.06
0.05

COS2@100
COS2@200
COS2@400

0.04

Digital Camera − Bigram

0.34
0.30
0.26

COS1@100
COS1@200
COS1@400

0.22

Digital Camera − Unigram

that Translation model retrieves more connected fragments
that are in the actual reviews.

100

1

2

5

2

5

10
X

20

20

50

100

MP3 Player − Bigram

0.02 0.03 0.04 0.05 0.06

0.28
0.24

COS1@100
COS1@200
COS1@400
1

10

50

100

X

0.32

X

MP3 Player − Unigram

Q
P
tion as score(t; R, Sz ) = F
k=1
w∈sz,k p(w|t), where p(w|t)
is smoothed as in equation (10). We suggested a modified
version of one of the standard summarization tools, MEADSIM in formula (5), which considers both query-relevance
and centrality. We employ MEAD-SIM as one of the baseline methods, and we also show results from the basic MEAD
in formula (4) to see the effect of query-relevance addition
to MEAD; we set wc = 1 and wo = 0 since position score is
inappropriate for reviews. We also introduced several probabilistic retrieval methods for the task. Review and Specifications Generation model (ReviewSpecGen) considers both
query-relevance and centrality, so we use it as another baseline method. Specifications Generation model (SpecGen) focuses on query-relevance, and we show its results to compare
with ReviewSpecGen and QL. We then suggested our novel
Translation model (Translation). We tuned X to be 100
for digital cameras and 10 for mp3 players, unless otherwise
specified, which showed the best TFIDF cosine similarity
values. The results from Translation model are mainly compared with the two baselines MEAD-SIM and ReviewSpecGen. † and ‡ are used to mark if the improvement for Translation model is statistically (paired t-test with p=0.05) significant in each measure from MEAD-SIM and ReviewSpecGen, respectively. We also record how much Translation
model outperforms MEAD-SIM in parentheses.
Table 6 shows TFIDF cosine similarity evaluation results
for both digital cameras and mp3 players. Both unigram
(COS1) and bigram (COS2) measures are listed for the suggested methods. In general, models that exploit specifications as query (MEAD-SIM, SpecGen, ReviewSpecGen, and
Translation) except QL outperform MEAD, which does not
compute query-relevance. QL outperforms MEAD in mp3
player data set, but it does not outperform other models in
both data sets, since does not consider specifications similarity between products. MEAD-SIM outperforms MEAD in
all cosine similarity measures (12/12), which means that centrality alone cannot perform well. ReviewSpecGen adds centrality computation to SpecGen, and the results show that
its centrality helps it outperform SpecGen in most measures
(9/12). ReviewSpecGen outperforms MEAD-SIM in all unigram measures (6/6) and most bigram measures (5/6). Translation model significantly outperforms MEAD-SIM in all
measures (12/12), and the average performance increase percentage is 162%. It also significantly outperforms ReviewSpecGen in all measures (12/12), which means that choosing
products similar to the query product as translating products was more effective than choosing only one product the
candidate sentence belongs. Translation model outperforms
other models especially in bigram measures, which means

0.20

Table 6: Unigram and bigram TFIDF cosine similarity
Category Model
COS1@100 COS1@200
Digital
QL
0.112
0.128
Camera
MEAD
0.131
0.124
MEAD-SIM
0.136
0.130
SpecGen
0.143
0.173
ReviewSpecGen 0.171
0.208
Translation
0.314†‡
0.327†‡
(increase %)
(+131%)
(+152%)
MP3
QL
0.090
0.99
Player
MEAD
0.089
0.078
MEAD-SIM
0.131
0.136
SpecGen
0.153
0.183
ReviewSpecGen 0.206
0.227
Translation
0.267†‡
0.297†‡
(+104%)
(+118%)
(increase %)

COS2@100
COS2@200
COS2@400
1

2

5

10

20

50

100

X

Figure 2: TFIDF cosine similarity evaluation results
for Translation model with different number (X) of
translating products. Upper figures are for digital
cameras, and lower figures are for mp3 players. Left
figures are results based on unigrams, and right figures are those base on bigrams.
We also evaluate retrieval results with ROUGE metrics.
Although ROUGE does not consider importance of words,
it is able to compute recall, precision, and F1 score in both
unigram (ROUGE1-R, ROUGE1-P, and ROUGE1-F) and
bigram (ROUGE2-R, ROUGE2-P, and ROUGE2-F) units.
The ROUGE evaluation results for mp3 players are shown
in Table 7. QL outperforms MEAD in all measures, but
it is outperformed by MEAD-SIM in all measures since QL
does not consider specifications similarity between products.
SpecGen outperforms ReviewSpecGen in most measures (13/
18) especially in bigram measures (9/9), which is different
from the TFIDF cosine similarity results; this means that
the sentences retrieved by SpecGen are more similar to ac-

K

100

200

400

Table 7:
Model
QL
MEAD
MEAD-SIM
SpecGen
ReviewSpecGen
Translation
(increase %)
QL
MEAD
MEAD-SIM
SpecGen
ReviewSpecGen
Translation
(increase %)
QL
MEAD
MEAD-SIM
SpecGen
ReviewSpecGen
Translation
(increase %)

Unigram and bigram ROUGE @ K for MP3 Players category.
ROUGE1-R ROUGE1-P ROUGE1-F ROUGE2-R ROUGE2-P
0.278
0.218
0.150
0.0545
0.0308
0.202
0.217
0.132
0.0364
0.0242
0.328
0.319
0.196
0.0615
0.0381
0.303
0.299
0.191
0.0727
0.0480
0.323
0.320
0.204
0.0650
0.0431
0.369†‡
0.375‡
0.236†‡
0.1151†‡
0.0742†‡
(+11%)
(+12%)
(+20%)
(+87%)
(+95%)
0.384
0.213
0.166
0.0812
0.0264
0.266
0.171
0.127
0.0453
0.0186
0.434
0.266
0.201
0.0834
0.0290
0.413
0.273
0.204
0.0913
0.0423
0.411
0.267
0.197
0.0848
0.0324
0.481†‡
0.318†‡
0.239†‡
0.1582†‡
0.0664†‡
(+11%)
(+20%)
(+19%)
(+90%)
(+129%)
0.501
0.186
0.175
0.1159
0.0205
0.370
0.154
0.141
0.0517
0.0111
0.560
0.224
0.210
0.1260
0.0207
0.535
0.221
0.207
0.1228
0.0293
0.546
0.221
0.204
0.1171
0.0254
0.595†‡
0.240†‡
0.225†‡
0.2112†‡
0.0431†‡
(+6%)
(+7%)
(+7%)
(+68%)
(+108%)

tual reviews than those retrieved by ReviewSpecGen, but
ReviewSpecGen retrieved more “important” relevant words.
Translation model outperforms all other models in all measures (18/18), and the increase from MEAD-SIM and ReviewSpecGen is statistically significant in most measures
(17/18 and 18/18, respectively). Similar to TFIDF cosine
similarity results, the performance difference in bigram is
clearer than in unigram, which means Translation model retrieves bigger fragments of actual reviews well. The increase
in unigram ROUGE measures is not as big as that in unigram TFIDF cosine similarity measures, which means that
the number of relevant words from Translation model is not
very different from other models, but Translation model retrieves much more important relevant words.
We also evaluated retrieved sentences for digital cameras
with ROUGE metrics. In general, Translation model outperforms other models in all measures. More specifically, it
significantly outperforms MEAD-SIM and ReviewSpecGen
in most measures (16/18 and 18/18, respectively). We do
not list ROUGE evaluation results for digital cameras since
the other patterns are similar to those for mp3 players.
Overall, ROUGE evaluation results are similar to cosine
similarity evaluation results in general. The difference between the two metrics is that the TFIDF cosine similarity metric differentiates various models more clearly since
they consider importance of word while the ROUGE metric
does not; TFIDF cosine similarity metric prefers retrieved
text that contains more important words, which is a desired
property in such evaluation. On the other hand, ROUGE
metric considers various evaluation aspects such as recall,
precision, and F1 score, which can possibly help us analyze
evaluation results in depth.
In order to reduce computation complexity of Translation
model, we proposed to exploit X number of most promising
products that are similar to the query product, instead of
all products, under the assumption that similar products are
likely to have similar reviews. We performed experiments
with different X values to find how many translating products are needed to obtain reasonably good performance. The
results are evaluated with TFIDF cosine similarity @ K for
unigrams and bigrams, and the results are shown in Figure
2. Surprisingly, only a few translating products (e.g., ten)

ROUGE2-F
0.0297
0.0227
0.0367
0.0406
0.0378
0.0634†‡
(+73%)
0.0300
0.0203
0.0333
0.0395
0.0344
0.0657†‡
(+97%)
0.0265
0.0146
0.0268
0.0342
0.0314
0.0542†‡
(+102%)

are enough to perform reasonably well especially for mp3
players. These results mean that only a few “good” translating products are enough to translate a candidate sentence
well, and the “good” translating products may be selected
by their similarity to the query product.

9.

CONCLUSION AND FUTURE WORK

In this paper, we studied the problem of automatic relevant review text retrieval for products having no reviews.
Relevant review sentences for new or unpopular products
can be very useful for consumers who seek for relevant opinions, but no previous work has addressed this novel problem. We proposed several methods to solve this problem,
including summarization-based methods such as MEAD and
MEAD-SIM and probabilistic retrieval methods such as Specifications Generation model, Review and Specifications Generation model, and Translation model. To evaluate relevance
of retrieved opinion sentences in the situation where humanlabeled judgments are not available, we measured the proximity between the retrieved text and the actual reviews of
a query product. Experiment results show that our novel
Translation model indeed retrieves useful sentences and significantly outperforms the baseline methods.
Our work opens up a new direction in text data mining
and opinion analysis. The new problem of review text retrieval for new products can be studied from multiple perspectives. First, it can be regarded as a summarization problem as the retrieved sentences need to be central across different reviews. Second, as done in this paper, it can also
be regarded as a special retrieval problem with the goal of
retrieving relevant opinions with product specifications as a
query. Finally, it can also be studied from the perspective of
collaborative filtering where we would leverage related products to recommend relevant “opinions” to new products. All
these are interesting future directions that can potentially
lead to even more accurate and more useful algorithms.

10.

ACKNOWLEDGMENTS

This work is supported in part by a gift fund from TCL
and by the National Science Foundation under Grant Number CNS-1027965.

401

11.

REFERENCES
[19]

[1] A. Berger and J. Lafferty. Information retrieval as
statistical translation. In Proceedings of ACM SIGIR
1999, pages 222–229, 1999.
[2] I. Bhattacharya, S. Godbole, and S. Joshi. Structured
entity identification and document categorization: two
tasks with one joint model. In Proceedings of ACM
KDD 2008, pages 25–33, 2008.
[3] C. Dellarocas, X. M. Zhang, and N. F. Awad.
Exploring the value of online product reviews in
forecasting sales: The case of motion pictures. Journal
of Interactive marketing, 21(4):23–45, 2007.
[4] H. Duan, C. Zhai, J. Cheng, and A. Gattani.
Supporting keyword search in product database: A
probabilistic approach. Proc. VLDB Endow.,
6(14):1786–1797, Sept. 2013.
[5] G. Ganu, N. Elhadad, and A. Marian. Beyond the
stars: Improving rating predictions using review text
content.
[6] E. Hovy and C.-Y. Lin. Automated text
summarization in SUMMARIST. In I. Mani and
M. T. Maybury, editors, Advances in Automatic Text
Summarization. MIT Press, 1999.
[7] M. Hu and B. Liu. Mining and summarizing customer
reviews. In Proceedings of KDD ’04, pages 168–177,
2004.
[8] F. Jelinek. Interpolated estimation of markov source
parameters from sparse data. Pattern recognition in
practice, 1980.
[9] H. D. Kim, K. Ganesan, P. Sondhi, and C. Zhai.
Comprehensive review of opinion summarization.
Computer Science Research and Tech Reports, 2011.
[10] H. D. Kim and C. Zhai. Generating comparative
summaries of contradictory opinions in text. In CIKM
’09: Proceeding of the 18th ACM conference on
Information and knowledge management, pages
385–394, New York, NY, USA, 2009. ACM.
[11] J. Kupiec, J. Pedersen, and F. Chen. A trainable
document summarizer. In Proceedings of ACM SIGIR
’95, pages 68–73, 1995.
[12] M. Lalmas. Xml retrieval (synthesis lectures on
information concepts, retrieval, and services). Morgan
and Claypool, San Rafael, CA, 2009.
[13] C.-Y. Lin. Rouge: A package for automatic evaluation
of summaries. In Text Summarization Branches Out:
Proceedings of the ACL-04 Workshop, pages 74–81,
2004.
[14] B. Liu. Sentiment analysis and subjectivity. In
N. Indurkhya and F. J. Damerau, editors, Handbook of
Natural Language Processing, Second Edition. CRC
Press, Taylor and Francis Group, Boca Raton, FL,
2010. ISBN 978-1420085921.
[15] B. Liu, M. Hu, and J. Cheng. Opinion observer:
analyzing and comparing opinions on the web. In
Proceedings of WWW ’05, pages 342–351, 2005.
[16] Y. Lu, C. Zhai, and N. Sundaresan. Rated aspect
summarization of short comments. In Proceedings of
WWW ’09, pages 131–140, 2009.
[17] C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel,
S. J. Bethard, and D. McClosky. The Stanford
CoreNLP natural language processing toolkit. In
Proceedings of 52nd Annual Meeting of the
Association for Computational Linguistics: System
Demonstrations, pages 55–60, 2014.
[18] Q. Mei, X. Ling, M. Wondra, H. Su, and C. Zhai.
Topic sentiment mixture: modeling facets and

[20]

[21]
[22]

[23]

[24]

[25]
[26]

[27]

[28]
[29]

[30]

[31]

[32]

402

opinions in weblogs. In Proceedings of WWW ’07,
pages 171–180, 2007.
C. D. Paice. Constructing literature abstracts by
computer: techniques and prospects. Inf. Process.
Manage., 26(1):171–186, 1990.
B. Pang and L. Lee. Seeing stars: Exploiting class
relationships for sentiment categorization with respect
to rating scales. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguistics,
pages 115–124. Association for Computational
Linguistics, 2005.
B. Pang and L. Lee. Opinion mining and sentiment
analysis. Found. Trends Inf. Retr., 2(1-2):1–135, 2008.
D. H. Park, C. Zhai, and L. Guo. Speclda: Modeling
product reviews and specifications to generate
augmented specifications. In Proceedings of the 2015
SIAM International Conference on Data Mining.
SIAM, 2015.
I. Peñalver-Martı́nez, R. Valencia-Garcı́a, and
F. Garcı́a-Sánchez. Ontology-guided approach to
feature-based opinion mining. In Natural Language
Processing and Information Systems, pages 193–200.
Springer, 2011.
J. M. Ponte and W. B. Croft. A language modeling
approach to information retrieval. In Proceedings of
the 21st annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 275–281. ACM, 1998.
A.-M. Popescu and O. Etzioni. Extracting product
features and opinions from reviews. In Proceedings of
HLT-EMNLP 2005, pages 339–346, 2005.
D. R. Radev, H. Jing, and M. Budzikowska.
Centroid-based summarization of multiple documents:
sentence extraction, utility-based evaluation, and user
studies. In Proceedings of the 2000 NAACL-ANLP
Workshop on Automatic Summarization, pages 21–30.
Association for Computational Linguistics, 2000.
A. I. Schein, A. Popescul, L. H. Ungar, and D. M.
Pennock. Methods and metrics for cold-start
recommendations. In Proceedings of the 25th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 253–260.
ACM, 2002.
I. Titov and R. McDonald. Modeling online reviews
with multi-grain topic models. In Proceedings of
WWW ’08, pages 111–120, 2008.
T. Wang, Y. Cai, G. Zhang, Y. Liu, J. Chen, and
H. Min. Product feature summarization by
incorporating domain information. In Database
Systems for Advanced Applications. Springer, 2013.
J. Yu, Z.-J. Zha, M. Wang, K. Wang, and T.-S. Chua.
Domain-assisted product aspect hierarchy generation:
towards hierarchical organization of unstructured
consumer reviews. In Proceedings of EMNLP 2011,
pages 140–150, 2011.
C. Zhai and J. Lafferty. A study of smoothing
methods for language models applied to ad hoc
information retrieval. In Proceedings of the 24th
annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 334–342. ACM, 2001.
L. Zhou and P. Chaovalit. Ontology-supported
polarity mining. Journal of the American Society for
Information Science and technology, 59(1):98–110,
2008.

