Reducing Hubness: A Cause of Vulnerability in
Recommender Systems
Kazuo Hara

Ikumi Suzuki

National Institute of Genetics
Mishima, Shizuoka, Japan

The Institute of Statistical Mathematics
Tachikawa, Tokyo, Japan

kazuo.hara@gmail.com
Kei Kobayashi

suzuki.ikumi@gmail.com
Kenji Fukumizu

The Institute of Statistical Mathematics
Tachikawa, Tokyo, Japan

The Institute of Statistical Mathematics
Tachikawa, Tokyo, Japan

kei@ism.ac.jp

fukumizu@ism.ac.jp

ABSTRACT

[5]. That is, in higher dimensions, a small number of objects,
called hubs, appear frequently in the k-NNs of other objects.
When k-NNs of users are computed in memory-based CF
systems, hub users emerge because past user-response data
is generally high dimensional [4, 2]. Because hub users contribute to the recommendation process, they can significantly influence recommendation systems.
We view shilling attacks as attacks that exploit hubness.
The attacks inject fake users that are likely to become hubs,
in order to gain maximum control over the output of the
systems. To mitigate the effects of the attacks, we propose reducing hubness by transforming the similarity measure used in the k-NN computation. Because shilling attacks fabricate objects that are similar to the data centroid
in an effort to maximize their influence, we transform similarity measure such that all objects, including the injected
objects, are equally similar to the centroid. This transformation is achieved by computing a commute time kernel from
a given similarity matrix [7], or more simply, by centering
the similarity matrix [8]. Using the MovieLens dataset, we
demonstrate that after such transformation, the hubness of
fake users tends to be reduced. As a result, the transformation provides systems with tolerance to attacks, without
degrading the accuracy of the predicted ratings.1

It is known that memory-based collaborative filtering systems are vulnerable to shilling attacks. In this paper, we
demonstrate that hubness, which occurs in high dimensional
data, is exploited by the attacks. Hence we explore methods for reducing hubness in user-response data to make these
systems robust against attacks. Using the MovieLens dataset,
we empirically show that the two methods for reducing hubness by transforming a similarity matrix—(i) centering and
(ii) conversion to a commute time kernel—can thwart attacks without degrading the recommendation performance.

Categories and Subject Descriptors
H.4 [Information Systems Applications]: Miscellaneous

Keywords
Collaborative filtering; Shilling attack; Hubness

1.

INTRODUCTION

Memory-based collaborative filtering (CF) is a type of recommendation system that predicts a user’s response to unseen items by referencing past responses from the k-nearest
neighbor (k-NN) users. A considerable drawback to memorybased CF is its vulnerability to shilling attacks. Shilling
attacks inject recommendation systems with fake users in
order to force the systems to generate biased recommendations [3, 1]. By design, memory-based CF issues recommendations based on the past responses to items by other users,
rather than the individual to whom the recommendation is
made. This design provides room for attackers to alter the
system’s decision by injecting fake users and forged ratings.
On the other hand, it has recently been found that hubness
is subject to a form of the so-called curse of dimensionality

2.
2.1

PRELIMINARY
Memory-Based Algorithm

We assume a set of observed responses is provided in a
matrix R of size Nuser × Nitem , where Nuser , Nitem is the
number of users and items, respectively. A component value
of R in the uth row and the ith column, R(u, i), denotes
the degree of preference of the uth user for the ith item.
Matrix R may contain missing values, represented by nil ; R
is usually very sparse, in the sense that greater part of its
components is nil. The goal of the system is to predict the
value of each missing component in R, which corresponds to
the future response from a user on an item. Using observed
responses, memory-based algorithms predict missing values
by interpolating the values of the k-NNs.

c 2015 Association for Computing Machinery. ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or affiliate of a national
government. As such, the Government retains a nonexclusive, royalty-free right to
publish or reproduce this article, or to allow others to do so, for Government purposes
only.
SIGIR’15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767823.

1
Although this paper focuses solely on reducing hubness exploited by average attacks against user-based algorithms,
the proposal can similarly be adopted for reducing hubness
exploited by segment attacks against item-based algorithms.

815

Prediction Function.

d = 50

3

10

2

where Sim(·, ·) is a function that returns the similarity of
two users, U is a set of the k most similar users to user u as
measured by Sim(·, ·) that satisfies R(n, i) 6= nil for n ∈ U ,
and
PNitem
R(u, i)δ[R(u, i) 6= nil]
i=1
R̄(u) =
PNitem
δ[R(u, i) 6= nil]
i=1

2

Frequency

10

1

10

0

10

d = 1000

3

10

Frequency

Below, P red(u, i) denotes a function that predicts a future
response of user u to item i.
P
n∈U Sim(u, n){R(n, i) − R̄(n)}
P
P red(u, i) =
+ R̄(u) (1)
n∈U Sim(u, n)

10

1

10

0

0

10

20

N10

30

10

40

0

(a) Low dimension.

20

40

d = 50

120

140

d = 1000

0.3

0.2

0.1

0.28
0.26
0.24
0.22
0.2

0
0

10

20

N10

30

(a) Low dimension.

(2)

40

0.18
0

20

40

60

N10

80

100

120

140

(b) High dimension.

Figure 2: Scatter plot of objects for N10 values and
similarity to data centroid. A strong correlation
emerges in high dimension (b).

where xu is an Nitem dimensional vector, the component of
which is given as xu (i) = R(u, i) if R(u, i) 6= nil, xu (i) = 0
otherwise. One disadvantage of using the above functions
is that a bias caused by the difference in each user’s mean
response is ignored. Therefore, a popular correction is to
subtract R̄(u) from each vector component [6, 4, 2]. The
resulting similarity using corrected vectors is known as the
Pearson correlation between users, as follows.2

ior for other items. More precisely, fake users are tailored
to give a high rating for target items, but average ratings
for each of the remaining items. As a result, fake users that
prefer target items become similar to any genuine users, and
therefore, memory-based algorithms tend to falsely associate
target items with all genuine users.

(3)

where, if R(u, i) 6= nil and R(v, i) 6= nil, then x0u (i) =
R(u, i) − R̄(u) and x0v (i) = R(v, i) − R̄(v), if otherwise, then
x0u (i) = 0 and x0v (i) = 0.

2.2

100

0.3

Similarity to centroid

Similarity to centroid

The appropriate selection of a similarity function, Sim(·, ·),
is important, because the function determines the k-NNs
(i.e., U ), as well as their weighting, to compute a weighted
sum of the k-NNs’ responses according to (1). A popular
similarity function calculates the cosine of the angles between row or column vectors of matrix R, after converting
the nil components of R to zeros [6].

hx0u , x0v i
p
,
hx0u , x0u i hx0v , x0v i

80

(b) High dimension.

0.4

Similarity Measure.

Sim(u, v) = P earson(u, v) = p

N10

Figure 1: N10 distribution. Objects with large N10
values emerge in high dimension: max N10 is 38 in
(a) but 133 in (b).

is the averaged response of user u to items. δ[·] is the indicator function taking on 1 if the proposition in the brackets
holds, or 0 otherwise.

hxu , xv i
p
,
Sim(u, v) = Cos(u, v) = p
hxu , xu i hxv , xv i

60

2.3

Average Attack

Memory-based CF determines recommendations based solely
on users’ responses (i.e., ratings) to items, and therefore, the
recommendations can be changed by manipulating the matrix R stored in the systems. One such (malicious) manipulation, on which we focus in the following, is called average
attack [3]. It injects fake users that pretend to prefer target
items, as well as imitating genuine users’ average behavAs opposed to cosine similarity, user vector x0u must be
recomputed for each user it is paired with. Moreover, the
number of nonzero elements of user vectors is determined
by the number of items that are given a (non-nil) response
by both paired users. For paired users giving responses to
a few items in common, the number of non zero elements
of vectors becomes small, and hence, the resulting Pearson
correlation becomes less trustworthy. Therefore, we employ
c
P earson(u, v), according to the
the shrunken variant NN
c +β
number of non zero elements of paired vectors, Nc , with a
positive valued parameter β [2].
2

Hubness

Hubness is a new aspect of the curse of dimensionality
[5]. Let D be a d-dimensional dataset and Nk (x) denote the
number of times an object x ∈ D occurs in the k-NNs of
other objects in D under some similarity measure. As the
dimension d increases, the shape of the distribution of Nk
changes to become right tail longer, or a small number of
objects takes large Nk values. Such objects are called hubs,
and the phenomenon is called hubness.3
We now review the emergence of hubness using synthetic
data, as reported in [5]. To simulate situations of CF, where
each user gives ratings to a few items and thus the rating
matrix R is sparse, we produce a sparse dataset. The dataset
consists of 2000 objects, each of which is a d-dimensional
vector. For each dimension i = 1, · · · , d, we draw a real
number from Lognormal(5; 1) distribution and compute its
rounded integer ni . We then choose ni objects out of the
3

Following [5], we evaluate hubness by the skewness of the
3
Nk distribution, defined as SNk = E (Nk − µNk )3 /σN
,
k
where E[ · ] is the expectation operator, and µNk and σNk are
the mean and the standard deviation of the Nk distribution,
respectively. A large SNk means the emergence of hubness.

816

2000 uniformly at random, and assign a random number
drawn uniformly from [0, 1] to their ith component. We
calculate the similarity between the objects using the cosine
similarity.
To illustrate the emergence of hubness, we compare the
distribution of N10 in two cases where the dimension is low
(d = 50) and high (d = 1000). Figure 1 shows that, in high
dimension, objects with large N10 values emerge, and as a
result, the distribution of N10 becomes skewed to the right.
We then compare a scatter plot of objects with respect to
the N10 value and similarity to the data centroid, shown in
Figure 2. We can see that a strong correlation arises in high
dimension, and this indicates that the origin of hubness is an
increased bias to the centroid caused by high dimensionality.

3.
3.1

Mean Absolute Error

0.76

4.

0.72

10

20

30

40

50

60

70

80

90

100

EXPERIMENT

We used the MovieLens 1M dataset (ml-1m)5 for evaluation. This dataset contains 1,000,209 ratings (i.e., integers from 1 to 5) from 6,040 users and 3,706 items, with
every user rating at least 20 items. We examined the algorithm (1), which was combined with two similarity measures: cosine similarity (Cos), and shrunken Pearson correlation (Pearson).6 The parameter β for Pearson was
set to β = 100, following [2]. To reduce hubness, we transformed the similarity matrix by converting it by (4) to obtain a commute time kernel (CT), or by (5) to center the
similarity matrix (Cent). The main goal of this experiment
is to compare the robustness of the system against attacks
before and after the transformations.

4.1

Prediction Accuracy without Attack

Before investigating the robustness against attacks, we
verified whether or not Cent and CT transformations affect the prediction accuracy when no attacks are present. To
simulate a recommendation task, we divided 1,000,209 ratings in the dataset into 939,809 training data (observed ratings) and 60,400 test data (for prediction).7 8 We evaluated
the prediction accuracy using mean absolute error (MAE),
a common metric usedP
for evaluating CF algorithms, calculated as MAE = 1/|T | (u,i)∈T |P red(u, i) − R(u, i)|, where
T is the set of user-item pairs given as test data (here,
|T | = 60400).
Figure 3 shows the MAE of the compared systems for various nearest neighbor parameter k. The figure indicates that
Cent almost always decreases MAE (i.e., improves prediction accuracy), and CT also decreases MAE for Pearson.
For the evaluation of robustness below, we set k = 50, since
these are the k values that achieved best MAE overall in the
above experiment.

Reducing Hubness by Eliminating a Bias
toward the Centroid

On the premise that specific objects become hubs because
they are particularly similar to the data centroid, hubness
can be reduced by transforming the similarity measures to
make all objects equally similar to the data centroid. Such
similarity measures can be obtained by computing a commute time kernel from a given similarity matrix [7], or more
simply, by centering the similarity matrix [8].
Let N be the number of objects and K denote a similarity
matrix of size N .4 A commute time kernel KCT is given as
(4)

4.2

where L = D − K is called a P
graph Laplacian and D is
a diagonal matrix with Dii =
j Kij . Next, let I be an
identity matrix and ~1 be an N -dimensional all-ones vector.
A centered similarity matrix is computed in the form
1
1 ~~ T
11 )K(I − ~1~1T ).
N
N

0.73

Figure 3: Prediction accuracy without attack using
several similarity measures for different k.

Relation between Attack and Hubness

KCent = (I −

0.74

k

REMEDIES AGAINST ATTACK

KCT = L+ (pseudo-inverse of L),

0.75

0.71

For memory-based CF, Nanopoulos et al. [4] and Knees
et al. [2] reported that hubness emerges because k-NNs are
computed in high dimensional spaces. Since the number of
users and items are usually large, the feature spaces used
for computing similarity, such as cosine and Pearson correlation, become high dimensional, and hence, hubness occurs.
Hub objects very often appear in the k-NNs of other objects,
and therefore, are responsible for determining many recommendations. However, hubs are not effectively similar to
many objects. In other words, hub objects frequently occur
in k-NNs only because they are similar to the data centroid
in higher dimensions. In fact, according to [2], the performance of recommender systems is affected by the existence
of hubs. In any case, because hubs are influential objects for
recommender systems, manipulating hubs from outside the
system can be effective for attacking the system.
Indeed, hubness renders memory-based recommender systems vulnerable to attacks. More precisely, fake users injected in the system during an average attack are more likely
to constitute hubs and affect the system. Therefore, we expect that, by reducing hubness, attacks can be thwarted.

3.2

Cosine (Original)
Cosine (Centering)
Cosine (Commute Time Kernel)
Pearson Correlation (Original)
Pearson Correlation (Centering)
Pearson Correlation (Commute Time Kernel)

5

Robustness against Attack

http://grouplens.org/datasets/movielens/
We also tested unshrunken versions of Pearson correlation,
but the results are not presented here because the prediction
accuracy was inferior to that of the shrunken variants.
7
We used the partition named ra.train and ra.test made
from the whole dataset ratings.dat in ml-1m.zip by running a script split_ratings.sh distributed in ml-10m.zip.
8
Test data is constructed by picking up 10 ratings randomly
for each of 6040 users.
6

(5)

4
When the algorithm (1) is used, N = Nuser and Sim(u, v)
in (2) or (3) gives the (u, v) component of K.

817

scatter plot of users for Pearson, where the horizontal axis
is N50 , and the vertical axis represents similarity to the data
centroid. The scatter plot indicates that a strong correlation
was observed, and hence, hubness occurred. In addition,
fake users injected by the average attack (displayed as red
points) had a higher similarity to the centroid, because they
were invented to imitate the average genuine user. Therefore, injected users became hubs that have large N50 values
(min 465 and max 961) as compared to most genuine users,
as seen from the N50 distribution in Figure 4(b). In contrast, hubness was reduced by Cent or CT transformations,
resulting in a decrease in the N50 values of injected users between min 101 and max 156 using Cent, as shown in Figure
4(c), and a decrease between min 0 and max 4 using CT,
as shown in Figure 4(d). It clearly shows that injected users
appeared less frequently in k-NNs of other users by using
Cent or CT than using original Pearson. Thus, fake users
became less influential in determining recommendations.
In total, applying transformations that reduce hubness,
we can obtain the system with more robust against attacks
as well as comparative or even better prediction accuracies
than using the original similarity measures.

Table 1: Prediction shift caused by average attack
and skewness of the Nk distribution (k = 50).
Similarity Measure

Transform

Skewness

PredShift

Cosine Similarity

(Original)
Cent
CT

3.422
1.629
1.526

0.902
0.319
0.205

Pearson Correlation

(Original)
Cent
CT

6.389
2.648
1.173

1.542
1.084
0.981

Pearson Correlation
(Original)

Pearson Correlation
(Original)

0.08
0.06
0.04

10 2
10 1

Injected users
Genuine users

0.02
0

Injected users
Genuine users

10 3

0.1

Frequency

Similarity to centroid

0.12

0

500

1000

1500

10 0

2000

0

500

N50

(b)

Pearson Correlation
(Centering)

Pearson Correlation
(Commute Time Kernel)

Injected users
Genuine users

10 1

200

400

N50

(c)

2000

600

5.

10 2
10 1
10 0

0

100

200

CONCLUSION

We have proposed a method for making memory-based
CF robust against shilling attacks, by reducing the hubness
that occurs in the user-response data. Our approach stands
on the ground that hubness is one of the primary factors
exploited by shilling attacks. We applied to the similarity
matrix two transformations that are known to reduce hubness: centering and conversion to a commute-time kernel.
Using the MovieLens dataset, we demonstrated that these
transformations make the recommender system less susceptible to shilling attacks without degrading its recommendation accuracy.

Injected users
Genuine users

10 3

Frequency

Frequency

10 2

0

1500

N50

(a)

10 3

10 0

1000

300

N50

(d)

Figure 4: Behavior of injected users (red points).

6.

REFERENCES

[1] R. Burke, B. Mobasher, R. Bhaumik, and C. Williams.
Segment-based injection attacks against collaborative
filtering recommender systems. In ICDM ’05, pages
577–580, 2005.
[2] P. Knees, D. Schnitzer, and A. Flexer. Improving
neighborhood-based collaborative filtering by reducing
hubness. In ICMR ’14, pages 161–168, 2014.
[3] S. K. Lam and J. Riedl. Shilling recommender systems
for fun and profit. In WWW ’04, pages 393–402, 2004.
[4] A. Nanopoulos, M. Radovanović, and M. Ivanović. How
does high dimensionality affect collaborative filtering?
In RecSys ’09, pages 293–296, 2009.
[5] M. Radovanović, A. Nanopoulos, and M. Ivanović. On
the existence of obstinate results in vector space
models. In SIGIR ’10, pages 186–193, 2010.
[6] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl.
Item-based collaborative filtering recommendation
algorithms. In WWW ’01, pages 285–295, 2001.
[7] I. Suzuki, K. Hara, M. Shimbo, Y. Matsumoto, and
M. Saerens. Investigating the effectiveness of
laplacian-based kernels in hub reduction. In AAAI ’12,
pages 1112–1118, 2012.
[8] I. Suzuki, K. Hara, M. Shimbo, M. Saerens, and
K. Fukumizu. Centering similarity measures to reduce
hubs. In EMNLP ’13, pages 613–623, 2013.

We selected 21 movies as target items for average attack,
such that the target movies are as close as possible to those
presented in Lam et al. [3, Table 1, p. 397], a pioneer work of
the average attack. For the setting of attack, we injected 100
fake users that were invented to introduce false ratings into
systems. To raise the reputation of the target items, fake
users assigned the highest rating (i.e., 5) to target items. For
items other than target ones, fake users gave average ratings.
That is, for each item, we generated a random number from
Normal(µ; σ), where µ equals the average rating of genuine
users who rated the item and σ = 1.0, and assigned the number after converting it to a nearest integer from 1 to 5. For
evaluation, we used a metric called prediction shift, which
measures the difference in the predicted rating before and
after the attack. We calculated the prediction shift for each
pair of genuine user and target item, except those belonging
to the training data. We report the average over all target
user-item pairs. Smaller the value of the shift, the better.
Table 1 shows the results of the prediction shift caused by
the attack. The amount of prediction shift is reduced after
the Cent or CT transformations are applied. This indicates
that using the transformed similarity measures makes the
recommendation system robust against the attack.
We now analyze why Cent and CT provided robustness
against the attack, in terms of hubness. Figure 4(a) shows a

818

