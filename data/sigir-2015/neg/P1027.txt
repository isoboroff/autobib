DINFRA: A One Stop Shop for Computing Multilingual
Semantic Relatedness
Siamak Barzegar* , Juliano Efson Sales* , Andre Freitas† , Siegfried Handschuh† ,
Brian Davis*
*

Insight Centre for Data Analytics, NUI Galway, Ireland

Firstame.Lastname@insight-centre.org
†

Department of Computer Science and Mathematics, University of Passau, Germany

Firstame.Lastname@uni-passau.de
ABSTRACT
This demonstration presents an infrastructure for computing
multilingual semantic relatedness and correlation for twelve
natural languages by using three distributional semantic models (DSMs). Our demonsrator - DInfra (Distributional Infrastructure) provides researchers and developers with a highly
useful platform for processing large-scale corpora and conducting experiments with distributional semantics. We integrate several multilingual DSMs in our webservice so end
user can obtain a result without worrying about the complexities involved in building DSMs. Our webservice allows
the users to have easy access to a wide range of comparisons
of DSMs with different parameters. In addition, users can
configure and access DSM parameters using a easy to use
API.

similarity using Spearman correlation for 12 natural languages2 . Our service can be tested online3 . It includes two
components: 1- Semantic Relatedness (Figure 1) that calculates the words similarity, 2- Correlation (Figure 2) that
calculates the spearman’s rank correlation.

2.

RELATED WORK

Categories and Subject Descriptors

Ferret [5] tested corpust-based approaches for measuring
semantic similarity. He also chose to use limited means because of deficit of linguistic tools are not, or at least freely
available, for all popular languages. Bullinaria et al. [3, 2]
have built semantic vectors from very small co-occurrence
windows, together with a cosine distance measure, stopwords, word stemming, and dimensionality reduction using
singular value decomposition to improve performance. The
BNC and (British National Corpus)4 and ukWaC5 corpus
were used In [2] and [3], respectively.

H.1.0 [Information Systems]: MODELS AND PRINCIPLES.

3.

Keywords
Distirbutional Infrastructure, Multilingual Semantic Relatedness, Distributional Semantic Models

1.

INTRODUCTION

Dinfra is an implementation of Explicit Semantic Analysis (ESA), Latent Semantic Analysis (LSA) and Random
Indexing based on the EasyESA [4] and S-Space [7] . It runs
as a JSON1 webservice, which allows users to submit queries
for similar terms in a multilingual fashion bases on a semantic relatedness measure which use Spearman’s correlation to
test relatedness scores.
The Dinfra webservice allows the user to obtain semantic
1

SYSTEM DEMONSTRATION

Three word similarity datasets WordSim353 (W353), the
Rubenstein & Goodenough (RG) (1965) and Miller & Charles
(MC) (1991) have been used in Dinfra. All these datasets
consist of human similarity ratings for word pairings. We
also consider Wikipedia6 corpus the years (2006, 2008, 2014)
and ukWaC [1] corpus from which to build the vectors.
In Dinfra, three DSMs were instantiated. Latent Semantic
Analysis (LSA) [9], Random Indexing (RI) [10] and Explicit
Semantic Analysis (ESA) [6]. The different combinations of
DSMs and corpora were evaluated for the computation of
semantic similarity and relatedness measures.
For the semantic relatedness component (Figure 1), four
parametrs such as main term, target set, language and similarity measure are used. The user can compare target words
to main word with three similarity measures in twelve different languages. For the example, we compared (Wife,
Child and love) with mother, also we used the Correlation 7
measure, Figure 1 shows the results that is returned by our

JSON - Java Script Object Notation

2
English, Portuguese, German, Spanish, French, Swedish,
Italian, Dutch, Chinese, Russian, Arabic and Persian
3
http://vmdgsit04.deri.ie:8008
4
http://www.natcorp.ox.ac.uk/
5
2 billion word corpus constructed from the Web limiting
the crawl to the .uk domain and using medium-frequency
words from the BNC
6
http://en.wikipedia.org/wiki/Wikipedia:Database download
7
A mean-adjusted version of Cosine as defined in [8]

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the Owner/Author(s). Copyright is held by the
owner/author(s).
SIGIR’15, August 09-13, 2015, Santiago, Chile.
ACM 978-1-4503-3621-5/15/08.
DOI: http://dx.doi.org/10.1145/2766462.2767870 .

1027

4.

ACKNOWLEDGMENTS

This publication has emanated from research conducted
with the financial support of Science Foundation Ireland
(SFI) under Grant Number SFI/12/RC/2289.
We would like in particular to thank Alexandros Poulis
and Juha Vilhunen from the Lionbridge Natural Language
Solutions ensuring the production word of high quality translations for our similarity datasets.

5.

Figure 1: Semantic Relatedness Component
webservice. The semantic relatedness measure is a real number within the [0,1] interval, representing the degree of semantic proximity between two terms. Semantic relatedness
can be used for semantic matching in the context of the development of semantic systems such as question answering,
text entailment, event matching and semantic search[4] and
also for entity/word sense disambiguation tasks.
The correlation component (Figure 2) calculates the Spearman’s rank correlation for the three similarity datasets, twelve
different languages and three similarity measures (Cosine,
Euclidean distance, Correlation)8 .

Figure 2: Correlation Component
All three datasets WS353, RG and MC were translated
and localised by native speakers for each of the target 11
languages. More importantly the localised datsets for each
language underwent a linguistic quality assurance by a well
know localisation company. Hence, we are confident that
our localised datasets per language are of high translated
quality.

8

See [8] page 3 for definitions of these similarity measures.

1028

REFERENCES

[1] M. Baroni, S. Bernardini, A. Ferraresi, and
E. Zanchetta. The wacky wide web: a collection of
very large linguistically processed web-crawled
corpora. Language resources and evaluation,
43(3):209–226, 2009.
[2] J. A. Bullinaria and J. P. Levy. Extracting semantic
representations from word co-occurrence statistics: A
computational study. Behavior research methods,
39(3):510–526, 2007.
[3] J. A. Bullinaria and J. P. Levy. Extracting semantic
representations from word co-occurrence statistics:
stop-lists, stemming, and svd. Behavior research
methods, 44(3):890–907, 2012.
[4] D. Carvalho, C. Callı, A. Freitas, and E. Curry.
Easyesa: A low-effort infrastructure for explicit
semantic analysis. In Proceedings of the 13th
International Semantic Web Conference (ISWC),
2014.
[5] O. Ferret. Testing semantic similarity measures for
extracting synonyms from a corpus. In LREC,
volume 10, pages 3338–3343, 2010.
[6] E. Gabrilovich and S. Markovitch. Computing
semantic relatedness using wikipedia-based explicit
semantic analysis. In IJCAI, volume 7, pages
1606–1611, 2007.
[7] D. Jurgens and K. Stevens. The s-space package: an
open source package for word space models. In
Proceedings of the ACL 2010 System Demonstrations,
pages 30–35. Association for Computational
Linguistics, 2010.
[8] D. Kiela and S. Clark. A systematic study of semantic
vector space model parameters. In Proceedings of the
2nd Workshop on Continuous Vector Space Models
and their Compositionality (CVSC) at EACL, pages
21–30, 2014.
[9] T. K. Landauer, P. W. Foltz, and D. Laham. An
introduction to latent semantic analysis. Discourse
processes, 25(2-3):259–284, 1998.
[10] M. Sahlgren. Vector-based semantic analysis:
Representing word meanings based on random labels.
In In ESSLI Workshop on Semantic Knowledge
Acquistion and Categorization. Citeseer, 2001.

