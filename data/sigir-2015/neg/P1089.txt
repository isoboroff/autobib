Practical Lessons for Gathering Quality Labels at Scale
Omar Alonso
Microsoft Corporation

omalonso@microsoft.com
Labeled corpora may be used to train and evaluate ranking
algorithms or may be used as reference datasets for implementing
and evaluating information retrieval techniques. Assigning a label
is usually considered a judgment task, performed by a human
(e.g., worker, expert, judge, annotator, rater, etc.), where the
judgments may be more or less subjective. A judgment on the
objective end of the spectrum may suggest a single right answer,
so we can rely on a single worker or a small number of workers to
accurately label an item. If the judgment is more subjective, we
probably need to ask more workers the question, and use an
aggregation method to determine the best candidate label.1

ABSTRACT
Information retrieval researchers and engineers use human
computation as a mechanism to produce labeled data sets for
product development, research and experimentation. To gather
useful results, a successful labeling task relies on many different
elements: clear instructions, user interface design, representative
high-quality datasets, appropriate inter-rater agreement metrics,
work quality checks, and channels for worker feedback.
Furthermore, designing and implementing tasks that produce and
use several thousands or millions of labels is different than
conducting small scale research investigations. In this paper we
present a perspective for collecting high quality labels with an
emphasis on practical problems and scalability. We focus on three
main topics: programming crowds, debugging tasks with low
agreement, and algorithms for quality control. We show examples
from an industrial setting.

In general, our approach for collecting labels not only looks at
designing work from the standpoint of results, but also explicitly
acknowledges the human aspects of human computation, and
identifies other issues that include worker motivation, fatigue,
subjective judgment, and making the most of worker
characteristics and diversity.

Categories and Subject Descriptors

In this paper, we assume a supervised or semi-supervised learning
setting. That is, there is a human involved in the production of
labels to bootstrap a learning mechanism or for evaluation
purposes. We also add two constraints from real-world systems:
scalability and continuity. By scalability, we mean gathering
several thousands or millions of labels depending on the
application. Rather than a one-off process for acquiring labels in a
single step, we see this as a continuous process that runs at
specific intervals (e.g., daily, weekly, etc.) as part of a bigger
workflow for training, modeling, or evaluation.

H.0 [Information Systems]: General

General Terms
Design, Experimentation, Human Factors

Keywords
Labeling; crowdsourcing; inter-rater agreement; debugging;
Captchas; worker reliability; experimental design

We argue that gathering high quality labels at scale is not a
straightforward activity. While it is desirable to use
crowdsourcing techniques, the process should not be outsourced.
Quite the contrary, teams that depend on these data need to have
tight control on the many factors that can influence the outcome
including specific crowdsourced data management techniques [6].

1. INTRODUCTION
Researchers and engineers use human computation via platforms
like CrowdFlower or Amazon Mechanical Turk as a mechanism
to produce labeled data sets. Several areas of information
management such as information retrieval, machine learning,
recommender systems, and natural language processing rely
heavily on such labels for product research and development.

We now describe three areas that impact the overall quality of
labels: wetware programing (or how to ask humans to perform a
task), debugging human computation tasks, and algorithms for
assessing work quality and worker performance.

Crowdsourcing has become increasingly important for curating,
labeling, and processing Web-scale datasets. Many members of
the information retrieval community have interest in
crowdsourcing and would like to incorporate it into their research
and practice. We share practical recommendations for
practitioners who are already working with crowdsourcing
platforms, and would like to step up their game.

2. WETWARE PROGRAMMING
We use the phrase “wetware programming” to describe a
computation that is designed by a human and performed by
another human in a crowdsourcing-like platform. The idea behind
human computation is to use humans as processors in a
distributed system for performing tasks that machines cannot do
well yet. Human computation is usually performed via formbased tasks or games (e.g., GWAP). When comparing to a
machine, the instruction set for humans is somewhat unknown,

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR '15, August 9-13, 2015, Santiago, Chile.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3621-5/15/08…$15.00
DOI: http://dx.doi.org/10.1145/2766462.2776778

1089

making it more difficult for an engineer to design and implement
solutions based on human computation.

2.3 Microtasks and workflows
Finally, it is very likely that each task or microtask will be part of a
larger workflow so, overall, there is an upfront cost when
instrumenting human computation as part of large systems. This
step may look overwhelming at first but there is great benefit if
implemented correctly. Data quality and experimental designs are
preconditions to make sure we get the right kind of labels before we
move further down the modeling pipeline. These labels will be used
for rankers, machine learning models, evaluations, etc.

2.1 Design considerations
We outline a number of issues that engineers and developers face
when writing software that uses human computation.






Asking questions. All tasks require at some point a
question. Is the document relevant for the query? Does
the image contain adult content? Are both articles the
same? Is the following entity the same as this Facebook
page? Asking questions is part art and part science.
Instructions are very important but, at the same time,
workers will not read nor remember several pages of
details. What we recommended is to be precise and to
the point on the question that is asked and what is
expected as a valid answer. Showing examples of what
is acceptable and not is always desirable. If possible
involve a technical writer or someone with an English
major to write the instructions.

Survey designers and researchers that produce annotated corpora
have a good process in place to achieve good results. Engineers use
a development process (e.g., agile, waterfall, etc.) to produce
software. However, when dealing with a human instruction set, the
process is a bit unclear. A number of issues with the adoption of
human computation and crowdsourcing in organizations are due to a
lack of development process and unrealistic expectations.
Crowdsourcing works but requires attention and care. In other
words, the work is outsourced to the crowd but not the process.

User interface and content presentation. All the
necessary information to perform the task should be
self-contained: what is the task, what needs to be done,
examples, tips, links, etc. A worker who is engaged
with a task and content would produce better labels.
The goal is to help the worker perform the task the best
way possible. Having the ability for workers to leave
feedback is also a good strategy for collection
suggestions and improvements for the task.

3.1 Development process
It is good practice to have a development process in place that
allows fast iteration and experimentation. We do not claim that ours
is unique but it has been tested in production with good results. It
consists on the following three phases.

Processing speed. As expected, in general, humans are
much slower than machines for computing. There is
also variation among humans. Tasks that are not
completed in time or with a high abandonment rate can
be an indication of assignment difficulty or poor
incentives.



Cost and incentives. Regardless if the humans are
experts or part of a crowdsourced pool, the right cost
and incentive structure can help retain and attract good
workers.



Task difficulty. All human tasks involve a degree of
difficulty. Even if we think that the request is very
simple, it may have an important cognitive load so it is
always desirable to create, if possible, the simplest unit
of work that can be performed.



3. REAL-WORLD PROBRLEMS

Errors. Labeling tasks are very repetitive and we all
humans are error prone. Collecting multiple answers
does help to eliminate those errors due to poor attention
by workers using consensus strategies.

1.

Prototype development. We start with a small data set and
use it with our very own development team for testing the
designs and gathering labels. If the team members do not
understand the design or if the inter-rater agreement is
low, we need to go back to the drawing board and redesign the task.

2.

Early stage production. This step uses the same design
and same data set as in the previous step but this time
using a different crowd: hired editors (if they are
available) or crowdsourced workers. The goal is to see if
other workers can perform the same task and produce
similar results. In other words, do workers agree with our
own internal team? If the task design is still unclear and
the agreement is low, we probably need to debug the task
using the framework presented in subsection 3.4.

3.

Continuous production. Now that we tested the design a
number of times and the results are good, the next step is
to enforce quality control, increase the size of the data set,
and perform this step continuously.

3.2 Labels for evaluation
Relevance evaluation is one of the most widely used applications of
crowdsourcing. The task is very simple: given a query and a web
page, assess its relevance in a binary or graded relevance scale. The
labels collected are used to measure the performance of a particular
algorithm and for building test collections.

2.2 Agreement and consensus
We need to be able to trust the data collected so far. If two or more
workers agree on a judgment, the higher the chances that the label is
correct. A good practice is to use inter-rater agreement statistics for
measuring the agreement among workers. An inter-rater statistic
produces a value between -1 and 1, where 1 is perfect agreement, 0
is due to chance, and -1 means total lack of agreement. The most
common statistics used in practice are Cohen’s kappa (2 raters),
Fleiss’ kappa (n raters), and Krippendorff’s alpha (n raters with
missing values). It is strongly recommended to always use a good
inter-rater static and avoid things like percentage agreement.

In practice, however, not all evaluation tasks are so simple. A
common mistake is trying to do too much on a single task. That is,
asking several questions while minimizing payment instead of a
more practical alternative of decomposing a task into smaller ones.
At the end, we want a single label but it does not mean that such
label needs to be produced in a single task in one pass. In the
context of machine translation, a very nice overview of task design
and data acquisition is presented in [4]. In a different example, for
evaluating the performance of a near-duplicate algorithm, the task

1090

The framework presented in Figure 2 consists of a “data-workertask” pattern where the aim is to dive into each of the three
contingent factors with the end goal of debugging (and fixing) each
potential problem.

Sample
document
pairs

Documents
labeled as
News

Build list of
document
identifiers

Build list of
valid pairs

News
?

N

Phase 2

Phase 1

was partitioned in two: one for detecting if a document is a news
article and a second one to compare if two news documents are
duplicates or near-duplicates. Each phase was conducted in two
different platforms (MTurk and Microsoft’s UHRS) with different
payment schemes [3]. Figure 1 summarizes the workflow.

Same
story
?

Y

1.

Data. We first start with the data (the work to be done)
with the aim of reducing the subjectivity of the data and
narrowing the data genre. Many factors can cause
conscious or unconsciously bias on the workers.
Reducing the range on these factors can make the final
results more accurate.

2.

Workers. We then look at the performance of the workers.
How can we detect poor quality work in the absence of a
gold set to check workers’ performance or high inter-rater
agreement to identify normative answers? A Captchaslike technique called HIDDEN (Human Intelligence DataDriven Enquiries) is useful for solving this problem.
Instead of asking a completely orthogonal question like
the traditional Captchas, the questions were designed to
focus the workers' attentions on the aspects that they are
about to judge. Figure 3 shows an example of HIDDENs.
In order to complete the task, workers were asked to read
the content three different times, attending to different
aspects of the post. In other words, workers had to reflect
on the tweet in multiple ways.

3.

Task. Many subjective concepts are controversial because
there are many different interpretations. By segmenting
the factors that lead to the subjective decision, it is
possible to narrow down the decision space. In our
experience, we found that most of the problems occur on
ill-defined questions that produce inconsistent answers.

N

Y

Figure 1. Two-phase evaluation for near-duplicate detection.
Phase 1 uses Mechanical Turk while phase 2 uses UHRS. Both
phases use different payment schemes and expertise.

3.3 Labeling and low agreement
In practice, a common approach is to start from the feature
engineering or modeling phase and reverse the type of labels that
are needed. For example, say that a given classifier model needs
three types of labels (“yes”, “no”, “not sure”). One solution is to
create a task so humans can provide those exact three labels for a
class. A problem with this strategy is that the kind of labels that are
needed for the machine (the model) do not necessary represent the
same labels for humans. In other words, we should use labels that
humans can perform best at and then map them into what input is
required for the machine. This may include, for example, using a
binary scale for a machine but a 5-point grade for humans.

Figure 3 shows an example of a production task using HIDDENs to
label tweets in a classification task. We can think of Q1 as the
algorithmic HIDDEN whereas Q2 is more semantic. Q3 is the
question that we use to collect labels. By maintaining high interrater agreement on the HIDDENs we can iterate and debug a
potentially problematic question, in our case Q3.

There are cases where a task will produce results with low interrater agreement in many runs. The typical reaction by the engineer
or developer is usually adversarial: identify the workers that are
spammers or low performers and replace them. While this may be
the case, the first thing that is needed is to debug the task. How do
we know if the task is working? Debugging a task that uses a human
instruction set has a number of problems and it is usually a difficult
thing in practice.

3.4 Debugging framework
We now describe a framework for debugging subjective judgment
tasks and for improving label quality before the crowdsourcing task
is run at scale. The framework alternately varies characteristics of
the work, assesses the reliability of the workers, and aims to
improve task design by disaggregating the labels into components
that may be less subjective to the workers, thereby potentially
improving inter-rater agreement [2].

Figure 3. HIDDEN structure. The first question is completely
objective and computable; the second is partially objective; and
the third could be much more subjective.

4. ALGORITHMS AND QUALITY
CONTROL
While there is quite of bit of research on quality control from an
adversarial perspective (i.e., workers are spammers), in practice
quality has to be applied to both requesters and workers. Workers
may be spamming or not performing, but, at the same time, our own

Figure 2. Debugging framework.

1091

How many workers to ask in a given task is also a fair question to
pose. For example, for the query {facebook} and the web page
{www.facebook.com}, one worker should be enough to produce a
good judgment. However for the query {mars expedition} and the
web page {www.mars-one.com} is not clear how many workers are
needed. Developing and implementing stopping rules that know
when to stop asking a worker for a new label is very important.

design may be so bad that is impossible for honest and experience
workers to produce anything useful.
How do we measure work quality? If we can produce honey pots
(known answers for questions), we can remove workers who are not
passing them. Other options are to compare the performance of a
worker to other workers in the same task or to use a tiered approach,
where certain workers are more experts. An example of a tiered
approach is the “find-fix-verify” pattern: one set of workers find and
fix a problem and a separate set of workers verify that the correction
is good. But more importantly, when and how should we enforce the
quality control?

5. CONCLUSION
Often perceived as boring and time consuming process, gathering
high quality labels requires full attention so it is key to own the
entire process end-to-end. These labels will be used for training set
creation, modeling, and evaluation. There is no need to rush things.
Repeatable label quality at scale works but requires a solid
framework. The end goal is to get the labels right first and then
move to modeling.

4.1 Quality checks
Quality control is an on-going activity, in particular in industrial
environments that require tasks to run continuously. Quality control
check points should be introduced as follows:
1.

Before the task is on production. This can be done by
using a qualification test or similar mechanism to filter,
screen, select, train, and recruit workers.

2.

During task execution time. We assess answers as worker
produces them. This can be implemented by introducing
honey pots as random checks.

3.

After task has completed. Once the experiment is done,
we compute the usual accuracy metrics and remove bad
performers.

In this paper we presented the areas that need attention that should
be useful for practitioners who are considering crowdsourcing at
scale but are a bit unsure about how to implement and focus
resources. We described a development process that is currently
used in-house to test human computation tasks. We also introduced
a reliability framework that shows the three aspects that need
attention: workers, data and task design. We also outline a number
of algorithmic solutions that can be used for managing quality.

A new proposed approach for enhancing worker performance is by
providing micro breaks [5]. As a lot of tasks are repetitive and, in
certain cases, probably boring, providing workers with a break can
potentially improve overall work quality.

Finally, lots of different skills and expertise are required for
implementing successful tasks that can produce high quality labels.
For example, social and behavioral science, human factors,
algorithms, economics, distributed systems, and statistics. It is
important to know our own limitations and be ready to collaborate
with other teams.

4.2 Algorithmic solutions

6. REFERENCES

As noted before, a good technique is to compare a label to a
predefined answer. This is very cheap to implement but assumes
that such gold set or ground truth exists or that is somewhat
affordable to produce down the road. A second problem is to
produce good honey pots that are not so easy to identify by workers.
Qualification tests have also some disadvantages. There is an extra
cost involved for designing and maintaining such tests. This may
also turn off workers and hurt completion time.

[1] I. Abraham, O. Alonso, V. Kandylas, R. Patel, S. Shelford, and
A. Slivkins. “How Many Workers to Ask? Adaptive
Exploration for Collecting High Quality Labels”.
http://arxiv.org/abs/1411.0149
[2] O. Alonso, C., Marshall, and M. Najork. “Debugging a
Crowdsourced Task with Low Inter-rater Agreement”.
Proceedings of JCDL, 2015.

Most of the algorithms for managing work quality are based on
majority vote, EM-based and maximum likelihood. Majority vote is
straightforward to implement and tends to produce good results.
However, there are cases when it is not possible to get majority so
easily. The algorithm “get another label” decides how many labels
to use on a given task based on the distribution of all previously
encountered tasks [7]. Vox Populi uses the aggregate label as an
approximate ground truth and eliminates the workers that provide
incorrect answers [8].

[3] O. Alonso, D. Fetterly, and M. Manasse. “Duplicate News
Story Detection Revisited”. Proceedings of AIRS, 2013.

4.3 Explore-exploit and adaptive strategies

[6] A. Marcus and A. Parameswaran. Crowdsourced Data
Management. Foundations and Trends in Databases, 2015.

[4] C. Callison-Burch. “Fast, Cheap, and Creative: Evaluating
Translation Quality using Amazon’s Mechanical Turk”.
Proceedings of EMNLP, 2009.
[5] P. Dai, J. Rzeszotarski, P. Paritosh, and E. Chi. “And Now for
Something Completely Different: Improving Crowdsourcing
Workflows with Micro-Diversions”. Proceedings of CSCW,
2015.

Very recently there are new directions that use adaptive approaches
for managing crowds [1]. In explore-exploit techniques there is a
quality-cost tradeoff. That is, maximizing the quality of the labels
gathered while minimizing the overall cost per task. The idea of
exploration is to try out alternatives to gather information.
Exploitation is about choosing alternatives that perform well based
on information already collected.

[7] V. Sheng, F. Provost, P. Ipeirotis. “Get Another Label?
Improving Data Quality Using Multiple, Noisy Labelers”.
Proceeding of KDD, 2008.
[8] O. Dekel and O. Shamir. “Vox Populi: Collecting HighQuality Labels from a Crowd”. Proceedings of COLT, 2009.

1092

