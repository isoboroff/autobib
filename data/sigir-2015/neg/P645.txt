Scientific Information Understanding via Open Educational
Resources (OER)
Xiaozhong Liu

Zhuoren Jiang

Liangcai Gao

School of Informatics and
Computing
Indiana University
Bloomington
Bloomington, IN, USA, 47405

College of Transportation
Management
Dalian Maritime University
Dalian, China, 116026

Institute of Computer Science
and Technology
Peking University
Beijing, China, 100871

jzr_dlmu@hotmail.com

liu237@indiana.edu

glc@pku.edu.cn

ABSTRACT

Keywords

Scientific publication retrieval/recommendation has been investigated in the past decade. However, to the best of our knowledge,
few efforts have been made to help junior scholars and graduate
students to understand and consume the essence of those scientific readings. This paper proposes a novel learning/reading environment, OER-based Collaborative PDF Reader (OCPR), that
incorporates innovative scaffolding methods that can: 1. autocharacterize student emerging information need while reading a
paper; and 2. enable students to readily access open educational
resources (OER) based on their information need. By using metasearch methods, we pre-indexed 1,112,718 OERs, including presentation videos, slides, algorithm source code, or Wikipedia pages,
for 41,378 STEM publications. Based on the computational information need, we use text mining and heterogeneous graph mining
algorithms to recommend high quality OERs to help students better understand the scientific content in the paper. Evaluation results
and exit surveys for an information retrieval course show that the
OCPR system alone with the recommended OERs can effectively
assist graduate students better understand the complex STEM publications. For instance, 78.42% of participants believe the OCPR
system and recommended OERs can provide precise and useful
information they need, while 78.43% of them believe the recommended OERs are close to exactly what they need when reading
the paper. From OER ranking viewpoint, MRR, MAP and NDCG
results prove that learning to rank and cold start solutions can efficiently integrate different text and graph ranking features.

Scaffolding; Information Need Characterization; Education; Heterogeneous Graph Mining; Evaluation; Information Understanding

1.

INTRODUCTION AND MOTIVATION

“I don’t think I need those books... I cannot understand them
anyway...” This was a student’s response when I suggested that
she should find some statistics methods books in the university library. Actually, her response is quite representative when students
face STEM (Science, Technology, Engineering, and Mathematics)
readings. In the last decade, while the volume of STEM publications has increased dramatically in university and digital libraries,
few efforts have been made to help students/junior scholars understand them. From an educational viewpoint, understanding the
content of scientific publications in STEM course environments remains daunting [?], i.e., Information Access 6= Information Understanding. For instance, a number of search/recommendation
algorithms ensure Google Scholar and Microsoft Academic give
users access to the high quality scientific readings, but junior scholars/graduate students may not necessarily understand them. In a
recent study of STEM [?], PhD students (in computer science domain) claimed that the complex models, formulas, and methods in
the readings were deemed too difficult to understand because of the
students’ limited knowledge in computer science and mathematics.
Students need innovative scaffolding methods/systems to better assist in understanding scientific publications1 .
While the content of scientific publications is challenging and
difficult for students, a large number of open educational resources
(OERs) are increasingly available and have great potential to assist students better understand the essence of these publications.
For example, scholars, organizations and institutions commonly
share conference slides, video lectures, tutorials, Wikipedia pages,
datasets and source codes generated from research. As these OERs
are often seamlessly blended or integrated with other scholarly publications or research topics, students can, we surmise, better understand the papers by leveraging those OERs. In this paper, we
define it as OER-based Scaffolding. In a study by Liu [?], for example, students expect that a blend of OER-based learning objects
could significantly improve learning. Unfortunately, not all OERs
exist in a publication as cited references; some may be scattered
across different social media (e.g., TED, SlideShare, authorStream,
GitHub, SourceForge, YouTube, Google Code, VideoLecture, and

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval

General Terms
System, Algorithms, Experimentation, Measurement
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
SIGIR’15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767750.

1
In education, scaffolding refers to a variety of instructional techniques used to move students progressively toward stronger understanding and, ultimately, greater independence in the learning process. http://edglossary.org/scaffolding/

645

2.

Wikipedia). While the cost of manually locating those OERs for
large-scale STEM topics or publications is prohibitive, as another
challenge, the existing system cannot use those innovative OERs
to address students’ emerging information needs, i.e., using or recommending OER(s) to address students’ questions on a particular
section/paragraph/sentence/formula of a scientific publication.
Motivated by these observations, we propose a new solution and
an innovative system framework to help students and scholars better understand and learn from scientific publications in a course
environment by leveraging OERs, i.e., OER-based scaffolding. A
new learning system, called the OER-based Collaborative PDF
Reader (OCPR), is to be implemented as a first step towards this
effort. Theoretically speaking, the system is also designed to accommodate the needs of students in MOOCs (massive open online
courses) in addition to students from varied backgrounds. As Figure ?? shows, the system can auto-characterize student’s emerging
information need while reading a paper, while recommending high
quality OERs to help them understand the publication.
With the exception of using text mining for OER recommendation (i.e., using highlighted text to find relevant OERs), OCPR
also employs heterogeneous graph mining to enhance scaffolding
performance by investigating the weighted relations between OER
pair; OER and paper; OER-topic association; paper-topic association; and the topic-topic relationships, which help to prioritize important OERs and filter out the noisy resources. The meta-path and
supervised random-walk-based graph algorithms [?] will be used
to explore the important OERs (vertices) given student’s implicit
and explicit information needs (topic and paper seed vertices). For
example, we can assume that when a student is reading a paper, she
can be interested in finding OERs relevant to the paper, specific sentences in the paper, the paper’s topics, or weekly teaching topics.
From an information need perspective, for link-based recommendations, student information need is characterized by some seed
vertices on the heterogeneous networks, i.e. the paper or topics
that the student is currently reading, and supervised random walk
algorithms can be used to recommend candidate OERs.
Evaluation result shows that, first, auto-recommended OERs can
provide important and useful information to help students better understand the content of the scientific publications, i.e., MRR (mean
reciprocal rank) score is 0.8609 (students find the useful OERs in
the 1st or 2nd position in the result ranking list). This assumption
is also confirmed in the exit survey, where 78.42% of participants
believe the OCPR system along with recommended OERs can provide precise and useful information they need, while 78.43% of
them believe the recommended OERs are close to exactly what they
need. From system perspective, 80.39% of participates believe the
OCPR system and real-time OER recommendation function is easy
to use. Second, based on the students’ judgments, we found all the
OER recommendation can be potentially useful, and the learning
to rank method can be quite useful to enhance the recommendation
performance. As Table ?? shows, learning to rank and cold start
methods significantly (p < 0.0001) outperform other methods.
The contribution of this paper is fourfold. First, we propose a
novel auto-scaffolding method to help students better understand
the scientific readings in a course environment by leveraging various kinds of OERs. Second, a new system, OCPR, is employed to
capture student reading behavior while enabling them to ask questions or highlight the confusing part of the readings. Third, novel
algorithms are proposed to characterize students’ information need
and recommend OERs to them while reading a paper. Last but not
least, an experiment (with 51 participates) is employed to validate
the usefulness of the system and to evaluate the OER recommendation algorithms.

RELATED WORK

Scaffolding is defined as a “process that enables a child or novice
to solve a problem, carry out a task, or achieve a goal which would
be beyond his unassisted efforts” [?]. Since 1976, the term scaffolding has been widely used in educational research [?], and the
scaffolding construct is being applied more broadly [?]. In particular, the concept of scaffolding is applied to the studies of computerassisted learning environments, i.e. computer-mediated scaffolding [?]. Naturally, the focus of scaffolding research is on system
design and usage. One of the most recent efforts is to utilize existing social tagging and annotation tools. Social tagging and annotation have produced positive results in a number of tasks, including
promoting student learning [?, ?, ?]. For example, students were
found to have positive perceptions of usefulness, ease of use, learning satisfaction, and willingness for future use towards the Personalized Annotation Management Software (PAMS) 2.0 system [?].
In Johnson’s et al. study [?], students achieved better reading comprehension and metacognitive skills when using a social annotation
approach within the context of small team collaboration. Such tools
have the advantage of allowing users to highlight learning materials in context, comment on part of a document, share ideas, and
provide feedback among a group of users.
However, prior studies also found those scaffolding approaches,
by leveraging social tagging or annotation, can be quite limited [?],
and students cannot essentially benefit from such systems when
reading a challenging text. Not until recently did researchers begin
to focus on the usefulness of OER. For instance, Dennis, et al. [?]
found that an additional link to video presentation had significantly
positive impacts on students’ learning based on quiz scores. More
recently, Liu [?, ?] found OERs, like YouTube video, dataset, and
Wikipedia page, can assist students better understand the scientific
readings. Meanwhile, he found information retrieval methods, i.e.,
language model and BM25, can help to build OER index for a large
number of publications automatically. However, this research cannot address students’ emerging information need while reading a
paper. For instance, existing method cannot help students to understand a specific sentence or paragraph. Meanwhile, existing methods generate a large number of noisy OERs [?]. For instance, Liu
found a large number of YouTube videos do not have high quality
text descriptions, and methods based on text similarity may be not
accurate.
For this study, we use text retrieval and heterogeneous graph
mining plus random walk [?] to recommend OERs while filtering
our the noisy data. Similar approaches have been used to recommend YouTube video. For instance, Baluja et al., [?] used uservideo graph plus random walk to provide personalized video suggestions for users. A heterogeneous network is defined as a directed graph G = (V, E) with an object type mapping function
τ : V → A and a link type mapping function φ : E → R, where
each object v ∈ V belongs to one particular object type τ (v) ∈ A,
each link e ∈ E belongs to a particular relation φ(e) ∈ R, and if
two links belong to the same relation type, the two links share the
same starting object type as well as the ending object type. The
network schema, denoted as TG = (A, R), is a meta template for
the heterogeneous network G = (V, E) with the object type mapping τ : V → A and the link mapping φ : E → R, which is a
directed graph defined over object types A, with edges as relations
from R [?].
The concept of meta-path was first proposed in [?], which can
systematically capture the semantic relationships between objects
in a heterogeneous information network scenario. A meta-path P is
a path defined on the graph of network schema TG = (A, R), and

646

Figure 1: OER-based Collaborative PDF Reader System.
R

R

R

1
2
l
is denoted in the form of Ȧ1 −→
Ȧ2 −→
. . . −→
Ȧl+1 , which
defines a composite relation R = R1 ◦ R2 ◦ . . . ◦ Rl between types
Ȧ1 and Ȧl+1 , where ◦ denotes the composition operator on relations. Different meta-path-based mining tasks are studied, including similarity search [?], relationship prediction [?, ?], user-guided
clustering [?], and recommendation [?, ?, ?]. It turns out that metapath serves as a very critical feature extraction tool for most of the
mining tasks in a heterogeneous information network. In this paper,
we also employed the supervised random walk methods which have
been investigated in [?,?] and prior studies show that the supervised
random walk can significantly enhance the classical unsupervised
graph mining methods.

3.

very large potential publication collection, we needed to send multiple queries for each publication. However, most search engines,
i.e., Google and YouTube, restrict the number of automated visits,
which makes this method inefficient.
Table 1: Meta-search for OER collection generation.

RESEARCH METHODS

In this section we discuss the research method in detail which
includes: to collect different kinds of candidate OERs for scientific
publications (??), to design OCPR system (??), and to design the
text- and graph-based OER recommendation algorithms (??).

3.1

OER Collection Indexation via Meta-Search

In order to recommend high-quality OERs to help students better
understand scientific readings, we need to index different kinds of
OERs. In this study, we collect four kinds of OERs, presentation
video, slides, source code, and Wikipedia pages. For OER collection, the most straightforward method is to search for the paper title (exact match) in different search engines. However, this method
has two major limitations. First, given a relatively long publication title, if we use exact string match, search engines can hardly
find any results. On average, based on our experiment with some
random sampled publications, only 0.35 resources were retrieved
(for each testing publication). We also found that the quality of
resources was not good. A large portion consisted of publisher or
digital library access pages for the target publication, which are
not helpful for scholars and students to understand the paper. Second, the computational cost of this method is quite high. Given the

647

Resource type
Wikipedia page
Slides

Search engines
Wikipedia dump (local database)
Google, Slideshare

Video
Source code

YouTube, TED, Videolecture
GitHub, SourceForge

Query
[Keyword]
Google: [Keyword] AND
(filetype:ppt OR slides);
Slideshare: [Keyword]
[Keyword]
[Keyword]

In this research, we used a more economical method to cope
with this problem. We assumed each publication was composed of
a list of topics with each topic represented by an author-assigned
keyword. For each resource type, we first aggregated all resources
retrieved using the publication keywords (topics) and then used a
ranking algorithm to identify the most important resources based
on publication content. By using this method, we can significantly
reduce the amount of queries sent to different search engines. For
instance, take ACM DL corpus as an example, for 41,378 publications, we only need to collect OERs for 9,263 popular author
assigned keywords (appear at least 5 times in the corpus). Such
resources are highly likely to be relevant to the publication content
and could help students or scholars understand the essence of the
publication. For this method, we assumed that helping scholars or
students understand the topics (keywords) of the paper will eventually help them to understand the paper itself.
In order to effectively generate OERs for each topic, we use
meta-search approaches where a Boolean query was sent to one
or more search engines for each scientific keyword and OER type.
The detailed query list for each resource type is listed in the Table ??. The first column is the OER type. The target query for
each type was sent to one or more search engines or web services.

For example, in order to get the slides OERs for a topic (keyword), the query “[Keyword] AND (filetype:ppt OR slides)” (column 3) was sent to Google (column 2), where [Keyword] was replaced with the target keyword content. Similarly, for video OERs,
the query “[Keyword]” was sent to YouTube, TED, and Videolecture. For slides and source code, different queries were sent to
different search engines. For performance reasons, we indexed the
Wikipedia page dump (2014 May) locally.
In this experiment, we used the top 15 retrieved results from each
search engine to aggregate the final result collection for each resource category. Each retrieved result is a triple of title, URL, and
snippet, and we use the result URL to download the HTML page
content. We then used a list of pre-defined rules (regular expressions) to identify the key content on the HTML page. For example,
for each YouTube or Videolecture video, we extract the video description and tags to represent the content of the video. However, if
the pre-defined rules don’t work for a specific page, we just use the
HTML (after removing HTML tags and script code) as the content
of the target resource.
In most cases, the result collection was a combination of informative resources and noisy results. Experience in information retrieval reveals that since different search engines return very diverse
and erratic results for the same or similar query, irrelevant data may
pollute the search results and mislead users. To address this problem, we used an information retrieval ranking algorithm, language
model2 [?, ?], to prioritize those informative resources for the target scientific keywords while filtering out the irrelevant information. In other words, we want to use ranking fusion to find out the
most important resources from multiple ranking lists. Because the
OERs will be used to help scholars and students better understand
the essence of the readings, most retrieval resources (low ranked),
at this stage, will be filtered out. We also use the paper abstract
as query to search the OER content to find the candidate OERs
for a publication. So, for each keyword Ki , we will index top m
OERs with similarity score S(Ki , Rj ) = PLM (Ki |Rj ), where
PLM (Ki |Rj ) is the language model ranking score for keyword
probability given a OER, Rj , content. Similarly, we also index
the publication-OER similarity score, PLM (Pi |Rj ) (Pi is a paper,
represented by title and abstract content). So, for topic, Ki , we
index triples {Ki , Rj , PLM (Ki |Rj )}, and for paper, Pi , we index
triples {Pi , Rj , PLM (Pi |Rj )}.
The performance of this method is highly dependent on the quality of the OER text description. However, different kinds of OERs
have varying text quality. Wikipedia pages (extracted from Wikipedia
dump) have very high text quality, but a large number of YouTube/
Videolecture videos and Google/Slideshare slides do not associate
with high quality text description. Consequently, a text search
may generate some noisy OER data which may pollute the recommendation results. In order to solve this problem, we will use
heterogeneous graph mining method to recommend OERs to students (introduced in Section 3.3). The connections between OERs,
then, can be important. For instance, Wikipedia pages are interlinked via incoming/outgoing links, and each YouTube, Slideshare,
Videolecture, TED, GitHub, and SourceForge OER has “related”
OERs. The related resources are provided by the target online service. While the algorithm behind that is a black box, most related
resources come from 1. resource content similarity, 2. the ownership of these resources (i.e., if two resources uploaded by the same
user), and 3. user co-visitation (or co-watched) probability [?].

2
In this paper, we use language model with Dirichlet smoothing for
all the text match tasks.

648

We index the relationships between OERs in a graph database, i.e.,
r
R1 → R2 .
Note that, unlike other meta-search problems, the ranking lists
of some OER types in this research were totally disjoint. For example, for videos, we sent queries to YouTube, TED, and Videolecture. Their indexes are almost disjoint (i.e. TED videos may
not exist in YouTube video libraries), and some sophisticated metasearch ranking fusion algorithms are not appreciate, e.g., Borda’s
method [?] and Markov chain methods [?].

3.2

OER-based Collaborative PDF Reader

To validate the new scaffolding hypothesis and help students better understand the scientific publications in a course environment,
we design and implement the novel learning and reading system,
OER-based Collaborative PDF Reader (OCPR). The new system
has three main functionalities:
• Capture evidence and characterize students’ emerging implicit/explicit information needs when reading a scientific paper. For instance, students can ask a specific question given
a piece of text, which serves as evidence of an explicit information need, or highlight part of a text in the paper, as
evidence of an implicit information need. In either case, the
OCPR is able to capture the student reading behavior and the
reading context from the target PDF document.
• Automatically recommend high quality OERs, such as videos,
slides, source code, or Wikipedia pages, to resolve students’
information needs, while helping them understand a publication more effectively.
• Other students (classmates) can find their colleagues’ interaction in the PDF system and actively contribute to this autolearning process by recommending, changing, annotating,
or removing certain resources given an information need.
They can also provide additional comments and suggestions
to help others.
The first two functions focus on OER-based scaffolding, and the
third one mainly addresses user incentive. For instance, students,
when reading the paper, can find her colleagues’ interactions with
the system (by clicking the question marks on the side of PDF reading), i.e., the questions or highlights with the target paper. Meanwhile, students could discuss for an existing interaction by using
OCPR. We will investigate the third function in the future study,
and in this study, we focus on the first two.
At the backend, the Collaborative PDF Reader has access to the
OER index (created in last section), the list of assigned class readings, and the algorithm package, which guarantees efficient OER
recommendations and different scaffolding methods.
OCPR is designed for course usage, and it can manage class
readings and student information.By using the OCPR, the class instructor can easily manage the class reading list by uploading the
class syllabus (in a pre-defined format). In this syllabus, the instructor needs to identify the weekly reading schedule and weekly
teaching topic; he or she can then upload the readings for each week
to the server. For instance, the following is an exemplar OCPR syllabus snippet, which is corresponding a one week teaching schedule. The exemplar snippet has two topics (for teaching), and students will need to read two papers for this week. Based on the paper
IDs, the system can locate the readings (in PDF format) from the
database. Each reading in the database has title, abstract, full content, associated topics, citation information, and associated OERs
(described in the last section).

light content of the paper; 2) students’ information may be related
to the target paper and paper’s associated topics; 3) in a course
environment, student’s information need may address the weekly
teaching schedule (in the syllabus). Because, even a single paper
can cover multiple topics, the weekly topics from the course syllabus can be important. Each assumption will be represented by a
number of OER ranking features, and the input space χ includes
1) user question or highlight text, 2) paper and paper’s topic, and
3) weekly topic. While we can hardly estimate the importance of
each feature, in this paper, we use learning to rank to weight each
of those, ΦL2R . The training data come from the OCPR collected
OER relevance judgments. However, we also need to address the
cold start problem. When training data is sparse, we assume information need is a linear function, Φcold , and the online features
(assumption 1) should be more important than other offline ones.
From a performance viewpoint, the first assumption is based on
online text ranking features. Since it cannot host any complex algorithms, we used text information retrieval algorithm to cope with
this problem. The second and third assumptions are offline, and we
can pre-index a number of OERs closely related to paper, paper’s
topic and syllabus topics with higher computational cost. We use
heterogeneous graph mining to implement it.
Figure ?? illustrates the graphical OER recommendation problem, where the OER recommendation is conceptualized as a random walk problem on a heterogeneous graph. Four kinds of vertices, paper, topic, weekly topic and OER, are interconnected by
using different kinds of edges. More detailed information is depicted in Table ??.
The links and paths between the vertices can be important to
enhance the OER recommendation performance. As section ??
shows, we use meta-search and crawler to index the OER by using keyword search. However, this method generates noisy data
which may pollute the recommendation results. For instance, given
the keyword (topic), “Question Answering”, the system may find
a Wikipedia page or YouTube video “Question the Answers” (content relevant), which is a pop song. Simply using content match,
this noisy resource may pollute the OER recommendation result.
s
However, from a random walk viewpoint, via meta-path K ∗ →
r
?
?
R ← R , the OER, R , useful probability equals the random walk
probability from the seed (query) topic K ∗ to the candidate OER
R? (via all the related interim OERs). Given the earlier example, even though “Question the Answers” is related to the keyword
with text match, but, all its related resources are about the related
music and artists (not related to the topic), and, then, the random
walk probability from the seed topic to this noisy OER will be very
low. Comparing with that, the correct Wikipedia page’s interlinked
pages are more likely to be relevant to the target topic or paper,
and the random walk probability (following this meta-path) can
be high. Similarly, because a large number of YouTube videos or
Slideshare slides do not associate with high quality textual descriptions, we cannot totally trust the video/slide-topic or video/slidepaper similarly, and the relationship between videos/slides can be
important. For instance, if two YouTube videos’ co-watched probability is high, they can be related [?], and this information may
not totally depend on the video text description. For this experiment, we collected video, slide or source code relationships by using YouTube, Videolecture, GitHub or Slideshare API. We find, in
most cases, useful OER’s related OERs can be also relevant to the
target topic/paper, and noisy video’s related ones are not likely to
be relevant to the target keyword. Random walk can be important
to filter those noisy OERs.
Explained it in another way, from the random walk perspective,
user information needs are represented by different kinds of seed

<week_5>
<topic> Information Extraction </topic>
<topic> Maximum Entropy Markov Model </topic>
<reading_ID> 5323544 </reading_ID>
<reading_ID> 2742755 </reading_ID>
</week_5>

Additionally, the instructor can also upload the student roster to
the system, which will automatically register each student while
sending student email invitation to download the OCPR reading
system. By logging into the reading system, students can access
the class weekly readings and use the new scaffolding functions.
The main function of OCPR is OER based scaffolding, which
captures evidence of students’ emerging implicit or explicit information needs when reading a scientific paper and recommends high
quality OERs to address their information needs. As Figure 1 shows,
by using OCPR, students could ask a specific question given a piece
of text, which serves as evidence of an explicit information need,
or highlight part of a text in the paper, as evidence of an implicit
information need. In either case, the OCPR is able to capture the
selected or input question and its reading context from a student in
the PDF document. The evidence of students’ information needs
will then be sent to the backend recommendation and ranking algorithms for information need characterization. For instance, as
Figure 1 shows, user highlighted text is focusing on “unsupervised
topic modeling for citation analysis”, and the paper content mainly
addresses “dynamic topic modeling and PageRank-based citation
recommendation”. The recommended Wikipedia pages (on the
right) are focusing on those topics.
The algorithms presented in the next section recommend the optimized OERs given the information need, which will be able to
help students better understand the essence of the targeted reading.
A student can also select the specific type of OERs in the “right
click menu”, you may want use "context menu" instead, i.e., Mac
is ctrl + click that he or she prefers and the system will capture
and log the selected OERs. Meanwhile, students can also provide
relevance and usefulness feedback for system recommended OERs.
For instance, as Figure 1 shows, students can click “Good”, “OK”,
“Bad” or “Not Sure” for each recommended OER given their information need. The judgments and student click information will
be saved as system logs, which will be important for OER recommendation algorithms, i.e., training learning to rank model, and
algorithm evaluation.
Unlike the common practice of accessing PDF documents locally on one’s personal computer, OCPR gives students and their
instructor cloud access on the server, i.e., everyone works on the
same document. All users’ asynchronous interactions with the PDFs
will be saved in a central database along with their user IDs, and
the interactions - i.e., PDF highlighting, proposing questions, recommending OERs, OER judgments to an existing information need
- will be useful to characterize computational user information need
and to train OER recommendation model.

3.3

OER Recommendation and Ranking

As aforementioned, student’s emerging information needs while
reading a paper can be very complex. For instance, when a student
highlights a paragraph in a paper, it’s challenging to understand the
reason and motivation. We employ multiple recommendation features to characterize students’ information need, Φ(f1 (χ), f2 (χ)...
fk (χ)), where each feature, fi , is a OER ranking function given
input space χ. We propose three OER ranking assumptions: 1),
straightforwardly, student’ information needs can be related to the
OCPR interactions while reading, i.e., explicit questions and high-

649

vertices (as input space χ), i.e., the paper/topic/weekly syllabus
topic that student is reading with, on the graph, and different kinds
of meta-paths can navigate the seeds vertices to the target OER vertices, instead of the noisy OER vertices. On the navigation tour, text
or topical relevance is characterized as the transitioning probability
between vertices, while the physical links, i.e., OER relationship
and citation links, can be important to calculate the random walk
probabilities.

connect keyword pair via co-occur and citation probability. Weekly
topic, W , is a special kind of topic, and all the readings from that
a
week connected to the target topic(s), P → W .
cont
The weight of K → P is the normalized topic authority score,
which characterize the importance (or contribution) of each paper.
cont
Note that topicj is contributed by paperi (Kj → Pi ) doesn’t necr
essarily mean paperi is relevant to topicj (Pi → Kj ). For example, some “natural language processing” papers can contribute to,
but not related to, “information retrieval” topic, which is similar to
the OER links, i.e., Wikipedia page “natural language processing”
is linked to “information retrieval” and “question answering”. As
a result, the contribution link may be able to help to find the correct OER. In order to estimate the contribution of each paper to a
topic, we calculated the paper importance given a topic K by using a PageRank with Prior algorithm [?]. The normalized topical
cont
PageRank authority score is used for the weights of K → P . For
this step, we used homogeneous graphs where, on each graph, the
vertex is a paper and each vertex is also characterized by a topic
prior vector, i.e., on the paper graph, the paper topical prior distribution is P (ZKi |paper). The result of PageRank (with prior) is
the paper topic authority vector, Authority(paperi |Zkj ) (weight

Figure 2: Heterogeneous graph for OER recommendation

cont

of Kj → Pi ). This link can help students to investigate the
background or foundation of a topic. For instance, the feature
cont
s
r
K ∗ → P → R ← R? explores the publications make essential
contribution to topic K ∗ , and the OERs related to those publications can be helpful to assist students understand the target topic.
All the meta-path based OER ranking features are listed in Table ??. Each feature start from a kind of seed vertex(s) ∈ χ, and
the R? is the candidate OER. The first 4 features are online or semionline ranking, which is based on the student input highlight text
or question content, also ∈ χ. For performance reason, we cannot
use LLDA to infer the topic of input text. Instead, greedy match is
employed to extract the keyword information from the input string,
m
m
i.e., H → K or Q → K. For example, if “music information
retrieval” existed in the user question, we won’t use the keyword
“information retrieval” as the interim topic K in the meta-path.
We use meta-path plus random walk from the seed vertices to
candidate OER, the random walk probability can be estimated by:
P
(1)
(l+1)
r(vi , vj
) = t=v(1) v(l+1) RW (t)

Table 2: Vertices and edges in the constructed heterogeneous graph
Vertex
R
P
K
W
∗
H
∗
Q
Edge
h

P →K
a
P →W
s
P →R
cite

K → K
co
K→K
cont

K → P
s
K→R
co
W →W
s
W →R
r
R→R
s
∗
H →R
m
∗
H →K
s
∗
Q→R
m
∗
Q→K

Description
Open Education Resource
Paper
Keyword
Weekly Topic (from Syllabus)
User highlight text
User additional question
Description
Paper is related to keyword (using LLDA)
Paper is assigned to weekly topic
Paper-resource relationship based on similarity

i

Keyword cites keyword (probability)
Keyword-keyword co-occurrence (probability)

j

(l+1)
vj

(1)
vi

following the meta-path,
where t is a tour from
to
and RW (t) is the simulated random walk probability of the tour t.
(1)
(2)
(l+1)
Suppose t = (vi1 , vi2 , . . . , vil+1 ), the random walk probability
is then,
Y
(j)
(j+1)
RW (t) =
w(vij , vi,j+1 )

Keyword is contributed by paper (PageRank with prior)
Keyword-resource relationship based on similarity
Weekly topic-weekly topic co-occurrence
Weekly topic-resource relationship based on similarity
OER is related to OER (collected from service sites)
Highlight text-OER content similarity
Highlight text related to keyword (greedy match algorithm)
User question-OER content similarity
User question related to keyword (greedy match algorithm)

j
(j)
(j+1)
w(vij , vi,j+1 )

(j)

(j+1)

where
is the weight of edge vij → vi,j+1 .
s
For example, given meta-path K ∗ → R? , the relevance score
s
∗
?
from Kk to Rr can be calculated by w(Kk ∗ → Rr ? ), where
∗ s
?
w(Kk → Rr ) is the weight of edge which is the similarity of
Kk between Rr ? .
The total ranking score from a set of starting vertices to a candidate OER vertex can be defined as:
X
(l+1)
(1)
(l+1)
r(Q, vj
)=
r(vi , vj
)

* Dynamically created by user when reading a paper
For any vertex on the graph, the sum of the same type of outh
going links equals 1. For instance, The weight of Pi → Kj is the
normalized LLDA (Labeled LDA) [?] probability of topic Kj given
the content of Pi , P (ZKj |Pi ), and Kj is the keyword provided by
the author of the paper Pi , and the topic probability is generated
by the paper title plus abstract content. All the similarity edges are
calculated by using language model with Dirichlet smoothing [?].
co
cite
Similarly, we also generate the edges K → K and K → K to

(1)

vi

∈Q

We can also consider two or multiple parallel meta-paths leading
R

R

1
2
to the same type of query nodes, for example, Ȧ1 −→
Ȧ2 −→

650

R0

R0

R0

R

l
l
2
1
(A0t+1 ), where
. . . −→
(A02 ) −→
. . . −→
Ȧl+1 and (A01 ) −→
0
Al+1 = At+1 . In this case, we can define similarity from different
sets of objects to a result node from different meta-paths:

s(Q1 ∪ Q2 , r) = sM P1 (Q1 , r)α · sM P2 (Q2 , r)1−α
cont

s

r

s

For instance, K ∗ → P → R ← R? ← K ∗ is a combined recont
s
stricted meta-path, which combines two meta-paths: K ∗ → P →
r
s
R ← R? and K ∗ → R? . So, based on the above formula, the ranking score of candidate OER, R? , is the combination of two random
walk scores for both sub-meta-paths. Theoretically, we need to tune
parameter α for each meta-path to optimize the weight of each submeta-path. For this study, because of the sparseness of the training
data, we set α = 0.5. More sophisticated parameter tuning will be
addressed in future work.
Based on text mining methods and meta-path based methods, we
proposed 21 ranking features for OER recommendation. All the
features investigated in this study are listed in table ??. Please note
that, we doesn’t consider the edge direction, as long as the two vertices are connected, the meta-path can be created. There are three
types of features: online feature, semi-online feature, offline feature. Online features can only be calculated online, offline features
are pre-calculated, while semi-online features combine online calculation and offline calculation results.
After we calculate all the ranking features, we need to integrate
them as a OER recommendation mode, Φ(f1 (χ), f2 (χ)...fk (χ)).
In this study, we use two methods. First, without user judgments,
we generate Φcold (f1 (χ), f2 (χ)...fk (χ)) based on a low-cost linear function (cold start). We assume the online or semi-online features (No. 1-4), based on user OCPR interactions, are more important then offline features. So, the online features’ weights are 3
times higher than the offline features.
When OER usefulness judgments are available (collected by OCPR
system), we use learning to rank to statistically combine different ranking features, ΦL2R (f1 (χ), f2 (χ)...fk (χ)), while avoiding
manual parameter tuning. As this study is not focusing on learning
to rank, we used a relative simple algorithm, Coordinate Ascent [?],
which iteratively optimizes a multivariate objective ranking function, for OER ranking feature integration and algorithm evaluation.

4.

character length for a question is 42.25 (with highlight character
length 64.08). Based on students’ feedback, averagely, they use the
system about 3 hours per week when reading the paper.
At the backend of OCPR, we created a heterogeneous graph
for OER recommendation. As aforementioned, there are 4 kinds
of vertices and 10 kinds of relations in this heterogeneous graph,
for paper vertices, we used 41,370 publications from 1,553 venues
(mainly from the ACM digital library). The paper vertices are connected to 9,263 keyword labeled topics. For this course, based on
course syllabus, there are a total of 34 weekly topics. By using
meta-search, we collected a total of 1,112,718 OERs. More detailed information can be found in Table ??. Note that even though
we only used a small number of readings in this experiment, the
keyword co-occur, keyword-paper contribution, and citation relationships on the large graph will help to enhance the OER recommendation performance. Theoretical, user could use OCPR to read
any paper in this collection.
Table 4: Vertices and edges in the heterogeneous graph
Vertex
Type
Paper
Keyword
Weekly Topic
Open Education Resource(wiki)
Open Education Resource(sourcecode)
Open Education Resource(slides)
Open Education Resource(video)
Open Education Resource(total)
Edge
Type
paper has keywords
paper is assigned to weekly topic (for 8 required readings)
paper-wiki relationship based on similarity
paper-slides relationship based on similarity
paper-sourcecode relationship based on similarity
paper-video relationship based on similarity
paper-OER relationship based on similarity (total)
keyword cites keyword
keyword-keyword co-occurrence
keyword is contributed by paper
keyword-wiki relationship based on similarity
keyword-slides relationship based on similarity
keyword-sourcecode relationship based on similarity
keyword-video relationship based on similarity
keyword-OER relationship based on similarity (total)
weekly topic-weekly topic co-occurrence
weekly topic-OER relationship based on similarity
OER is related to OER

EXPERIMENT

In this section, we describe the experimental setting, results and
exit survey.

4.1

Count
41,378
9,263
34
128,772
45,775
151,616
786,555
1,112,718
Count
587,252
34
8,068,451
8,039,382
7,973,611
8,112,065
32,193,509
206,522
1,250,910
3,577,111
2,196,341
2,684,119
557,426
2,094,405
7,532,291
114
68,000
1,963,866

Dataset
4.2

We tested this reading system and the associated OER ranking
algorithms in a real learning environment. A graduate-level information retrieval course at Indiana University is used for this experiment. 51 students (masters and PhDs) voluntarily participated
this experiment (33 male and 18 female), and they were required
to use the OCPR system for 8 weeks (with 8 required readings) 3 .
They could use OCPR functions to get access to the system recommended OERs. Meanwhile, we asked each participant to provide OER relevance judgments for the top 5 system-recommended
OERs (they can rate more). There are totally valid 5,132 judgments
we collected (for 1,051 student requests). We use those judgments
to train the learning to rank model and to evaluate the algorithm performance. For all the requests, there are 77 explicit questions from
students (each question is also associated with a piece of highlight
text). On average, each paper got 9.36 questions, and the average

Experiment results

For all the OER judgments, participants rated 21.63% of the recommended OERs as “Good”, 29.36% as “OK”, 40.10% as “Bad”
and 8.9% as “Not Sure”. For this experiment, we use the cold start
solution to recommend OERs to students. From an NDCG viewpoint, we score Good = 2, OK = 1, and Bad = 0. The OER
recommendation performance can be found in Table ??. As the
OER recommendation is more like a QA problem, and students are
more interested to find the first useful resource, we use MRR as
the indicator to train the learning to rank model. For evaluation,
10-folder cross-validation was used.
The ranking feature 3, 14, and 17 are the most important features (highest feature weights in Coordinate Ascent model). Ranking feature 14 and ranking feature 17 were offline feature. Feature
cite
s
r
s
14 (K ∗ → K → R ← R? ← K ∗ ) carried the keyword citation, keyword-OER similarity, keyword-OER related and OERcont
s
OER related information. Similarly, feature 17 (K ∗ → P →

3
The dataset can be download in the project website
http://scholarwiki.indiana.edu/OCPR

651

Table 3: Graphic representation for all the ranking features
No.
1
2

Ranking feature
s
H ∗ → R?
s
∗ m
H → K → R?

3
4

Q∗ → R?
m
s
Q∗ → K → R?

5
6
7
8
9

P ∗ → R?
s
K ∗ → R?
s
W ∗ → R?
r
∗ s
P → R ← R?
s
r
s
P ∗ → R ← R? ← P ∗

10

K ∗ → R ← R?

Description(hypothesis)
The OER can be useful if OER content is similar as the user highlight text (language model)
The OER can be useful if OER content is similar as the keywords (topics) extracted from the user highlight
content (greedy match keyword extraction)
The OER can be useful if OER content is similar as the question content from user (language model)
The OER can be useful if OER content is similar as the keywords (topics) extracted from the user query
content (greedy match keyword extraction)
The candidate OER should related to the target paper
The candidate OER should related to the target keywords (topics)
The candidate OER should related to the weekly topics (extracted from syllabus)
The candidate OER should related to the paper’s related OER
1. The candidate OER should related to the paper’s related OER; 2. the candidate OER should related to
the paper
The candidate OER should related to the important keyword’s related OER

s

s

s

r

∗ cite

s

r

11
12

K → K → R ← R?
co
s
r
K ∗ → K → R ← R?

13

K ∗ → R ← R? ← K ∗

14

K ∗ → K → R ← R? ← K ∗

15

K ∗ → K → R ← R? ← K ∗

16

K ∗ → P → R ← R?

s

r

s

cite

co

K

17

The OER can be useful is it is related to the OER related to the keyword cited by the important keyword
The OER can be useful is it is related to the OER related to the keyword co-occurred with the important
keyword
1. The candidate OER should related to the important keyword’s related OER; 2. The candidate OER
should related to the important keyword

s

r

s

s

r

s

cont

s

r

∗ cont

s

r

The OER can be useful is it is related to the OER related to the paper contributes the important keyword
s

→ P → R ← R? ← K ∗

∗

s

r

?

18
19

W →R←R
s
r
s
W ∗ → R ← R? ← W ∗

20

W ∗ → W → R ← R?

21

W ∗ → W → R ← R? ← W ∗

co

s

r

co

s

r

online feature

1.The OER can be useful is it is related to the OER related to the keyword cited by the important keyword;
2.the candidate OER should related to the important keyword
1.The OER can be useful is it is related to the OER related to the keyword co-occurred with the important
keyword; 2. the candidate OER should related to the important keyword

s

semi-online feature

1. The OER can be useful is it is related to the OER related to the paper contributes the important keyword;
2. the candidate OER should related to the important keyword
The candidate OER should related to the weekly topic’s related OER
1.The candidate OER should related to the corresponding weekly topic’s related OER; 2.the candidate OER
should related to the corresponding weekly topic
The OER can be useful is it is related to the OER related to the weekly topic co-occurred with the corresponding weekly topic
1.The OER can be useful is it is related to the OER related to the weekly topic co-occurred with the
corresponding weekly topic; 2. the candidate OER should related to the corresponding weekly topic

offline feature

Table 5: Measures of different OER ranking algorithms (significant test: 1. cold start vs. other features; 2. L2R vs. cold start; * p < 0.01, **
p < 0.001, *** p < 0.0001)
Ranking
Feature1
Feature2
Feature3
Feature 4
Feature 5
Feature 6
Feature 7
Feature 8
Feature 9
Feature 10
Feature 11
Feature 12
Feature 13
Feature 14
Feature 15
Feature 16
Feature 17
Feature 18
Feature 19
Feature 20
Feature 21
Cold Start
L2R

r

MAP@3
0.6375
0.5317
0.3014
0.2970
0.4586
0.6436
0.6398
0.5556
0.5827
0.6196
0.376
0.3778
0.5926
0.3577
0.3591
0.3495
0.3556
0.6089
0.608
0.5937
0.5997
0.7114***
0.7409***

MAP@5
0.8146
0.7222
0.4894
0.4860
0.6372
0.8188
0.8123
0.732
0.7545
0.7926
0.5625
0.5651
0.7682
0.5461
0.5474
0.5376
0.545
0.7787
0.7772
0.765
0.7694
0.8856***
0.9090***

MAP@all
0.8222
0.7306
0.4974
0.494
0.6452
0.8258
0.8188
0.7382
0.7615
0.7994
0.5705
0.5732
0.7739
0.5542
0.5555
0.5459
0.5531
0.7848
0.7834
0.7714
0.7754
0.8946***
0.9161**

NDCG@3
0.7504
0.6430
0.4287
0.4233
0.5756
0.7446
0.7421
0.6602
0.6913
0.7144
0.4942
0.4967
0.6940
0.4801
0.4809
0.4748
0.4768
0.7069
0.7153
0.6956
0.7045
0.8079***
0.8426***

NDCG@5
0.8093
0.7321
0.5448
0.5415
0.6599
0.8045
0.8013
0.7358
0.7601
0.7814
0.6041
0.6067
0.7648
0.5916
0.5923
0.5861
0.5903
0.7708
0.7764
0.7628
0.7694
0.8559***
0.8789***

s

R ← R? ← K ∗ ) carries keyword-paper contribution, paper-OER
similarity, keyword-OER similarity and OER-OER related information, in which, the keyword-paper contribution information is
also calculated from citation information and key-paper related information (via LLDA). Question-OER (feature 3) is an online fea-

NDCG@all
0.8140
0.7379
0.5507
0.5474
0.6650
0.8076
0.8042
0.7386
0.7633
0.7839
0.6093
0.6120
0.7663
0.5969
0.5977
0.5917
0.5956
0.7733
0.7791
0.7653
0.7717
0.8613***
0.8811***

P@1
0.7412
0.5765
0.0682
0.0624
0.4012
0.7635
0.7624
0.6329
0.6847
0.7318
0.2459
0.2518
0.6871
0.2118
0.2141
0.2071
0.2071
0.7047
0.7212
0.6894
0.7047
0.8600***
0.8871*

P@3
0.6000
0.4890
0.1286
0.1239
0.3871
0.6322
0.6086
0.4980
0.5196
0.5965
0.2294
0.2322
0.5604
0.2035
0.2059
0.1933
0.2024
0.5729
0.5533
0.5475
0.5388
0.7208***
0.7373***

MRR(OK)
0.8257
0.6992
0.2885
0.2838
0.5524
0.8393
0.8315
0.7384
0.7775
0.8227
0.4307
0.4355
0.7871
0.4013
0.4027
0.3986
0.3976
0.8010
0.8038
0.7866
0.7907
0.9140***
0.9329*

MRR(Good)
0.7872
0.6249
0.2868
0.2768
0.4707
0.7085
0.732
0.5857
0.6427
0.6493
0.3819
0.3881
0.6307
0.3535
0.3543
0.354
0.3479
0.6414
0.6729
0.6379
0.6625
0.8179
0.8609***

ture, and this indicates the question content from the student can
improve the recommendation performance. From a performance
viewpoint, evaluation results show that, first, auto-recommended
OERs can provide important and useful information to help students better understand the content of the scientific publications,

652

i.e., both learning to rank and cold start methods’ MRR (Good)
score is higher than 0.8 (which means students are finding the useful OERs in the 1st or 2nd position in the result ranking list). In this
table, MRR (OK) investigated the multiplicative inverse of the rank
of the first OER user rated "OK", and MRR (Good) used first user
rated “Good” OER in the ranking list. This finding is also confirmed in the exit survey where 78.42% of participants believe the
OCPR system along with recommended OERs can provide precise
and useful information.
Second, based on the student judgments, we found that a number
of OER recommendation features, including online and offline, can
be potentially useful, and the learning to rank method can be useful
to enhance the recommendation performance comparing with cold
start model (p < 0.01). When training instances are sparse or not
available, manual parameter setup (cold start) can be a effective solution. Cold start significantly outperforms other ranking features
(p < 0.0001), except for MRR(Good) (p = 0.014). From a ranking perspective, both MAP and NDCG proved that the cold start
solution and learning to rank can be efficient for OER recommendation. Third, we find that different kinds of seed vertices can be
useful. Weekly topic, W ∗ , is especially useful. For instance, features 18-21 performed well in this experiment, which means class
teaching schedule can be important in helping the system better
understand student information needs. The evaluation results prove
that cold start solution and learning to rank significantly outperform
the classic text-based or graph-based approachs.
From performance viewpoint, as the online or semi-online features are implemented via language model and greedy match and
the offline features are pre-computed, the system performance is
acceptable. In most cases, after students highlight text or ask a
question, system can give students OER results within 5 seconds.

4.3

graph mining algorithms can potentially contribute to the OER recommendation. Meanwhile, different seed vertices on the heterogeneous graph, i.e., paper, topic, weekly topic, highlights, and questions, are important to characterize student information need. Learning to rank is efficient to integrate different OER ranking features,
when training data is available. However, when training data is
sparse or unavailable, human generated ranking rules can be useful
to cope with the cold start problem. We also find heterogeneous
graph mining can be useful to solve this proposed problem. When
OER text quality is not high (e.g. some YouTube videos have low
quality descriptions), relations between the OERs become very important to filter out the noisy resources. For instance, we find the
relations between OERs and keyword relations are useful to enhance the OER recommendation performance which is difficult for
classical text mining methods.
From statistical analysis of student survey responses, we found
only a handful of them like to propose an explicit question when using the OCPR system: only 77 questions out of the 1,051 requests
are found to be explicit, although proposing explicit questions can
help them locate the useful OERs. Instead, most students prefer to
highlight some text and ask for system help. Evaluation shows that
highlighted text provides important information to find the correct
OER (features 1 and 2). However, noisy OER can pollute the quality of the recommendation result. In this case, graph mining methods help to enhance the OER ranking accuracy as well as filtering
out the noisy OERs. For instance, features 13, 18, 20, and 21 based
on paper topic or weekly topic achieve good recommendation and
ranking performance.
One major limitation of this study is the lack of personalization.
Intuitively, different students may have different kinds of information needs even though they pose the same question or select the
same part of a text in a paper. For example, given the same piece
of text in a target paper that students want to understand, some students may prefer to watch a video, while others may want to read a
Wikipedia page or access the source code of the scaffolding. In order to tailor the recommendation results for each student, we need
to generate a computational profile for each of them. However, we
will also need to address the challenge of data spareness problem
for personalization model training.
We also hope to have a chance to use the user comment function to enhance the OER ranking and scaffolding performance in
the future. Supported by OCPR, other students (classmates) and instructors can actively contribute to the auto-learning process by recommending, changing, annotating, or removing certain resources
given an information need. Users can easily click the question
marks on the side of PDF reading for viewing or commenting on
their colleagues’ interactions with the system, which provide important incentives for using the OER-scaffolding functions. In the
future, we will enhance the OER recommendation by using this
function and motivate students participation.
Finally, this system can serve other purposes in addition to student learning. For instance, by using OCPR, a system can help
journal readers better understand the scientific content within the
articles. Different experiments and ranking features need to be designed to achieve this goal.

Exit Survey

From a usability perspective, we used an exit survey to investigate the usefulness of the new reading environment and the effectiveness of the new OER-based scaffolding method. In the survey,
we asked 13 questions for each participate. More detailed information can be found in the project website 4 . Based on students’ feedback, 78.42% of participants believe the OCPR system along with
recommended OERs can provide precise and useful information,
while 78.43% believe the recommended OERs are close to what
they need when reading the target paper. From a system perspective, 80.39% of participates believe the OCPR (prototype) system,
i.e., highlight and ask question function, is easy to use.
Meanwhile, based on students’ feedback, we found Wikipedia
page and video presentation (or video lecture) are more helpful
for them to understand the publication, compared with slides and
source code. In this experiment, very few students actually read
the source code to understand the paper content. Overall, 49.02%
of students found the system can effectively help them better understand the target publication, while 33.33% found the system and
OERs somewhat helpful. Only 1.96% of the participants described
the recommended OERs as useless.

5.

ANALYSIS AND CONCLUSION

In this study, we propose a novel task to assist students to better
understand scientific publications by leveraging high-quality OERs.
By using the OCPR system, a number of reading behaviors are captured to characterize students’ information needs. Text and graph
mining algorithms are used as ranking features to recommend highquality OERs. Experiment results show that both text mining and
4

6.

REFERENCES

[1] Javed A Aslam and Mark Montague. Models for metasearch.
In Proceedings of the 24th annual international ACM SIGIR
conference on Research and development in information
retrieval, pages 276–284. ACM, 2001.
[2] Shumeet Baluja, Rohan Seth, D Sivakumar, Yushi Jing, Jay
Yagnik, Shankar Kumar, Deepak Ravichandran, and

Project URL is http://scholarwiki.indiana.edu/OCPR

653

[3]

[4]

[5]

[6]

[7]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

Mohamed Aly. Video suggestion and discovery for youtube:
taking random walks through the view graph. In Proceedings
of the 17th international conference on World Wide Web,
pages 895–904. ACM, 2008.
James Davidson, Benjamin Liebald, Junning Liu, Palash
Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He,
Mike Lambert, Blake Livingston, et al. The youtube video
recommendation system. In Proceedings of the fourth ACM
conference on Recommender systems, pages 293–296. ACM,
2010.
Alan R Dennis, Kelly O McNamara, Stacy Morrone, and
Joshua Plaskoff. Improving learning with etextbooks. In
Proceedings of the 48th Hawaii International Conference on
System Sciences, pages 5253–5259, 2015.
Cynthia Dwork, Ravi Kumar, Moni Naor, and Dandapani
Sivakumar. Rank aggregation methods for the web. In
Proceedings of the 10th international conference on World
Wide Web, pages 613–622. ACM, 2001.
Tristan E Johnson, Thomas N Archibald, and Gershon
Tenenbaum. Individual and team annotation effects on
students’ reading comprehension, critical thinking, and
meta-cognitive skills. Computers in human behavior,
26(6):1496–1507, 2010.
Ni Lao and William W Cohen. Relational retrieval using a
combination of path-constrained random walks. Machine
learning, 81(1):53–67, 2010.
Xiaozhong Liu. Generating metadata for cyberlearning
resources through information retrieval and meta-search.
Journal of the American Society for Information Science and
Technology, 64(4):771–786, 2013.
Xiaozhong Liu and Han Jia. Answering academic questions
for education by recommending cyberlearning resources.
Journal of the American Society for Information Science and
Technology, 64(8):1707–1722, 2013.
Xiaozhong Liu and Jian Qin. An interactive metadata model
for structural, descriptive, and referential representation of
scholarly output. Journal of the Association for Information
Science and Technology, 65(5):964–983, 2014.
Xiaozhong Liu, Yingying Yu, Chun Guo, and Yizhou Sun.
Meta-path-based ranking with pseudo relevance feedback on
heterogeneous graph for citation recommendation. In
Proceedings of the 23rd ACM International Conference on
Conference on Information and Knowledge Management,
pages 121–130. ACM, 2014.
Donald Metzler and W Bruce Croft. Linear feature-based
models for information retrieval. Information Retrieval,
10(3):257–274, 2007.
Elena Novak, Rim Razzouk, and Tristan E Johnson. The
educational use of social annotation tools in higher
education: A literature review. The Internet and Higher
Education, 15(1):39–49, 2012.
Roy D Pea. The social and technological dimensions of
scaffolding and related theoretical concepts for learning,
education, and human activity. The journal of the learning
sciences, 13(3):423–451, 2004.
Jay M Ponte and W Bruce Croft. A language modeling
approach to information retrieval. In Proceedings of the 21st
annual international ACM SIGIR conference on Research
and development in information retrieval, pages 275–281.
ACM, 1998.
Sadhana Puntambekar and Roland Hubscher. Tools for
scaffolding students in a complex learning environment:

[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

[27]

[28]

654

What have we gained and what have we missed?
Educational psychologist, 40(1):1–12, 2005.
Daniel Ramage, David Hall, Ramesh Nallapati, and
Christopher D Manning. Labeled lda: A supervised topic
model for credit attribution in multi-labeled corpora. In
Proceedings of the 2009 Conference on Empirical Methods
in Natural Language Processing: Volume 1-Volume 1, pages
248–256. Association for Computational Linguistics, 2009.
Addison Su, Stephen JH Yang, Wu-Yuin Hwang, and Jia
Zhang. A web 2.0-based collaborative annotation system for
enhancing knowledge sharing in collaborative learning
environments. Computers &amp; Education, 55(2):752–766,
2010.
Y. Sun, R. Barber, M. Gupta, C. Aggarwal, and J. Han.
Co-author relationship prediction in heterogeneous
bibliographic networks. In Proc. 2011 Int. Conf. Advances in
Social Network Analysis and Mining (ASONAM’11),
Kaohsiung, Taiwan, 2011.
Y. Sun, J. Han, C. C. Aggarwal, and N. Chawla. When will it
happen? relationship prediction in heterogeneous
information networks. In Proc. 2012 ACM Int. Conf. on Web
Search and Data Mining (WSDM’12), Seattle, WA, 2012.
Y. Sun, J. Han, X. Yan, P. S. Yu, and T. Wu. PathSim: Meta
path-based top-k similarity search in heterogeneous
information networks. In Proc. 2011 Int. Conf. Very Large
Data Bases (VLDB’11), Seattle, WA, 2011.
Y. Sun, B. Norick, J. Han, X. Yan, P. S. Yu, and X. Yu.
Integrating meta-path selection with user guided object
clustering in heterogeneous information networks. In Proc.
of 2012 ACM SIGKDD Int. Conf. on Knowledge Discovery
and Data Mining (KDD’12), Beijing, China, 2012.
Scott White and Padhraic Smyth. Algorithms for estimating
relative importance in networks. In Proceedings of the ninth
ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 266–275. ACM, 2003.
Joanna Wolfe. Annotations and the collaborative digital
library: Effects of an aligned annotation interface on student
argumentation and reading strategies. International Journal
of Computer-Supported Collaborative Learning,
3(2):141–164, 2008.
David Wood, Jerome S Bruner, and Gail Ross. The role of
tutoring in problem solving*. Journal of child psychology
and psychiatry, 17(2):89–100, 1976.
X. Yu, X. Ren, Y. Sun, B. Sturt, U. Khandelwal, Q. Gu,
B. Norick, and J. Han. Heterec: Entity recommendation in
heterogeneous information networks with implicit user
feedback. In Proc. of 2013 ACM Int. Conf. Series on
Recommendation Systems (RecSys’13), Hong Kong, 2013.
Xiao Yu, Xiang Ren, Yizhou Sun, Quanquan Gu, Bradley
Sturt, Urvashi Khandelwal, Brandon Norick, and Jiawei Han.
Personalized entity recommendation: A heterogeneous
information network approach. In Proc. 2014 ACM Int. Conf.
on Web Search and Data Mining (WSDM’14), New York,
2014.
Chengxiang Zhai and John Lafferty. A study of smoothing
methods for language models applied to ad hoc information
retrieval. In Proceedings of the 24th annual international
ACM SIGIR conference on Research and development in
information retrieval, pages 334–342. ACM, 2001.

