Sign-Aware Periodicity Metrics of User Engagement for
Online Search Quality Evaluation
Alexey Drutsa
Yandex
Moscow, Russia

adrutsa@yandex.ru
ABSTRACT

make their changes more detectable. The ability of the metric to detect the statistically significant difference when the
treatment effect exists is referred to as the sensitivity.
User engagement metrics (e.g., the number of sessions per
user [9], the absence time [5], etc.) are considered to be
the best ones and are popular in the A/B testing practice
of many companies, because user engagement reflects how
often a user solves her needs (e.g., to search for something)
by means of the considered service (e.g., a search engine) [6,
7]. Hence, on the one hand, these metrics are measurable in
the short-term experiment period, and, on the other hand,
they are predictive of long-term goals of the company [6, 7].
Recently, several metrics of user engagement periodicity
were developed and were used in A/B experiments [3]. These
metrics are the amplitudes of the discrete Fourier transform
(DFT) of daily time series of several standard user engageCategories and Subject Descriptions: H.1.2 [User/Machine ment metrics (e.g., the daily number of sessions). On the one
Systems]: Human information processing; H.5.2 [User interhand, the amplitude metrics were found to be more sensiface]: Evaluation/methodology
tive than the state-of-the-art ones, i.e., the average values of
the time series (e.g., the number of sessions per day). HowGeneral Terms: Measurement, Experimentation
ever, on the other hand, the amplitudes (as considered in [3])
Keywords: User engagement; online controlled experiment;
could not be used to determine, whether the treatment effect
periodicity; DFT; sign-aware metric; A/B test
is positive or negative: if one of these metrics changes (increases or decreases) significantly, it just tells us that a user
1. INTRODUCTION
changes her behavior in response to the treatment variant
with no insight on whether it is positive or negative for the
Online controlled experiments, or A/B testing, have beuser or for the service. Thus, these metrics are sign-agnostic
come the state-of-the-art technique of improving web serwith respect to an evaluated treatment effect.
vices based on data-driven decisions [10, 7]. An A/B test
In our work, we develop a novel approach, which improves
compares two variants of a service1 at a time by exposing
several amplitude metrics by taking into account the phases
them to two user groups and by measuring the difference
of the DFT. We find that the DFT components with the first
between them in terms of a key metric (e.g., the revenue,
frequency detect signals of the UE measure trend along the
the number of visits, etc.), also known as an overall evaluexperiment time period. Hence, these metrics could be profation criterion [8]. Many existing studies were devoted to
itable for delayed treatment effects (which are often caused
an invention of new metrics [5, 9, 3] or to improvements of
by primacy and novelty effects [8, 6]). Our approach proexisting ones [2, 4] in order to make them more consistent
vides an experimenter with several intuitively clear guidewith the long-term goals of the company [1, 6, 7] and to
lines (referred to as symptoms), that allow him to deter1
mine whether the treatment effect is positive or negative,
e.g., a current version of the service (the control variant)
when he or she observes changes in the periodicity compoand a new one (the treatment variant)
nents of a user engagement metric. Thus, we overcome the
sign-agnostic issue of the amplitudes.
Permission to make digital or hard copies of all or part of this work for personal or
We reinforce our theoretical results by applying the proclassroom use is granted without fee provided that copies are not made or distributed
posed approach to evaluation of changes in a search engine.
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
We considered 55 real large-scale online experiments (32
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or reA/B tests and 23 A/A tests) run at Yandex, one of the poppublish, to post on servers or to redistribute to lists, requires prior specific permission
ular search engines. We find that, on the one hand, the novel
and/or a fee. Request permissions from Permissions@acm.org.
approach is more sensitive than the state-of-the-art user enSIGIR’15, August 09 - 13, 2015, Santiago, Chile.
gagement metrics (it allows us to detect the treatment efc 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
Modern Internet companies improve evaluation criteria of
their data-driven decision-making that is based on online
controlled experiments (also known as A/B tests). The amplitude metrics of user engagement are known to be well sensitive to service changes, but they could not be used to determine, whether the treatment effect is positive or negative.
We propose to overcome this sign-agnostic issue by paying
attention to the phase of the corresponding DFT sine wave.
We refine the amplitude metrics of the first frequency by the
phase ones and formalize our intuition in several novel overall evaluation criteria. These criteria are then verified over
A/B experiments on real users of Yandex. We find that our
approach holds the sensitivity level of the amplitudes and
makes their changes sign-aware w.r.t. the treatment effect.

DOI: http://dx.doi.org/10.1145/2766462.2767814.

779

Figure 1: The amplitude A1 , the phase ϕ1 , and the
sine wave f 1 .

Figure 2: A positive, a negative, and neutral trends
of the sine wave f 1 w.r.t. the phase ϕ1 .

fect in more experiments). On the other hand, the novel
approach is sign-aware (it allows us to understand, whether
the detected treatment effect is positive or negative), while
the classic amplitude metrics in [3] are sign-agnostic.

2.

out sharply in the UE trend, then the sine wave with this
frequency should change. Moreover, the shift of this sine
wave can help to determine the sign of the treatment effect
(whether it is positive or negative).
From here on in this paper we will consider only the first
amplitude A1 and its normalized version A1 /A0 . In order to
understand the meaning of a change in such amplitude, let
us consider it together with the corresponding phase ϕ1 of
the sine wave f 1 with the frequency ω1 . Let us consider the
following cases (see Fig.2):

PERIODICITY METRICS

We briefly review the key points of the periodicity metrics,
that were studied in [3]. Let us consider any user engagement (UE) measure calculated for an individual user, e.g.,
the number of sessions [9, 3]. Let x = (x0 , x1 , .., xN −1 ) be
a daily time series of N numbers, that represent this UE
measure calculated for N consecutive days (e.g., the daily
number of sessions). Then, we apply the discrete Fourier
transform (the DFT ) to x and obtain the sequence of its
−1
coordinates in the harmonic basis {f k }N
k=0 :
N
−1
X
2πk
, k ∈ ZN ,
Xk =
xn e−iωk n , ωk =
N
n=0

(a) if ϕ1 ≈ π/2 (or sin ϕ1 ≈ 1), then the sine wave f 1
oscillates in x with a positive trend;
(b) if ϕ1 ≈ −π/2 (or sin ϕ1 ≈ −1), then the sine wave f 1
oscillates in x with a negative trend;
(c) if ϕ1 ≈ 0 or π (or sin ϕ1 ≈ 0), then the sine wave f 1
oscillates in x with a neutral trend.

where f k = (eiωk n /N )n∈ZN is the sine wave (harmonic) with
the frequency ωk . From the polar form2 of each complex
number Xk = |Xk |eiϕk , the amplitude Ak = |Xk |/N and
the phase ϕk are obtained, k ∈ ZN . The amplitude Ak
represents the magnitude of the sine wave f k with the frequency ωk , presented in the series x, whereas the phase ϕk
represents how this wave is shifted (see Fig.1).
The amplitudes Ak and the normalized ones Ak /A0 , k ∈
ZN , are found to be considerably more sensitive to different
changes in a search engine than the state-of-the-art baseline
metric A0 3 . However, these metrics do not determine the
sign of the treatment effect: whether the evaluated service
change is negative or positive for users (i.e., these metrics
are sign-agnostic). In this study, we will show how several
amplitudes could be refined in order to obtain sign-aware
evaluation criteria, that will also be of high sensitivity.

3.

From these cases, we see that, for instance, an increase
of the amplitude A1 may led to both negative, and positive
consequences. It depends on how the sine wave is shifted:
if sin ϕ1 < 0, then the magnitude of the sine wave with a
negative trend increases; on the contrary, if sin ϕ1 > 0, then
the magnitude of the sine wave with a positive trend increases. Furthermore, even through the amplitude A1 does
not change at all, a change in the phase ϕ1 could shift the
type of the sine wave trend: make it more or less negative
or positive. Therefore, we conclude that, in order to understand the positiveness or negativeness of changes in the
amplitudes A1 and A1 /A0 of the sine wave f 1 , we should pay
attention to its phase ϕ1 .
Before presenting in detail our evaluation criteria, let us
dwell on two points. First, note that the equality ImX1 =
N A1 sin ϕ1 holds (see Fig.1). Therefore, from here on in
this paper we will study the novel5 metric ImX1 and its
normalized version ImX1 /A0 . Second, the straightforward
approach to determine the trend of the N -day time series
x is just a comparison of the first half of the series with
the second one. Therefore, we will consider the difference D
between the average value of the time series x over the last
[N/2] days avg2 x and the one over the first [N/2] days avg1 x
(i.e., D = avg2 x−avg1 x) as our baseline metric (besides the
state-of-the-art metric A0 ).
Growth and fall symptoms. We refer to a case in
an A/B test, when the trend of a considered UE measure
becomes more positive (or less negative), as a growth symptom, while we refer to a case, when the trend becomes more
negative (or less positive), as a fall symptom. Let avgA M
and avgB M be the average values of the metric M over the
users, exposed to the variant A and B, respectively, and
∆M = avgB M − avgA M be the difference between the variants. Then, we summarize all symptoms under our study

CHANGE IN ENGAGEMENT TRENDS

In A/B testing practice, we are often faced with primacy
and novelty effects [8, 6], that cause a delay in the treatment effect, which could not be easily detected by the stateof-the-art metrics. We supposed that the metrics aimed to
detect changes in the trend of UE measures may be helpful in
such and other cases. For instance, if a user is enjoyed with
the treatment version, then the number of sessions should
growth during the experiment. Otherwise, if the treatment
harms user engagement, the number of sessions should decrease to the end of the period. Such UE measure changes
along the whole experiment period should leave traces in
the first frequency4 (i.e., ω1 ) of the DFT of the UE measure time series. Therefore, if the treatment effect stands
2

i.e., z = reiϕ = r cos ϕ + r sin ϕi
A0 is the average value of the source time series x of the
UE measure (e.g., the average number of sessions per day).
4
The other frequencies (i.e., ωk , k > 1) are responsible for
more frequent changes (e.g., the week periodicity, etc.).
3

5
The authors of [3] considered only the amplitudes Ak and
Ak /A0 , k ∈ ZN , as evaluation metrics.

780

Table 1: The baseline (BASE) and the novel (DFT) growth and fall symptoms.
Growth symptoms
Fall symptoms
Baseline (BASE): G0
∆D > 0
Gn
∆D/A
>
0
F
∆D
<
0
Fn
∆D/A0 < 0
0
0
0
0
∆ImX1 > 0
∆ImX1 /A0 > 0
∆ImX1 < 0
∆ImX1 /A0 < 0
n
n
G1
G1
F1
F1
∆A1 = 0
∆A1 /A0 = 0
∆A1 = 0
∆A1 /A0 = 0
∆ϕ1 = 0
∆ϕ1 = 0
∆ϕ1 = 0
∆ϕ1 = 0
G2 avgA ImX1 > 0 Gn
avgA ImX1 /A0 > 0 F2 avgA ImX1 > 0 Fn
avgA ImX1 /A0 > 0
2
2
Novel (DFT):
∆A1 > 0
∆A1 /A0 > 0
∆A1 < 0
∆A1 /A0 < 0
∆ϕ1 = 0
∆ϕ1 = 0
∆ϕ1 = 0
∆ϕ1 = 0
G3 avgA ImX1 < 0 Gn
avgA ImX1 /A0 < 0 F3 avgA ImX1 < 0 Fn
avgA ImX1 /A0 < 0
3
3
∆A1 < 0
∆A1 /A0 < 0
∆A1 > 0
∆A1 /A0 > 0
Table 2: The studied user engagement measures.
S the number of sessions
Q the number of queries
C the number of clicks

PT

Table 3: The number of failed A/A tests (out of 23).
Metric

the presence time

CpQ the number of clicks per query
ATpS the absence time per session

S
Q
C
PT
CpQ
ATpS

in Table 1 (where all equalities and inequalities are treated
as statistically significant). The growth (fall) symptoms are
denoted by the letter G (F). The superscript n denotes the
symptoms based on the normalized versions of the metrics.
We will describe only the growth non-normalized symptoms (all other cases are similar). The baseline symptom
G0 is simple: the condition ∆D > 0 means that the average
value over the second part of the time period tends to growth
w.r.t. the one over the first part. The novel DFT symptom
G1 : the condition ∆A1 = 0 means that the amplitude does
not change, hence, the condition ∆ImX1 > 0 infers that the
phase ϕ1 changes in the direction of π/2 (see Fig. 2), i.e., the
sine wave shift changes in the positive direction. The symptom G2 : the condition ∆ϕ1 = 0 means that the shift of the
sine wave f 1 does not change, the condition avgA ImX1 > 0
means that this shift is positive relating to the trend of x,
and ∆A1 > 0 means that the magnitude of this sine wave
increases. The intuition of the symptom G3 is similar.
Note, that the sign of a symptom (growth or fall) is the
sign of a change in the trend of the UE measure during an
A/B test. Therefore, it should not be confused with the sign
of the treatment effect (positiveness or negativeness). The
latter one depends on the UE measure under consideration:
whether its increase is preferable or not. For instance, the
increase of the number of sessions is a positive effect, but
increase in the absence time [5] is negative effect. Thus, the
sign of the symptom ought to be properly translated into
the treatment effect sign for each UE measure individually.

4.

A0

D

D/A0

A1

A1 /A0

ImX1

ImX1 /A0

0
0
1
1
1
0

1
3
2
2
1
2

2
2
1
3
1
2

1
0
1
1
1
1

1
0
0
2
0
1

2
1
1
2
1
1

2
2
1
2
2
1

Table 4: The number of A/B tests (out of 32) with
detected treatment effect (+ the number of those of
them, where it is not detected by A0 ).
Metric
S
Q
C
PT
CpQ
ATpS

A0
2
1
8

D

D/A0

A1

A1 /A0

ImX1

ImX1 /A0

0 (+0) 1 (+1) 3 (+2) 3 (+2) 0 (+0)
2 (+2) 1 (+1) 3 (+2) 3 (+3) 1 (+1)
2 (+1) 1 (+1) 11 (+3) 2 (+1) 2 (+1)

1 (+1)
1 (+1)
1 (+1)

4 1 (+1) 2 (+1) 4 (+1) 1 (+1) 0 (+0)
15 1 (+1) 1 (+0) 12 (+1) 2 (+1) 1 (+1)
4 0 (+0) 0 (+0) 2 (+1) 3 (+2) 2 (+1)

2 (+1)
1 (+0)
0 (+0)

perimentation [8, 1]. An A/A test, which compares the same
versions of the service, should be failed about 5% of the time
for our p-value threshold 0.05 [8, 1]. The number of failed
A/A tests for each metric and each UE measure is reported
in Table 3. We found that, first, the results for the metrics
A0 , A1 , A1 /A0 , ImX1 , and ImX1 /A0 are acceptable (the
two latter ones have the borderline number of failed A/A
tests). Second, the metrics D and D/A0 (which correspond
to the baseline symptoms) failed an unacceptable number of
A/A tests for the vast majority of the UE measures.
A/B tests. Table 4 summarizes the number of A/B
experiments (out of 32) whose treatment effect is detected
(i.e., pval < 0.05) by each metric and each UE measure (the
best result in each row is highlighted in boldface). First,
the measures C and CpQ demonstrate the highest sensitivity
among all measures. Second, we see that the amplitude A1
and sometimes the amplitude A1 /A0 outperform the baseline metric A0 by the number of A/B tests with detected
treatment effect (i.e., they are more sensitive). Thus, we
reproduce the findings of the study [3]. Moreover, we see
that, in almost all cases of the UE measures, the metrics A1
and A1 /A0 detect the treatment effect in those A/B tests,
where it is not detected by the baseline metric A0 (see the
values in brackets in Table 4). Unfortunately, it could be
noted that the novel metrics ImX1 and ImX1 /A0 are worse
than both the amplitudes A1 , A1 /A0 and the average one
A0 . Thus, these novel metrics do not improve noticeably
the sensitivity, but the main benefit from them will be seen
further in detection of the growth/fall symptoms.
Detected symptoms: examples. We start our study
from consideration of two A/B experiments as examples. For
these A/B tests, the differences of all studied metrics, as well

EXPERIMENTS

Experimental setup. In our paper, we consider 32 A/B
and 23 A/A experiments conducted on real users of Yandex
in order to validate our approach. Each experiment lasted
two weeks (N = 14), the user samples used in the A/B tests
were all uniformly randomly selected, and the control and
the treatment groups were almost of the same sizes (at least,
hundreds of thousands of users). Each A/B experiment compared a production version of the search engine with its
noticeable deterioration or its clear improvement. We apply a commonly used two-sample t-test with the threshold
pval = 0.05 for the p-value (as in [2, 9, 3]). We study the
6 main user engagement measures (Table 2) with the same
definitions as in [9, 3, 4].
A/A tests. First of all, 23 control experiments (i.e., A/A
tests) were conducted in order to check correctness of the ex-

781

Table 5: Examples of A/B tests with detected symptoms (significant differences (pval < 0.05) are highlighted).
#

UE meas.

∆A0

∆D

∆A1

∆ImX1

avgA ImX1

∆D/A0

∆A1 /A0

∆ImX1 /A0

avgA ImX1 /A0

∆ϕ1

Symptoms

1

CpQ
ATpS

0.0032
1437

−0.0009
23.3

0.0004
−7.78

−0.0034
144.37

−0.0119
−503.38

−0.0039
0.0004

0.0009
−0.0001

−0.0014
0.0002

0.0055
−0.0005

−0.003
0.005

S

0.0493

0.0012

−0.0016

0.0057

−0.0024

0.0025

0.0033

−0.008

−0.0012

0.0005

0.0001
3·10−5

0.0008

CpQ

−0.0039

0.0085

0.0068

−0.003

0.0026

0.0042

−0.017

n
F3 Fn
0 F1
G1
n
F2
Fn
2

2

Table 6: The number of growth and fall symptoms found in A/B tests (out of 32) for each UE measure and
the number of A/B tests with detected treatment effect by symptoms from BASE, DFT, or ALL sets.
UE
measure

S
Q
C
PT
CpQ
ATpS

G0

G1

G2

G3

Gn
0

Gn
1

Gn
2

Gn
3

F0

F1

F2

F3

Fn
0

Fn
1

Fn
2

Fn
3

0
1
1
1
0
0

0
0
0
0
0
1

0
0
2
0
3
1

0
1
3
1
2
1

1
1
0
1
0
0

1
1
0
1
0
0

1
1
0
0
0
1

0
0
0
0
0
1

0
1
1
0
1
0

0
1
0
0
1
1

0
2
1
1
2
0

2
0
0
2
5
0

0
0
1
1
1
0

0
0
1
1
1
0

1
1
2
1
2
1

0
0
0
0
0
0

5.

as the average values of the metrics ImX1 and ImX1 /A0 over
the control group A are presented in Table 5.
The first A/B test evaluates a treatment, which is an artificial deterioration of the ranking algorithm of the search
engine. We consider this experiment regarding the clicks per
query measure CpQ and the absence time measure ATpS. We
see that, first, the state-of-the-art metric A0 detects a significant increase of ATpS and CpQ for an average user (expected
effects for this experiment). Second, for the measure CpQ, a
statistically significant difference ∆ is observed for several
metrics: the baseline metric D/A0 , the amplitude A1 , and
the novel Fourier coefficient ImX1 /A0 . These differences
and the statistically significant positiveness of avgA ImX1
yields to a detection of three fall symptoms: F3 , Fn
0 , and
Fn
1 (according to Table 1). Therefore, we conclude that the
negative trend of the number of clicks per query per day
stands out more sharply for an average user of the treatment variant B than of the control one. In this experiment,
the novel growth symptom G1 is also observed for ATpS.
The second A/B test evaluates a deterioration of the user
interface. The treatment effect of this experiment is not detected by the state-of-the-art metric A0 both for the number
of sessions measure S and for the clicks per query measure
CpQ (their differences are not statistically significant). However, the amplitude A1 /A0 detects (i.e., pval < 0.05) the
treatment effect in the both measures. The statistically significant positiveness of avgA ImX1 /A0 helps us to conclude
that we observe the fall symptom Fn
2 twice: both the trend
of S, and the trend of CpQ are more negative in the treatment
variant B than in the control one. Thus, we demonstrated
how the novel metrics helped us to understand the sign of the
observed change in the amplitudes, even though the state-ofthe-art metrics do not detect any treatment effect.
Detected symptoms: overall. Finally, Table 6 summarizes the number of detected symptoms overall 32 A/B
experiments. The last three columns of this table report the
number of A/B tests, whose treatment effects were detected
n
by one of the baseline symptoms (i.e., G0 , Gn
0 , F0 , or F0 :
the col. “BASE”), by one of the DFT symptoms (i.e., Gk ,
n
Gn
k , Fk , or Fk , k = 1, 2, 3: the col. “DFT”), or by any
symptom (the col. “ALL”), respectively. We conclude that,
first, the novel symptoms allow us to detect noticeably more
changes in the UE measure trends than the baseline symptoms, i.e. they are more sensitive. Second, being based on
the amplitudes A1 and A1 /A0 , the novel symptoms are more
sensitive than the state-of-the-art metric A0 (see Table 4),
and, on the contrary to the single A1 or A1 /A0 usage, could
explain whether the treatment effect is positive or negative.

# of A/B tests with
BASE DFT
ALL

1
3
3
3
2
0

5
7
9
6
14
5

5
8
11
7
14
5

CONCLUSIONS AND FUTURE WORK

In this paper, we considered the problem of amplitude
metrics that are well sensitive, but are not sign-aware w.r.t.
an evaluated treatment effect. Since the DFT sine wave
with the first frequency carries signals of the UE measure
trend along the experiment time period, we found out that
its phase could shed light on the correct understanding of
changes of its amplitudes. We combined the amplitude metrics with the phase ones and formalized our intuition in several novel overall evaluation criteria (consisting of growth
and fall symptoms of the UE measure trend). We verified
our approach over 55 large-scale A/B experiments on real
users of Yandex, one of the popular search engines. We
found that, on the one hand, the most novel criteria outperformed the baseline and state-of-the-art metrics in terms
of sensitivity. On the other hand, our approach refined the
single amplitude metrics by making them sign-aware w.r.t.
the treatment effect. As future work we can study the signawareness of the amplitudes with other frequencies.

6.

REFERENCES

[1] T. Crook, B. Frasca, R. Kohavi, and R. Longbotham. Seven
pitfalls to avoid when running controlled experiments on the
web. In KDD’2009, pages 1105–1114. ACM, 2009.
[2] A. Deng, Y. Xu, R. Kohavi, and T. Walker. Improving the
sensitivity of online controlled experiments by utilizing
pre-experiment data. In WSDM’2013, pages 123–132. ACM,
2013.
[3] A. Drutsa, G. Gusev, and P. Serdyukov. Engagement
periodicity in search engine usage: Analysis and its application
to search quality evaluation. In WSDM’2015, pages 27–36,
2015.
[4] A. Drutsa, G. Gusev, and P. Serdyukov. Future user
engagement prediction and its application to improve the
sensitivity of online experiments. In WWW’2015, 2015.
[5] G. Dupret and M. Lalmas. Absence time and user engagement:
evaluating ranking functions. In WSDM’2013, pages 173–182,
2013.
[6] R. Kohavi, A. Deng, B. Frasca, R. Longbotham, T. Walker,
and Y. Xu. Trustworthy online controlled experiments: Five
puzzling outcomes explained. In KDD’2012, pages 786–794.
ACM, 2012.
[7] R. Kohavi, A. Deng, R. Longbotham, and Y. Xu. Seven rules of
thumb for web site experimenters. In KDD’2014, 2014.
[8] R. Kohavi, R. Longbotham, D. Sommerfield, and R. M. Henne.
Controlled experiments on the web: survey and practical guide.
Data Mining and Knowledge Discovery, 18(1):140–181, 2009.
[9] Y. Song, X. Shi, and X. Fu. Evaluating and predicting user
engagement change with degraded search relevance. In
WWW’2013, pages 1213–1224, 2013.
[10] D. Tang, A. Agarwal, D. O’Brien, and M. Meyer. Overlapping
experiment infrastructure: More, better, faster
experimentation. In KDD’2010, pages 17–26. ACM, 2010.

782

