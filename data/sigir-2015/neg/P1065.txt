Improving Search using Proximity-Based Statistics
Xiaolu Lu
RMIT University
Melbourne, Australia
xiaolu.lu@rmit.edu.au

Categories and Subject Descriptors

some queries. Although the proximity part doesn’t always improve
the effectiveness, it is more stable.
From the computational perspective, we have found that extracting single term dependency proximity using the plane-sweep algorithm is not a bottleneck. But it is a computational intensive job
when processing each dependency feature separately. However, the
extra cost of considering proximity independently can be reduced
by extracting all dependencies together [4]. Further, since most of
retrieval systems keep both a direct file and an inverted file, it is
possible to exploit both representation to maximize the efficiency.
Although the cost of extracting proximity features can be reduced compared to separate calculations, it is still less efficient
when processing frequent query terms with long documents. Considering the characteristics of all term dependency features, there is
some redundant information. Thus, it may be possible for us to design a rank-safe early termination strategy by only computing partial proximity features instead of extracting them all. Heuristically,
to achieve this, we could map the proximity extraction problem
into weighted interval ranking by taking TF and IDF values into account. Besides the early termination method, it is also worth seeking an indexable organization of proximity statistics. The planesweep method in the extraction process reveals possibilities for
building auxiliary structures to augment existing index structures.
Usually, accurate distances between terms will be captured in
ranking models, but the definition varies. Clarke et al. [1] consider
the cover and scores over sets of the covers, whereas Metzler and
Croft [5] utilize an unordered window. But capturing actual distance sets a higher requirement on both space and query time. Instead of calculating the distance between terms, an approximation
of proximity can also be considered. Especially when using the
proximity feature as an effective first pass ranker, efficiency will
be considered as a higher priority, without sacrificing too much effectiveness. With suitable approximations, it may be possible to
optimize the efficiency within a given threshold of effectiveness.

H.3.1 [Information Storage and Retrieval]: Content Analysis
and Indexing—Indexing methods; H.3.4 [Information Storage and
Retrieval]: Systems and Software—Performance evaluation

Keywords
k-term Proximity, Indexing, Efficiency

ABSTRACT
Modern retrieval systems often use more sophisticated ranking models. Although more new features are added in, term proximity has
been studied for a long time, and still plays an important role. A
recent study by Huston and Croft [2] shows that many-term dependency is a better choice for a large corpus and long queries.
However, utilizing proximity-based features often leads to computational overhead, and most of the existing solutions are tailored to
term pairs. Fewer studies have focused on many-term proximity
computation, and the plane-sweep approach proposed by Sadakane
and Imai [6] is still state-of-the-art. Consider a multi-pass retrieval
process where the proximity features could be an effective first pass
ranker if we can reduce the cost of the proximity calculation.
In this PhD project, we consider the following questions: (i) How
important are the proximity statistics in the term dependency models and what is the cost of extracting the proximity features? (ii)
Although all term dependencies are considered in ranking models,
can we design an early termination strategy considering only partial
proximity? Moreover, instead of viewing the term from the same
level, can we utilizing its locality for obtaining more efficiency?
(iii) How do we best organize the term proximity statistics to be
more indexable, facilitating the extraction process? (iv) How do
we best define the approximation form of term proximity in order
to find the best trade-off between effectiveness and efficiency?
In a preliminary experimental study, Lu et al. [3] compare how
different term dependency components affect the entire ranking
models show that although the phrase component helps to improve
the effectiveness in an overall sense, it degrades dramatically on

References
[1] C. L. Clarke, G. V. Cormack, and E. A. Tudhope. Relevance ranking
for one to three term queries. Inf. Proc. & Man., 36(2):291–311, 2000.
[2] S. Huston and W. B. Croft. A comparison of retrieval models using
term dependencies. In Proc. CIKM, pages 111–120, 2014.
[3] X. Lu, A. Moffat, and J. S. Culpepper. How effective are proximity
scores in term dependency models? In Proc. ADCS, page 89, 2014.
[4] X. Lu, A. Moffat, and J. S. Culpepper. On the cost of extracting proximity features for term-dependency models. Submitted.
[5] D. Metzler and W. B. Croft. A Markov random field model for term
dependencies. In Proc. SIGIR, pages 472–479, 2005.
[6] K. Sadakane and H. Imai. Text retrieval by using k-word proximity
search. In Proc. Symp. Database Appl. Non-Trad. Envs., pages 183–
188, 1999.

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the Owner/Author(s). Copyright is held by the
owner/author(s).
SIGIR’15, August 09 - 13, 2015, Santiago, Chile.
ACM 978-1-4503-3621-5/15/08.
DOI:http://dx.doi.org/10.1145/2766462.2767847.

1065

