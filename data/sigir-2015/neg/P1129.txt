IR Evaluation: Designing an
End-to-End Offline Evaluation Pipeline
Jin Young Kim

Emine Yilmaz

Microsoft

University College London

jink@microsoft.com

emine.yilmaz@ucl.ac.uk

ABSTRACT

2.

This tutorial aims to provide attendees with a detailed understanding of end-to-end evaluation pipeline based on human judgments (offline measurement).
The tutorial will give an overview of the state of the art
methods, techniques, and metrics necessary for each stage of
evaluation process. We will mostly focus on evaluating an
information retrieval (search) system, but the other tasks
such as recommendation and classification will also be discussed. Practical examples will be drawn both from the
literature and from real world usage scenarios in industry.

Evaluation methodologies have played a significant role
in the development of information retrieval and knowledge
management systems. Human judgments, and evaluation
metrics based on human judgments are critical to the advancement of a wide range of applications that rely on labelled data, such as those that make use of machine learning
techniques, e.g., search, recommendation, etc. The labels
and evaluation metrics guide the design of a technique, or
directly become an objective function on which algorithms
are trained.
However, designing suitable metrics, labelling systems and
designing complete evaluation workflows and experiments
from which reasonable and reliable conclusions can be drawn
is no trivial matter. There are many decisions that need to
be made when the quality of a system is evaluated, such as
which metrics to use, how to obtain a representative sample
of tasks, how to collect high-quality human judgments, etc.
Erroneous or biased labels or unsuitable metrics can not
only invalidate the evaluation of a system, but they can also
cause serious issues when used in the training of machine
learning components of the system.
In this tutorial, we aim to cover the theory and practice of
human judgment based evaluation, including topics such as
experiment and task design, human label collection, metric
selection and evaluation. We review the end-to-end process
of judgment-based evaluation and detail each step of the
workflow. We first illustrate the basic principles in designing
an offline evaluation experiment. We then describe the key
stages, such as metric selection, query (or task) sampling,
judging interface design, judge training, evaluation of the
adopted metrics and the experiments.

Keywords
evaluation; measures; crowdsourcing; human judges; experiment design

1.

TUTORIAL OBJECTIVES
The primary learning objectives of the tutorial are:
• Provide participants a thorough understanding of the
full workflow and the methodological decisions involved
in a human judge based (offline) measurement process.
• Provide a review of the state of the art relating to each
step of the evaluation process, covering topics such as
sampling methods, experiment and metric design, human (judge) resource management, judging interface
design, and methods for the evaluation of judgments
and metrics.
• Provide participants with further insights on offline
measurement by means of real world examples and
practical considerations both from the literature and
from industry.

3.

BACKGROUND

SCOPE

This tutorial should be of interest to a wide range of SIGIR attendees. Many (if not most) SIGIR participants work
on areas where the use of human judgments is indispensable
for evaluation, including information retrieval (IR), information extraction, recommender systems, and so on. IR in
particular has a long and rich tradition of label-based evaluation, and many of the techniques for IR evaluation are
applicable to other fields.
This tutorial will provide a broad overview of the stateof-the-art, both in the theory and practice of human label
based evaluation of search and other types of intelligent algorithms. Through this tutorial, those new to the concept of
evaluation will come away with a solid understanding of how
to design human judgment based evaluation experiments,
while those with intermediate knowledge will gain deeper
insights and further understanding of the mechanisms used
in different stages of the evaluation process.

• Enable participants to make informed decisions on experiment design choices, metric selections, sampling
methods and judging system designs based on their
own needs.

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the Owner/Author(s). Copyright is held by the
owner/author(s).SIGIR ’15 Chile, Santiago
ACM 978-1-4503-3621-5/15/08.
DOI: http://dx.doi.org/10.1145/2766462.2767875 .

1129

4.

OUTLINE

∗ Statistical tools
• Practical Guide (30 minutes)

• Designing Offline Measurements (20 mins) [23, 24, 25,
26, 27, 33, 34]

– Examples from TREC [12, 27]
∗ Evolution of the TREC Web Track judging
process

– Introduction to measurement
– Evaluation process overview

– Examples from Industry

– Major components: task, judgment, metric and
experiment

∗ Lessons from Bing offline measurement

– When to use users (online) vs. judges (offline) for
measurement

5.

• Metrics for Offline Measurement (50 mins)
– Experiment design
∗ Understanding goals, criteria and requirements
∗ Workflow design
– Task sampling [27, 28, 29, 30, 31, 32, 35, 36, 37,
38]
∗ Sampling criteria
∗ Recent advances in task sampling (e.g., Priority queue)
– Metric selection
∗ Metric choices by model [6, 27, 39, 46, 47]
· User model based metrics [11, 13, 20]
· System based metrics [14, 15, 16]
∗ Metric choices based on label type (absolute
vs preference rating) [7, 22, 40]
∗ Metrics choices based on metric properties
[41, 42, 43, 44, 45]
· Fidelity
· Sensitivity
· Reliability
· Cross-metric agreement [1, 2]
∗ Metrics for other tasks

TARGET AUDIENCE

The target audience includes any researchers and practitioners who have a need for human judge based evaluation,
for example, to train or evaluate an IR system. Since we also
cover many of recent papers in IR evaluation, this tutorial
will be also helpful for those who wants to follow up on stateof-the-art research in this area. A college-level background
in statistics is required and a basic domain knowledge in IR
is helpful but not mandatory.
After attending the tutorial, we expect the audience to
design, execute and evaluate human judge-based measurement for their need in an effective and efficient manner.
They would be understand basic procedure, key decisions
and trade-offs to make, and know where to look for when
they need more guidance. Also, they will have some code to
start with if they want to use some of the metrics explained.

6.
6.1

PRESENTER BIOGRAPHIES
Jin Young Kim

Jin Young Kim graduated from UMass Amherst with Ph.D
in Computer Science at 2012. His thesis focused on the retrieval and evaluation techniques for personal information.
He is currently an Applied Researcher at Relevance Measurement team in Microsoft Bing, where he spends most of
his time in improving the measurement of search quality,
consulting on challenging measurement issues, establishing
best practice on measurement across the company. He published dozens of papers in the area of ranking model, user
modelling, and evaluation for IR. He was a lecturer for offline measurement in recent Microsoft internal course for
new employees, and gave numerous talks in conferences including SIGIR, ECIR, CIKM, WSDM and WWW.

– Hypothesis testing for evaluation [48, 49]
• Break (20 minutes)
• Human Judgments for Offline Measurement (60 minutes)

6.2

– Judging system design [3, 7, 8, 9, 10, 19, 22]
∗ Lab versus online
∗ UI design
∗ Guidelines

Emine Yilmaz

Emine Yilmaz is a lecturer (assistant professor) at the
Computer Science Department of University College London. She is also working as a research consultant for Microsoft Research Cambridge. She obtained her Ph.D. from
Northeastern University in 2008. Her main interests are
information retrieval and applications of information theory, statistics and machine learning. She has published
research papers extensively at major information retrieval
venues such as SIGIR, CIKM and WSDM. She has previously given several tutorials on evaluation at the SIGIR 2012
and SIGIR 2010 Conferences and at the RuSSIR/EDBT
Summer School in 2011. She has also organized several
workshops on Crowdsourcing (WSDM2011, SIGIR 2011 and
SIGIR 2010) and User Modeling for Retrieval Evaluation
(SIGIR 2013). She has served as one of the organizers of
the ICTIR Conference in 2009 and as the demo chair for the
ECIR Conference in 2013.

– Judge hiring & training [56, 57, 58, 59]
∗ In-house vs crowdsourcing
∗ Recruitment and incentives
∗ Training
– Online or adaptive judging workflows
∗ Early stopping
∗ Task and judge selection algorithms
– Analysis & evaluation [21, 54, 55]
∗ Judging noise and bias
∗ Judging quality vs metrics and evaluation outcome

1130

7.

REFERENCES

[20] Mark Sanderson, Monica Lestari Paramita, Paul
Clough, and Evangelos Kanoulas, Do user preferences
and evaluation measures line up? SIGIR ’10.
[21] Falk Scholer, Diane Kelly, Wan-Ching Wu, Hanseul S.
Lee, and William Webber. 2013. The effect of
threshold priming and need for cognition on relevance
calibration and assessment. SIGIR ’13.
[22] Paul Thomas and David Hawking, Evaluation by
comparing result sets in context, CIKM’ 06.
[23] Jean Tague-Sutcliffe. The pragmatics of information
retrieval evaluation. In Information Retrieval
Experiment: Experiment, pages 59–102.
Butterworth-Heinemann, 1981.
[24] Jean Tague-Sutcliffe. The pragmatics of information
retrieval experimentation, revisited. Inf. Process.
Management., 28(4):467–490, 1992.
[25] Jean Tague-Sutcliffe. The pragmatics of information
retrieval experimentation, revisited. Readings in
Information Retrieval. lnformation retrieval, pages
205–216, 1997.
[26] Ellen M. Voorhees. The philosophy of information
retrieval evaluation. In CLEF ’01: Revised Papes from
the Second Workshop of the Cross-Language
Evaluation Forum on Evaluation of Cross-Language
Information Retrieval Systems, pages 355–370,
London, UK, 2002. Springer-Verlag.
[27] Ellen M. Voorhees and Donna K. Harman. TREC:
Experiment and Evaluation in Information Retrieval.
MIT Press, 2005.
[28] Javed A. Aslam, Virgil Pavlu, and Emine Yilmaz. A
statistical method for system evaluation using
incomplete judgments. In SIGIR ’06: Proceedings of
the 29th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 541–548. ACM Press, August 2006.
[29] Ben Carterette, James Allan, and Ramesh Sitaraman.
Minimal test collections for retrieval evaluation. In
SIGIR ’06: Proceedings of the 29th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 268–275,
2006.
[30] Ben Carterette and Rosie Jones. Evaluating search
engines by modeling the relationship between
relevance and clicks. In NIPS ’07 : Proceedings of
Advances in Neural Information Processing Systems,
2007.
[31] Ben Carterette, Virgil Pavlu, Evangelos Kanoulas,
Javed A. Aslam, and James Allan. If i had a million
queries. In Advances in Information Retrieval: 31st
European Conference on IR Research, Lecture Notes
in Computer Science. Springer-Verlag, April 2009.
[32] Matteo Cattelan and Stefano Mizzaro. IR evaluation
without a common set of topics. In ICTIR ’09:
Proceedings of the 2nd International Conference on
Theory of Information Retrieval, pages 342–345.
Springer-Verlag, 2009.
[33] Filip Radlinski, Madhu Kurup, and Thorsten
Joachims. How does clickthrough data reflect retrieval
quality? In CIKM ’08: Proceeding of the 17th ACM
conference on Information and knowledge
management, pages 43–52, New York, NY, USA, 2008.
ACM.

[1] Azzah Al-Maskari, Mark Sanderson, and Paul Clough,
The relationship between IR effectiveness measures
and user satisfaction. SIGIR ’07.
[2] Al-Maskari, A., Sanderson, M., Clough, P., and Airio,
E. The good and the bad system: does the test
collection predict users’ effectiveness? SIGIR ‘08.
[3] Peter Bailey et al., ”Evaluating search systems using
result page context,” in IIiX, 2010.
[4] Ben Carterette. System effectiveness, user models, and
user utility: a conceptual framework for investigation.
SIGIR ’11.
[5] Ben Carterette, Paul N. Bennett, David Maxwell
Chickering, and Susan T. Dumais, ”Here or There,” in
ECIR, 2008.
[6] Ben Carterette and Ian Soboroff. 2010. The effect of
assessor error on IR system evaluation. SIGIR ’10.
[7] Praveen Chandar and Ben Carterette. Using
preference judgments for novel document retrieval.
SIGIR ’12.
[8] Chandar, P. and Carterette, B. Preference Based
Evaluation Measures for Novelty and Diversity.
SIGIR’13.
[9] Olivier Chapelle, Donald Metzler, Ya Zhang, and
Pierre Grinspan. Expected reciprocal rank for graded
relevance. CIKM ’09.
[10] Charles L.A. Clarke, Maheedhar Kolla, Gordon V.
Cormack, Olga Vechtomova, Azin Ashkan, Stefan
Büttcher, Ian Mackinnon. Novelty and Diversity in
Information Retrieval Evaluation. SIGIR ’08.
[11] Nick Craswell, Onno Zoeter, Michael Taylor and Bill
Ramsey. An experimental comparison of click
position-bias models. WSDM ’08.
[12] Steve Fox, Kuldeep Karnawat, Mark Mydland, Susan
Dumais, and Thomas White. 2005. Evaluating implicit
measures to improve web search. ACM Trans. Inf.
Syst. 23, 2 (April 2005), 147-168.
[13] Peter B. Golbus , Javed A. Aslam , Charles L. Clarke,
Increasing evaluation sensitivity to diversity,
Information Retrieval, v.16 n.4, p.530-555, August
2013.
[14] Kalervo Järvelin and Jaana Kekäläinen. Cumulated
gain-based evaluation of IR techniques. ACM TOIS,
20(4):422-446, October 2002.
[15] Katz, S. M. (1987). Estimation of probabilities from
sparse data for the language model component of a
speech recogniser. IEEE Transactions on Acoustics,
Speech, and Signal Processing, 35(3), 400–401.
[16] Jinyoung Kim, Gabriella Kazai, and Imed Zitouni,
Relevance Dimensions in Preference-based IR
Evaluation. SIGIR ’13.
[17] Alistair Moffat and Justin Zobel. Rank-biased
precision for measurement of retrieval effectiveness.
ACM Transactions on Information Systems,
27(1):2:1-2:27, December 2008.
[18] E. Pronin, ”Perception and misperception of bias in
human judgment,” Trends in cognitive sciences, vol.
11, no. 1, pp. 37-43, 2007.
[19] Tetsuya Sakai and Ruihua Song. Evaluating diversified
search results using per-intent graded relevance.
SIGIR ’11.

1131

[34] Kuansan Wang, Toby Walker, and Zijian Zheng.
Pskip: estimating relevance ranking quality from web
search clickthrough data. In KDD ’09: Proceedings of
the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages
1355–1364, New York, NY, USA, 2009. ACM.
[35] Emine Yilmaz and Javed A. Aslam. Estimating
average precision with incomplete and imperfect
judgments. In Philip S. Yu, Vassilis Tsotras, Edward
Fox, and Bing Liu, editors, Proceedings of the
Fifteenth ACM International Conference on
Information and Knowledge Management, pages
102–111. ACM Press, November 2006.
[36] Emine Yilmaz, Evangelos Kanoulas, and Javed A.
Aslam. A simple and efficient sampling method for
estimating AP and NDCG. In Sung-Hyon Myaeng,
Douglas W. Oard, Fabrizio Sebastiani, Tat-Seng Chua,
and Mun-Kew Leong, editors, SIGIR ’08: Proceedings
of the 31st Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 603–610. ACM Press,
July 2008.
[37] Jianhan Zhu, JunWang, Vishwa Vinay, and Ingemar
J. Cox. Topic (query) selection for IR evaluation. In
SIGIR ’09: Proceedings of the 32nd international
ACM SIGIR conference on Research and development
in information retrieval, pages 802–803, New York,
NY, USA, 2009. ACM.
[38] Mehdi Hosseini, Ingemar J. Cox, Natasa
Milic-Frayling, Milad Shokouhi, Emine Yilmaz: An
uncertainty-aware query selection model for evaluation
of IR systems. SIGIR 2012: 901-910
[39] Ben Carterette, Evangelos Kanoulas and Emine
Yilmaz. Evaluating Web Retrieval Effectiveness”. In
Web Search Engine Research, chapter Evaluating Web
Retrieval Effectiveness, Emerald Library and
Information Science Book Series, 2011
[40] Paul N. Bennett, Ben Carterette, Olivier Chapelle,
Thorsten Joachims: Beyond binary relevance:
preferences, diversity, and set-level judgments. SIGIR
Forum 42(2): 53-58 (2008)
[41] Javed A. Aslam, Emine Yilmaz, Virgiliu Pavlu: The
maximum entropy method for analyzing retrieval
measures. SIGIR 2005: 27-34
[42] Tetsuya Sakai: Evaluating evaluation metrics based on
the bootstrap. SIGIR 2006: 525-532
[43] Voorhees, E. M. and Buckley, C.: The Effect of Topic
Set Size on Retrieval Experiment Error, ACM SIGIR
2002 Proceedings, pp. 316-323, 2002.
[44] Sanderson, M. and Zobel, J.: Information Retrieval
System Evaluation: Effort, Sensitivity, and Reliability,
ACM SIGIR 2005 Proceedings, pp. 162-169, 2005.
[45] Chris Buckley and Ellen M. Voorhees. Evaluating
evaluation measure stability. In SIGIR ’00:
Proceedings of the 23rd annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 33–40, 2000.
[46] Olivier Chapelle, Donald Metlzer, Ya Zhang, Pierre
Grinspan: Expected reciprocal rank for graded
relevance. CIKM 2009: 621-630

[47] Emine Yilmaz, Milad Shokouhi, Nick Craswell,
Stephen Robertson: Expected browsing utility for web
search evaluation. CIKM 2010: 1561-1564
[48] Mark D. Smucker, James Allan, and Ben Carterette.
A comparison of statistical significance tests for
information retrieval evaluation. In CIKM ’07:
Proceedings of the sixteenth ACM conference on
Conference on information and knowledge
management, pages 623–632, New York, NY, USA,
2007. ACM.
[49] David Banks, Paul Over, and Nien-Fan Zhang. Blind
men and elephants: Six approaches to TREC data.
Information Retrieval Journal, 1(1-2):7–34, 1999.
[50] Mihai Georgescu and Xiaofei Zhu. 2014. Aggregation
of Crowdsourced Labels Based on Worker History. In
Proceedings of the 4th International Conference on
Web Intelligence, Mining and Semantics (WIMS14).
[51] Matteo Venanzi, John Guiver, Gabriella Kazai,
Pushmeet Kohli, and Milad Shokouhi. 2014.
Community-based bayesian aggregation models for
crowdsourcing. In Proceedings of the 23rd
international conference on World wide web (WWW
’14).
[52] Falk Scholer, Alistair Moffat, and Paul Thomas. 2013.
Choices in batch information retrieval evaluation. In
Proceedings of the 18th Australasian Document
Computing Symposium (ADCS ’13).
[53] Gabriella Kazai, Emine Yilmaz, Nick Craswell, and
S.M.M. Tahaghoghi. 2013. User intent and assessor
disagreement in web search evaluation. In Proceedings
of the 22nd ACM international conference on
Conference on information & knowledge management
(CIKM ’13).
[54] Luca Busin and Stefano Mizzaro. 2013. Axiometrics:
An Axiomatic Approach to Information Retrieval
Effectiveness Metrics. In Proceedings of the 2013
Conference on the Theory of Information Retrieval
(ICTIR ’13).
[55] Jinyang Gao, Xuan Liu, Beng Chin Ooi, Haixun
Wang, and Gang Chen. 2013. An online cost sensitive
decision-making method in crowdsourcing systems. In
Proceedings of the 2013 ACM SIGMOD International
Conference on Management of Data (SIGMOD ’13).
[56] Djellel Eddine Difallah, Gianluca Demartini, and
Philippe Cudré-Mauroux. 2013. Pick-a-crowd: tell me
what you like, and i’ll tell you what to do. In
Proceedings of the 22nd international conference on
World Wide Web (WWW ’13).
[57] Omar Alonso. 2013. Implementing
crowdsourcing-based relevance experimentation: an
industrial perspective. Inf. Retr. 16, 2 (April 2013),
101-120.
[58] Gabriella Kazai, Jaap Kamps, and Natasa
Milic-Frayling. 2013. An analysis of human factors and
label accuracy in crowdsourcing relevance judgments.
Inf. Retr. 16, 2 (April 2013), 138-178.
[59] Carsten Eickhoff and Arjen P. Vries. 2013. Increasing
cheat robustness of crowdsourcing tasks. Inf. Retr. 16,
2 (April 2013), 121-137.

1132

