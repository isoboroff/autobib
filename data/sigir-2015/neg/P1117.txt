IR Evaluation: Modeling User Behavior for Measuring
Effectiveness
Charles L. A. Clarke

Mark D. Smucker

Emine Yilmaz

Computer Science
University of Waterloo

Management Sciences
University of Waterloo

Computer Science
University College London

ABSTRACT

measures from a user perspective. These measures include
precision, recall, MAP [30–32,49,50], NDCG [18], ERR [10],
and RBP [27]. We discuss efforts to extend these measures to
address novelty and diversity [10, 12, 13], and to incorporate
observed user behavior.
We then discuss sources of user behavior that can be used
to develop and calibrate user models for IR evaluation [23].
These sources can be divided into two broad classes: lab
studies and implicit feedback. Such lab studies include direct relevance assessments, think aloud protocols, eye tracking, as well as the observation of searchers completing search
tasks. Implicit feedback requires the observation of users “in
the wild”, including clickthroughs, dwell times, and mouse
movements [29].
Finally, we introduce techniques for developing evaluation
methodologies that directly model key aspects of the user’s
interaction with the system. For example, for a standard
web search result page, we might model the searcher’s interaction with the summaries, their probability of clicking on
a result, their time to read a page, and other factors. Both
benefits and costs must be expressed in meaningful units.
For example, benefits might be measured by the number of
unique relevant documents seen, while costs might be measure by the time spent interacting with the system. Alternatively, benefits might be measured in time well spent, i.e.,
the time spent viewing relevant material [11].
By careful modeling of user behavior, we can evaluate
systems not only in terms of their average behavior, but
also in terms of variance across a population of users. In
developing these measures, we must strike a balance between
fidelity and complexity. We cannot model all aspects of user
behavior, and simplification is required. Moreover, to make
accurate predictions these models must be calibrated against
actual user behavior; we provide detailed examples of how
to achieve this aim.

This half-day tutorial on IR evaluation combines an introduction to classical IR evaluation methods with material on
more recent user-oriented approaches. We primarily focus
on off-line evaluation, but some material on on-line evaluation is also covered. The broad goal of the tutorial is to equip
researchers with an understanding of modern approaches to
IR evaluation, facilitating new research on this topic and
improving evaluation methodology for emerging areas.

Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems
and Software—Performance evaluation (efficiency and effectiveness)

General Terms
Experimentation, Measurement, Performance

Keywords
user models; effectiveness; efficiency; tutorial

1.

OVERVIEW

An IR effectiveness measure makes a prediction about the
benefits of an IR system to its users. For example, we might
evaluate a search engine result page using the NDCG measure [18], or another traditional measure. In applying this
measure, we assume that a larger NDCG value corresponds
to a better result, providing more benefit to its user. Recent
research extends traditional IR evaluation methodology to
directly model user behavior while interacting with an IR
system, with the goal of improving the quality of the prediction [8, 9, 11, 27, 39–41, 51–53].
In this tutorial, we cover IR evaluation methodologies and
measures from a user modeling perspective. We start with
a review of standard approaches to evaluation, building on
the Cranfield paradigm [14], including a review of traditional

2.

OBJECTIVES

We assume an audience of early doctoral students and established researchers from outside IR. These individuals may
have seen basic IR measures, e.g., NDCG from work on web
search, but require a broader understanding of the important
role that evaluation plays with respect to IR research. As
new IR tasks emerge, we hope to equip researchers working
on these tasks with the tools to appropriately and meaningfully evaluate their efforts. A full bibliography, partialy
reproduced below, allows participants to extend their understanding of each topic after the tutorial.

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the Owner/Author. Copyright is held by the owner/author(s).
SIGIR’15, August 09–13, 2015, Santiago, Chile.
Copyright 2015 ACM 978-1-4503-3621-5/15/08
http://dx.doi.org/10.1145/2766462.2767876 .

1117

3.

FORMAT

4. Calibration and validation

An outline of topics appears below, organized into four
broad categories. The depth varies from topic to topic, with
a emphasis on examples and insights. After a basic overview
of evaluation methodologies, we discuss evaluation from a
user viewpoint, examing how user models may be translated
into evaluation measures. We then discuss approaches to
studying user behavior to inform these models, both through
laboratory studies and through inplicit feedback from live
systems. Finally, we discuss how the results of these user
studies may be applied to validate effectiveness measures. In
addition, we provide exercises allowing attendees to compute
measures, make relevance judgments, and otherwise actively
engage in the process of IR evaluation. After the tutorial,
the slides and exercises from the tutorial will be made openly
available through the authors’ personal websites.

4.

(a)
(b)
(c)
(d)
(e)
(f)
(g)

Costs and benefits; simulation
Time based calibration
Time well spent
Exercise: simulating a search task
Economic models of search
Foraging theory
Applications of reinforcement learning

5.

FULL BIOGRAPHIES

5.1

Charles L. A. Clarke

Charles Clarke is a Professor in the School of Computer
Science at the University of Waterloo, Canada. His research
interests include information retrieval, web search, and text
data mining. He has published on a wide range of topics — including papers related to question answering, XML,
filesystem search, user interfaces, and statistical natural language processing — as well as the evaluation of information
retrieval systems. He was a Program Co-Chair for SIGIR
2007 and SIGIR 2014. He is currently Chair of the SIGIR
Executive Committee, and Co-Editor-in-Chief of the Information Retrieval Journal. He is a co-author of the graduate
textbook Information Retrieval: Implementing and Evaluating Search Engines, MIT Press, 2010.

TOPICS
1. Basic evaluation methodology
(a) Cranfield paradigm
(b) Test collections
(c) Queries and topics
(d) Relevance assessment; the pooling method
(e) Precision; recall; mean average precision
(f) Exercise: computing MAP

5.2

(g) Significance testing; confidence intervals

Mark D. Smucker

(f) Expected browsing utility

Mark Smucker is an Associate Professor in the Department of Management Sciences at the University of Waterloo. Mark’s recent work has focused on making information retrieval evaluation more predictive of actual human
search performance. Mark has been a co-organizer of two
TREC tracks, a co-organizer of the SIGIR 2013 workshop on
modeling user behavior for information retrieval evaluation
(MUBE) and the SIGIR 2010 workshop on the simulation of
interaction. He is a recipient of the SIGIR best paper award
(2012) for his work with Clarke on the time-based calibration of effectiveness measures. He is also a recipient of the
University of Waterloo, Faculty of Engineering’s Teaching
Excellence Award.

(g) Novelty and diversity; intent aware measures

5.3

(h) Session based evaluation

Emine Yilmaz is an Assistant Professor in the Department of Computer Science University College London and a
research consultant for Microsoft Research Cambridge. She
is the recipient of the Google Faculty Award in 2014/15.
Her main interests are evaluating quality of retrieval systems, modeling user behavior, learning to rank, and inferring user needs while using search engines. She has published research papers extensively at major information retrieval venues such as SIGIR, CIKM and WSDM. She has
previously given several tutorials on evaluation at the SIGIR 2012 and SIGIR 2010 Conferences and at the RuSSIR/EDBT Summer School in 2011. She has also organized
several workshops on Crowdsourcing (WSDM2011, SIGIR
2011 and SIGIR 2010) and User Modeling for Retrieval Evaluation (SIGIR 2013). She has served as one of the organizers of the ICTIR Conference in 2009, as the demo chair for
the ECIR Conference in 2013, and as the PC chair for the
SPIRE 2015 conference. She is also a co-coordinator of the
Tasks Track in TREC 2015.

(h) Limitations on recall estimates
(i) TREC and other evaluation experiments
2. A user oriented view
(a) User models behind traditional metrics
(b) Graded relevance and NDCG
(c) The cascade model
(d) Rank biased precision; expected reciprocal rank
(e) Exercise: computing ERR

(i) Passage oriented evaluation; U-measure
(j) Incorporating variability in user behavior
3. The role of user studies
(a) Lab studies
(b) Relevance assessment
(c) Relevance, effort, and user satisfaction
(d) Exercise: relevance judgments
(e) Side-by-side and whole page judgments
(f) Observational studies; eye tracking
(g) Think aloud protocols
(h) On-line evaluation; A/B testing
(i) Clicks, skips and other implicit signals
(j) Result interleaving

1118

Emine Yilmaz

6.

REFERENCES

[1] Azzah Al-Maskari, Mark Sanderson, and Paul Clough.
The relationship between IR effectiveness measures
and user satisfaction. In 30th Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 773–774,
2007.
[2] Javed A. Aslam, Virgil Pavlu, and Emine Yilmaz. A
statistical method for system evaluation using
incomplete judgments. In 29th Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 541–548,
2006.
[3] Javed A. Aslam, Emine Yilmaz, and Virgiliu Pavlu.
The maximum entropy method for analyzing retrieval
measures. In 28th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 27–34, 2005.
[4] Stefan Büttcher, Charles L. A. Clarke, Peter C. K.
Yeung, and Ian Soboroff. Reliable information
retrieval evaluation with incomplete and biased
judgements. In 30th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 63–70, 2007.
[5] Ben Carterette. Robust test collections for retrieval
evaluation. In 30th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 55–62, 2007.
[6] Ben Carterette. System effectiveness, user models, and
user utility: A conceptual framework for investigation.
In 34th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 903–912, 2011.
[7] Ben Carterette, James Allan, and Ramesh Sitaraman.
Minimal test collections for retrieval evaluation. In
29th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 268–275, 2006.
[8] Ben Carterette, Evangelos Kanoulas, and Emine
Yilmaz. Simulating simple user behavior for system
effectiveness evaluation. In 20th ACM International
Conference on Information and Knowledge
Management, pages 611–620, 2011.
[9] Ben Carterette, Evangelos Kanoulas, and Emine
Yilmaz. Incorporating variability in user behavior into
systems based evaluation. In 21st ACM International
Conference on Information and Knowledge
Management, pages 135–144, 2012.
[10] Olivier Chapelle, Donald Metlzer, Ya Zhang, and
Pierre Grinspan. Expected reciprocal rank for graded
relevance. In 18th ACM Conference on Information
and Knowledge Management, pages 621–630, 2009.
[11] Charles L. A. Clarke and Mark D. Smucker. Time well
spent. In 5th Information Interaction in Context
Symposium, pages 205–214, 2014.
[12] Charles L.A. Clarke, Nick Craswell, Ian Soboroff, and
Azin Ashkan. A comparative analysis of cascade
measures for novelty and diversity. In 4th ACM
International Conference on Web Search and Data
Mining, pages 75–84, 2011.
[13] Charles L.A. Clarke, Maheedhar Kolla, Gordon V.
Cormack, Olga Vechtomova, Azin Ashkann, Stefan

[14]
[15]

[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

1119

Büttcher, and Ian MacKinnon. Novelty and diversity
in information retrieval evaluation. In 31st Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages
659–666, 2008.
Cyril W. Cleverdon. The Cranfield tests on index
language devices. AsLib proceedings, 19:173–192, 1967.
Gordon V. Cormack, Christopher R. Palmer, and
Charles L. A. Clarke. Efficient construction of large
test collections. In 21st Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 282–289, 1998.
Georges Dupret and Mounia Lalmas. Absence time
and user engagement: Evaluating ranking functions.
In 6th ACM International Conference on Web Search
and Data Mining, pages 173–182, 2013.
Georges Dupret, Vanessa Murdock, and Benjamin
Piwowarski. Web search engine evaluation using
clickthrough data and a user model. In 16th
International WWW Conference Workshop on Query
Log Analysis: Social and Technological Challenges,
May 2007.
Kalervo Järvelin and Jaana Kekäläinen. Cumulated
gain-based evaluation of IR techniques. ACM
Transactions on Information Systems, 20(4):422–446,
2002.
Thorsten Joachims, Laura Granka, Bing Pan, Helene
Hembrooke, and Geri Gay. Accurately interpreting
clickthrough data as implicit feedback. In 28th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages
154–161, 2005.
Evangelos Kanoulas, Ben Carterette, Paul D. Clough,
and Mark Sanderson. Evaluating multi-query sessions.
In 34th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 1053–1062, 2011.
Gabriella Kazai and Mounia Lalmas. eXtended
cumulated gain measures for the evaluation of
content-oriented XML retrieval. ACM Transactions on
Information Systems, 24(4):503–542, 2006.
Gabriella Kazai, Emine Yilmaz, Nick Craswell, and
Seyed M. M. Tahaghoghi. User intent and assessor
disagreement in web search evaluation. In 22nd ACM
International Conference on Information and
Knowledge Management, pages 699–708, 2013.
Diane Kelly and Cassidy R. Sugimoto. A systematic
review of interactive information retrieval evaluation
studies, 1967–2006. Journal of the American Society
for Information Science and Technology,
64(4):745–770, 2013.
Jinyoung Kim, Gabriella Kazai, and Imed Zitouni.
Relevance dimensions in preference-based IR
evaluation. In 36th International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 913–916, 2013.
Yiqun Liu, Yupeng Fu, Min Zhang, Shaoping Ma, and
Liyun Ru. Automatic search engine performance
evaluation with click-through data analysis. In 16th
International WWW Conference Workshop on Query
Log Analysis: Social and Technological Challenges,
May 2007.

[26] Alistair Moffat, William Webber, and Justin Zobel.
Strategic system comparisons via targeted relevance
judgments. In 30th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 375–382, 2007.
[27] Alistair Moffat and Justin Zobel. Rank-biased
precision for measurement of retrieval effectiveness.
ACM Transactions on Information Systems,
27(1):1–27, 2008.
[28] Benjamin Piwowarski, Andrew Trotman, and Mounia
Lalmas. Sound and complete relevance assessment for
XML retrieval. ACM Transactions on Information
Systems, 27(1):1–37, 2008.
[29] Filip Radlinski and Nick Craswell. Optimized
interleaving for online retrieval evaluation. In 6th
ACM International Conference on Web Search and
Data Mining, pages 245–254, 2013.
[30] Stephen Robertson. On GMAP: and other
transformations. In 15th ACM International
Conference on Information and Knowledge
management, pages 78–83, 2006.
[31] Stephen Robertson. A new interpretation of average
precision. In 31st Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 689–690, 2008.
[32] Stephen E. Robertson, Evangelos Kanoulas, and
Emine Yilmaz. Extending average precision to graded
relevance judgments. In 33rd International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 603–610, 2010.
[33] Tetsuya Sakai. Evaluating evaluation metrics based on
the bootstrap. In 29th Annual International ACM
SIGIR Conference, pages 525–532, 2006.
[34] Tetsuya Sakai and Zhicheng Dou. Summaries, ranked
retrieval and sessions: A unified framework for
information access evaluation. In 36th International
ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 473–482,
2013.
[35] Mark Sanderson and Hideo Joho. Forming test
collections with no system pooling. In 27th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages
33–40, 2004.
[36] Mark Sanderson, Monica Lestari Paramita, Paul D.
Clough, and Evangelos Kanoulas. Do user preferences
and evaluation measures line up? In 33rd
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages
555–562, 2010.
[37] Mark Sanderson and Justin Zobel. Information
retrieval system evaluation: effort, sensitivity, and
reliability. In 28th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 162–169, 2005.
[38] Mark D. Smucker, James Allan, and Ben Carterette.
A comparison of statistical significance tests for
information retrieval evaluation. In 16th ACM
Conference on Conference on Information and
Knowledge Management, pages 623–632, 2007.
[39] Mark D. Smucker and Charles L. A. Clarke. Modeling
user variance in time-biased gain. In 6th Internation

[40]

[41]

[42]

[43]

[44]

[45]

[46]

[47]

[48]

[49]

[50]

[51]

[52]

[53]

1120

Symposium on Human-Computer Interaction and
Information Retrieval, pages 3:1–3:10, 2012.
Mark D. Smucker and Charles L. A. Clarke.
Stochastic simulation of time-biased gain. In 21st
ACM International Conference on Information and
Knowledge Management, pages 2040–2044, 2012.
Mark D. Smucker and Charles L.A. Clarke.
Time-based calibration of effectiveness measures. In
35th International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 95–104, 2012.
Ian Soboroff, Charles Nicholas, and Patrick Cahan.
Ranking retrieval systems without relevance
judgments. In 24th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 66–73, 2001.
Karen Sparck Jones and C. J. van Rijsbergen.
Information retrieval test collections. Journal of
Documentation, 32(1):59–75, 1976.
Jean Tague-Sutcliffe. The pragmatics of information
retrieval evaluation. In Information Retrieval
Experiment: Experiment, pages 59–102.
Butterworth-Heinemann, 1981.
Jean Tague-Sutcliffe. The pragmatics of information
retrieval experimentation, revisited. Readings in
Information Retrieval, pages 205–216, 1997.
Ellen M. Voorhees. Variations in relevance judgments
and the measurement of retrieval effectiveness.
Information Processing & Management,
36(5):697–716, 2000.
Ellen M. Voorhees and Donna K. Harman. TREC:
Experiment and Evaluation in Information Retrieval.
MIT Press, 2005.
William Webber, Alistair Moffat, and Justin Zobel.
Statistical power in retrieval experimentation. In 17th
ACM Conference on Information and Knowledge
Management, pages 571–580, 2008.
Emine Yilmaz and Javed A. Aslam. Estimating
average precision when judgments are incomplete.
International Journal of Knowledge and Information
Systems, 16(2):173–211, August 2008.
Emine Yilmaz, Evangelos Kanoulas, and Javed A.
Aslam. A simple and efficient sampling method for
estimating AP and NDCG. In 31st Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages
603–610, 2008.
Emine Yilmaz, Milad Shokouhi, Nick Craswell, and
Stephen Robertson. Expected browsing utility for web
search evaluation. In 19th ACM International
Conference on Information and Knowledge
Management, pages 1561–1564, 2010.
Emine Yilmaz, Manisha Verma, Nick Craswell, Filip
Radlinski, and Peter Bailey. Relevance and effort: An
analysis of document utility. In 23rd ACM
International Conference on Conference on
Information and Knowledge Management, pages
91–100, 2014.
Yuye Zhang, Laurence A. Park, and Alistair Moffat.
Click-based evidence for decaying weight distributions
in search effectiveness metrics. Information Retrieval,
13:46–69, February 2010.

