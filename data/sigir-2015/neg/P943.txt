Combining Orthogonal Information in Large-Scale
Cross-Language Information Retrieval
Shigehiko Schamoni

Stefan Riezler

Department of Computational Linguistics
Heidelberg University
69120 Heidelberg, Germany

Department of Computational Linguistics
Heidelberg University
69120 Heidelberg, Germany

schamoni@cl.uni-heidelberg.de

riezler@cl.uni-heidelberg.de

ABSTRACT

also needs to incorporate diﬀerent languages and specialized
domains. Thus, techniques that combine specialized systems
into an improved joint system are a promising research direction. In this paper we show that a linear system combination
can yield improvements of more than 10 MAP/NDCG points
over the best single system, if the combined systems represent orthogonal information. We focus on machine learningbased approaches to CLIR that need large sets of relevanceranked data to train high dimensional models. The systems
investigated in this paper are systems based on direct use of
SMT technology, systems that apply learning-to-rank techniques, systems based on probabilistic neural networks, and
methods that incorporate domain-specific meta-information
into linear learners. We present various measures of correlation/orthogonality on the level of scores (Pearson’s correlation coeﬃcient and principal component analysis), ranks
(Kendall’s rank correlation coeﬃcient), and retrieved documents (Jaccard coeﬃcient), and show on two diﬀerent domains (patents, Wikipedia) and two diﬀerent language pairs
(Japanese-English, German-English) that the contribution
of a single system to the combination is best determined
by the orthogonality of the information it represents, rather
than by its standalone retrieval performance.

System combination is an eﬀective strategy to boost retrieval performance, especially in complex applications such
as cross-language information retrieval (CLIR) where the
aspects of translation and retrieval have to be optimized
jointly. We focus on machine learning-based approaches
to CLIR that need large sets of relevance-ranked data to
train high-dimensional models. We compare these models under various measures of orthogonality, and present an
experimental evaluation on two diﬀerent domains (patents,
Wikipedia) and two diﬀerent language pairs (Japanese-English, German-English). We show that gains of over 10 points
in MAP/NDCG can be achieved over the best single model
by a linear combination of the models that contribute the
most orthogonal information, rather than by combining the
models with the best standalone retrieval performance.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.7 [Artificial Intelligence]:
Natural Language Processing

General Terms
Algorithms, Experimentation

2.

Keywords

RELATED WORK

Various publications have investigated diﬀerent methods
of system combination for CLIR, including logical operations on retrieved sets [3], voting procedures based on retrieval scores [1], or machine learning techniques that learn
combination weights directly from relevance rankings [14].
The focus of this paper is on machine learning-based CLIR
approaches and on metrics to measure orthogonality between these systems. Since all of our models require large
sets of relevance-ranked training data, e.g. for learning highdimensional cross-lingual word matrices, we cannot use standard CLIR datasets from CLEF or TREC campaigns that
consist of a few hundred queries with precomputed features.
Instead, we use specialized domains such as patents or Wikipedia where relevance information can be induced from the
citation or link structure.

Machine translation, cross-lingual retrieval, patent search

1. INTRODUCTION
Cross-Language Information Retrieval (CLIR) needs to
jointly optimize the tasks of translation and retrieval, however, it is standardly approached with a focus on one aspect.
For example, the industry standard leverages state-of-theart statistical machine translation (SMT) to translate the
query into the target language, in which standard retrieval
is performed [4]. Most research approaches start from a retrieval perspective [13], or, more recently, from a machine
learning direction [11]. Besides two diﬀerent tasks, CLIR
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGIR ’15, August 09-13 2015, Santiago, Chile
Copyright is held by the author(s). Publication rights licensed to ACM.
Copyright 2015 ACM 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2600428.2609539

3.

CLIR MODELS

Translation Models. SMT-based models translate a query
and then perform monolingual retrieval. Our first model is
called Direct Translation (DT) and uses the SMT framework
cdec [5] to generate a single best query translation.
A second model is called Probabilistic Structured Queries

943

(PSQ). The central idea of this approach is to project query
terms into the target language by probabilistically weighted
translations from the n-best list of a full SMT system [17].
In both models, we use the Okapi BM25 scoring scheme
for document retrieval.

setups where we let the architecture learn distributed representations from: (a) data based on expert translations (family patents) and comparable data (Wikipedia articles on the
same topic in diﬀerent languages), which we call CVMF M ,
and, (b) generally relevant documents (cited patents, linked
Wikipedia articles), which we refer to as CVMR .

Ranking Models. Let q ∈ {0, 1}Q be a query and d ∈

{0, 1}D be a document where the nth vector dimension indicates the occurrence of the nth word for dictionaries of size
Q and D. A linear ranking model is defined as
f (q, d) = q⊤ W d =

Q D
∑
∑

Domain Knowledge Model. The final model (DK ) for
comparison uses highly informative dense features which
capture similar aspects of e.g. patents or Wikipedia articles. Domain knowledge features for patents were inspired
by [8]: a feature fires if two patents share similar aspects,
e.g. a common inventor, similar number of claims, or common patent classes in the IPC hierarchy.
For Wikipedia, we implemented features that compare
the relative length of documents, number of links and images, the number of common links and common images, and
Wikipedia categories (hypernym and hyponym relations).

qi Wij dj ,

i=1 j=1

where W ∈ IRQ×D encodes a matrix of ranking-specific word
associations [2, 14] . We optimize this model by pairwise
ranking, which assumes labeled data in the form of a set R of
tuples (q, d+ , d− ), where d+ is a relevant (or higher ranked)
document and d− an irrelevant (or lower ranked) document
for query q. We compare two methods to find a weight
matrix W such that an inequality f (q, d+ ) > f (q, d− ) is
violated for the fewest number of tuples from R.
The first method uses the Vowpal Wabbit (VW) toolkit [6]
to optimize the following ℓ1 -regularized hinge loss objective:
∑(
)
Lhng =
f (q, d+ ) − f (q, d− ) + + λ||W ||1 ,

4.

MEASURES OF ORTHOGONALITY

Jaccard similarity coefficient. This coeﬃcient measures
the percentage of overlap between two sets. In the retrieval
setup, we limit our attention to the relevant documents within the top-k results for each query. The overlap metric expressing the similarity of two candidate systems is then:

(q,d+ ,d− )∈R

where (x)+ = max(0, m − x) with margin m and λ is the
regularization parameter. VW was run on a data sample of
5M to 10M tuples from R. On each step, W is updated with
a scaled gradient vector ∇W Lhng and clipped to account for
ℓ1 -regularization.
The second method is a boosting model (BM) that optimizes an exponential loss [16]:
∑
−
+
Lexp =
D(q, d+ , d− )ef (q,d )−f (q,d ) ,

J@ksi ∩sj =

|retrieved@ksi ∩ retrieved@ksj |
,
|retrieved@ksi ∪ retrieved@ksj |

where D(q, d , d− ) is a non-negative importance function
on tuples. The algorithm combines batch boosting with bagging over independently drawn bootstrap data samples of
100k instances each from R. In every step, the single word
pair feature is selected that provides the largest decrease of
Lexp . The final scoring function comprises the averaged resulting models. For regularization we rely on early stopping.

where retrieved@k are the relevant documents retrieved within the top-k results. We report the pairwise overlap of two
systems si and sj for k = 100.
Pearson’s ρ and Kendall’s τ . The Pearson product-moment correlation coeﬃcient, Pearson’s ρ, is used to measure
the linear correlation between the scores assigned to each
retrieved relevant document. Kendall’s τ works directly on
the ranks and is insensitive to the absolute score values.
We calculate the metrics on a per-query basis and report
the arithmetic mean. Again, we discard all irrelevant documents from the retrieved results by assigning them a score of
0. Then for each pair of systems, we select the queries which
have at least 3 data points (i.e. relevant documents) in common, as 2 data points are always correlated. On average,
this method selects about 75% of the queries for evaluation.

Neural Network Models. These models utilize the bilin-

Principal Component Analysis (PCA). PCA is a method

gual compositional vector model (biCVM) of [9] to train
a retrieval system based on a bilingual autoencoder. The
training task is to learn two functions f : Q → Rd and
g : D → Rd , which map a query q and a relevant document
d from a corpus C onto a distributed semantic representation in Rd . The energy of a query-document pair (q, d)
is defined by Ebi (q, d) = ||f (q) − g(d)||2 . Introducing a
large margin m into the noise-contrastive update prevents
the model from degenerating. This results in the following
regularized hinge-loss objective:
( k
)
∑
∑(
λ
+
− )
H=
m + Ebi (q, d ) − Ebi (q, d ) + + ||θ||2 ,
2
+
i=1

to find the set of n principal components (PC) that span
the subspace of the data where most of the data variance
resides. The straightforward approach would be to identify
all the PCs (or eigenvectors) describing the retrieved data
and to compare them. Our experiments showed that at least
850 PCs are required to capture more than 90% of the data
variance, making a thorough comparison infeasible. Thus,
we opt for a simplified approach where we consider only a
small subset of the most important PCs.
We start by creating |q| × |d| matrices of retrieval scores
for each system, where |q| and |d| are the numbers of queries
and documents. PCA returns the first k principal components for each system. By calculating their dot products
we obtain a sequence of values, or a k-dimensional vector,
which describes the diﬀerence between the retrieval results
of two candidate systems. To further reduce this vector to a
single value, we report the normalized ℓ2 -norm of this vector
of the top-k PC’s similarity. This reflects our requirement

(q,d+ ,d− )∈R
+

(q,d )∈C

where we treat less relevant documents d− as noise samples
during training. θ represents the model parameters.
While [9] train their system exclusively on parallel data on
sentence and document level, we examine diﬀerent training

944

#d

#d+ /q

models

MAP

NDCG

PRES

(JP-EN)

DT
PSQ
VW
BM
CVMF M
CVMR
DK

0.2554
0.2659
0.2205
0.1730
0.2504
0.1767
0.2203

0.5397
0.5508
0.4989
0.4335
0.5399
0.4229
0.4874

0.5680
0.5851
0.4911
0.5431
0.6104
0.6121
0.5171

(DE-EN)

DT
PSQ
VW
BM
CVMF M
CVMR
DK

0.3678
0.3642
0.1249
0.1386
0.1467
0.1686
0.1824

0.5691
0.5671
0.3389
0.3418
0.3326
0.3515
0.3393

0.7219
0.7165
0.6466
0.6145
0.5584
0.6178
0.4937

Patents (JP-EN)

train
dev
test

107,061
2,000
2,000

888,127
100,000
100,000

13.28
13.24
12.59

Wikipedia (DE-EN)

train
dev
test

225,294
10,000
10,000

1,226,741
113,553
115,131

13.04
12.97
13.22

Patents

#q

Wikipedia

Table 1: Ranking data statistics: number of queries and documents, and average number of relevant documents per query.

that only the dimension of variance is of interest:
v
u
k
u1 ∑
||P C||@ksi ,sj = t
⟨bn , bn ⟩2
k n=1 i j
bn
i

Table 2: Test results for standalone CLIR models using direct
translation (DT ), probabilistic structured queries (PSQ), sparse
ranking model (VW ), sparse boosting model (BM ), compositional vector model trained on parallel/comparable documents
(CVMF M ) and on all relevant documents (CVMR ), and dense
domain knowledge features (DK ).

th

represents the n normalized PC describThe vector
ing the space of relevant documents retrieved by system si ,
thus the range of values for ||P C||@k lies between 0 (all orthogonal) and 1 (all similar). In this sense, our PCA-based
analysis is directly connected to the notion of orthogonality.
We used k = 10 principal components in our experiments.

5. EXPERIMENTS

[10], where the latter considers relevance levels, and the
recall-oriented PRES [12]. All scores were computed on the
top 1,000 retrieved documents.

Patent Prior-art Search. Our first dataset consists of a

Results. Table 2 shows the performance of single retrieval

Japanese-English (JP-EN) corpus of patent abstracts from
the MAREC and NTCIR data.1 It contains automatically
induced relevance judgments for patent abstracts [7]: EN
patents are regarded as relevant to a JP query patent with
level (3) if they are in a family relationship (e.g., same invention), (2) if cited by the patent examiner, or (1) if cited by
the applicant. On average, queries and documents contain
about 5 sentences. Table 1 shows the size of the dataset,
consisting of over 100k queries and nearly 1M documents,
with approximately 13 relevant documents per query.

systems according to MAP, NDCG, and PRES. SMT-based
CLIR-methods clearly outperform all others. Only on specialized domains like patent-prior-art-search and by training
on very clean data (expert translations), the neural networkbased CVMF M model is competitive. On the task of Wikipedia article retrieval, SMT-based methods outperform other
approaches by a large margin.
Our hypothesis is that rather than combining the systems with the best standalone retrieval performance, the
best overall system is gained by combining systems that
are least similar and contribute orthogonal information to
the combination. Table 3 lists all possible pairwise system
combinations, together with their retrieval performance and
their orthogonality/correlation.
An inspection of the patents in Table 3 shows that all
measures of orthogonality/correlation capture the high similarity of the two SMT-based methods, DT and PSQ. Combining these two models results only in a small improvement in retrieval performance. Similar relations are found
for all pairs of systems from same groups: ranking-based
approaches such as VW and BM or neural network approaches such as CVMF M and CVMR are similar according to all measures of orthogonality/correlation, and lead to
small improvements in retrieval performance in combination.
Picking the least similar systems among the four groups, irrespective of their standalone retrieval performance, yields
much higher improvements in combination. This is very pronounced for the DK system that is orthogonal to all other
models. The biCVM-models also seem to contribute new
information, where the gains are mostly higher for combinations with CVMR despite its lower performance as a standalone model compared to CVMF M . The last row in the
Patents section presents the best performing combination of
the four groups’ systems, showing that the improvements by
orthogonal combinations add up.
On Wikipedia data, shown in the lower part of Table
3, we find similar relations. The lower similarity between
CVMR and CVMF M can be explained by training data dif-

Wikipedia Article Retrieval. Our second dataset consists
of relevance-linked Wikipedia pages.2 Relevance judgments
were extracted by aligning German (DE) queries with their
English (EN) counterparts (“mates”) via the graph of interlanguage links available in articles and Wikidata. The highest relevance level is assigned to the EN mate, the next relevance level to all other EN articles that link to the mate,
and are linked to by the mate. Instead of using all outgoing
links from the mate, only articles with bidirectional links are
used. EN documents are restricted to the first 200 words to
reduce the number of features for BM and VW models. To
avoid rendering the task too easy for literal keyword matching of queries about named entities, title words are removed
from German queries. Data statistics are given in Table 1.

Parallel Data for SMT Models. DT and PSQ require an
SMT system trained on parallel corpora. A JP-EN system
was trained on 1.8M parallel sentences from the NTCIR-7
JP-EN PatentMT subtask. For Wikipedia, we trained a DEEN system on 4.1M parallel sentences provided by WMT3 .

System Combination. We reapply the VW ranking approach described in Section 3 on dev set data for system
combination. This method shows stable gains over three different IR-metrics: the precision-based MAP [11] and NDCG
1

www.cl.uni-heidelberg.de/boostclir
www.cl.uni-heidelberg.de/wikiclir
3
www.statmt.org/wmt11/translation-task.html
2

945

combination

MAP

NDCG

PRES

J

ρ

τ

||P C||

works, and linear learners based on meta-information. We
showed experimentally that combining models from these
orthogonal groups outperforms standalone models or combinations of best-performing models.
Acknowledgments. This research was supported in part
by DFG grant RI-2221/1-2 “Weakly Supervised Learning of
Cross-Lingual Systems”.

Patents (JP-EN)
PSQ + DT
PSQ + VW
PSQ + BM
PSQ + CVMF M
PSQ + CVMR
PSQ + DK
DT + VW
DT + BM
DT + CVMF M
DT + CVMR
DT + DK
VW + BM
VW + CVMF M
VW + CVMR
VW + DK
BM + CVMF M
BM + CVMR
BM + DK
DK + CVMF M
DK + CVMR
CVMF M +CVMR
PSQ+VW
+CVMR +DK

L

.2707
.2912
.2661
.3071
.3095
.3554
.2799
L
.2523
.3068
.3084
.3515
.2389
.2923
.2883
.3283
.2739
.2402
.3083
.3388
.3169
L
.2529

.5578
.5862
.5611
.6105
.6140
.6560
.5742
L
.5472
.6108
.6139
.6530
.5324
.5970
.5983
.6366
.5708
.5222
.6167
.6443
.6241
L
.5407

.5941
.6286
.6257
.6808
.7059
.7320
.6095
.6114
.6804
.7071
.7295
.5985
.6623
.6912
.7104
.6490
.6630
.7092
.7493
.7487
.6608

.7318
.5077
.5358
.5528
.4666
.3893
.5345
.5485
.5357
.4560
.3870
.4802
.4729
.3853
.3677
.4929
.4290
.3461
.3667
.3217
.5787

.7488
.4475
.4964
.4336
.3139
.2001
.4682
.5041
.4109
.3082
.2026
.4139
.3880
.2850
.1942
.4018
.3197
.1627
.1931
.1439
.5149

.7591
.5387
.5413
.5154
.3806
.3081
.5574
.5537
.5036
.3730
.3084
.4899
.4832
.3584
.2998
.4607
.3694
.2500
.2919
.2133
.5098

.4717
.2590
.2541
.2359
.2342
.1018
.3129
.2707
.1470
.1651
.1147
.2163
.2380
.2237
.0890
.3047
.3567
.1454
.1379
.1505
.5358

.3834

.6860

.7804

–

–

–

–

.8445
.4092
.4956
.4017
.3841
.3309
.4042
.4899
.3926
.3809
.3275
.3805
.3547
.3618
.2930
.3648
.3546
.2804
.2803
.2785
.3652

.8535
.2630
.4224
.2866
.1299
.0617
.2551
.4119
.2711
.1258
.0557
.2831
.2492
.1701
.0918
.2321
.1541
.0314
.0267
.0339
.2224

.8110
.2452
.3949
.2923
.1376
.1207
.2406
.3894
.2806
.1345
.1168
.2446
.2079
.1300
.1119
.2064
.1371
.0772
.0694
.0550
.1986

.7202
.3054
.2850
.2061
.1298
.0221
.2985
.2965
.1941
.1621
.0168
.2981
.2154
.2890
.0202
.2097
.2390
.0211
.0141
.0073
.2379

–

–

–

–

L

7.

Wikipedia (DE-EN)
PSQ + DT
PSQ + VW
PSQ + BM
PSQ + CVMF M
PSQ + CVMR
PSQ + DK
DT + VW
DT + BM
DT + CVMF M
DT + CVMR
DT + DK
VW + BM
VW + CVMF M
VW + CVMR
VW + DK
BM + CVMF M
BM + CVMR
BM + DK
DK + CVMF M
DK + CVMR
CVMF M +CVMR
DT+VW
+CVMR +DK

.3724
L
.3623
.2908
.3718
.3843
.3894
L
.3714
.2993
.3770
.3870
.4009
R
.1337
.1652
R
.1663
.2239
.1024
.1315
.1893
L
.1856
.2243
.1880

.5758
.5935
.5106
.5840
.6006
.6110
.5997
.5170
.5873
.6021
.6186
.3559
.3922
.3929
.4616
.3006
L
.3372
.4031
.4023
.4455
.3905

.4009

.6352

.7258
.7857
.7207
.7467
.7888
.7772
.7888
L
.7243
.7521
.7911
.7814
.6792
.6952
.7189
.7331
L
.6093
.6546
.6669
.6780
.7226
.6652
L

.8312

REFERENCES

[1] J. A. Aslam and M. Montague. Models for
metasearch. In SIGIR, 2001.
[2] B. Bai, J. Weston, D. Grangier, R. Collobert,
K. Sadamasa, Y. Qi, O. Chapelle, and K. Weinberger.
Learning to rank with (a lot of) word features.
Information Retrieval Journal, 13(3), 2010.
[3] N. J. Belkin, P. Kantor, E. A. Fox, and J. A. Shaw.
Combining the evidence of multiple query
representations for information retrieval. Inf. Process.
Manage., 31(3):431–448, 1995.
[4] J. Chin, M. Heymans, A. Kojoukhov, J. Lin, and
H. Tan. Cross-language information retrieval. Patent
Application, 2008. US 2008/0288474 A1.
[5] C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models. In
ACL, 2010.
[6] S. Goel, J. Langford, and A. L. Strehl. Predictive
indexing for fast search. In NIPS, 2009.
[7] E. Graf and L. Azzopardi. A methodology for building
a patent test collection for prior art search. In EVIA
Workshop, 2008.
[8] Y. Guo and C. Gomes. Ranking structured
documents: A large margin based approach for patent
prior art search. In IJCAI, 2009.
[9] K. M. Hermann and P. Blunsom. Multilingual models
for compositional distributed semantics. In ACL, 2014.
[10] K. Järvelin and J. Kekäläinen. Cumulated gain-based
evaluation of IR techniques. ACM Transactions in
Information Systems, 20(4):422–446, 2002.
[11] H. Li. Learning to Rank for Information Retrieval and
Natural Language Processing. Morgan & Claypool,
2014.
[12] W. Magdy and G. J. Jones. PRES: a score metric for
evaluating recall-oriented information retrieval
applications. In SIGIR, 2010.
[13] J.-Y. Nie. Cross-Language Information Retrieval.
Morgan & Claypool, 2010.
[14] S. Schamoni, F. Hieber, A. Sokolov, and S. Riezler.
Learning translational and knowledge-based
similarities from relevance rankings for cross-language
retrieval. In ACL, 2014.
[15] M. D. Smucker, J. Allan, and B. Carterette. A
comparison of statistical significance tests for
information retrieval evaluation. In CIKM, 2007.
[16] A. Sokolov, L. Jehl, F. Hieber, and S. Riezler.
Boosting cross-language retrieval by learning bilingual
phrase associations from relevance rankings. In
EMNLP, 2013.
[17] F. Ture, J. Lin, and D. W. Oard. Looking inside the
box: Context-sensitive translation for cross-language
information retrieval. In SIGIR, 2012.

Table 3: Test results for combined CLIR models (see Table 2).
Jaccard index J@100, Pearson’s ρ, Kendall’s τ , and the PCAbased ||P C||@10 show correlation/orthogonality of a system pair.
Preceding superscript letters indicate non-significant diﬀerence of
the combined system to the L eft or R ight component at p = .001
using the paired randomization test described in [15].

ferences: the latter expects pairs of comparable documents,
thus we employed the first 200 unfiltered article words as
queries for training. As a result, both models are less similar and the combination shows notable gains compared to
the patent task. The similarity measures between VW and
BM on Wikipedia are blurred for an analogous reason: BM
is trained on the full vocabulary, while VW uses correlated
feature hashing to lower the memory footprint [2].

6. CONCLUSION
We presented an empirical validation of the conjecture
that best results in CLIR system combination are achieved
by combining systems that comprise orthogonal information. We measured correlation/orthogonality on various levels, and identified the groups of translation-based models agnostic of ranking, direct ranking optimizers unapt for translation, distributed semantic representations by neural net-

946

