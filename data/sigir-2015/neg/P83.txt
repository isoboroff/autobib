High Quality Graph-Based Similarity Search
Weiren Yu† , Julie A. McCann†
†

Imperial College London, United Kingdom

{weiren.yu, j.mccann}@imperial.ac.uk

ABSTRACT

node-to-node relationships based on graph topologies, with
a wide range of successful applications, e.g., link prediction,
collaborative filtering, and co-citation analysis.
SimRank, conceived by Jeh and Widom [5], is one of the
most influential similarity measures. The central idea underpinning SimRank is a simple recursion that “two nodes are
assessed as similar if they are in-linked from similar nodes”.
For a digraph G = (V, E) with |V | nodes and |E| edges, let
Na = {x ∈ V |(x, a) ∈ E} be the in-neighbor set of node a.
Then, SimRank score between nodes a and b is defined by1
(
1 P
(a = b)
s(a, b) =
(1)
(i,j)∈Na ×Nb s(i,j)
γ·
(a 6= b)
|Na ||Nb |

SimRank is an influential link-based similarity measure that
has been used in many fields of Web search and sociometry.
The best-of-breed method by Kusumoto et al. [7], however,
does not always deliver high-quality results, since it fails to
accurately obtain its diagonal correction matrix D. Besides,
SimRank is also limited by an unwanted “connectivity trait”:
increasing the number of paths between nodes a and b often
incurs a decrease in score s(a, b). The best-known solution,
SimRank++ [1], cannot resolve this problem, since a revised
score will be zero if a and b have no common in-neighbors.
In this paper, we consider high-quality similarity search.
Our scheme, SR# , is efficient and semantically meaningful:
(1) We first formulate the exact D, and devise a “varied-D”
method to accurately compute SimRank in linear memory.
Moreover, by grouping computation, we also reduce the time
of [7] from quadratic to linear in the number of iterations.
(2) We design a “kernel-based” model to improve the quality
of SimRank, and circumvent the “connectivity trait” issue.
(3) We give mathematical insights to the semantic difference
between SimRank and its variant, and correct an argument
in [7]: “if D is replaced by a scaled identity matrix (1 − γ)I,
top-K rankings will not be affected much”. The experiments
confirm that SR# can accurately extract high-quality scores,
and is much faster than the state-of-the-art competitors.

where γ ∈ (0, 1) is a decay factor. Generally, γ = 0.6 [12] or
0.8 [5], which penalizes long paths relative to short ones.
In contrast with other similarity measures, SimRank has
the following prominent features: (a) It takes a concise form
that captures both direct and indirect neighbors recursively,
unlike Bibliographic Coupling and Co-citation that focus only
on direct neighbors. (b) It considers structural equivalence
of two nodes, whereas Personalized PageRank focuses on reachability from every node to a query. Therefore, SimRank has
attracted increasing attention in recent years [3, 4, 12].

1.1 The Quality of SimRank Search

Categories and Subject Descriptors

Despite much effort devoted to fast SimRank computation
(e.g., [2, 7, 8, 12, 13]), the quality of SimRank search is still
less desirable, due to the following two reasons:
(1) Superfluous Diagonal Correction Error ǫdiag . The
best-of-breed SimRank method by Kusumoto et al. [7] is
based on the following “linearized SimRank formula”:

H.3.3 [Information Search and Retrieval]: Information
Storage and Retrieval

Keywords
Link Analysis; Graph-Based Similarity; High Quality Search

1.

⊤

⊤
2
2
2
s(a, b) = e⊤
a Deb + γ(P ea ) D(P eb ) + γ (P ea ) D(P eb ) + · · ·

INTRODUCTION

The Web today is a huge, self-organized, and hyperlinked
network. These salient features bring striking challenges to
data management, and call for new search abilities to extract
meaningful knowledge automatically from the gigantic Web.
Link-based similarity search is a modern means to quantify

(2)

where D is a precomputed diagonal correction matrix, ea is
a unit vector with a 1 in the a-th entry, and
n P is the column
b |, if (a,b)∈E;
normalized adjacency matrix, with Pa,b = 1/|N
0,
if (a,b)∈E.
/
According to [7], before Eq.(2) is computed, D requires
to be determined in advance. However, it is too difficult to
compute the exact D (not to mention within linear memory)
since SimRank results have a recursive impact on D. Note
that even Kusumoto et al. [7] have not obtained the exact D,
but simply approximated D by D̃ := (1−γ)I. Consequently,
the diagonal correction error is produced:

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
SIGIR’15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767720.

ǫdiag := |s(a, b) − sD̃ (a, b)|,
1

83

(3)

To avoid division by 0 in Eq.(1), s(a, b) = 0 if |Na ||Nb | = 0.

11

4

1

6

12

3

2

5

8

7

SR
SR++
RS
SR#

s(1, 2)
0.24
0.20
0.34
0.38

s(4, 5)
0.30
0.23
0.24
0.24

s(2, 8)
0.12
0
0.05
0.14

s(8, 10)
0.18
0
0.07
0.10

s(3, 9)
0.30
0.15
0.12
0.17

The results are partly depicted in the table. We notice that
SR++ and RS do not well resolve the SR “connectivity trait”.
For example, most people may agree s(1, 2) > s(4, 5) since
node-pair (1, 2) has 3 common in-neighbors {4, 6, 3} whereas
(4, 5) has only 2 in common {11, 12}. However, although
SR++ narrows the gap between s(1, 2) and s(4, 5), it gives
the same counter-intuitive answer s(1, 2) < s(4, 5) as SR.
Another example is the comparison of s(2, 8) and s(8, 10).
For SR++ , s(2, 8) = s(8, 10) = 0. This is because (2, 8) has
no common direct in-neighbors, N2 ∩ N8 = ∅; neither has
(8, 10). Thereby, their “evidence factors” γ̃ = 0. However,
there are 4 indirect path-pairs in-linked from (2, 8):

9

10

s(1, 2) > s(4, 5)
s(2, 8) > s(8, 10)
s(4, 5) > s(3, 9)

SR
✗
✗
✗

SR++
✗
✗
✓

RS
✓
✗
✓

SR#
✓
✓
✓

Figure 1: SimRank++ (SR++ ) and RoleSim (RS) may not
resolve the “connectivity trait” problem of SimRank (SR)
where sD̃ (a, b) is the estimated similarity when D is replaced
by D̃ in Eq.(2). After D is estimated, [7] uses an iterative
method that sums up only the first k terms of series sD̃ (a, b),
(k)
denoted as sD̃ (a, b). This yields the iterative error :
(k)

ǫiter := |sD̃ (a, b) − sD̃ (a, b)| ≤

γ k+1
.
1−γ

2 ← 4 ← 11 → 5 → 8,

2 ← 3 ← 11 → 5 → 8

2 ← 4 ← 12 → 5 → 8,

2 ← 3 ← 12 → 5 → 8

as opposed to only 1 from (8, 10): 8 ← 5 ← 12 → 9 → 10.
Thus, node-pair (2, 8) has a higher connectivity than (8, 10),
but this connectivity trait is ignored by SR++ . Regarding RS,
since it is a “role” similarity measure, it emphasizes more on
similar node degrees than high connectivities. Thus, RS can
only partially resolve the SR “connectivity trait” problem.

(k)

Hence, the total error for approximating s(a, b) by sD̃ (a, b),
in a nutshell, consists of two ingredients: ǫdiag and ǫiter .
We argue ǫdiag is far more serious than ǫiter , because ǫiter
is guaranteed to converge by [7], and can be minimized by
increasing the number of iterations. This increase, however,
cannot minimize ǫdiag . Worse still, there is no bound on
ǫdiag for Eq.(3). The only argument about ǫdiag in [7] is that
“estimating D as D̃ := (1 − γ)I does not much affect the
top-K rankings of sD̃ (∗, ∗) and s(∗, ∗)”, but this, as will be
shown in Section 4.1, bears a blemish.
This motivates us to design an accurate and fast approach
that has no ǫdiag and can avoid computing the exact D.
(2) “Connectivity Trait” Problem. Another factor that
plagues the quality of SimRank is the “connectivity trait”:
increasing the number of paths between nodes a and b often incurs a contrary decrease in s(a, b). However, a paucity
of existing works [1, 2, 11] only noticed a special case (1hop neighbor) of the above phenomenon: “increasing the
number of common in-neighbors between nodes a and b will
decrease s(a, b).” The best-known treatment is due to Antonellis et al. who proposed SimRank++ [1] that replaces γ
in Eq.(1) with the following “evidence factor”:
P a ∩Nb | 1
γ̃ := γ(1 − e−|Na ∩Nb | ) or γ̃ := γ |N
(4)
i=1
2i

Example 1 suggests that the state-of-the-art methods (e.g.,
SimRank++ [1] and RoleSim [6]) cannot solidly circumvent
the “connectivity trait” problem of SimRank. Unfortunately,
as illustrated by our statistical experiments in Section 5.2,
there are many node-pairs suffering from this problem (e.g.,
62.3% in social networks, 82.7% in Web graphs, and 56.4% in
citation graphs), which has adversely affected the quality of
similarity search. This highlights our need for a high-quality
model to resolve the “connectivity trait” problem.

1.2 Our Contributions
Our main contributions are summarized as follows:
• We formulate the exact diagonal correction matrix D,
and propose a “varied-D” method to accurately compute
SimRank with no ǫdiag and in linear memory. Moreover,
by grouping computation, we also optimize the algorithm
[7] from quadratic to linear time w.r.t. k. (Section 2)
• We observe a “connectivity trait” problem for SimRank,
which SimRank++ [1] cannot resolve in a recursive style.
To circumvent this problem, we design a “kernel-based”
model and improve the search quality. (Section 3)

These revised “evidence factors” have a good property: γ̃ is
increasing with respect to |Na ∩Nb |. Hence, a larger γ̃ means
that there are more common direct in-neighbors (i.e., more
paths of length 2) between a and b.
However, we observe a weakness of SimRank++ [1]: SimRank++ score s̃(a, b) is always zero if nodes a and b have no
common (direct) in-neighbors. This is because, by the definition in Eq.(4), if Na ∩Nb = ∅, then γ̃ = 0. Thus, s̃(a, b) = 0,
regardless of how many common l-hop in-neighbors (l > 2)
exist between a and b.
Other pioneering works (e.g., RoleSim [6], PSimRank [2],
and MatchSim [11]) to quantify s(a, b) also resort to common
direct in-neighbors between a and b, all of which can resolve
the special case (1-hop) of the SimRank “connectivity trait”
problem (see related work in Section 1.3.2 for more details).
However, increasing the number of paths with length > 2
between a and b may still lead to a decrease in s(a, b).

• We give mathematical insights to the semantic difference
between Jeh and Widom’s model [5] and its variant [9],
and correct an argument [7]: if D is replaced by (1 − γ)I,
top-K rankings will not be affected much. (Section 4)
The comprehensive experiments verify that our methods
(1) improve an accuracy of average NDCG200 by ∼30% over
SimRank on various real networks, and (2) are ∼10x faster
than the state-of-the-art competitors on large datasets with
65.8M links for 1000 queries.

1.3 Related Work
1.3.1 SimRank Computation
Recent years have witnessed a surge of efficient methods
to compute SimRank. They can be categorized as follows:
• Single-source SimRank [3, 7, 8]. Compute all s(i, ∗).

Example 1. Consider a real Web graph G in Figure 1.
We evaluate the similarity of each node-pair by 4 measures:
(a) SR (Jeh and Widom’s SimRank [5]); (b) SR++ (SimRank++ [1]); (c) RS (RoleSim [6]); (d) SR# (our method).

• All-pairs SimRank [9, 12, 13, 16]. Compute all s(∗, ∗).
• Single-pair SimRank [2, 7, 10]. Compute s(i, j).

84

Type
single
source

all
pairs

Algorithm
Proposed
Kusumoto et al. [7]
Fujiwara et al. [3]
Lee et al. [8]
Proposed
Kusumoto et al. [7]
Yu et al. [13]
Lizorkin et al. [12]
Yu et al. [16]
Li et al. [9]
Jeh et al. [5]

Error
γ k+1

γ k+1

+ ǫdiag
1−γ
ǫrank-r + ǫdiag
γ k+1
γ k+1

γ k+1
+ ǫdiag
1−γ
k+1
γ
k+1
γ
ǫrank-r + ǫdiag
ǫrank-r + ǫdiag
γ k+1

Time
O(k|E|)
O(k2 |E|)
O(r|V |2 )
O(d2k )
O(k|V ||E|)
O(k2 |V ||E|)
O(kd′ |V |2 )
O(k|V ||E|)
O(r|V |2 )
O(r 4 |V |2 )
O(k|E|2 )

Memory
O(|E| + k|V |)
O(|E| + |V |)
O(r|V |2 )
O(d2k + |V |)
O(|E| + k|V |)
O(|E| + |V |)
O(|V |2 )
O(|V |2 )
O(|V |2 )
2
O(r 2 |V | )
O(|V |2 )

2.1 Sensitivity of Diagonal Correction Matrix
In matrix forms, SimRank in Eq.(1) can be rewritten as
S = max{γP ⊤ SP, I},

where max{∗} denotes the matrix entry-wise maximum, i.e.,
(max{A, B})i,j = max{Ai,j , Bi,j }.
Kusumoto et al. [7] have showed that there exists a unique
diagonal matrix D such that Eq.(6) can be converted to
S = γP ⊤ SP + D,

In Table 1, we briefly summarize the accuracy, time, and
memory of previous works for each type of SimRank search.
Compared with the best-known method [7], our techniques
not only well preserve the scalability of [7], but also achieve
high accuracy and fast computational time. Furthermore,
for high accuracy, our methods not only remove superfluous
error ǫdiag but also attain a better bound on ǫiter than [7].

Lemma 1. Let S be the solution to Eq.(7), and SD̃ be the
solution to the equation:
SD̃ = γP ⊤ SD̃ P + D̃,

(8)

and let ∆D := D − D̃ and ∆S := S − SD̃ . Then,

1.3.2 SimRank “Connectivity Trait”
Fogaras et al. [2] is the first to notice one special case of the
SimRank “connectivity trait” problem: “if two nodes a and
b have β common (direct) in-neighbors, then s(a, b) ≤ 1/β.”
To address this problem, they employed an unwieldy method
that divides the entire search space into three probabilities:
|Na ∩Nb | |Na −Nb |
b −Na |
,
, and |N
. However, this complicates
|Na ∪Nb | |Na ∪Nb |
|Na ∪Nb |
the revised SimRank equation, which is rather tedious.
Recently, Antonellis et al. [1] gave an excellent revision,
called SimRank++, by introducing the “evidence factor” γ̃.
Unfortunately, γ̃ can only, in part, alleviate a special case
of the “connectivity trait” problem, since, if |Na ∩ Nb | = 0,
then γ̃ = 0 has no recursive impact on SimRank any more.
Jin et al. [6] also gave an excellent exposition on “role
similarity”. Their proposed model, namely RoleSim, has the
advantage of utilizing “automorphic equivalence” to improve
the quality of similarity search in “role” based applications.
Their initial intention, however, was not to deal with the
SimRank “connectivity trait” problem.
There is also a SimRank-like “connectivity trait” problem in other SimRank variant models, such as MatchSim,
SimRank*, SimFusion+ [15]. Our proposed methods for
SimRank are also extensible to SimRank*. Due to space
limitation, we omit it in this paper.

k∆Skmax ≤

1
k∆Dkmax . 2
1−γ

(9)

Proof. The recursion of SD̃ in Eq.(8) naturally leads to
the following series:
2

SD̃ = D̃ + γP ⊤ D̃P + γ 2 (P ⊤ ) D̃P 2 + · · · ,

(10)

We subtract Eq.(10) from Eq.(2), and then take k ∗ kmax
norms on both sides:
≤k∆Dkmax
z
}|
{
P∞ i
i
k∆Skmax ≤ k∆Dkmax + i=1 γ k(P ⊤ ) ∆D(P i )kmax
≤ (1 + γ + γ 2 + · · · )k∆Dkmax =

1
k∆Dkmax .
1−γ

2.2 Formulating Diagonal Correction Matrix
We next derive an exact explicit formulation of D in Eq.(7).
For ease of exposition, the following notations are adopted.
Definition 1 (Entry-Wise Product). For matrices
X and Y , their entry-wise product X ◦ Y is defined as
(X ◦ Y )i,j = Xi,j Yi,j .
Let diag(Z) be a diagonal matrix whose diagonal entries
are those of Z, i.e., (diag(Z))i,i = Zi,i .
Using this notation, Eq.(6) can be represented as

1.3.3 Semantics between SimRank and Its Variant

S = γP ⊤ SP + I − γdiag(P ⊤ SP ).

There are some interesting works (e.g., [3, 4, 9, 14, 17]),
based on the following model, to evaluate similarity S̃:

(11)

Due to D uniqueness, Eqs.(7) and (11) imply that

(5)

D = I − γdiag(P ⊤ SP ).

[7] argued that “the top-K rankings of S̃ in Eq.(5) and S
in Eq.(1) are not affected much”. However, we correct this
argument, and provide new mathematical insights into the
subtle difference of S̃ and S from a semantic perspective.

2.

(7)

where D is called the diagonal correction matrix, which needs
to be determined beforehand.
However, [7] did not mention how to accurately compute
the exact D, but simply approximated D by D̃ = (1 − γ)I.
In fact, D is very sensitive to the resulting S. Even small
errors in D may lead to large changes in SimRank scores S
1
by a factor of up to 1−γ
, as shown in Lemma 1.

Table 1: A comparison with previous deterministic methods
|E|
(with low-rank r ≤ |V |, degree d = |V
, and d′ ≤ d)
|

S̃ = γP ⊤ S̃P + (1 − γ)I.

(6)

(12)

To formulate the exact D in Eq.(12) only in terms of P ,
we introduce the following lemma.
−−→
Lemma 2. Let diag(Z) be a column vector of the diagonal
−−→
entries of Z, i.e., (diag(Z))i = Zi,i . For two n × n matrices
X and Y , and an n × n diagonal matrix Z, we have
−−→ ⊤
−−→
diag(X ZY ) = (X ◦ Y )⊤ diag(Z).

ACCURATE AND FAST SIMRANK

We first show the sensitivity of diagonal correction matrix
D to SimRank matrix S, and formulate the exact D. Then,
we devise an accurate fast “varied-D” model to compute S.

2

85

k ∗ kmax returns the maximum element of a matrix.

where {Dk } is a diagonal matrix sequence (convergent to D),
which can be iteratively obtained while S is being iterated.
Different from the model Eq.(2) by Kusumoto et al. [7],
our “varied-D” model Eq.(16) replaces all Ds by a convergent
sequence {Dk }. The main advantage of our replacement is
that Eq.(16) can avoid determining the exact D beforehand,
and thereby, will not produce the superfluous error ǫdiag .
The correctness of our “varied-D” model can be verified by
taking limits k → ∞ on both sides of Eq.(16). As k → ∞,
Dk → D and S (k) → S.4 Thus, Eq.(16) converges to Eq.(2).

Combining Lemma 2 with Eq.(12), we next formulate D.
Theorem 1. The diagonal correction matrix D in Eq.(7)
can be explicitly formulated as

P+∞ k k
−−→
k −⊤~
diag(D) =
1,
(13)
k=0 γ (P ◦ P )

where ~1 is a |V | × 1 vector of all 1s, and (∗)−⊤ := ((∗)⊤ )−1 .
−−→
Proof. Taking diag(∗) on both sides of Eq.(2) produces
−−→
−−→
−−→
−−→
2
diag(S) = diag(D) + γ diag(P ⊤ DP ) + γ 2 diag((P ⊤ ) DP 2 ) + · · ·

(14)

2.3.1 Finding Dk in “Varied-D” Model

By SimRank definition Eq.(6), we have Si,i = 1 (∀i ∈ V ),
−−→
which implies that diag(S) = ~1.
Applying Lemma 2 to the right-hand side of Eq.(14) yields

⊤ −→
~1 = I + γ(P ◦ P ) + γ 2 (P 2 ◦ P 2 ) + · · · −
diag(D), (15)

The challenging problem in our “varied-D” Eq.(16) is to
determine the diagonal matrix Dk . Our main idea is based
on two observations: (a) S (k) in Eq.(16) can be iterated as
S (l) = γP ⊤ S (l−1) P + Dl

Since 0 ≤ (P ◦ P )i,j ≤ Pi,j ≤ 1, one can readily show that
⊤
(I + γ(P ◦ P ) + γ 2 (P 2 ◦ P 2 ) + · · · ) is diagonally dominant.
Multiplying both sides by its inverse produces Eq.(13).

Dl = I − γdiag(P ⊤S (l−1) P ).

where the auxiliary vectors h1 , · · · , hk are derived from

h0 = ei
√
(20)
hl = γP hl−1 (l = 1, 2, · · · , k)

Proof. First, we derive a complete matrix formula of Dk .
By Lemma 2, Eq.(19) can be converted to
P
(Dk )i,i = 1 − kl=1 hl ⊤ Dk−l hl
(21)
p
Successive substitution applied to Eq.(20) yields hl = γ l P l ei .
Then, substituting this back into Eq.(21) produces

P
⊤
Dk = I − kl=1 γ l diag (P l ) Dk−l (P l )
(22)

3

Next, we show that Dk in Eq.(22) satisfies Eqs.(16)–(18).
It follows from Eq.(16) that
P
⊤
l+1
diag(γP ⊤ S (k−1) P ) = diag( k−1
(P l+1 ) Dk−1−l P l+1 )
l=0 γ
P
⊤
= diag( kl=1 γ l (P l ) Dk−l P l ).

(P ◦ P )k ≤ (P k ◦ P k ) ≤ P k .
Applying this to Eq.(13) yields

P+∞ k k −⊤
P+∞ k
−→
k −⊤~
~1 ≤ −
diag(D) ≤
1.
k=0 γ P
k=0 γ (P ◦ P )
⊤
P+∞ k
−1
Since k=0 X = (I − X) , it follows that I − γP ~1 ≤
⊤
−−→
diag(D) ≤ I − γ(P ◦ P ) ~1. Then, applying Lemma 2 on
both sides, we obtain the results.

Thus, the above equation implies that

P
⊤
I − diag(γP ⊤S (k−1) P ) = I − kl=1 γ l diag (P l ) Dk−l (P l )
Applying Eq.(22) to the right-hand side yields Eq.(18).

Theorem 2 provides a simple efficient way to compute Dk .

In comparison, the best-known bounds for the range of D
in Proposition 2 of [7] (i.e., (1 − γ)I ≤ D ≤ I) are loose, and
independent of P , which is isolated from graph structures.

Algorithm 1: Compute Diagonal Matrix Dk
1 initialize t := 0, h0 := ei , D0 := I ;
2 for l := 1, 2, · · · , k do
√
3
compute hl := γP hl−1 ;
−−→
4
update t := t + (hl ◦ hl )⊤ diag(Dk−l ) ;

2.3 A “Varied-D” Iterative Model
Another important consequence of Theorem 1 is to derive
an accurate SimRank algorithm without ǫdiag .
Instead of determining the exact D in advance, our method
is to iteratively update D and S at the same time. Precisely,
we leverage the “varied-D” SimRank model as follows:

3

(18)

Theorem 2. The diagonal correction matrices in Eq.(16)
can be iteratively obtained as follows:
P
−−→
(Dk )i,i = 1 − kl=1 (hl ◦ hl )⊤ diag(Dk−l ) with D0 = I, (19)

Proof. Since 0 ≤ Pi,j ≤ 1, we can readily show that

k

(17)

Coupling these observations, we can compute Dk in Eq.(16).

which suggests that the approximation D ≈ (1 − γ)I in [7] is
equivalent to the approximation P k ◦P k ≈ P k . Clearly, most
2
people will not agree that ((P k )i,j ) ≈ (P k )i,j is reasonable.
In Section 4.2, we will further discuss D ≈ (1 − γ)I from the
viewpoint of semantics.
One benefit of Theorem 1 is that it narrows the boundaries
for the range of D in [7], based on the following corollary.

S (k) := Dk + γP ⊤ Dk−1 P + · · · + γ k (P ⊤ ) D0 P k ,

S (0) = D0 .

(b) To ensure diag(S (l) ) = I, Dl in Eq.(17) must satisfy

Theorem 1 characterizes the exact D as an infinite series.
Hence, prior to computing S, it is too difficult to obtain the
exact D in only a finite number of iterations. This tells us
that using the method of [7] will innately produce ǫdiag .
Theorem 1 also implies that the estimation D ≈ (1−γ)I in
[7] is not appropriate for accurately computing S in Eq.(7).
This is because replacing (P k ◦ P k ) by P k in Eq.(13) yields
P+∞ k k −⊤
−−→
~1 = (I − γP )⊤~1 = (1 − γ)~1,
diag(D) ≈
k=0 γ P

Corollary 1. (1 − γ)I ≤ D ≤ I − γdiag(P ⊤ P )

with

5 return (Dk )i,i := 1 − t ;

The correctness of Algorithm 1 is verified by Theorem 2.
Regarding complexity, we have the following result.

(16)
4

For matrices A and B, A ≤ B refers to Ai,j ≤ Bi,j , ∀i, j.

86

The convergence of S (k) will be proved in Section 2.3.2.

Theorem 3. Given the total iteration number k = 1, 2, · · · ,
Algorithm 1 is in O(k|V |) memory and O(k(|E|+|V |)) time.

x1

...

In contrast to the linear-memory SimRank method in [7],
Theorem 3 implies that our “varied-D” method to compute
Dk will not compromise the scalability of [7] for high quality
search, since Dk can be computed in linear memory as well.

by a group of the resulting vectors added together”. Hence,
we rearrange the computation of Eq.(26) as follows:
(S (k) )i,∗ = Dk x0 + γP ⊤ (Dk−1 x1 + γP ⊤ (Dk−2 x2 + · · ·
· · · + γP ⊤ (D1 xk−1 + γP ⊤ (D0 xk ))))

Theorem 4. Let S (k) and S be the k-th iterative and the
exact SimRank in Eqs.(16) and (7), respectively. Then,

Proof. We subtract Eq.(7) from Eq.(17) to obtain, ∀k,
(24)

Algorithm 3: Optimized Single-Source SimRank(i)

We notice from Eq.(18) that (S (k) )i,i = Si,i = 1, ∀i ∈ V .
Thus, when i 6= j, it follows from Eq.(24) that, ∀i, j ∈ V ,
≤ γkS (k−1) − Skmax ≤ · · · ≤ γ k kI − Skmax

(27)

and obtain the result by starting with the innermost brackets
and working outwards. In contrast with the method [7],
Eq.(27) has only O(k) matrix-vector products in k brackets,
as opposed to O(k2 ) products in Procedure 2.
Based on Eq.(27), we give an efficient way of Procedure 2.

(23)

(S (k) − S)i,j = γ(P ⊤ )i,∗ (S (k−1) − S)P∗,j .

b

Figure 2: SimRank “Connectivity Trait” Problem

Besides no ǫdiag and no need to precompute the exact D,
our “varied-D” model Eq.(16) also converges faster than [7].

S (k) − S = γP ⊤ (S (k−1) − S)P + (D(k) − D).

...

...
a

2.3.2 Fast Convergence of “Varied-D” Model

kS (k) − Skmax ≤ γ k+1 .

xn

1 initialize x0 := ei ;
2 for l := 1, 2, · · · , k do
3
update xl := P xl−1 ;
−−→
4 initialize y0 := diag(D0 ) ◦ xk ;
5 for l := 1, 2, · · · , k do
−−→
6
update yl := diag(Dl ) ◦ xk−l + γP ⊤ yl−1 ;

(25)

By Eq.(1), kI − Skmax ≤ γ. Thus, Eq.(23) holds.

7 return (S (k) )i,∗ := yk ;

k+1 
In comparison to the bound γ1−γ (see Eq.(10) of [7]),
Theorem 4 shows that our “varied-D” model not only eliminates ǫdiag , but also has a better bound on ǫiter than [7].
Thus, our “varied-D” model achieves both high-quality and
fast convergence rate at the same time.

Algorithm 3 can reduce not only the time of single-source
SimRank from O(k2 |E|) [7] to O(k|E|), but also the time of
all-pairs SimRank from O(k2 |V ||E|) [7] to O(k|V ||E|), since
all-pairs SimRank runs |V | times of single-source SimRank.

2.4 Efficiently Computing S (k)

3. ENHANCING SIMRANK QUALITY

Having determined Dk in our “varied-D” model Eq.(16),
we next propose our method to efficiently compute S (k) .
The method [7] requires O(k2 |E|) and O(k2 |V ||E|) time,
respectively, to compute single-source and all-pairs SimRank.
If we merely apply the method [7] and replace D with Dk ,
then our “varied-D” Eq.(16) to compute S (k) will retain the
same complexity as [7] except with no ǫdiag , as follows:

After the superfluous ǫdiag is avoided, we next focus on
the “connectivity trait” problem of SimRank.

3.1 The “Connectivity Trait” Problem
We observe that the root cause of the “connectivity trait”
1
problem is that the order of the normalized factor |Na ||N
b|
in the SimRank definition Eq.(1) is too high. To clarify this,
let us consider the following situation in Figure 2:
Let δ be the number of paths {a ← x → b} to be inserted
between nodes a and b. By SimRank definition Eq.(1), after
insertions, s(a, b) will become a function of δ:

Procedure 2: Single-Source “Varied-D” SimRank(i)
1 initialize h := ~0, x := ei ;
2 for l := 0, 1, · · · , k do
l
3
update h := h + γ l (P ⊤ ) (Dk−l )x, x := P x ;

sδ (a, b) = γ ·

4 return (S (k) )i,∗ := h ;

|Na ∩Nb |+δ
(|Na |+δ)(|Nb |+δ)

∼γ·

δ
δ2

→ 0.

(δ → ∞) (28)

This suggests that, for large δ, sδ (a, b) behaves like (γ · 1δ ),
which is eventually decreasing w.r.t. δ.

However, we observe that there exist many duplicate products in [7]. Precisely, to obtain the result of the sums

3.2 Our Kernel-Based SimRank Model

k

(S (k) )i,∗ = Dk x0 + γP ⊤ Dk−1 x1 + · · · + γ k (P ⊤ ) D0 xk , (26)

To avoid the order inconsistency between denominator
and numerator in Eq.(28), our goal is to judiciously adjust
the order of |Na 1||Nb | while normalizing s(a, b) correctly.


the method [7] separately computes every γ l (P ⊤ )l (Dk−l )xl
and then adds them together. Its main limitation is that,
to compute any power of (P ⊤ ), [7] has to go through all of
the previous powers from scratch. As a result, there are l
matrix-vector
products to compute each h in Line 3, leading
P
to kl=1 l = O(k2 ) products for k iterations in total.
We now propose an efficient method for Procedure 2, which
reduces O(k2 |E|) to O(k|E|) time, with no loss of accuracy.
Our key observation is that “doing each matrix-vector multiplication separately is equivalent to multiplying a matrix

Definition 2. Let A be an adjacency matrix. The “cosinebased” SimRank Ŝa,b between a and b is defined by
Ŝa,b = (1 − γ)
where kxk2 :=

87

pP

i

∞
X

k=0

⊤

γk

k
k
e⊤
a (A ) A eb
,
k
k
kA ea k2 kA eb k2

(29)

|xi |2 denotes the L2 -norm of vector x.

Example 2. Recall the δ paths {a ← x → b} to be added
into G in Figure 2. After insertion, Ŝa,b (δ) in Eq.(29) can
circumvent the “connectivity trait” problem. This is because

To prevent division by zero in Eq.(29), we define the k-th
term of the sums to be 0 if (Ak )∗,a or (Ak )∗,b = ~0.
Our cosine-based SimRank Ŝa,b integrates weighted cosine
similarities between a’s and b’s multi-hop in-neighbor sets.
This can be seen more clearly when we rewrite Eq.(29) as
Ŝa,b = (1 − γ)

∞
P

k

k

k

γ φ(A ea , A eb ) with φ(x, y) :=

k=0

x⊤ y
.
kxk2 kyk2

Aea = (1, 1, · · · , 1, 0, 0, · · · 0, 1, 1, · · · , 1)⊤
| {z } | {z } | {z }
|Na |

Aeb = (0, 0, · · · 0, 1, 1, · · · , 1, 1, 1, · · · , 1)⊤
| {z } | {z } | {z }

(30)

|Na −Nb |

We call φ(x, y) a kernel similarity function. In Definition 2,
we take φ(x, y) as the well-known cosine similarity function.
The vector Ak ea (resp. Ak eb ) in Eq.(30) collects the information about k-hop in-neighbors of node a (resp. b). Hence, the
term φ(Ak ea , Ak eb ) in Eq.(30) evaluates how similar node
a’s and b’s k-hop in-neighbor sets are likely to be in terms of
the number of length-k paths in-linked from both a and b.
The factor γ k penalizes connections made with distant k-hop
in-neighbors, and (1 − γ) normalizes Ŝa,b into [0, 1]. Thus,
Ŝa,b not only distills the self-referentiality of SimRank, but
also extends a one-step cosine similarity to a multi-step one.

kAea k2 =

(k)

1

.

1
|hopk (a)∩hopk (b)|+δ
1
1

−

1
2(|hop k (a)|+δ)
1

→ (1 − γ)γ

+ (1 − γ)γ k (u(k) )⊤ v (k)

(32)

1
2(|hopk (b)|+δ)
1

(δ → ∞) (33)

(0)

with Ŝa,b = 0 (34)

2

(k)

4. SEMANTIC DIFFERENCE
Apart from Jeh and Widom’s SimRank model [5]:
S = max{γP ⊤ SP, I},

(δ > 0)

−

|Nb |+δ

|Nb | + δ

Eqs.(34)–(35) provide an algorithm to compute Ŝa,b , which
is in O(k|E|) time and O(|E|+k|V |) memory for k iterations.

(36)

recent years have witnessed many studies (e.g., [3, 4, 9, 14])
to compute similarity, based on Li et al.’s model [9]:

To show f (δ) increases w.r.t. δ, we take log(∗) on both sides,
and then use implicit differentiation w.r.t. δ on both sides:
f ′ (δ) = f (δ)

√

2

we notice that, only for k1 = k2 , the k1 -th term of the series
Eq.(31) will be changed to a function of δ:
1

|Na ∩Nb |+δ
|Na |+δ

p

where the auxiliary vectors u(k) and v (k) are obtained by
( (0)
( (0)
u = ea
v = eb
(35)
Au(k−1)
Av (k−1)
u(k) = kAu
v (k) = kAv
(k−1) k
(k−1) k

k2 edges

k1 (a)|+δ)·(|hopk1 (b)|+δ)

(k−1)

Ŝa,b = Ŝa,b

(31)

a←
· · ← ◦ ←} ◦ →
· · → ◦ →} b
| ◦ → ·{z
| ◦ ← ·{z
|hopk (a)∩hopk (b)|+δ

kAeb k2 =

In contrast to SimRank++ [1] and PSimRank [2] whose
revised weight factors rely only on common Na and Nb , our
method Eq.(29), even if Na ∩ Nb = ∅, can evaluate s(a, b)
from common multi-hops neighbors hopk (a) ∩ hopk (b).
To compute the cosine-based SimRank score Ŝa,b , if a = b,
Eq.(29) implies Ŝa,b = 1. If a 6= b, we compute Ŝa,b as

When inserting the following δ paths between a and b:

f (δ) = γ k1 √(|hop

δ

|Na |

p
|Na | + δ,

Comparing this with Eq.(28), Ŝa,b (δ) is not eventually decreasing w.r.t. δ, which is due to norm k∗k2 used in Eq.(29).

Plugging these into Eq.(29) produces

k1 edges

12 + · · · + 12 + 12 + · · · + 12 =
|
{z
} |
{z
}

Ŝa,b (δ) = (1 − γ)γ · √

k ⊤ k
e⊤
a (A ) A eb = |hopk (a) ∩ hopk (b)|,
p
p
k ⊤ k
kAk ea k2 = e⊤
|hopk (a)|.
a (A ) A ea =

|hop (a) ∩ hopk (b)|
γk p k
.
|hopk (a)| · |hopk (b)|
k=0

p

Therefore, it follows from Eq.(29) that

Proof. Let hopk (x) = {i ∈ V |(Ak ex )i > 0} be the k-hop
in-neighbor set of node x. Then, we have

∞
X

δ

|Nb |

Then, we have (Aea )⊤ Aeb = |Na ∩ Nb | + δ and

Theorem 5. The cosine-based SimRank model in Eq.(29)
can circumvent the SimRank “connectivity trait” problem.

Ŝa,b = (1 − γ)

δ

|Nb −Na |

S̃ = γP ⊤ S̃P + (1 − γ)I.


.

(37)

In this section, we explore their relationship from a semantic perspective, and correct two arguments in [9] and [7].

Since f (δ) > 0 and |hopk1 (a)| ≥ |hopk1 (a) ∩ hopk1 (b)| and
|hopk1 (b)| ≥ |hopk1 (a) ∩ hopk1 (b)|, we can obtain f ′ (δ) > 0.
Thus, f (δ) increases w.r.t. δ, which implies that paths (32)
insertion will not decrease Ŝa,b .

4.1 A Fly in the Ointment of [7, 9]
There are only two works [7, 9] that have mentioned the
relationship between S̃ and S. (a) Li et al. [9] argued that
“S̃ affects only the absolute similarity value of S, but not the
relative similarity ranking of S.” (b) The recent work by
Kusumoto et al. [7] states that “S̃ does not much affect the
top-K ranking of S. 7 ” However, either of them implies a
limitation, as disproved by the following counterexample.

Indeed, by using P eb = Aeb /kAeb k1 5 to the original SimRank Eq.(2), we notice that both Eqs.(2) and (29) tally the
same paths in-linked from a and b. The difference is norms
k ∗ k2 and k ∗ k1 used by Eq.(29) and Eq.(2)6 , respectively.
Since the SimRank “connectivity trait” problem is due to
1
the high order of |Na ||N
in Eq.(1), it is reasonable for us
b|
to prevent its high order by replacing k ∗ k1 with k ∗ k2 since
kxk2 ≤ kxk1 . Moreover, by using k∗k2 , Ŝa,b can be correctly
normalized into [0, 1]. This is because φ(∗, ∗) ∈ [0, 1], which
P
k
indicates that 0 ≤ Ŝa,b ≤ (1 − γ) ∞
k=0 γ ≤ 1 in Eq.(30).
P
5
kxk1 := i |xi | denotes the L1 -norm of vector x.
6
1
(= kP ea k11kP eb k1 ) in Eq.(1).
P is associated with |Na ||N
b|

Example 3. Consider graph G in Figure 3, for γ = 0.6,
the top-10 similarity rankings by S and S̃ are shown in part:
node pairs
rank by S
rank by S̃
7

88

(3, 3)
1
4

(6, 6)
1
3

...
...
...

(1, 2)
9
10

(7, 8)
9
9

In essence, S ≈ S̃ is equivalent to D ≈ (1 − γ)I.

7

1

k=0

k=1

4
6

3
5

8

2

Figure 3: A Citation Graph (A Counterexample)

Definition 3 (Off-diagonal Operator). For square
matrix X, let (∗)off be a matrix operator defined by
(X)off := X − diag(X).
This notation is introduced to bring new insights into S.
Theorem 6. The similarity S in Jeh and Widom’s model
Eq.(36) can be characterized as follows:

Proof. Applying (∗)off , Eq.(36) can be iterated as

(39)

Sk+1 = γ(P ⊤ Sk P )off + I

= γ

⊤

P (P

(using the hypothsis Sk = Rk )

· · · (P P )off · · · P )off P off + γ(P ⊤ Rk−1 P )off + I
{z
}
|
{z
}
⊤

k nested (∗)off

|

⊤

={using Eq.(39)}

· · · (P P )off · · · P )off P
{z
(k+1) nested (∗)off



+Rk = Rk+1 .

off

}

∀k = 1, 2, · · · (41)

k nested (∗)off

In contrast, S̃ in Eq.(37) is the weighted sums of the terms
(P
|

⊤

⊤

⊤

· · · (P (P P )P ) · · · P )
{z
}

∀k = 1, 2, · · ·

i

i

j

i (j)

j

k edges

Example 4. Recall the graph in Figure 3. By Theorem 7,
the path 7 ← 6 ← 5 ← 3 → 4 → 6 → 8 is tallied by the

⊤
term (P 3 ) P 3 , but not by (P ⊤ (P ⊤ (P ⊤ P )off P )off P )off .
Indeed, regarding the term (P ⊤ (P ⊤ (P ⊤ P )off P )off P )off , the
innermost (∗)off disallows paths with repetition of nodes 5
and 4; the second nested (∗)off disallows the repetition of
nodes 6 and 6 (which the considered path violates); the outermost (∗)off disallows the repetition of nodes 7 and 8.

(P ⊤ (P ⊤ P )off P )off

(42)



i,j

= (P 2 )⊤ P 2

➂



i,j

− P ⊤ diag(P ⊤ P )P

➂➃➄➅



i,j

−

➃➅



− diag((P 2 )⊤ P 2 ) i,j + diag(P ⊤ diag(P ⊤ P )P ) i,j

To find out the semantic difference between S and S̃, we
merely need to compare the paths tallied by (41) and (42):

➄➅
➅
where a circled number beneath each term is associated with
a path numbered in the upper-left corner of Figure 4.

Theorem 7. Given a graph G, the terms in Eq.(41) tally
the following paths in G:
k edges

i (j)

...

i (j)

k nested brackets

x0 ← x1 ← · · · ← xk−1 ← xk → xk+1 → · · · → x2k−1 → x2k
|
|
{z
}
{z
}

j

In light of Theorem 7, the semantic relationship between S
and S̃ is evident: S̃ often aggregates more paths than S, and
S excludes the paths with self-intersecting nodes that are
considered by S̃. Figure 4 depicts an illustrative comparison
of the paths tallied by (Sk )i,j and (S˜k )i,j for k = 0, 1, 2.
For verification, let us apply (∗)off definition to expand,
e.g., the term (P ⊤ (P ⊤ P )off P )off as follows:

By Theorem 6, S in Eq.(36) is the weighted sums of
(P ⊤ · · · (P ⊤ (P ⊤ P )off P )off · · · P )off
|
{z
}

i

with no repetition of nodes xi and x2k−i and i 6= j.

Using induction on k, we next show that Sk = Rk (∀k).
Clearly, S0 = I = R0 . Assume Sk = Rk holds, we consider

⊤

j

i

k edges

(40)

k nested (∗)off

k+1

i (j)

i ← x0 ← x1 ← · · · ← xk−1 ← xk → xk+1 → · · · → x2k−1 → x2k → j
{z
}
|
{z
}
|

We now construct the iterations: starting with R0 = I,

P (P
|

j

i

tallies the length-2k paths (43) with no repetition of nodes
xl and x2k−l (∀l). We now consider the term Ek+1 for k + 1.
Due to (Ek+1 )i,j = (P ⊤ )i,∗ Ek (P )∗,j if i 6= j, and 0 if i = j,
(Ek+1 )i,j tallies the length-(2k + 2) paths concatenated by
i ← x0 , paths (43), and x2k → j, which is

k nested (∗)off

= γ

➅

k nested (∗)off

+ γ k (P ⊤ · · · (P ⊤ (P ⊤ P )off P )off · · · P )off + · · · (38)
|
{z
}

⊤

...

➄

Ek := (P ⊤ · · · (P ⊤ (P ⊤ P )off P )off · · · P )off
{z
}
|

S =I + γ(P ⊤ P )off + γ 2 (P ⊤ (P ⊤ P )off P )off + · · · +

⊤

➃

where x0 , · · · , x2k can be any nodes, but with no repetition
of nodes xi and x2k−i allowed, ∀i ∈ {0, 1, · · · , 2k} − {k}.
In comparison, the terms in Eq.(42) tally the paths of (43)
in G without having such a constraint on nodes xi and x2k−i .
Proof. By the power property of the adjacency matrix,

⊤
(P k ) P k i,j tallies the paths of (43) between i and j.
To show the terms in Eq.(41) tally the paths of (43) with
the additional constraint, we use induction on k as follows.
When k = 1, ((P ⊤ P )off )i,j = (P ⊤ P )i,j for i 6= j, and 0
for i = j. Thus, ((P ⊤ P )off )i,j tallies i ← x → j with i 6= j.
Assume that, for the fixed k, the term

To “rekindle” the semantic relationship between S and S̃,
let us first introduce the following notation:

k+1

➂

Figure 4: Different Paths Tallied by S and S̃

4.2 Semantic Relationship Between S and S̃

Rk = γ k (P ⊤ · · · (P ⊤ (P ⊤ P )off P )off · · · P )off +Rk−1 .
|
{z
}

k=2

➁

...

i (j)

Jeh and
Widom’s
SimRank
Sk

From the table, we can discern the following:
(a) S̃ does not preserve the relative similarity rankings of S;
(b) At least 4 out of top-10 rankings of S are affected by S̃.
Thus, neither of the statements by [7, 9] is correct.

Sk = γ(P ⊤ Sk−1 P )off + I.

➀

Li et al.’s
SimRank
Variation
S̃k

➂

➂

➃

➄

➅

i (j)

i (j)

=

(43)

i

k edges

89

j

➃

➅

−
i

j

i

j

➄

➅

i (j)

i (j)

−
i

j

i (j)

➅

+
i (j)

The following result shows the specific types of paths that
are tallied by S̃ but not by S.

(4) Algorithms. We implement the following, all in VC++.
Name
SR#
MSR
OIP
PSUM
SMAT
JSR
LSR
SR++
RS
RWR
COS

Corollary 2. Let P(S) and P(S̃) be the sets of paths
tallied by S and S̃, respectively. Then, P(S̃) ⊇ P(S), and
P(S̃) − P(S) is the set of “special” cycles of length 2k (k =
1, 2, · · · ), with first k contiguous edges oriented in one direction, and next k contiguous edges in the opposite direction.

5.

EXPERIMENTAL STUDIES

(5) Parameters. We set (a) γ = 0.6, as suggested in [12].
(b) k = 10, guaranteeing S (k) accurate to 2 decimal places.
(6) Evaluation Metrics. To evaluate the semantic quality
of the similarity search, we consider four metrics:
(a) Normalized Discounted Cumulative Gain at position p:
Pp
1
2reli −1
NDCGp := IDCG
i=1 log2 (1+i) , where reli is the graded
p
relevance at position i, and P
IDCGp is the ideal DCG ranking.

5.1 Experimental Setting
(1) Real Data. The details are described below:
Dataset
WikiV
CaD
CitH
WebN
ComY
SocL

|V |
7,115
15,683
34,546
325,729
1,134,890
4,847,571

|E|
103,689
55,064
421,578
1,497,134
2,987,624
68,993,773

|E|/|V |
14.57
5.31
12.20
4.59
2.63
14.23

Type
Directed
Undirected
Directed
Directed
Undirected
Directed

6

(a) WikiV, a Wikipedia who-votes-on-whom graph , where
nodes are users, and an edge i → j means user i voted on j.
(b) CaD, a collaboration graph, where each node is an
author, and edges co-authorships. The graph is derived from
6-year publications (2006–2011) in seven major conferences.
(c) CitH, a citation graph from arXiv high energy physics
theory, where each node is a paper labeled with meta information (e.g., title, authors, abstract) and an edge a citation.
(d) WebN, a web graph from University of Notre Dame,
where a node is a page (from nd.edu) and an edge a link.
(e) ComY, an undirected Youtube social graph, where a
node is a user and an edge a friendship.
(f) SocL, a friendship graph of a LiveJournal community,
where i → j is a recommendation of user j from user i.
(2) Synthetic Data. To produce SYN, we adopt a scalefree graph generator based on the Barabasi-Albert model9 .
This generator takes as input two parameters: (|V |, |E|).
(3) Query Generator. (i) To evaluate all-pairs s(∗, ∗), we
generate the query-pair set (A, B), by using two criteria:
(a) Importance coverage is to ensure the selected (A, B) to
comprehensively contain a broad range of any possible pairs.
To this end, we first sort all nodes in V in descending order
by PageRank (PR), and split them into 10 buckets: nodes
with PR ∈ [0.9, 1] are in the first bucket; nodes with PR ∈
1
|A|⌉
[0.8, 0.9) the second, etc. We next randomly select ⌈ 10
1
(resp. ⌈ 10 |B|⌉) nodes from each bucket to A (resp. B). Thus,
(A, B) has both important and non-important pairs.
(b) Overlapping coverage is to ensure that (A, B) contains
node-pairs with many multi-hop in-neighbors overlapped.
To achieve this, we first sort node-pair (a, b) in descending
P
|hop (a)∩hop (b)|
order via a scoring function:10 fa,b := 5k=1 |hopk (a)∪hopk (b)| .
k
k
We then split all pairs into 5 buckets: pairs with fa,b ∈ [4, 5]
are in the first bucket; pairs with fa,b ∈ [3, 4) the second, etc.
For each bucket, we next sort node-pair (a, b) in descending
P
order based on the value of ga,b := 5k=1 |hopk (a) ∩ hopk (b)|,
1
and select top ⌈ 5 |A||B|⌉ node-pairs from each bucket. Hence,
(A, B) covers node-pairs with many multi-hop in-neighbors
in common. (ii) Similarly, to evaluate single-source s(⋆, q),
the query set for q can be sampled as “importance coverage”.
8

n

d2

i
(b) Spearman’s ρ:= 1− n(ni=1
2 −1) , where di is the difference
of two ranks at position i, and n is the number of elements.
of discordant pairs)
(c) Kendall’s τ : = (# of concordant pairs)−(#
.
0.5n(n−1)
(d) Query coverage is the queries from our query sample.
(7) Ground Truth. (a) To label ground truth for similar
users on WikiV, a manual evaluation is carried out by 50
professional members who have accumulated a long history
of activity on Wikipedia. Each pair of users is considered by
an evaluator, and is assigned a score on a scale from 1 to 4,
with 1 meaning irrelevant, 4 meaning completely relevant,
and 2 and 3 meaning “somewhere in between”. The judgement is based on evaluator’s knowledge and public votes on
promotion of individuals to adminship. (b) To mark ground
truth labels for similar authors on CaD, 30 members from 5
database groups are invited. Each pair of authors is given a
score based on the collaboration distance between authors.
The judgement relies on evaluator’s knowledge and “separations” of Co-Author Path in Microsoft Academic Search.11
(c) To establish the ground truth of similar articles on CitH,
28 research associates from the School of Physics are hired.
Each pair of articles is assigned a score based on evaluator’s
knowledge on paper abstracts and citation relations.
All experiments are run with an Intel Core(TM) i7-4700MQ
CPU @ 2.40GHz CPU and 32GB RAM, on Windows 7.

8

9

Description
our scheme (“cosine” kernel + computation sharing)
the state-of-the-art SimRank [7]
all-pairs SimRank (fine-grained clustering) [13]
all-pairs SimRank (partial sums memoization) [12]
single-source SimRank (matrix decomposition) [3]
Jeh and Widom’s SimRank [5]
Li et al.’s SimRank [9]
SimRank++ (revised “evidence factor”) [1]
RoleSim (automorphism equivalence) [6]
Random Walk with Restart
classic cosine similarity

5.2 Experimental Results
5.2.1 Semantic Quality
We first evaluate the high semantic quality of SR# against
SR++ , JSR, LSR12 , RS, COS, RWR on real WikiV, CitH, CaD.
For each dataset, we randomly issue 300 queries for s(∗, q)
and s(∗, ∗) via importance coverage criterion, and use 3 metrics (NDCG, Kendall, Spearman) to evaluate each method,
respectively. Fig. 5a shows the average quantitative results.
(1) In all cases, SR# exhibits higher semantic quality than
the other methods. This is because SR# can avoid “connectivity trait” issue by utilizing a “cosine” kernel recursively
in a SimRank-like style, whereas COS considers only direct
overlapped in-neighbors, and JSR and LSR both have a “connectivity trait” problem. (2) In several cases, the NDCG200
of SR++ (on CitH) and RS (on CaD) may even worse than
that of JSR and LSR. This is because, for SR++ , its evi-

http://snap.stanford.edu/data/index.html
http://graphstream-project.org/doc/Generators/

11

10

12

All paths of length up to 10 between a and b can be tallied
by our queries, ensuring results accurate to 2 decimal places.

http://academic.research.microsoft.com/VisualExplorer

For semantics evaluation, MSR produces the same similarity values as LSR, since [7] approximates D by (1 − γ)I.

90

Ave. Spearman’s ρ

Ave. NDCG200

0.9
0.8
0.7
0.6
0.5

WikiV

CitH

SR++

JSR

RS

COS
1

1
0.8
0.6
0.4
0.2

CaD

WikiV

CitH

0.8
0.6
0.4
0.2

CaD

RWR

Query Coverage (%)

LSR (MSR)

Ave. Kendall’s τ

SR#
1

WikiV

CitH

1

0.6

SR# LSR JSR SR++ RS

WikiV

CitH

0.6
0.4
0.2

300 Sample Queries

0

4-5
3-5
2-5
1-5
# of Hops of In-neighbors (Depths)

CaD

4K
#

SR
MSR

3K
2K
1K
5

10 15 20 25
# of Iterations k

30

SR#
#

SR
MSR

200

|V | = 1M
k = 10

100
0

0.6




.

100

CitH

PSUM

OIP

×

×

×

WebN

SocL

ComY

(50000
cols)

(1000
cols)

(10000
cols)

WikiV

CaD

0.3



  
  

MSR
SR#
Est. Bound γ k+1

0.2
0.1


.

0


.
.

-655DQN

.


(k) LSR and JSR Relative Ordering (l) Ranking on WikiV

1

3

5 7 9 11 13 15
# of Iterations k

(m) (ǫdiag + ǫiter ) vs. k

SMAT
# of Iterations
k = 10

105
103
101
×

×

×

WebN SocL ComY WikiV CaD

CitH

(f) Time for All Pairs
1.5

SR∗

SR#
MSR
1
0.5

CitH

5

(i) Memory for Single Source/All Pairs

on SYN






WebN SocL ComY WikiV CaD

WebN SocL ComY WikiV CaD CitH

MSR

SR∗

(50000 (1000 (10000
cols) cols) cols)

×

102







top 50
top 200
top 500

×

0.4

0.9
0.8

×

k = 10


.


/655DQN

Kendall’s τ

1

|E|
|V |

×

104

2
5
10 15 20 25
Graph Density (|E|/|V |)

(g) Time vs. k on SocL (h) Time vs.

0.7

101

(e) Time for Single Source

300

Error

0

(d) Depth Coverage
Elapsed Time (sec)

Elapsed Time (sec)

(c) Overlapping Coverage

# of Iterations
k = 10

0.01

OIP

Memory (GB)

0

0.8

104

PSUM
Elapsed Time (Sec)

0.2

1

COS RWR

(b) Query Coverage

MSR

% of Node Pairs with
“Connectivity Trait”
Problem

0.4

SR#

RWR
Elapsed Time (Sec)

0.6

COS

Memory (MB)

300 Sample
Queries

RS

s

% of Sample Queries

1

0.8

SR++

JSR

s

% of Common Multi-hops
In-neighbors Coverage

LSR (MSR)

WikiV
CitH
CaD

0.4

CaD

(a) Semantics on Real Data (Measured by NDCG, Spearman’s ρ, Kendall’s τ )
SR#

300 Sample Queries

0.8

10
15
20
# of Iterations k

25

(j) Memory vs. k on SocL
1

0.8
0.6
0.4

82.7%

78.1%
62.3%

56.4%
44.4% 41.5%

0.2
0

WebN SocL ComY WikiV CaD CitH

(n) % of “Connectivity Trait” Issues

Figure 5: Performance Evaluations on Real and Synthetic Datasets
To further evaluate the search depth of SR# , SR++ , JSR,
LSR, RS, COS, RWR, we first apply overlapping coverage
criterion to generate 2,000 queries, and then generate 300
queries via importance coverage criterion, and classify them
into 4 groups, e.g., “4-5” collects queries (a, b) whose path
length between a and b is 8–10. Fig. 5d depicts the search
depth of all the methods on WikiV. (1) For each group, SR++
and COS have the lowest quality of depth search among all
the methods, since they cannot capture the paths of length
> 2 between nodes. (2) SR# achieves the highest quality,
and its superiority is more pronounced in the groups that
have longer paths. This tells us that the “connectivity trait”
issue has a large influence on node-pairs with long paths.

dence factor will be 0 whenever there are no common direct
in-neighbors; for RS, automorphism equivalence has priority
over connectivity in similarity assessment. Thereby, SR++
and RS may not resolve the “connectivity trait” problem.
Fig. 5b compares the percentage of queries from the 300
queries sample (based on importance coverage criterion) that
SR# , SR++ , JSR, LSR, RS, COS, RWR provide similarities
for on real WikiV, CitH, CaD, respectively. For each dataset,
SR# substantially improves the coverage of JSR/LSR (∼0.73)
and SR++ /RS (∼0.81) to ∼0.95. This can be considered as
expected, since (a) the “connectivity trait” problem of JSR
and LSR will downgrade similarities of node-pairs with high
connectivity, and (b) SR++ can only partially fix the “connectivity trait” issue within 1-hop in-neighborhood.
Fig. 5c depicts the percentage of queries with overlapped
multi-hop in-neighbors from the 300 queries sample (via
overlapping coverage criterion) that SR# , SR++ , JSR, LSR,
RS, COS, RWR are able to identify on real WikiV, CitH, CaD,
respectively. (1) For each dataset, SR# significantly achieves
∼0.95 coverage of common multi-hop in-neighbors, much superior to JSR/LSR (∼0.20), SR++ (∼0.51) and COS(∼0.41).
The reason is that our “cosine” kernel provides an appropriate normalization factor k ∗ k2 that can recursively fix the
“connectivity trait” problem. In contrast, the k ∗ k1 normalization factor of JSR/LSR excessively squeezes the range
of similarity [0, 1]. (2) Under overlapping coverage criterion,
COS (∼0.41) outperforms JSR/LSR (∼0.20) since COS is not
limited by the “connectivity trait” problem.

5.2.2 Time Efficiency
Fig. 5e illustrates the running time of SR# , MSR, PSUM,
OIP, SR∗ , SMAT for single-source s(∗, q) on 6 real datasets.
(1) In all cases, SR# always substantially outperforms the
other methods. This is because SR# can eliminate duplicate
computations for maximal sharing, whereas MSR computes
each term separately. (2) PSUM, OIP, SR∗ , SMAT will crash
on large WebN, SocL, ComY, due to the memory allocation.
On WikiV and CaD, they are 3-4 orders of magnitude slower
than SR# , since their iterative models to compute s(∗, q)
rely on all-pairs outputs of the previous iteration.
Fig. 5f shows the time of SR# , MSR, PSUM, OIP, SR∗
for all-pairs s(∗, ∗) on 6 real datasets. (1) Only SR# , MSR
survive on all datasets, whereas PSUM, OIP, SR∗ fail on large

91

to be 0 when k increases. (2) The SR# curve is always below
the Est. Bound curve, showing the correctness of Theorem 4.
Fig. 5n statistically shows the percentage of node-pairs
with the “connectivity trait” problem over all real datasets.
From the results, we see that the percentages are all high
(e.g., 82.7% on WebN, 62.3% on SocL, 78.1% on ComY),
showing the seriousness of this problem in real scenarios.

WebN, SocL, ComY, due to the memory allocation. (2) MSR
is slower than others as it sacrifices speed for scalability. In
contrast, SR# not only scales well on large graphs, but also
has comparable speed to those of PSUM, OIP, SR∗ .
Fig. 5g presents the impact of the iteration number k on
the time of SR# and MSR. When k grows from 5 to 30, the
time of SR# does not increase significantly (just from 42s
to 301s), as opposed to the time of MSR growing from 152s
to 3744s. The reason is that MSR contains many duplicate
computations among different iterations, whereas SR# can
merge these results after rearranging the computation order.
It is consistent with our analysis in Subsection 2.4.
Fig. 5h demonstrates the impact of network density on
the computational time of SR# and MSR on synthetic data.
Fixing |V | = 1, 000, 000 and k = 10, we generate a synthetic
dataset by increasing the graph density from 2 to 25. (1)
When the density increases, the time of both algorithms will
increase. (2) For dense graphs, the speedup for SR# is significantly higher than MSR, due to the number of iterations
with a huge influence on MSR compared with SR# . This is
in agreement with the complexity of SR# and MSR.

6. CONCLUSIONS
We consider the problem of high-quality similarity search.
Observing that (1) the best-of-breed SimRank [7] produces
diagonal correction error ǫdiag and (2) SimRank++ [1] does
not well resolve the “connectivity trait” problem, we proposed our scheme, SR# . First, we characterize the exact D,
and devise a “varied-D” model to compute SimRank with
no ǫdiag in linear memory. We also speed up the computational time from quadric [7] to linear in terms of k. Next,
we devise a “kernel-based” model to circumvent the “connectivity trait” problem. Finally, we give new insights into
the semantic difference between Jeh and Widom’s SimRank
and its variant, and correct an argument in [7]. We empirically show that SR# (1) improves an accuracy of average
NDCG200 by ∼30% on real graphs, and (2) can be ∼10x
faster than [7] on SocL with 65.8M links for 1000 queries.

5.2.3 Memory Efficiency
Fig. 5i shows the memory of SR# , MSR, PSUM, OIP, SR∗
on six real datasets. (1) For large WebN, SocL and ComY,
only SR# and MSR survive, highlighting their scalability.
(2) For each dataset, SR# requires slightly more memory
than MSR because it requires to store Dk .
Fig. 5j reports the impact of the iteration number k on the
memory of SR# and MSR on SocL. (1) When k varies from
5 to 25, the memory requirements of SR# and MSR increase,
since they need to memorize the k intermediate vectors from
previous iterations, as expected. (2) The disparity in the
memory between SR# and MSR is due to storing Dk .

Acknowledgment. This work forms part of the Big Data
Technology for Smart Water Network research project funded
by NEC Corporation, Japan.

7. REFERENCES
[1] I. Antonellis, H. G. Molina, and C. Chang. SimRank++:
Query rewriting through link analysis of the click graph.
PVLDB, 1(1), 2008.
[2] D. Fogaras and B. Rácz. Scaling link-based similarity
search. In WWW, 2005.
[3] Y. Fujiwara, M. Nakatsuji, H. Shiokawa, and M. Onizuka.
Efficient search algorithm for SimRank. In ICDE, 2013.
[4] G. He, H. Feng, C. Li, and H. Chen. Parallel SimRank
computation on large graphs with iterative aggregation. In
KDD, 2010.
[5] G. Jeh and J. Widom. SimRank: A measure of
structural-context similarity. In KDD, 2002.
[6] R. Jin, V. E. Lee, and H. Hong. Axiomatic ranking of
network role similarity. In KDD, 2011.
[7] M. Kusumoto, T. Maehara, and K. Kawarabayashi.
Scalable similarity search for SimRank. In SIGMOD, 2014.
[8] P. Lee, L. V. S. Lakshmanan, and J. X. Yu. On top-k
structural similarity search. In ICDE, 2012.
[9] C. Li, J. Han, G. He, X. Jin, Y. Sun, Y. Yu, and T. Wu.
Fast computation of SimRank for static and dynamic
information networks. In EDBT, 2010.
[10] P. Li, H. Liu, J. X. Yu, J. He, and X. Du. Fast single-pair
SimRank computation. In SDM, 2010.
[11] Z. Lin, M. R. Lyu, and I. King. MatchSim: A novel
similarity measure based on maximum neighborhood
matching. Knowl. Inf. Syst., 32(1), 2012.
[12] D. Lizorkin, P. Velikhov, M. N. Grinev, and D. Turdakov.
Accuracy estimate and optimization techniques for
SimRank computation. VLDB J., 19(1), 2010.
[13] W. Yu, X. Lin, and W. Zhang. Towards efficient SimRank
computation on large networks. In ICDE, 2013.
[14] W. Yu, X. Lin, and W. Zhang. Fast incremental simrank
on link-evolving graphs. In ICDE, pages 304–315, 2014.
[15] W. Yu, X. Lin, W. Zhang, Y. Zhang, and J. Le.
SimFusion+: Extending SimFusion towards efficient
estimation on large and dynamic networks. In SIGIR, 2012.
[16] W. Yu and J. A. McCann. Sig-SR: SimRank search over
singular graphs. In SIGIR, 2014.
[17] W. Yu and J. A. McCann. Efficient partial-pairs SimRank
search for large networks. PVLDB, 8(5):569–580, 2015.

5.2.4 Relative Order
Fig. 5k compares the relative order between LSR and JSR
for the top K results on 6 real datasets (K = 50, 200, 500).
The order gap is measured by Kendall’s τ . (1) For different
graphs, the quality of the relative order is irrelevant to top K
size. For instance, on SocL, top 500 (0.94) is better preserved
than top 200 (0.91) and top 50 (0.9), whereas on CaD, top
500 (0.84) is worse than both top 200 (0.9) and top 50 (0.92).
(2) On each dataset, the average Kendall’s τ for top 50 is
0.77–0.92, which indicates that LSR does not maintain the
relative rank of JSR, even for top 50. Thus, approximating
D by (1 − γ)I would adversely affect the top K ranking.
Further, a qualitative result on WikiV is depicted in Fig. 5l,
where x (y) axis is the ranking by JSR (LSR). Other datasets
also statistically exhibit similar phenomena. (1) Many points
below the diagonal imply that low-ranked node-pairs by JSR
have greater likelihood to get promoted to a high rank of
LSR. This association does not imply a (near) linear relationship between the rankings of JSR and LSR. (2) For high
top-K ranking (e.g., K = 15), the top 15 of JSR may be
inconsistent with those of LSR. Hence, the relative order
preservation of JSR and LSR hinges on network structure.
Fig. 5m tests (ǫdiag + ǫiter ) of MSR and SR# w.r.t. k. (1)
When k increases from 1 to 15, the error of each algorithm
decreases. While the error of SR# approaches 0, MSR levels
off at 0.28. The large disparity between their convergent
solutions is due to the approximation of D by (1 − γ)I in
MSR; our “varied-D” iterative model can guarantee the error

92

