In Situ Insights
Yuanhua Lv

Ariel Fuxman

Microsoft Research
Redmond, WA 98052, USA

∗

Google Inc.
Mountain View, CA 94043, USA

yuanhual@microsoft.com

afuxman@gmail.com

ABSTRACT

General Terms

When consuming content in applications such as e-readers,
word processors, and Web browsers, users often see mentions
to topics (or concepts) that attract their attention. In a
scenario of significant practical interest, topics are explored
in situ, without leaving the context of the application: The
user selects a mention of a topic (in the form of continuous
text), and the system subsequently recommends references
(e.g., Wikipedia concepts) that are relevant in the context
of the application. In order to realize this experience, it
is necessary to tackle challenges that include: users may
select any continuous text, even potentially noisy text for
which there is no corresponding reference in the knowledge
base; references must be relevant to both the user selection
and the text around it; and the real estate available on the
application may be constrained, thus limiting the number of
results that can be shown.
In this paper, we study this novel recommendation task,
that we call in situ insights: recommending reference concepts in response to a text selection and its context in-situ
of a document consumption application. We first propose
a selection-centric context language model and a selectioncentric context semantic model to capture user interest. Based
on these models, we then measure the quality of a reference
concept across three aspects: selection clarity, context coherence, and concept relevance. By leveraging all these aspects,
we put forward a machine learning approach to simultaneously decide if a selection is noisy, and filter out low-quality
candidate references. In order to quantitatively evaluate our
proposed techniques, we construct a test collection based on
the simulation of the in situ insights scenario using crowdsourcing in the context of a real-word e-reader application.
Our experimental evaluation demonstrates the effectiveness
of the proposed techniques.

Algorithms

Keywords
In situ insights, entity recommendation

1. INTRODUCTION
When consuming content in applications such as e-readers,
word processors, and Web browsers, users often see mentions
to topics (or concepts) that attract their attention. In a
scenario of significant practical interest, topics are explored
in situ, without leaving the context of the application: The
user selects a mention of a topic (in the form of continuous
text), and the system subsequently recommends references
(e.g., Wikipedia concepts) that are relevant in the context
of the application.1 We call this capability in situ insights.
As an example, consider a user who is reading an article on Health Hazard. At some point, the user may select
the term “antibiotics”. Figure 1 shows a screenshot of the
resulting in situ insights experience. Recommended references include the Wikipedia concepts “Antibacterial” 2 and
“Antibiotic misuse” 3 .
In order to realize the in situ insights experience, a number
of challenges must be faced. First, users may select any
continuous text, even potentially noisy text for which there is
no corresponding reference in the knowledge base. (See [20]
for a discussion on users selecting text “unconsciously”). In
such cases, the in situ insights system should generally avoid
recommending any result. Back to the screenshot in Figure
1, suppose the user unconsciously selects “conjunction” in
the third line. It is unclear what result would be relevant to
the selection, and it is probably best not to show any result.
Second, the in situ insights system should understand not
only the text selection but also the context of the document
that the user is reading. For instance, consider a user who is
reading an article on History of Antibiotics and selects the
term “antibiotics”. It would now make sense to recommend
the reference “Alexander Fleming” 4 instead of the “Antibiotic misuse” reference recommended in Figure 1. However,
context may not always be useful, and may even be harmful
if it is incoherent with the text selection [30]. Moreover, although the reading context is critical in the in situ insights
experience, revealing it to an external recommender system
may raise privacy concerns or even legal problems, as the

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information
Filtering
∗This work was done while the author was at Microsoft Research.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
SIGIR’15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767696.

1

Previous work [20] has shown that “text selection”, by dragging the
mouse pointer, double clicking the text, etc., is one of the most precise
indicators of user interest.
2
http://en.wikipedia.org/wiki/Antibacterial
3
http://en.wikipedia.org/wiki/Antibiotic misuse
4
http://en.wikipedia.org/wiki/Alexander Fleming

655

Disease in humans can be caused by either bacteria or
viruses. A bacteria is a living, single-celled organism, while a
virus is only living in conjunction with a host and is
completely unresponsive to antibiotics. So, it is the reaction of
bacteria, not viruses, to antibiotics that poses a hazard to
human beings. This does bring to light the frequent
inappropriate use of antibiotics in “treating” the common
cold which is caused by a virus. Using antibiotics to battle the
cold is done in vain, the only thing to be done in this case
would be to let it run its course. By using antibiotics at this
inappropriate time, the only effect on the body is the
increased destruction of beneficial bacteria.

Antibacterial
An antibacterial is an agent that
inhibits bacterial growth or kills
bacteria. The term is often used
synonymously with the term
antibiotic(s). Today, however, with
increased knowledge of the
causative agents of various
infectious diseases, …

Antibiotic misuse
Antibiotic misuse, sometimes called
antibiotic abuse or antibiotic overuse,
refers to the misuse or overuse of
antibiotics, with potentially serious
effects on health. It is a contributing
factor to the creation of multidrugresistant bacteria, …

Figure 1: In situ insights experience.

2. RELATED WORK

document may be a private email or enterprise confidential
documentation. In this paper, we address both issues.
Finally, since the problem is defined in situ of a document consumption application, the “real estate” (display
space) available for the recommendation results may be constrained. As a result, it is acceptable, and occasionally even
desirable, to limit the number of results shown for a given
text selection. This is in contrast to conventional information retrieval systems which always return results if the corpus contains documents lexically related to the query (e.g.,
some query terms appear in the document).
In this paper, we study this novel recommendation task,
that we call in situ insights: recommending reference concepts in response to a text selection and its context in-situ
of a document consumption application. Our contributions
include:

2.1 Content Recommendation
Recommendation has been studied extensively in the past.
Content-based filtering and collaborative filtering are the
two main types of recommendation techniques that have
been considered [1]. In situ insights is a new instance of
content-based filtering. Content-based filtering, e.g., [34,
42], typically focuses on recommending documents that reflect users’ long-term interests (e.g., a user might generally
like sports). In contrast, our work recommends content related to users’ ad hoc interests implied by the text selection
when reading a document.
Related content recommendation (e.g., [28]), cumulative
citation recommendation (e.g., [4, 44]), contextual citation
recommendation(e.g., [19]), and contextual advertising (e.g.,
[33]) are also in the direction of ad hoc content-based filtering. They recommend related content, such as news articles,
Wikipedia articles, academic papers, Web documents or ads,
to a target document. Of particular interest is entity-centric
document filtering [44], where, given an entity represented
by its Wikipedia page, the task is to find relevant documents
with respect to the input entity. These works are based on a
problem formulation that is insufficient for in situ insights,
as they do not allow for a text selection as part of its input.
There are other applications that also attempt to enhance
the reading experience by bringing to bear other content,
e.g., enriching Web tables with entities [26] and enriching
textbooks with images [2]. They also differ with in situ insights in that they do not take user interaction into account.
Inline citation recommendation proposed by Livne et al. [27]
is arguably the most related work to ours. Their setting can
be regarded as a special case of in-situ recommendation that
focuses on recommending reference papers to an academic
paper being authored by a user. In contrast, in situ insights
is an open domain problem, in which there are no restrictions
on the type of user selections or references to be retrieved.
Our recent poster paper [15] gives a high-level overview of
a recommendation system, called Leibniz, that was powered
by the situ insights algorithm that we present and evaluate in
this work. In another recent work, we have further explored
in situ insights by exploiting the Wikipedia graphs [25].

• We explore the in situ insights problem, frame it as the
task of automatically predicting whether or not a candidate reference is contextually relevant, and motivate
a client-side solution to relax the privacy concerns.
• We conduct a user study using crowdsourcing by simulating the environment of a reader consuming content
in an application that has in situ insights capabilities,
and construct a dataset5 for training and evaluating in
situ insights techniques.
• We present a selection-centric context language model
and a selection-centric context semantic model for modeling and understanding context.
• We propose three aspects (context coherence, selection
clarity and reference relevance) for measuring context
quality, detecting noisy selections, and computing the
relevance of a reference concept, respectively. We then
take these three aspects as a roadmap, and design and
compare a set of features. Finally, we develop a filtering algorithm to leverage the complementary relative
strengths of various features, by employing a state-ofthe-art machine learning algorithm.
• We conduct a thorough experimental evaluation, which
shows the effectiveness of the proposed technique. We
also show that regression plus filtering works more effectively than ranking for in situ insights.
5

2.2 Information Retrieval
Our work is related to context-sensitive search, such as
IntelliZap [12], Y!Q [24, 23], UCAIR [38], and the recent

research.microsoft.com/en-us/people/yuanhual/insights.aspx

656

work by Sun and Lou [40]. All these context-sensitive search
systems and our system leverage general-purpose search engines, and exploit contextual information to postprocess the
search results. However, a significant difference is that the
problem at hand involves dealing with entity recommendation, while these related works are in the scope of Web
search. Moreover, conventional information retrieval in general, and context-sensitive search [12, 24, 23, 38] in particular, always return results if the corpus contains documents
lexically related to the query. In contrast, our problem is
defined in-situ of a document consumption application, and
thus it is acceptable, and occasionally even desirable, not to
show any results (or to limit the number of results) for a
given text selection. This need to decide “whether to swing”
has been observed in many other application domains; an
eloquent argument in the online advertising domain can be
found in [6]. In any case, as a point of comparison throughout our paper, we use as a baseline a context-sensitive search
(re-ranking) system as done in [12, 24, 23, 38], in order to
highlight the need for our proposed methods.
Entity search [32, 5] and related entity finding [3] are also
related to our work. Entity search aims to answer a query
with direct entity answers extracted from documents or entity records in a knowledge base; related entity finding takes
as input a query entity, the type of the target entity, and the
nature of their relation, and outputs a ranked list of related
entities. Besides, another two related problems are named
entity linking (e.g., [11]) and Wikipedia cross-reference (e.g.,
[31]), where the goal is to disambiguate the mention of an
entity in unstructured text to the entity’s record in some
knowledge base. However the task at hand is principally different: recommending related entities (often more than one)
that are related to (beyond the notion of “linking”) any text
selection (which can be any continuous text, including but
not limited to entity mentions) in the context of the document being consumed.

In situ insights. Given a selection s and a context c, the
problem of in situ insights consists of the following 3 steps:
First, identify a set of candidate references D(s, c). This
is a typical information retrieval task; and the retrieval system must be scalable and fast. Moreover, our goal is to
recommend relevant reference concepts for a selection from
a knowledge source, in a way similar to recommending citations for a research paper, so we choose Wikipedia as our
target corpus, which provides a reasonably large number of
concepts with high quality. We thus leverage a major commercial search engine to obtain candidate results, where each
query is appropriately site-restricted by adding an operator
site:en.wikipedia.org.
Second, design a prediction function to score each candidate as h(d|s, c, D(s, c)), where d ∈ D(s, c). This is the focus
of this paper, and will be discussed in detail later.
Third, recommend d, if h(d|s, c, D(s, c)) ≥ τ , where τ is
a pre-defined threshold; discard d otherwise. By setting the
threshold τ very high, we will only show references in a small
number of cases where the reference quality is very high. On
the other hand, if the threshold is set very low, then references will be shown for many text selections, although the
quality of the references would not be as good. Therefore,
the threshold should be considered a parameter that can be
used in conjunction with a well-accepted evaluation metric
to determine whether or not to show a reference.
There is another important issue regarding the in situ insights experience: privacy. When retrieving candidate references, if we send much context to an external search engine,
it may raise privacy concerns or even legal problems, since
the context may be a private email or enterprise confidential
documentation. To relax this issue in our insights system,
inspired by the idea of client-side personalized search [38],
we only send to the search engine a query purely based on
the text selection, while leaving all the context-related computation at the client-side to postprocess the search results.

4. DATASET CONSTRUCTION

3. PROBLEM FORMULATION

In this section, we first present a crowdsourcing exercise
that we conducted in order to collect text selections and contexts by simulating the environment of a reader consuming
content in an application that has in situ insights capabilities. We then describe the construction of our evaluation
dataset.

We next define the problem of in situ insights.
Selection or Text Selection. A selection s is a mention
to a topic or concept that attracts a user’s attention, and for
which the user would like to explore more and gain insights
about. It can be a word, phrase or any span of continuous
text in the article that the user is reading. In our example in
Figure 1, the selection is “antibiotics”. Table 1 shows more
examples of selections chosen by crowd annotators.
When reading an article, a user can focus his/her attention
in various ways, such as text tracing, link pointing, text
selection, and text highlighting [20, 8]. In situ insights can
potentially take any of these actions as input. Without loss
of generality, we mainly employ “text selection” to describe
the scenarios, as it is one of the most precise indicators of
user interest [20].
Context. The context c represents the content that the
user is consuming. In Figure 1, the context consists of the
article on Health Hazard. In this paper, we will evaluate
an ad hoc context – the text around the text selection. We
leave long-term context, such as all the text that the user
has read so far, as future work.
Reference, Concept, or Document. A reference d
can be any concept, entity, or topic. In this paper, we use
Wikipedia concepts/documents. In Figure 1, there are two
references recommended: the Wikipedia concepts ‘Antibacterial” and “Antibiotic misuse”.

4.1 Crowdsourcing Text Selections
We employed a corpus consisting of all English textbooks
from the Wikibooks site (2600 books in total). The textbooks cover a broad spectrum of topics, such as engineering,
humanities, health sciences, and social sciences. We randomly sampled 500 books from the corpus, and one paragraph from each book. The 500 paragraphs were then shown
to each of the 100 annotators via crowdsourcing, using a
UI that shows the paragraph in the context of the page of
the book where it appears. To acquire the text selections,
the annotators were asked to select from the paragraph any
mentions of topics or concepts for which they would like to
see additional references from an external knowledge source
(such as Wikipedia), and they could choose not to make any
selection if they did not find anything interesting. Table 1
shows some representative examples of the user selections we
collected.
As a result of the crowdsourcing exercise, each paragraph
was assigned 13.3 selections on average (we kept the selections made by at least two annotators); 2.2 out of these

657

Probability of being Perfect/Good/Bad

0.9

1

Bad
Good
Perfect

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

Bad
Good
Perfect

0.9
Probability of being Perfect/Good/Bad

Creators of Java or C#
antibiotics
Byzantine literature
zeroed out
Punya Mishra
Sir Martin Evans
ISO
Python and C++ don’t need interfaces

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

0

5

10

15

20

25

0

1

2

3

Position

4

5

6

7

8

Position

Table 1: Examples of user selections.
Figure 4: Relevance label distributions for regular
(left) and random (right) selections respectively.

100000

Num of selections

10000

1000

#selections
avg(#words)
stdev(#words)
#perfect
#good
#bad

100

10

1

1

10

Regular Selections
490
3.02
10.22
1353
2165
6477

Random Selections
299
1
0
219
177
1923

100

Num of annotators choosing the selection

Table 2: Dataset characteristics.

Figure 2: Power-Law distribution of selections.

sis of the crowsourced results showed that there on average
7.2 perfect/good results in the top-25 documents. For each
random selection, we only sent the top-8 documents for relevance labeling, since based on our analysis in Figure 4, we
can safely assume any result after the eighth position has a
negligible probability of being relevant.
We had each <selection, document> pair labeled by ten
crowd labelers using a user interface that we designed for
that purpose: on the left hand side, a page with the selected text; and on the right hand side, the corresponding Wikipedia document. The labelers were provided with
appropriate guidelines and asked to answer a succession of
questions, such as:

13.3 selections were chosen by at least 10 annotators. For
78.8% of the paragraphs, there is at least one selection that
was chosen by at least 10 annotators. On the one hand,
this indicates a reasonable level of agreement, given that the
annotators can choose any continuous text from the entire
paragraph (and they can also choose not to select anything).
On the other hand, there is still a large number of long-tail
selections, which is evidenced by the power-law distribution
of the selection frequency in Figure 2.
We sampled 500 distinct selections proportional to frequency. We refer to them as “regular selections”. In addition, we create another set to account for the fact that in
actual reading environments users may select text “unconsciously” [20] (i.e., random text selections for which they may
not necessarily expect a reasonable result). When selections
are made by clicking with a mouse or tapping with a finger,
it is reasonable to assume that these selections will tend to
be single words. To simulate these selections, we randomly
sampled another 300 words as “selections” from the 500 paragraphs proportional to word frequency. We refer to this set
as “random selections”. Finally, we combined the two sets
and constructed a dataset with 800 selections in total.

• Is this the result that you expected?
• Do you see a connection between this result and the
selected text?
• Would you like to explore the article in the right hand
side?
The questions were then mapped to three relevance labels:
“Perfect”; “Good” and “Bad”. If the result was what the
labeler expected, then it was labeled as “Perfect”; if the labeler could find a connection, and she was willing to explore
the result, then it was labeled as “Good”; otherwise, it was
labeled as “Bad”.
We followed the majority voting method [22] to aggregate
labels from multiple crowd labelers, but adjusted it empirically to the characteristics of our task: (1) if the majority
of the labels is “Bad” we consider the result to be “Bad”; (2)
otherwise, if the number of “Perfect” labels is larger than
the number of “Good” labels, we consider the result to be
“Perfect”; (3) otherwise, we consider the result to be “Good”.
We plot the label distribution in Figure 4, which shows that,
although the search engine has done a good job to rank documents, it is not satisfactory for in situ insights due to the
existence of many “Bad” results on top.
We summarize the dataset characteristics in Table 2, including the number of selections with relevance judgments,
the average and standard deviation of the length of selections, and the total numbers of “Perfect”, “Good” and “Bad”
judgments in each dataset.

4.2 Dataset Construction
After running each selection as a query through a commercial search engine, we obtained a set of (selection, document)
pairs. A document is comprised of three components: title,
url and snippet, as shown in Figure 3.

Figure 3: A sample document (i.e., search result).
Overall, we retrieved Wikipedia documents for 490/500
regular selections and 299/300 random selections. There
were a few selections for which the search engine did not
return any result. In the following sections, we only considered these 490 regular selections and 299 random mentions.
For each regular selection, we sent the top-25 documents,
if any, to crowd labelers for relevance labeling. An analy-

658

0.045

different from the conventional keyword queries, the selection
s as a whole often represents a single semantic unit in which
there is strong dependency among the words. Considering
this, when computing p(s|c, i), we take the selection s (which
may consist of multiple words) as a single “term”,
Q rather than
assuming word independency and computing w∈s p(w|c, i)
instead. This was also shown to work better than the bagof-words representation of s in our task.
We choose the Gaussian kernel to implement the context
language model, as it is consistent to the curves observed in
Figure 5 and was also shown to be effective in [29, 30]:

Perfect
Good

Probability in Perfect/Good documents

0.04

0.035

0.03

0.025

0.02

0.015

0.01

0.005
-100

-80

-60

-40

-20
0
20
Distance from the text selections

40

60

80

100

Figure 5: Relevance of a context term against its
proximity to the text selection.

p(s|c, i) =

5.1 Selection-Centric Context Language Model
It has been shown in the information retrieval literature
(e.g., [30]) that terms closer to the occurrences of the query
(in analogy to the text selection) in a document are more
likely to be related to the query. To examine this heuristic
in the in situ insights problem, we compute the probability of
every context term occurring in Perfect/Good documents6 ,
and plot the average probability of terms with the same distance to their corresponding text selections with respect to
the distance in Figure 5. As we can see, this verifies the
proximity heuristic that the relevance of a context term decreases with the distance to the text selection.
This motivates us to factor in the selection s in the estimation of the context language model. Specifically, we
estimate the conditional probability p(w|s, c) in terms of the
joint probability of observing a term w with the selection s
at every position given the context c. Formally,
p(w, s, i|c)

(1)

i=1

where i is a random variable which indicates a “term” position in context c, and |c| is the total number of positions in c.
We then factor the probability p(w, s, i|c) in a way similar to
[30]. Formally, p(w, s, i|c) = p(i|c) · p(s|c, i) · p(w|c, i), where we
1
assume that every position is equally likely, i.e., p(i|c) = |c|
.
Then, we obtain the following estimate:
p(w|s, c) ∝

|c|
X

1{c[j] = s} exp
√
2πσ2

h

−(i−j)2
2σ 2

i

(3)

5.2 Selection-Centric Context Semantic Model

Context may not be always useful, and at times may even
be harmful if it is incoherent to the text selection [30]. Thus,
one key to the in situ insights problem is properly modeling the context around a text selection. In this section,
we present a selection-centric context language model and a
selection-centric context semantic model, which will be used
to compute several important features in Section 6.

|c|
X

j=1

where the standard deviation σ in the Gaussian kernel is set
to 60 empirically.

5. MODELING CONTEXT

p(w|s, c) ∝ p(w, s|c) =

P|c|

p(s|c, i) · p(w|c, i)

(2)

One potential problem to apply this context language model
to score each reference document is that a document is very
short (see the snippet in Figure 3), which may make the
score sub-optimal due to the word mis-matching problem.
To address this problem, we adopt a state-of-the-art semantic analysis technique, namely Explicit Semantic Analysis
(ESA) [16, 17], to map the context (as well as the text selection and the document) into the semantic space. ESA
represents a word w as a weighted vector of actual (explicit)
Wikipedia concepts V(w). That is, the meaning of a word is
given by a vector of concepts paired with the relevance score
of each concept to the word. And then, a text fragment
can be represented as the centroid of the vectors representing its words. So the relatedness of any pair of texts can
be computed as the cosine similarity between their concept
vectors.
We build an ESA index using a Wikipedia dump (English) from March 2014, which contains 700, 514 Wikipedia
concepts after preprocessing and pruning the noise from the
total 10, 836, 201 concepts using the heuristic rules proposed
in [17]. Note that the ESA index can be regarded as a
highly compressed inverted index, but it is not appropriate
for directly computing the relevance score of every candidate reference (which can be any Wikipedia article), due to
the low coverage (6.5%) of the index, in particular for fresh
Wikipedia articles. To assess the quality of our ESA index, we apply it to compute word relatedness on the widelyaccepted WS-353 benchmark dataset [12], which contains
353 word pairs, and our experiments show a Spearman’s rank
correlation of 0.735, which is consistent to the previously reported numbers [16, 17].
With both the ESA index and the proposed selectioncentric context language model p(w|s, c), we can compute
a selection-centric context semantic vector V(s, c) based on
the centroid of the semantic vector of each term. Formally,
V(s, c) =

i=1

X

w∈c

p(w|s, c) · V(w)

(4)

Although using ESA for semantic matching is not entirely
novel, we are the first to leverage the term proximity evidence when computing the ESA vector.

where p(w|c, i) is the probability of sampling term w at position i in context c. For efficiency reasons, p(w|c, i) is simply
calculated using an indicator function 1{c[i] = w}, which is
1 if w occurs at position i in c, and 0 otherwise.
The other term p(s|c, i) is the key component to model the
term proximity and position information, which can be estimated using the positional language model [29]. However,

6. PREDICTING REFERENCE QUALITY
We model reference quality from three aspects: the coherence of the context, the clarity of the selection, and the
relevance of the reference with respect to the selection and
the context.

6

That is, the ratio of the number of Perfect/Good documents containing the term to the total number of Perfect/Good documents

659

6.1 Context Coherence

IDF is a useful signal for predicting query performance. We
use maximum IDF, average IDF and the standard deviation
of IDF.
The above features are all pre-retrieval features without
using the results from the search engine. After receiving the
search results, we can also explore post-retrieval features.
One well-known post-retrieval query performance predictor
is the relative entropy of the language model estimated on
the search results to the background language model p(w|B),
namely query clarity [10]. We employ this idea to measure the clarity of the document set D. Specifically, we
first estimate a standard multinomial language model based
on the maximum likelihood estimator and Formula 9, formally p(w|D) = P ′ f (w,D)
. Then a new feature Docuf (w′ ,D)
w ∈D
mentSetClarity can be computed as follows:

Intuitively, if the context is more coherent, it tends to be
more consistent with the text selection. Therefore, a more
coherent context may be trusted more than a less coherent
one.
One classic method to measure text coherence is using the
entropy of its language model. If context c is about a clear
and coherent topic, the language model is usually characterized by large probabilities for a small number of topical
terms, while if c is non-coherent (e.g., only consisting of uncorrelated words), the language model would be smoother.
Since the classic entropy may be dominated by common
terms (e.g., ‘the’, ‘and’, ...), we instead compute a relative
entropy [10], which essentially measures the language usage
associated with c as compared to the background language
model p(w|B) of the whole collection. With the selectioncentric context language model p(w|s, c) proposed in Section 5.1 (Equation 2), we can now compute the relative entropy of context, which we denote as ContextClarity:
X
p(w|s, c)
clarity(c) =
p(w|s, c) log
(5)
P (w|B)
w∈V

clarity(D) =

p

L(s, p)
L(s, p)
log2 P
′)
′
L(s,
p
′
p
p′ L(s, p )

(7)

6.3.1 Syntactic Similarity
We first present several features that use a standard retrieval function, i.e., cosine similarity, to compute how a document is syntactically relevant to the text selection and the
context. The cosine similarity sim(Q, D) between a “query”
Q and a “document” D is computed as follows:
P

sim(Q, D) = qP

The motivation of modeling the clarity of the selection s
is that, if s is unclear/ambiguous/noisy, we may rely more
on the context, or decide not to show any results for this s.
We present one feature by exploiting the anchor texts.
Specifically, we calculate the entropy of the distribution of
the links for which s is the anchor text, and denote it as
SelectionEntropy:
P

p(w|D)
P (w|B)

6.3 Reference Relevance

6.2 Selection Clarity

X

p(w|D) log

w∈V

where generally the higher the coherence, the larger the ContextClarity score.
Arguably, if the selected text s repeats in many sentences
in the context, the context is more likely to be coherent with
the selected text. We thus propose another feature using the
number of the sentences that contain the selected text.
Yet in some cases, even if the selected text s occurs in
many sentences, it is still unclear whether or not the remaining sentences that do not contain s are also coherent with s,
since it might be the case that the first half of the context
contains s and is more coherent, whereas the second half
does not contain s and is about a different topic. To capture
this intuition, we propose another feature, namely TopicDominanceInContext, as the cosine similarity (Equation
8) between the set of sentences that contain s and the set
of sentences that do not, where the tf · idf weight of each
term w in both vectors is calculated in the same way as Dw
in Section 6.3.1.

Entropy(s) = −

X

(6)

where L(s, p) denotes the frequency that a web page p is
linked with an anchor text s. A smaller entropy means that
s is used as an anchor text to mostly link to a single web page,
suggesting that s is more likely to be a specific topic/concept;
a larger entropy indicates that s is used as the anchor text
to link to different web pages, and thus s might be a too
general or ambiguous topic/concept.
SelectionEntropy measures the selection clarity at the“topic”
level. We also introduce a set of features to measure the
clarity at the term level based on the IDF distribution – if s
contains high-IDF terms, it is more likely to be a clear topic.
Indeed, Scholer et al. [37] have shown that the query term

660

w∈Q∩D

2
w∈Q Qw

·

Q w · Dw
qP

w∈D

2
Dw

(8)

where Qw and Dw are the tf · idf weights of term w in Q
and in D, respectively. Following the term frequency normalization formulas in BM25 [35, 39], the tf · idf weight in
+1
the “query” is as Qw = f (w, Q) log DF N
, while the tf · idf
(w)+0.5
f (w,D)·(k1 +1)
+1
,
weight in the “document”is as Dw = f (w,D)+k1 log DF N
(w)+0.5
where N is the total number of (Wikipedia) documents,
DF (w) is the number of documents that contain w, and
k1 is a free parameter in BM25 that is empirically set to 1.5.
Our query consists of two parts: the text selection s and
the context c. Note that the term frequency f (w, c) for the
context is special, as we would like to assign more weights
to terms closer to the text selection based on the proximity
heuristic [30]. To this end, we use the selection-centric context language model developed in Section 5.1 as the context
term frequency, i.e., f (w, c) = p(w|s, c).
Our “document” consists of two fields, as shown in Figure 3: (1) the title field dt ; (2) the snippet field ds . Besides,
we represent the whole document using a weighted linear
combination of the term frequencies from both fields, similar
to the idea of BM25F [36], which is denoted as “document” d,
with empirically tuned weights 3 and 1 for title and snippet
respectively: f (w, d) = 3 · f (w, dt ) + f (w, ds ).
Therefore, we generate six features by combining two types
of “queries” and three types of “documents” in different ways.
In addition, we introduce another feature that attempts
to capture the topical consistency of the entire set of documents as a whole to the context. Intuitively, the relevance of
an individual document should be promoted if all the documents in the set are likely to share the same topic with the
context. We denote this feature as DocumentSetRelatedness, which measures the similarity between the context
c and the document set D. The key step is to calculate the
term frequency for D, i.e., f (w, D). We follow the idea of
BM25F [36], and use a weighted linear combination of the

term frequencies from all documents d ∈ D with an appropriate weight w(d) for each document. We borrow the idea
of DCG [21], and set w(d) = log−1
2 (R(d) + 1), where R(d) is
the ranking position of d. Then, the frequency of w in D is
computed as:
X
f (w, D) =
w(d) · (3 · f (w, dt ) + f (w, ds ))
(9)

candidate reference concept. We first evaluated our system
based on the metrics widely used in the TREC filtering task
[34], including precision, recall and Fβ .
Precision, recall and Fβ are all binary metrics. Hence,
we employed two strategies to map the trinary label (i.e.,
“Perfect”, “Good” and “Bad”) to a binary label:
• Aggressive Recommendation: we map labels “Perfect” and “Good” to “Correct”, and “Bad” to “Incorrect”. In this way, the system is encouraged to recommend not only perfect results that users expect but
also good results that can help users explore the topic.

d∈D

where dt and ds are the title and snippet fields of d, respectively. Finally, we can compute DocumentSetRelatedness
using the cosine similarity between the context vector and
the document set vector.

• Conservative Recommendation: we map labels “Perfect” to “Correct”, and both “Good” and “Bad” to “Incorrect”. In this way, the system is encouraged to only
recommend perfect results.

6.3.2 Semantic Similarity
The syntactic similarity would often suffer from the wordmismatching problem, since we are dealing with short “documents”. To relax this problem, as discussed in Section 5.2,
we map each text fragment into the semantic space using
ESA [16]. We have presented how to build a selection-centric
context semantic vector. Similarly, following the notation
in Section 5.2, we can also build a semantic vector for the
P
selection s: V(s) = w∈s P ′f (w,s)
V(w), as well as a sef (w′ ,s)
w ∈s
mantic vector for each of the three types of “document” and
′
P
the document set: V(d) = w∈d P ′f (w,d)
V(w), where
f ′ (w′ ,d)

Precision is then computed by averaging the precision of
all selections for which the system shows results. Recall is
computed by averaging the recall of all selections that have
at least one correct label in the judgment set. With precision
and recall ready, Fβ is computed as follows:
Fβ = (1 + β 2 )

precision · recall
β 2 · precision + recall

w ∈d

f ′ (w, d) =

f (w,d)·(k1 +1)
f (w,d)+k1

where we follow the TREC filtering task [34] to set β = 0.5.
Next, in order to account for the multiple-graded relevance
labels, we also report DCG [21] at different positions:

.
Finally, we generate a set of 7 semantic-similarity based
features by comparing the corresponding pairs of semantic
vectors.

DCG@k =

6.3.3 Surface Matching

k
X
i=1

Inspired by the entity linking works (e.g., [43]), we also
include several features to measure the surface/string similarity between the selection s and the document title dt , including the edit distance between s and dt , and abbreviation
matching that indicates if s is an abbreviation of dt or dt is
an abbreviation of s. Moreover, the length of the selections
often varies a lot as shown in Table 2, which motivates us
to propose another two normalized matching features, e.g.,
the normalized edit distance, which divides the edit distance
by the length of the longer one between s and dt (to avoid
overly penalizing long selections).

r(i)
(i+1)

log2

where k is the cutoff position.
The in situ insights problem differs from conventional information retrieval in that it is acceptable, and occasionally
even desirable, not to show any results for a given selection.
As a result, the “additive” nature of DCG (and other Web
search metrics) is not suitable in this application. To see
why, look at two scenarios: (1) algorithm A returns exclusively a “Perfect” result, while algorithm B returns a “Perfect” result followed by a “Bad” one; (2) algorithm A returns
nothing, while algorithm B returns a set of “Bad” results. In
both scenarios, DCG will produce the same score for both
algorithms, although algorithm A is more desirable in both
cases. This is mainly caused by the always non-negative relevance labels in DCG, which does not penalize false positives
results appropriately. To overcome this limitation, in our
work, we map “Bad” to a negative value −1, and “Perfect”
and “Good” to 2 and 1 respectively. DCG is then computed
as the average DCG of all selections.7
Our algorithm is labeled as Insights. We employed a
3-fold cross-validation for training and evaluating different
methods. We randomly split the dataset (including 490 regular selections and 299 random selections) into 3 folds, and
then fixed the folds in the experiments. Each time we trained
the algorithm on two folds, and then ran the algorithms on
the remaining test fold. Since there was a threshold τ that
required tuning, we ran another process of cross-validation,
in which we tuned τ to optimize the Fβ score on two folds,
and evaluated it on the remaining fold.
In situ insights is a novel problem, and there is no stateof-the-art system readily available to be used as a baseline.
For this reason, we resorted to the two following baselines:

6.4 Learning a Prediction Function
We have proposed 27 features from three aspects. Given
the proposed features and the constructed dataset, we use a
state-of-the-art regression algorithm, namely MART (Multiple Additive Regression Trees) [41], to develop a prediction function. MART is based on the stochastic gradient
boosting approach described in [13, 14] which performs gradient descent optimization in the functional space. In our
experiments, we used the log-likelihood as the loss function,
steepest-descent (gradient descent) as the optimization technique, and binary decision trees as the fitting function.
We construct a training instance for each (selection, context, reference, reference set) quadruple that consists of a
set of features and a relevance label. The training data is
fed into MART to build a regression model, which we use to
score each document in the test data.

7. EXPERIMENTS
7.1 Experimental Setting

7

Note that we do not use NDCG because the DCG of a selection may
be negative in our setting, for which the “normalization” in NDCG
does not make sense.

In Situ Insights is in the line of content-based filtering.
It makes a set of binary decisions to accept or reject each

661

Scenario

Aggressive

Conservative

Metric
Fβ
Precision
Recall
Coverage
Fβ
Precision
Recall
Coverage

All Selections
Baseline Insights
0.521
0.606
0.509
0.651
0.576
0.474
93.3%
0.316
0.575
0.279
0.709
0.673
0.328
51.2%

Regular Selections
Baseline Insights
0.567
0.612
0.629
0.707
0.408
0.398
98.6%
0.365
0.567
0.336
0.695
0.550
0.326
65.9%

Random Selections
Baseline Insights
0.362
0.561
0.314
0.544
0.942
0.638
84.6%
0.221
0.606
0.185
0.764
0.972
0.331
27.1%

Table 3: Performance comparison. “Coverage” shows the percentage of selections with any results shown. All
improvements in Fβ and precision pass the Wilcoxon non-directional significant test at the 0.001 level.
• The first baseline consists of straightforward querying of the commercial search engine without applying the proposed filtering algorithm, labeled as Baseline. In fact, we also evaluated several query expansion methods which extracted a few keywords from the
context and sent them to the search engine together
with the “query”. In particular, we tried a modified
IntelliZap method [12] (where we used explicit semantic analysis [16] to compute word relatedness), a positional relevance model [30, 18], and a machine learning approach [9]. All these query expansion methods
improved the precision slightly, but significantly decreased the recall and the result coverage. For example, the search engine returned candidate references
for 490/500 regular selections using pure text selections as queries, but only returned results for at most
415/500 selections using these query expansion methods. As a result, these query expansion methods performed worse than this simple “Baseline” on both Fβ
and DCG.

Aggressive Recommendation

Conservative Recommendation

0.62

0.6

0.6

0.55

0.58

0.5

0.56

0.45
Fβ

Fβ

0.54
0.4

0.52
0.35

0.5

0.3

0.48
0.46
0.44

0.25

Baseline
Insights
1

2

3

4

5

Number of Results

6

7

8

0.2

Baseline
Insights
1

2

3

4

5

6

7

8

Number of Results

Figure 7: Comparison of Fβ w.r.t. the maximum
number of references to show.

• We also compare Insights with a machine-learned ranker
using a state-of-the-art learning to rank algorithm, LambdaMART [7]. This baseline is labeled as CtxRank. It
was trained on the same feature set to re-rank the candidate documents using the same 3-fold cross-validation
by optimizing DCG. We include CtxRank mainly to
investigate if filtering/classification works better than
ranking, since a recent work has shown that ranking is
better than classification for a related problem, namely
cumulative reference recommendation [4].

7.2 Performance Comparison
We first compare Insights with Baseline for aggressive recommendation. Note that the problem at hand is defined insitu of a document consumption application, in which the
display space for the recommendation results is constrained
and only allows to show a few results. In this work, the comparison is done at the fourth position, by assuming that at
most four references can be shown.
The results are summarized in Table 3. We can see that,
for all selections, Insights outperforms the Baseline significantly by 16.3% and 27.9% in terms of Fβ and precision,
respectively. Furthermore, we inspect the regular and random selections separately. The results show that Insights
works effectively on both types of selections. In particular,
Insights even improves precision significantly (+12.4%) with
only a slight decrease (-2.5%) in recall on the regular selections, while on the random selections, Insights achieves an
Fβ improvement as high as 55.0%. In addition, Insights can
produce reference recommendation for over 93.3% selections.

662

Next, we compare the conservative recommendation performance in Table 3. Conservative recommendation is harder
than aggressive recommendation, because there are often no
“Perfect” references for many selections; this is also reflected
by the numbers: we can see that the Fβ of Baseline drops
dramatically for conservative recommendation as compared
with aggressive recommendation. In contrast, the Fβ of Insights only decreases slightly, or even increases for random
selections. As a result, over all selections, Insights outperforms Baseline by 82.0% in terms of Fβ . Here, we would like
to highlight the “coverage” numbers: Insights can cover more
regular selections (65.9%) than random selections (27.1%),
and automatically discards 72.9% random selections; this is
highly valuable for improving user experience.
We also compare the precision-recall curves of Insights and
Baseline in Figure 6. We can see that Insights can improve
the precision significantly with a reasonable degradation of
recall for both aggressive and conservative recommendation.
We fixed the maximum number of references to four in the
previous experiments. And now we examine if our Insights
algorithm can still do well when the system allows to display
more or fewer references. We plot the Fβ w.r.t. the maximum number of references allowed in Figure 7. It clearly
shows that Insights consistently outperforms Baseline in all
cases, and the improvement of Insights becomes even larger
when we show more results.
So far, the experiments are mainly based on binary evaluation metrics. Next, we evaluate Insights using a gradedrelevance metric, i.e., DCG@k, with different length cutoff
k. Besides the “Baseline”, we also compare Insights with
a contextual ranking model, CtxRank, as described in Section 7.1. CtxRank serves as a strong representative of the
contextual search systems [12, 24, 23, 38]. The comparison
is presented in Table 4. We can see that: (1) Insights improves over Baseline significantly, especially for larger k; (2)
CtxRank also performs better than Baseline, confirming the
observation in previous work [12, 24, 23, 38]; (3) Insights
outperforms CtxRank significantly, suggesting that the proposed filtering technique is desirable, and that filtering works
better than ranking for in situ insights; (4) the DCG scores

Aggressive Recommendation
1

Conservative Recommendation
1

Baseline
Insights

0.95

Baseline
Insights

0.9

0.9
0.8
0.8

Precision

Precision

0.85

0.75
0.7

0.7
0.6
0.5

0.65
0.4
0.6
0.3

0.55
0.5

0

0.1

0.2

0.3

0.4

0.5

0.2

0.6

0

0.1

0.2

0.3

Recall

0.4

0.5

0.6

0.7

0.8

Recall

Figure 6: Comparison of the precision-recall curves
Metric
DCG@1
DCG@2
DCG@3
DCG@4
DCG@5

Baseline
0.951
1.080
1.096
1.045
0.952

CtxRank
0.972
1.1451
1.1851
1.1381
1.0631

Insights
0.9971
1.2671
1.43512
1.53412
1.60512

Insights+
1.0181
1.30612
1.47812
1.57512
1.663123

Ablated Features
Selection Clarity
Context Coherence
Syntactic & Semantic
Similarity
Surface Matching

Table 4: DCG Comparison. Superscripts 1, 2 and
3 indicate significant improvements over Baseline,
CtxRank and Insights at the 0.01 level using the
Wilcoxon non-directional test.
0.65

1.8

Aggressive
Conservative

0.6

DCG at different positions

0.5

Fβ

0.45
0.4
0.35
0.3
0.25

1.4

0.1

1.2
1
0.8
0.6
0.4

0

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

Threshold

1.8

0.2

0

0.2

0.4

0.6

0.8

1

1.2

1.4

1.6

0.579

0.548

0.563

0.516

0.607

0.547

0.559

0.537

the Fβ score decreases after excluding one category.8 The
results are presented in Table 5.
The proposed features all work well. Features based on
syntactic and semantic similarity contribute the most. And
for regular selections, these similarity based features play a
more significant role in aggressive recommendation than in
conservative recommendation, suggesting that they are better at excluding “Bad” results rather than identifying “Perfect” results, probably because “Perfect” and “Good” results
are easily mixed up for regular selections. For random selections, however, these similarity features work more effectively in conservative recommendation (i.e., identifying “Perfect” results) than in aggressive recommendation (i.e., excluding “Bad” results), probably because “Good” and “Bad”
results are easily mixed up for random selections.
The other three categories of features, i.e., surface matching, selection clarity, and context coherence, also contribute
to Insights, especially for random selections.

0.2
0.15

Conservative Fβ
Regular Random
0.567
0.606
0.560
0.576
0.562
0.599

Table 5: Feature ablation analysis.

DCG@1
DCG@2
DCG@3
DCG@4
DCG@5

1.6

0.55

Aggressive Fβ
Regular Random
0.612
0.561
0.611
0.554
0.610
0.550

1.8

Threshold

Figure 8: Sensitivity of Fβ /DCG to the threshold.
of Baseline and CtxRank decrease when we increase k to
some value, but in contrast, Insights increases steadily.
Can we combine Insights and CtxRank to further improve
the performance? To answer this question, we introduce “Insights+”, which does both filtering and ranking. We report
its DCG numbers in Table 4. We observe that Insights+
only improves over Insights slightly, i.e., re-ranking does not
help much after result filtering. This is probably because
the search engine already does a good job at ranking “Perfect” results relatively higher than “Good” results, though it
might not work well for recognizing “Bad” results without the
contextual information. This suggests that Insights alone already works very well, and that it may not be necessary to
maintain an additional ranking model.
The setting of the threshold is important for a filtering
task. We now examine the sensitivity of the Fβ and DCG
scores against the threshold in Figure 8. We can see that in
both aggressive and conservative recommendation scenarios,
Fβ is sensitive to the threshold value, and the optimal threshold value for the aggressive and conservative recommendation is within [0.5, 0.7] and [1.0, 1.3], respectively. DCG is
also sensitive to the threshold value, but DCG at different positions appears to largely share the optimal threshold,
which is usually in [0.5, 0.7].

8. CONCLUSIONS
In this paper, we proposed a novel recommendation task,
In Situ Insights, which adaptively recommends reference concepts in response to a text selection in-situ of a document
consumption application. We developed a selection-centric
context language model and a selection-centric context semantic model to measure user interest. We then proposed
different aspects for characterizing reference quality, including context coherence, selection clarity, and reference relevance with respect to the selection and the context. We
also constructed a dataset using crowdsourcing by simulating the environment of a reader consuming content in an
application that has in situ insights capabilities. Finally, we
developed a prediction model based on the proposed features
using machine learning, and conducted a thorough experimental evaluation to show the effectiveness of the proposed
techniques.
As a new class of content recommendation, in situ insights
opens up many interesting future directions. One of the
most interesting directions is to conduct online evaluation

7.3 Feature Analysis
We go in depth to analyze what the relative contributions
of each category of features are (see Section 6). We remove
each category of features one by one, and examine how much

8
We combine syntactic similarity and semantic similarity together
since they highly correlate to each other.

663

for measuring insights experience in real applications. Another interesting direction is to leverage rich user reading history (such as other documents a user has consumed/created,
history insights experience, etc.) to improve the recommendation performance.

[19] Q. He, J. Pei, D. Kifer, P. Mitra, and L. Giles.
Context-aware citation recommendation. In WWW ’10,
pages 421–430, 2010.
[20] Y. Hijikata. Implicit user profiling for on demand relevance
feedback. In IUI ’04, pages 198–205, 2004.
[21] K. Järvelin and J. Kekäläinen. Cumulated gain-based
evaluation of ir techniques. In ACM Trans. Inf. Syst.,
volume 20, pages 422–446, 2002.
[22] G. Kazai, J. Kamps, M. Koolen, and N. Milic-Frayling.
Crowdsourcing for book search evaluation: Impact of hit
design on comparative system ranking. In SIGIR ’11, pages
205–214, 2011.
[23] R. Kraft, C. C. Chang, F. Maghoul, and R. Kumar.
Searching with context. In WWW ’06, pages 477–486, 2006.
[24] R. Kraft, F. Maghoul, and C. C. Chang. Y!q: Contextual
search at the point of inspiration. In CIKM ’05, pages
816–823, 2005.
[25] J. Lee, A. Fuxman, B. Zhao, and Y. Lv. Leveraging
Knowledge Bases for Contextual Entity Exploration. To
appear in KDD ’15, 2015.
[26] G. Limaye, S. Sarawagi, and S. Chakrabarti. Annotating
and searching web tables using entities, types and
relationships. Proc. VLDB Endow., 3(1-2):1338–1347, Sept.
2010.
[27] A. Livne, V. Gokuladas, J. Teevan, S. T. Dumais, and
E. Adar. Citesight: Supporting contextual citation
recommendation using differential search. In SIGIR ’14,
pages 807–816, 2014.
[28] Y. Lv, T. Moon, P. Kolari, Z. Zheng, X. Wang, and
Y. Chang. Learning to model relatedness for news
recommendation. In WWW ’11, pages 57–66, 2011.
[29] Y. Lv and C. Zhai. Positional language models for
information retrieval. In SIGIR ’09, pages 299–306, 2009.
[30] Y. Lv and C. Zhai. Positional relevance model for
pseudo-relevance feedback. In SIGIR ’10, pages 579–586,
2010.
[31] D. Milne and I. H. Witten. Learning to link with wikipedia.
In CIKM ’08, pages 509–518, 2008.
[32] D. Petkova and W. B. Croft. Proximity-based document
representation for named entity retrieval. In CIKM ’07,
pages 731–740, 2007.
[33] B. Ribeiro-Neto, M. Cristo, P. B. Golgher, and E. Silva de
Moura. Impedance coupling in content-targeted
advertising. In SIGIR ’05, pages 496–503, 2005.
[34] S. Robertson and I. Soboroff. The trec 2002 filtering track
report. In TREC ’02, 2002.
[35] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu,
and M. Gatford. Okapi at trec-3. In TREC ’96, pages
109–126, 1996.
[36] S. Robertson, H. Zaragoza, and M. Taylor. Simple bm25
extension to multiple weighted fields. In CIKM ’04, pages
42–49, 2004.
[37] F. Scholer, H. E. Williams, and A. Turpin. Query
association surrogates for web search: Research articles. J.
Am. Soc. Inf. Sci. Technol., 55(7):637–650, May 2004.
[38] X. Shen, B. Tan, and C. Zhai. Implicit user modeling for
personalized search. In CIKM ’05, pages 824–831, 2005.
[39] A. Singhal. Modern information retrieval: A brief overview.
IEEE Data Eng. Bull., 24(4):35–43, 2001.
[40] A. Sun and C. Lou. Towards Context-aware Search with
Right Click. In SIGIR ’14, pages 847–850, 2014.
[41] Q. Wu, C. J. Burges, K. M. Svore, and J. Gao. Adapting
boosting for information retrieval measures. Inf. Retr.,
13(3):254–270, June 2010.
[42] Y. Zhang, J. Callan, and T. Minka. Novelty and
redundancy detection in adaptive filtering. In SIGIR ’02,
pages 81–88, 2002.
[43] Z. Zheng, F. Li, M. Huang, and X. Zhu. Learning to link
entities with knowledge base. In HLT ’10, pages 483–491,
2010.
[44] M. Zhou and K. C.-C. Chang. Entity-centric document
filtering: Boosting feature mapping through meta-features.
In CIKM ’13, 2013.

9. ACKNOWLEDGMENTS
We thank Ashok Chandra, Bernhard Kohlmeier, Bo Zhao,
Dhyanesh Narayanan, and Pradeep Chilakamarri for their
invaluable comments in many discussions about this project.
We also thank the anonymous reviewers for their helpful
feedback.

10. REFERENCES
[1] G. Adomavicius and A. Tuzhilin. Toward the next
generation of recommender systems: A survey of the
state-of-the-art and possible extensions. IEEE TKDE,
17(6):734–749, June 2005.
[2] R. Agrawal, S. Gollapudi, A. Kannan, and K. Kenthapadi.
Enriching textbooks with images. In CIKM ’11, pages
1847–1856, 2011.
[3] K. Balog, A. P. de Vries, P. Serdyukov, P. Thomas, and
T. Westerveld. Overview of the trec 2009 entity track. In
TREC ’09. NIST, November 2009.
[4] K. Balog and H. Ramampiaro. Cumulative citation
recommendation: Classification vs. ranking. In SIGIR ’13,
pages 941–944, 2013.
[5] I. Bordino, Y. Mejova, and M. Lalmas. Penguins in
sweaters, or serendipitous entity search on user-generated
content. In CIKM ’13, pages 109–118, 2013.
[6] A. Broder, M. Ciaramita, M. Fontoura, E. Gabrilovich,
V. Josifovski, D. Metzler, V. Murdock, and V. Plachouras.
To swing or not to swing: learning when (not) to advertise.
In CIKM ’08, pages 1003–1012, 2008.
[7] C. J. Burges. From ranknet to lambdarank to lambdamart:
An overview. Technical Report MSR-TR-2010-82,
Microsoft Research, 2010.
[8] G. Buscher, R. W. White, S. Dumais, and J. Huang.
Large-scale analysis of individual and task differences in
search result page examination strategies. In WSDM ’12,
pages 373–382, 2012.
[9] G. Cao, J.-Y. Nie, J. Gao, and S. Robertson. Selecting
good expansion terms for pseudo-relevance feedback. In
SIGIR ’08, pages 243–250, 2008.
[10] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting
query performance. In SIGIR ’02, pages 299–306, 2002.
[11] S. Cucerzan. Large-scale named entity disambiguation
based on Wikipedia data. In EMNLP-CoNLL ’07, pages
708–716, 2007.
[12] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin,
Z. Solan, G. Wolfman, and E. Ruppin. Placing search in
context: The concept revisited. ACM Trans. Inf. Syst.,
20(1):116–131, Jan. 2002.
[13] J. H. Friedman. Greedy function approximation: A gradient
boosting machine. Annals of Statistics, 29:1189–1232, 2000.
[14] J. H. Friedman. Stochastic gradient boosting. Comput.
Stat. Data Anal., 38:367–378, February 2002.
[15] A. Fuxman, P. Pantel, Y. Lv, A. Chandra, P. Chilakamarri,
M. Gamon, D. Hamilton, B. Kohlmeier, D. Narayanan,
E. Papalexakis, and B. Zhao. Contextual insights. In
WWW Companion ’14, pages 265–266, 2014.
[16] E. Gabrilovich and S. Markovitch. Computing semantic
relatedness using wikipedia-based explicit semantic
analysis. In IJCAI ’07, pages 1606–1611, 2007.
[17] E. Gabrilovich and S. Markovitch. Wikipedia-based
semantic interpretation for natural language processing. J.
Artif. Int. Res., 34(1):443–498, Mar. 2009.
[18] S. Gottipati and J. Jiang. Linking entities to a knowledge
base with query expansion. In EMNLP ’11, pages 804–813,
2011.

664

