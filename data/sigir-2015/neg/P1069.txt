Finding Answers in Web Search
Evi Yulianti
School of Computer Science & Information Technology
RMIT University
Melbourne, Australia

evi.yulianti@rmit.edu.au
see whether the documents that have better quality answer
summaries should be ranked higher in the result list. A set of
features are derived from answer summaries to re-rank documents
in the result list. Our experiment shows that answer summaries
can be used to improve state-of-the-art document ranking. The
method is also shown to outperform a current re-ranking approach
using comprehensive document quality features. A manuscript
was submitted for this result.

ABSTRACT
There are many informational queries that could be answered with
a text passage, thereby not requiring the searcher to access the full
web document. When building manual annotations of answer
passages for TREC queries, Keikha et al. [6] confirmed that many
such queries can be answered with just passages. By presenting
the answers directly in the search result page, user information
needs will be addressed more rapidly so that reduces user
interaction (click) with the search result page [3] and gives a
significant positive effect on user satisfaction [2, 7].

For future work, we plan to conduct deeper analysis on top
matching questions and their corresponding best answers from
Y!A to better understand their benefit to the generated summaries
and re-ranking results. For example, how do the results differ on
different relevance level of top best answers from Y!A that were
used to generate summaries. There are also opportunities to
improve the use of Y!A in generating answer summaries, such as
by predicting the quality of best answers from Y!A corresponding
to the query. We also aim to combine the related Y!A pages into
our initial result list when there is a question from Y!A, which is
well matched with the query. Next, it is important to think about
an approach to generate answer summaries for the queries that do
not have related result from CQA.

In the context of general web search, the problem of finding
answer passages has not been explored extensively. Retrieving
relevant passages has been studied in TREC HARD track [1] and
in INEX [5], but relevant passages are not required to contain
answers. One of the tasks in the TREC genomics track [4] was to
find answer passages on biomedical literature. Previous work has
shown that current passage retrieval methods that focus on topical
relevance are not effective at finding answers [6]. Therefore, more
knowledge is required to identify answers in a document.
Bernstein et al. [2] has studied an approach to extract inline direct
answers for search result using paid crowdsourcing service. Such
an approach, however, is expensive and not practical to be applied
for all possible information needs. A fully automatic process in
finding answers remains a research challenge.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval.

The aim of this thesis is to find passages in the documents that
contain answers to a user’s query. In this research, we proposed to
use a summarization technique through taking advantage of
Community Question Answering (CQA) content. In our previous
work, we have shown the benefit of using social media to
generate more accurate summaries of web documents [8], but this
was not designed to present answer in the summary. With the high
volume of questions and answers posted in CQA, we believe that
there are many questions that have been previously asked in CQA
that are the same as or related to actual web queries, for which
their best answers can guide us to extract answers in the
document. As an initial work, we proposed using term
distributions extracted from best answers for top matching
questions in one of leading CQA sites, Yahoo! Answers (Y!A),
for answer summaries generation. An experiment was done by
comparing our summaries with reference answers built in
previous work [6], finding some level of success. A manuscript is
prepared for this result.

Keywords
Web Search, Answer, Summarization, CQA

REFERENCES
[1]
[2]
[3]
[4]
[5]
[6]

Next, as an extension of our work above, we were interested to

[7]

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or
distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for third-party components of this
work must be honored. For all other uses, contact the Owner/Author.
Copyright is held by the owner/author(s).
SIGIR '15, August 09-13, 2015, Santiago, Chile
ACM 978-1-4503-3621-5/15/08.
http://dx.doi.org/10.1145/2766462.2767846

[8]

1069

Allan, J. 2004. HARD Track Overview in TREC 2004 - high
accuracy retrieval from documents. Proc of TREC.
Bernstein, M.S., Teevan, J., Dumais, S., Liebling, D. and Horvitz,
E. 2012. Direct Answers for Search Queries in the Long Tail. Proc
of SIGCHI, 237–246.
Chilton, L.B. and Teevan, J. 2011. Addressing People’s
Information Needs Directly in a Web Search Result Page. Proc of
WWW, 27–36.
Hersh, W.R., Cohen, A.M., Roberts, P.M. and Rekapalli, H.K.
2006. TREC 2006 genomics track overview. Proc of TREC.
Kamps, J., Pehcevski, J., Kazai, G., Lalmas, M. and Robertson, S.
2008. INEX 2007 Evaluation Measures. Focused Access to XML
Documents, 24–33.
Keikha, M., Park, J.H., Croft, W.B. and Sanderson, M. 2014.
Retrieving Passages and Finding Answers. Prof of ADCS, 81-84.
Lagun, D., Hsieh, C.-H., Webster, D. and Navalpakkam, V. 2014.
Towards Better Measurement of Attention and Satisfaction in
Mobile Search. Proc of SIGIR, 113–122.
Yulianti, E., Huspi, S. and Sanderson, M. 2015. Tweet-biased
summarization. JASIST. (2015).

