Personalized Semantic Ranking for Collaborative
Recommendation
Song Xu

Shu Wu

Liang Wang

Institute of Automation,
Chinese Academy of Sciences
Beijing, China

Institute of Automation,
Chinese Academy of Sciences
Beijing, China

Institute of Automation,
Chinese Academy of Sciences
Beijing, China

song.xu@nlpr.ia.ac.cn

shu.wu@nlpr.ia.ac.cn

wangliang@nlpr.ia.ac.cn

ABSTRACT
Recently a ranking view of collaborative recommendation
has received much attention in recommendation systems.
Most of existing ranking approaches are based on pairwise
assumption, i.e., everything that has not been selected is of
less interest for a user. However it is usually not proper
in many cases. To alleviate the limitation of this assumption, in this work, we present a unified framework, named
Personalized Semantic Ranking (PSR). PSR models the personalized ranking and the user-generated content (UGC) simultaneously, and the semantic information extracted from
UGC can make a remedy for the pairwise assumption. Moreover, utilizing the semantic information, PSR can capture
the more subtle information of the user-item interaction and
alleviate the overfitting problem caused by insufficient ratings. The learned topics in PSR can also serve as proper explanations for recommendation. Experimental results show
that the proposed PSR yields significant improvements over
the competitive compared methods on two typical datasets.

Figure 1: An illustration of user-generated content
in website recommendation system. All the three
websites, YouTube.com, last.fm and EASports.com,
are represented by their logos respectively. The
green solid lines represent “Selection”, and the orange dashed lines denote “Recommendation”. For
example, Sam tags the website “YouTube” with “sports” and is recommended with “EASports”.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval—Information Filtering

Keywords

(CF) exploits the similarity among users and recommend
items the similar users liked.
In most of real-world circumstances, implicit feedback,
such as clicks, purchases, is more frequently available. A
characteristic of implicit feedback is that it is one-class, i.e.,
only positive examples are observed. Motivated by the work
in the domain of learning-to-rank, a ranking view to recommendation [4, 7] provides us a good way to handle the
implicit feedback. One of the most successful methods is
Bayesian Personalized Ranking (BPR), which assumes that
everything that has not been selected is of less interest for
a user [7]. However this assumption is usually not proper in
many cases. In order to overcome the limitation mentioned
above, we utilize semantic information extracted from UGC
to construct personalized ranking for better recommendation. In the recommendation scenarios, the user-generated
content is assigned by certain user to certain item in the
process of user-item interaction (such as rating, purchasing,
etc.). In these scenarios, UGC provides us a further clue not
only on the user interest but also on the item characteristic.
By incorporating UGC, recommendation systems have the
potential to generate more meaningful and effective recommendations for users.

Learning to rank; recommendation; user-generated content

1.

INTRODUCTION

In recent years there have been growing concerns over the
problem of information overload. To cater the users’ needs
for finding right information, a lot of researches have been
done on recommender systems (RS). Personalized recommendation involves a process of learning users’ preferences
by analyzing their feedback, either in explicit or implicit form, and delivering the right items to each user. As one of the
most well-known approaches in RS, collaborative filtering
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
SIGIR’15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767772.

971

For ease of explanation, we take the tags in website recommendation systems (e.g. Delicious1 ) as a specific example to
illustrate some benefits of UGC. (1) As shown in Figure 1,
we may infer that Bob prefers “EASports” over “Lastfm”
following the pairwise assumption in BPR. Nevertheless it
is obviously unreasonable to making a judgment that Bob
prefers “EASports” over “YouTube”. The semantic similarity (e.g. common tags) between “EASports” and “YouTube”
can make a remedy for the pairwise assumption, whereas
the semantic difference between “EASports” and “Lastfm”
strengthen the pairwise hypothesis. (2) Utilizing the UGC
is a good choice to solve the rating sparsity problem. In
Figure 1, both Sam and Lucy have selected the website “YouTube”. Merely from this behavior, we may infer that they
have the same interest. However, through further observation of their tags on “YouTube”, we realize Sam prefers
“sports” video while Lucy prefers “music” video. These behaviors cannot differentiate users’ interests or item properties while the UGC can. From the above example, we notice that UGC is capable to reflect more subtle information,
which may not be revealed by very sparse rating behavior.
(3) As the UGC captures both the user interest and item
characteristic explicitly, it can help us understand user preferences explicitly, which makes a great difference in recommendation systems. After capturing the user preference, RS
can demonstrate the topics user interested in, rather than
describe redundantly and blindly with a lot of annoying details. In the case described in Figure 1, we can recommend
Sam “EAsports” with proper explanation, such as, “it is a
popular sports site”. This adequate and persuasive explanation would make the recommendation more acceptable.
In this paper, we present a novel probabilistic framework,
named Personalized Semantic Ranking (PSR), which jointly modeling the personalized ranking and UGC in a common subspace. It alleviates the rigid pairwise assumption in
BPR by seamlessly incorporating semantic information for
recommendation. Our method captures both the collaborative preference and the content-based preference, whick can
also alleviate the overfitting problem caused by sparse rating
data. PSR can also exhibit the user interests and item characteristics in an explicit manner, which can be interpreted
as a vector of topics discovered from UGC.

2.

There are several methods which integrate attribute information about users and items into recommender systems. For example fLDA [1] and CTR [9] utilize topic models
[2] for recommendation. Instead of learning one representation from a single document in these methods, PSR learns each user/item latent vector from all the relevant documents, which is more appropriate for UGC. Even some general frameworks like [6] can not well utilize the UGC.

3.

∀i, j ∈ V : i ∈ Vu+ ∧ j ∈ V\Vu+ ⇔ i u j.

(1)

For convenience, we employ a random variable Ru,i,j to denote the above pairwise property
(
1 if i u j
Ru,i,j =
(2)
0 else
For any two different items i, j ∈ V, we have Ru,i,j = 1 ⇔
Ru,j,i = 0 (or Ru,i,j = 0 ⇔ Ru,j,i = 1). In our experiments,
we just take the Ru,i,j = 1 (Ru,j,i = 0) into consideration.
The set of all pairwise preferences can be represented as
T = {(u, i, j)|i ∈ Vu+ ∧ j ∈ V\Vu+ }, and the ranking set
is R = {Ru,i,j |(u, i, j) ∈ T }. Our model is expected to
accurately preserve the ranking order.

RELATED WORK

Collaborative filtering with implicit feedbacks has been
steadily receiving more attention. For item prediction, [3]
and [5] propose a regularized least-square optimization with
case weights (WR-MF). The case weights can be used to reduce the impact of negative examples. Despite its popularity, the above approaches often performs poorly compared to
more recent models based on ranked loss minimization. The
main reason is that all these approaches focus on the task of
accurately predicting the exact rating values, while ranked
loss minimization focus on accurately picking the top N recommended items, which is a more practical goal. As one of
the most popular methods, Bayesian Personalized Ranking
(BPR) is still under a rigid assumption, that everything that
has not been selected is of less interest for a user [7]. Besides, all the above approaches suffer from the problem of
poor explanation for recommendation.
1

PRELIMINARY

Notations In our scenario, all the observations can be
denoted by G = (U, V, E + , W), where U = {u1 , ..., uN } is
the user set, V = {v1 , ..., vM } is the item set. The positive
feedback is given by user-item interaction set E + ⊆ U × V,
and the element (u, i) ∈ E + represents that user u ∈ U has
purchased/clicked on item i ∈ V. All the UGC is a collection
of documents denoted by W = {wu,i |u ∈ U, i ∈ V}. The
document wu,i is the content posted by user u to item i,
and its t-th word is Wu,i,t , where t ∈ {1, ..., Tu,i }. If there is
no content between user u and item i, word count Tu,i = 0.
Personalized Ranking from Implicit Feedback For
each user, we transform the implicit feedback E + to personalized ranking R. Instead of modeling the implicit feedback
E + directly, we adopt a popular pairwise approach mentioned in BPR [7]. The idea is to discriminate the positive
items Vu+ = {i|(u, i) ∈ E + } from the remaining items V\Vu+
for each user u. Let “user u prefers item i to item j” be
specified as i u j. If an item i has been selected by user u,
then we assume that the user prefers this item over all other
non-observed items. This property is formulated as follows:

4.

PERSONALIZED SEMANTIC RANKING

Probabilistic Framework PSR maps both users and
items to a common latent factor space RK (each dimension
represents a latent factor). Each user u is associated with a
factor vector Uu ∈ RK , which encodes the user preferences,
and each item i is associated with a factor vector Vi ∈ RK ,
representing the item characteristics. Both W and R can be
generated from the latent factors.
For constructing the ranking R, we first assume that each
user-item pair has a rating score, which reflects the preference degree. Thus we have a preference score xu,i for useritem pair (u, i) and another score xu,j for a different pair
(u, j). The probability that user u prefers item i to item j is
p(Ru,i,j = 1), which is parameterized by the relative score
xu,i,j = xu,i −xu,j . Here, we adopt a scoring function, which
is widely used in factor models, xu,i = UTu Vi .

http://www.delicious.com

972

Maximum A Posterior The objective of our model is
to find the optimal latent factors U and V for accurately
modeling ranking and UGC. Maximum a posterior (MAP)
estimation can be used to obtain a point estimate of the posterior based on the observed data, [Û,V̂]=arg max p(U,V|W,R;Θ).
U,V

The negative log likelihood can be used as loss function
LR

LM AP

}|
{
z
= − log p(U, V|W, R; Θ) = − log p(R|U, V)
Lreg

LW

(4)

}|
{z
}|
{
z
− log p(W|U, V) − log p(U) − log p(V) .
In Equation (4), we notice that loss function consists of
three parts, ranking loss LR , content loss LW and regularization loss Lreg . The ranking loss LR , which ensures
successfully preserving the ranking, will be discussed in the
next
section. PThe content loss can be denoted by LW =
PP
nu,i,t log θu,i,k βk,Wu,i,t , where nu,i,t denotes how
−

Figure 2: Graphical model of PSR. There are N
users and M items. R is the personalized ranking.
For some of the user-item pairs, there are documents
extracted from the UGC. The Gaussian prior parameters for U and V have been omitted for simplicity in this figure.

u,i t

For constructing the content W, a latent representation
for wu,i is introduced as θ u,i . It is also called topic proportions in topic models [2], where zu,i is the topic assignments for each word in document wu,i and each topic β k
is a distribution over words. θ u,i encodes not only the latent semantic of document wu,i , but also the user u’s topic
preferences on item i. Each component of θ u,i is expected
to be positively correlated with both the corresponding user
factor and item factor. That is, if an item has higher value of a certain factor, or a user values higher on a certain
topic, the corresponding topic is more likely to appear in
the UGC. In order to preserve such dependent correlation,
we introduce a simple way to build a direct connection between the user-item pair and document, θ u,i = π(Uu + Vi ).
For a K-dimensional vector x,Pπ(x) is logistic transformation function π(x) = exp(x)/ k exp(xk ), which makes it
possible to bound x within the range [0, 1].
In order to balance the model’s complexity against overfitting on the training data, Gaussian priors are also imposed
on the factors U and V. The graphical model is depicted in Figure 2. The generative process for all the textual
documents W and personalized ranking R is as follows:

5.

2. For each item i, draw item factors Vi ∼ N (µV , ΣV )
3. For each word Wu,i,t / Wu,j,t
(a) Draw topic assignment Zu,i,t |Uu , Vi ∼ M ult(θ u,i )
(b) Draw word Wu,i,t |Zu,i,t ∼ M ult(β Zu,i,t )
4. For each triplet (u, i, j) ∈ T , draw the ranking
Ru,i,j ∼ Ber(1/(1 + exp(−xu,i,j )))
For ease of exposition, let Θ = [µU , µV , ΣU , ΣV , β] denote the model parameters, ∆ = [U, V, Z] denote the latent
variables. The joint probability distribution for the observed
variables can be written as
ZZ
p(U)p(V)p(R|U, V)p(W|U, V)dUdV,

EXPERIMENTS

Datasets and Settings We evaluate our model on two
published datasets: social bookmarking dataset from Delicious and scientific literature from Citation-network. In
Delicious dataset, each user bookmarks a set of webpages
which can be regarded as positive examples of the user’s interests. Here, the bookmark is a kind of UGC, and the task
is to recommend webpages to users. In our experiments,
we draw a subsample such that each user has bookmarked
at least 5 webpages, each webpage has been selected by at
least 3 users and each bookmark appears at least 5 times.
Citation-network V 1 is a dataset released by Arnetminer.
In this circumstance, we treat the paper title as a kind of
UGC between the author and the publication, and our task
is to recommend publications to authors. Removing papers
which have no publication information, we obtain a subset
of papers published from 2001 to 2010.
Baselines and Evaluation Metric We conduct comparisons with several state-of-the-art methods, including SVD,
WR-MF [3, 5], BPR-MF [7] and FM [6]. In SVD, the missing is treated as negative examples. In FM, the UGC are
directly treated as additional dimension in feature vector. In
order to evaluate the qualities of the recommendations, we
randomly select 20% of the implicit ratings to be the test set,
and use the remaining entries for training. Two metrics, F1score and Mean Average Precision (MAP), which are widely
used for recommendation evaluation, are adopted here.
Result Analysis Table 1 gives the performance comparison with a variety of dimensionalities on the Delicious and
the Citation-network datasets. From that we can see, under different settings of K, our models consistently outperform the compared methods. Figure 3a is the precisionrecall curve on Delicious dataset. From the results we notice
that the personalized ranking methods have great superiority over rating prediction methods. Owing much to the se-

1. For each user u, draw user factors Uu ∼ N (µU , ΣU )

p(R, W|Θ) =

k

often the word Wu,i,t occurred in document wu,i . If we place
zero-mean spherical Gaussian priors on U and V, Lreg works like L2 regularization which prevent models from overfitting [8]. Given the model parameters Θ, the optimal factors
[U, V] should be obtained by minimization of all three losses. Then each score xu,i can be predicted by UTu Vi . After
sorting all the scores for each user, the top N items will be
recommended to the user.

(3)

where Θ is omitted in the right for brevity.

973

Table 1: Performance comparison on the Delicious and Citation-network datasets with three different factor
dimensionalities K = 10, 30, 50. The predictive accuracy is measured by MAP and F1-score. The values are
under the percentage scale.
Delicious
Method
K=10
2.52
3.03
3.35
5.57
6.12

SVD
WR-MF
BPR-MF
FM
PSR

F1@10
K=30
2.73
3.17
3.50
5.52
6.35

Citation-network

MAP@10
K=10 K=30
1.91
2.24
2.65
2.90
2.54
2.87
4.43
4.63
4.72
4.93

K=50
2.73
4.34
3.57
5.61
6.41

K=50
1.99
2.97
2.71
4.72
5.23

SVD
WR−MF
BPR−MF
FM
PSR

0.08

MAP@10

Precision

0.035

0.1

0.03
0.025

SVD
WR−MF
BPR−MF
FM
PSR

0.06

0.01

topic 00
“CV”
learning
analysis
recognition
image
algorithm
neural
classification
face
Jennifer C.
Guoliang X.
Radha P.
Loukas L.
ACCV
SSVM
PR
TIP
IJCV

0.02

0

0.2

0.4

0.6

Recall

(a)

0.8

1

K=10
3.17
4.02
6.08
7.82
11.4

MAP@10
K=30
3.33
4.15
5.80
7.67
11.2

K=50
3.42
4.80
6.12
7.88
12.2

0.04

0.02
0.015

K=50
1.07
1.32
1.57
1.74
2.50

Table 2: Five topics discovered by PSR on the
Citation-network dataset. Each topic is shown with
the top 8 words, top 4 authors and top 5 publications. (K=30)

0.045
0.04

K=10
0.91
1.27
1.53
1.72
2.39

F1@10
K=30
0.91
1.28
1.51
1.75
2.26

0
20%

30%

40%

50%

60%

70%

#WofWTrainingWRatings

80%

(b)

Figure 3: Left panel: precision-recall curves of the
Delicious dataset. Right panel: MAP with different amounts of training ratings on Delicious dataset.
(K=30)

Topic 02
“Wireless”
network
sensor
wireless
hoc
routing
energy
mobile
protocol
Joseph P.
Sha L.
Ajit W.
Antonio G.
IJSNet
TOSN
EWSN
SECON
WiSec

topic 05
“Education”
computer
science
teaching
student
design
education
assessment
study
Brian H.
Jorma S.
Marja K.
Raymond L.
ICER
ACE
SIGCSE
JERIC
FECS

topic 013
“Graphics”
motion
surface
interactive
simulation
dynamic
texture
mesh
human
Andrew S.
Bryan E.
Eran G.
Yiying T.
SIGGRAPH
I3D
SGP
SPM
TOG

topic 020
“Grid Computing”
grid
control
access
web
computing
resource
management
composition
Chunlin L.
Layuan L.
Brahim M.
Steve B.
IJWGS
CCGrid
HPDC
JSSPP
AusGrid

good properties of interpretation, user experience would be
greatly improved in real-world recommender systems.

mantic analysis from UGC, PSR increases the performance
by more than 20% compared to SVD and WR-MF. PSR
achieves great improvement even compared with BPR-MF,
which largely due to the semantic offset from naive pairwise
assumption in BPR-MF. PSR works better than the general
framework FM for well utilizing the UGC.
We further evaluate the ability of compared methods in
handling different amount of training ratings (20% − 80%).
The performance comparison with K = 30 on the Delicious
dataset is shown in Figure 3b. The experimental results indicate that PSR can obtain the best performance with
varying amounts of training ratings. These improvements
are largely due to the incorporation of UGC. Moreover, our
method outperforms other methods more significantly when
fewer ratings are provided. When we use 80% of the ratings
as training set, PSR increases the performance of SVD with
130%, while given 20% ratings, PSR enhances the performance by more than 360%. The evidences show that when
the rating matrix is sparse, the impact of UGC is very significant in PSR.
Another advantage of PSR is that it can explain the user and item latent space using the topics discovered from
the user-generated content. For user u, we can find the top
matched topics by ranking the entries of factor vector Uu .
For item i, we also can rank the entries of factor vector
Vi . The top matched topics serve as an explanation of user
interest and item characteristic. Some of the topics discovered in the Citation-network are displayed in Table 2. For
example, the table illustrates that the top words in topic
02 are “network” , “sensor” etc., the most related authors of
this topic are “Joseph P.”, “Sha L. ” etc, and the most related publications are “IJSNet”, “TOSN” etc. Owing to the

6.

ACKNOWLEDGMENTS

This work is jointly supported by National Basic Research
Program of China (2012CB316300) and National Natural
Science Foundation of China (61403390, 61175003, 61135002,
U1435221).

7.

REFERENCES

[1] D. Agarwal and B.-C. Chen. flda: Matrix factorization
through latent dirichlet allocation. In WSDM, 2010.
[2] D. M. Blei, A. Y. Ng, and M. Jordan. Latent dirichlet
allocation. JMLR, 2003.
[3] Y. Hu, Y. Koren, and C. Volinsky. Collaborative
filtering for implicit feedback datasets. In ICDM, 2008.
[4] J. Lee, S. Bengio, S. Kim, G. Lebanon, and Y. Singer.
Local collaborative ranking. In WWW, 2014.
[5] R. Pan, Y. Zhou, B. Cao, N. N. Liu, R. Lukose,
M. Scholz, and Q. Yang. One-class collaborative
filtering. In ICDM, 2008.
[6] S. Rendle. Factorization machines. In ICDM, 2010.
[7] S. Rendle, C. Freudenthaler, Z. Gantner, and
L. Schmidt-Thieme. Bpr: Bayesian personalized
ranking from implicit feedback. In UAI, 2009.
[8] R. Salakhutdinov and A. Mnih. Probabilistic matrix
factorization. In NIPS, 2007.
[9] C. Wang and D. M. Blei. Collaborative topic modeling
for recommending scientific articles. In ACM SIGKDD,
2011.

974

