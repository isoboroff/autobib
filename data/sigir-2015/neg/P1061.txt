Transfer Learning for Information Retrieval
Pengfei Li
RMIT University
Melbourne, Australia

li.pengfei@rmit.edu.au

Categories and Subject Descriptors

tasks be measured? Q4. What is the difference between conventional transfer learning and TLTR? Q5. How to tackle
so-called heterogeneous TLTR problems, where the feature
spaces of datasets are different? Q6. How to measure the
effectiveness of TLTR models?
We tested the generalization of LTR models by training
and testing in different datasets. We validated that the performances of LTR models will be leveraged when applying
to a different dataset. Study of the stability of LTR models with respect to training data will also provide evidence
that standard LTR is not sufficient to improve ranking effectiveness under certain circumstances. Our study showed
that training LTR models do not require too many queries,
but these queries should better represent the queries in the
entire collection. The finding also suggests that identifying
similar queries or query groups in the source domain might
be a clue to solving TLTR problems. We proposed a TLTR
algorithm that trains LTR models with queries that are the
most similar to source domain queries, from the source domain. These queries are weighted with importance, calculated by query similarities. This approach is similar to most
instance-weighting based TLTR algorithms [1, 3], but the
algorithm we proposed requires no relevance judgment from
the target domain.
Our study showed that transferring LTR models between
different document sets might have a more terrible effect.
However, our experiments are limited by lacking a pair of
test collections with the same query set but different document sets. A further step of the research is to construct
two test collections with different document sets, yet keep
all other factors the same. The influences of the divergence
between document sets on LTR algorithms will be studied
based on the datasets. As mentioned before, heterogeneous
TLTR is an area yet to be well explored. The study of how
to tackle heterogenous TLTR is also under the scope of this
study.

H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval

Keywords
Information retrieval, Learning to rank, Transfer learning

ABSTRACT
Learning to rank algorithms (LTR) are techniques that utilize machine learning to learn ranking functions for particular document and topic sets. LTR models are designed and
trained to optimize the effectiveness of the ranking function
on the assessed topics of a document set. One challenge of
applying LTR on information retrieval systems is obtaining
the relevance judgments. Transfer learning is a technique
that can be used to train models for new datasets (target
domain) using related datasets (source domain), under the
circumstances where limited training data is accessible, and
thus can help training LTR models in similar situations. The
aim of this research is to study the issues of transfer learning techniques for information retrieval (TLTR) and develop
new TLTR algorithms to help improve the effectiveness of
information retrieval systems.
The idea of applying transfer learning to LTR is not new
but requires deeper exploring. Transfer learning techniques
have been extensively studied in machine learning community [2]. However, the study of applying transfer learning to
LTR had not drawn too much attention until recent years.
A few attempts have been tried. However, better understanding of the issues is needed for future research.
To better understand TLTR and develop new TLTR algorithms, several research questions are to be answered. Here
we list the research questions we want to address in this
study. Q1. What are the factors that can affect the effectiveness of LTR algorithms on a dataset? Q2. Is it necessary to apply TLTR? Moreover, under what circumstances
TLTR is needed? Q3. How should the relatedness of LTR

References
[1] Peng Cai, Wei Gao, K.F. Wong, and Aoying Zhou. Weightbased boosting model for cross-domain relevance ranking
adaptation. Adv. Inf. Retr., pages 562–567, 2011.
[2] Sinno Jialin Pan and Qiang Yang. A Survey on Transfer
Learning. IEEE Trans. Knowl. Data Eng., 22(10):1345–1359,
October 2010.
[3] Shangkun Ren, Yuexian Hou, Peng Zhang, and Xueru Liang.
Importance Weighted AdaRank. Adv. Intell. Comput., pages
448–455, 2012.

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the Owner/Author(s). Copyright is held by the
owner/author(s).
SIGIR’15, August 09-13, 2015, Santiago, Chile.
ACM 978-1-4503-3621-5/15/08.
DOI: http://dx.doi.org/10.1145/2766462.2767845.

1061

