Adaptive User Engagement Evaluation
via Multi-task Learning
Hamed Zamani, Pooya Moradi, and Azadeh Shakery
School of Electrical and Computer Engineering, College of Engineering,
University of Tehran, Tehran, Iran

{h.zamani, po.moradi, shakery}@ut.ac.ir
ABSTRACT

ommender systems. It is shown that the total number of
users’ interactions on tweets can represent the satisfaction
of users. In other words, the value of achieved engagements
is correlated with the interest of users in the received messages [7]. In addition, Uysal and Croft [10] have investigated
an important usage of engagement evaluation, which is designing personalized content filters. It is notable that the
problem of engagement prediction or online participation
has been extensively studied in news websites, social networks, and discussion forums. Petrovic et al. [8] have tried
to predict the number of retweets or to detect the tweets
that will be retweeted. Note that the aforementioned methods have investigated tweets with arbitrary content, while we
are interested in predicting the engagement of tweets with
predefined content. This characteristic of tweets eliminates
the influence of textual features in our task.
According to the importance of user engagement evaluation and its influences on recommender systems, ACM
Recommender Systems Challenge 20141 focused on ranking IMDb tweets of each user based on their engagements.
Similar to this challenge, in this paper the “engagement” is
computed as the total number of retweets and favorites that
a tweet has gained.
In this paper, we try to answer two crucial research questions in user engagement evaluation. (1) Do tweets of various web applications differ with each other in terms of being
engaging? (2) Is there any commonality among tweets of
different web applications, which could be used to improve
engagement evaluation? And how to benefit from these commonalities? To answer these questions, we focus on the problem of detecting the tweets with positive engagement. We
first create a dataset containing tweets about items of four
popular web applications in completely different domains:
IMDb (movies), YouTube (Video clips), Goodreads (books),
and Pandora (musics). By analyzing these datasets as well
as performing in-domain and cross-domain experiments, we
investigate the first question. We further propose an adaptive method based on multi-task learning, a transfer learning technique, to share knowledge between these domains.
Since the data is highly imbalanced, we modify the loss function of multi-task learning algorithms by adding an instance
weighting matrix to the loss function formulation. To the
best of our knowledge, this is the first work that has focused
on adaptive models for user engagement evaluation. The
experiments demonstrate that the proposed method outperforms the baselines and in most cases, the improvements are
statistically significant.

User engagement evaluation task in social networks has recently attracted considerable attention due to its applications in recommender systems. In this task, the posts containing users’ opinions about items, e.g., the tweets containing the users’ ratings about movies in the IMDb website,
are studied. In this paper, we try to make use of tweets
from different web applications to improve the user engagement evaluation performance. To this aim, we propose an
adaptive method based on multi-task learning. Since in this
paper we study the problem of detecting tweets with positive engagement which is a highly imbalanced classification
problem, we modify the loss function of multi-task learning
algorithms to cope with the imbalanced data. Our evaluations over a dataset including the tweets of four diverse
and popular data sources, i.e., IMDb, YouTube, Goodreads,
and Pandora, demonstrate the effectiveness of the proposed
method. Our findings suggest that transferring knowledge
between data sources can improve the user engagement evaluation performance.

Categories and Subject Descriptors
H.2.8 [Database Management]: Data Mining; J.4 [Computer Applications]: Social and Behavioral Sciences

Keywords
User engagement; transfer learning; multi-task learning; adaptive model

1.

INTRODUCTION

Micro-blogging platforms such as Twitter have become
tremendously popular in the past few years. These platforms let people express their opinions and thoughts as fast
as possible and this fact makes them a rich source of information about people’s everyday lives. Due to the high speed
of information diffusion, several web applications have been
integrated with Twitter to let people share their opinions
about items (e.g., movies in IMDb) [7, 11].
Tweets containing the opinions of users about items (or
products) could be used to improve the performance of recPermission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
SIGIR’15, August 09 - 13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767785.

1

1011

http://2014.recsyschallenge.com/

2.

ADAPTIVE USER ENGAGEMENT EVALUATION

In this section, we first briefly introduce the employed
multi-task learning algorithms and describe how we deal
with the imbalanced data in multi-task scenarios. Then,
we introduce our features for user engagement evaluation.

2.1

Multi-task Learning

The problem of dealing with different distributions between training and test data has been extensively studied in
the literature [2, 3, 4]. Various transfer learning approaches
have been so far proposed to share knowledge between data
with different distributions. In this paper, we use multi-task
learning (MTL) [3], an inductive transfer learning technique
whose goal is to improve the generalization performance by
leveraging the domain-specific signals of related tasks.
We first introduce our notation. Let T be a set of t tasks
and each task i has ni training instances {(xi1 , yi1 ), · · · , (xini ,
yini )} in which yij ∈ R and xij ∈ Rd respectively denote
the label and the features vector, where d is the number
of features. The input features and labels can be stacked
together as Xi = {xi1 , · · · , xini } and Yi = {yi1 , · · · , yini },
respectively. The weight of features are represented in matrix W ∈ Rd×t . We consider the logistic loss function which
is widely used for linear classification. The logistic loss function in multi-task learning algorithms is defined as follows:
L(W, X, Y ) =

ni
t X
X

Table 1: Dataset Characteristics
IMDb
YouTube Goodreads
Items type
movie
video clip
book
# of tweets 100,206
239,751
65,445
# of users
6,852
6,480
3,813
# of items
13,502
154,041
31,558
Avg eng.
0.1097
0.4737
0.1632
% of tweets
4.139
14.193
6.931
with eng>0

Pandora
music
98,212
3,312
32,321
0.0778
6.285

assumes that tasks have group structure and thus, all tasks
in each cluster are related to each other.

2.2

Features

In this task, each tweet is tweeted by a user about an
item (e.g., movie). For each tweet, we extract 23 features
partitioned into three categories: user-based, item-based,
and tweet-based. Since the contents of tweets are predefined
by the web applications and users usually do not edit tweets
contents, we do not consider textual features. More details
about the exact definition of features can be found in [11].2
User-based features. Number of followers, Number
of followees, Number of tweets, Number of tweets about
domain’s items, Number of liked tweets, Number of lists,
Tweeting frequency, Attracting followers frequency, Following frequency, Like frequency, Followers/Followees, FollowersFollowees.
Item-based features. Number of tweets about the item.
Tweet-based features. Mention count, Number of hashtags, Tweet age, Membership age at the tweeting time, Hour
of tweet, Day of tweet, Time of tweet, Holidays or not, Same
language or not, English or not.

ln (1 + exp (−Yij (WiT Xij + ci )))

i=1 j=1

where ci is the bias term for task i. Since the distribution
of data in our problem is highly skewed, we assign higher
weights to the instances from the minority class and vice
versa. To this aim, for each task i we define an instance
weighting matrix Λi ∈ Rni ×1 whose elements correspond to
the weight of task i training instances. The elements of Λ is
computed as:
(j)
1/n
(1)
Λij = Pn i (k)
i
k=1 1/ni

3. EXPERIMENTS
3.1 Dataset
In this paper, we investigate the tweets of four diverse
and popular web applications (hereafter, called domains):
IMDb, YouTube, Goodreads, and Pandora which contain
movies, video clips, books, and musics, respectively. These
applications let people express their opinions about items in
Twitter. Tweets contents are predefined, but can be edited
by users.
Recently, Dooms et al. [6] created a similar dataset by collecting recent tweets. Since their dataset is very sparse, we
expand their approach by also gathering all related tweets
of new users. Therefore, our created dataset is more realistic. Statistics of our created dataset are reported in Table 1.
The dataset is freely available for research purposes.3

(j)

where ni denotes the number of training instances in task
i with label yij . A similar idea for coping with imbalanced
data in single-task classification problems has been previously proposed in [1]. In Equation (1), the weights of instances in each task are normalized to make the influence
of all tasks equal in the total loss function. We redefine the
logistic loss function for MTL as follows:
ni
t X
X
L(W, X, Y ) =
Λij ln (1 + exp (−Yij (WiT Xij + ci )))
i=1 j=1

3.2

There are a number of MTL methods with different learning strategies and different assumptions. In this paper, we
consider two linear MTL algorithms as follows:
MTL Trace [4]. This MTL method assumes that all
tasks are related to each other and it tries to transfer knowledge between all tasks. The objective function in MTL Trace
considers the trace-norm of matrix W for regularization.
MTL CASO [5]. This MTL method uses a convex
relaxed alternative structured optimization (CASO) in its
computations which decomposes the model of each task into
two components: task-specific and task-shared feature mappings. It has been shown that there is an equivalence relationship between CASO and clustered MTL (CMTL) which

Experimental Setup

To decrease the effect of having considerably different amounts of training data in different domains and to equally
weight the contribution of each domain in the learning process, we train all models using the same number of instances
from each domain. Hence, we select 65, 445 instances (size
2
A few number of features employed in [11] which cannot
be applied for all the domains are ignored. Note that the
baseline results with and without using these features are
approximately equal. Designing an adaptive model to handle different features in each domain will be considered in
the future.
3
Available at http://ece.ut.ac.ir/node/100770

1012

Table 2: F1-measure and BA achieved by in-domain and
cross-domain experiments.

PP

PP Test
PP
P

Train

IMDb
YouTube
Goodreads
Pandora

F1
BA
F1
BA
F1
BA
F1
BA

IMDb

YouTube

0.1206
0.5948*
0.1406
0.5470
0.1288
0.5732
0.1304
0.5732

0.3696
0.6679
0.3932*
0.6975*
0.3317
0.6633
0.3329
0.6639

Goodreads
0.2033
0.5851
0.1970
0.5596
0.2175*
0.6353*
0.2050
0.6109

test data from the same domain) experiment is significantly
higher than all the cross-domain experiments (training and
test data are from different domains). Except for IMDb,
there is a similar pattern for F1-measure, i.e., in-domain result is higher than cross-domain results in each test set. In
IMDb, all the cross-domain experiments achieve higher F1measure compared to the in-domain experiment, although
the improvements are not significant. The reason could be
that IMDb is the most sparse dataset among other domains
(see Table 1) and because of this highly imbalanced situation, the learning models cannot create a proper model using
the IMDb training data and the learned model is biased towards the majority class. That is why F1-measure (which
considers the precision and recall of the positive class) for
cross-domain experiments are higher, but BA (which considers both classes equally) are not.

Pandora
0.1661
0.5667
0.1350
0.5387
0.1572
0.5774
0.1741
0.5899*

of the smallest domain) for each domain and cut it in half to
create training and test sets. We repeat this process 30 times
by shuffling the data of each domain randomly. Average of
the results obtained on 30 shuffles are reported.
As expected and shown in Table 1, percentage of data with
positive engagement is by far lower than percentage of those
with zero engagement. This makes our data highly imbalanced and thus, accuracy could not be a proper evaluation
metric for this task. We consider two widely used evaluation metrics for classification in imbalanced environments:
F1-measure and balanced accuracy (BA) [9].4
In all experiments, the instance weighting technique is
applied for both baselines and the proposed method.5 For
hyper-parameter optimization in all methods, we perform
stratified k-fold cross validation over training data and find
the optimum parameters using grid search. Moreover, statistical t-test with 95% confidence is used to find the significant
differences between results. The experiments are done using
Scikit-learn6 and MALSAR7 packages.

3.3

3.3.2

Results and Discussion

In this section, we try to experimentally answer the two
research questions mentioned in Section 1.

3.3.1

Analysis of the Adaptive Model

In the second set of experiments, to evaluate the performance of MTL methods, we compare them with two baselines: (1) STL: a single-task classifier which is trained on
in-domain data, and (2) STL-Pooling: a single-task classifier which is trained on the data of all domains. We employ
SVM as a single-task classifier in our experiments, which has
been shown to be highly effective in different tasks. Since
the mentioned MTL methods are linear, we also consider
linear kernel for SVM to have a fair comparison.
To evaluate the methods with different amounts of training data, we consider 10 subsets of the created training set
for each domain in each random shuffle with different sizes
ranging from 10% to 100% of the instances. Figure 1 shows
the learning curves on the four target domains in terms of
F1-measure and BA.
According to Figure 1, the results of MTL methods in each
domain are very close to each other. The reason is that all
domains are related and the group structured assumption
in MTL CASO does not affect the results, significantly. In
the following, we analyze the results for each target domain
separately and then, provide a general analysis.
IMDb. The results show that STL always performs better than STL-Pooling in terms of BA, but there is not a similar observation in terms of F1-measure; the reason could be
that based on Table 1, IMDb domain is the most sparse domain and with the limited amount of training data, STL cannot create a proper model for detecting positive instances.
Therefore, pooling the domains can help to improve the results in terms of F1-measure which combines the precision
and recall of the positive class. Note that since the distributions of data in different domains vary a lot, by increasing
the number of instances the performance of STL is also increased, but the STL-Pooling’s performance is not. In other
words, distribution of all the training data together differs
with the distribution of the IMDb test data. The results
also show that MTL methods outperform the baselines, in
particular when the portion of training data is more than
50%. Statistical test shows that these improvements are
statistically significant.
YouTube. Since other domains are more sparse than
YouTube domain, pooling the domains can be effective in
detecting the tweets with zero engagement. That is why the
BAs achieved by STL-Pooling in beginning of the learning
curve are higher than those obtained by the other methods.
As expected, STL-pooling could not perform well in terms

Exploring Gaps Between Domains

The statistics in Table 1 demonstrate several differences
among different domains. According to this table, average
engagement of tweets varies a lot between different domains.
For instance, average engagement in YouTube tweets is six
times higher than that in Pandora tweets. These differences
might be related to different functionalities of these applications and also their popularity. Similar differences can
be captured by considering number of tweets with positive
engagement in the domains.
In addition to the different nature of the domains explained above, we design a set of experiments to investigate
differences between the distributions of data in various domains. To this end, we train an SVM classifier with linear
kernel on the training data of each domain and test it on the
test data of all domains. The results in terms of F1-measure
and BA are reported in Table 2. In each target domain (each
column), the result which is significantly higher than its corresponding values is marked by “*”. Table 2 shows that in
all target domains, BA obtained by in-domain (training and
4

BA is computed as the mean of accuracy in each class.
For the sake of space, the results without instance weighting which are far below the reported results are not reported.
6
http://scikit-learn.org/
7
https://github.com/jiayuzhou/MALSAR
5

1013

●

0.15

0.41
●
●

0.14

●

●

●

●

●

●
●

0.40

●

0.23

0.17

0.22

0.16

●
●

●

●
●

●

●

0.10
0.09
0.08
0.07
10

20

30

40

50

0.38

●

0.37
0.36
0.35

MTL CASO
MTL Trace
STL
STL−Pooling

●

0.34
0.33
60

70

80

90 100

10

20

0.62

30

0.19
0.18

●

0.16
0.15
60

70

80

90 100

10

20

30

40

50

●

0.14
●

0.13
0.12
0.11

0.09
60

70

80

90 100

10

20

●

●

●

●

●

●

0.62

●

●

0.60

●

40

50

60

70

80

90 100

0.58

10

20

30

40

50

60

70

80

90 100

60

70

●

10

0.50

MTL CASO
MTL Trace
STL
STL−Pooling

●

0.52
20

30

40

50

MTL CASO
MTL Trace
STL
STL−Pooling

●

0.48
60

70

80

90 100

% training instances

% training instances

0.54
0.52

0.54

MTL CASO
MTL Trace
STL
STL−Pooling

●

●

●

0.56

0.60

●

0.56

●

BA

BA

BA

0.66

0.62

MTL CASO
MTL Trace
STL
STL−Pooling

●
●
●
●

●
●

0.64

% training instances

90 100

●
●

0.58

●

●

●

30

80

0.60

●

●

●

0.54

20

50

●

0.68

10

40

●
●

0.70

●

●

30

% training instances

●

0.50

MTL CASO
MTL Trace
STL
STL−Pooling

●

0.10

0.64

●

●

0.52

70

0.15

% training instances

0.72

●

60

●
●

●

MTL CASO
MTL Trace
STL
STL−Pooling

●

% training instances

●

0.60

BA

50

●

●

●

●

●
●

0.56

40

0.20

●

●
●

0.17

MTL CASO
MTL Trace
STL
STL−Pooling

●

% training instances

0.58

●

●

0.21

F1−measure

●

0.39

F1−measure

F1−measure

F1−measure

0.12
0.11

●

●

●

0.13

●
●

●

10

20

30

40

50

80

90 100

% training instances

IMDb
YouTube
Goodreads
Pandora
Figure 1: Learning curve of STL, STL-Pooling, and MTL methods over four domains in terms of F1-measure and BA.
of F1-measure; because, pooling with more sparse data reduces the accuracy of SVM to classify positive instances. By
increasing the amounts of data, MTL methods outperform
both STL methods. In addition, since YouTube domain is
more balanced than the other domains, the results obtained
on YouTube are higher than the others.
Goodreads and Pandora. The results of these two domains show that MTL methods outperform the baselines.
The BA improvements are always significant, but the F1measure improvements usually are not. This shows that
transferring the knowledge helps more to correctly classify
the zero engagement instances in these two domains. Because of the differences between training data in different
domains, STL-Pooling could not perform well.
Considering the results of all domains, by increasing the
number of training instances from 10% to 100%, the results
of MTL methods vary from 4% to 6% in terms of F1-measure
and from 3% to 5% in terms of BA. The learning curves in
terms of BA become stable earlier than that of F1-measure.
To wrap it up, when enough amount of training data is available, STL in general performs better than STL-Pooling and
MTL methods perform better than STL. Conversely, if limited amount of training data is available, STL-Pooling may
perform better than STL and again, MTL methods usually
perform better than the baselines. Therefore, we can conclude that multi-task learning can transfer knowledge between these domains to improve the performance.

4.

work is to recognize which domains can affect a given target
domain. Other user engagement evaluation problems, such
as ranking the tweets based on their engagements, could also
be studied in the future.

5.

REFERENCES

[1] R. Akbani, S. Kwek, and N. Japkowicz. Applying
support vector machines to imbalanced datasets. In
ECML, pages 39–50, 2004.
[2] J. G. C. de Souza, H. Zamani, M. Negri, M. Turchi,
and D. Falavigna. Multitask learning for adaptive
quality estimation of automatically transcribed
utterances. In NAACL-HLT, pages 714–724, 2015.
[3] R. Caruana. Multitask learning. Machine Learning,
28(1):41–75, 1997.
[4] J. Chen, J. Liu, and J. Ye. Learning incoherent sparse
and low-rank patterns from multiple tasks. ACM
Trans. Knowl. Discov. Data, 5(4):22:1–22:31, 2012.
[5] J. Chen, L. Tang, J. Liu, and J. Ye. A convex
formulation for learning shared structures from
multiple tasks. In ICML, pages 137–144, 2009.
[6] S. Dooms, T. De Pessemier, and L. Martens. Mining
cross-domain rating datasets from structured data on
twitter. In MSM@WWW, 2014.
[7] D. Loiacono, A. Lommatzsch, and R. Turrin. An
analysis of the 2014 recsys challenge. In
RecSysChallenge, pages 1–6, 2014.
[8] S. Petrovic, M. Osborne, and V. Lavrenko. Rt to win!
predicting message propagation in twitter. In ICWSM,
pages 586–589, 2011.
[9] D. Powers. Evaluation: From precision, recall and
f-measure to roc, informedness, markedness &
correlation. J. Mach. Learn. Tech., 2(1):37–63, 2011.
[10] I. Uysal and W. B. Croft. User oriented tweet ranking:
A filtering approach to microblogs. In CIKM, pages
2261–2264, 2011.
[11] H. Zamani, A. Shakery, and P. Moradi. Regression
and learning to rank aggregation for user engagement
evaluation. In RecSysChallenge, pages 29–34, 2014.

CONCLUSIONS AND FUTURE WORK

In this paper, we proposed an adaptive method based on
multi-task learning technique for user engagement evaluation. To be able to cope with imbalanced data, we modified
the logistic loss function in the mutli-task learning methods
by adding an instance weighting matrix to its formulation.
We considered four popular web applications in our experiments: IMDb, YouTube, GoodReads, and Pandora. The experimental results showed that distributions of data in these
domains are different and multi-task learning methods can
transfer knowledge between the domains to improve user
engagement evaluation performance. A direction for future

1014

