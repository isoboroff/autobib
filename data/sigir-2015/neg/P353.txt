BROOF: Exploiting Out-of-Bag Errors, Boosting and
Random Forests for Effective Automated Classification
Thiago Salles

Marcos Gonçalves

Victor Rodrigues

∗

Leonardo Rocha

Federal University of Minas Gerais
Computer Science Department
Belo Horizonte, Brazil

Federal University of São João Del-Rei
Computer Science Department
São João Del-Rei, Brazil

{tsalles, mgoncalv, victor.rodrigues}@dcc.ufmg.br

lcrocha@ufsj.edu.br

ABSTRACT

ray domains, a surprising result given the knowledge that
there is no single top-notch classifier for all datasets.

Random Forests (RF) and Boosting are two of the most successful supervised learning paradigms for automatic classification. In this work we propose to combine both strategies in order to exploit their strengths while simultaneously
solving some of their drawbacks, especially when applied
to high-dimensional and noisy classification tasks. More
specifically, we propose a boosted version of the RF classifier (BROOF), which fits an additive model composed by
several random forests (as weak learners). Differently from
traditional boosting methods which exploit the training error estimate, we here use the stronger out-of-bag (OOB)
error estimate which is an out-of-the-box estimate naturally
produced by the bagging method used in RFs. The influence of each weak learner in the fitted additive model is
inversely proportional to their OOB error. Moreover, the
probability of selecting an out-of-bag training example is
increased if misclassified by the simpler weak learners, in
order to enable the boosted model to focus on complex regions of the input space. We also adopt a selective weight
updating procedure, whereas only the out-of-bag examples
are updated as the boosting iterations go by. This serves
the purpose of slowing down the tendency to focus on just
a few hard-to-classify examples. By mitigating this undesired bias known to affect boosting algorithms under high
dimensional and noisy scenarios—due to both the selective
weighting schema and a proper weak-learner effectiveness
assessment—we greatly improve classification effectiveness.
Our experiments with several datasets in three representative high-dimensional and noisy domains—topic, sentiment
and microarray data classification—and up to ten state-ofthe-art classifiers (covering almost 500 results), show that
BROOF is the only classifier to be among the top performers in all tested datasets from the topic classification domain,
and in the vast majority of cases in sentiment and microar-

Categories and Subject Descriptors
I.5.4 [Applications]: Text processing

General Terms
Algorithms; Experimentation

Keywords
Classification; Random Forests; Boosting

1.

INTRODUCTION

This paper advances the state of the art in automatic classification by “smoothly” combining, in an original way, characteristics of two learning strategies which excel in a variety
of learning tasks: the Random Forest classifier and boosting. Proposed in [2], the Random Forest (RF) classifier has
been surprisingly successful in a wide variety of automatic
classification tasks, being considered by many [6, 14] as a
top-notch supervised algorithm, comparable (sometimes superior) to Support Vector Machine (SVM) classifiers.
The RF classifier is a variation of the classic bagging
(bootstrapped aggregation) procedure, where an ensemble
of m decision tree classifiers are trained with sets of bootstrapped samples1 drawn from the training set Dtrain . The
crucial aspects, as pointed out in [2], that make the RF
classifier a good classifier are (i) the reduced correlation between the decision trees composing the ensemble and (ii) the
better-than-random-guess predictions of each tree. In fact,
to achieve reduced correlation, each tree is learned with a
bootstrapped sample of Dtrain and each decision node is specified by a chosen attribute drawn from a randomly chosen
subset of features. To achieve strong decision trees (i.e.,
trees with better prediction performance than random guessing), each tree is typically grown to its maximum depth.
Taken together, the RF classifier averages several de-correlated (rather complex) models, ultimately reducing variance
while keeping the bias unchanged [14]. This typically brings
up better models, with higher generalization power, that can
successfully be applied to a wide variety of datasets.
A very important aspect for the sake of our work, is that
the RF classifier is able to produce robust estimates of generalization power, the so-called out-of-bag (OOB) error estimate, which comes naturally from the bagging procedure

∗
This work was partially supported by CNPq, CAPES,
FINEP, FAPEMIG and INWEB.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’15, August 09–13, 2015, Santiago, Chile
Copyright 2015 ACM 978-1-4503-3621-5/15/08...$15.00.
http://dx.doi.org/10.1145/2766462.2767747.

1

353

Examples randomly sampled with replacement.

this classifier exploits. As the RF classifier bootstraps the
training dataset, roughly e−1 training examples are left out
when learning a decision tree2 . These examples, the socalled OOB samples, can be effectively used to assess generalization power of the tree. Considering the tree ensemble,
an averaged estimate can be used to assess the generalization
power of the RF classifier. As it is already known, the outof-bag estimates are efficient to compute, since they can be
measured when learning the classifier, with little additional
effort. Moreover, it is capable of producing close approximations of the expected error rate, which is unbiased (unlike
the training error rate). As we shall see, the exploitation of
the OOB error is one of the core aspects of our proposal.
However, RFs are not free of drawbacks. Despite being a
classifier with great generalization capabilities, when learned
from noisy data, common in datasets based on human language or imprecise measurements, RFs have been known to
cause overfitting [25], degrading classification effectiveness
on unseen test data. More specifically, it has been shown
that the RF classifiers whose decision trees are grown to
their maximum depth are deemed to perform poorly in the
presence of many noisy attributes (e.g., rare, anomalous,
irrelevant, with low discriminative power and low generalization in the test sets).
In fact, [25, 27] validate the overfitting issue faced by random forest models when learning to classify high-dimensional
noisy data. In both works, the authors showed that there exist some data distributions where maximal unprunned trees
used in the random forests do not achieve as good performance as the trees with smaller number of splits and/or
smaller node size. This has to do with the unnecessary
variance [14] incurred by the model: the bagged decision
trees become plagued by irrelevant or noisy attributes, which
introduces unnecessary model complexity. This ultimately
hampers its effectiveness when applied to new unseen data.
However, if one simply prunes the RF’s trees, the overall classification effectiveness may be compromised in highdimensional classification tasks plagued by irrelevant attributes, since the probability of selecting informative attributes
is reduced [18]. This is a critical issue, since in real world
problems it is entirely conceivable that one can be confronted
with noisy, skewed, correlated and high-dimensional data.
As we shall see in our experiments, contrasting the effectiveness of the traditional Random Forest classifier in a
series of datasets—characterized by high dimensionality and
noise—against the baselines we can observe that the effectiveness of such a classifier is often not satisfactory, performing poorly when compared to other classifiers in most cases.
This confirms some of the posed arguments regarding the
drawbacks of this algorithm in the target scenario.
Another well studied and successful classification technique is boosting. Recent studies [3, 10] have shown that
boosting provides excellent predictive performance across a
wide variety of tasks. It is an iterative algorithm which combines several models, called weak-learners, which influence
the final decision proportionally to their accuracy. Furthermore, each successive classification model focuses on even
more complex regions of the input space, by means of input
data manipulation. Two aspects play a key role here: (i) the

influence of each learner in the fitted additive model, and
(ii ) the focus on hard to classify regions of the input space.
More importantly, since boosting combines the predictions
of multiple weak-learners it also is able to significantly reduce unnecessary variance. Thus, the fundamental aspects
that must be defined include: (i) how to re-weight the training examples in each subsequent iteration, and (ii) how to
weight each single prediction in order to generate the final
prediction. Such weights are usually based on the in-sample
error rate. The downside is that it is an overly optimistic
estimate of generalization performance.
The above discussion highlights potential opportunities
to combine random forests and boosting in order to exploit
their strengths while, at the same time, helping to solve
some of their drawbacks, especially for automatic classification of high-dimensional noisy data. That is exactly what
we do. In more details, we propose to exploit a boosting strategy to guide a sequence of random forest classifiers in specific sub-regions of the input space (those with
higher uncertainty), ultimately constraining the input space
search to relevant sub-regions and thus mitigating the overfitting issue by avoiding noisy and irrelevant features. This
has the potential to circumvent some of the aforementioned
drawbacks that may degrade the performance of the traditional RF classifier under high-dimensional noisy classification tasks. However, differently from traditional boosting
methods, which uses the training error estimate as a basis
to update the ensemble, we here propose to use the stronger
out-of-bag (OOB) error estimate which is out-of-the-box estimate naturally produced by the bagging method used in
RFs. In such way, the random forest classifier can produce
much more robust estimates for generalization capability,
which can be exploited by the boosting process both to reweight training examples and to weight the sequence of predictors in order to come up with a more robust final classification model. A second important difference is that we
employ a less aggressive selective weight update. In other
words, by selectively updating only the weights associated
with the out-of-bag examples, we slow down the undesired
bias of the boosting strategy towards a few hard-to-classify
examples, which helps to prevent overfitting, while still driving down misclassification rate of the ensemble.
In sum, the main contribution of this work is an advance
in the state of the art in automatic classification by the
proposition of a new, original classifier (BROOF), which
smoothly combines properties of RFs and boosting. The
originality of our proposal comes from two aspects: (i) we
propose a new weighting scheme which is able to greatly
improve the boosting strategy by means of a much more reliable measure of generalization power, offered by the RF
itself: the out-of-bag error estimate, and (ii) we also propose to exploit a selective weight updating scheme, which
slows down the undesired bias towards a few input points,
thus preventing overfitting while boosting effectiveness. We
demonstrate that both contributions, as well as their interactions, are equally important for our improvements. Moreover, our experiments (covering almost 500 results) show
that, in all cases, the proposed BROOF is the only classifier
to be among the top performers in all tested datasets from
the topic classification domain, and in the vast majority of
cases in sentiment and microarray domains, each composed

2
The constant e denotes the base of the natural logarithm.
e−1 ≈ 0.37.

354

by a series of high-dimensional and noisy datasets and a rich
set of 10 state-of-the-art representative baselines. This is a
surprising result given the knowledge that there is no single
top-notch classifier for all datasets. This highlights the benefits of such a synergy between both widely used strategies.
Moreover, we also empirically demonstrate that our classifier is more “training-efficient”, i.e., it requires less training
than the baselines to produce its best results.

2.

a weighted average of the individual decision trees, whereas
each weight is proportional to the prediction performance
of the trees considering the out-of-bag training examples in
the neighborhood of x.
Aware of the potential overfitting issue faced by boosting strategies when confronted with noisy data, in [1] the
authors propose an improved random forest classifier which
exploits the adaptive re-sampling principle of boosting: the
dynamic random forests classifier. Instead of boosting random forest classifiers, as we do here, the authors propose
to proceed with the usual bagging approach of a traditional
RF. However, when learning the decision trees composing
the ensemble, an additional sample re-weighting schema (reminding boosting by re-sampling) is followed in order to
build new decision trees that focus more on hard-to-classify
regions of the input space. Starting with a uniform probability mass assigned to every training example, as new trees are
added to the ensemble, the current misclassification rate of
the out-of-bag samples are computed and used to re-weight
them. When learning the next decision tree to be added
in the ensemble, the weights of each training sample are
used to influence both the splitting criterion and the base
prediction. As stated by the authors, this serves the purpose of guiding the ensemble to hard-to-classify regions of
the input space in a slower and smoother way (one only
updates the weights of out-of-bag samples), thus avoiding
the bias towards a few hard-to-classify examples. Thus, instead of applying boosting and then bagging (such as in the
proposed BROOF classifier), the authors propose to apply
bagging and then boosting. As the authors observed in the
reported experimental evaluation, such strategy is not robust to the presence of many irrelevant attributes. In fact,
the compromise between compensating previous errors (by
means of sample re-weighting) and reinforcing correct prediction (by means of relating the weights to the accuracy
of the entire ensemble on the previous iteration) may not
be enough to guarantee robustness to datasets with many
irrelevant attributes, since overfitted trees may lead to misleading weights due to the presence of many irrelevant paths
on the decision trees, specially on the initial iterations. Since
the evaluation of the entire ensemble is critical for the reweighting step on the next iteration, and the previous ensemble state may be already overfitted, the errors may be
unwittingly propagated as the random forest is built, being
not robust to such high dimensional noisy data.
In sum, most of the previous work has tackled issues related to improving the choice of features or the quality of
the forest of trees. Here, we tackle a different issue, trying
to overcome problems related to overfitting, known to happen in RFs in the presence of noisy/irrelevant data mainly
in highly dimensional problems.
In the light of the peculiarities observed in high dimensional noisy classification tasks, here we focus on exploring
the RF classifier in a boosting strategy in order to provide
a highly effective learner for such tasks, by means of a novel
approach which is able to overcome the mentioned challenges
regarding both the RF classifier and the boosting strategy.
We also revisit the state-of-the-art in automatic classification, providing an in-depth comparison of RF-based classifiers (including BROOF) with a series of baseline algorithms
known to produce highly accurate predictions, considering

RELATED WORK

This work deals with two well known learning strategies,
the RF classifier, as well as the meta-algorithm boosting. As
we shall detail, we here propose to take advantage of a RF’s
key characteristic to provide a very accurate boosting classifier, which, as we empirically show, excels in automatic
classification of high-dimensional and noisy data. In this
section, we discuss some relevant work on these matters,
pointing out our motivations and tying together with the
literature the major contributions of this work.

2.1

Random Forests

The RF classifier was proposed in [2] as a variation of
bagging of decision trees. An ensemble of low-correlated decision trees is built through a series of random procedures,
such as bootstrapping of training data and random attribute
selection to compose the decision nodes. A plethora of proposals aimed at improving the RF effectiveness can be found
in the literature, usually characterized by reducing the correlation among the trees composing the ensemble. We review
some of them here. Since our proposal is based on boosting,
considering the RF classifier as weak-learner, we also discuss
some important work on this matter.
The popularity of RF classifiers is highlighted by their
successful application in several domains, such as object
segmentation [24] and human pose recognition [26], outperforming state-of-the-art strategies, such as the SVM classifier. Thus, it is natural to expect several extensions to such
classifier, in order to improve its effectiveness even more.
In this direction, in [20] two strategies were proposed to
improve RF by increasing the strength of the decision trees
or reducing the correlation among them. The author proposes the use of several attribute evaluation metrics to select
the attributes to be used in decision nodes, and the inclusion of a sophisticated weighted voting schema which takes
into account the effectiveness of decision trees when applied
to the training instances most similar to the test instance.
In order to introduce more randomness (and thus, reduce
the correlation among the decision trees), in [21] the socalled Rotation-RF was proposed. This strategy randomly
groups the attributes, building subproblems which are projected into new feature subspaces, reduced via PCA. The
principal components are, then, used to learn the classifier.
Some previous work dynamically build the RF classifier,
taking a closer look into the potential each decision tree in
the ensemble has to increase the overall ensemble effectiveness. More specifically, in [29], the RF classifier is modified
in order to keep just the most useful decision trees in the
ensemble. When classifying a new example x, the decision
trees with highest prediction effectiveness considering the
out-of-bag examples most similar to x receive a higher vote
in the final prediction average. That is, the final prediction is

355

three representative automatic classification domains covering a variety of high dimensional and noisy datasets.

2.2

gorithm may focus on erroneous regions of the input space,
leading to suboptimal decision boundaries. In fact, such
issue presents a challenge to boosting algorithms when applied to real-world datasets [8, 12], and it can be even more
challenging if one consider real-world data, such as textual
data (due to not only noise but also to some aspects inherently related to natural languages, such as ambiguity and
the dynamics of the language, to name a few).
We thus propose to use the RF classifier as weak-learner
for a boosting algorithm. But, unlike previous work which
did not deal with the challenging high dimensional and noisy
automatic classification scenario, we here pay special attention to the overfitting issue. Towards this end, we take
advantage of the OOB estimation, readily available when
building such a classifier, as well as of a selective weight updating strategy, in order to circumvent the overfitting issue
which can arise from both the stronger nature of the RF classifier and to the intrinsic characteristics of such challenging
data. Next we describe in details our proposal.

Boosting Methods

Before delving into our proposed strategy, we briefly discuss some work regarding boosting algorithms for classification. Boosting [23] is a meta-algorithm which employs a
general voting method for learning from a sequence of (simple) models. It is a sequential algorithm which learns several
weak-learners (that is, classifiers able to produce predictions
slightly better than random guessing) to devise very accurate predictions. For each iteration i of the boosting algorithm, let ∆i be a probability distribution over the training
1
set of size m. When i = 0, ∆0 (i) = m
, ∀i|m
i=1 . At iteration
i > 0, for each training example xk , if xk is misclassified, its
weight is increased so that, in the next iteration, an updated
training set distribution ∆i+1 is considered, which puts more
emphasis on the previously misclassified examples (i.e., the
hardest to classify ones).
The perhaps most widely used strategy is AdaBoost, which
has been shown to significantly improve the performance
of base learners. Based on the traditional AdaBoost, several boosting strategies have been proposed in the literature,
ranging from the updating strategy along the boosting iterations, to the choice of weak-learners. Regarding multiclass
boosting, there are some variations of the AdaBoost algorithm, such as the AdaBoost.M1 and AdaBoost.M2, where
the main distinctions have to do with the assumed weaklearning conditions, as detailed in [19].
Of special importance here, we cite the boosted version
of the RF classifier, proposed in [13] (here called ADARF ),
further studied in [32] and successfully applied in domains
such as breast cancer survivability prediction [28] and face
detection [31]. As the name suggests, it explores the random forest classifier as weak-learner in a boosting algorithm
based on AdaBoost. In [13] such a technique was shown
to perform well in a traffic flow problem, outperforming the
traditional AdaBoost algorithm. In [31], ADARF outperformed eight classifiers considering the UCI’s glass dataset.
Finally, in [28], ADARF outperformed ten classifiers (including AdaBoost, Decision Trees, Naı̈ve Bayes and SVM)
considering a breast cancer survivability dataset. We here
inspire ourselves in the promising synergy between boosting
and Random Forests in order to tackle the challenging text
classification task. As we shall see, there is opportunity to
leverage automatic classification quality by exploring some
advantages of both algorithms. However, we performed such
integration with different strategies, which we believe are
more suitable to our target domains.
In fact, boosting algorithms have their limitations. Although the boosting strategy was initially considered to be
immune to overfitting, it is already known that such strategy
suffers from this problem, specially in the presence of noisy
data [22, 30]. The overfitting problem can be observed when
inconsistent data (e.g., misclassified training data) is given
to the learner. It becomes even more critical if the employed
weak-learner is too strong to fit the model. That is, if instead
of combining predictions of simple learners, one combines
several complex hypothesis, such as done in [13, 28, 31, 32],
chances are that the model becomes even more overfitted to
noise data. Under an overfitted situation, the boosting al-

3.

BOOSTED RANDOM FOREST

In this section, we describe the proposed learning algorithm: the BROOF classifier, a smooth combination of boosting and random forests aimed at exploiting their strengths
while simultaneously solving some of their drawbacks in
noisy and high dimensional settings.
BROOF is an additive model composed of several random
forest classifiers, which act as weak-learners. Each fitted
model influences the final decision proportionally to its accuracy, focusing—as the boosting iterations go by—on ever
more complex regions of the input space, in order to drive
down the expected error. As usual in a boosting strategy,
two aspects play a key role: (i) the influence of each learner
in the fitted additive model, and (ii) the strategy to update
the sample distribution ∆i based on the previous boosting
iteration i − 1. As we shall describe next, we deal with both
aspects by means of the out-of-bag (OOB) error estimate
computed when learning each random forest classifier.
The out-of-bag error yields an estimate of the expected
error, sometimes referred to as the generalization error, and
is directly computed when learning a random forest classifier. Recall that such a classifier uses bootstrapped examples to build a series of decision trees. For each decision tree,
roughly 1e ≈ 37% training examples are left out by the sampling procedure [14]. Unlike the training error estimation,
which considers the same data used to learn the model to
assess misclassification rate, these out-of-bag examples gets
classified by the corresponding decision trees, acting as an
independent set (since they were not used to learn the tree).
The final classification is given by averaging all predictions
for each example. In fact, it can be shown [14] that the
OOB error estimate is less biased than the training error
estimate usually adopted to weight the weak-learners votes
in traditional boosting methods. Consequently, it leads to
better approximations for the expected error rate. This is
rather an expected result since the out-of-bag examples are
not used to learn a classifier, similarly to cross validation
procedures to assess an unbiased estimate of expected error
rate. Thus, we hypothesize that a less biased combination
of weak-learners (by means of a proper weight calculation

356

Algorithm 1 BROOF: Pseudocode
1: function Train(Dtrn )
1
2:
wi ← |Dtrn
|
3:
for each m = 1 to M do
oob
4:
hhRF
i ←LearnRF(Dtrn , ntrees )
m , (x, y, ŷ)
Pi

through OOB, as we shall see) potentially leads to an improved boosting algorithm.
More formally, let Dtrn be the training set. Initially, all
training samples have the same associated probability mass
wi . Such sample probability distribution (i.e., wi , ∀i) is employed to drive the ensemble towards hard to classify regions of the input space. Let hi be a random forest classifier
trained at iteration i of the boosting algorithm, considering the bootstrapped samples Drnd ⊂ Dtrn . The out-of-bag
samples O ← Dtrn \ Drnd are used to assess the accuracy
of hi . Considering the samples x ∈ O, those misclassified
by hi typically lie down in hard to classify regions of the
input space. Thus, if the model focuses on such regions,
the ensemble capability of accurately covering such complex
regions increases. We do so by considering a weighted out-ofbag error estimate, denoted by OOBw
err , when updating the
sample distribution. In this case, the probability of correctly
classifying the currently misclassified out-of-bag samples will
tend to increase as the iterative process continues.
The proposed strategy to update the probability distribution ∆i at boosting iteration i, which associates a probability
mass with each training example x, is based on a weighted
error estimate OOBw
err . It considers not only the misclassification of x by the weak learner but also the probability mass
in the previous boosting iteration, which is proportional to
the complexity of classifying x. More specifically, the probabilities associated to the out-of-bag
P examples are updated
w I(y6=ŷ)
w
P i
based on the estimate OOBerr ← i∈O
, where I(·)
i∈O wi
is the indicator function. As the iterative process continues,
the influence of hi predictions are properly updated (according to hi effectiveness when classifying the out-of-bag samples), as well as the probability
mass
 associated with each


5:

6:
7:
8:
9:

, where O = (x, y, ŷ)oob
i

err

wi ← wi eαm I(y6=ŷ) , where i ∈ O
end for
end function

scription of the experimental workload (i.e., classifiers and
datasets)3 . Then, we report and discuss our results.
In our experiments, we considered six real-world textual
datasets for topic classification, ten for sentiment classification, and six for microarray classification, covering overall
22 datasets. As we shall detail, we consider 11 classification
algorithms and two evaluation metrics. This encompasses a
set of 484 results, an extensive experimental evaluation.

4.1

Baseline Classifiers

As the traditional classifiers (SVM, KNN, DT, NB) are
well known in the literature, here we only describe in details
the less known ones or those more similar to our approach.
AdaBoost.M2 (ADA.M2) is an extension of the traditional AdaBoost classifier for multiclass classification problems [7]. For each boosting iteration, a bootstrap sample of
the training set is picked in order to learn the weak classifier.
A pseudo-loss function is then computed in order to update
the weights of each training example and the weight associated to the weak-learner which reflects its influence in the
final model. We here adopt as weak-learner the so-called decision stumps—decision trees with a single decision node—
as it is the most common strategy. We also consider what
we call AdaBoost.FGDT (ADA.FGDT), where full grown
decision trees are used as weak-learners [3]. Some previous
work have shown that this strategy may outperform boosted
decision stumps.
AdaBoost.RF (ADARF) is an AdaBoost-based classifier in which a random forest classifier acts as a weaklearner [13, 32]. Unlike our approach, here the usual weighting strategy is adopted, where the in-sample error rate is key
when updating the probability mass associated with each
training example as the boosting iterations go by. Similarly to the AdaBoost algorithm, the weight of a misclassified example is increased, enabling the boosting procedure
to concentrate on the hard examples in the training set in
subsequent rounds. Such weighting schema, which is based
on training error and updates all training instances as the
boosting iterations go by is prone to overfitting in scenarios such as the observed in text classification tasks, since it
tends to focus on a few hard-to-classify examples, as previously mentioned.
Gradient Boosted Decision Trees (GBDT). Unlike
the AdaBoost variants described above, here we fit an additive model in a stage-wise manner, in the form of a typical

1−OOBw

x ∈ O, by the factor log OOBwerr .
err
Recall that, as previously discussed, boosting algorithms
tend to overly increase the weights of a few hard-to-classify
examples, specially in the presence of noisy data. In the
proposed algorithm, just the probabilities wi related to the
selected out-of-bag examples are updated at each round, minimizing the undesired bias towards these hard-to-classify
examples—thus reducing the negative effect of the overfitting problem—while still driving down the misclassification
rate. That is, we hope that the weak learner hi+1 will have
better generalization power than the weak learner hi without overfitting to hard-to-classify regions, while maintaining
high classification effectiveness due to the stronger nature of
the base-learners. We summarize the proposed method in
Algorithm 1. The final prediction rule is then given by an
additive combination of the weak-learners, weighted by αm .

4.

w I(y6=ŷ)

i∈O
P i
OOBw
err ←
i
i∈O w

1−OOBw
αm ← log OOBwerr

EXPERIMENTAL EVALUATION

In order to evaluate the effectiveness of our proposal, we
contrast BROOF against a large set of baselines covering a
variety of learning paradigms, including traditional ones—
SVM, KNN, Decision Trees (DT), Naı̈ve Bayes (NB)—as
well as “natural baselines”: Random Forests, AdaBoost.M2,
Gradient Boosted Decision Trees and the closest approach
to ours: ADARF. In the following, we provide a brief de-

3
All reference datasets can be downloaded at
http://homepages.dcc.ufmg.br/~tsalles/broof/.
Our
implementation of the classifiers will be available soon
under request.

357

functional gradient descent procedure. At each iteration a
new tree that best minimizes a loss function L is added to the
model, according to L’s negative gradient. Since our task is
classification, we optimize for the deviance loss function [9].
We also consider its stochastic counterpart (SGBDT), by
fitting trees considering a random subset of training data
(thus reducing the variance of the final model).

4.2

macro averaged F1 (MacroF1 ). While the MicroF1 measures the classification effectiveness over all decisions (i.e.,
the pooled contingency tables of all classes), the MacroF1
measures the classification effectiveness for each individual
class and averages them.
All experiments were executed using a two-round 10-fold
cross-validation procedure. This is a cross-validation procedure in which the free parameters of each classifier are set by
means of an additional cross-validation step over the training
set and the effectiveness of the algorithms are measured in
the test partition6 . Concerning the proposed BROOF classifier, we fixed the size of the weak-learners to 5 trees, setting
the maximum number of iterations to 200. For the RF classifier, we learned at most 200 trees and, for all datasets,
convergence was reached with at most 100 trees. We assess the statistical significance of our results by means of
a paired t-test with 95% confidence. The obtained results
can be found in Table 1. The top-performers, with 95%
confidence, are shown in bold.
Before diving into the results, we would like to notice that
some of the results obtained in some datasets may differ
from the ones reported in other works for the same datasets
(e.g., [11, 16]). Such discrepancies may be due to several
factors such as differences in dataset preparation7 , the use
of different splits of the datasets (e.g., some datasets have
“default splits” such as REUT and RCV18 ), the application of some score thresholding, such as SCUT, PCUT, etc.,
which, besides being an important step for multilabel problems, also affects classification performance by minimizing
class imbalance effects, among other factors. We would like
to stress that we ran all algorithms under the same conditions, with the best standard weighting schemes for each of
them, using standardized and well-accepted cross-validation
procedures that optimize parameters for each of them and
apply the proper statistical tools for the analysis of the results. We consider that those decisions are in accordance
with our main evaluation goal (i.e., to compare the learning
algorithms), since its is fundamental to carefully isolate the
important factors under study. Therefore, we believe our results are a valid comparison across datasets and algorithms.
Moreover, all our datasets (and implementations in the near
future) are available for others to replicate our results and
test different configurations.

Topic Categorization

One of the challenges when categorizing textual data into
topics is that textual documents are usually represented by
a great amount of features (high dimensionality) and most
of them could be irrelevant or noisy, due to inherent properties of natural languages, such as the presence of synonyms,
ambiguities, to name a few. However, despite being a challenging application domain, it is of great importance nowadays, due to its wide applicability and demand. In order to
evaluate the BROOF classifier for topic categorization, we
consider here six real-world datasets, related to computer
science articles (ACM), news (REUT and UniRCV1), web
pages (4UNI) and medicine (MEDLINE). Due to space restrictions, a detailed description of each can be found in an
online appendix4 .
For all datasets, we performed a traditional preprocessing
task: we removed stopwords, using the standard SMART
list, and applied a simple feature selection by removing terms
with low “document frequency” (DF)5 . In particular, in the
case of the original RCV1 dataset, a multi-label one, the
multi-label cases need special treatment, such as score thresholding, etc. (see [17] for details), in order to be properly
consumed by unilabel classifiers. As our current focus is on
unilabel tasks, to allow a fair comparison among the other
datasets (which are also unilabel) and all baselines (which
also focus on unilabel tasks), we decided to remove the documents assigned to more than one class from RCV1, deriving
a new dataset which we call UniRCV1. This collection has
101 classes and about 20% less documents. Nevertheless, as
we shall see, the effectiveness levels obtained by our method
and the best baselines are still compatible with those of the
original multilabel RCV1.
In terms of baseline implementations, for SVM we use
the liblinear library [5] while for the other algorithms we
developed our own implementations. In particular, for NB
we adopt the Multinomial Naı̈ve Bayes approach. The free
parameters of these classifiers include the cost C for SVM,
neighborhood size k for KNN and minimum number of examples δmin in a leaf node for DT. For the Random Forest (RF) algorithm, we considered unprunned trees, since
it is already known that unprunned trees perform better
than pruned trees when applied to high dimensional noisy
datasets [18]. The free parameter of this classifier has to
do with the number of trees to compose the ensemble. Finally, all the training and test examples were represented by
TFIDF vectors for all algorithms, except for Multinomial
NB (whose optimal performance was obtained using TF).

4.2.1

4.2.2

Briefly, regarding our experimental results, they show that,
overall, BROOF outperforms or ties all the state-of-the-art
topical text classifiers (SVM, NB and DT), as well as the traditional RF (that, to the best of our knowledge, had not been
thoroughly investigated in this scenario). It also outperforms, by large margins, the widely used lazy classifier KNN.
In our experiments, the proposed BROOF classifier was
the only classifier to produce the best results in all tested
6

A more reliable procedure than setting up an single “validation set”, since it allows us to assess statistical significance
when tuning the free parameters.
7
For instance, some works do exploit complex feature
weighting schemes or feature selection mechanisms that do
favor some algorithms in detriment to others.
8
In fact, we do believe that running experiments only in the
default splits is not the best experimental procedure as it
does not allow a proper statistical treatment of the results.

Experimental Protocol

The methods were compared using two standard information retrieval measures: micro averaged F1 (microF1 ) and
4
5

Discussion and Analyses

http://homepages.dcc.ufmg.br/~tsalles/broof/.
We removed all terms with DF ≤ 5.

358

20NG

4UNI

REUT

ACM

UniRCV1

MEDLINE

BROOF

micF1
macF1

87.98 ± 0.71
88.02 ± 0.78

82.03 ± 1.54
69.43 ± 1.95

65.07 ± 1.44
31.98 ± 1.59

72.02 ± 1.14
60.77 ± 1.54

84.76 ± 0.03
63.63 ± 0.04

86.84 ± 0.11
76.87 ± 0.44

RF

micF1
macF1

70.65 ± 1.41
69.67 ± 1.28

82.00 ± 0.98
69.56 ± 1.90

61.76 ± 2.17
23.58 ± 2.31

69.51 ± 0.92
55.38 ± 1.00

70.96 ± 0.27
46.74 ± 1.00

81.09 ± 0.15
71.78 ± 0.41

DT

micF1
macF1

44.25 ± 1.05
43.60 ± 0.89

70.49 ± 1.25
59.22 ± 2.81

55.20 ± 1.19
18.20+ = 1.37

57.54 ± 0.73
45.87 ± 1.50

65.58 ± 1.08
37.49 ± 1.27

76.20+ = 0.10
64.87+ = 0.37

NB

micF1
macF1

86.49 ± 1.12
85.71 ± 1.02

61.41 ± 1.38
53.18 ± 2.18

66.64 ± 1.77
33.58 ± 1.97

73.77 ± 1.03
57.21 ± 0.84

66.65 ± 0.25
49.97 ± 0.09

80.94 ± 0.12
64.66 ± 0.63

KNN

micF1
macF1

89.16 ± 0.63
88.84 ± 0.59

71.63 ± 1.36
57.15 ± 1.91

65.18 ± 1.67
32.25 ± 2.60

71.34 ± 0.96
60.92 ± 1.47

75.28 ± 0.13
62.51 ± 0.21

83.01 ± 0.13
72.70 ± 0.42

SVM

micF1
macF1

84.51 ± 1.56
83.85 ± 1.70

75.37 ± 1.17
65.56 ± 1.81

66.90 ± 2.19
33.53 ± 3.27

74.55 ± 1.00
63.01 ± 1.74

84.56 ± 0.09
63.55 ± 0.12

86.99 ± 0.09
77.11 ± 0.56

ADA.FGDT

micF1
macF1

76.36 ± 0.81
74.80 ± 0.75

78.61 ± 1.78
58.18 ± 1.03

61.45 ± 0.60
30.02 ± 0.42

60.19 ± 0.78
46.08 ± 1.16

68.07 ± 0.23
47.12 ± 0.10

78.51 ± 0.03
64.91 ± 0.06

GBDT

micF1
macF1

80.38 ± 1.20
80.45 ± 0.75

78.93 ± 1.42
62.88 ± 1.99

59.23 ± 2.61
29.82 ± 2.17

72.70 ± 0.77
60.66 ± 2.04

81.42 ± 1.03
58.23 ± 1.25

85.05 ± 0.14
77.83 ± 0.30

SGBDT

micF1
macF1

81.45 ± 1.10
80.63 ± 0.86

82.96 ± 1.16
70.33 ± 1.98

59.27 ± 1.58
32.24 ± 1.60

72.98 ± 0.77
61.85 ± 0.77

83.37 ± 0.44
61.38 ± 0.61

81.79 ± 0.17
73.71 ± 0.35

ADA.RF

micF1
macF1

84.75 ± 0.98
83.98 ± 0.87

78.45 ± 1.34
62.23 ± 1.75

62.73 ± 1.88
31.42 ± 1.99

70.21 ± 1.11
47.50 ± 0.99

67.96 ± 0.14
48.02 ± 0.09

77.19 ± 0.05
65.63 ± 0.02

ADA.M2

micF1
macF1

53.01 ± 0.82
54.27 ± 0.93

73.00 ± 1.55
62.79 ± 2.14

57.43 ± 0.98
25.02 ± 1.41

57.98 ± 1.77
48.44 ± 1.33

65.80 ± 0.55
45.96 ± 0.22

63.78 ± 0.98
66.82 ± 0.04

Table 1: Experimental Evaluation—Obtained Results for Topic Categorization.
datasets considering all metrics, a surprising result given the
knowledge that there is no single universal top-notch classifier for all problems. Moreover, when compared to its traditional counterpart (RF), we obtained gains of up to 24.53%
in MicroF1 (20NG) and 35.62% in MacroF1 (REUT), being
better than traditional RF in five out of six datasets (and
tied in the other one). Notice also that the traditional RF
does not perform so well, losing to traditional classifiers such
as KNN and SVM in several datasets. These results corroborates two of our arguments: (i) the drawbacks of the traditional RF for classification of noisy and high dimensional
data; and (ii ) our proposal was successful in overcoming
these drawbacks.
Considering the boosting baselines, our experiments show
that our proposal achieves consistently higher effectiveness.
Particularly, when compared to the closest approach to ours
(ADARF), we can see that our proposal beats this classifier
in 5 out of 6 datasets, tying only in one case. In fact, large
gains of up to 24.72% in MicroF1 (UniRCV1) and 32.5%
in MacroF1 (also UniRCV1) can be obtained. The gains
against GBDT and other boosting baselines are also similar.
This clearly demonstrates the benefits of using the out-ofbag estimates, along with the selective weight updates, as
described in Section 3. When compared to the overall best
baseline (SVM), we statistically outperformed it in 2 out of
6 datasets, with gains of up to 8.84% in MicroF1 and 5.90%
in MacroF1 (4UNI), tying in the other four datasets. Compared to the lazy baseline classifier (i.e., KNN), we obtained
statistically significant improvements in most datasets, with
gains of up to 149.87% and 131.76% in Micro-F1 and MacroF1 (UniRCV1).
Overall, it is now clear the benefits of taking advantage
of the successful boosting strategy smoothly coupled with
the RF classifier, which is able to produce rich information
through the out-of-bag samples.

4.2.3

factorial design experiment [15] in which the two factors under study are the use of OOB based weights and the use of
selective updates. For each factor, we assume k = 2 levels:
presence or absence. The response variable is the classification effectiveness, given by micro-averaged F1 .
Dataset

20NG
4UNI
ACM

Explained Variation (%)
OOB Weights

Selective Updates

Interaction

ε

32.03
32.55
32.99

33.65
33.54
33.99

31.30
31.24
30.00

2.02
2.67
3.02

Table 2: The explained variation of the OOB weighting and the selective weight updates on MicroF1 .
For each configuration, we run a 10-fold cross validation
to assess classification effectiveness and account for experimental errors. The results of this experiment are shown in
Table 2. For space reasons, in this analysis we only show results for the datasets in which we obtained the largest gains.
As we can observe, not only the main factors (the use of selective updates and an OOB based weighting schema) play
an important role in improving the boosting algorithm with
random forests as weak-learners, but also their interactions.
As reported in Table 2, each factor explains roughly 33% of
the variations observed in classification effectiveness, as well
as their interactions. This means that, although one can
achieve some improvements in classification effectiveness by
considering a single factor in isolation, it is usually better to
consider both, since their interactions also provide observable variations in the response variable.

4.2.4

Sample Size Experiments

Finally, we discuss issues related to the convergence and
the need of training samples for our proposed method and
the baselines. We have empirically found that in most cases
BROOF needs less training data to achieve good classification results. This has positive implications in the practical
application of the algorithm, mainly regarding labeling effort
in real-world scenarios, which is usually very expensive and
cumbersome. For space reasons, in here we only show results
for MacroF1 , but results for MicroF1 follow somewhat similar patterns. Also for space reasons, we show results only

Effects of the Weighting Strategy

An important aspect to analyze is the extent to which
the two key components of our proposed algorithm, namely:
(i ) the use of OOB weights and (ii ) the selective weight
updates, influence the final results in order to better understand the obtained improvements. For this we run a full 2k

359

(a) 20NG

(b) 4UNI

(c) ACM

(d) UniRCV1

Figure 1: Effect of training sample size on classification effectiveness (Macro-F1 ).
for four datasets, as the patterns found in the other two
datasets are similar. In our analysis, we compare BROOF
with the best baselines in each dataset. Figure 1 shows the
results of this analysis, when we vary the size of the training
set to 10%, 20%, · · · , 100%. Results correspond to the average of 10 test runs. We do not show confidence intervals
for the sake of readability.
As it can be seen in Figure 1(a), for 20NG, the best baseline (KNN) is only competitive when using all the available
training, while BROOF stabilizes around 70% of training
data. Similarly, for 4UNI (Figure 1(b)), the best baseline
in this dataset (RF) could only achieve competitive results
when using the entire training data. However, differently
from before, our proposed method keeps improving with
more training. In any case BROOF results with around 7080% of the training data are very competitive. For ACM,
BROOF stability is achieved with about 60% of the training
data, with small fluctuations for larger fractions, all within
the statistical tie margins. The best baseline, SVM, becomes competitive with BROOF when it is uses about 70%
of training, loosing in all cases when smaller fractions are
used. The second best baseline (NB) is only competitive
using 100% of training. Notice also that in this dataset the
effectiveness of BROOF is much more stable, with results
with about 50% of training not far away from the peak in
effectiveness. Finally, in UniRCV1 (Figure 1(d)), the curves
for BROOF and the best baseline (SVM) are the closest
ones among those analyzed, but, as before, SVM only ties
with our proposed method when using about 100% of the
training data. More importantly, differently from the previous datasets, BROOF performance stabilizes around 30% of
training, which means that only this fraction of the training
data is enough for our solution to achieve its best effectiveness. In sum, BROOF needs much less training than the
best baselines in all datasets to achieve its highest effectiveness, which usually occurs, when using around 30-80% of
the original training set.

4.3

we do not delve into the specific analyses we have performed
in the previous section regarding the effects of the weighting
strategies and sample size, but experiments in the domains
analyzed in this section indicate that results are very similar.

4.3.1

Sentiment Analysis

Considering sentiment analysis, we tackle the problem of
automatically identifying the polarity of user provided content, which can be reviews for some topic or item (e.g.,
movies, products), posts on social networks (e.g., tweets),
among others. The polarity of a user generated content
refers to the degree to which such content express a positive or negative opinion about a topic. As with any human
generated content in natural language, there is a lot of noise
(due to several factors such as misspellings, the presence of
ambiguity and so on) generally embedded in a high dimensional space. Clearly, accurately predicting the polarity of
such noisy high dimensional data is paramount to support
decision making, being of great business importance.
We evaluate the effectiveness of BROOF considering ten
datasets for polarity detection, consisting of reviews (e.g.,
Amazon), posts on online social networks (e.g., Twitter, Debate), user comments (e.g, Youtube) and snippets of opinion
news (NYT). Due to space issues, a detailed description of
each can be found in the online appendix. The obtained
results can be found in Table 3.
As we can observe in Table 3, BROOF is among the top
performers in 8 out of 10 datasets, being the single classifier to be among the top performers in the vast majority of
cases. Compared to the traditional RF classifier, BROOF
achieved gains of up to 16.70% and 17.72% in MicroF1 and
MacroF1 , respectively. Compared to the ADA.RF baseline,
BROOF achieved gains of up to 6.44% in MicroF1 (Yelp)
and 17.84% in MacroF1 (MySpace). Compared to SVM classifier, our approach obtained gains of up to 2.44% and 3.89%
in MicroF1 (BBC) and MacroF1 (Amazon), respectively.
The cases in which BROOF was not the top performer were
Debate and Digg datasets. Considering Debate, SVM was
the best performer, achieving gains of 4.23% and 3.91% over
BROOF. The situation for Digg was a bit more competitive:
neither SVM nor KNN were able to outperform BROOF in
both metrics, being tied to BROOF in one of them.

Other Application Domains

Topic categorization is not the only domain with noisy and
high dimensional data where there is great benefit of providing highly accurate classifiers. Clearly, other domains do
exist and we here focus on two of them: sentiment analysis
and microarray data classification.
In order to evaluate BROOF over such different domains,
we adopted the same experimental protocol and setup discussed in Section 4.2.1, evaluating the same set of nine classifiers described in Section 4.1. In the following we detail
the performed evaluation. In particular, for space reasons,

4.3.2

Microarray Analysis

Microarray analysis is a popular method in bioinformatics,
allowing the investigation of thousands of genes simultaneously. Gene expression microarray data usually contains a
very large number of attributes, related to the expression of
several genes, but a small number of samples (due to the

360

Amazon

BBC

Debate

Digg

MySpace

NYT

Tweets

Twitter

Yelp

Youtube

BROOF

micF1
macF1

69.94
71.02

86.59
62.72

72.07
71.86

73.04
59.70

84.27
65.27

55.98
56.03

75.64
75.36

73.73
70.89

89.94
89.87

78.36
72.20

RF

micF1
macF1

59.93
60.33

84.80
58.37

73.01
72.82

74.38
58.49

82.83
59.28

53.43
52.71

74.48
74.14

68.84
65.03

87.29
87.23

77.88
71.89

DT

micF1
macF1

56.70
55.41

73.29
50.11

68.66
65.42

68.01
55.13

79.67
53.02

49.97
50.11

68.45
68.22

66.19
63.40

79.88
80.01

71.19
66.44

NB

micF1
macF1

69.87
70.04

87.38
58.32

71.34
70.84

73.54
59.65

84.45
56.91

54.38
54.84

74.08
73.85

67.45
66.21

87.49
87.45

78.00
74.07

KNN

micF1
macF1

69.02
69.84

86.37
51.49

71.99
71.53

76.29
53.27

83.47
51.40

53.85
53.28

73.99
73.52

68.47
66.17

83.48
83.30

78.03
72.34

SVM

micF1
macF1

68.73
68.36

84.53
62.95

75.12
74.67

74.62
63.09

82.74
65.78

56.37
56.38

74.80
74.51

72.14
71.02

90.73
90.26

79.12
74.61

ADA.FGDT

micF1
macF1

69.38
68.97

82.78
57.11

72.81
70.42

72.50
59.16

84.23
64.97

51.03
52.03

73.20
75.33

68.33
67.90

85.55
85.11

77.54
72.04

GBDT

micF1
macF1

67.68
67.54

83.07
51.15

75.39
75.22

73.85
60.10

83.21
58.89

53.36
52.98

71.84
71.75

70.13
67.32

89.80
89.79

77.43
69.83

SGBDT

micF1
macF1

67.36
67.08

83.07
51.68

74.51
74.41

73.46
58.29

82.97
60.90

53.44
53.13

72.56
72.26

70.34
67.99

90.05
90.49

76.28
68.49

ADA.RF

micF1
macF1

67.75
67.11

84.23
58.17

74.98
72.49

72.01
58.09

83.14
55.39

52.05
56.76

72.64
71.22

69.27
68.30

85.00
84.96

76.83
72.88

ADA.M2

micF1
macF1

66.24
67.93

82.21
60.22

70.49
71.44

70.09
58.99

81.33
59.56

51.74
52.60

71.33
70.44

67.16
66.07

84.01
83.88

76.94
73.02

Table 3: Experimental Evaluation—Obtained Results for Sentiment Analysis.
analysis evaluation). Interestingly, in this domain, SVM was
not the closest competitor as in the previous domains.

associated costs of producing such samples, availability, privacy issues, among others). Furthermore, such kind of data
is usually plagued with both technical noise and the presence of genes weakly correlated or not correlated at all with
the outcome. Such a high dimensional characteristic, along
with the presence of noise, provide a real challenge to come
up with accurate prediction models capable of uncovering
the relationship between gene expression and outcome (e.g.,
presence of cancer).
Unlike topic categorization and sentiment analysis domains,
which deal specifically with textual data, the microarray
analysis domain deals with continuous, real-valued, gene expression measurements. Thus, for this particular domain,
some of the previously adopted baselines may be suboptimal. More specifically, instead of adopting a Multinomial
NB classifier, which is specifically designed (and known to
perform better) for textual data, we adopt the Gaussian NB
approach. Furthermore, we adopt, for the microarray domain, the libsvm implementation [4] for the SVM classifier
since, unlike in textual data, non-linear class boundaries may
be observed due to the much more complex relationship between gene expression and the outcome 9 . We adopt a SVM
with non-linear RBF kernel, tuning both the cost parameter
and the gamma parameter for the kernel function.
We evaluate the effectiveness of BROOF considering six
gene expression microarray datasets. Due to space limitation, their details can be found in an online appendix. The
obtained results can be found in Table 4. As it can be observed, BROOF again was the top performer in the majority of evaluated cases (4 out of 6). BROOF outperformed
RF with gains of up to 13.22% in MicroF1 and 22.53%
in MacroF1 (both in the Prostate dataset). Considering
the SVM classifier, BROOF achieved gains of up to 8.10%
and 12.54% in MicroF1 and MacroF1 , also in the Prostate
dataset. ADA.RF was outperformed in all but 9tumors and
Brain1 datasets, in some cases by large margins. Brain2 was
the only dataset in which BROOF was outperformed by another baseline (e.g., KNN) in both metrics, with a somewhat
mild margin (just as observed with our previous sentiment
9

5.

CONCLUSIONS AND FUTURE WORK

In this work, we propose an original classifier which exploits the synergy between the classification paradigms exploited by RF classifiers and boosting schemes, especially for
classification of high dimensional noisy data. We have done
that in an original way, by: (i) exploiting, in the boosting
algorithm (which uses RFs as base learners), new weighting strategies that naturally come from the OOB sets produced by RFs, and by (ii) applying a less aggressive, selective weight updating strategy that mitigates overfitting, a
common problem high-dimensional noisy task.
We summarize our findings regarding the behavior of all
analyzed classifiers in the three explored application domains in Table 5, which counts the number of times each
algorithm figured out as a top performer. As our experimental results clearly show, our proposal is in great advantage over the other explored baselines in terms of prediction
accuracy, being the classifier of choice in the vast majority
of cases. We also show that BROOF requires less training
to achieve its best results, which is of great importance to
guarantee its practical applicability.
As future work, we intend to provide theoretical bounds
over our proposed setting, in order to provide some guarantees regarding misclassification rates, complementing our
extensive empirical comparison.

References
[1] S. Bernard, S. Adam, and L. Heutte. Dynamic random forests.
Patt. Recog. Letters, 33(12):1580–1586, 2012.
[2] L. Breiman. Random forests. Mach. Learn., 45(1):5–32, 2001.
[3] R. Caruana and A. Niculescu-Mizil. An empirical comparison of
supervised learning algorithms. In ICML, pages 161–168, 2006.
[4] C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and
Technology, 2:27:1–27:27, 2011.
[5] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J.
Lin. Liblinear: A library for large linear classification. J. Mach.
Learn. Res., 9:1871–1874, 2008.
[6] M. Fernández-Delgado, E. Cernadas, S. Barro, and D. Amorim.
Do we need hundreds of classifiers to solve real world classification problems? JMLR, 15:3133–3181, 2014.

In fact, linear SVM obtained poor results in this domain.

361

9tumors

Brain1

Brain2

DLBCL

Leukemia

Prostate

BROOF

micF1
macF1

86.63
77.26

72.07
71.86

73.04
59.70

86.99
64.42

73.99
73.52

77.84
79.07

RF

micF1
macF1

86.80
58.37

73.01
72.82

74.38
58.49

86.19
62.92

72.74
73.27

68.75
64.53

DT

micF1
macF1

25.76
15.52

70.42
61.63

66.24
53.24

67.93
60.29

68.83
67.98

66.35
58.27

NB

micF1
macF1

84.28
72.34

70.94
69.28

73.22
60.34

84.45
56.91

74.84
73.85

69.04
64.93

KNN

micF1
macF1

80.28
55.38

77.22
62.91

75.50
61.69

76.79
73.18

73.27
74.28

60.91
56.39

SVM

micF1
macF1

83.96
68.95

74.01
74.10

71.43
59.21

85.50
65.80

73.24
74.51

72.01
70.26

ADA.FGDT

micF1
macF1

84.33
72.90

73.42
72.09

72.60
61.77

82.02
62.77

73.65
74.88

67.33
63.01

GBDT

micF1
macF1

80.11
57.98

76.67
72.63

75.06
58.00

82.75
64.04

72.99
73.62

72.65
67.27

SGBDT

micF1
macF1

81.67
60.28

74.44
68.86

72.00
60.00

87.01
60.71

72.71
70.52

78.97
79.97

ADA.RF

micF1
macF1

86.02
71.14

72.44
71.17

70.73
58.25

84.00
65.22

71.42
70.69

70.17
65.54

ADA.M2

micF1
macF1

81.32
70.98

72.00
71.22

69.81
60.30

83.01
63.24

70.55
71.10

67.98
63.79

Table 4: Experimental Evaluation—Obtained Results for Microarray Analysis.
Winning Counts
Algorithm

BROOF
SVM
KNN
SGBDT
GBDT
NB
ADA.FGDT
RF
ADA.RF

Topic

Sentiment

Microarray

Sum of Totals

MicroF1

MacroF1

Total

MicroF1

MacroF1

Total

MicroF1

MacroF1

Total

6
4
2
2
2
2
0
1
1

6
4
3
3
2
1
1
1
1

12
8
5
5
4
3
1
2
2

8
5
2
2
2
4
2
1
1

7
9
1
2
2
2
2
1
1

15
14
3
4
4
6
4
2
2

4
2
3
2
3
1
1
2
1

3
2
3
1
1
1
2
1
0

7
4
6
3
4
2
3
3
1

34
26
14
12
12
11
8
7
5

Table 5: Number of times each algorithm was the top performer in the performed evaluation. ADA.M2 and
DT not shown since they were not top performers in any evaluated case.
[7] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. J. Comput. Syst. Sci., 55(1):119–139, Aug. 1997.

[20] M. Robnik-Sikonja. Improving random forests. In ECML, pages
359–370, 2004.
[21] J. J. Rodriguez, L. I. Kuncheva, and C. J. Alonso. Rotation
forest: A new classifier ensemble method. IEEE Trans. Pattern
Anal. Mach. Intell., 28(10):1619–1630, 2006.

[8] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic
regression: a statistical view of boosting, 1998.
[9] J. H. Friedman. Greedy function approximation: A gradient
boosting machine. Ann. Statist., 29(5):1189–1232, 2001.

[22] B. N. Saha, G. Kunapuli, N. Ray, J. A. Maldjian, and S. Natarajan. Ar-boost: Reducing overfitting by a robust data-driven regularization strategy. In ECML/PKDD, pages 1–16, 2013.

[10] M. Galar, A. Fernández, E. Barrenechea, H. Bustince, and
F. Herrera. A review on ensembles for the class imbalance problem: Bagging-, boosting-, and hybrid-based approaches. IEEE
Trans Syst Man Cybern C Cybern, 42(4):463–484, 2012.

[23] R. E. Schapire and Y. Freund. Boosting: Foundations and Algorithms. The MIT Press, 2012.
[24] F. Schroff, A. Criminisi, and A. Zisserman. Object class segmentation using random forests. In British Machine Vision Conf.,
pages 1–10, 2008.

[11] S. Godbole and S. Sarawagi. Discriminative methods for multilabeled classification. In PAKDD, pages 22–30, 2004.
[12] A. J. Grove and D. Schuurmans. Boosting in the limit: Maximizing the margin of learned ensembles. In AAAI/IAAI, pages
692–699, 1998.

[25] M. R. Segal. Machine Learning Benchmarks and Random Forest
Regression. Technical report, Univ. of California, 2004.
[26] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio,
R. Moore, A. Kipman, and A. Blake. Real-time human pose
recognition in parts from single depth images. In CVPR, pages
1297–1304, 2011.

[13] Y. R. Guy Leshem. Traffic flow prediction using adaboost algorithm with random forests as a weak learner. Int. Jour. Intell.
Tech., 2:1305–6417, 2007.
[14] T. Hastie, R. Tibshirani, and J. H. Friedman. The Elements of
Statistical Learning. Springer, 2009.

[27] A. Statnikov, L. Wang, and C. F. Aliferis. A comprehensive
comparison of random forests and support vector machines for
microarray-based cancer classification. BMC bioinformatics,
9(1):319+, 2008.

[15] R. Jain. The Art of Computer Systems Performance Analysis:
Techniques for Experimental Design, Measurement, Simulation, and Modeling. John Wiley, New York, NY, 1991.

[28] J. Thongkam, G. Xu, and Y. Zhang. Adaboost algorithm with
random forests for predicting breast cancer survivability. In
IJCNN, pages 3062–3069. IEEE, 2008.

[16] M. Lan, C.-L. Tan, and H.-B. Low. Proposing a new term weighting scheme for text categorization. In AAAI, pages 763–768,
2006.

[29] A. Tsymbal, M. Pechenizkiy, and P. Cunningham. Dynamic integration with random forests. In ICML, pages 801–808, 2006.

[17] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. Rcv1: A new
benchmark collection for text categorization research. JMLR.,
5:361–397, 2004.

[30] A. Vezhnevets and O. Barinova. Avoiding boosting overfitting
by removing confusing samples. In ECML, pages 430–441, 2007.
[31] J.-Y. Zeng, X.-H. Cao, and J.-Y. Gan. An improvement of adaboost for face detection with random forests. In Adv. Intell.
Comp. Theor. and App., pages 22–29. 2010.

[18] Z. H. Li X. Weighted random subspace method for high dimensional data classification. Stat Interface, 2(2):153–159, 2009.
[19] I. Mukherjee and R. E. Schapire. A theory of multiclass boosting.
JMLR, 14(1):437–497, Feb. 2013.

[32] Z. Zhang and X. Xie. Research on adaboost.m1 with random
forest. In ICCET, pages 647–652, 2010.

362

