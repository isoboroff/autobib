An Entity Class-Dependent Discriminative Mixture Model
for Cumulative Citation Recommendation
∗

Jingang Wang

Dandan Song†

Qifan Wang

School of Computer Science
Beijing Institute of Technology

School of Computer Science
Beijing Institute of Technology

Dept. of Computer Science
Purdue University

bitwjg@bit.edu.cn
Zhiwei Zhang

sdd@bit.edu.cn
Luo Si

wang868@purdue.edu

Dept. of Computer Science
Purdue University

Dept. of Computer Science
Purdue University

School of Computer Science
Beijing Institute of Technology

zhan1187@purdue.edu

lsi@purdue.edu

liaolj@bit.edu.cn

Lejian Liao

Chin-Yew Lin
Knowledge Mining Group
Microsoft Research

cyl@microsoft.com

ABSTRACT

Keywords

This paper studies Cumulative Citation Recommendation
(CCR) for Knowledge Base Acceleration (KBA). The CCR
task aims to detect potential citations of a set of target entities with priorities from a volume of temporally-ordered
stream corpus. Previous approaches for CCR that build an
individual relevance model for each entity fail to handle unseen entities without annotation. A baseline solution is to
build a global entity-unspecific model for all entities regardless of the relationship information among entities, which
cannot guarantee to achieve satisfactory result for each entity. In this paper, we propose a novel entity class-dependent
discriminative mixture model by introducing a latent entity
class layer to model the correlations between entities and
latent entity classes. The model can better adjust to different types of entities and achieve better performance when
dealing with a broad range of entities. An extensive set of experiments has been conducted on TREC-KBA-2013 dataset,
and the experimental results demonstrate that the proposed
model can achieve the state-of-the-art performance.

Cumulative Citation Recommendation; Knowledge Base Acceleration; Mixture Model; Information Filtering

1.

INTRODUCTION

In recent years, we have witnessed a proliferation of open
domain Knowledge Bases (KBs) such as Freebase1 and Yago2 .
They have been used in many applications such as query
answering, entity search and entity linking and have shown
great promises. These KBs are usually organized around
entities such as persons, organizations, locations, and so on.
Currently, the maintenance of a KB mainly relies on human
editors. However, with the explosion of information, largescale KBs are hard to be kept up-to-date solely by human
editors. Taking English Wikipedia for example, there are
approximately 4.7 million entities but merely 132,938 active
editors3 . The less popular entities cannot be updated in
time because they are not spotlighted. As reported in [14],
the median time delay between a cited document’s publishing and its citation in Wikipedia is almost one year. An
outdated KB severely limits the effectiveness of applications
depending on it. This gap could be bridged if relevant documents of KB entries can be automatically detected as soon
as they emerge online and then be recommended to the editors with various levels of relevance. This is called the Cumulative Citation Recommendation (CCR). Formally, given
a KB entity, CCR is a task to filter highly relevant documents from a chronological stream corpus and evaluate their
citation-worthiness to the target entity.
Most previous approaches (e.g., [2, 25]) for CCR are highly
supervised and require sufficient training data to build an individual relevance model for each entity. These approaches
are infeasible when dealing with a large-scale KB, since the

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval
Models
∗

This work was partially done when the first author was
visiting Purdue University and Microsoft Research Asia.
†
Corresponding Author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
SIGIR’15, August 09-13, 2015, Santiago, Chile.
c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767698.

1

https://www.freebase.com/
http://www.mpi-inf.mpg.de/departments/
databases-and-information-systems/research/
yago-naga/yago/
3
http://meta.wikimedia.org/wiki/List_of_
Wikipedias#1_000_000.2B_articles
2

635

labeling work is labor intensive. One solution is to build a
global entity-unspecific discriminative model and optimize
it to achieve an overall optimal performance for all entities [29, 36]. However, these models ignore the distinctions
between different entities and learn a set of fixed model parameters for all entities, which leads unsatisfactory performance when dealing with a diverse entity set. For instance,
it is not intuitve to apply the same discriminative model
for Geoffery Hinton and Appleton Museum of Art. The former entity is a computer scientist, while the latter one is a
museum. Nevertheless, the global model treats them equally
without considering the prior entity class knowledge. We assume that entities from a same class have similar tastes and
preferences when citing relevant documents, which means
they have similar combination weights in the discriminative
model. Therefore, for an entity with little training data, the
training data of its similar entities from the same class can
be utilized to learn the combination weights. In comparison
to the global model, more accurate combination weights are
learned for each entity by this manner.
Based on this observation, we build an adaptive discriminative model for different types of entities by utilizing the
underlying entity class information, i.e. entity class dependent discriminative mixture model. We introduce an intermediate latent entity class layer and define a joint distribution over the entity-document pairs and latent classes
conditioned on the observations. The aim is to achieve relevance estimation through learning a mixture model which
is expected to outperform the global model, while maintaining the capability to reveal the hidden correlations between
entities and entity classes. The model can be viewed as a
hierarchical combination of a discriminative component and
a mixing component, so two types of features are required:
entity-document features for the discriminative component
and entity-class features for the mixing component.
For the discriminative component, we develop a set of
bursty features as temporal features in addition to semantic
features. The bursty features are detected from two independent data sources: the stream corpus (internal) and certain
third-party data (external) like Google Trends.
For the mixing component, we explore two types of entityclass features to model the correlations between entities and
hidden classes, including profile-based features and categorybased features. Profile-based features are constructed from
the entity’s profile in KB, while category-based features rely
on the existing category labels for the entity in KB.
To the best of our knowledge, this is the first research
work that focuses on modeling correlations between entities and hidden entity classes in discriminative model for
CCR. Our model is capable of tackling less popular entities with little training data and unseen entities that do not
exist in the training set, which is indispensable in a practical CCR system. Empirical studies have been conducted
on TREC-KBA-2013 dataset to show the effectiveness and
robustness of the proposed mixture model. Experimental
results demonstrate that our model achieves the state-ofthe-art performance on TREC-KBA-2013 dataset.
The rest of this paper is organized as follows. Section 2
summarizes related works. Section 3 introduces an entity
class-dependent discriminative mixture model for CCR. Section 4 describes features required in our model, especially
the temporally bursty features and their detection methods.
Section 5 presents the detailed experimental results and pro-

vides some discussion. Section 6 concludes this paper and
points out possible future work.

2.

RELATED WORK

Although CCR was first proposed in TREC-KBA tracks,
the similar research problem has been studied in several topics of information retrieval.

Topic/Event Detection and Tracking.
Topic Detection and Tracking (TDT) is a track hosted by
TREC from 1997 to 2004 [1]. A similar research topic in
recent years is event detection. Both TDT and event detection are concerned with the development of techniques
for finding and following events in broadcast news or social
media. The techniques adopted for TDT and event detection can be broadly classified into two categories: (1) clustering documents based on the semantic distance between
them [34], or (2) grouping the frequent words together to
represent events [22]. In [22], a finite automaton model is
proposed to detect events in stream by modeling events as
state transitions. This method has been validated widely by
lots of other studies [18, 17, 35]. We also adopt this model to
detect KB entities’ bursts in the stream corpus and then extract bursty features for them. Different from above works,
we model entities’ occurrences to capture bursty activities
instead of words’ occurrences. Another difference between
CCR and TDT is that CCR needs to make fine-grained
citation-worthiness distinctions between relevant documents
further.

Cumulative Citation Recommendation.
TREC has launched the KBA-CCR track since 2012. Participants treat CCR as either a ranking problem [3, 2, 4] or
a classification problem [3, 5, 29]. Classification and Learning to Rank methods have been compared and evaluated
[2, 15], and both of them can achieve the state-of-art performance with a powerful feature set. Several supervised
learning techniques, such as SVM [21], language models [26,
10], Markov Random Fields [7], and Random Forests [4, 5,
29] are utilized. Meanwhile, a variety of relevance scoring
methods have been tried, including standard Lucene scoring [6], and custom ranking functions based on entity cooccurrences [25]. A time-aware evaluation paradigm is developed to study time-dependent characteristics of CCR [9].
However, some highly supervised methods require training
instances for each entity to build a relevance model, limiting their scalabilities. Entity-unspecific methods, regardless
of entity distinctions, are employed to address this problem
[29, 28]. Nevertheless, characteristics of different entities
are lost in the entity-unspecific methods. Some other researchers employ transfer learning techniques to learn across
entities by using entity-unspecific meta-features [36], or utilize a semi-supervised approach to profile an entity by leveraging its related entities and weighting them with the training data [23]. These methods have demonstrated that the
correlations between entities are useful for CCR. Nevertheless, all these methods are empirically designed and the performance can be improved further.
What’s more, query expansion is often employed because
the name of the target entity is too sparse to be a good query.
Other name variants and contextual information of terms or
related entities from Wikipedia or from the document stream

636

[7, 6] are used to enrich the semantic features of entities. In
addition to semantic features, temporal features have been
proved especially helpful in CCR [2, 5, 29].

where δ(x) = 1/(1 + exp(−x)) is the standard logistic function, and ωi is the combination parameter for the ith entry
of the feature vector. For the irrelevant class, we have

Mixture Model.

P (r = −1|e, d)=1 − P (r = 1|e, d)=δ(−

Mixture model has been proved effective to address the
problem of data insufficiency in several information retrieval
tasks, including expert search [11], federated search [19], collaborative filtering [20] and image retrieval [30]. By introducing latent layers to learn flexible combination weights for
different feature vectors, mixture model can always outperform simple discriminative models with fixed combination
weights. Hence, we propose an entity class-dependent discriminative mixture model to deal with the entities with
little training data, which will be described in next section.

3.

It is worth noting that for different values of r, the only
difference in P (r|e, d) is the sign within the logistic function. Therefore,Pwe adopt the general representation of
P (r|e, d) = δ(r K
i=1 ωi fi (e, d)) in the following sections.
The conditional probability of relevance P (r|e, d) represents
the extent to which the document d is relevant to the entity
e. The entity-documents pairs are then classified as positive
or negative according to the value of P (r = 1|e, q). Since the
learned weights are identical for all entity-document pairs
and regardless of specific entities, this model is also denoted
as global discriminative model (GDM) in this paper.
Several other approaches for CCR [5, 29] can be deemed as
global discriminative models adopting different classification
functions such as decision trees and Support Vector Machine
(SVM).

DISCRIMINATIVE MODEL FOR CCR

3.3

Problem Statement

P (r, z|e, d; α, ω) = P (z|e; α)P (r|e, d, z; ω)

Global Discriminative Model

K
X

ωi fi (e, d))

(3)

where P (z|e; α) is the mixing coefficient, representing the
probability of choosing hidden entity class z given entity e,
and α is the corresponding parameter. P (e, d, z; ω) denotes
the mixture component which takes a logistic functions for
r = 1 (or r = −1). ω = {ωzi } is the set of combination parameters where ωzi is the weight for the ith feature vector
entry for the given training instance (e, d) under the hidden class z. By marginalizing out the latent variable z, the
corresponding mixture model can be written as
!
Nz
K
X
X
P (r|e, d; α, ω) =
P (z|e; α)δ r
ωzi fi (e, d)
(4)

This paper utilizes logistic regression, a traditional discriminative model, to estimate the conditional probability
P (r|e, d), in which r(r ∈ {1, −1}) is a binary label to indicate the relevance of the entity-document pair (e, d). The
value of r is 1 if the document d is relevant to the entity
e, otherwise r = −1. Formally, the parametric form of
P (r = 1|e, d) can be expressed as follows in terms of logistic
functions over a linear combination of features,
P (r = 1|e, d) = δ(

Entity Class-Dependent Mixture Model

In the GDM introduced in Subsection 3.2, a fixed set of
combination weights (i.e., ω) are learned to optimize the
overall performance for all entities. However, the best combination strategy for a given entity is not always the best
for others. The entities stored in KBs are extremely diverse,
including persons, organizations, locations, events, etc. Different entities have personalized criteria to detect relevant
documents.
We propose an entity class-dependent discriminative mixture model (ECDMM) by introducing an intermediate latent class layer to capture the entity class information in
the learning framework. A latent variable z is utilized to
indicate which entity class the combination weights ωz· =
(ωz1 , · · · , ωzK ) are drawn from. The choice of z depends on
the target entity e in the entity-document pair (e, d). The
joint probability of relevance r and the latent variable z is
represented as

We consider CCR as a binary classification problem that
treats the relevant entity-document pairs as positive instances
and irrelevant ones as negative instances. Many probabilistic classification techniques in the literature generally fall
into two categories: generative models and discriminative
models. Discriminative models have attractive theoretical
properties [24] and generally perform better than their generative counterparts in the field of information retrieval [16,
32]. Therefore, we adopt discriminative probabilistic models
in this paper.
Given a set of KB entities E = {eu }(u = 1, · · · , M ) and a
document collection D = {dv }(v = 1, · · · , N ), our objective
is to estimate the relevance of a document d to a given entity
e. In other words, we need estimate the conditional probability of relevance P (r|e, d) with respect to an entity-document
pair (e, d). Each entity-document pair (e, d) is represented
as a feature vector f (e, d) = (f1 (e, d), · · · , fK (e, d)), where
K indicates the number of entity-document features. Moreover, to model the hidden entity class information, each
entity can be represented as an entity-class feature vector
g(e) = (g1 (e), · · · , gL (e)), where L indicates the number
of entity-class features. The entity-document features and
entity-class features will be introduced in Section 4 later.

3.2

ωi fi (e, d)) (2)

i=1

This section proposes a novel learning framework by modeling each entity’s distribution across hidden entity classes
and combining it with a logistic regression model to form a
final discriminative model. First we provide a formal definition of the research problem and model it as a classification
task, and then present two discriminative models: a global
model and an entity class-dependent mixture model.

3.1

K
X

z

i=1

where Nz is the number of latent entity classes. If P (z|e; α)
follows the multinomial distribution, the model cannot easily generalize the combination weights to unseen entities
beyond the training set since each parameter in multinomial distribution specifically corresponding to a training entity. ToP
address this problem, we adopt a soft-max function
1
z
exp( L
j=1 αzj gj (e)) to model P (z|e; α) instead. αzj is
Ze
the weight parameter associated with the jth entity feature

(1)

i=1

637

αz∗ =

in the latent entity class z and Ze is the normalization factor
that scales the exponential function to be a proper probability distribution. In this representation, each entity e is
denoted by a bag of entity-class features (g1 (e), · · · , gLz (e))
where Lz is the number of entity features. By plugging the
soft-max function into Eq. 4, we can get
!
!
Nz
Lz
K
X
X
1 X
P (r|e, d; α, ω)=
exp
αzj gj (e) δ r
ωzi fi (e, d)
Ze z=1
j=1
i=1

arg max
αz

v

The M-step can be optimized by any gradient descent method.
To optimize Eq. 9 and Eq. 10, we employ the minFunc toolkit4 ,
a collection of Matlab functions for solving optimization
problems using Quasi-Newton strategy. When the value of
L(ω, α) converges to a local optima, the estimated parameters can be plugged back into the model to compute the
probability of relevance for entity-document pairs. Since
EM is only guaranteed to converge to local optima given
different starting points, we try several starting points and
choose the model that leads to the greatest log-likelihood.

Because αzj is associated with each entity feature instead of
each entity, the above model allows the estimated αzj to be
applied to less popular entities and even unseen entities.
Suppose entity-document pairs in training set are represented as T = {(eu , dv )}, and R = {ruv } denotes the corresponding relevance judgment (i.e., +1 or −1) of (eu , dv ),
where u = 1, · · · , M and v = 1, · · · , N . Assume training
instances in T are independently generated, the conditional
likelihood of the training data is written as follows.
M Y
N
Y

u

(10)

(5)

P (R|T )=

!
Lz
X
1
exp(
αzj gj (eu ))
P (z|eu , dv ) log
Zeu
j=1
!

X X

3.5

Discussion

The ECDMM can exploit the following two advantages
over the GDM: (1) the combination weights are able to
change across entities and hence lead to a gain of flexibility. (2) it offers probabilistic semantics for the latent entity
classes and thus each entity can be associated with multiple
classes.

P (ruv |eu , dv )=

u=1 v=1
M Y
N
Y
u=1v=1

!
Nz
Lz
K
X
X
1 X
ωzi fi (eu , dv ))
exp( αzj gj (eu ))δ(ruv
Zeu z=1
j=1
i=1

Determining the number of latent Variables.
The number of hidden entity classes can be determined
by some model selection criterion. We choose Akaike Information Criteria (AIC), which has been shown suitable in
determining the number of latent classes in mixture models.
As a measure of the goodness of fit of an estimated statistical
model, AIC is defined as

(6)

3.4

Parameter Estimation

The parameters (i.e. ω and α) in Eq. 6 can be estimated
by maximizing the following data log-likelihood function,
L(ω, α)=

M X
N
X
u=1 v=1

log

Nz
X
z=1

X
1
exp(
αzj gj (eu )))
Zeu
j=1
!
K
X
ωzi fi (eu , dv ))
δ(ruv

2m − 2L(ω, α)

(11)

where m is the total number of parameters in the model.
AIC offers a relative estimation of the information loss when
a given model is used to represent the process that generates
the data. Given a set of models, the preferred model is the
one with the minimum AIC value.

(

(7)

i=1

4.

where M is the number of the entities and N is the number of
the documents in training set. gj (eu ) denotes the jth feature
for the uth entity and ruv denotes the relevance judgment
for the pair (eu , dv ). A typical approach to maximize Eq. 7
is to use Expectation-Maximization (EM) algorithm [8].

FEATURES

E-Step.

In this section, we present the two types of features used in
the discriminative models. Entity-document features f (e, d)
are used in the discriminative components of GDM and
ECDMM. In addition, ECDMM requires entity-class features g(e) to learn the mixing coefficients in the mixture
component.

The E-step can be derived as follows by computing the
posterior probability of z given entity eu and document dv .

4.1

P (z|eu , dv ) =
P z
PK
exp( L
j=1 αzj gj (eu ))δ(ruv
i=1 ωzi fi (eu , dv ))
P
PLz
PK
z exp(
j=1 αzj gj (eu ))δ(ruv
i=1 ωzi fi (eu , dv ))

(8)

4.1.1

By optimize the auxiliary Q function, we can derive the
following parameter update rules.

4.1.2

arg max
ωz

uv

P (z|eu , dv ) log δ(ruv

K
X

!
ωzi fi (eu , dv ))

Temporal Features

Entities are evolving in the stream corpus as time goes
by, yet semantic features are not capable of portraying the

=
X

Semantic Features

We adopt the semantic features listed in Table 1, which
have been proved effective in CCR [28, 29]. Semantic features can model semantic characteristics of document-entity
pairs.

M-Step.

ωz∗

Entity-Document Features

Entity-document features (i.e., f (e, d)) are composed of
semantic and temporal features.

(9)

4
http://www.cs.ubc.ca/~schmidtm/Software/minFunc.
html

i=1

638

After performing the burst detection algorithm, if the automaton of entity e is in the state q1 during a time period
[tstart , tend ], [tstart , tend ] is a burtsy period of e with a bursty
weight bw(tstart ,tend ) (e). The bursty weight is defined as the
cost improvement incurred by assigning state q1 over the
bursty period instead of q0 , and can be found in [22].

Table 1: Semantic features.
Feature
N (erel )
N (d, e)
N (d, erel )
F P OS(d, e)
F P OSn (d, e)
LP OS(d, e)
LP OSn (d, e)
Spread(d, e)
Spreadn (d, e)
Simcos (d, si (e))
Simjac (d, si (e))
Simcos (d, ci )
Simjac (d, ci )

Description
# of entity e’s related entities found in
its profile page
# of occurrences of e in document d
# of occurrences of the related entities in
document d
First occurrence position of e in d
F P OS(d, e) normalized by the document
length
Last occurrence position of e in d
LP OS(d, e) normalized by the document
length
LP OS(d, e) − F P OS(d, e)
Spread(d, e) normalized by document
length
Cosine similarity between d and the ith
section of e’s profile
Jaccard similarity between d and the ith
section of e’s profile
Cosine similarity between d and the ith
citation of e in the KB
Jaccard similarity between d and the ith
citation of e in the KB

External Burst Detection.
External resources, such as daily view statistics of entities’ profile pages, are utilized as temporal features in previous work [2, 29]. Since some KBs do not provide page
view statistics for entities as Wikipedia, we also include
Google Trends5 to detect external bursts. Akin to Wikipedia
statistics, Google Trends can provide a numeric sequence
v = (v1 , · · · , vT ) for each entity e, where vi denotes the
normalized search volume of e in the ith day.
We detect external bursts of entity e from v with a tailored
moving average (MA) method [27]. More concretely, for each
vi in v,
1. Calculate a moving average sequence of length w as
M Aw (i) =

vi + vi−1 + · · · + vi−w+1
w

2. Calculate a cutoff c(i) based on previous MA sequences
P reM A = (M Aw (1), · · · , M Aw (i)) as

dynamic characteristics of entities. So we resort temporal
features to make up this deficiency. Previous work [2, 5,
29] considering temporal features can be summarized as a
straightforward strategy that counts the daily (or hourly)
occurrences of target entities in the stream corpus and calculates some statistical indicators as temporal features. To
exploit the effectiveness of temporal features, novel bursty
features are introduced in this paper. The underlying intuition is that the occurrences of entities in the stream do not
distribute uniformly. If the amount of documents referring
to an entity increases sharply in a short time period when
something important is happening around the entity, this
time period is detected as one bursty period of this entity.
We make an assumption that documents occur in a bursty
period of an entity are more likely to be related to it than
those not.
The bursty periods of an entity can be detected either
from stream corpus or from third-party data sources, denoted as internal bursty periods and external bursty periods respectively. Due to the heterogeneity of data sources,
we use different burst detection methods to identify internal
bursty periods and external bursty periods for entities.

The moving average length can be varied to detect long-term
or short-term bursts. We set the moving average length as
7 days (i.e., w = 7). The cutoff value is empirically set as 2
times the standard deviation of the M A (i.e., β = 2).

Internal Burst Detection.

Bursty Feature Representation.

c(i) = mean(P reM A ) + β · std(P reM A )
3. Detect bursty day sequence d, where d = {i|M Aw (i) ≥
c(i)}
4. Calculate the bursty weight sequence w = (w1 , · · · , wT )
for e as follows.

 0, i 6∈ d
wi =
M Aw (i)

,i ∈ d
c(i)
5. Compact each segment of consecutive days in d into a
bursty period [tstart , tend ] of entity e, and the bursty
weight bw(tstart ,tend ) is calculated as the average weight
of all the bursts in this period.

Burst detection from a stream of documents have been
thoroughly investigated in TDT and event detection [22, 17,
31].
Since our goal is not to develop a new burst detection algorithm, we simply adopt Kleinberg’s 2-state finite automaton
model [22] to identify bursty periods of entities. There are
two states q0 and q1 in the finite automaton A. For every
target entity e, when A in state q0 , it has low emission rate
β0 = |RdT(e)| , where Rd (e) is the number of all documents
referring to e over the whole time range T . When A in state
q1 , the rate is increased to β1 = s· |RdT(e)| , where β1 > β0 because s is a scaling factor larger than 1.0 and s is empirically
set as 2.0 in our work. The larger the number of documents
referring to entity e at time t, the higher the likelihood of e
being identified as a bursty entity at t.

Given an entity-document pair (e, d), we define a bursty
value b(e, d) to represent the temporal correlation between
d and e. Let t be the timestamp of d. If t falls in one of e’s
bursty periods, say [tstart , tend ], then b(d, e) is calculated as
Eq. 12 shows. If t is not in any bursty period of e, b(d, e) is
set as 0.
t − tstart
) · bw(tstart ,tend ) (e),
b(d, e) = (1 −
tend − tstart
(12)
t ∈ [tstart , tend ]
t−tstart
In Eq. 12, 1 − tend
is a decaying coefficient reflecting
−tstart
the intuition that the documents appear at the beginning
of a bursty period are more informative than those appear
5

639

http://www.google.com/trends/

Geoffery
Hinton

Labeled
Categories

Parent
Categories

Labeled
Categories

Canadian
Computer
Scientists

Computer
Scientists

American
Computer
Scientists

AI Researchers

Researchers

Programming
Language
Researchers

Fellows
of AAAI

Fellows of
Learned
Societies

Fellows
of ACM

5.

In this section, we first introduce the dataset for experiments. After that, we report an extensive set of experimental results of our proposed models and baselines in two
scenarios of CCR. At last, analysis and discussion are presented based on the experimental results.

Barbara
Liskov

5.1

Entity Set.
The target entity set includes 121 Wikipedia entities and
20 Twitter entities, more specifically, 98 people, 19 organizations, and 24 facilities from 14 inter-related communities
such as small towns like Danville, KY and academic communities like Turing award winners.

at the end. Please note that b(d, e) can be calculated based
on external bursts and internal bursts respectively. To avoid
using future information during burst detection, we carefully
perform burst detection algorithm (either internal or external) in a daily incremental manner. When dealing with an
entity-document pair, the bursty periods are determined by
the data before the timestamp of this document.

Stream Corpus.
The temporally-ordered stream corpus, containing approximately 1 billion documents crawled from October 2011 to
the end of February 2013. Each document is associated with
a timestamp indicating its time of crawling. The corpus have
been split with documents from October 2011 to February
2012 as training instances and the remainder for evaluation.
We adopt the same training/test range setting in our experiments.

Entity-Class Features

In ECDMM, besides entity-document features, entity-class
features (i.e., g(e) in Eq. 5) are required to learn the mixing
coefficients. Here we consider two types of prior knowledge
to design entity-class features.

4.2.1

Annotation.

Profile-based features

The relevance of entity-document pairs are labeled following a four-point scale relevance setting, including vital, useful, neural and garbage. The definitions are listed in Table 2.

Each entity in KBs is uniquely identified by its profile
page, which contains the basic information of this entity,
such as name, address and experiences. We crawl the profile pages of all the entities as a profile collection. After
removing stop words, we represent each entity as a feature
vector with the bag-of-words model, where term weights are
determined by the TF-IDF scheme.

4.2.2

Dataset

We conduct our experiments on TREC-KBA-2013 dataset6 ,
a standard test bed provided by TREC. The data set is composed of a target entity set and a document collection called
stream corpus.

Figure 1: Two entities without common labeled categories but with shared parent categories.

4.2

EXPERIMENTS

Table 2: Four-point scale relevance estimation in
TREC-KBA-2013.
Vital
timely info about the entity’s current state,
actions, or situation. This would motivate a
change to an already up-to-date KB article.
Useful
possibly citable but not timely, e.g., background biography, secondary source information.
Neutral informative but not citable, e.g., tertiary
source like Wikipedia article itself.
Garbage no information about the target entity could
be learned from the document, e.g., spam.

Category-based features

Some KBs like Wikipedia organize entities with hierarchical categories. For example, Geoffrey Hinton in Wikipedia,
is labeled with categories such as Canadian computer scientists, Artificial intelligence researchers, and Fellows of AAAI. Besides these labeled categories, we take the
parent categories of the labeled categories into consideration to deal with the circumstance in Figure 1. The two
alike entities can not be correlated if we only consider labeled categories.
Similar to profile-based feature vector, we leverage a “bagof-categories” model to represent each entity as a categorybased feature vector. Given an entity without category information, we manually assign a meta-category for it according to its profile. We supplement three meta-categories:
person, facility and organization, which can cover all the
entities in our dataset. The category-based feature vector of
entity e is denoted as g c (e) = (c1 (e), · · · , cN (e)), where N
is the total number of categories. ci (e) equals to 1 if e is
labeled with category ci , otherwise ci (e) is 0.
Therefore, given a target entity set E, we can generate two
feature vectors for each e ∈ E: profile-based vector gp (e) and
category-based vector gc (e) respectively.

The details of the annotations for Wikipedia and Twitter
entities are demonstrated in Table 3.

5.2

Evaluation Scenarios

According to different granularity settings, we evaluate
the proposed models in two classification scenarios respectively.

6

640

http://trec-kba.org/kba-stream-corpus-2013.shtml

Table 3: The number of training and test instances (entity-document pairs) for Wikipedia and Twitter entities
respectively.
Training
Test
Vital Useful Neutral Garbage Vital Useful Neutral Garbage
Wikipedia 2096
2257
1162
1756
8639
16053
5649
18694
Twitter
182
326
72
569
1808
2953
1491
4103
Total
2278
2583
1234
2325
10447 19006
7140
22797

Vital Only.

• BIT-MSRA [29]. An entity-unspecific random forests
classification model, which is the first place approach
in TREC-KBA-2013 track. This approach can be considered as a variant of GDM utilizing a different kernel.

Only vital entity-document pairs are treated as positive
instances, and the others are negative instances. This scenario is the essential task of CCR.

• UDEL [23]. An entity-centric query expansion approach that achieves the second best performance in
TREC-KBA-2013 track. Given a target entity, the approach first detect related entities from the profile page
of the entity. Then, these related entities are utilized
as expansion terms and combine with the target entity as a new query to detect and rank the relevant
documents. The optimal weights of query terms are
learned from the training data. The relevance score of
a document is estimated according to its position in
the ranking list.

Vital + Useful.
Both vital and useful entity-document pairs are treated
as positive instances, and the others are negative ones.

5.3

Experimental Methodology

Experiments in this section investigate the effectiveness
of our proposed mixture model and baseline methods in the
two scenarios. The following methods are compared:
• Global Discriminative Model (GDM). As presented
in Subsection 3.2, this approach learns a set of fixed
weights for all entity-documents pairs.

5.4

• Naı̈ve Entity Class-Dependent Discriminative Mixture
Model (Naı̈ve ECDMM). This approach uses entitydocument features instead of entity-class features for
the mixing component (i.e., g(e) := f (e, d) in Eq. 5) of
ECDMM.

Hidden Classes Analysis

For all mixture models, the number of hidden classes are
determined according to AIC value. The optimal numbers of
latent classes of all variants of ECDMM are reported in Table 4. The number of optimal classes of category ECDMM
is obviously larger than the optimal numbers of the other
mixture models, which possibly caused by the hierarchical
structures of categories in our category-based feature set.
Although the incorporation of parent categories can build
the correlation between two similar entities without common labeled categories, it brings some noisy correlations in
the meantime. For instance, a politician and a business
man both living in Florida share a common parent category “Living people from Ocala, FLorida”, this correlation will mislead the model and come to an non-optimal fit
to the data.

• Profile-based Entity Class-Dependent Discriminative
Mixture Model (profile ECDMM). This approach
utilizes profile-based features as entity-class features
for the mixing component of ECDMM.
• Category-based Entity Class-Dependent Discriminative
Mixture Model (category ECDMM). This approach
utilizes category-based features as entity-class features
for the mixing component of ECDMM.
• Combination Entity Class-Dependent Discriminative
Mixture Model (combine ECDMM). This approach
utilizes profile-based and category-based features together as entity-class features for the mixing component of ECDMM. In our experimental setting, we simply union the two feature vectors together into an integral feature vector.

Table 4: Number of hidden classes determined by
AIC for each mixture model.
Model
Vital Vital + Useful
naı̈ve ECDMM
9
10
profile ECDMM
7
6
category ECDMM
13
12
combine ECDMM
9
8

For reference, we also include three top-ranked approaches
in the TREC-KBA-2013 track as baselines.

5.5

• Official Baseline [13]. A string matching approach
implemented by TREC-KBA organizers. For each target entity, they split the entity’s name into different tokens and manually composite them into reliable aliases
of the entity. These alias are utilized to filter relevant
documents from the stream corpus with the strategy
that documents referring to any alias are rated as vital to the target entity. A relevance score is estimated
according to the length of matched string.

Overall Results

This section presents the overall performance of all experimental methods. We adopt F1 (harmonic mean between precision and recall), accuracy and AUC (Area Under
Curve) [12] as the evaluation measurements. All the measurements are computed in an entity-insensitive manner. In
other words, the measurements are computed based on the
test pool of all entity-document pairs regardless of specific
entities. The results are reported in Table 5.

641

Table 5: Overall classification results of evaluated models.
Vital Only
Vital + Useful
P
R
F1
Accu AUC
P
R
F1
Accu
Official Baseline
.171 .942 .290
.175
.475
.540 .972 .694
.532
BIT-MSRA
.214 .790 .337
.445
.580
.589 .974 .734
.615
UDEL
.169 .806 .280
.259
.473
.573 .893 .698
.579
GDM
.218 .507 .304
.587
.556
.604 .913 .727
.565
.223 .400 .286
.644
.548
.627 .912 .744
.656
naı̈ve ECDMM
profile ECDMM
.332 .376 .353
.754
.606
.669 .866 .755
.692
category ECDMM .316 .422 .362 ..734 .612
.672 .894 .767
.704
combine ECDMM .397 .418 .407 .783 .640 .703 .877 .780 .731
Methods

We notice that combine ECDMM achieves best on all
measurements except recall. The official baseline achieve
the best recall of all methods, which is not surprising since
the official baseline is a manual method to detect as many
relevant documents as possible by manually selecting reliable aliases of an entity in advance.
Compared with GDM regardless of entity class information, all the mixture models employing entity-class features
explicitly (i.e., profile ECDMM, category ECDMM and combine ECDMM) achieve better classification performance in
both scenarios. Even Naı̈ve ECDMM which does not employ entity-class features explicitly can outperform GDM
and other three baselines. This reveals that the mixture
model is an effective strategy to enhance the straightforward
discriminative model. Naı̈ve ECDMM is not robust in two
scenarios. Although it outperforms GDM in vital + useful
scenario, it cannot beat GDM in vital only scenario. This
is possibly caused by its implicitly employment of entityclass features. The entity-document features are noisy, because the document-related counterpart contributes nothing
to capture hidden entity classes.
Both profile ECDMM and category ECDMM outperform
naı̈ve ECDMM remarkably, revealing that profile-based features and category-based features are effective in modeling
hidden entity classes. Category-based features are more
promising than profile-based features, which is reasonable
because the category labels in KBs contain prior human
knowledge on entity class and taxonomy information. Even
though combine ECDMM combines profile-based features
and category-based features in a straightforward manner,
it achieves the best performance. In comparison to GDM,
combine ECDMM improve F1 more than 10 percent and
AU C approximately 10 percent. We believe that the performance can be enhanced further with more comprehensive
entity class information and combination strategy.

5.6

The macro-averaged measurements in two scenarios are
reported in 2(a) and 2(b) respectively. The three baselines
are labeled with red color, and the blue dots represent our
proposed methods. The best method is labeled with pentagram in both figures. The parallel solid curves are contour lines of F1 value, which means the dots in the same
curve achieve same F1 values. The dots lying in upper right
achieve higher F1 than the lower left ones. Obviously, combine ECDMM achieves the best F1 in both scenarios. We
also find our mixture models (i.e., blue dots) achieve higher
precision in vital only scenario, demonstrating our models
can detect vital documents more accurately than the baselines.

5.7

Table 7: The averages of accuracies over 10 unseen
entities.
Methods
accu@(vital) accu@(vital + useful)
Official Baseline
.175
.532
BIT-MSRA
.445
.614
UDEL
.259
.579
GDM
.552
.565
naı̈ve ECDMM
.587
.608
profile ECDMM
.623
.647
category ECDMM
.565
.431
combine ECDMM
.580
.582

Fine-grained Results

P (e )

M

Performance on Unseen Entities

This section evaluates the generalization ability of our proposed models to handle unseen entities in the training set.
A robust model is able to handle unseen entities as well as
training entities. As listed in Table 6, there are 10 unseen
entities in the TREC-KBA-2013 dataset. We evaluate the
performance of our models on the unseen entity set composed of these 10 entities. We choose macro-averaged accuracy as the evaluation measurement. Due to the sparse
positive instances for some unseen entities, it is improper to
adopt precision, recall and F1 for evaluation because they
possibly become 0, in which case these measurements cannot
reflect the performance suitably. The results are reported in
Table 7.

This section compares the methods in a fine-grained level.
We need guarantee our mixture models not only achieve
remarkable overall performance, but also perform well in
entity-level. Hence, we recomputed the measurements in an
entity-sensitive manner.
Based on the classification results for each entity ei (i =
1, · · · , M ) in the test set, precision and recall of each model
are first calculated as P (ei ) and R(ei ). Then, we compute
the macro-averaged precision
and recall over all entities,
deP
P
M

AUC
.488
.578
.547
.588
.631
.675
.685
.716

In both scenarios, the best classification results are achieved
by profile ECDMM, which outperforms category ECDMM
and combine ECDMM. A possible explanation for the unsatisfactory performance of category ECDMM is that the
category information of unseen entities are not covered well
in the training set, especially the Twitter entities. For these
entities, there is too little category information to model
their hidden classes accurately.

R(e )

noted as macro P = 1 M i and macro R = 1 M i
respectively. At last, macro-averaged F1 is computed according to macro P and macro R.

642

0.9
Official Baseline
BIT−MSRA
UDEL
GDM
naïve_ECDMM
profile_ECDMM
category_ECDMM
combine_ECDMM

0.8

Official Baseline
BIT−MSRA
UDEL
GDM
naïve_ECDMM
profile_ECDMM
category_ECDMM
combine_ECDMM

0.8

0.7

Recall

Recall

0.75
0.6

0.7
0.5

0.65
0.4

0.3

0.15

0.2

0.25

0.3
Precision

0.35

0.4

0.45

0.6
0.45

0.5

0.5

0.55

0.6

0.65

0.7

Precision

(a) Vital Only

(b) Vital + Useful

Figure 2: Macro-averaged recall VS. macro-averaged precision over all test entities. The best approach is
dotted as pentagram.
Table 6: The statistics of
Entity
The Ritz Apartment (Ocala,Florida)
Keri Hehn
Chiara Nappi
Chuck Pankow
John H. Lang
Joshua Boschee
MissMarcel
evvnt
GandBcoffee
BartowMcDonald

test instances for 10 unseen entities.
KB
vital useful neutral/garbage
Wiki
4
1
5
Wiki
3
0
0
Wiki
2
3
55
Wiki
7
0
10
Wiki
2
0
1
Wiki
191
23
5
Twitter
52
13
3
Twitter
1
3
40
Twitter
0
2
2
Twitter
1
18
9

In vital only scenario, all the variants of our mixture model
can achieve better classification performance than GDM and
the other baselines. The results validate the flexibility of
our mixture model as expectation, which is essential for a
practical CCR system. Our mixture model is not only good
at handling existing entities in the training set, but also
capable of dealing with unseen entities.

5.8

total
10
3
60
17
3
219
68
44
4
28

Table 8: Information gain values of features.
Information Gain
Feature
Vital Only Vital + Useful
external bursty feature
0.130
0.286
internal bursty feature
0.020
0.008
max1
0.121
0.175
mean 2
0.046
0.081
median3
0.039
0.067
1
maximum IG of all semantic features
2
mean IG of all semantic features
3
median IG of all semantic features

Bursty Feature Analysis

To further validate the effectiveness of the proposed bursty
features, we evaluate them with the help of Information Gain
(IG). Table 8 reports the IGs of the proposed features in two
scenarios. All the IGs are computed following the method
proposed in [33]. The higher of the IG achieved by a feature,
the more powerful role it plays in the classification. The
maximum, mean and median IGs of semantic features are
also presented for reference. Since the bursty features are
only used in the discriminative counterpart of ECDMM, we
evaluate them with GDM.
In Table 8, external bursty features perform best out of all
features in both scenarios, conforming that external bursts
of an entity are accompanied with occurrences of its relevant
documents. However, internal bursts are not so helpful in
vital + useful scenario as in vital only scenario. This is
possibly caused by the incompleteness of the stream corpus.
As we know, the stream corpus are crawled from the web, so
it is possibly a biased snapshot of the true web. In addition,
we only utilize the occurrences of a target entity itself in
the stream corpus to detect its internal bursts currently.
We can include more evidences to improve the accuracy of

internal burst detection. For instance, contextual related
entities can be resorted to enhance the detection accuracy
of internal burst.

6.

CONCLUSIONS AND FUTURE WORK

The objective of Cumulative Citation Recommendation
(CCR) is to detect citation-worthy documents for a set of
KB entities from a chronological stream corpus. To address
the problem of training data insufficiency for less popular
entities, we propose an entity class-dependent discriminative mixture model (ECDMM) by introducing a latent entity class layer to model the hidden entity class information.
The model can be adjusted to different types of entities by
learning flexible combination parameters according to underlying entity classes. Experimental results demonstrate
that ECDMM can improve the performance of CCR. Entity-

643

document features and entity-class features are developed
for the discriminative and mixing components of ECDMM
respectively. In terms of entity-class features, profile-based
and category-based features are validated separately and in
a combination strategy. The novel bursty features developed as entity-document features are proved rewarding. Our
ECDMM with proposed semantic and temporal features can
achieve the state-of-the-art performance on TREC-KBA2013 dataset.
For future work, we wish to explore more useful entityclass features and apply more proper combination strategies
to improve the entity class-dependent mixture model.

[15] G. G. Gebremeskel, J. He, A. P. d. Vries, and J. Lin.
Cumulative citation recommendation: A feature-aware
comparison of approaches. In Database and Expert Systems
Applications (DEXA), pages 193–197. IEEE, 2014.
[16] A. Genkin, D. D. Lewis, and D. Madigan. Large-scale
bayesian logistic regression for text categorization.
Technometrics, 2007.
[17] Q. He, K. Chang, and E.-P. Lim. Using burstiness to
improve clustering of topics in news streams. In ICDM,
pages 493–498. IEEE, 2007.
[18] Q. He, K. Chang, E.-P. Lim, and J. Zhang. Bursty feature
representation for clustering text streams. In SDM, pages
491–496. SIAM, 2007.
[19] D. Hong and L. Si. Mixture model with multiple centralized
retrieval algorithms for result merging in federated search.
In SIGIR, pages 821–830. ACM, 2012.
[20] R. Jin, L. Si, and C. Zhai. A study of mixture models for
collaborative filtering. Information Retrieval, 9(3):357–382,
2006.
[21] B. Kjersten and P. McNamee. The hltcoe approach to the
trec 2012 kba track. In TREC. NIST, 2012.
[22] J. Kleinberg. Bursty and hierarchical structure in streams.
In KDD, pages 91–101. ACM, 2002.
[23] X. Liu, J. Darko, and H. Fang. A related entity based
approach for knowledge base acceleration. In TREC. NIST,
2013.
[24] A. Y. Ng and M. I. Jordan. On discriminative vs.
generative classifiers: A comparison of logistic regression
and naive bayes. In T. Dietterich, S. Becker, and
Z. Ghahramani, editors, Advances in Neural Information
Processing Systems 14, pages 841–848. MIT Press, 2002.
[25] A. D. O. Gross and H. Toivonen. Term association analysis
for named entity filtering. In TREC. NIST, 2012.
[26] J. H. C. B. S. Araujo, G. Gebremeskel and A. de Vries. Cwi
at trec 2012 kba track and session track. In TREC. NIST,
2012.
[27] M. Vlachos, C. Meek, Z. Vagena, and D. Gunopulos.
Identifying similarities, periodicities and bursts for online
search queries. In SIGMOD, pages 131–142. ACM, 2004.
[28] J. Wang, L. Liao, D. Song, L. Ma, C.-Y. Lin, and Y. Rui.
Resorting relevance evidences to cumulative citation
recommendation for knowledge base acceleration. In
WAIM, 2015.
[29] J. Wang, D. Song, C.-Y. Lin, and L. Liao. Bit and msra at
trec kba ccr track 2013. In TREC. NIST, 2013.
[30] Q. Wang, L. Si, and D. Zhang. A discriminative
data-dependent mixture-model approach for multiple
instance learning in image classification. In ECCV, pages
660–673. 2012.
[31] J. Weng and B.-S. Lee. Event detection in twitter. In
ICWSM, volume 11, pages 401–408. AAAI, 2011.
[32] Y. Yang and X. Liu. A re-examination of text
categorization methods. In SIGIR, pages 42–49. ACM,
1999.
[33] Y. Yang and J. O. Pedersen. A comparative study on
feature selection in text categorization. In ICML, pages
412–420, 1997.
[34] Y. Yang, T. Pierce, and J. Carbonell. A study of
retrospective and on-line event detection. In SIGIR, pages
28–36. ACM, 1998.
[35] W. X. Zhao, R. Chen, K. Fan, H. Yan, and X. Li. A novel
burst-based text representation model for scalable event
detection. In ACL, pages 43–47. ACL, 2012.
[36] M. Zhou and K. C.-C. Chang. Entity-centric document
filtering: boosting feature mapping through meta-features.
In CIKM, pages 119–128. ACM, 2013.

Acknowledgement
The authors would like to thank Jing Liu and Ning Zhang
for their valuable suggestions and the anonymous reviewers for their helpful comments. This work is funded by
the National Program on Key Basic Research Project (973
Program, Grant No. 2013CB329600), National Natural Science Foundation of China (NSFC, Grant Nos. 61472040
and 60873237), and Beijing Higher Education Young Elite
Teacher Project (Grant No. YETP1198).

7.[1] J.REFERENCES
Allan. Introduction to topic detection and tracking. In
[2]

[3]

[4]

[5]

[6]
[7]

[8]

[9]

[10]
[11]

[12]
[13]

[14]

Topic Detection and Tracking, volume 12 of The
Information Retrieval Series, pages 1–16. Springer US,
2002.
K. Balog and H. Ramampiaro. Cumulative citation
recommendation: classification vs. ranking. In SIGIR,
pages 941–944. ACM, 2013.
K. Balog, H. Ramampiaro, N. Takhirov, and K. Nørvåg.
Multi-step classification approaches to cumulative citation
recommendation. In OAIR, pages 121–128. ACM, 2013.
R. Berendsen, E. Meij, D. Odijk, M. d. Rijke, and
W. Weerkamp. The university of amsterdam at trec 2012.
In TREC. NIST, 2012.
L. Bonnefoy, V. Bouvier, and P. Bellot. A
weakly-supervised detection of entity central documents in
a stream. In SIGIR, pages 769–772. ACM, 2013.
Z. W. C. Tompkins and S. G. Small. Sawus: Siena’s
automatic wikipedia update system. In TREC. NIST, 2012.
J. Dalton and L. Dietz. Bi-directional linkability from
wikipedia to documents and back again: Umass at trec 2012
knowledge base acceleration track. In TREC. NIST, 2012.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum
likelihood from incomplete data via the em algorithm.
Journal of the Royal Statistical Society. Series B
(Methodological), pages 1–38, 1977.
L. Dietz and J. Dalton. Time-aware evaluation of
cumulative citation recommendation systems. In SIGIR
2013 Workshop on Time-aware Information Access
(TAIA2013), 2013.
L. Dietz, J. Dalton, and K. Balog. Umass at trec 2013
knowledge base acceleration track. In TREC. NIST, 2013.
Y. Fang, L. Si, and A. Mathur. Discriminative probabilistic
models for expert search in heterogeneous information
sources. Information Retrieval, 14(2):158–177, 2011.
T. Fawcett. An introduction to roc analysis. Pattern
Recogn. Lett., 27(8):861–874, June 2006.
J. Frank, S. J. Bauer, M. Kleiman-Weiner, D. A. Roberts,
N. Triouraneni, C. Zhang, and C. Rè. Evaluating stream
filtering for entity profile updates for trec 2013. In TREC.
NIST, 2013.
J. R. Frank, M. Kleiman-Weiner, D. A. Roberts, F. Niu,
C. Zhang, C. Re, and I. Soboroff. Building an
Entity-Centric Stream Filtering Test Collection for TREC
2012. In TREC. NIST, 2012.

644

