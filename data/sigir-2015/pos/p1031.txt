Shiny on Your Crazy Diagonal
Giorgio Maria Di Nunzio
Department of Information Engineering
University of Padua
Via Gradenigo 6/a, 35131 Padua, Italy

dinunzio@dei.unipd.it

ABSTRACT

In this demo, we want to study the problem of the optimisation of the parameters of the two BIM and BM25 models
by means of an interactive visualisation approach based on
the idea of Likelihood Spaces [5], a two-dimensional representation of probabilities. We have developed a web application which allows users to be directly involved into the
process of the optimisation of the retrieval function in a real
machine learning setting. The goal of this demo is to give
students deeper insight in the consequences of modeling assumptions (BIM vs. BM25) and the consequences of tuning
parameter values. As a showcase, we used the TREC2004
Robust test collection (528,155 documents and 250 topics). 1
We added to the original collection a pseudo-relevance feedback set constituted by the top 100 documents obtained by
a ‘standard’ BM25 approach for each query. For the online
version of this demo, we used a sample of the collection in
order to make the application usable. In particular, we built
the training/validation sets by using all the pseudo-relevant
feedback documents of the BM25 and the test set with all
the relevant documents of the pool.

In this demo, we present a web application which allows
users to interact with two retrieval models, namely the Binary Independence Model (BIM) and the BM25 model, on
a standard TREC collection. The goal of this demo is to
give students deeper insight into the consequences of modeling assumptions (BIM vs. BM25) and the consequences of
tuning parameter values by means of a two-dimensional representation of probabilities. The application was developed
in R, and it is accessible at the following link:
http://gmdn.shinyapps.io/shinyRF04.

Categories and Subject Descriptors
H.3.3 [[Information Search and Retrieval]: Retrieval
Models; D.2.8 [Software Engineering]: Metrics—complexity measures, performance measures

General Terms
Theory, Algorithms, Experimentation

Keywords

2.

Probabilistic Models, Bayesian Inference, Text Retrieval

The BIM ranks documents according to the probability
of relevance (R = 1) given a document d and a query q,
P (R = 1|d, q). The logarithm of this probability can be
approximated by (see [4] for the details of all the passages):
X
log (P (R = 1|d, q)) ≈
wi
(1)

1.

INTRODUCTION

The Binary Independence Model (BIM) has been one of
the most influential models in the history of Information
Retrieval [3]. It is a probabilistic model that considers documents as binary vectors and ranks them in order of their
probability of relevance given a query according to the Probability Ranking Principle [2]. The Okapi BM25 model goes
one step further by taking into account the frequencies of the
terms and the length of the documents. These two models
are easy to train and reach satisfactory results even with
standard default parameters. The optimisation of the parameters can be performed by means of machine learning
approaches; nevertheless, this process may be computationally demanding and only lead to partial improvements [6].

MATHEMATICAL BACKGROUND

ti ∈d∩q

where wi is the relevance score of the term ti , defined as
wi = log

(1 − qi )
pi
,
(1 − pi )
qi

(2)

where pi is the probability that a relevant document contains
the term ti , while qi is the probability that a non-relevant
document contains the term ti . The estimates of pi and qi
are smoothed with two parameters α and β to avoid zero
probabilities (by default, α = β = 0.5, see [1]). The BM25
model has the same form of Eq.1 by replacing wi with wi0 :

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the Owner/Author(s). Copyright is held by the
owner/author(s).
SIGIR’15, August 09-13, 2015, Santiago, Chile.
ACM 978-1-4503-3621-5/15/08.
DOI: http://dx.doi.org/10.1145/2766462.2767867 .

wi0 =

tfi
· wi
tfi + k1 ∗ ((1 − b) + b ∗ dl/∆)

(3)

where tfi is the frequency of the term ti in the document,
k1 and b are two parameters (usually set to 1.2 and 0.75,
respectively), dl is the length of document d, and ∆ is the
1

1031

http://trec.nist.gov/data/t13_robust.html

average document length. Eq. 3 derives from [4] (in this
formulation, (k1 + 1) is not present in the numerator).
In the two-dimensional representation of probabilities, we
keep P (R = 1|d, q) distinct from the probability of a document being not relevant P (R = 0|d, q), and we order documents according to the difference:
 X



X
qi
pi
−
log
(4)
log
1 − pi
1 − qi
t
t

6
1
2
3

i

i

4

With two more parameters M and Q, we define a decision
line y = M x + Q (see the details in [1]):
X pi
X qi
M
+Q −
(5)
1
−
p
1 − qi
i
ti
ti
| {z }
| {z }
x

5
Figure 1: Web application developed in Shiny.

y

This formulation allows us to study the problem on a twodimensional space where documents are represented by two
coordinates x and y and ranking is performed by the line
M x + Q − y.

3.

The blue line changes according to the parameters selected
in (5) while the green line remains fixed to the bisecting line
of the third quadrant.

SHINY WEB APPLICATION
4.

The application has been developed in R using the Shiny
package, an R package that makes it easy to build interactive
web applications straight from R. 2 The main window of the
application is split into two parts, see Fig. 1: on the left side,
the user can interact with the retrieval models and see the
results on the right side in terms of both the performance of
the retrieval and the visualisation of the coordinates.
Interaction: (1) The user chooses the topic of interest from
the drop-down menu, and the query is shown in the text
area. (2) The user selects the retrieval model (if BM25 is
not selected, the BIM is on), the pseudo-relevance feedback
is used (default is on), and whether the sum of Eq. 5 of the
probabilities is computed over the selected features (4) or
over the query terms. If BM25 is selected, the parameters k1
and b can be adjusted. (3) The parameters α and β are used
to smooth the probabilities pi and qi . (4) The fourth part
focuses on the machine learning approach to the problem:
select the number of folds k and the number of features we
need to train the retrieval model. By default we have a fivecross validation and the top 50 features selected according
to the difference pi − qi . The user can change the ranking
line by adjusting the two sliders Angular coefficient M and
Intercept Q. The default values are 1 and 0, respectively,
which correspond to a zero-one loss function. When these
parameters are changed, a green line remain fixed in the
position of the zero-one loss function for comparison.
Visualization: The right panel is divided into two columns:
(6) shows the results on the validation set, (7) the results on
the test set. Both columns contain the following pieces of information (from top to bottom): (i) The text box shows the
total number of objects used for validation and the number
of positive examples (red points, the pseudo-relevant documents of the chosen topic). The box in the validation column
also tells the user in what fold we are validating. (ii) The
table shows performance measures in terms of precision-atj (j = 5, 10, 20, 100, 500, 1000). (iii) The two-dimensional
plot shows in red the relevant documents of the chosen topic
(pseudo-relevant for the validation fold, true relevant for test
set) and in black all the other documents of the collection.
2

7

CONCLUSIONS

In this demo, we have presented a Web application developed in R which allows users to interact with two retrieval
models, the BIM and the BM25 models, on a standard
TREC collection. The probabilistic models are visualised
on a two-dimensional space based on the idea of Likelihood
spaces. The interactive application shows, in a real machine
learning setting, how the human pattern recognition capabilities can immediately detect whether the model is close
to the optimal solution or not. In theory, a classical fullyautomatic machine learning approach that searches the best
combination of parameters can find the optimal solution [6].
However, the number of possible combinations of the values of the parameters grows exponentially with the number
of the parameters; consequently, this interactive approach
may be a crucial step in setting the initial parameters of the
function that optimises these parameters automatically.

5.

REFERENCES

[1] Giorgio Maria Di Nunzio. A new decision to take for
cost-sensitive naı̈ve bayes classifiers. Information
Processing & Management, 50(5):653 – 674, 2014.
[2] S. E. Robertson. The Probability Ranking Principle in
IR. Journal of Documentation, 33(4):294–304, 1977.
[3] Stephen E. Robertson and Karen Sparck Jones.
Relevance weighting of search terms. In Peter Willett,
editor, Document retrieval systems, pages 143–160.
Taylor Graham Publishing, London, UK 1988.
”
[4] Stephen E. Robertson and Hugo Zaragoza. The
probabilistic relevance framework: BM25 and beyond.
Foundations and Trends in Information Retrieval,
3(4):333–389, 2009.
[5] Rita Singh and Bhiksha Raj. Classification in likelihood
spaces. Technometrics, 46(3):318–329, 2004.
[6] Andrew Trotman, Antti Puurula, and Blake Burgess.
Improvements to bm25 and language models examined.
In Proceedings of the 2014 Australasian Document
Computing Symposium, ADCS ’14, pages 58:58–58:65,
New York, NY, USA, 2014. ACM.

http://shiny.rstudio.com/

1032

