A Word Embedding based Generalized Language Model for
Information Retrieval
Debasis Ganguly

Dwaipayan Roy

ADAPT Centre, School of Computing
Dublin City University
Dublin, Ireland

CVPR Unit
Indian Statistical Institute
Kolkata, India

dganguly@computing.dcu.ie
Mandar Mitra

dwaipayan_r@isical.ac.in
Gareth J.F. Jones

CVPR Unit
Indian Statistical Institute
Kolkata, India

ADAPT Centre School of Computing
Dublin City University
Dublin, Ireland

mandar@isical.ac.in

gjones@computing.dcu.ie

ABSTRACT

Keywords

Word2vec, a word embedding technique, has gained significant interest among researchers in natural language processing (NLP) in
recent years. The embedding of the word vectors helps to identify a
list of words that are used in similar contexts with respect to a given
word. In this paper, we focus on the use of word embeddings for
enhancing retrieval effectiveness. In particular, we construct a generalized language model, where the mutual independence between
a pair of words (say t and t0 ) no longer holds. Instead, we make use
of the vector embeddings of the words to derive the transformation
probabilities between words. Specifically, the event of observing a
term t in the query from a document d is modeled by two distinct
events, that of generating a different term t0 , either from the document itself or from the collection, respectively, and then eventually
transforming it to the observed query term t. The first event of generating an intermediate term from the document intends to capture
how well a term fits contextually within a document, whereas the
second one of generating it from the collection aims to address the
vocabulary mismatch problem by taking into account other related
terms in the collection. Our experiments, conducted on the standard TREC 6-8 ad hoc and Robust tasks, show that our proposed
method yields significant improvements over language model (LM)
and LDA-smoothed LM baselines.

Generalized Language model, Word Embedding, Word2Vec

1.

INTRODUCTION

Word embedding as technique for representing the meaning of
a word in terms other words, as exemplified by the Word2vec approach [7]. The embedding of the word vectors enables the identification of words that are used in similar contexts to a specufic word.
a list of words that are used in similar contexts with respect to a
given word. While word Embedding has gained significant interest
among researchers in natural language processing (NLP) in recent
years, there has to date been little exploration of the potential for
use of these methods in information retrieval (IR).
This paper explores the use of word embeddings of enhance IR
effectiveness. We begin with a brief introduction to word embedding techniques and then motivate how can these be applied in IR.
A brief introduction to word embedding. Word embedding
techniques seek to embed representations of words. For example,
two vectors ~t and t~0 , corresponding to the words t and t0 , are close
in an abstract space of N dimensions if they have similar contexts
and vice-versa, (i.e. the contexts in turn having similar words) [4].
Use of a cosine similarity measure on this abstract vector space of
embedded words can be used to identify a list of words that are
used in similar contexts with respect to a given word. These semantically related words may be used for various natural language
processing (NLP) tasks. The general idea is to train moving windows with vector embeddings for the words (rather than training
with the more conventional word count vectors), and classify the individual windows [2]. This finds application for examples in applications such as POS tagging, semantic role labeling, named-entity
recognition and other tasks. The state-of-the-art word embedding
approaches involve training deep neural networks with the help of
negative sampling [7]. It is reported that this process of negative
sampling (commonly known as word2vec1 ) produces reliable word
embeddings in a very efficient manner [7].
Potential use in IR. We now discuss how word embeddings can
potentially be helpful in improving retrieval quality. In the context
of IR, vocabulary mismatch, i.e. the inherent characteristic of using different but semantically similar terms across documents about

Categories and Subject Descriptors
H.3.3 [INFORMATION STORAGE AND RETRIEVAL]: Information Search and Retrieval—Retrieval models, Relevance Feedback, Query formulation

General Terms
Theory, Experimentation
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’15, Aug 9–13, 2015, Santiago, Chile.
Copyright 2015 ACM ISBN 978-1-4503-3621-5/15/08 ...$15.00.
http://dx.doi.org/10.1145/2766462.2767780 .

1

The name word2vec comes from the name of the software tool
released by Micholov et. al. (https://code.google.com/
p/word2vec/

795

the same topic or between a query and its relevant documents, is a
difficult problem to solve.
However, the working principle of most standard retrieval models in IR involves an underlying assumption of term independence,
e.g. the vector space model (VSM) assumes that the documents are
embedded in a mutually orthogonal term space, while probabilistic models, such as the BM25 or the language model (LM) assume
that the terms are sampled independently from documents. Standard approaches in IR take into account term association in two
ways, one which involves a global analysis over the whole collection of documents (i.e. independent of the queries), while the other
takes into account local co-occurrence information of terms in the
top ranked documents retrieved in response to a query. The latter
approach corresponds to the relevance feedback step in IR which
we do not investigate in this paper. Existing global analysis methods such as the latent semantic indexing (LSA) [3] or latent Dirichlet allocation (LDA) [1] only take into account the co-occurrences
between terms at the level of documents instead of considering the
context of a term. Since the word embedding techniques that we introduced in the beginning of this section, leverage the information
around the local context of each word to derive the embeddings
(two words have close vector representations if and only if they
are used in similar contexts), we believe that such an approach can
potentially improve the global analysis technique of IR leading to
better retrieval effectiveness.
The rest of the paper is organized as follows. Section 2 discusses
related work. In Section 3, we propose the generalized LM, which
is evalaued in Section 4. Finally, Section 5 concludes the paper.

2.

according to the Bayes rule [8, 6, 10].
P (d|q) = P
=

λP̂ (t|d) + (1 − λ)P̂ (t|C) =

Y tf (t, d)
cf (t)
+ (1 − λ)
λ
|d|
cs
t∈q
(1)

In Equation 1, the set C represents a universe of documents (commonly known as the collection), P̂ (t|d) and P̂ (t|C) denote the
maximum likelihood estimated probabilities of generating a query
term t from the document d and the collection respectively, using
frequency statistics. The probabilities of these two (mutually exclusive) events are denoted by λ and 1 − λ respectively. The notations
tf (t, d), |d|, cf (t) and cs denote the term frequency of term t in
document d, the length of d, collection frequency of the term t and
the total collection size respectively.

3.2

Term Transformation Events

As per Equation 1, terms in a query are generated by sampling
them independently from either the document or the collection. We
propose the following generalization to the model. Instead of assuming that terms are mutually independent during the sampling
process, we propose a generative process in which a noisy channel
may transform (mutate) a term t0 into a term t. More concretely,
if a term t is observed in the query corresponding to a document
d, according to our model it may have occurred in three possible
ways, shown as follows.
• Direct term sampling: Standard LM term sampling, i.e.
sampling a term t (without transformation) either from the
document d or from the collection.
• Transformation via Document Sampling: Sampling a term
t0 (t0 6= t) from d which is then transformed to t by a noisy
channel.
• Transformation via Collection Sampling: Sampling the
term t0 from the collection which is then transformed to t
by the noisy channel.
Transformation via Document Sampling. Let P (t, t0 |d) denote the probability of generating a term t0 from a document d and
then transforming this term to t in the query.

RELATED WORK

P (t, t0 |d) = P (t|t0 , d)P (t0 |d)

(2)

0

In Equation 2, P (t |d) can be estimated by maximum likelihood
with the help of the standard term sampling method as shown in
Equation 1. For the other part, i.e. transforming t0 to t, we make use
of the cosine similarities between the two embedded vectors corresponding to t and t0 respectively. More precisely, this probability
of selecting a term t, given the sampled term t0 , is proportional to
the similarity of t with t0 . Note that this similarity is independent of
the document d. This is shown in Equation 3, where sim(t, t0 ) is
the cosine similarity between the vector representations of t and t0
and Σ(d) is the sum of the similarity values between all term pairs
occurring in document d, which being the normalization constant,
can be pre-computed for each document d.

A GENERALIZED LANGUAGE MODEL

sim(t, t0 )
sim(t, t0 )
=
00 )
sim(t,
t
Σ(d)
t00 ∈d

P (t|t0 , d) = P

In this section, we propose the generalized language model (GLM)
that models term dependencies using the vector embeddings of terms.

3.1

Y
t∈q

Latent semantic analysis (LSA) [3] is a global analysis technique in which documents are represented in a term space of reduced dimensionality so as to take into account inter-term dependencies. More recent techniques such as the latent Dirichlet allocation (LDA) represent term dependencies by assuming that each
term is generated from a set of latent variables called the topics
[1]. A major problem of these approaches is that they only consider word co-occurrences at the level of documents to model term
associations, which may not always be reliable. In contrast, the
word embeddings take into account the local (window-based) context around the terms [7], and thus may lead to better modeling of
the term dependencies.
Moreover, most of these global analysis approaches, e.g. LDA,
have been applied in IR in an ad-hoc way for re-assigning term
weights without explicitly representing the term dependencies as
an inherent part of an IR model. For example, an LDA document
model (term sampling probabilities marginalized over a set of latent topic variables) is linearly added as a smoothing parameter to
the standard LM probability [9], as a result of which the term dependencies are not clearly visible from the model definition. Contrastingly, in this paper, we intend to directly model the term dependencies as a part of an IR model.

3.

Y
P (q|d).P (d)
∝ P (q|d).P (d) = P (d).
P (t|d)
P (q|d0 ).(d0 )
t∈q

d0 ∈C

(3)

Consequently, we can write Equation 2 as

Language Modelling

P (t, t0 |d) =

In LM, for a given query q, documents are returned as a ranked
list sorted in decreasing order by the posterior probabilities P (d|q).
These posterior probabilities are estimated for each document d
during indexing time with the help of the prior probability (P (q|d))

sim(t0 , t) tf (t0 , d)
Σ(d)
|d|

(4)

Equation 4 favours those terms t0 s that are not only tend to co-occur
with the query term t within d, but are also semantically related to

796

1−λ−α−β
C

β
d

t0

α

Noisy Channel

TREC
disks

t

TREC 6
TREC 7
4&5
TREC 8
Robust

λ

Figure 1: Schematics of generating a query term t in our proposed Generalized Language Model (GLM). GLM degenerates
to LM when α = β = 0.

cf (t0 )
cs

sim(t, t0 )
sim(t, t0 )
=
00
Σ(Nt )
t00 ∈Nt sim(t, t )

301-350
351-400
401-450
601-700

2.48
2.42
2.38
2.88

92.22
93.48
94.56
37.20

t0 ∈d

β

X

0

P (t, t |C)P (t0 ) + (1 − λ − α − β)P (t|C)

(7)

t0 ∈Nt

3.3

Implementation Outline

An efficient approach to get the neighbours of a given term is
to store a pre-computed list of nearest neighbours in memory for
every word in the vocabulary. After this step, for each document
d in the collection, we iterate over term pairs (t, t0 ) and assign a
new term-weight to the term t representing the document sampling
transformation according to Equation 4. Then we iterate again over
every term t in d and use the pre-computed nearest neighbours of t
(Nt ) to compute a score for the collection sampling transformation,
as shown in Equation 6. To account for the fact that these transformation probabilities are symmetrical, we add the term t0 to d. Note
that it is not required to add the term t0 in case of the document
sampling transformation event because t0 is already present in d.

(5)

Now P (t|t0 , C) can be estimated in a way similar to computing
P (t|t0 , d), as shown in Equation 3. However, instead of considering all (t, t0 ) pairs in the vocabulary for computation, it is reasonable to restrict the computation to a small neighbourhood of terms
around the query term t, say Nt because taking too large a neighbourhood may lead to noisy term associations. This is shown in
Equation 6.
P (t|t0 , C) = P

title
title
title
title

transformation model in the term generation process.
X
P (t|d) = λP (t|d) + α
P (t, t0 |d)P (t0 )+

it. Thus, words that are used in similar contexts with respect to the
query term t over the collection, as predicted by the vector embeddings, are more likely to contribute to the term score of t. In other
words, Equation 4 takes into account how well an observed query
term t contextually fits into a document d. A term contextually fits
well within a document if it co-occurs with other semantically similar terms. Terms, score high by Equation 4, potentially indicate a
more relevant match for the query as compared to other terms with
low values for this score.
Transformation via Collection Sampling. Let the complementary event of transforming a term t0 , sampled from the collection
instead of a particular document, to the observed query term t be
denoted by P (t, t0 |C). This can be estimated as follows.
P (t, t0 |C) = P (t|t0 , C).P (t0 |C) = P (t|t0 , C).

Table 1: Dataset Overview
Qry Fields Qry Avg. qry Avg. #
set
Ids
length rel. docs

4.

EVALUATION

Experimental Setup. Our experiments were conducted on the
standard TREC ad hoc tasks from TREC 6, 7, 8 and the Robust
track. Information about the document and the query sets is outlined in Table 1. We implemented GLM using the Lucene2 IR
framework. As one of our baseline retrieval models, we used standard LM with Jelinek Mercer smoothing [6, 10], which is distributed as a part of Lucene. Additionally, we also used LM with
LDA smoothing [9] as our second baseline to compare against. In
contrast to [9], which reports retrieval results with LDA smoothed
LM (LDA-LM) on individual document subsets (and their corresponding relevance judgements) from the TREC collection as categorized by their sources, i.e. the “LA Times” and the “Financial
Times”, we instead executed LDA on the whole TREC collection.
The rationale for using LDA as a baseline is that analogous to our
model, LDA also attempts to model term dependencies by taking
into account latent variables (called the topics). This baseline was
also implemented in Lucene.
Parameters. The parameter λ of the LM baseline was empirically set to 0.2 (after varying it within a range of [0.1, 0.9]). This
value of λ for the TREC collection agrees with the observations
reported in [6]. According to the findings of [9], the number of
topics in LDA, i.e. K, was set to 800. As prescribed in [5], we set
the LDA hyper-parameters α and β (note that these are different
from the GLM parameters) to 50/K and 0.01 respectively. Obtaining effective word embedding is an integral part of the GLM.
The word embeddings for the experiments reported in this section
were obtained on the TREC document collection with the parameter settings as prescribed in [7], i.e., we embedded the word vector in a 200 dimensional space, and used continuous bag-of-words

(6)

While P (t, t0 |d) measures the contextual fitness of a term t in a
document d with respect to its neighbouring (in the vector space
of embedded terms) terms t0 in d, P (t, t0 |C), on the other hand,
aims to alleviate the vocabulary mismatch between documents and
queries in the sense that for each term t in d it expands the document with other related terms t0 s. From an implementation perspective, P (t, t0 |d) reweights existing document terms based on
their contextual fit, whereas P (t, t0 |C) expands the document with
additional terms with appropriate weights.
Combining the Events. Finally, for putting all the events together in the LM generation model, let us assume that the probability of observing a query term t without the transformation process (as in standard LM) be λ. Let us denote the probability of
sampling the query term t via a transformation through a term t0
sampled from the document d with α, and let and the complementary probability of sampling t0 from the collection be then β, as
shown schematically in Figure 1. The LM term generation probability in this case can thus be written as shown in Equation 7. This
is a generalized version of the standard LM, which we now henceforth refer to as generalized language model (GLM), that takes into
account term relatedness with the help of the noisy channel transformation model, which in turn uses the word embeddings to derive
the likelihood of term transformations. Note that the GLM degenerates to standard LM by setting α and β to zero, i.e. not using the

2

797

http://lucene.apache.org/core/

Table 2: Comparative performance of LM, LDA and GLM on
the TREC query sets.

0.1955

0.2288

0.1945

0.228

MAP

MAP

0.2284

0.2276

Metrics
0.1935

0.2272

β=0.1
β=0.3

β=0.2
β=0.4

0.3

0.4

0.1925

0.2268
0.1

0.2

β=0.1
β=0.3
0.1

β=0.2
β=0.4
0.2

0.3

(a) TREC-6

Method

MAP

GMAP

Recall

TREC-6

LM
LDA-LM
GLM

0.2148
0.2192
0.2287

0.0761
0.0790
0.0956

0.4778
0.5333
0.5020

TREC-7

LM
LDA-LM
GLM

0.1771
0.1631
0.1958

0.0706
0.0693
0.0867

0.4867
0.4854
0.5021

TREC-8

LM
LDA-LM
GLM

0.2357
0.2428
0.2503

0.1316
0.1471
0.1492

0.5895
0.5833
0.6246

Robust

LM
LDA-LM
GLM

0.2555
0.2623
0.2864

0.1290
0.1712
0.1656

0.7715
0.8005
0.7967

0.4

α

α

Topic Set

(b) TREC-7
0.2865

0.2505

β=0.1

β=0.2

β=0.3

β=0.7

MAP

MAP

0.2855
0.2495

0.2845

β=0.1
β=0.3

0.2485

0.1

β=0.2
β=0.4
0.2

0.2835
0.3

α

(c) TREC-8

0.4

0.1

0.2

0.3

0.4

α

(d) Robust

Figure 2: Effect of varying the GLM parameters α and β on
the MAP values for the TREC query sets.
either a document or the collection and then changing it to another
term after passing it through a noisy channel. The term transformation probabilities of the noisy channel, in turn, are computed by
making use of the distances between the word vectors embedded in
an abstract space. We argue that this model has two fold advantage,
firstly it is able to estimate how well a term fits in the context of a
document, and secondly it is able to decrease the vocabulary gap
by adding other useful terms to a document. Empirical evaluation
shows that our method significantly outperforms the standard LM
and LDA-LM. Possible future work will be to investigate compositionality of terms from the vector embeddings of words.
Acknowledgement. This research is supported by SFI through
the CNGL Programme (Grant No: 12/CE/I2267) in the ADAPT
Centre (www.adaptcentre.ie) at Dublin City University, and by a
grant under the SFI ISCA India consortium.

with negative sampling. The neighbourhood Nt of the GLM (see
Equation 7) was set to 3, i.e., for each given term in a document,
we consider adding at most 3 related terms from the collection.
Results. First, we varied the GLM parameters, namely α and
β within the range [0.1, 0.4] so as to ensure that α + β + λ < 1
(λ being set to 0.2) for all the query sets used in our experiments.
The results are shown in Figure 2. It can be seen that the optimal
values of α and β depend on the query set, e.g. for the TREC 8
query set (Figure 2c, the optimal results are obtained for (α, β) =
(0.3, 0.2), whereas this combination does not produce the optimal
results for the other query sets. It can be observed that a reasonable
choice for these parameters is in the range [0.2, 0.3], which means
imparting more or less uniform weights to all the term generation
events, namely α, β and λ. In Table 2, we show the optimal results
obtained with GLM for each individual query set and compare the
results with the baselines, i.e. the LM and the LDA-LM. It can be
observed that for each query set, GLM significantly3 outperforms
the baselines. It turns out that the LDA-LM (almost) consistently
outperforms the standard LM. However, the results (as measured by
the percentage gains in comparison to standard LM) do not seem to
be as high as reported in [9] (about 3% as compared to about 8%).
We believe that the reason for this is due to the diversity in the LDA
topics caused by the news articles from different sources.
From Table 2, we observe that GLM consistently and significantly outperforms both LM and LDA-LM for all the query sets.
Not only does it increase the recall values in comparison to LM,
but it also increases precision at top ranks by always outperforming
LDA in terms of MAP. Although LDA achieves higher recall than
GLM in two cases (TREC-6 and Robust), the higher recall in the
case of LDA does not significantly increase the MAP, which is indicative of the fact that the precision at top ranks does not improve.
For GLM however, an increase in the recall value is always associated with a significant increase in MAP as well, which indicates
that precision at top ranks remains relatively stable in comparison
to LDA.

5.

6.

REFERENCES

[1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet Allocation.
Journal of Machine Learning Research, 3:993–1022, March 2003.
[2] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and
P. Kuksa. Natural language processing (almost) from scratch. J.
Mach. Learn. Res., 12:2493–2537, Nov. 2011.
[3] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and
R. A. Harshman. Indexing by latent semantic analysis. JASIS,
41(6):391–407, 1990.
[4] Y. Goldberg and O. Levy. word2vec Explained: deriving Mikolov et
al.’s negative-sampling word-embedding method. CoRR,
abs/1402.3722, 2014.
[5] T. L. Griffiths and M. Steyvers. Finding scientific topics. Proceedings
of the National Academy of Sciences (PNAS), 101(suppl.
1):5228–5235, 2004.
[6] D. Hiemstra. Using Language Models for Information Retrieval. PhD
thesis, Center of Telematics and Information Technology, AE
Enschede, 2000.
[7] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean.
Distributed representations of words and phrases and their
compositionality. In Proc. of NIPS ’13, pages 3111–3119, 2013.
[8] J. M. Ponte and W. B. Croft. A language modeling approach to
information retrieval. In SIGIR, pages 275–281. ACM, 1998.
[9] X. Wei and W. B. Croft. LDA-based document models for ad-hoc
retrieval. In SIGIR ’06, pages 178–185, 2006.
[10] C. Zhai and J. Lafferty. A study of smoothing methods for language
models applied to information retrieval. ACM Trans. Inf. Syst.,
22(2):179–214, Apr. 2004.

CONCLUSIONS AND FUTURE WORK

We proposed a generalized version of the language model for IR.
Our model considers two possible cases of generating a term from
3
Measured by Wilcoxon statistical significance test with 95% confidence.

798

