The Best Published Result is Random: Sequential Testing
and its Effect on Reported Effectiveness
Ben Carterette
Department of Computer and Information Sciences
University of Delaware
Newark, DE, USA 19716

carteret@udel.edu

35

ABSTRACT

20

0.24

0.26

0.28

0.30

0.32

0.34

maximum MAP in sample

Categories and Subject Descriptors: H.3.4 [Information Storage and Retrieval] Performance Evaluation
General Terms: Experimentation, Measurement
Keywords: information retrieval; test collections; evaluation; statistical analysis

1.

15
0

5

10

Density

25

30

Reusable test collections allow researchers to rapidly test
different algorithms to find the one that works “best”. But
because of randomness in the topic sample, or in relevance
judgments, or in interactions among system components, extreme results can be seen entirely due to chance, particularly
when a collection becomes very popular. We argue that the
best known published effectiveness on any given collection
could be measured as much as 20% higher than its “true” intrinsic effectiveness, and that there are many other systems
with lower measured effectiveness that could have substantially higher intrinsic effectiveness.

Figure 1: The distribution of the maximum of 100
samples from a normal distribution with mean 0.20
and standard deviation 0.027.
to find most reasonable IR systems. It is the systems that
are well above that baseline that we are most interested in,
and in particular, the one with the maximum measured effectiveness is widely considered the best possible system to
compare against.
But just as two systems can have different measured effectiveness due to chance, a system can have effectiveness
higher than the expected collection baseline due to chance.
This randomness in turn means that there must be randomness in our determination of which system has produced the
maximum value. It is actually likely that whatever method
produces the largest measured effectiveness on a given collection has lower intrinsic effectiveness than reported. This is
because of sequential testing: the more tests are done with a
given collection, the more likely it becomes that an extreme
effectiveness value will be observed.
Rather than take the maximum value at face value, we argue that we should analyze it in the context of other known
results and the likelihood that such a value could be produced by a system whose intrinsic effectiveness is much lower
simply due to random factors. Of course, the opposite is
true as well: any given result could be produced by a system whose intrinsic effectiveness is much higher. We investigate both sides to argue that the intrinsic effectiveness of
the best known system could be as much as 20% lower than
reported, and systems as much as 20% less effective than the
best could have intrinsic effectiveness much higher.
In Section 2 we introduce extreme value theory, and in
Sections 3 and 4 we show how we can use it for deeper analysis of IR experiments. We conclude in Section 5 with some
discussion about the implications of this work.

INTRODUCTION

Statistical significance testing is an important aspect of
experimentation in IR. Without it, differences in effectiveness on the order of 5% would be difficult to interpret: they
could represent a “real” improvement in effectiveness, or they
could be the product of random noise. Significance testing
helps us differentiate between the two [4, 5, 3].
That we use significance testing implies that we accept
there is randomness in measuring effectiveness. There is
randomness due to a topic sample, due to documents in
a collection, due to relevance judgments, and other factors
of a test collection. Significance testing asks whether the
variance in effectiveness that can be ascribed directly to differences in the systems being tested outweighs those other
sources of variance [1].
A full accounting of variance (such as that done for an
ANOVA) could compute the total variance due to collection
factors, suggesting that there is a “baseline” level of effectiveness for a given collection close to which we should expect
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
SIGIR’15, August 09 - 13, 2015, Santiago, Chile.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3621-5/15/08 ...$15.00.
DOI: http://dx.doi.org/10.1145/2766462.2767812.

747

0.27

0.28
0.26
0.25

2.2
10

20

50

100

200

500

1000

number of samples

Figure 2: Increase in the expected maximum as the
number of samples increases.

2.

EXTREME VALUE DISTRIBUTIONS

Suppose we have 100 normal distributions, each of which
has an identical population mean µ and population standard
deviation σ. From each of these we sample 50 values and
average them so that we have 100 sample means µ
b1 · · · µ
b100 .
We can think of these as 100 measures of mean effectiveness
over 50 topics from systems with the same intrinsic effectiveness. Each of them will be “close to” the population mean.
But some will be further away than others, and in particular, one of them must be the maximum of the 100. Given
that we’ve sampled 100 values, it is likely that whatever the
maximum is, it will be more than two standard deviations
above the population mean; that is, it will “look like” a significant improvement over the population mean even though
it was sampled from exactly the same distribution.
Figure 1 shows the distribution of maximum MAP when
100 values are sampled from normal distributions with mean
0.20 and standard deviation 0.027. The mean of the distribution of maximums is 0.267, which, if taken at face value,
would look like a 34% improvement over the mean of 0.2!
Furthermore, 0.267 is outside of the 95% confidence interval
around 0.2; in fact only 0.6% of the distribution is greater
than 0.267. A full 99% of the distribution of maximums is
greater than the upper bound of the 95% confidence interval,
meaning it is a near-certainty that out of 100 systems with
identical intrinsic effectiveness, at least one will be measured
above the 95% confidence interval around the mean.
Figure 2 shows the increase in expected maximum MAP as
the number of samples increases. From this we argue that as
the number of experiments performed on a particular collection increases, the expected maximum effectiveness reported
on that collection will increase logarithmically, even if the
“real” effectiveness of the systems being experimented on is
not significantly different from the overall mean.

2.1

3.

ANALYSIS USING EVD

We typically answer statistical questions such as “is one
algorithm better than another?” using statistical hypothesis
testing. Procedures for hypothesis testing start by forming
a reference distribution for the statistic in question (say,
difference in mean effectiveness), then checking whether the
measured value is in the tail of that distribution. If so, we
say the systems are significantly different.
The extreme value question is “is this algorithm’s effectiveness better than the maximum expected among N algorithms with the same intrinsic effectiveness on the same
collection?” 2 Note that the question includes the number
of samples N ; this is a key difference between a one-off test
of significance versus a test that accounts for the history of
experiments done.
We would answer that question by forming a reference
distribution for maximum effectiveness rather than mean effectiveness. That distribution must be based on variation
across a sample of systems as well as topics, and must also
be based on the number of systems N . It must be specific to a collection and an effectiveness measure, since different collections and measures exhibit different variability
—average precision typically has low variance compared to
other measures, while P@10 has high variance; more recent
collections (which tend to be larger and more heterogeneous)
like ClueWeb12 tend to exhibit higher variance than older
(smaller and less heterogeneous) collections like WSJ or AP.
To form a reference distribution for a given collection and
measure, we will first need to obtain a set of mean effectiveness values. Once obtained, we will assume that all of
those values came from the same distribution: a normal distribution centered at their means, with variance equal to the
variance of those means divided by the number of topics in

Extreme value theory

Extreme value theory is the area of statistics devoted to
distributions of maximum and minimum sampled values [2].
The Gumbel distribution is an example of an extreme value
distribution (EVD) that is useful for normally-distributed
random variables. Its cumulative density function is:

1

P (X ≤ x|α, β) = e

Estimating an extreme value distribution

The exact relationship between β, σ 2 , N has no closed
form1 . It is easy enough to estimate a Gumbel distribution using sampling, however. Given a population mean µ
and population standard deviation σ, we sample n values
from a normal distribution with those parameters N times
(n represents the number of topics; N the number of systems), average those n values for each of the N samples, and
take the maximum average. Over many trials, this produces
an approximation of the Gumbel distribution.
We can simplify this further by just taking N samples
from a normal
distribution with mean µ and standard devi√
ation σ/ n—the latter is known as the standard error or the
sampling distribution of the mean. Figure 1 was generated
this way, as was Figure 2.

0.24

expected maximum value

parameter α is equivalent to µ, and the parameter β is an
increasing function of both σ 2 and N . The expected value
of the maximum is then given as α + γβ, where γ is Euler’s
constant, which has a value of about 0.5772. Since α = µ, γ
is constant, and β is an increasing function of N , this means
that the expected maximum increases with the number of
identically-distributed samples.

Closed forms exist only for N ≤ 5 [7].
We use the phrase “intrinsic effectiveness” as a shorthand
for “population effectiveness”, which refers to the system’s
effectiveness measured over the full population of queries.
In practice there may not be a finite population of queries
that could be measured even in principle.

−e−(x−α)/β

2

This could be the distribution of random variable X representing the maximum value of N samples from a normal
distribution with mean µ and variance σ 2 . In that case, the

748

the collection (this is the variance of the sampling distribution). Then the maximum mean effectiveness has a Gumbel
distribution, parameterized by α (the mean of the original
normal distribution) and β (which is a function of the original variance as well as the number of means in the set); we
estimate that distribution as described above.
Though we have described how to estimate a reference
distribution that could be used in a significance test, we are
not actually going to propose a significance test. Instead, we
will use a reference distribution to analyze results reported
using different collections.

80

60% interval containing likely non−extreme values

40
0

20

Density

60

95% confidence interval

0.22

0.24

0.26

0.28

0.30

0.32

MAP

3.1

TREC-7 run analysis
Figure 3: A normal distribution with mean 0.2705
and standard deviation 0.0114 (solid line), along
with its maximum value distribution and minimum
value distribution for N = 103. Blue lines show
the 95% confidence interval of the normal distribution; red lines show the 60% interval in which
non-extreme values are likely to fall.

There were 103 submissions to TREC-7, so we have N =
103 mean average precisions (MAPs). The mean of means,
which we will use for µ, is about 0.2; we consider this the
“baseline” effectiveness by MAP for the collection. The standard deviation among means, which we will use for σ, is 0.08.
We assume that each mean is drawn from a normal
√ distribution with mean 0.2 and standard deviation 0.08/ 50, which
is the standard deviation of the sampling distribution of the
mean, also known as the standard error. To generate the
reference maximum value distribution (MxVD), we repeatedly sample 103 values from a normal distribution with those
parameters and take the maximum of those values.
One possible analysis similar to a significance test is as
follows: identify the MAP in the MxVD such that 5% of
the distribution is greater than or equal to that value. That
represents the minimum MAP a system would need for us
to conclude with high confidence that it is not just a random
extreme value from a distribution with mean 0.2. In TREC7, that value is about 0.2375. 35 of the 103 submitted runs
have a MAP greater than 0.2375; we would say that it is
likely 33 (95% of 35, since we expect 5% to be false positives)
of those have intrinsic effectiveness above the overall mean.
We could do the same for minimum MAP. We find that
5% of the minimum value distribution (MnVD) is less than
0.1625, and 34 TREC-7 submissions have MAPs lower than
that. This leaves 34 systems with MAPs within the bounds
of what would be expected given that we’ve sampled 103
total MAPs, the highest and lowest of which are nearly 20%
different from the mean.
For the systems outside those bounds, we might also ask
what distribution they could have reasonably come from.
What is the minimum mean that could generate the maximum observed MAP with high enough probability that we
do not consider it significant? Let us take the maximum
MAP of any automatic TREC-7 run, since we expect a priori that manual runs will have higher MAPs. That is 0.303
for the ok7ax run [6]. Then the question is how low a population mean could produce an extreme value of 0.303 or
higher (with the same variance and N ) with probability 0.2.
Applying a linear search, we find that value to be 0.2705,
which is 11% lower than 0.303.
We might also ask how low a MAP we could measure
when 103 are sampled from a distribution centered at 0.2705
rather than 0.2, or, what is the minimum MAP that we could
observe with probability greater than 0.2 sampled from that
distribution? It turns out that it could be as low as 0.2378.
Therefore any MAP between 0.2378 and 0.303 could have
come from a distribution centered at 0.27 if 103 values are
sampled, and the probability of observing a MAP between
those values is 60%.

venue
SIGIR
ECIR
CIKM

years
1995–2014
2005–2014
2005–2014

papers
2,413
759
2,620

stats
1,159 short, 1,254 long
346 short, 413 long
1,015 short, 1,605 long;

Table 1: IR research paper corpora.
To summarize, if we sample 103 mean average precisions
from a distribution centered at 0.2705, there is a 20% chance
the maximum sampled MAP would be greater than 0.303,
and a 20% chance the minimum sampled MAP would be less
than 0.2378. These bounds are outside of the 95% confidence
interval around 0.2705, so would likely be considered statistically significantly different than 0.2705, even if the systems
turned out to be equivalent over a much larger sample of
topics. Figure 3 illustrates this, comparing the normal distribution and its 95% confidence interval to its extreme value
distributions and the corresponding 60% interval in which
non-extreme values are likely to fall.
The conclusion of this example is that there is a wide
range of possible MAPs that are likely to be observed when
sampling 100 from this distribution, significantly wider than
is implied from its 95% confidence interval. The fact that the
largest of them is 0.303 is random; under slightly different
conditions that same system could have produced a MAP
closer to 0.2378, and a system with a MAP of 0.24 could
have produced a MAP of 0.3. Yet the change from 0.2378
to 0.303 represents a 27% improvement in effectiveness.

4.

ANALYSIS OF IR EXPERIMENTS

In this section we analyze the IR literature to find distributions of effectiveness for different standard collections.
We have a corpus of IR conference papers from 1995–2014,
some statistics of which are shown in Table 1.We searched
this corpus for papers using some standard collections: the
Wall Street Journal (WSJ) and Associate Press (AP) collections on TREC disks, the GOV2 collection, the WT10g
collection, and the TREC Robust 2004 track collection. We
transcribed results from these papers, specifically mean effectiveness results. Then for a collection and a measure, we
have a set of mean effectiveness values that we can analyze.

749

collection
Robust ’04
WSJ
AP
GOV2
WT10g

N
55
31
31
17
17

µ
0.2660
0.2577
0.2091
0.2523
0.1721

σ
0.0024
0.0108
0.0096
0.0144
0.0059

max
0.3591
0.4033
0.2982
0.3806
0.2352

researchers. Moreover, we have shown that the best known
result could come from a system whose intrinsic effectiveness is as much as 20% lower than its observed mean, while
a system with much lower observed effectiveness could have
intrinsic effectiveness up to 20% greater. This means that
there is a wide range of possible overlap in effectiveness,
more than what is implied by the standard deviation normally computed for significance testing, due solely to the
effect of reusing the collection, and enough that results that
are statistically significantly different may actually not be
once the extreme value distributions are taken into account.
This means there are extra considerations when reusing
test collections and when comparing to best known results.
The danger of reusable test collections is that the longer they
are used, the more likely it is that an extreme value will be
observed by chance alone. This implies that we must mentally adjust reported results downward some, particularly for
older or very popular collections, and especially for methods
that have not been shown to consistently work across collections. A relatively simple retrieval approach like BM25 that
we know to work well in many different settings, is likely to
be a better point of comparison than a much more complex
model that happens to have the highest effectiveness on a
single collection.
It also suggests that it is not always beneficial to rely on
reusable test collections to advance the field. Proprietary
collections can be beneficial in that they will not be used by
as many different researchers, and thus their N may remain
relatively small. Strictly non-reusable collections can never
have N > 1, and therefore will never have an issue with extreme values being observed due to large N . Therefore it is
probably best for the field to publish a portfolio of results
across reusable test collections (which will always be good
for unit testing, for prototyping, for training, and for failure
analysis), proprietary collections (which can include data
unavailable outside of the group that owns it, and therefore
suggest new avenues of discovery), and non-reusable collections (which should be considered the true “test set”).

Table 2: Summary statistics on mean average precisions reported in published IR papers for different
standard reusable collections.
collection
Robust ’04
WSJ
AP
GOV2
WT10g

µ0
0.3448
0.3768
0.2747
0.3489
0.2227

±20% c.i.
(0.3384, 0.3591)
(0.3502, 0.4033)
(0.2513, 0.2982)
(0.3170, 0.3806)
(0.2096, 0.2352)

# above l.b.
3
5
12
6
1

Table 3: Means of distributions that could produce
the maximum values in Table 2 along with the 60%
confidence interval for non-extreme values. The final column is the number of MAPs greater than the
lower bound.
Table 2 shows summary statistics about data sets. Each
row gives the number of results we transcribed (N ), the
mean of those results (µ), their standard deviation (σ), and
the maximum MAP in our sample. Our N s are fairly low,
and necessarily a lower bound on the actual value of N . (In
fact N cannot be known, since it includes experiments done
but never published.)
Table 3 shows results of the analysis we described in Section 3.1. For each collection we report the mean µ0 of the
normal distribution for which the maximum value reported
in Table 2 has 80% cumulative probability in the MxVD—so
the upper 20th percentile value in this table is the same as
the maximum in Table 2. The lower 20th percentile is the
value of MAP at the 20th percentile in the MnVD for the
distibution centered at µ0 . These numbers are essentially a
mean and 60% confidence bound (like the one in Figure 3)
for a distribution that reasonably could have produced the
maximum observed value for each collection.
The lower limit of the confidence interval is the value we
are most interested in, as it gives an idea of how low measured effectiveness could be while intrinsic effectiveness is
still competitive with the best observed effectiveness. Note
that the ranges are wide, with the upper bound up to 20%
higher than the lower (for GOV2) and over 15% for three
of the collections (WSJ, AP, GOV2). The range is lowest
for Robust ’04, because that collection has a much larger
number of topics (249) and therefore lower standard error.
The last column in Table 3 gives the number of systems
in our sample with effectiveness greater than or equal to
the minimum value. In all but WT10g there is more than
one system that could reasonably be a candidate for “best
performing” on the collection. If one performs slightly worse
than another, it is most likely due to randomness.

5.

Acknowledgments This material is based upon work supported by the National Science Foundation under Grant No.
IIS-1350799. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the
authors and do not necessarily reflect the views of the National Science Foundation.

6.

REFERENCES

[1] B. Carterette. Multiple testing in statistical analysis of
systems-based information retrieval experiments. ACM
TOIS, 30(1), 2012.
[2] S. Coles. An Introduction to Statistical Modeling of Extreme
Values. Springer, 2001.
[3] M. Smucker, J. Allan, and B. Carterette. A comparison of
statistical significance tests for information retrieval
evaluation. In Proceedings of CIKM, pages 623–632, 2007.
[4] J. Tague. The pragmatics of information retrieval evaluation.
pages 59–102. Buttersworth, 1981.
[5] C. J. van Rijsbergen. Information Retrieval. Butterworths,
London, UK, 1979.
[6] E. M. Voorhees and D. Harman. Overview of the Seventh
Text REtrieval Conference (TREC-7). In Proceedings of the
Seventh Text REtrieval Conference (TREC-7), pages 1–24,
1999.
[7] E. W. Weisstein. Gumbel distribution. From MathWorld–A
Wolfram Web Resource.
http://mathworld.wolfram.com/GumbelDistribution.html.

CONCLUSION

We have argued that the best known result on any given
test collection has a component of randomness due to the
number of times the collection has been experimented with—
something that is out of the control of and unknown to most

750

