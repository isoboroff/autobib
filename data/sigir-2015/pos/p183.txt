How Many Results Per Page? A Study of
SERP Size, Search Behavior and User Experience
Diane Kelly

Leif Azzopardi

School of Information and Library Science,
University of North Carolina
Chapel Hill, NC, USA

School of Computing Science, University of
Glasgow
Glasgow, United Kingdom

dianek@email.unc.edu

leif.azzopardi@glasgow.ac.uk

ABSTRACT

1.

The provision of “ten blue links” has emerged as the standard for the design of search engine result pages (SERPs).
While numerous aspects of SERPs have been examined, little attention has been paid to the number of results displayed
per page. This paper investigates the relationships among
the number of results shown on a SERP, search behavior
and user experience. We performed a laboratory experiment with 36 subjects, who were randomly assigned to use
one of three search interfaces that varied according to the
number of results per SERP (three, six or ten). We found
subjects’ click distributions differed significantly depending
on SERP size. We also found those who interacted with
three results per page viewed significantly more SERPs per
query; interestingly, the number of SERPs they viewed per
query corresponded to about 10 search results. Subjects who
interacted with ten results per page viewed and saved significantly more documents. They also reported the greatest
difficulty finding relevant documents, rated their skills the
lowest and reported greater workload, even though these
differences were not significant. This work shows that behavior changes with SERP size, such that more time is spent
focused on earlier results when SERP size decreases.

Over the years, the standard design for a search engine
results pages (SERPs) has converged on a textual listing of
ten results per page; this presentation style is often referred
to as ‘ten blue links.’ Despite many scholars and designers
questioning this convention, this standard persists. While
the literature contains many examples of alternatives to this
design including variable snippet styles e.g., [31] and layout
methods e.g., [15], few works have questioned or considered
how the number of results presented per page (SERP Size),
impacts search behavior and if a different size might lead to a
better user experience. Although many experimental search
interfaces, in particular pre-Web interfaces, have presented
more than ten results per page, and many current search
interfaces allow users to configure the number of results per
page, to our knowledge, the size of the SERP has only been
systematically investigated in a few studies [19, 23, 25].
Understanding how SERP size influences user behavior
is important for several reasons. First, users are now accessing search results via a multitude of devices, including
desktop computers, mobile phones, tablets, smart watches
and smart glasses. Each device displays varying numbers of
results above-the-fold and research has shown that this can
impact click behavior [12, 18]. Thus understanding how the
behaviors and performance of users changes with respect to
SERP size can potentially help guide and inform researchers
and practitioners who are interested in designing effective interfaces. Second, many IR evaluations measures [11, 22, 28]
make assumptions about the number of results presented to
the user and how the user will evaluate these results; specifically, that the user examines results linearly and the likelihood of the user examining a result decreases exponentially
with rank. This underlying user model might be insufficient
if variable SERP sizes are considered. Finally, little is known
about how SERP size impacts the user experience; it might
be the case that presenting fewer results per page has positive psychological consequences as found by Oulasvirta et
al. [23] who examined user preferences for mocked-up search
result sets of varying lengths. On the other hand, fewer
results may lead to increased browsing costs and increased
mental load as users need to flip between result pages detracting from their search experience.
Thus, the purpose of this paper is to investigate the impact of SERP size on search behavior and user experiences.
To do so, we create three search interfaces that vary according to the number of results per SERP (three, six or ten)
and conduct a between-subjects laboratory experiment.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval:Search Process; H.5.2 [Information
Interfaces and Presentation]: User Interfaces:Screen Design

Keywords
Search behavior; user studies; search interface; search result
page

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
SIGIR’15, August 09 - 13, 2015, Santiago, Chile
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3621-5/15/08 ...$15.00
DOI: http://dx.doi.org/10.1145/2766462.2767732.

183

INTRODUCTION

2.

BACKGROUND

Numerous aspects of search engine result pages (SERPs)
have been investigated including layout (e.g., ranked list,
grid), ordering (ranking of results), components (e.g., aggregated search pages, query suggestion), length and content of snippet summaries (e.g., query-biased), snippet style
(e.g., thumbnails) and SERP size. Below we provide a brief
overview of some of the research focused on SERP design
and SERP size. We also present research on screen size and
position effects, which are relevant to this work.

speculated the interface had promise in overcoming the position bias observed in [12] (see Subsection 2.4).
Finally, researchers investigating aggregated search have
studied SERP design [29, 1, 36]. Both Sushmita et al. [29]
and Arguello et al. [1] studied the differences between blended
and tabbed search result pages. Even these more complicated SERP pages, which blend results from multiple
sources, still tend to present about 10 results per page, primarily in list format. However, for specific verticals such as
images, the number of results shown per page is often larger
and grid formats are more common [20].

2.1

2.2

SERP Design

SERP Size

With regard to SERP size, or the number of results presented per page, there has been very little systematic research [23, 25, 19]. Although many experimental search
interfaces, in particular pre-Web interfaces, have presented
more than ten results per page, and many current search
interfaces allow users to configure the number of results per
page, to our knowledge, the size of the SERP has only been
explicitly studied as a variable in a few of studies. For example, Oulasvirta et al. [23] explored the idea that showing
fewer results may be preferable to showing more. In their experiment, they showed participants result lists with 6 results
or 24 results, as a way to investigate the so called, “paradox
of choice.” They found that participants rated their satisfaction, carefulness and confidence higher for the six results
per page condition. However, the study was performed using paper based mock-ups of screens, where there was no
pagination (i.e., 6 or 24 results were shown on one page), so
it is unclear how these findings generalize to on-screen desktop conditions, where typically only about 6-7 results are
viewed at one time. Also, it is not clear whether the effect
would be significantly different between 6 results per page,
and the standard 10 results per page, and how the results
would differ in an interactive search context.
In an industry report from Google, Linden [19] stated
feedback from Google users indicated they desired more than
ten results per page. However, it was observed that when
the number of results were increased to 30 per page, traffic dropped by 20%. The drop in traffic was attributed
to the increase in time to dispatch the page, since a page
with 10 results took approximately 0.4 seconds to generate,
while a page with 30 results took approximately 0.9 seconds.
This report is nearly ten years old so it is unclear if the results regarding user preference are still applicable to today’s
searchers and search environments.

A range of experiments have been conducted investigating
different snippet types, sizes and features, as well as layouts
and result combinations. Perhaps the most researched aspect of SERP design has been on the potential advantages
of visual snippets (e.g., thumbnails) over textual snippets [2,
13, 24, 31, 34]. For example, Woodruff et al. [34] examined
the usefulness of enhanced thumbnail images to represent
webpages. The enhanced thumbnail representation magnified salient features of webpages so that searchers could more
readily identify whether a page contained relevant material.
In comparison with textual summaries and standard thumbnails, it was found that the enhanced thumbnails reduced
search time. In more recent work, Teevan et al. [31] explored
a variation on generating enhanced thumbnails, referred to
as visual snippets, and also found that such an approach
decreased the time required to identify relevant documents.
TileBars [10] is another example of how researchers have
used visual information on the results page to help users
make better decisions about the relevance of the result represented by the snippet.
There have also been many studies on the content and size
of snippets [6, 8, 32]. For example, Tombros and Sanderson [32] investigated the potential usefulness of query-biased
summaries for snippets and found they improved both user
accuracy and speed of relevance judgements. Clarke et al. [6]
studied different aspects of result snippets which users found
attractive and desirable as determined by the number of
clicks the snippet received. Clarke et al.’s [6] study showed
that well formed snippets with title, summary and URLs
that were more readable, contained query terms and query
phrases were seen as more desirable.
Cutrell and Guan [8] explored the effect of different snippet lengths (short: 1 line, medium: 2-3 lines and long: 6-7
lines) using eye tracking data. For navigational queries, they
found that users examined more results as snippet length
increased, but performed slightly worse. They attributed
the differences to snippets being a distractor; specifically, as
snippets grew longer, users would often ignore the URL. For
informational queries, users performed somewhat better as
snippet length increased and looked at more results when
snippets were medium length.
Alternatives to the linear result list have also been explored, in particular, grid-based layouts [5, 15, 26]. For example, Krammerer and Beinhauer [15] examined differences
in user behavior when interacting with a standard list interface (single column of title, URL and snippet stacked vertically), a tabular interface (title, snippet and URL stacked
horizontally in 3 columns for each search result) and a grid
layout (search results placed in 3 columns). They found
users of the grid layout scrutinized the snippets more, and

2.3

Screen Sizes

While few studies have explicitly examined the impact
of SERP size on user interaction, this has been indirectly
examined in the context of screen size, since screen size impacts the number of results that are viewable. Jones et
al. [14] investigated the differences in search performance
when searchers used WAP based mobile phones which displayed 5 title snippets per page, PDA devices with 5 snippets per page and desktop search with 10 snippets per page.
They found participants using the devices showing 5 results
per page would on average examine 2-3 pages of results.
A similar experiment using more modern mobile phones
and PDAs was conducted by Sweeney and Crestani [30], but
the focus was on what length snippet to present to searchers
on the different devices. They note that because transmis-

184

sion costs were high (at the time), SERPs often contained
fewer results per page, thus increasing page to page navigation costs [14]. They varied the length of the snippet summary, but kept the number of results per page on each device
constant and set to 10. They hypothesized participants using the larger screen would examine more documents, but
found participants examined approximately the same number of documents regardless of the condition.
Jaewon et al. [18] compared the differences in people’s
interaction styles with two screen sizes (desktop vs mobile)
where SERP size was set to 10; on one interface all ten could
be seen, while on the other, only 3 could be seen. This latter
condition simulated the screen size of a mobile phone. Jaewon et al. [18] found for both informational and navigational
tasks, participants using the small screen took longer to click
on a link and had longer task completion times (though only
the task completion times were significantly different). They
also found participants scanned slightly fewer documents on
the small screen, but scrutinized each link for a longer duration, especially the first three links. On the smaller screen,
links 1-3 attracted more clicks for both informational and
navigational tasks: 88% and 91% versus 83% and 90%, respectively. This suggests when interacting with restricted
screen space, searchers focus more on the visible set of results and make more clicks on these results.

2.4

question is if these findings generalize to situations where
the screen and page sizes vary.

Position Bias and Click-through Rates

How users interact with the standard baseline SERP has
been informed by studies examining where users click on
the SERP [4, 7, 12, 16, 35]. In one of the first studies of
click bias, Joachims et al. [12] found users inspected 70%
down to 20% of result snippets in the top 6 (above the fold),
and then 10% down to 5% of results snippets at positions
7-10 (below-the-fold). Essentially, the probability of a click
decayed exponentially, while the probability of examining a
snippet decreased linearly (above-the-fold, and the flattened
out). Similar findings have been reported by others [4, 16,
21]. Joachims et al. [12] wondered whether the distribution of clicks could be used as an absolute indicator of relevance. However, they pointed out two problems in interpreting click data stemming from (i) trust bias, where users
trust the search engine to deliver the most relevant item first,
i.e., following the probability ranking principle [27], and (ii)
quality bias, where the behavior depends on the quality of
the retrieval system. They concluded users are more likely
to click on highly ranked documents and that quality influences click behavior, such that if the relevance of the items
retrieved decreases, users click on items that are less relevant, on average.
Craswell et al. [7] noted this as a fundamental problem
with click data, and referred to the problem as position bias,
where in the top 10 result lists, the probability of a click decreases with the rank of the document. They put forward
several hypotheses for why position bias is observed. From
their analysis, they showed a cascade model, which assumed
the user would click a result snippet with some probability
(ps ) or skip it (1 − ps ), fit click behavior at earlier ranks,
and a baseline model, which assumed the probability a user
would click a result snippet was proportional to its relevance,
fitted click behavior at later ranks. Of note is that the click
distributions that have been observed are based on interaction with SERPs that display ten results per page. An open

Figure 1: A standard search interface that has a
query box and search button, houses n results per
page along with pagination buttons. The number of
results returned is also shown.

3.

METHOD

We conducted a between-subject experiment where subjects were randomly assigned to one of three interfaces that
differed according to how many results were displayed per
search result page. Figure 1 displays the basic design of the
interfaces. One interface functioned as the baseline and displayed 10 results per page (10RPP). The other two interfaces
displayed 3 and 6 results per page (3RPP and 6RPP, respectively). For the 3RPP and 6RPP interfaces, all search results were displayed above-the-fold (i.e., no scrolling was required). For the interface displaying 10RPP, the first six results were visible above-the-fold. Our decision to use 6RPP
was to make our work more comparable to past research.
Specifically, to Oulasvirta et al.’s [23] study where 6RPP
were evaluated through a simulation and to Joachims et
al. [12] who described click distributions for results abovethe-fold (those at ranks 1-6). Although we restrict our investigation to desktop search, we choose 3RPP as another
comparison point similar to Jaewon et al. [18].
As shown in Figure 1 all three interfaces had a query box
at the top and a search button. Below the search button was
information displaying the number of retrieved results and
the result surrogates. The surrogates displayed the title of
the result, metadata about the result and a short summary.
At the bottom of the SERP were buttons that allowed subjects to move forward and backward through the set of result
pages. All subjects used the same desktop computer when

185

Factors
Mental
Demand
Physical
Demand
Temporal
Demand
Effort
Performance
Frustration
Level

System
How mentally demanding was it to use this
system to complete the search tasks?
How physically demanding was it to use this
system to complete the search tasks?
How hurried or rushed did you feel when using
this system to complete the search tasks?
How hard did you have to work to accomplish
your level of performance with this system?
How successful were you using this system to
complete the search tasks?
How insecure, discouraged, etc. were you while
using this system?

Navigate Results
navigate through the search results?
navigate through the search results?
navigating through the search
results?
navigate through the search results?
was your navigation through
the search results?
navigating through the search
results?

Assess Results
assess and judge documents for
relevance?
assess and judge documents for
relevance?
assessing and judging documents for relevance?
assess and judge documents for
relevance?
were you at assessing and judging documents for relevance?
assessing and judging documents for relevance?

Table 1: Modified NASA TLX factor definitions for overall system load, navigation load and assessment load.

3.2

completing the study, which was in our laboratory and under
our control. Subjects used a 19 inch monitor and all aspects
of the display, including font size and resolution, were held
constant during the study.

3.1

Search Behaviors

Search behavior was operationalized with three types of
measures: (1) interaction; (2) performance and (3) time
spent doing different search activities. All of these measures
were computed from log data and TREC q-rels. Interaction measures included: number of queries issued, number
of SERPs viewed per query, number of documents viewed,
number of documents viewed per query and deepest SERP
click. Performance measures included: number of documents marked relevant, number of TREC-relevant documents marked relevant and precision at 3, 5 and 10 documents. Time-based measures included: time spent issuing
queries, time spent examining SERPs per query and time
spent viewing documents.

Corpus, Search Topics and System

The Aquaint TREC test collection of over one million
newspaper articles was used [33]. We selected three search
topics from this collection: 344 (Abuses of E-mail); 347
(Wildlife Extinction) and 435 (Curbing Population Growth).
We selected topics that had some contemporary relevance,
we thought would be of interest to our target subjects and
had a similar number of relevant documents available (123,
165 and 152, respectively). Our selection was also based
on evidence from previous user studies with a similar system setup [17] where it was shown that the difficulty of these
topics did not significantly differ. Subjects searched all three
topics and topics were rotated with a Latin-square.
To situate the search tasks, subjects were instructed to
imagine they were newspaper reporters and needed to gather
documents to write stories about the provided topics. Subjects were told that there were over 100 relevant documents
in the collection for each topic and they should try to find as
many of these as possible during the allotted time (10 minutes per topic). While recall-oriented searches are not as
common in the general Web population as searches where
users are looking for one or a small number of items, these
types of searches are still performed by many people, in particular, professionals such as newspaper reporters, patent
searchers and business analysts. Because we were interested
in examining traditional performance measures, we choose
to use a test collection. Since the collection contained newspaper articles, we created a work task situation representing
one type of user model (i.e., journalist) and one type of task
model (i.e., find as many relevant documents as you can to
write a story) that was appropriate given the collection and
to which we thought our target participants (i.e., undergraduate students) could relate.
The Whoosh IR Toolkit was used as the core of the retrieval system, with BM25 as the retrieval algorithm, using
standard parameters, but with an implicit ANDing of query
terms to restrict the set of retrieved documents to only those
that contain all the query terms (similar to BM25A used in
[3]). This was chosen because most systems, such as web
search engines and library catalog systems, implicitly AND
terms together. However, subjects could also explicitly use
OR, AND, or NOT in their queries. Subjects were not provided with a tutorial of the system.

3.3

User Experience

To capture user experience, subjects evaluated the search
tasks before each search, the system after each search and
their experienced workload after completing all search tasks.
Task evaluations were elicited via pre-search questionnaire,
which contained five items: (1) How much do you know
about this topic? (1=nothing; 5=I know details); (2) How
relevant is this topic to your life? (1=not at all; 5=very
much); (3) How interested are you to learn more about this
topic? (1=not at all; 5=very much); (4) How often have you
searched for information related to this topic? (1=never;
5=very often) and (5) How difficult do you think it will be
to search for information about this topic? (1=very easy;
5=very difficult).
System evaluations were elicited via post-search questionnaire, which also contained five items: (1) How difficult was
it to find relevant documents? (1=very easy; 5=very difficult); (2) How would you rate your skill and ability at
finding relevant documents? (1=not good; 5=very good);
(3) How would you rate the system’s abilities at retrieving
relevant documents? (1=not good; 5=very good); (4) How
successful was your search? (1=unsuccessful; 5=successful)
and (5) How many of the relevant documents do you think
you found? (1=a few of them; 5=all of them).
Experienced workload was elicited using a modified version of the NASA Task Load Index (TLX). This instrument
elicited ratings of the following: Mental Demand, Physical Demand, Temporal Demand, Performance, Effort and
Frustration (Table 1). We modified the TLX statements so
they matched the target task (search) and asked subjects to
make their evaluations with a 7-point scale. Subjects also
completed two other workload questionnaires, which were fo-

186

Interface
3RPP
10.86 (8.89)
3.56 (2.35)
20.06 (9.09)
3.67 (3.44)
7.43 (7.34)

Queries
SERP views
Doc views
Docs/query
SERP depth

6RPP
10.53 ( 6.04)
2.66 ( 3.52)
15.67 ( 7.12)
3.36 ( 5.91)
10.40 (22.24)

10RPP
9.58 ( 6.89)
1.92 ( 0.98)
24.64 (13.25)
5.66 ( 7.08)
10.23 ( 8.78)

F
0.38
4.24*
8.18**
1.95
0.54

Topic
344
15.44 (8.44)
1.80 (0.86)
14.92 (7.63)
1.67 (2.02)
3.84 (3.72)

347
7.64
3.79
25.11
6.53
15.76

( 5.85)
( 3.75)
(13.07)
( 6.70)
(22.14)

435
7.89
2.55
20.33
4.50
8.45

(4.24)
(1.83)
(8.23)
(6.24)
(7.26)

F
16.78**
6.34*
10.58**
7.42**
7.06**

Table 2: Mean (sd) search interactions & F-statistics for interface & topic (df=2, 107) *p<0.01; **p<0.001

Marked rel
TREC rel
P@3
P@5
P@10

Interface
3RPP
11.08 (8.27)
4.25 (4.00)
0.184 (0.180)
0.160 (0.155)
0.140 (0.130)

6RPP
8.28 (5.88)
3.92 (3.21)
0.200 (0.161)
0.156 (0.129)
0.142 (0.128)

10RPP
16.08 (12.05)
6.17 (4.48)
0.216 (0.198)
0.194 (0.143)
0.163 (0.116)

F
7.70**
3.45
0.28
0.80
0.41

Topic
344
7.44 (6.25)
3.67 (4.23)
0.148 (0.173)
0.112 (0.126)
0.095 (0.108)

347
16.03 (12.12)
5.14 (4.38)
0.220 (0.193)
0.193 (0.168)
0.156 (0.140)

435
11.97 (7.51)
5.53 (3.23)
0.234 (0.164)
0.206 (0.113)
0.193 (1.03)

F
9.08**
2.25
2.36
4.87*
6.15*

Table 3: Mean (sd) performance & F-statistics for to interface & topic (df=2, 107) *p<0.01; **p<0.001
Time
Query
SERP
Page

Interface
3RPP
12.21 (10.72)
40.20 (27.17)
300.97 (117.69)

6RPP
15.25 (1.24)
44.28 (57.19)
259.81 (73.01)

10RPP
12.93 (1.08)
56.77 (72.92)
261.42 (97.91)

F
2.35
0.88
2.10

Topic
344
13.74 (1.08)
33.11 (55.79)
236.22 (97.94)

347
14.01 (1.08)
60.86 (59.64)
291.31 (100.21)

435
12.64 (1.08)
47.29 (49.14)
294.67 (89.65)

F
0.49
2.27
4.16

Table 4: Mean (sd) time and F-statistics according to interface and topic (df=2, 107). Query Time is the
time per query, SERP time is the time spent on the SERP per query, and Page time is the total amount of
time spent viewing and assessing documents across the session.

4.1

cused on the workloads associated with navigating the search
results and assessing the relevance of results. We included
these two additional scales to isolate any difficulties arising
from the varying number of results shown per page.

3.4

Subjects

Subjects were recruited by sending an email solicitation
to undergraduate students at a large research university.
Thirty-six undergraduate students participated (12 per interface). Twenty-six subjects were female and ten were
male. Their average age was 20.4 (SD=2.6). Sixty-seven
percent were science majors, 14% were social science majors
and 19% were humanities majors. Each subject was compensated with $15 USD and could earn an extra $5 USD per
topic for being one of the top three performers in his/her interface condition.
Subjects’ search experience was measured using a modified version of the Search Self-Efficacy scale [9]. This instrument contains 14-items describing different search-related
activities. Subjects respond to each item by indicating their
confidence in completing each activity on a 10-point scale
(1=totally unconfident; 10=totally confident). Reliability
analysis of these items demonstrated a high Cronbach’s alpha (0.934), so responses were averaged. Overall, subjects
reported fairly high search self-efficacy (M=7.51, SD=0.97).

4.

Search Behaviors

Interactions. Table 2 displays the average number of
queries submitted by subjects, SERP pages viewed per query,
total number of documents viewed, documents viewed per
query, and average SERP depth of the last document viewed
per query. Participants entered similar numbers of queries in
each interface, but they viewed significantly different numbers of SERP pages and documents. Specifically, those who
interacted with 3RPP viewed significantly more SERPs than
those who interacted with 10RPP and those who interacted
with 6RPP viewed significantly less documents than those
who interacted with 10RPP. Interestingly, the average number of SERPs viewed per query for those interacting with
3RPP (3.56) corresponded to about 10 results. Table 2 also
displays search interactions according to topic. Significant
differences were detected for all measures; in all cases the
means for Topic 344 significantly differed from those for
Topic 347, as well as from Topic 435 for queries and documents viewed. There were no significant interaction effects
between interface and topic.
Performance. Table 3 displays the number of documents
subjects marked relevant, the number of documents marked
relevant that were also TREC relevant and several precision
scores that were calculated using the TREC-relevant documents. Subjects who interacted with 10RPP performed
the best according to all performance measures, but only
significantly so with respect to number marked relevant.
Subjects’ performances differed significantly according to
topic for three measures: number marked relevant, P@5 and
P@10. Subjects marked significantly fewer documents as
relevant when completing Topic 344 than Topic 347. Their
P@5 scores were significantly worse for Topic 344 than the
other two topics and their P@10 scores were significantly

RESULTS

Both the search behavior and user experience measures
were analyzed according to interface and topic. To evaluate
these data, ANOVAs were conducted using interface and
topic as factors; both main effects and interaction effects
were examined with alpha=0.01. Bonferroni tests were used
for post-hoc analysis.

187

worse for Topic 344 than Topic 435. There were no significant interaction effects between interface and topic.

10RPP, Top, Middle and Bottom, respectively). The graphs
show the average probability of a user clicking on a result
snippet at rank position i. Subjects who interacted with
3RPP mainly clicked on the first 3 results (0.41 down to
0.33). A noticeable drop in the probability of a click is
observed when subjects went to page 2. Interestingly, the
probability stays around 0.22-0.23 for results at ranks 4-6,
before another step down in the probability of a click on
documents at ranks 7-12. After result 12, i.e., page 5 and
onwards, the probability becomes increasingly smaller. Subjects who interacted with 6RPP appeared to be less likely
to click on documents overall, but again we observe a small
step change after the first page (i.e., 6-12), and again after
the second page (i.e., 13-18). In contrast, subjects interacting with 10RPP, clicked with higher probability across most
of the ranks, and even after the first page of results (i.e., 1120), there is no noticeable drop in click probability. To determine if the click distributions were drawn from the same
underlying distribution a two-sample Kolmogorov-Smirnov
test was used. For 3RPP and 6RPP the distributions were
not significantly different (p=0.3420, k=0.2333), indicating
that they were drawn from the same underlying distribution.
However, the click distributions for 3RPP and 6RPP were
found to be significantly different from 10RPP (p=0.0046,
k=0.4333, and p=0.0006, k=0.5, respectively).

Time-Based Measures. Table 4 displays the amount
of time subjects spent performing different search activities:
issuing queries, viewing SERP pages per query and viewing
documents. Subjects who interacted with 3RPP spent the
most time viewing documents, those who interacted with
6RPP spent the most time issuing queries and those who
interacted with 10RPP spent the most time on SERPs per
query, none of these differences were statistically significant.
There were also no significant differences in the amount of
time spent engaged in different search activities according to
topic. Finally, there were no significant interaction effects
between interface and topic.

3RPP

0.4
0.3
0.2
0.1
0

5

10

15

20

25

4.2

30

Task Evaluations. Table 5 displays subjects’ task evaluations. With the exception of interest in the topic, subjects’
responses to these items differed significantly according to
topic. Subjects indicated they had significantly more topic
knowledge about Topic 435 (Curbing Population Growth)
than Topics 344 (Abuses of E-mail) and 347 (Wildlife Extinction), F(2, 107)=6.53, p=0.002 and had searched more
often in the past for information about this topic, F(2,107)=
13.44, p<0.0001. They expected it to be significantly more
difficult to search for information about Topic 344, than the
other two topics, F(2, 107)=8.79, p=0.0003. An analysis was
also conducted to see if subjects’ responses differed according to interface condition. Since subjects were randomly assigned to condition, we did not expect differences and none
were found. There were also no significant interaction effects.

6RPP

0.4
0.3
0.2
0.1
0

5

10

15

20

25

30

10RPP

0.4
0.3

Topic ID
knowledge*
relevance
interesting
searched**
difficulty**

0.2
0.1
0

5

10
15
20
25
Probability of a Click at Rank i

User Experience

30

344
2.17 (1.03)
2.92 (1.27)
2.86 (1.10)
1.31 (0.67)
3.58 (0.77)

347
2.00 (0.93)
2.14 (0.99)
3.14 (0.93)
1.56 (0.74)
2.94 (0.75)

435
2.81 (1.04)
2.61 (1.15)
3.44 (1.05)
2.31 (1.09)
2.94 (0.72)

Table 5: Mean (sd) responses to task evaluation
items. *p<0.01; **p<0.0001
Figure 2: The probability of clicking on a snippet (yaxis) for each rank i (x − axis). Top: 3RPP, Middle:
6RPP and Bottom: 10 RPP. The distributions for
3RPP and 6RPP were significantly different to the
distribution on the 10RPP interface.

System Evaluations. Table 6 displays subjects’ system evaluations. For all items, subjects’ responses to the
items for Topic 344 were significantly different than their
responses to the items for the other two topics (note: one
subject skipped this questionnaire for Topic 344). This topic
was rated as significantly more difficult, F(2, 106)=20.01,
p<0.0001, and subjects rated their own skill at finding rele-

Click Distributions. Figure 2 shows the distribution
of clicks on each of the three interfaces (3RPP, 6RPP and

188

Topic ID
difficulty*
skill*
system*
success*
number found*

344
4.03 (0.92)
2.43 (0.95)
2.40 (1.14)
2.46 (1.12)
2.09 (1.04)

347
2.50 (0.97)
3.42 (0.81)
3.56 (0.81)
3.64 (0.68)
3.00 (0.89)

7

435
3.03 (1.18)
3.08 (0.77)
3.33 (0.76)
3.39 (0.87)
2.67 (0.79)

3RPP

6RPP

10RPP

6
5
4
3
2
1

Table 6: Mean (sd) responses to system evaluation
items according to topic. *p<0.0001

0

mental

physical

temporal performance

effort

frustration

overall

mental

physical

temporal performance

effort

frustration

overall

mental

physical

temporal performance

effort

frustration

overall

7
6

Topic ID
difficulty
skill
system
success
number found

3RPP
2.92 (1.25)
3.08 (0.98)
3.25 (0.91)
3.33 (1.04)
2.69 (1.04)

6RPP
3.14 (1.24)
3.03 (0.89)
3.03 (1.20)
2.91 (1.04)
2.51 (1.01)

5

10RPP
3.47 (1.08)
2.83 (0.94)
3.03 (1.00)
3.25 (1.00)
2.56 (0.91)

4
3
2
1
0
7

Table 7: Mean (sd) responses to system evaluation
items according to interface.

6
5
4
3

vant documents significantly lower, F(2,106)=12.56, p<0.0001,
as well as the system’s ability at retrieving relevant documents, F(2,106)=12.29, p<0.0001. They also felt they were
less successful, F(2,106)=13.71, p<0.0001, and found fewer
of the relevant documents, F(2,106)=7.58, p<0.0002.
An analysis was also conducted to see if subjects’ responses differed according to interface condition (Table 7).
Interestingly, subjects who interacted with 10RPP reported
the greatest difficulty finding relevant documents and rated
their skills lowest; while those who interacted with 3RPP reported the least difficulty. Those who interacted with 6RPP
reported less successful searches. However, no significant differences were found according to interface condition. There
were also no significant interaction effects between condition
and topic. Taken together, these results suggest subjects had
a different search experience with Topic 344 regardless of interface condition, and interface condition did not uniformly
impact their system evaluations.
Workload. Figure 3 displays the system, navigation and
assessment workloads reported by subjects. With respect to
system load, subjects who interacted with 10RPP reported
the highest values for all items, with the exception of physical demand. The difference between 10RPP and 3 and 6
RPPs is especially pronounced for mental demand. The
same general trends were observed for the navigation and assessment loads, with the exception of performance. Despite
these differences, no significant differences were detected.

5.

DISCUSSION

This purpose of this paper was to investigate the impact
of SERP size on search behavior and user experience. We
found that subjects who interacted with 3RPP viewed significantly more SERPs per query, which is perhaps to be
expected, as subjects needed to paginate more to see more
results. Interestingly, the average number of SERPs they
examined per query corresponded to about ten results, so
perhaps people have been conditioned to view ten results,
even if viewing ten results requires expending a bit more
effort through pagination. This finding is consistent with

189

2
1
0

Figure 3: Top: System Load, Middle: Navigation
Load, Bottom: Assessment Load

the findings of Jones et al. [14] who found that participants
who used a device that displayed five results per page examined about 2-3 pages of results. However, in our study, those
who interacted with 6RPP and 10RPP viewed 2.66 and 1.92
SERPs per query, respectively, which corresponds to about
20 results per query, so there might be an interaction between the number of results subjects desire to see (whether
consciously or subconsciously) and the effort required for
pagination.
Subjects who interacted with 3RPP examined fewer documents, but seemed to compensate by spending more time
on each document they examined. This is interesting because it suggests that the additional cost and effort of paging
through results led to deeper inspection of the documents,
which is consistent with Jaewon et al.’s [18] results. From
the click distributions, it is interesting to note that the results on the first page were treated more or less equally, but
for each subsequent page, fewer and fewer subjects visited
them. It might be that with smaller sets of results, subjects
treat them more independently or treat them as a batch.
Subjects who interacted with 6RPP examined significantly
fewer documents, but spent more time formulating queries
(although not significantly so). This suggests that subjects
might have seen enough results to determine their queries
were unsuccessful, and thus spent more time querying. It is
interesting to consider how the SERP functions as feedback
to the user about the quality of his or her query and how
the provision of a smaller SERP size might disrupt or change
this function.
We found those who interacted with 10RPP viewed significantly more documents than those who interacted with

6RPP. This is in contrast to Sweeney and Crestani’s [30]
findings where there was no difference in the number of documents examined according to the number of results viewable on the device. Subjects who interacted with 10RPP
also marked significantly more documents as relevant. This
might be because they had cheaper access to more documents compared to subjects in the other conditions. Interestingly though, there were no significant differences in the
amount of time subjects in the different conditions spent examining SERP pages or documents. Given that each SERP
in the 10RPP condition displayed more snippets, one would
have expected this difference to be significant. Seeing more
results per page might have allowed those in the 10RPP
to more quickly identify or distinguish relevant documents
from non-relevant documents, or since there were no significant differences in the number of TREC relevant saved,
they might have been more liberal with their assessments.
Those who interacted with 3RPP likely faced an additional
memory burden of having to internally keep track of what
they saw on different pages and make comparisons of the
information, while those who interacted with 10RPP likely
had fewer things to keep track of internally.
A key finding from this research is that the distribution of
clicks changes with SERP size. Specifically, the click distributions for those interacting with 3RPP and 6RPP differed
significantly from those who interacted with 10RPP. Essentially these participants gave more time and attention to
the top results, which may have been a way to reduce the
potential memory load described above. When the SERP
size was small, fewer snippets were inspected and more time
was spent on the top documents. These findings suggest
that precision-oriented search algorithms will be of increasing importance as the page size decreases. While we did
not test interfaces that display one or two results per page,
our findings suggest that users are likely to be even more
focused on the top documents in these situations. Further
work is need to examine how screen size (i.e. the number of
results viewable) and the page size (the number of results
per page) interact with one another and how this is related
to the user’s task and search goals.
Results regarding user experience were not as revealing,
but some interesting, although not significant, trends were
observed. Most notably, subjects who interacted with 10RPP
reported the greatest difficulty finding relevant documents,
rated their skills the lowest and reported greater workload.
When considered in light of Oulasvirta et al.’s [23] work
these findings suggest that smaller SERP sizes might have
positive psychological consequences for users since results
are chunked in smaller sets. Oulasvirta et al. [23] work was
motivated by the idea of the “paradox of choice” and participants in their study rated their satisfaction, carefulness and
confidence higher when interacting with 6 results as opposed
to 10. The potential psychological benefits of alternative
SERP sizes is an interesting avenue for future research.
Although we tried to select search topics that would not
differ according to difficulty, we found that one topic (344)
was much more difficult than the other two. Not only did
subjects expect this topic to be more difficult, they also described it as more difficult after they searched. They exhibited significantly different search behaviors for this topic and
performed significantly worse according to most measures.
This topic was about abuses of email at the workplace. Our
subjects were likely familiar with the general topic of email,

but since they were all undergraduates, their knowledge and
familiarity with the workplace was likely impoverished which
might explain why this topic was rated differently in the
pre-search task evaluations. The differences in the system
evaluations for this topic might be a result of the challenges
of searching for information about email, which can be expressed in a variety of ways (e.g., email, e-mail, electronic
mail). While we did not find a significant interaction effect between topic and interface condition, future studies
might explore this more systematically with a greater range
of topics from different points along the difficulty spectrum.
The additional clicking required by the interface with 3RPP
might intensify people’s feelings of stress if they are experiencing difficulty. Alternatively, fewer results per page might
help the person stay more focused and in control.

6.

CONCLUSION

In this paper, we have studied how the number of results per page impacts search behavior and search experience
in the context of ad-hoc topic retrieval. We created three
search interfaces that varied according to the number of results per SERP (three, six or ten) and conducted a betweensubjects user-centered evaluation. Our major finding was
that subjects’ click distributions differed significantly depending on SERP size. Specifically, those interacting with
3RPP and 6RPP spent more time focused on top-ranked
results and those interacting with 10RPP. This result has
implications for evaluation. Many evaluation measures assume click distributions that neatly decay following an exponential distribution [11, 22, 28]. However, this might be
an artifact of the SERP size modeled, and prompts a question as to whether such measures generalize well to SERPs
of different sizes. Further work will need to be conducted to
determine the impact of different SERP sizes on the distribution of clicks. This is of particular interest as the types
of devices used for search are changing from desktop to mobile and wearables devices (such as tablets, mobiles, watches
and glasses), where the number of results displayed and the
number of results viewable vary.
Furthermore, while we have empirically approached the
problem, it would be of interest to formally model how users
interact with different screen and page configurations, in order to find optimal or ideal settings for these parameters.
Creating a formal model would provide researchers and practitioners with a compact representation of how interaction
costs, behaviors and performance are affected when SERP
size and display size changes. Such an analytical tool would
help in reasoning about the optimal SERP size for different
devices and display sizes, as well as other aspects of search
context such as type of task and user.
Finally, we noted that different SERP sizes appear to have
different psychological effects which may lead to a positive
or negative search experience. While none of our user experience measures were significant, the number of users in
our study was quite low. Nor did we ask the right questions
or study the right constructs to really get at this issue in
detail, so further work on this front is also required.

7.

ACKNOWLEDGEMENT

We would like to thank to Kathy Brennan from UNC for
help with data collection.

190

8.

REFERENCES

[1] J. Arguello, W.-C. Wu, D. Kelly, and A. Edwards.
Task complexity, vertical display and user interaction
in aggregated search. In Proceedings of the 35th
International ACM SIGIR Conference, SIGIR ’12,
pages 435–444, 2012.
[2] A. Aula, R. M. Khan, Z. Guan, P. Fontes, and
P. Hong. A comparison of visual and textual page
previews in judging the helpfulness of web pages. In
Proceedings of the 19th International WWW
Conference, pages 51–60, 2010.
[3] L. Azzopardi, D. Kelly, and K. Brennan. How query
cost affects search behavior. In Proc. of the 36th
International ACM SIGIR Conference, pages 23–32,
2013.
[4] J. Bar-Ilan, K. Keenoy, M. Levene, and E. Yaari.
Presentation bias is significant in determining user
preference for search results: A user study. J. of the
Am. Soc. for Info. Sci. and Tech., 60(1):135–149, 2009.
[5] F. Chierichetti, R. Kumar, and P. Raghavan.
Optimizing two-dimensional search results
presentation. In Proc. of the 4th Int. ACM WSDM
Conference, pages 257–266, 2011.
[6] C. L. A. Clarke, E. Agichtein, S. Dumais, and R. W.
White. The influence of caption features on
clickthrough patterns in web search. In Proc. of the
30th Annual International ACM SIGIR Conference,
pages 135–142, 2007.
[7] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An
experimental comparison of click position-bias models.
In Proceedings of the 2008 International Conference
on Web Search and Data Mining, WSDM ’08, pages
87–94, 2008.
[8] E. Cutrell and Z. Guan. What are you looking for?:
an eye-tracking study of information usage in web
search. In Proc. of the SIGCHI conference, pages
407–416, 2007.
[9] S. Debowski, R. Wood, and A. Bandura. The impact
of guided exploration & enactive exploration on
self-regulatory mechanisms & information acquisition
through electronic enquiry. J. of Applied Psych.,
86:1129–1141, 2001.
[10] M. A. Hearst. Tilebars: Visualization of term
distribution information in full text information
access. In Proceedings of the SIGCHI Conference,
pages 59–66, 1995.
[11] K. Järvelin and J. Kekäläinen. Cumulated gain-based
evaluation of ir techniques. ACM Trans. Inf. Syst.,
20(4):422–446, Oct. 2002.
[12] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and
G. Gay. Accurately interpreting clickthrough data as
implicit feedback. In Proceedings of the 28th
International ACM SIGIR Conference, pages 154–161,
2005.
[13] H. Joho and J. M. Jose. A comparative study of the
effectiveness of search result presentation on the web.
In Proceedings of the 28th European Conference on
Information Retrieval, pages 302–313, 2006.
[14] M. Jones, G. Marsden, N. Mohd-Nasir, K. Boone, and
G. Buchanan. Improving web interaction on small
displays. In Proceedings of the Eighth International
Conference on World Wide Web, WWW ’99, pages

[15]

[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

[27]
[28]

[29]

191

1129–1137, New York, NY, USA, 1999. Elsevier
North-Holland, Inc.
Y. Kammerer and P. Gerjets. How the interface design
influences users’ spontaneous trustworthiness
evaluations of web search results: comparing a list and
a grid interface. In Proc. of the Symp. on Eye-Tracking
Research & Applications, pages 299–306, 2010.
M. T. Keane, M. O’Brien, and B. Smyth. Are people
biased in their use of search engines? Communications
of the ACM, 51(2):49–52, 2008.
D. Kelly, K. Gyllstrom, and E. W. Bailey. A
comparison of query and term suggestion features for
interactive searching. In Proceedings of the 32nd ACM
SIGIR conference, pages 371–378, 2009.
J. Kim, P. Thomas, R. Sankaranarayana, T. Gedeon,
and H.-J. Yoon. Eye-tracking analysis of user behavior
and performance in web search on large and small
screens. J. of the Assoc. for Information Science and
Technology, 2014.
G. Linden. Marissa mayer at web 2.0, November 2006.
http: // glinden. blogspot. com/ 2006/ 11/
marissa-mayer-at-web-20. html .
H. Liu, X. Xie, X. Tang, Z.-W. Li, and W.-Y. Ma.
Effective browsing of web image search results. In
Proceedings of the 6th ACM SIGMM International
Workshop on Multimedia Information Retrieval, pages
84–90, 2004.
L. Lorigo, B. Pan, H. Hembrooke, T. Joachims,
L. Granka, and G. Gay. The influence of task and
gender on search and evaluation behavior using
google. Information Processing & Management,
42(4):1123–1131, 2006.
A. Moffat and J. Zobel. Rank-biased precision for
measurement of retrieval effectiveness. ACM Trans. on
Information Systems, 27(1):2:1–2:27, 2008.
A. Oulasvirta, J. P. Hukkinen, and B. Schwartz. When
more is less: The paradox of choice in search engine
use. In Proceedings of the 32nd International ACM
SIGIR Conference, pages 516–523, 2009.
T. Paek, S. Dumais, and R. Logan. Wavelens: A new
view onto internet search results. In Proceedings of the
SIGCHI Conference, pages 727–734, 2004.
H. Reiterer, G. Tullius, and T. M. Mann. Insyder: a
content-based visual-information-seeking system for
the web. Int. Journal on Digital Libraries, 5(1):25–41,
2005.
M. L. Resnick, C. Maldonado, J. M. Santos, and
R. Lergier. Modeling on-line search behavior using
alternative output structures. In Proc. of the Human
Factors and Ergonomics Soc. Annual Meeting,
volume 45, pages 1166–1170, 2001.
S. E. Robertson. The probability ranking principle in
ir. Journal of documentation, 33(4):294–304, 1977.
M. D. Smucker and C. L. Clarke. Time-based
calibration of effectiveness measures. In Proceedings of
the 35th ACM SIGIR conference, pages 95–104, 2012.
S. Sushmita, H. Joho, M. Lalmas, and R. Villa.
Factors affecting click-through behavior in aggregated
search interfaces. In Proceedings of the 19th
International ACM CIKM Conference, CIKM ’10,
pages 519–528, 2010.

[30] S. Sweeney and F. Crestani. Effective search results
summary size and device screen size: Is there a
relationship? IPM, 42(4):1056–1074, 2006.
[31] J. Teevan, E. Cutrell, D. Fisher, S. M. Drucker,
G. Ramos, P. André, and C. Hu. Visual snippets:
Summarizing web pages for search and revisitation. In
Proceedings of the SIGCHI Conference, pages
2023–2032, 2009.
[32] A. Tombros and M. Sanderson. Advantages of query
biased summaries in information retrieval. In Proc. of
the 21st International ACM SIGIR Conference, pages
2–10, 1998.
[33] E. M. Voorhees. Overview of the trec 2005 robust
retrieval track. In Proceedings of TREC-14, 2006.

[34] A. Woodruff, R. Rosenholtz, J. B. Morrison,
A. Faulring, and P. Pirolli. A comparison of the use of
text summaries, plain thumbnails, and enhanced
thumbnails for web search tasks. J. Am. Soc. Inf. Sci.
Technol., 53(2):172–185, 2002.
[35] Y. Yue, R. Patel, and H. Roehrig. Beyond position
bias: Examining result attractiveness as a source of
presentation bias in clickthrough data. In Proc. of the
19th Int. Conference on World Wide Web, pages
1011–1018, 2010.
[36] K. Zhou, R. Cummins, M. Lalmas, and J. M. Jose.
Evaluating aggregated search pages. In Proceedings of
the 35th International ACM SIGIR Conference,
SIGIR ’12, pages 115–124, 2012.

192

