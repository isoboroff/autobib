Document Comprehensiveness and User Preferences
in Novelty Search Tasks
Ashraf Bah

Praveen Chandar

Ben Carterette

University of Delaware
Newark, Delaware, USA

University of Delaware
Newark, Delaware, USA

University of Delaware
Newark, Delaware, USA

ashraf@udel.edu

pcr@del.edu

carteret@udel.edu

Popular evaluation metrics such as precision and recall do not
account for intents and subtopics. Both assume binary relevance.

ABSTRACT
Different users may be attempting to satisfy different information
needs while providing the same query to a search engine.
Addressing that issue is addressing Novelty and Diversity in
information retrieval. Novelty and Diversity search models the
retrieval task wherein users are interested in seeing documents
that are not only relevant, but also cover more aspects (or
subtopics) related to the topic of interest. This is in contrast with
the traditional IR task in which topical relevance is the only factor
in evaluating search results. In this paper, we conduct a user study
where users are asked to give a preference between one of two
documents B and C given a query and also given that they have
already seen a document A. We then test a total of ten hypotheses
pertaining to the relationship between the “comprehensiveness” of
documents (i.e. the number of subtopics a document is relevant to)
and real users’ preference judgments. Our results show that users
are inclined to prefer documents with higher comprehensiveness,
even when the prior document A already covers more aspects than
the two documents being compared, and even when the less
preferred document has a higher relevance grade. In fact, users are
inclined to prefer documents with higher overall aspect-coverage
even in cases where B and C are relevant to the same number of
novel subtopics.

Newer metrics go beyond simple binary relevance, and use graded
relevance instead. The shortcoming of binary relevance addressed
by graded relevance is that some documents are simply more
useful to users than others. Two of the most widely used metrics
that make use of graded judgments are nDCG [10] and ERR [7].
What these two metrics do not account for, however, are different
user intents and subtopics.
The simplest metric that accounts for different subtopics is
subtopic recall [12]. Perhaps the more widely used measures in
that category are α-nDCG [9] and ERR-IA [6]. These measures
produce specific hypotheses about user preferences, and their
recapitulative idea is that a user is always interested in seeing
more novel subtopics, with decreasing interest in seeing redundant
subtopics. Although this idea is sound, we argue that there are
other factors that impact users’ preferences. Specifically, we adopt
an experimental design described by Chandar and Carterette [4] to
show that users are interested in seeing more subtopics, but also
that users are biased towards more “comprehensive” documents—
that is, they prefer documents that cover the most subtopics
regardless of how novel the subtopics are.

2. PREFERENCE JUDGMENTS

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]
Keywords: diversity; preference judgment; user study

Absolute judgments in IR such as Boolean relevance and graded
relevance have been used widely in the literature. An alternative is
pairwise preference judgments, in which an assessor is presented
with two documents and gives a preference to one document over
the other. Early work on preference judgments in IR involved
inferring preferences from those absolute judgments [2]. However
more recently, pairwise (i.e. binary) preference judgments have
been adopted as an alternative that may offer advantages in terms
of alleviating the burden on assessors by reducing the complexity
of the assessment, rendering the assessment task easier than
assigning grades and reducing assessor disagreement [3].

1. INTRODUCTION
In the recent past, more and more researchers in information
retrieval (IR) evaluation have directed their attention to evaluation
measures that account for both redundancy and ambiguity in
search. Novelty aims at dealing with redundancy in search results,
while diversity aims at handling ambiguity in queries. Research in
that direction has seen such an interest that two different IR
evaluation conferences have organized diversity retrieval tasks.
The Text Retrieval Conference (TREC) included a diversity
retrieval task as part of the Web track between 2009 and 2012 [8].
Unlike its ad-hoc counterpart in the same track, the diversity task
judged documents based on subtopics as well as to the topic as a
whole. In the same manner, the NTCIR11-IMine task incorporates
a subtask focused on diversified document retrieval for which
different user intents must be taken into account [11].

The preference judgment scheme we adopt here, however, is a bit
different and was first proposed by Chandar et al. [4]. This
scheme is based on the so-called triplet framework, wherein given
a query and a document A that the assessor is to pretend contains
everything they know about the topics, the assessor chooses the
next document they would like to see between two documents.

2.1 Data
For our experiments, we use 10 queries from the TREC 2012 Web
track dataset [8]. The 10 queries selected are a sample of broad
queries (i.e. queries good for intrinsic diversity tasks). Documents
were selected for preference assessment from those judged for

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than the author(s) must be honored. Abstracting
with credit is permitted. To copy otherwise, or republish, to post on servers or
to redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from Permissions@acm.org.
SIGIR '15, August 09 - 13, 2015, Santiago, Chile
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3621-5/15/08…$15.00
DOI: http://dx.doi.org/10.1145/2766462.2767820

735

Figure 1. Screenshot of the HIT Layout
design, the quality control decisions and the HIT properties were
the same as described by Carterette and Chandar [5]. Figure 1
shows an example of a triplet as it appeared in a HIT.

the Web track. All documents are therefore from the ClueWeb09
collection of millions of web pages. We use the publicly available
subtopic relevance judgments produced by experienced NIST
assessors and based on graded relevance. We use the publicly
available subtopic relevance judgments produced by experienced
NIST assessors and based on graded relevance. Since documents
were not judged for subtopic relevance, we consider the maximum
subtopic relevance grade of a document to be its topical relevance.
For each one of the 10 queries, we obtained triplets from several
users. In the next section, we describe the experimental design that
yielded the triplets.

3. HYPOTHESES AND RESULTS
In this section we enumerate some specific hypotheses about
relationships between document “comprehensiveness”, ad-hoc
relevance, and user preferences. Our goal is to test the degree to
which comprehensiveness (in the sense of covering more aspects
in relevance judgments) is more important to users than
relevance; in general, we hypothesize that it is the more
important of the two factors in their preferences.

2.2 Experimental Design

3.1 Comprehensiveness Hypotheses

The framework we adopt in our study for preference judgment is
based on the work of Chandar and Carterette [4]. In the
framework, an assessor (i.e. user) is shown three documents, one
appearing at the top of the page, one appearing at the bottom
left, and the third appearing at the bottom right. We will refer to
the top document as DT, and the bottom ones as D1 and D2, in
concordance with naming conventions of Chandar and
Carterette. Given DT and a query, the assessor is asked to choose
between D1 and D2. Essentially the assessor would need to
indicate their preference for the second document they would
like to see in a ranking by selecting either D1 or D2.

In the following, D1 > D2 means document D1 is preferred to
document D2. R1 > R2 means document D1 was judged by NIST
assessors to have a higher ad-hoc relevance grade than
document D2. S1 > S2 means document D1 contains more
subtopics than document D2. S1new > S2new means document D1
contains more novel subtopics (with respect to the subtopics
already seen in DT) than document D2. Regardless of whether a
document was placed on the left or right of a triplet, we will
refer to the more comprehensive one as D1.

3.1.1 Hypotheses Set 1 (H1 through H3).

Since we have graded topical and subtopic relevance judgments
for the documents, we can use that information to determine
what subtopics each document is relevant to, as well as the
corresponding relevance grades. A document can thus be
represented as the set of subtopics it has been judged relevant to.
For instance, Di = {Sj, Sk} means document i is relevant to
subtopics j and k.

This first set of hypotheses posits that a document D1 with
higher aspect coverage, in general, tends to be preferred by users
– regardless of whether D1 covers more novel aspects than the
document it is being compared to. The three hypotheses are:
H1: If S1 > S2 and R1 > R2 then D1 > D2. This means users prefer
a document with higher aspect coverage and higher ad-hoc
relevance grade than a document with lower aspect coverage
and lower ad-hoc relevance grade.

We used Amazon Mechanical Turk (AMT) [1]; an online labor
marketplace to collect user judgments. AMT works as follow: a
requestor creates a group of Human Intelligence Task (HITs)
with various constraints and workers from the marketplace work
to complete the tasks. Workers were instructed to assume that
everything they know about the topic is in the top document, and
they are now trying to find a document that would be most
useful for learning more about the topic. No mentions of
subtopics, novelty, or redundancy were given to them except as
examples of properties assessors might take into account in their
preferences (along with recency, ease of reading, and relevance).
Each preference triplet consists of three documents, all of which
were relevant to the topic; the documents were picked randomly
from the data described in Section 2.1. One document appeared
at the top followed by two documents below it. The HITs layout

H2: If S1 > S2 and R1 < R2 then D1 > D2. This means users prefer
a document with higher aspect coverage but lower ad-hoc
relevance grade than a document with lower aspect coverage but
higher ad-hoc relevance grade.
H3: If S1 > S2 and R1 = R2 then D1 > D2. This means that for
documents with equal ad-hoc relevance grade, users prefer a
document with higher aspect coverage than a document with
lower aspect coverage.
We expected H1 to be largely true, and H2 and H3 to be more
mitigated (or possibly inconclusive for H2) due to the fact that
relevance grades are an important factor as well. The results in

736

Table 1. Results for all the hypotheses, all results in the last column are significant (++) at p<0.01, except for H5 and H9
Q 152

Q 157

Q 158

Q 167

Q171

Q 173

Q178

Q 184

Q 196

Q199

All Q

H1 true/false

196/ 22

87/ 13

148/19

97/ 28

181/31

155/30

119/51

157/19

108/53

136/22

1384/288 (82.78% true)++

H2 true/false

0/0

0/0

4/1

10/2

5/0

0/0

1/1

4/4

0/0

6/1

30/9 (76.92% true)++

H3 true/false

10/15

0/0

33/13

18/6

12/1

6/11

14/11

21/14

18/15

12/14

144/90 (61.54% true)++

H4 true/false

40/3

20/2

49/14

31/6

42/15

82/21

37/17

56/8

59/32

41/10

457/128 (78.12% true)++

H5 true/false

0/0

0/0

0/0

0/0

0/0

0/0

1/1

2/2

0/0

6/1

9/4 (69.23% true)

H6 true/false

4/1

0/0

21/4

0/0

0/0

2/7

6/1

1/6

5/2

8/6

47/27 (63.51% true)++

H7 true/false

181/21

67/11

97/5

79/24

149/18

73/9

105/50

115/11

49/21

91/11

1006/181 (84.75% true)++

H8 true/false

0/0

0/0

4/2

10/10

5/0

0/0

1/1

2/2

0/0

0/0

22/7 (75.86% true)++

H9 true/false

10/5

0/0

9/24

18/6

12/1

4/4

13/17

16/8

13/13

4/8

99/86 (53.51% true)

H10 true/false

15/1

20/2

63/18

20/5

32/13

84/28

18/5

49/16

64/34

59/18

424/140 (75.18% true)++

All H’s T/F

293/68

194/28

400/100

283/87

306/79

406/110

315/155

423/90

316/170

363/91

3622/960++

The results in Table 1 support the claims made by Hypotheses
H4 through H6. That is, even when the prior document DT has
higher aspect coverage than each of D1 and D2, the document D1
with higher aspect coverage is preferred by users. And here
again, whether documents D1 and D2 have the same relevance
grade or not, it is the one with the highest aspect coverage that
gets selected by users as most preferred. Although, as we
expected, H5 and H6 are more mitigated than H4. H4 is very often
true (in 78.12% of qualifying user preferences): given DT with
higher aspect coverage than D1 and D2, the document D1 with
higher aspect coverage and higher ad-hoc relevance grade is
preferred. Also, H5 is often true (69.23% of qualifying user
preferences, but this is not significant): given DT with higher
aspect coverage than D1 and D2, the document D1 with higher
aspect coverage but lower ad-hoc relevance grade is preferred.
And finally, H6 is also often true (63.51% true and 36.49%
false): given DT with higher aspect coverage than D1 and D2, the
document D1 with higher aspect coverage but same ad-hoc
relevance grade as the other document, is preferred. However
the proportions in which H5 and H6 are true are not as strong as
that of H4, which suggests that the bias against the leastcomprehensive document is reduced in the cases of H5 and H6.

Table 1 support our hypotheses that, a document D1 with higher
aspect coverage, in general, tend to be preferred by users –
regardless of whether D1 covers more novel aspects than the
document D2 it is being compared to. In fact, even H2 and H3 are
true far more often than we expected them to be. According to
the results, when D1 covers more aspects than D2 and D1 also
has a higher ad-hoc relevance grade than D2, D1 was by far
preferred by users. In our experiment this happened 1384 times
(82.78%), and failed to happen 288 times (17.22%).
The results also confirm H3 which posits that, for documents
with equal ad-hoc relevance grade, users prefer the document
with higher aspect coverage. And this happened 144 times
(61.54%), and failed to happen 90 times (38.46%). H2 is also
true more often than not, i.e. 30 times (76.92%) against 9 times
(23.08%). The results, while proving H2 and H3 to be true, also
suggest that when the least-comprehensive document has a
higher relevance grade than the most-comprehensive document,
the bias against the least-comprehensive document is reduced.

3.1.2 Hypotheses Set 2 (H4 through H6).

The second set of hypotheses zooms into special cases where the
prior document DT (i.e. document shown at the top) has higher
aspect coverage than each of D1 and D2 and posits that, even
then, the document with higher aspect coverage is preferred by
users. The three hypotheses are:

3.1.3 Hypotheses Set 3 (H7 through H9).

This second set of hypotheses focuses on “novelcomprehensiveness”. Given a prior document DT, a document
D1 is more “novel-comprehensive” than D2 if D1 covers more
novel subtopics (with respect to the subtopics already seen in
DT). We posit that a document D1 with higher novel aspect
coverage, in general, tends to be preferred by users. The three
hypotheses are:

H4: If S1 > S2 | (ST > S1 and ST > S2) and R1 > R2 then D1 > D2.
This means, given the prior document has higher aspect
coverage than each of D1 and D2, users still prefer a document
with higher aspect coverage and higher ad-hoc relevance grade
than a document with lower aspect coverage and lower ad-hoc
relevance grade.

H7: If S1new > S2new and R1 > R2 then D1 > D2. This means users
prefer a document with higher novel-aspect coverage and higher
ad-hoc relevance grade than a document with lower novel-aspect
coverage and lower ad-hoc relevance grade.

H5: If S1 > S2 | (ST > S1 and ST > S2) and R1 < R2 then D1 > D2.
This means, given the prior document has higher aspect
coverage than each of D1 and D2, users prefer a document with
higher aspect coverage but lower ad-hoc relevance grade than a
document with lower aspect coverage but higher ad-hoc
relevance grade.

H8: If S1new > S2new and R1 < R2 then D1 > D2. This means users
prefer a document with higher novel-aspect coverage but lower
ad-hoc relevance grade than a document with lower novel-aspect
coverage but higher ad-hoc relevance grade.

H6: If S1 > S2 | (ST > S1 and ST > S2) and R1 = R2 then D1 > D2.
This means given the prior document has higher aspect coverage
than each of D1 and D2, for documents with equal ad-hoc
relevance grade, users prefer a document with higher aspect
coverage than a document with lower aspect coverage.

H9: If S1new > S2new and R1 = R2 then D1 > D2. This means that
for documents with equal ad-hoc relevance grade, users prefer a
document with higher novel-aspect coverage than a document
with lower novel-aspect coverage.

737

But is it the case that users prefer left docs to right docs (or vice
versa) even when the preferred document has lower aspect
coverage? That is, does the position of the document have an
effect on it being preferred by a user? The results in Figure 2
suggest that is not the case. In fact, the proportions in which the
hypotheses are true/false in both situations are relatively close.
The triplets were indeed placed randomly in either left or right,
that is, they are not placed according to any factor.

The results shown in Table 1 support hypotheses H7 through H9.
In fact, H7 is true in 1006 cases (84.75%), and fails 181 times
(15.25%). This means when D1 covers more novel aspects than
D2 and D1 also has a higher ad-hoc relevance grade than D2, D1
was by far preferred by users. H9 is true in 99 cases (53.51%),
and fails 86 times (46.49%). This means when D1 covers more
novel aspects than D2 but has same ad-hoc relevance grade as
D2, D1 was still preferred by users, but not significantly. Also,
H8 is true only slightly more often than it is false. It is true 22
times (75.86%), and false 7 times (24.14%). These results
suggest that when the least novel-comprehensive document has
less than or equal ad-hoc relevance grade as the most novelcomprehensive document, the bias towards the most novelcomprehensive document is reduced.

4. CONCLUSION AND FUTURE WORK
In this paper, we have used the triplet framework to empirically
show that users tend to prefer in large proportions documents
with high aspect coverage, regardless of the topical relevance
grade. We asked users to choose, given a prior document DT,
between two documents D1 and D2 the one that is most useful
for learning more about the topic. According to the results, users
overwhelmingly prefer documents that are relevant to the largest
number of aspects (i.e. highest aspect coverage), even when the
prior document DT already covers more subtopics than each of
D1 and D2. In fact, even in cases where D1 and D2 are relevant to
the same number of novel subtopics, the one that is relevant to
the largest overall subtopics tends to be preferred.

3.1.4 Hypothesis H10.
This final hypothesis puts an emphasis on cases where the two
documents being compared are equally “novel-comprehensive”
– i.e. cover the same number of new subtopics – and posits that
even in that case, users are more likely to prefer the one that
covers the most number of subtopics.
H10: If S1 > S2 | (S1new = S2new) then D1 > D2. This means users
are more likely to prefer a document that covers the most
number of subtopics, even when both documents contain the
same number of novel-aspects. In other words, users are biased
towards more comprehensive documents, even in cases where
both documents have the same number of novel-aspects.

ACKNOWLEDGMENTS

The result for this hypothesis is perhaps the most interesting
one. It shows that, even when the two documents being
compared are relevant to an equal number of novel aspects,
users are more inclined to choose the one with the highest
overall subtopic coverage. And this happens in 75.18% of cases.

REFERENCES

This material is based upon work supported by the National
Science Foundation under Grant No. IIS-1350799.
Any
opinions, findings, and conclusions or recommendations
expressed in this material are those of the authors and do not
necessarily reflect the views of the National Science Foundation.
[1] Amazon mechanical turk. http://www.mturk.com.
[2] Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M.,
Hamilton, N., Hullen-der, G.: Learning to rank using
gradient descent. In ICML. (2005)
[3] Carterette, B., Bennett, P. N., Chickering, D. M., &
Dumais, S. T.: Here or there. In ECIR. (2008).
[4] Chandar, P. & Carterette, B.: What Qualities Do Users
Prefer in Diversity Rankings? In Proc. WSDM Workshop
on Diversity in Document Retrieval (2012)
[5] Chandar, P., & Carterette, B.: Using preference judgments
for novel document retrieval. In SIGIR. (2012)
[6] Chapelle, O., Ji, S., Liao, C., Velipasaoglu, E., Lai, L., Wu,
S. L.: Intent-based diversification of web search results:
metrics and algorithms. IR14(6) (2011) 572-592
[7] Chapelle, O., Metlzer, D., Zhang, Y., …: Expected
reciprocal rank for graded relevance. In CIKM. (2009)
[8] Clarke, C. L., Craswell, N., & Soboroff, I.: Overview of the
trec 2012 web track. In TREC (2012).
[9] Clarke, C. L., Kolla, M., Cormack, G. V., Vechtomova, O.,
Ashkan, A., Büttcher, S., …: Novelty and diversity in
information retrieval evaluation. In SIGIR. (2008)
[10] Järvelin, K., & Kekäläinen, J.: Cumulated gain-based
evaluation of IR techniques. TOIS 20(4) (2002) 422-446
[11] Liu, Y., Song, R., Zhang, M., Dou, Z., Yamamoto, T., ….:
Overview of the ntcir-11 imine task. In NTCIR. (2014).
[12] Zhai, C. X., Cohen, W. W., & Lafferty, J. (2003, July).
Beyond independent relevance: methods and evaluation
metrics for subtopic retrieval. In SIGIR (2003)

It should be noted that there are very few cases (17 cases) where
the preferred document covers more aspects but fewer novelaspects; and even fewer cases (2 cases) where it contains more
novel-aspects but fewer aspects.

100%
80%
60%
40%
20%
0%

H False
H true

Figure 2. Comparison of proportions in which hypotheses
are true/false for three cases (considering all-pref, left-pref
only and right-pref only).
It is important to note that, in most cases, the document with
higher aspect coverage (i.e. more comprehensive) is either more
relevant or equally relevant to the other document. There were
not many cases where one of the document being compared is
more comprehensive but with lower ad-hoc relevance. So those
cases are underrepresented, possibly due to the fact that
documents with high coverage tend to be very relevant.

738

