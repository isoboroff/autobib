Short Research Papers 3C: Search

SIGIR ’19, July 21–25, 2019, Paris, France

Name Entity Recognition with Policy-Value Networks
Yadi Lao†

Jun Xu‡∗

Sheng Gao†

Jun Guo†

Ji-Rong Wen‡

†

‡

Beijing University of Posts and Telecommunications
School of Information, Beijing Key Laboratory of Big Data Management and Analysis Methods, Renmin University of China
{laoyadi,gaosheng,junguo}@bupt.edu.cn,{junxu,jrwen}@ruc.edu.cn

ABSTRACT

linear statistical models, including the maximum entropy (ME) classifier [10] and maximum entropy Markov models (MEMMs) [8].
These models predict a distribution of the tags at each time step
and then use beam-like decoding to find optimal tag sequences.
Lafferty et al. proposed conditional random fields (CRF) to leverage
global sentence level feature and solve the label bias problem in
MEMM [5]. All the linear statistical models rely heavily on handcrafted features, e.g., the true entities usually start with capital
letters. In recent years deep neural networks based models have
been proposed for NER. Most of them directly combine the deep
neural networks with CRF. For example, Huang et al. [4] used a
bidirectional LSTM to automatically extract word-level representations and then combined with CRF for jointly label decoding. Ma
and Hovy [6] introduced a neural network architecture that both
word level and character level features are used, in which bidirectional LSTM, CNN, and CRF are combined. Reinforcement learning
has also been proposed for the task. For example, Maes et al. [7]
formalized the sequence tagging task as a Markov decision process
(MDP) and used SARSA to construct optimal sequence directly in
a greedy manner. Chang et al. [1] proposed a novel bandit-like
strategy to search a better policy than its reference teacher in structured prediction task. Inspired by the reinforcement learning model
of AlphaGO Zero [11] programs designed for the Game of Go, in
this paper we solve the NER task with a Monte Carlo tree search
(MCTS) enhanced Markov decision process (MDP). The new model,
referred to as MM-NER (MCTS enhanced MDP for Name Entity
Recognition), makes use of an MDP to model the sequential tag
assignment process. At each position (corresponding to word position), based on the past words and tags, two Gated Recurrent
Unit (GRUs) are used to summarize the past words and tags respectively. Based on the outputs of these two GRUs, a policy function
(a distribution over the valid tags) for guiding the tag assignment
and a value function for estimating the accuracy of tagging are
produced. To avoid the problem of assigning tags without utilizing
the whole sentence level tags, in stead of choosing a tag directly
with the raw policy predicted by the policy function, MM-NER
explores more possibilities in the whole space. The exploration is
conducted with the MCTS guided by the produced policy function
and value function, resulting a strengthened search policy for the
tag assignment. Moving to the next iteration, the algorithm moves
to the next position and continue the above process until at the end
of the sentence.
Reinforcement learning is used to train the model parameters.
In the training phase, at each learning iteration and for each training sentence (and the corresponding labels), the algorithm first
conducts an MCTS inside the training loop, guided by the current
policy function and value function. Then the model parameters are

In this paper we propose a novel reinforcement learning based
model for named entity recognition (NER), referred to as MMNER. Inspired by the methodology of the AlphaGo Zero, MM-NER
formalizes the problem of named entity recognition with a MonteCarlo tree search (MCTS) enhanced Markov decision process (MDP)
model, in which the time steps correspond to the positions of words
in a sentence from left to right, and each action corresponds to
assign an NER tag to a word. Two Gated Recurrent Units (GRU)
are used to summarize the past tag assignments and words in the
sentence. Based on the outputs of GRUs, the policy for guiding
the tag assignment and the value for predicting the whole tagging
accuracy of the whole sentence are produced. The policy and value
are then strengthened with MCTS, which takes the produced raw
policy and value as inputs, simulates and evaluates the possible tag
assignments at the subsequent positions, and outputs a better search
policy for assigning tags. A reinforcement learning algorithm is
proposed to train the model parameters. Empirically, we show that
MM-NER can accurately predict the tags thanks to the exploratory
decision making mechanism introduced by MCTS. It outperformed
the conventional sequence tagging baselines and performed equally
well with the state-of-the-art baseline BLSTM-CRF.
ACM Reference Format:
Yadi Lao, Jun Xu, Sheng Gao, Jun Guo, Ji-Rong Wen. 2019. Name Entity
Recognition with Policy-Value Networks. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information
Retrieval (SIGIR ’19), July 21–25, 2019, Paris, France. ACM, New York, NY,
USA, 4 pages. https://doi.org/10.1145/3331184.3331349

1

INTRODUCTION

Named entity recognition (NER) has gained considerable research
attention for a few decades. The main goal is to extract entities
such as PERSON, LOCATION etc. in a given sentence. Existing
models can be categorized into the statistical models and the deep
neural networks based models. Traditional research focuses on the
∗

Corresponding author

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’19, July 21–25, 2019, Paris, France
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00
https://doi.org/10.1145/3331184.3331349

1245

Short Research Papers 3C: Search

SIGIR ’19, July 21–25, 2019, Paris, France

map the left and right context window Xlt , Xrt in the state st to
two real vectors, and then define the value function as nonlinear
transformation of the weighted sum of the MLP’s outputs g(s) and
current candidate action in one-hot representation at :

adjusted to minimize the loss function. The loss function consists
of two terms: 1) the squared error between the predicted value and
the final ground truth accuracy of the whole sentence tagging; and
2) the cross entropy of the predicted policy and the search probabilities for tags selection. Stochastic gradient descent is utilized for
conducting the optimization.
To evaluate the effectiveness of MM-NER, we conducted experiments on the basis of CoNLL 2003 NER dataset. The experimental results showed that MM-NER can outperform the baselines of BLSTM,
BLSTM-CNN, and CRF, BLSTM-CRF with random initialization.
The results also showed that MM-NER performed almost equally
well with the state-of-the-art baseline of BLSTM-CRF with Glove
initialization. We analyzed the results and showed that MM-NER
achieved the performances through conducting lookahead MCTS
to explore in the whole tagging space.

V (s) = σ (hWv g(s), at i)

(1)

R | A(s) |∗ |д(s) |

where Wv ∈
is the weight vector to be learned during
training, |д(s)|, |A(s)| are the dimension of MLP’s output and the
number of possible action, h·, ·i is dot product operation, σ (x) =
1
1+e −x is the nonlinear sigmoid function.
h
iT
l(s) = GRUl (Xlt )T , GRUr (Xrt )T , g(s) = Wд l(s) + bд (2)
The two GRU networks are defined as follows: given st = [Xlt =
{xt −w , · · · , xt }, Xrt = {xt , · · · , xt +w }, Yt = {y1 , · · · , yt −1 }], where
xk (k = 1, · · · , t) is the word at k-th position, represented with its
word embedding. GRUl outputs a representation hk for position k:

2 MDP FORMULATION OF NER
2.1 NER as an MDP

zk =σ (Wz xk + Uz hk −1 ), rk = σ (Wr xk + Ur hk −1 ),
h̃k = tanh(Wh xk + Uz (fk ◦ ck −1 )),

Suppose that X = {x1 , · · · , xM } is a sequence of words to be labeled
for NER, and Y = {y1 , · · · , y M } is the corresponding ground truth
NER tag sequence. All components xi of X are the L-dimensional
preliminary representations of the words, i.e., the word embedding.
All components yi of Y are assumed to be selected from possible
NER tags set Y. The goal is to learn a model that can automatically
assign a tag to each word in the input sentence X.
MM-NER formulates the assignment of tags to sentences as a
process of sequential decision making with an MDP in which each
time step corresponds to a position in the sentence. The states,
actions, transition function, value function, and policy function of
the MDP are set as:
States S: We design the state at time step t as a triple st = [Xlt =
{xt −w , · · · , xt }, Xrt = {xt , · · · , xt +w }, Yt = {y1 , · · · , yt −1 }] where
Xlt and Xrt are the sequences of the left context window and right
context window of the input sentence with length M. w is the
window size and Yt is the prefix of the label sequence of length
t − 1. At the beginning (t = 1), the state is initialized as s 1 =
[{x1 }, {x1 , · · · , x1+w }, ∅], where ∅ is the empty sequence.
Actions A: At each time step t, the A(st ) ⊆ Y is the set of
actions the agent can choose. That is, the action at ∈ A(st ) actually
is a tag yt ∈ Y for word xt .
Transition function T : T : S × A → S is defined as

hk =(1 − zk ) ◦ tanh(ck ) + zt ◦ h̃k
where h and c are initialized with zero vector; operator “◦” denotes
the element-wise product and “σ ” is applied to each of the entries.
The last hidden state vector is used as the output of GRU, that is
GRUl (Xlt ) = h1+w T .
The function GRUr (Xrt ), which is used to map the right context
Xrt into a real vector, is defined similarly to that of for GRUl (Xlt ).
Policy function p: The policy p(s) defines a function that takes
the state as input and output a distribution over all of the possible
actions a ∈ A(s). Specifically, each probability in the distribution is
a normalized softmax function whose input is the bilinear product
of the state representation in Equation (2):
p(a|s) = softmax(Up g(s)),
where Up ∈

R | A(s) |∗ |д(s) |

is the parameter. The policy function is:

p(s) = hp(a 1 |s), · · · , p(a | A(s) | |s)i.

2.2

(3)

Strengthening raw policy with MCTS

Selecting the NER tags directly with the predicted raw policy p in
Equation (3) may lead to suboptimal results because the policy is
calculated based on the past tags. The raw policy has no idea about
the tags that will be assigned for the future words. To alleviate the
problem, following the practices in AlphaGo Zero [11], we propose
to conduct lookahead search with MCTS. That is, at each position
t, an MCTS search is executed, guided by the policy function p and
the value function V , and output a strengthened new search policy
π . Usually, the search policy π has high probability to select a tag
with higher accuracy than the raw policy p defined in Equation (3).
Algorithm 1 shows the details of the MCTS in which each tree
node corresponds to an MDP state. It takes a root node s R , value
function V and policy function p as inputs. The algorithm iterates
K times and outputs a strengthened search policy π for selecting a
tag for the root node s R . Suppose that each edge e(s, a) (the edge
from state s to the state T (s, a)) of the MCTS tree stores an action
value Q(s, a), visit count N (s, a), and prior probability P(s, a). At
each of the iteration, the MCTS executes the following steps:

st +1 = T (st , at ) = T ([Xlt , Xrt , Yt ], at ) = [Xlt +1 , Xrt +1 , Yt ⊕ {at }]
where ⊕ appends at to Yt . At each time step t, based on state st
the system chooses an action (tag) at for the word position t. Then,
the system moves to time step t + 1 and the system transits to a
new state st +1 : first, the left and right context window Xlt , Xrt are
updated by moving its window to obtain Xlt +1 , Xrt +1 ; second, the
system appends the selected tag to the end of Yt , generating a new
tag sequence.
Value function V : The state value function V : S → R is a
scalar evaluation, predicting the accuracy of the tag assignments
for the whole sentence (an episode), on the basis of the input state.
The value function is learned so as to fit the real tag assignment
accuracy of the training sentences.
In this paper, we use two GRUs and one-layer MLP to respectively

1246

Short Research Papers 3C: Search

SIGIR ’19, July 21–25, 2019, Paris, France

Algorithm 1 TreeSearch

Selection: Each iterations starts from the root s R and iteratively
selects the tags that maximize an upper confidence bound:
at = arg max(Q(st , a) + λU (st , a)),
a

Input: root s R , value V , policy p, search times K
1: for k = 0 to K − 1 do
2:
sL ← sR
3:
{Selection}
4:
while s L is not a leaf node do
5:
a ← arg maxa ∈A(s L ) Q(s L , a) + λ · U (s L , a){Eq. (4)}
6:
s L ← child node pointed by edge (s L , a)
7:
end while
8:
{Evaluation and expansion}
9:
v ← V (s L ) {simulate v with value function V }
10:
for all a ∈ A(s L ) do
11:
Expand an edge e to s = [s L .Xlt +1 , s L .Xrt +1 , s L .Yt ⊕ {a}]
12:
e.P ← p(a|s L ); e.Q ← 0; e.N ← 0{init edge properties}
13:
end for
14:
{Back-propagation}
15:
while s L , s R do
16:
s ← parent of s L ; e ← edge from s to s L
e .Q ×e .N +v
17:
e.Q ← e .N +1 {Eq. (5)}; e.N ← e.N + 1; s L ← s
18:
end while
19: end for
20: for all a ∈ A(s R ) do
e(s R ,a).N
21:
π (a|s R ) ← Í 0
e(s ,a 0 ).N

(4)

where √λÍ> 0 is the tradeoff coefficient, and the bonus U (st , a) =
N (s t ,a 0 )

0

∈A(s t )
p(a|st ) a 1+N
. U (st , a) is proportional to the prior prob(s t ,a)
ability but decays with repeated visits to encourage exploration.
Evaluation and expansion: When the traversal reaches a leaf
node s L , the node is evaluated with the value function V (s L ) (Equation (1)). Note following the practices in AlphaGo Zero, we use the
value function instead of rollouts for evaluating a node.
Then, the leaf node s L may be expanded. Each edge from the leaf
position s L (corresponds to each action a ∈ A(s L )) is initialized as:
P(s L , a) = p(a|s L ) (Equation (3)), Q(s L , a) = 0, and N (s L , a) = 0. In
this paper all of the available actions of s L are expanded.
Back-propagation and update: At the end of evaluation, the
action values and visit counts of all traversed edges are updated.
For each edge e(s, a), the prior probability P(s, a) is kept unchanged,
and Q(s, a) and N (s, a) are updated:

Q(s, a) ←

Q(s, a) × N (s, a) + V (s L )
; N (s, a) ← N (s, a) + 1.
N (s, a) + 1

(5)

Calculate the strengthened search policy: Finally after iterating K times, the strengthened search policy π for the root node s R
can be calculated according to the visit counts N (s R , a) of the edges
starting from s R :
π (a|s R ) = Í

N (s R , a)
,
N (s R , a 0 )
R)

a 0 ∈A(s

22:
23:

(6)

Learning and inference algorithms

|E |
Õ
Õ
1 ª
©
πt (a|st ) log
­(V (st ) − r )2 +
®.
p(a|s
t)
t =1 «
a ∈A(s t )
¬

R

N , learning rate η, K
Input: Labeled data D = {(X(n) , Y(n) )}n=1
1: Initialize Θ ← random values in [−1, 1]
2: repeat
3:
for all (X, Y) ∈ D do
4:
s 1 = [{x1 }, {x1 , · · · , x1+w }, {∅}]; M ← |X |; E ← ∅
5:
for t = 1 to M do
6:
π ← TreeSearch(s, V , p, K) {Alg. (1)}
7:
a = arg maxa ∈A(s) π (a|s) {select the best tag}
8:
E ← E ⊕ {(s, π )}
9:
s ← [s.Xlt +1 , s.Xrt +1 , s.Yt ⊕ {a}]
10:
end for
11:
r ← Metric(Y, s.YM ){overall tagging metric}
∂`(E,r )
12:
Θ ← Θ − η ∂Θ {` is defined in Eq. (7)}
13:
end for
14: until converge
15: return Θ

2.3.1 Reinforcement learning of the parameters. The model has
parameters Θ (including Wv , Wд , bд , Up and parameters in GRUs)
to learn. In the training phase, suppose we are given N labeled
N . Algorithm 2 shows the training
sentence D = {(X(n) , Y(n) )}n=1
procedure. First, the parameters Θ is initialized to random weights
in [−1, 1]. At each subsequent iteration, for each (X, Y), a tag sequence is predicted for X with current parameter setting: at each
position t, an MCTS search is executed, using previous iteration of
value function and policy function, and a tag at is selected according to the search policy πt . The tagging terminates at the end of
the sentence and achieved a predicted tag sequence (a 1 , · · · , a M ).
Given the ground truth tag sequence Y, the overall tagging metric
of the sentence X is calculated, denoted as r . The data generated
at each time step E = {(st , πt )}tM=1 and the final evaluation r are
utilized as the signals in training for adjusting the value function.
The model parameters are adjusted to minimize the error between
the predicted value V (st ) and tagging metric r , and to maximize the
similarity of the policy p(st ) to the search probabilities πt . Specifically, the parameters Θ are adjusted by gradient descent on a loss
function ` that sums over the mean-squared error and cross-entropy
losses, respectively:
`(E, r ) =

a ∈A(s R )

Algorithm 2 Train MM-NER model

for all a ∈ A(s R ).

2.3

end for
return π

The model parameters are trained by back propagation and stochastic gradient descent. Specifically, we use AdaGrad [3] on all
parameters in the training process.
2.3.2 Inference. The inference of the NER tag sequence for a
sentence is shown in Algorithm 3. Given a sentence X, the system
state is initialized as s 1 = [{x1 }, {x1 , · · · , x1+w }, ∅]. Then, at each
of the time steps t = 1, · · · , M, the agent receives the state st =
[Xlt , Xrt , Yt ] and search the policy π with MCTS, on the basis of the
value function V and policy function p. Then, it chooses an action

(7)

1247

Short Research Papers 3C: Search

SIGIR ’19, July 21–25, 2019, Paris, France

Algorithm 3 MM-NER Inference

Table 2: F1 w.r.t different search times k. F1 scores for the
raw policy p and value function v are also shown.

Input: sentence X = {x1 , · · · , xM }, value V , policy p, and K,
1: s ← [{x1 }, {x1 , · · · , x1+w }, {∅}]; M ← |X|
2: for t = 1 to M do
3:
π ← TreeSearch(s, V , p, K)
4:
a ← arg maxa ∈A(s) π (a|s)
5:
s ← [s.Xlt +1 , s.Xrt +1 , s.Y ⊕ {a}]
6: end for
7: return s.Y

F1

BLSTM
BLSTM-CNN
CRF (random)
BLSTM-CRF (random)
MM-NER
CRF (Glove)
BLSTM-CRF (Glove)

Recall

F1

80.14%
83.48%
82.93%
83.61%
84.19%
85.32%
88.57%

72.81%
83.28%
79.94%
84.78%
86.28%
84.55%
89.04%

76.29%
83.38%
81.41%
84.19%
85.22%
84.93%
88.30%

4

a for the word at position t. Moving to the next iteration t + 1, the
state becomes st +1 = [Xlt +1 , Xrt +1 , Yt +1 ]. The process is repeated
until the end of the sentence is reached. The code of MM-NER can
be found at https://github.com/YadiLao/MM_Tag.

3

300
85.16%

600
85.20%

1000
85.22%

1500
85.22%

p
85.17%

v
83.11%

policy π have on CoNLL 2003. For raw policy p, value network
v, agent will greedily choose action with highest probability or
value at each time step. As illustrated in table 2, we can see search
policy π achieves highest F1. If we ignore the small F1 difference
between π and p, we can greatly reduce the time complexity to
O(M ∗ |A|), which is more efficient than Viterbi decoding whose
time complexity is O(M ∗ |A| 2 ). Since the goodness of the search
policy π rely on the search times K, it is possible to make a trade-off
between the tagging result and efficiency.

Table 1: Performance comparison for all methods .
Precision

100
85.11%

EXPERIMENTS

We tested the performances of MM-NER on CoNLL 2003 NER
dataset1 , which contains 4 types of entities: persons (PER), organizations (ORG), locations (LOC), and miscellaneous names (MISC). In
the experiments, we used the publicly available GloVe 100-dimensional
embeddings as the representations of the words [9]. For MM-NER,
the tag level macro F 1 of the whole sentence is used as the tagging
metric R during the training of the MM-NER model. The learning
rate η, the tree search trade-off parameter λ, the number of hidden
units h in GRUs, the window size w and the number of search times
K. They were empirically set to η = 0.01, λ = 1.0, h = 100, w = 3,
K = 600. Spelling features are concatenated in the state.
We reproduced models in [4] and [2] , including linear statistical
model of CRF and neural models of BLSTM, BLSTM-CNN and its
combination model BLSTM-CRF. Table 1 reports the performances
of MM-NER and baseline methods in terms of NER precision, recall,
F1. From the result we can see that, MM-NER outperformed the
conventional baseline CRF with random initialization, deep learning
baselines of BLSTM, BLSTM-CNN, and BLSTM-CRF with random
initialization, indicating the effectiveness of the proposed MM-NER
model. The reason why MM-NER underperform BLSTM-CRF with
Glove initialization is that MCTS is more important than embedding
for guiding the agent to search a better tagging result.
One of the key steps in MM-NER is using MCTS to improve the
raw policy p of MDP. It is likely that the search policy π is better
than the raw policy p. We conducted experiments in inference stage
to test the effects that raw policy p, value network v and search
1 https://www.clips.uantwerpen.be/conll2003/ner/

1248

CONCLUSION

In this paper we have proposed a novel approach to named entity
recognition, referred to as MM-NER. MM-NER formalizes NER for
a sentence as sequential decision making with MDP. The lookahead
MCTS is used to strengthen the raw predicted policy so that the
search policy has high probability to select the correct tags for
each word. Reinforcement learning is utilized to train the model
parameters. MM-NER enjoys several advantages: tagging with the
shared policy and the value functions, end-to-end learning, and low
time complexity in inference. Experimental results show that MMNER outperformed or performed equally well with the baselines of
CRF, BLSTM, BLSTM-CNN, and BLSTM-CRF. Future work includes
improving the effectiveness and efficiency of MCTS.

5

ACKNOWLEDGEMENT

This work was funded by National Natural Science Foundation of
China (61872338 and 61702047), Beijing Natural Science Foundation
(4174098), Fundamental Research Funds for the Central Universities,
and Research Funds of Renmin University of China (2018030246).

REFERENCES
[1] Kai-Wei Chang, Akshay Krishnamurthy, Alekh Agarwal, Hal Daume III, and John
Langford. 2015. Learning to search better than your teacher. In ICML (2015).
[2] Jason P. C. Chiu and Eric Nichols. 2016. Named Entity Recognition with Bidirectional LSTM-CNNs. TACL, 4, 357–370 (2016).
[3] John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods
for online learning and stochastic optimization. JMLR 12, Jul (2011), 2121–2159.
[4] Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional LSTM-CRF models for
sequence tagging. arXiv preprint arXiv:1508.01991 (2015).
[5] John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and labeling sequence data.
In ICML (2001).
[6] Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bidirectional lstm-cnns-crf. arXiv preprint arXiv:1603.01354 (2016).
[7] Francis Maes, Ludovic Denoyer, and Patrick Gallinari. 2007. Sequence labeling
with reinforcement learning and ranking algorithms. In ECML, 648–657 (2007).
[8] Andrew McCallum, Dayne Freitag, and Fernando CN Pereira. 2000. Maximum
Entropy Markov Models for Information Extraction and Segmentation.. In ICML,
591–598, (2000).
[9] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove:
Global vectors for word representation. In EMNLP. 1532–1543. (2014)
[10] Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging.
In EMNLP. (1996)
[11] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
et al. 2017. Mastering the game of go without human knowledge. Nature 550,
7676 (2017), 354.

