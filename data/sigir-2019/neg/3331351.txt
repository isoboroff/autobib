Short Research Papers 3A: AI, Mining, and others

SIGIR â€™19, July 21â€“25, 2019, Paris, France

Syntax-Aware Aspect-Level Sentiment Classification with
Proximity-Weighted Convolution Network
Chen Zhang

Qiuchi Li

Dawei Songâˆ—

Beijing Institute of Technology
Beijing, China
& Zhejiang Lab
Hangzhou, China
gene@bit.edu.cn

University of Padua
Padua, Italy
qiuchili@dei.unipd.it

Beijing Institute of Technology
Beijing, China
dwsong@bit.edu.cn

ABSTRACT

context, i.e. a comment or a review. For example, in the following
comment about food â€œThey use fancy ingredients, but even fancy
ingredients donâ€™t make for good pizza unless someone knows how to
get the crust right.â€, the sentiment polarities for aspects ingredients,
pizza and crust are positive, negative and neutral respectively.
Aspect-level sentiment classification has attracted an increasing
attention in the fields of Natural Language Processing (NLP) and
Information Retrieval (IR), and plays an important role in various
applications such as personalized recommendation. Earlier works
in this area focused on manually extracting refined features and
feeding them into classifiers like Support Vector Machine (SVM) [7],
which is labor intensive. In order to tackle the problem, automatic
feature extraction has been investigated. For example, Dong et al.
[4] proposed to adaptively propagate the sentiments of context
words to the aspect via their syntactic relationships. Vo and Zhang
[15] built a syntax-free feature extractor to identify a rich source
of relevant features. Despite the effectiveness of these approaches,
Tang et al. [13] claimed that the modelling of semantic relatedness
of an aspect and its context remained a challenge, and proposed to
use target-dependent LSTM network to address this challenge.
As the attention mechanism and memory network have yielded
good results in many NLP tasks such as machine translation [1, 9],
LSTM combined with attention [6, 10] or memory network [3, 14] is
deployed to aspect-level sentiment classification to aggregate contextual features for prediction. Being capable of modelling semantic interactions between aspects and their corresponding contexts,
these models have improved performance over previous approaches.
However, they generally ignore the syntactic relations between the
aspect and its context words, which may hinder the effectiveness
of aspect-based context representation. For instance, a given aspect
may attend on several context words that are descriptively near
to the aspect but not correlated to the aspect syntactically. As a
concrete example, in â€œIts size is ideal and the weight is acceptable.â€,
the aspect term size may easily be depicted by acceptable based on
the semantic relatedness, which is in fact not the case. Syntactic
parsing has been used in some previous work [4], however, the
word-level parsing could impede feature extraction across different
phrases, as the sentiment polarity of an aspect is usually determined
by a key phrase instead of a single word [5].
In order to address the limitations mentioned above, we propose
an aspect-level sentiment classification framework that leverages
the syntactic relations between an aspect and its context and aggregates features at the n-gram level, within a LSTM-based architecture.
Inspired by the position mechanism [3, 6, 8, 14], the framework
utilizes a context wordâ€™s syntactic proximity to the aspect, a.k.a

It has been widely accepted that Long Short-Term Memory (LSTM)
network, coupled with attention mechanism and memory module,
is useful for aspect-level sentiment classification. However, existing
approaches largely rely on the modelling of semantic relatedness
of an aspect with its context words, while to some extent ignore
their syntactic dependencies within sentences. Consequently, this
may lead to an undesirable result that the aspect attends on contextual words that are descriptive of other aspects. In this paper,
we propose a proximity-weighted convolution network to offer an
aspect-specific syntax-aware representation of contexts. In particular, two ways of determining proximity weight are explored, namely
position proximity and dependency proximity. The representation
is primarily abstracted by a bidirectional LSTM architecture and
further enhanced by a proximity-weighted convolution. Experiments conducted on the SemEval 2014 benchmark demonstrate the
effectiveness of our proposed approach compared with a range of
state-of-the-art models1 .

KEYWORDS
Sentiment classification, Syntax-awareness, Proximity-weighted
convolution
ACM Reference Format:
Chen Zhang, Qiuchi Li, and Dawei Song. 2019. Syntax-Aware Aspect-Level
Sentiment Classification with Proximity-Weighted Convolution Network.
In Proceedings of the 42nd International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR â€™19), July 21â€“25, 2019, Paris,
France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.
3331351

1

INTRODUCTION

Aspect-level sentiment classification, also called aspect-based sentiment classification, is a fine-grained sentiment classification task
aiming at identifying the polarity of a given aspect within a certain
âˆ— Corresponding
1 Code

author.
is available at https://github.com/GeneZC/PWCN.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR â€™19, July 21â€“25, 2019, Paris, France
Â© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00
https://doi.org/10.1145/3331184.3331351

1145

Short Research Papers 3A: AI, Mining, and others

â€¦

proximity weight

ğ‘ğ‘ğ‘›ğ‘›âˆ’1

ğ‘Ÿğ‘Ÿ0

â„0

ğ‘£ğ‘£0

ğ‘Ÿğ‘Ÿ1

â„1

ğ‘£ğ‘£1

max-pooling

â€¦

â€¦

ğ‘ğ‘0 ğ‘ğ‘1 ğ‘ğ‘2 ğ‘ğ‘ğœğœ

ğ‘Ÿğ‘Ÿ2

â„2

ğ‘£ğ‘£2

â€¦

ğ‘Ÿğ‘Ÿğœğœ

â„ğœğœ

ğ‘£ğ‘£ğœğœ

of aspects by incorporating syntactical dependencies to uncover
component wordsâ€™ characteristics to the aspect2 . Such syntactical
dependency information in our proposed model is formalized as
proximity weight, which describes the contextual wordsâ€™ proximity to the aspect. Recall the example related to weight of a laptop
saying that â€œIts size is ideal and the weight is acceptable.â€. The set
of words including {ideal, acceptable} that are closer to the aspect
term weight in terms of semantics, should have a larger probability
describing the weight of a laptop. Further, from the perspective of
syntax parsing, ideal could be safely excluded from the word set
as it is syntactically too far from weight. Actually, acceptable is the
true descriptor of weight, indicating a positive sentiment.
Following this idea, we propose two different methods, namely
position proximity and dependency proximity, to model the syntactical dependency between contextual words and the aspect term
respectively.

softmax & sentiment

ğ‘ğ‘ğ‘ ğ‘ 

â€¦

SIGIR â€™19, July 21â€“25, 2019, Paris, France

â€¦

â€¦

â€¦

â€¦

â€¦

â€¦

â€¦

â€¦

â€¦

â€¦

â€¦

â€¦

â€¦

â€¦

â€¦

â€¦

proximity-weighted
convolution
ğ‘Ÿğ‘Ÿğ‘›ğ‘›âˆ’1

â„ğ‘›ğ‘›âˆ’1

ğ‘£ğ‘£ğ‘›ğ‘›âˆ’1

bidirectional LSTM

word embedding

Figure 1: Overview of the model architecture.

2.1.1 Position Proximity. Generally, it is more likely to see that
words around an aspect are describing the aspect. Thus, we view
such position information as an approximated syntactical proximity
measurement. Position proximity weights are computed by the
formula below:
ï£±
ï£´ 1 âˆ’ Ï„ nâˆ’i
0â‰¤i <Ï„
ï£´
ï£²
ï£´
pi = 0
(1)
Ï„ â‰¤ i < Ï„ +m
ï£´
ï£´
ï£´ 1 âˆ’ iâˆ’Ï„ âˆ’m+1 Ï„ + m â‰¤ i < n
n
ï£³
where proximity weight pi âˆˆ R. Intuitively, the weight decreases
in proportion to the wordâ€™s distance to the nearest border of the
aspect term.

proximity weight, to determine its importance in the sentence. We
then integrate the proximity weights into a convolution network
to capture n-gram information, called as Proximity-Weighted
Convolution Network (PWCN). Finally, a layer of max-pooling
is adopted to select the most significant features for prediction.
Experiments are conducted on SemEval 2014 Task4 datasets. The
results show that our model achieves a higher performance than a
range of state-of-the-art models, and hence illustrate that syntactical dependencies are more beneficial than semantic relatedness to
aspect-level sentiment classification.

2

2.1.2 Dependency Proximity. Apart from the absolute position in
the context, we also consider measuring the distances between
words in a syntax dependency parsing tree. For example, in a comment â€œthe food is awesome - definitely try the striped bass.â€ with food
as the aspect, we first construct a dependency tree3 , then compute
for a context word the tree-based distance, i.e. the length of the
shortest path in the tree, between the word and food. If the aspect
otherwise contains more than one word, we take the minimum of
the tree-based distances between a context word and all the aspect
component words. In the uncommon case where more than one
dependency trees are present in a context, we manually set the
distance between the aspect term and context words in other trees
to a constant, i.e. half of the sentence length4 .
For a better illustration of the proposed method, an example sentence is shown in Figure 2. With the above described approach, the
sequence of tree-based distances for all words in the sentence with
respect to the aspect term aluminum, d = {d 0 , d 1 , . . . , dÏ„ , dÏ„ +1 , . . . ,
dÏ„ +mâˆ’1 , . . . , dnâˆ’1 } are marked below the words in the figure. The
dependency proximity weights of the sentence are then assigned
as:
(
1 âˆ’ dni 0 â‰¤ i < Ï„ or Ï„ + m â‰¤ i < n
pi =
(2)
0
Ï„ â‰¤ i < Ï„ +m

THE PROPOSED MODEL

An overview of our proposed model is given in Figure 1. In the
model, an n-word sentence containing a target m-word aspect
term is formulated as S = {w 0 , w 1 , . . . , wÏ„ , wÏ„ +1 , . . . , wÏ„ +mâˆ’1 , . . . ,
w nâˆ’1 }, where Ï„ denotes the start token of the aspect term. Each
word is embedded into a low-dimensional real-valued vector with
a matrix E âˆˆ R |V |Ã—de [2], where |V | is the size of dictionary while
de is the dimensionality of a word vector. After word vectors V =
{e 0 , e 1 , . . . , eÏ„ , eÏ„ +1 , . . . , eÏ„ +mâˆ’1 , . . . , enâˆ’1 } are obtained through
word embedding, a bidirectional LSTM is adopted to produce the
hidden state vectors H = {h 0 , h 1 , . . . , hÏ„ , hÏ„ +1 , . . . , hÏ„ +mâˆ’1 , . . . ,
hnâˆ’1 }. Particularly, hi âˆˆ R2dh is a concatenation of hidden states
respectively obtained from the forward LSTM and the backward
LSTM, where dh is the dimensionality of a hidden state vector in
an unidirectional LSTM. The hidden state representation is further
enhanced by proximity-weighted convolution and then used for
prediction of sentiment polarity.

2.1

Proximity Weight

Previous attention-based models mainly focus on how to obtain
a context representation based on its component wordsâ€™ semantic
correlations with a corresponding aspect [3, 5, 6, 8, 10, 14]. These
models calculate attention weights referring to word vector representation in the latent semantic space, without taking into consideration syntax information. This may limit the effectiveness of these
models in term of mis-identify crucial context words for characterizing the aspect. Therefore, we replace this complicated modelling

2 We have conducted experiments using proximity weight combined with attention
weight, i.e. the combination of semantic relatedness and syntactic proximity, but we
get unexpected sub-optimal results, which will be shown in experiments section.
3 With spaCy toolkit: https://spacy.io/.
4 Itâ€™s a proper number that could serve as the boundary of possible descriptive contextual words in our experiments.

1146

Short Research Papers 3A: AI, Mining, and others

SIGIR â€™19, July 21â€“25, 2019, Paris, France

first

of

all

yes

this

is

a

mac

and

it

has

that

nice

brushed

aluminum

7

7

7

7

7

7

7

7

2

2

1

1

1

1

0

3 EXPERIMENTS
3.1 Datasets and Experimental Settings
We conduct experiments on two benchmarking datasets from SemEval 2014 [12]. The datasets consist of reviews and comments
from two categories: laptop and restaurant, respectively.
In all of our experiments, 300-dimensional GloVe is leveraged to
initialize word embedding [11]. All parameters of our model are
initialized with the uniform distribution. The dimensionality of
hidden state vectors is set to 300. We use Adam as the optimizer
with a learning rate of 0.001. The coefficient of L 2 regularization is
10âˆ’5 and batch size is 64. We adopt Accuracy and Macro-Averaged
F1 as the evaluation metrics. Additionally, the length of n-gram is
set to 35 .

Figure 2: Dependency distance with respect to aluminum.

2.2

Proximity-Weighted Convolution

Compared with the use of word-level features, Aspect-level sentiment classification with phrase-level features have been shown
more effective [5, 8]. We are thus inspired to propose a proximityweighted convolution, which is essentially 1-dimensional convolution with a length-l kernel, i.e. l-gram. Different from the original
definition of convolution, the proximity-weighted convolution assigns proximity weight before convolution calculation. The proximity weight assigning process is formulated below:
r i = pi h i

3.2

(3)

where r i âˆˆ R2dh represents the proximity-weighted representation
of the i-th word in the sentence.
Additionally, we zero-pad the sentence to ensure the convolution
outputs a sequence of the same length as the input sentence. The
convolution process contains:
 
l
t=
(4)
2
qi = max(WcT [r iâˆ’t âŠ• Â· Â· Â· âŠ• r i âŠ• Â· Â· Â· âŠ• r i+t ] + bc , 0)

(5)

where qi âˆˆ
denotes the features extracted by convolution
layer, and Wc âˆˆ Rl Â·2dh Ã—2dh and bc âˆˆ R2dh are weight and bias of
the convolution kernel, respectively.
As only few output features of the convolution layer are expected
to be instructive for classification, we choose the most prominent
feature qs âˆˆ R2dh through a 1-dimension max-pooling layer with
a kernel of length n, such that:
R2dh

qs = [ max qi, j ]T
0â‰¤i <n

0 â‰¤ j < 2dh

(6)

where qi, j is the j-th element of qi .
Finally, the most prominent feature vector qs is fed to a fully
connected layer, followed by a softmax normalization to obtain the
distribution y âˆˆ Rdp over the decision space on dp -way sentiment
polarity:
y = softmax(WTf qs + bf )

Model Comparison

A comprehensive comparison is carried out between our proposed
models, i.e. PWCN with position proximity (PWCN-Pos) and with
dependency proximity (PWCN-Dep), against several state-of-theart baseline models, as listed below:
â€¢ LSTM [13] only uses the last hidden state vector to predict
sentiment polarity.
â€¢ RAM [14] considers hidden state vectors of context as external memory and applies Gated Recurrent Unit (GRU) structure to multi-hop attention. The top-most representation is
used for predicting polarity.
â€¢ IAN [10] models attention between aspect and its context
interactively with two LSTMs.
â€¢ TNet-LF [8] leverages Context-Preserving Transformation
to preserve and strengthen the informative part of context.
It also benefits from a multi-layer architecture.
We also present comparison with two variants of PWCN-Pos.
Firstly, we propose Att-PWCN-Pos model, in which the proximity weight is multiplied by the normalized attention weight, to
check whether semantic relatedness and syntax relationship could
be incorporated with each other. Further, we intend to measure
the effectiveness of n-gram via setting l-gram to 1-gram, which
naturally degrades convolution process to point-wise feed-forward
network, and we call it Point-PWCN-Pos.

3.3

Experimental Results

The experimental results in Table 1 are yielded by averaging the
performances of 3 runs with random initialization. The results
demonstrate the general effectiveness of PWCN, which largely outperforms LSTM, RAM and IAN, and also achieves some increase
over TNet-LF, the best-performing baseline model under comparison. Among the two types of underlying syntactic structure of
sentences captured by PWCN model, dependency proximity brings
more benefits to the overall performance than position proximity,
with consistently higher Macro-F1 scores on both datasets. The
results also support our claim that n-gram information is critical
for feature extraction, which can be observed from the disparity
between Point-PWCN-Pos and PWCN-Pos.
Moreover, it is interesting to see that PWCN-based methods with
solely syntactic information outperform the Att-PWCN model that

(7)

where bf âˆˆ Rdp is the bias of the fully connected layer and Wf âˆˆ
R2dh Ã—dp is the learned weight.
Our model is trained by the standard gradient descent algorithm,
with the loss being the cross entropy loss with L 2 regularization:
Ã• Ã•
L=âˆ’
yÌ‚u log yu + Î» âˆ¥Î˜âˆ¥ 2
(8)
(S, yÌ‚)âˆˆD u

Here, yÌ‚ means one-hot vector of golden label while D is the collection of (sentence, label) pairs. And Î˜ denotes all trainable parameters, Î» is the coefficient of L 2 regularization.

5 We

1147

have tried several numbers and 3 performed the best.

Short Research Papers 3A: AI, Mining, and others

SIGIR â€™19, July 21â€“25, 2019, Paris, France

Table 1: Experimental results. Average accuracy and macro-F1 score over 3 runs with random initialization. The best results
are in bold. The marker â€  refers to p-value < 0.05 when comparing with IAN, while the marker â€¡ refers to p-value < 0.05 when
comparing with TNet-LF. The relative increase over the LSTM baseline is given in bracket.
Laptop

Model

Restaurant

Acc

Macro-F1

Acc

Macro-F1

LSTM
RAM
IAN
TNet-LF

69.63
72.81 (+4.57%)
71.63 (+2.87%)
75.16 (+7.94%)

63.51
68.59 (+8.00%)
65.94 (+3.83%)
71.10 (+11.95%)

77.99
79.89 (+2.44%)
78.59 (+0.77%)
80.20 (+2.83%)

66.91
69.49 (+3.86%)
68.41 (+2.24%)
70.78 (+5.78%)

Att-PWCN-Pos
Point-PWCN-Pos

72.92 (+4.72%)
74.45 (+6.92%)

68.14 (+7.29%)
69.47 (+9.38%)

80.15 (+2.77%)
80.00 (+2.58%)

70.17 (+4.87%)
69.93 (+4.51%)

PWCN-Pos
PWCN-Dep

75.23â€  (+8.17%)
76.12â€ â€¡ (+9.32%)

70.71â€  (+11.34%)
72.12â€ â€¡ (+13.56%)

81.12â€ â€¡ (+4.01%)
80.96â€  (+3.81%)

71.81â€  (+7.32%)
72.21â€  (+7.92%)

Table 2: Visualization of a case with respect to food

We believe it is a promising direction to dive into concrete examples to analyze the difference between PWCN models and attentionbased models to achieve a deep understanding of where the syntactical dependencies overwhelm semantic relatedness.

Method

Visualization

Pred.

Att.

great food but the service was dreadful !

negative

Pos.

great food but the service was dreadful !

positive

ACKNOWLEDGEMENTS

positive

This work is supported by The National Key Research and Development Program of China (grant No. 2018YFC0831700), Natural Science Foundation of China (grant No. U1636203), and Major Project
Program of Zhejiang Lab (grant No. 2019DH0ZX01).

Dep.

great food but the service was dreadful !

combines syntactic and semantic information. While this shows
the superiority of leveraging syntactical dependency information
to using semantic relatedness, we further conjecture that the attention mechanism could erroneously render term dependencies thus
adversely affect the correct decisions of PWCN.

3.4

REFERENCES
[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine
Translation by Jointly Learning to Align and Translate. In ICLR.
[2] Yoshua Bengio, RÃ©jean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A
neural probabilistic language model. Journal of machine learning research 3, Feb
(2003), 1137â€“1155.
[3] Peng Chen, Zhongqian Sun, Lidong Bing, and Wei Yang. 2017. Recurrent Attention Network on Memory for Aspect Sentiment Analysis. In EMNLP. 452â€“461.
[4] Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming Zhou, and Ke Xu. 2014.
Adaptive Recursive Neural Network for Target-dependent Twitter Sentiment
Classification. In ACL. 49â€“54.
[5] Chuang Fan, Qinghong Gao, Jiachen Du, Lin Gui, Ruifeng Xu, and Kam-Fai Wong.
2018. Convolution-based Memory Network for Aspect-based Sentiment Analysis.
In SIGIR. New York, NY, USA, 1161â€“1164.
[6] Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel Dahlmeier. 2018. Effective
Attention Modeling for Aspect-Level Sentiment Classification. In COLING. 1121â€“
1131.
[7] Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Targetdependent Twitter Sentiment Classification. In ACL. 151â€“160.
[8] Xin Li, Lidong Bing, Wai Lam, and Bei Shi. 2018. Transformation Networks for
Target-Oriented Sentiment Classification. In ACL. 946â€“956.
[9] Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective Approaches to Attention-based Neural Machine Translation. In EMNLP. 1412â€“1421.
[10] Dehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng Wang. 2017. Interactive
Attention Networks for Aspect-level Sentiment Classification. In IJCAI. 4068â€“
4074.
[11] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove:
Global Vectors for Word Representation. In EMNLP. 1532â€“1543.
[12] Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion
Androutsopoulos, and Suresh Manandhar. 2014. SemEval-2014 Task 4: Aspect
Based Sentiment Analysis. In SemEval. 27â€“35.
[13] Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu. 2016. Effective LSTMs for
Target-Dependent Sentiment Classification. In COLING. 3298â€“3307.
[14] Duyu Tang, Bing Qin, and Ting Liu. 2016. Aspect Level Sentiment Classification
with Deep Memory Network. In EMNLP. 214â€“224.
[15] Duy-Tin Vo and Yue Zhang. 2015. Target-dependent Twitter Sentiment Classification with Rich Automatic Features. In IJCAI. 1347â€“1353.

Impact of Syntax

To understand the effect proximity weight has brought, we conduct
a case study on an example which could be seen in Table 2. More
specifically, we visualize the weights given by attention in AttPWCN-Pos, position proximity in PWCN-Pos, and dependency
proximity in PWCN-Dep separately along with their predictions.
We can observe that the existing attention mechanism makes
wrong decision on which context word depicts food in an extreme
way, while both sorts of proximity weight in our model handle this
problem properly, which is within our expectation.

4

CONCLUSIONS AND FUTURE WORK

Previous methods of utilizing aspect information for the aspectlevel sentiment classification depend on the modelling of aspect
representation from a semantic perspective, while the syntactic relationship between the aspect and its context is generally neglected.
In this paper, we have built a framework that leverages n-gram
information and syntactic dependency between aspect and contextual terms into an applicable model. Experimental results have
demonstrated the effectiveness of our proposed models and suggested that syntactic dependency is more beneficial to aspect-level
sentiment classification than semantic relatedness.

1148

