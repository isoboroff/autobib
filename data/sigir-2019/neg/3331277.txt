Short Research Papers 1A: AI, Mining, and others

SIGIR ’19, July 21–25, 2019, Paris, France

Length-adaptive Neural Network for Answer Selection
Taihua Shao

Fei Cai∗

Science and Technology on Information Systems
Engineering Laboratory
National University of Defense Technology
Changsha, China
shaotaihua13@nudt.edu.cn

Science and Technology on Information Systems
Engineering Laboratory
National University of Defense Technology
Changsha, China
caifei@nudt.edu.cn

Honghui Chen

Maarten de Rijke

Science and Technology on Information Systems
Engineering Laboratory
National University of Defense Technology
Changsha, China
chh0808@gmail.com

Informatics Institute
University of Amsterdam
Amsterdam, The Netherlands
derijke@uva.nl

ABSTRACT

1

Answer selection focuses on selecting the correct answer for a
question. Most previous work on answer selection achieves good
performance by employing an RNN, which processes all question
and answer sentences with the same feature extractor regardless
of the sentence length. These methods often encounter the problem of long-term dependencies. To address this issue, we propose
a Length-adaptive Neural Network (LaNN) for answer selection
that can auto-select a neural feature extractor according to the
length of the input sentence. In particular, we propose a flexible
neural structure that applies a BiLSTM-based feature extractor for
short sentences and a Transformer-based feature extractor for long
sentences. To the best of our knowledge, LaNN is the first neural
network structure that can auto-select the feature extraction mechanism based on the input. We quantify the improvements of LaNN
against several competitive baselines on the public WikiQA dataset,
showing significant improvements over the state-of-the-art.

Answer selection plays a crucial role in a Question Answering (QA)
system, which selects the most appropriate answer for a question
from a set of candidate answers. Deep learning-based approaches
have been well studied for this task [4]. These approaches mainly
attempt to generate high-quality sentence embeddings of question
and answer, which are then utilized to measure the relevance of a
candidate answer to a question.
Previous research has achieved impressive performance on this
task. For instance, Tan et al. [6] employ an attentive BiLSTM to
measure the relevance of segments in candidate answers for a
particular question. Wang and Nyberg [8] apply a stacked BiLSTMbased feature extractor to learn a joint feature vector for questionanswer pairs. However, these RNN-based models do not consider an
important aspect that has been shown to affect sentence embeddingbased tasks, namely the length of a sentence. As the sentence length
grows, models will suffer from a long-term dependency problem
due to the sequential nature of RNNs. It becomes difficult to learn
dependencies between words in distant positions. This, in turn,
impacts the quality of embeddings for long sentences, as longdistance interactions between words of may be not be captured [3].
To address this issue, we propose a Length-adaptive Neural Network (LaNN) for the task of answer selection. Specifically, we deploy
a hierarchical length-adaptive neural structure to generate sentence
embeddings for question and answer sentences, which aims at extracting high-quality sentence features by employing a different
neural feature extractor depending on the length of the input sentences. We first generate a word representation for each word in
the input sentence by concatenating a frozen word embeddingand
a fine-tuning word embedding. Then, we propose a flexible neural
structure that applies a BiLSTM-based feature extractor for short
sentences and a Transformer-based feature extractor for long sentences, respectively. Finally, an attentive pooling layer that takes
the interaction between question and answer sentences into consideration is employed to generate the sentence embeddings that
are used to measure the relevance of a question and an answer.
We evaluate the performance of the proposed LaNN model
against several competitive baselines on a popular QA dataset,

CCS CONCEPTS
• Information systems → Question answering.

KEYWORDS
Answer selection, Neural network, Question answering.
ACM Reference Format:
Taihua Shao, Fei Cai, Honghui Chen, and Maarten de Rijke. 2019. Lengthadaptive Neural Network for Answer Selection. In Proceedings of the 42nd
International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’19), July 21–25, 2019, Paris, France. ACM, New York,
NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331277
∗ Corresponding

author.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’19, July 21–25, 2019, Paris, France
© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00
https://doi.org/10.1145/3331184.3331277

869

INTRODUCTION

Short Research Papers 1A: AI, Mining, and others

SIGIR ’19, July 21–25, 2019, Paris, France

2.2

Pooling Layer

…

…

…

…

…

…

…

+

Transformer

* flaglstm

flagtsfm *

flaglstm =1; flagtsfm =0

flaglstm =0; flagtsfm =1

YES

NO
L < L threshold

Frozen word
embeddings

Hidden Layer

……
……
……
……
……

look up

look up

where L threshold is a preset threshold to judge whether the input
sentence s is long or not. The flags will be employed to activate
the corresponding feature extractor by multiplying the input word
representation matrix RW with the value of flags as follows:

……
……
……
……
……

Fine-tuning word
embeddings

Feature extractor

We design a length-adaptive neural network as the feature extractor,
making good use of the context information of words in s to generate a high-quality sentence representation for s. Most previous
neural networks for answer selection [e.g., 1, 6] do not distinguish
between input sentences of different lengths. In contrast, we deploy
a BiLSTM-based and a Transformer-based feature extractor to deal
with sentences of different lengths for generating sentence embeddings. We employ two flags, i.e., flaglstm and flagtsfm , for each
feature extractor according to the sentence length L as follows:

flaglstm = 1 and flagtsfm = 0, (L < L threshold )
(3)
flaglstm = 0 and flagtsfm = 1, (L ≥ L threshold ),

fine tune

What county is Jacksonville Florida in ?
Question / Answer

Figure 1: Overview of the length-adaptive neural network
for answer selection.

lstm
where RW

APPROACH

Let s be an input sentence (question or answer) with length L. To
keep as many word features as possible, we produce the word representation corresponding to each word w t in s not only from frozen
pre-trained word embeddings but fine-tuning word embeddings.
Then, we concatenate two word embeddings to form a combined
word vector. Finally, we deploy a hidden layer to select useful features from the concatenated word vector. The final representation
rw t (of dimension D) of the word w t is calculated according to:
fr

ft

fr

(5)

=
=
=
=
=
=

lstm +U · h
σ (Wf · rw
t −1 + b f )
f
t
lstm
σ (Wi · rw t +Ui · ht −1 + bi )
lstm +U · h
tanh(Wc · rw
c
t −1 + bc )
t
ft
ft · Ct −1 + i t · C

lstm +U
σ (Wo · rw
o
t
ot · tanh(Ct ),

(6)

· ht −1 + bo )

where i, f and o represent the input gate, the forget gate and the output gate, respectively; h represents the memorized word representation (of dimension H ); Ce and C are the overall and the present memory; σ is the activation function sigmoid; W ∈ RH ×D , U ∈ RH ×H
and b ∈ RH ×1 are the network parameters, determining the input information, output information and bias, respectively. After
deploying an LSTM in two directions, we obtain a BiLSTM-based
sentence representation matrix RsLS as follows:
→
− ←
−
LS
rw
= ht ⊕ ht ,
(7)
t


LS
LS
LS
RsLS = rw
,
(8)
rw
· · · rw
1
2
L

Word representation

rw t = tanh(Wh · (rw t ⊕ rw t ) + bh ),

tsfm
RW
= RW · flagtsfm,
tsfm
and RW


ft




it



 C

ft

 Ct


 ot



 ht

Fig. 1 presents an overview of the proposed length-adaptive neural
network for answer selection. The model consists of three main
components, i.e., word representation (see §2.1), feature extractor
(see §2.2), and sentence embedding and answer ranking (see §2.3).

2.1

(4)

are updated input representation matrixes
for the BiLSTM-based and Transformer-based feature extractors.
For a short sentence, we activate the BiLSTM-based feature extractor by setting flaglstm to 1 and flagtsfm to 0, which leads to a null
representation matrix in the Transformer-based feature extractor.
lstm in R lstm are:
Operations on the t-th word representation rw
t
W

WikiQA. The experimental results show that the LaNN model outperforms the state-of-the-art baselines, showing a general improvement of 1.67% and 2.06% in terms of MAP and MRR, respectively.
Our main technical contributions are: (1) The LaNN model for
answer selection, which can auto-select a neural feature extractor
for questions and answers based on the length of an input sentence; and (2) An evaluation of the performance of LaNN, which
shows that it improves performance on the answer selection task,
especially for short questions with long correct answers.

2

lstm
RW
= RW · flaglstm

where || indicates concatenation of two vectors.
For a long sentence, we activate the Transformer-based feature
extractor by setting flaglstm to 0 and flagtsfm to 1. Following [7], we
employ a positional encoding to inject sequential information and
tsfm . After that, a scaled perception function
generate an updated RW
tsfm :
is applied to calculate self-attentive similarity in RW

(1)

ft

where rw t and rw t are corresponding word vectors from the frozen
word embeddings and the fine-tuning word embeddings; Wh ∈
RD×D and bh ∈ RD×1 are network parameters of the hidden layer.
The word representations of s form the word representation matrix:

RW = rw 1 rw 2 · · · rw L .
(2)

tsfm tsfm
tsfm
tsfm
f (RW
, RW ) = O aT · tanh(Wa · RW
+ Ua · RW
),

(9)

where O a ∈ RD×L , Wa ∈ RD×D and Ua ∈ RD×D are the attention
parameters. Then, the self-attentive sentence representation matrix

870

Short Research Papers 1A: AI, Mining, and others

SIGIR ’19, July 21–25, 2019, Paris, France

Table 1: WikiQA corpus statistics.

Rsa is produced by:
Rsa

=
=

tsfm , R tsfm )
© f (RW
W
· softmax ­­
q
d R tsfm
W
«

a
a
a
rw 1 rw 2 · · · rw L .

Variables

T

tsfm
RW

ª
®
®
¬

tsfm ;
where d R tsfm is the dimension of the row vector in RW
W

q
d R tsfm

(12)

vq = RQ · softmax(ColumnMax(G)),

(13)

va = R A · softmax(RowMax(G)),

(14)

where G is the attentive similarity between RQ and R A ; U ∈ RD×D
is the attention parameter; ColumnMax(·) (or RowMax(·)) is a function that returns the max value of a column (or row) vector of a
matrix. The relevance of an answer to a question is computed using
the cosine similarity of the sentence embeddings [5, 6].
In the training phase, each training instance consists of a question q, a positive answer a + (a ground truth) and a negative answer
a − (an incorrect answer) randomly sampled from all answers in the
training set. We train the neural network for the best training epoch
by minimizing the following ranking loss of candidate answers:

126
140
1,130
6.72
24.59

243
293
2,351
6.46
25.02

1,242
1,473
12,153
6.42
25.33

Model

Bs

Mg

Dp

Lr

L2

Hn

Hs

Rs

Dr

LaNN

30

0.1

0.5

10−4

10−3

7

40

280

0.85

(15)

4 RESULTS AND DISCUSSION
4.1 Overall performance

where m is a preset margin to judge if a training instance will be
terminated or not. By doing so, we can rank the candidate answers
according to their relevance towards the corresponding question.

3

873
1,040
8,672
6.36
25.51

combine CNN and BiLSTM to generate sentence embeddings. LaNN
is the answer selection model proposed in this paper.
Research questions. (RQ1) Does LaNN beat competitive answer
selection models? (RQ2) How does LaNN compare to baseline
models for question-answer pairs of different lengths, i.e., short
questions with long answers (short-long) and long questions with
long answers (long-long)? (As all answers in WikiQA are long, shortshort and long-short question-answer pairs are absent.)
Dataset and parameters. The dataset we use to evaluate the performance of LaNN is a publicly available open domain dataset,
the WikiQA dataset released in 2015 [9]; statistics of the WikiQA
dataset are listed in Table 1.
We set the length threshold to 5 in our experiments,1 which is
close to the average length of questions. Following [5], the pretrained word embedding’s dimension and the size of the hidden
layer are set to 300. We pad the sentence length for all questions and
answers to 40 [5]. For optimizing the loss, Adam [2] is employed. We
train our models in mini-batches and employ exponential decay to
vary the learning rate in every epoch. L2 regularization and dropout
methods are included in our training process to avoid over-fitting.
Table 2 details the main parameters of LaNN.
Evaluation metrics. We view the answer selection task as a ranking problem; it is aimed at ranking the candidate answers according
to their relevance towards the question. Hence, following prior
research on answer selection [8], we adopt Mean Average Precision
(MAP) and Mean Reciprocal Rank (MRR) as evaluation metrics.

(11)

Sentence embedding and answer ranking

loss = max{0, m − cos(vq , va + ) + cos(vq , va − )},

Overall

Table 2: Main experimental settings. Bs: batch size; Mg: margin; Dp: dropout; Lr: learning rate; L2: L2 regularization coefficient; Hn and Hs: the number and the size of the Transformer head; Rs: hidden size of BiLSTM; Dr: decay rate.

After generating the sentence representation matrix in §2.2, we employ an attentive pooling to generate the sentence embeddings vq
and va for question and answer from their corresponding sentence
representation matrixes RQ and R A according to:
G = RQ T · U · RA,

Test

W

where Rsai is the i-th self-attentive sentence representation matrix.

2.3

Validation

#Questions
#Correct Answers
QA Pairs
Avg. len. of ques.
Avg. len. of answ.

(10)

a
aims to scale the softmax function to regulate its value. Finally, rw
t
a
is the t-th self-attentive word vector in Rs .
We then adopt a multi-head mechanism [7] to jointly incorporate
information from different representation subspaces. Assuming that
there are n heads in the Transformer-based feature extractor, the n
self-attentive sentence representation matrices will be concatenated
to form the output sentence representation matrix RTF
s as:
an
a1
a2
RTF
s = Rs ⊕ Rs ⊕ · · · ⊕ Rs


TF
TF
TF
rw
· · · rw
= rw
,
1
2
l

Training

To answer RQ1, we present the results of the three baselines and the
LaNN model on the test set of WikiQA. In addition, we investigate
the model performance on questions of various types. In particular,
we categorize the test set into 5 groups according to the question
type, i.e., how, what, who, where, and when. The detailed evaluation
scores in terms of MAP and MRR are presented in Table 3.
As shown in Table 3, in general, QA-BiLSTM beats QA-CNN in
terms of both metrics. AB-LSTM/CNN shows superiority compared

EXPERIMENTS

Model summaries. We examine the effectiveness of the proposed
LaNN model by comparing its performance against the following
competitive state-of-the-art baselines: (1) QA-CNN [1]: a CNNbased model that employs a CNN-based feature extractor behind
a hidden layer to generate sentence embeddings. (2) QA-BiLSTM
[6]: a BiLSTM-based model that employs the BiLSTM-based feature
extractor to generate sentence embeddings. (3) AB-LSTM/CNN [6]:
an attention-based hybrid model that applies a serial structure to

1 We

test different threshold values in preliminary experiments but the best performance was observed with a value of 5.

871

Short Research Papers 1A: AI, Mining, and others

SIGIR ’19, July 21–25, 2019, Paris, France

Table 3: Model performance in terms of MAP and MRR. For each category, the best result is highlighted.
MAP
Models

overall how

what

who

MRR
where when overall how

what

who

where when

QA-CNN
0.6613 0.5837 0.6887 0.6439 0.6845 0.6169 0.6732 0.5919 0.7044 0.6493 0.6932 0.6221
QA-BiLSTM
0.6709 0.6061 0.6812 0.7426 0.7121 0.5259 0.6790 0.6115 0.6945 0.7450 0.7080 0.5259
AB-LSTM/CNN 0.6780 0.5764 0.6875 0.7561 0.7265 0.6008 0.6882 0.5802 0.7003 0.7629 0.7376 0.6094
0.6893 0.5621 0.6933 0.8005 0.7576 0.6196 0.7024 0.5721 0.7083 0.8083 0.7761 0.6273

LaNN

to the other two baselines without integrating an attention mechanism. Regarding LaNN, compared to the baselines, it achieves the
highest performance in terms of MAP and MRR. In particular, LaNN
outperforms QA-BiLSTM with up to 2.74% and 3.45% in terms of
MAP and MRR, respectively. The Transformer-based feature extractor in LaNN can help deal with the long-term dependencies
problem in the long sentences. Furthermore, the overall MAP and
MRR scores of LaNN are increased by up to 1.67% and 2.06% against
the best baseline AB-LSTM/CNN. This indicates an LaNN model
can help improve the performance of answer selection.
As to questions of different types, LaNN beats three baselines for
questions of all types except for type how. The baseline model with
the BiLSTM structure, QA-BiLSTM, presents the best results; the
BiLSTM-based structure is more effective than the CNN structure
as well as the Transformer structure in extracting contextual features hidden in sequential data that is prominent in answering the
questions with the type how. The LaNN model achieves its highest
improvements in the who group, with an increase of 5.87% and
5.95% against the best baseline AB-LSTM/CNN in terms of MAP
and MRR, respectively. The Transformer-based structure employed
in the LaNN model, which can deal with long-term dependencies
to some extent, is good at extracting features hidden in long-range
words for answering the question with the type who.

4.2

Thus, LaNN can help improve the performance of answer selection, especially for question-answer pairs with short question
and long correct answers. That is, the Transformer-based feature
extractor is beneficial for long answers; but when dealing with long
answers it is more beneficial for short questions than for long ones.
We further analyze the impact of length gap between question and
answer: the longer the answer is and the shorter the question is,
the better performance our LaNN model achieves.

5

Impact of length type

ACKNOWLEDGMENTS

To answer RQ2, we consider short-long and long-long question
answer pairs; as explained previously, these are the only pairs available in the WikiQA dataset. We plot the results in Fig. 2. Compared
0.80

QA-CNN
QA-BiLSTM

0.70
0.65
0.60
0.55
0.50

0.80

AB-LSTM/CNN
LaNN

0.75

MRR

MAP

0.75

QA-CNN
QA-BiLSTM

This research was supported by the National Natural Science Foundation of China under No. 61702526, the Defense Industrial Technology Development Program under No. JCKY2017204B064, Ahold
Delhaize, the Association of Universities in the Netherlands (VSNU),
and the Innovation Center for Artificial Intelligence (ICAI).

AB-LSTM/CNN
LaNN

0.70
0.65

REFERENCES

0.60

[1] Minwei Feng, Bing Xiang, Michael R Glass, et al. 2015. Applying deep learning to
answer selection: A study and an open task. In ASRU’2015. 813–820.
[2] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).
[3] Xiangpeng Li, Jingkuan Song, et al. 2019. Beyond RNNs: Positional Self-Attention
with Co-Attention for Video Question Answering. In AAAI’19. To appear.
[4] Jinfeng Rao, Hua He, and Jimmy Lin. 2017. Experiments with convolutional neural
network models for answer selection. In SIGIR’17. ACM, 1217–1220.
[5] Cicero Dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. 2016. Attentive
Pooling Networks. arXiv: Computation and Language (2016).
[6] Ming Tan, Cicero Dos Santos, Bing Xiang, and Bowen Zhou. 2016. Improved
representation learning for question answer matching. In ACL’16. 464–473.
[7] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, et al.
2017. Attention is all you need. In NIPS’17. 5998–6008.
[8] Di Wang and Eric Nyberg. 2015. A long short-term memory model for answer
sentence selection in question answering. In ACL’15. 707–712.
[9] Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. WikiQA: A challenge dataset
for open-domain question answering. In EMNLP’15. 2013–2018.

0.55
short-long

long-long

Length Types

(a) MAP.

CONCLUSIONS AND FUTURE WORK

We propose a length-adaptive neural network (LaNN) for answer
selection, which employs a BiLSTM-based feature extractor and
a Transformer-based feature extractor to capture global interactions of words to obtain improved sentence embeddings. LaNN can
auto-select the neural feature extractor according to the length of
the input sentence. Experimental results show LaNN can achieve
considerable improvements in terms of MAP and MRR over stateof-the-art baselines. Applying different neural feature extractors
to short questions and long answers leads to substantial improvements in of answer selection performance. As to future work, we
would like to examine the scalability of our proposal by evaluating
its effectiveness on other datasets. In addition, we have interest
in applying the proposed length-adaptive neural network to other
tasks, e.g., text summarization and natural language inference.

0.50

short-long

long-long

Length Types

(b) MRR.

Figure 2: Performance on different length types of questionanswer pairs in terms of MAP and MRR.
to the baselines, LaNN shows obvious improvements in terms of
MAP and MRR for the short-long group, while the improvements
for the long-long are modest. For instance, LaNN outperforms the
best baseline AB-LSTM/CNN by 7.34% and 8.99% in terms of MAP
and MRR on the short-long group; the increases in MAP and MRR
are only 0.74% and 0.97% on the long-long group.

872

