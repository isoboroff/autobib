Doctoral Consortium

SIGIR ’19, July 21–25, 2019, Paris, France

From Query Variations To Learned Relevance Modeling
Binsheng Liu

RMIT University
Melbourne, Australia
binsheng.liu@rmit.edu.au
variations. Approaching the generation using NMT techniques also
allows us to exploit the advantages of transfer learning [7] and attention mechanisms [10], both of which are proven to be effective
in tasks involving text understanding.
However, simply modeling query generation as a translation task
does not fully solve the problem. The second reason why query variations are important is they contain extra information not in the
original query, for example the variation “image jordan black gold”
contains new keywords “image jordan” compared to the original
query “black gold” and improves retrieval effectiveness significantly.
Translation tasks are not supposed to generate new knowledge out of
the original text. This motivates us to go beyond simply “translating”
queries into variations of the same meaning, but to also augment
variations with extra keywords. We consider incorporating pseudorelevance feedback (PRF) techniques. PRF techniques use initially
retrieved documents as additional resources to build a language
model for retrieval. In our proposal, we follow the assumption that
the initial documents are pseudo-relevant, and thus they may contain
useful information worth exploiting. We plan to use NMT models to
extract information from PRF documents and generate query variations for retrieval, and the mixture of the query variations can also
be viewed as a learned relevance model.
In order to best satisfy users’ information needs, we hope to understand how a query can be transformed into multiple representations
of the underlying information need to maximize effectiveness. To this
end, we propose a combination of state-of-the-art neural network architectures and pseudo-relevance feedback techniques in this study.

CCS CONCEPTS
• Information systems → Information retrieval; Query reformulation; Language models; Query representation;

KEYWORDS
Query variations; Relevance models; Representation learning
Thinking in terms of an information need instead of simply queries
provides a rich set of new opportunities in improving the effectiveness of search [6]. User queries may vary a lot for a single information
need [3, 9], as a query is often under-specified. Many techniques have
been proposed to enrich a single query, for example relevance modeling [8]. These techniques focus on improving overall system performance but may fail in some occasions. Instead of optimizing for a single query, another direction is to use multiple query variations to represent an information need. With fusion techniques, query variations
can improve system performance while failing fewer queries [2, 6].
Using query variations is a simple and appealing idea, but collecting them is a manually intensive task. Existing high quality query
variations are collected from crowd-sourcing workers and domain
experts [1]. Therefore the process cannot be automated and deployed
in a production system. Researchers have investigated random walks
on click graphs [5] and relevance models [4] to automatically generate query variations, but experiments show that there is still a big gap
between automatic variations and human generated ones. In this PhD
project, we focus on automatic query variation generation and try to
reduce the gap to human generated variations. We plan to leverage
advances of transfer learning and classic relevance modeling to generate high quality query variations to improve system performance.
Understanding how query variations improve system effectiveness helps us shape our research directions. First, they may address
vocabulary mismatches better. For example, in our preliminary experiments, the original query “teenage pregnancy” and a variation
“teen pregnancy” are found to deliver significantly different results in
a language model, although the terms only differ slightly. Generating
this kind of query variation is similar to a machine translation task,
in terms of generating new text from existing text while remaining
the meaning. Variation generation and translation are even more
similar in that there are often more than one answer to the original
text. Motivated by this, we propose to leverage neural machine translation (NMT) models in understanding text and generating query

Acknowledgments. This work is supported by the Australian Research Council’s Discovery Projects Scheme (DP170102231).

REFERENCES
[1] P. Bailey, A. Moffat, F. Scholer, and P. Thomas. 2016. UQV100: A Test Collection
with Query Variability. In Proc. SIGIR. 725–728.
[2] P. Bailey, A. Moffat, F. Scholer, and P. Thomas. 2017. Retrieval Consistency in
the Presence of Query Variations. In Proc. SIGIR. 395–404.
[3] N. J. Belkin, C. Cool, W. B. Croft, and J. P. Callan. 1993. The Effect Multiple Query
Representations on Information Retrieval System Performance. In Proc. SIGIR.
339–346.
[4] R. Benham, J. S. Culpepper, L. Gallagher, X. Lu, and J. Mackenzie. 2018. Towards
Efficient and Effective Query Variant Generation. In Proc. DESIRES. 62–67.
[5] N. Craswell and M. Szummer. 2007. Random walks on the click graph. In Proc.
SIGIR. 239–246.
[6] J. S. Culpepper. 2018. Single Query Optimisation Is the Root of All Evil. In Proc.
DESIRES. 100.
[7] J. Devlin, M. Chang, K. Lee, and K. Toutanova. 2018. BERT: Pre-Training of Deep
Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs]
(2018).
[8] V. Lavrenko and W. B. Croft. 2001. Relevance Based Language Models. In Proc.
SIGIR. 120–127.
[9] A. Moffat, F. Scholer, P. Thomas, and P. Bailey. 2015. Pooled Evaluation Over
Query Variations: Users Are as Diverse as Systems. In Proc. CIKM. 1759–1762.
[10] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser,
and I. Polosukhin. 2017. Attention Is All You Need. arXiv:1706.03762 [cs] (June
2017). arXiv:cs/1706.03762

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
SIGIR ’19, July 21–25, 2019, Paris, France
© 2019 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6172-9/19/07.
https://doi.org/10.1145/3331184.3331418

1450

