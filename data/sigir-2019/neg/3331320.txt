Short Research Papers 2C: Search

SIGIR ’19, July 21–25, 2019, Paris, France

Learning Unsupervised Semantic Document Representation for
Fine-grained Aspect-based Sentiment Analysis
Hao-Ming Fu

Pu-Jen Cheng

National Taiwan University
r06922092@ntu.edu.tw

National Taiwan University
pjcheng@csie.ntu.edu.tw

ABSTRACT

a document, often with sequential architectures such as RNN. The
effectiveness of these models drops significantly when the text being
processed gets much longer than a sentence. Consequently, simpler
models from non-sequential family often outperforms sequential
ones on the task. However, semantic meaning is intuitively lost
when ordering of words is discarded.
For instance, consider these two reviews on beer: “ I love the
smell of it, but the taste is terrible.” and “This one tastes perfect, but
not its smell.” Obviously, for models discarding the order of words,
recognizing which aspect each sentimental word “love”, “terrible”,
“perfect”, “not” refers to is not possible.
The overall sentiment of the reviews cannot be well captured either without aspect separation. That is because an overall sentiment
can be viewed as a combination of individual aspects weighted by
their importance. The best a non-sequential model can do with a
mixture of sentimental words without knowing importance of each
of them is a rough average.
In this paper, we propose a model that overcomes difficulties
encountered by both sequential and non-sequential models. Our
model is tested on widely used IMDB [5] sentiment analysis dataset
and the challenging aspect-based Beeradvocate [6] dataset. Our
results significantly outperform state-of-the-art methods on both
datasets.

Document representation is the core of many NLP tasks on machine
understanding. A general representation learned in an unsupervised
manner reserves generality and can be used for various applications.
In practice, sentiment analysis (SA) has been a challenging task that
is regarded to be deeply semantic-related and is often used to assess
general representations. Existing methods on unsupervised document representation learning can be separated into two families:
sequential ones, which explicitly take the ordering of words into
consideration, and non-sequential ones, which do not explicitly do
so. However, both of them suffer from their own weaknesses. In this
paper, we propose a model that overcomes difficulties encountered
by both families of methods. Experiments show that our model
outperforms state-of-the-art methods on popular SA datasets and a
fine-grained aspect-based SA by a large margin.

CCS CONCEPTS
• Information systems → Information retrieval.

KEYWORDS
Document representation, Sentence embedding, Unsupervised learning, Sentiment analysis, Semantic learning, Text classification
ACM Reference Format:
Hao-Ming Fu and Pu-Jen Cheng. 2019. Learning Unsupervised Semantic
Document Representation for Fine-grained Aspect-based Sentiment Analysis. In Proceedings of the 42nd International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR ’19), July 21–25, 2019, Paris,
France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.
3331320

1

2

RELATED WORKS

Non-sequential methods range widely from early Bag-of-Word
model and topic models including LDA to more complex models
such as Denoising Autoencoders [8], Paragraph Vectors[4] and
doc2vecC[2]. Sequential methods emerge quickly in recent years
thanks to the development of neural networks. Models for text
sequence representation include Skip-thoughts [3], a sentence level
extension from word level skip-gram model, and many other CNN
or RNN based methods.
Modeling a document as a group of sentences is not a new idea,
but an effective design to learn without supervision under this
framework is yet to be done. The closest work to our model should
be doc2vecC and Skip-thoughts Vectors. Our model is similar to
doc2vecC in the way that our model represents a document by
averaging embedding of sentences in it, while doc2vecC averages
embedding of words in the document. Besides, both doc2vecC and
our model explicitly use mean of embedding during training to
assure a meaningful aggregation of embedding vectors. Our model
is similar to Skip-thought Vectors in the way that both models try
to capture relations between adjacent sentences. Skip-thought Vectors chooses a generic prediction model, while our model projects
sentences into a shared hidden space and learn meaningful features
by managing relations of sentences in the space.

INTRODUCTION

An informative document representation is the key to many NLP
applications such as document retrieval, ranking, classification and
summarization. Learning without supervision reserves generality
of learned representation and takes advantage of large corpus with
no labels.
There are two families on learning document representation
without supervision: Sequential and non-sequential models. The
former takes ordering of words into consideration when processing
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’19, July 21–25, 2019, Paris, France
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00
https://doi.org/10.1145/3331184.3331320

1105

Short Research Papers 2C: Search

3

SIGIR ’19, July 21–25, 2019, Paris, France

THE PROPOSED MODEL

Given a document D composed of n sentences [s 0 , s 1 , . . . , sn ] in
order, our goal is to obtain a vector representation v D for the document. Note that [. . .] stands for an ordered list in the rest of this
paper.

3.1

Overview

Figure 1 is an overview of our model. The purpose of the model
is to obtain a vector representation for document D in an unsupervised manner. We update variables in the model by training
it to predict a target sentence among some candidate sentences
given its context sentences. The context sentences are defined
by k sentences on each side of the target sentence st . Namely,
Scnt x = [st −k , . . . , st −1 , st +1 , . . . , st +k ].
Besides the target sentence, r negative samples are coupled
with each target sentence st . The model will calculate a probability distribution over these r + 1 candidate sentences to make
prediction. We refer to the list of candidate sentences as Scdd =
[st , sneд1 , . . . , sneдr ]. The model will output r + 1 scalars, corresponding to each sentence in Scdd . These scalars are referred to
as logits of the sentences. A higher logit indicates a higher probability is distributed to the sentence by the model. Logit of the
target sentence st is denoted as lt and logits of negative samples
sneд1 , . . . , sneдr are denoted as lneд1 , . . . , lneдr .
According to Mikolov et al.[7], with those logits given, optimizing the following loss function will approximate optimizing the
probability distribution over all possible sentences in the world:
loss = −loд(σ (lt )) +

r
Õ
i=0

loд(σ (lneдi ))

Figure 1: Overview of our model. In the figure, number of
context sentences on each side is 1 and number of negative samples r is 2. Context sentences st −1 , st +1 are fed to the
model from the bottom. The target sentence st and negative
samples sneд1 , sneд2 are fed from the top. Logit of the target
sentence lt and negative samples lneд1 , lneд2 are obtained in
the middle. These will be used to calculate the loss.

(1)

Applying negative sampling, a softmax function is not literally
operated while a distribution over infinite number of all possible
sentences in the world is optimized. After the model is trained
this way, it can be used to calculate a vector representation for a
document.

3.2

where lenдth(x) denotes l2 norm of x and size(y) denotes number
of elements in y. This process solves the length vanishing problem
of element-wise averaging many vectors.
Now, we have a single vector vcnt x containing unified information from context sentences. If the sentence vector of a candidate
is similar to vcnt x , it is probability the sentence to be predicted.
Similarity is evaluated with inner product. So, vcnt x will dot with
the target sentence vector vt and negative sentence vectors in Vneд
to obtain a logit for each of them. Logit of the target sentence is
called lt = dot(vcnt x , vt ) and logits of negative samples are called
lneд1 , . . . , lneдr , where lneдi = dot(vcnt x , vneдi ).
With these logits, the loss can be calculated with Equation (1).

Architecture

3.2.1 model. As illustrated in Figure 1, we use sentence encoders
to encode a sentence into a fixed-length sentence vector. Two sentence encoders are used in the model, the context encoder Ecnt x
and the candidate encoder Ecdd . Sentences in Scnt x are encoded
into sentence vectors Vcnt x = [vt −k , . . . , vt −1 , vt +1 , . . . , vt +k ] by
Ecnt x . Those in Scdd are encoded into a target sentence vector vt
and negative samples vectors Vneд = [vneд1 , . . . , vneдr ] by Ecdd .
To merge information captured by each sentence vector in Vcnt x
into a single context vector, vectors in Vcnt x are element-wise averaged. The obtained context vector is called vcnt x .
vcnt x will go through a process called length adjustment except
when calculating Lcnt x in Section 3.3.1. Length adjustment process will normalize vcnt x and lengthen it to the average length of
sentence vectors which are used to obtain vcnt x itself. The process
is as follow:
Í
vcnt x
v i ∈Vc nt x lenдth(vi )
adjusted vcnt x =
×
(2)
lenдth(vcnt x )
size(Vcnt x )

3.2.2 Sentence encoders. Ecnt x and Ecdd have the same structure,
as elaborated in Table 1. Nevertheless, they do not share variables
except the word embedding table. This allows a sentence to be
represented differently when playing different roles. We choose
convolutional networks for sentence encoders for its simplicity and
efficiency of training. Note that a global average pooling layer is
placed on top of convolutional layers to form a fix-length vector
for sentences of variable length.

1106

Short Research Papers 2C: Search

SIGIR ’19, July 21–25, 2019, Paris, France

Table 1: Structure of sentence encoders. For consistency with
Figure 1, first layer is placed at the bottom and the last layer
at the top.

3.3

Layer type

parameters

Output Layer
Dropout
Fully connected
Fully connected
Global average pooling
Max pooling
Convolutional
Convolutional
Max pooling
Convolutional
Convolutional
Word embedding table
Input Layer

a fixed-length sentence vector.
dropout rate 0.5
100 nodes
1024 nodes with ReLU

Lt ot al is then minimized to update model variables. In particular, Lcnt x and Ldoc are responsible for capturing local and global
relations among sentences respectively. Ldoc also guarantees an
effective aggregation for sentence vectors.

3.4

size 2 with stride 2
256 filters with size 2
256 filters with size 2
size 2 with stride 2
256 filters with size 2
128 filters with size 2
embedding dimension 100
a sentence.

4

EXPERIMENTS

We first test our model on the widely used IMDB review dataset
[5] for SA. To go further, we test our model on the Beeradvocate
beer review dataset [6] for aspect-based SA. This dataset challenges
document representations with much more fine-grained SA.

Training

4.1

During training, a list of sentences S D = [s 0 , s 1 , . . . , sn ] from a
single document D is fed to the model as a single training sample.
The total loss to be minimized, Lt ot al , is the weighted sum of two
terms: the context loss Lcnt x and the document loss Ldoc . The
model is then trained end to end by minimizing Lt ot al .

Sentiment analysis

4.1.1 Dataset. We use IMDB review dataset in this sentiment analysis experiment. The dataset consists of 100k movie reviews. 25k
of the data are labeled for training and another 25k are labeled
for testing. The rest 50k reviews are unlabeled. Both training and
testing data are balanced, containing equivalent number of reviews
labeled as semantically positive and negative.

3.3.1 Context loss. For each sentence in S D , k sentences before
and k sentences after the target sentence are given in Scnt x as context sentences. Besides this, randomly selected negative samples
sneд1 , . . . , sneдr are selected from sentences in other documents
in the dataset. Length adjustment process is not applied when
calculating context loss. Target sentence logit lt and negative sentences logits lneд1 , lneд2 , . . . , lneдr are obtained and used to calculate Lcnt x t with Equation (1). The context loss Lcnt x is defined by
averaging losses from each sentence in S D except the first k and
the last k sentences for incomplete context sentences.
Ín−k
Lcnt x i
Lcnt x = i=k +1
(3)
n − 2k

4.1.2 Experiment design. We follow the design of Chen[2] to assess
our model under two settings: use all available data for representation learning or exclude testing data. Both of them make sense
since representation learning is totally unsupervised. After model
training, a linear SVM classifier is used to classify learned document representation under supervision. The performance of the
classifier, evaluated by accuracy, indicates the quality of learned
representation.
We compare our model with intuitive baseline methods including
Bag-of-Words, Word2Vec+AVG and Word2Vec+IDF, word-embedding
based method like SIF [1], sequential models including RNN language model, Skip-thought Vectors [3] and WME [9], and nonsequential models including Denoising Autoencoder [8], Paragraph
Vectors [4] and Doc2vecC [2]. Representative models from both
sequential and non-sequential families along with some intuitive
baselines are compared with.
We use a shared word embedding table of 100 dimensions and
train it from scratch. Dimensions of learned document representation are set as 100, which can be inferred from the outputs of
sentence encoders. Dropout rate is 0.5 and α is tuned to be 0.7.

where Lcnt x i is the context loss of a single target sentence.
3.3.2 Document loss. For document loss, there are only two differences from context loss: 1) length adjustment process is applied on
vcnt x . 2) all the sentences in S D , including the target sentence st
itself, are regarded as context sentences for each target sentence.
Consequently, each sentence in S D can be used as target sentence.
The document loss Ldoc is defined by averaging losses from all the
sentences in the document:
Ín
Ldoc i
Ldoc = i=1
(4)
n

4.1.3 Results and discussion. The results are shown in Table 2.
Our model considerably outperforms state-of-the-art models. As
we discussed, sequential models suffer from long text and nonsequential models lose semantic information for discarding ordering
of words. Our model, on the other hand, successfully overcomes
the difficulties encountered by both families of methods. Our model
considers ordering of words within each single sentence, which is

3.3.3 Total loss. The total loss is the weighted sum of context loss
and document loss. A hyper-parameter α is used to assign weights.
Total loss Ltot al is obtained by:
Ltot al = α × Lcnt x + (1 − α) × Ldoc

Inference of document representation

For a document D, its representation is the length adjusted average
of sentence vectors from all sentences in it. No extra training is
needed for new documents seen for the first time. Notice that it
is exactly the context vector vcnt x used for calculating Ldoc . It is
explicitly used during model training on purpose. This leads the
model to learn sentence vectors that can be effectively aggregated
by average. Also, the aggregated representation is guaranteed to
be informative since it is also learned during training.

(5)

1107

Short Research Papers 2C: Search

SIGIR ’19, July 21–25, 2019, Paris, France

Table 2: Sentiment analysis results on IMDB dataset in accuracy (%). Extra adv. column marks extra advantages out of
experiment settings. D for representation dimension greater
than 100, E for external data other than IMDB dataset used,
S for supervision by label during training. Methods in the sequential family are marked with (Seq.). Results sources: [9]
for WME, [1] for SIF and [2] for others.
Methods

Extra adv.

Table 3: Results of aspect-based sentiment analysis on Beeradvocate dataset. Reported numbers are accuracy (%).

Acc.(%)

Model

Appearance

Aroma

Palate

Taste

Overall

doc2vecC
Our model

80.826
85.070

82.810
86.695

82.500
86.795

86.154
91.020

82.366
87.280

Acc.(w/o test,%) 4.2.2 Results and discussion. Results of the experiment are shown
in Table 3. Our model far outperforms doc2vecC on every aspectSkip-thought Vectors (Seq.)
D, E
82.58
based classification tasks including overall. The results indicate that
SIF with GloVe
E
85.00
information of all aspects is better captured and stored in a single
RNN-LM (Seq.)
S
86.41
86.41
vector learned by our model. It also illustrates the generality of our
Word2Vec + AVG
E
87.89
87.31
model to perform well on different aspects and tasks with different
Bag-of-Words
D
87.47
87.41
difficulties.
Denoising Autoencoders
88.42
87.46
We notice that even though doc2vecC does not explicitly consider
Paragraph Vectors
89.19
87.90
ordering of words, it still achieves an acceptable accuracy on aspectWord2Vec + IDF
E
88.72
88.08
based classification. This may be caused by the fact that many words
Doc2VecC
89.52
88.30
used in the reviews are aspect-related on its own. For instance,
WME (Seq.)
E
88.50
“delicious” is a strongly taste-related word that is useful for aspectOur model (Seq.)
92.78
90.83
based sentiment analysis even without knowing its context.
Surprisingly, we find in experiments that performance of our
model is hardly sensitive to any of the hyper-parameters except α.
considered the fundamental unit of a concept. At the same time,
We tuned α in the range between 0 and 1 and picked 0.7. We find
instead of processing long text at once, pieces of concepts extracted
the value generalizable to different tasks and datasets. As for other
from sentences are effectively aggregated to form a meaningful
hyper-parameters, we find the model insensitive to them in a wide
representation of documents.
range. That is why we use exactly the same parameters on both
IMDB and Beeradvocate datasets. This observation indicates the
4.2 Aspect-based sentiment analysis
effectiveness as well as robustness of our model design.
Aspect-based sentiment analysis is a more challenging task for
5 CONCLUSIONS
document representation. Besides capturing an overall image of
a document, detailed information mentioned in only part of the
Experimental results show that our model outperforms state-of-thedocument has to be recognized and well preserved. We test the
art unsupervised document representation learning methods by a
ability of our model to learn a single representation that includes
large margin on both classic SA task and its aspect-based variance.
information from all different aspects. We compare our model with
We attribute this improvement to the design of our model that
doc2vecC on this task, since it is the strongest competitor in the
enables it to reserve ordering of words and aggregate sentence vecsentiment analysis experiment without any extra advantage.
tors effectively at the same time. Splitting long text into sentences
avoids the curse of length for sequential models. Aggregation with
4.2.1 Dataset and Experiment design. We choose the Beeradvocate
average is made effective by explicitly using the obtained represenbeer review dataset for aspect-based SA task. It consists of over 1.5
tation during training.
million beer reviews; each has four aspect-based scores and one
overall score. All the scores are in the range of 0 to 5 and given by
REFERENCES
the reviewers. The four aspects are appearance, aroma, palate and
[1] Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017. A simplebut tough-to-beat
taste. For a fair comparison with the SA experiment, we only use
baseline for sentence embeddings. In ICLR.
[2] Minmin Chen. 2017. Efficient vector representation for documents through corthe first 500k reviews of the dataset.
ruption. In ICLR 2017.
To follow the settings of the SA experiment, we reassign labels
[3] R. Kiros, Y. Zhu, R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba, and S. Fidler.
to each aspect to simplify it to a binary classification task. A review
2015. Skip-thoght vectors. In In Advances in neural information processing systems.
[4] Q. V. Le and T. Mikolov. 2014. Distributed representations of sentences and
is labeled as positive/negative on a certain aspect if its score on
documents. In In ICML, volume 14.
the aspect is not lower/higher than 4.5/3.5. For each aspect, we
[5] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Ng, and
construct two pools of positive and negative reviews respectively.
Christopher Potts. 2011. Learning word vectors for sentiment analysis. In ACL.
[6] Julian McAuley, Jure Leskovec, and Dan Jurafsky. 2012. Learning attitudes and
We randomly select 50k samples from each pool. The selected data
attributes from multi-aspect reviews. In In Proceedings of ICDM. IEEE.
are split in half for training and testing. Now we have 50k balanced
[7] T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013. Efficient estimation of word
representations in vector space. In arXiv preprint arXiv:1301.3781.
data for training and 50k data for testing for each aspect.
[8] P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol. 2008. Extracting and
In this experiment, all available data (500k data used in the excomposing robust features with denoising autoencoders.. In ICML.
periment) are used for representation learning. For each aspect, a
[9] Lingfei Wu, Ian En-Hsu Yen, Kun Xu, Fangli Xu, Avinash Balakrishnan, PinYu Chen, Pradeep Ravikumar, and Michael J. Witbrock. 2018. Word mover's
linear SVM classifier will be trained. We use the same parameters
embedding: From word2vec to document embedding. In EMNLP.
as on IMDB review dataset.

1108

