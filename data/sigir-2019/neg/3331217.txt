Session 8B: Hashing

SIGIR ’19, July 21–25, 2019, Paris, France

Online Multi-modal Hashing with Dynamic Query-adaption
Xu Lu1 , Lei Zhu1 , Zhiyong Cheng2 , Liqiang Nie3 , Huaxiang Zhang1
1
2

School of Information Science and Engineering, Shandong Normal University
Shandong Computer Science Center (National Supercomputer Center in Jinan),
Qilu University of Technology (Shandong Academy of Sciences)
3 School of Computer Science and Technology, Shandong University
leizhu0608@gmail.com,huaxzhang@hotmail.com

ABSTRACT

CCS CONCEPTS

Multi-modal hashing is an effective technique to support large-scale
multimedia retrieval, due to its capability of encoding heterogeneous multi-modal features into compact and similarity-preserving
binary codes. Although great progress has been achieved so far,
existing methods still suffer from several problems, including: 1)
All existing methods simply adopt fixed modality combination
weights in online hashing process to generate the query hash codes.
This strategy cannot adaptively capture the variations of different queries. 2) They either suffer from insufficient semantics (for
unsupervised methods) or require high computation and storage
cost (for the supervised methods, which rely on pair-wise semantic
matrix). 3) They solve the hash codes with relaxed optimization
strategy or bit-by-bit discrete optimization, which results in significant quantization loss or consumes considerable computation
time. To address the above limitations, in this paper, we propose an
Online Multi-modal Hashing with Dynamic Query-adaption (OMHDQ) method in a novel fashion. Specifically, a self-weighted fusion
strategy is designed to adaptively preserve the multi-modal feature
information into hash codes by exploiting their complementarity. The hash codes are learned with the supervision of pair-wise
semantic labels to enhance their discriminative capability, while
avoiding the challenging symmetric similarity matrix factorization.
Under such learning framework, the binary hash codes can be directly obtained with efficient operations and without quantization
errors. Accordingly, our method can benefit from the semantic labels, and simultaneously, avoid the high computation complexity.
Moreover, to accurately capture the query variations, at the online
retrieval stage, we design a parameter-free online hashing module which can adaptively learn the query hash codes according to
the dynamic query contents. Extensive experiments demonstrate
the state-of-the-art performance of the proposed approach from
various aspects.

• Information systems → Information systems applications;
Multimedia information systems; Nearest-neighbor search.

KEYWORDS
Online multi-modal hashing; Efficient discrete optimization; Dynamic query-adaption; Self-weighted
ACM Reference Format:
Xu Lu, Lei Zhu, Zhiyong Cheng, Liqiang Nie, Huaxiang Zhang. 2019. Online
Multi-modal Hashing with Dynamic Query-adaption. In Proceedings of the
42nd International ACM SIGIR Conference on Research and Development in
Information Retrieval (SIGIR’19), July 21–25, 2019, Paris, France. ACM, NY, NY,
USA, 10 pages. https://doi.org/10.1145/3331184.3331217

1

INTRODUCTION

Hashing [11, 24, 31] encodes the high-dimensional data into binary
hashing codes and preserves data similarity in the low-dimensional
Hamming space, which can greatly accelerate the large-scale multimedia retrieval process and also significantly save the storage cost.
Currently, most existing hashing work make efforts on uni-modal
hashing [2, 5, 14, 17, 37, 38] and cross-modal hashing [1, 4, 10, 22,
26, 29], and less attention has been paid to the multi-modal hashing
[7, 12, 21, 25, 30].
In multimedia retrieval, it is common that both the targeted query
and database data are described by heterogeneous multi-modal features. Each modality characterizes the instance from a different
view and possess its own characteristics [32, 33, 39]. Therefore, it is
necessary to combine features from different modalities to comprehensively represent query and database data for accurate retrieval.
However, multi-modal feature representation usually resulted in
high dimensional features and high-cost matching problems. To
support efficient large-scale multimedia retrieval, the multi-modal
hashing method , which considers combining the multi-modal features at both the offline hash code learning and the online query
hashing stages, has been studied [7, 12, 21, 30]. This technique
is different from the uni-modal and cross-modal hashing where
only one of the modalities is provided at query. One may argue to
extend uni-modal hashing to multi-modal hashing by simply substituting the input with the concatenated unified vector comprising
of multi-modal features. However, this simple extension may fail
to fully exploit the complementary of different modality features.
Based on the motivation, several multi-modal hashing methods
have been proposed with various unsupervised and supervised
learning approaches.
Although great progress has been achieved so far [13, 18, 19],
they still suffer from several fatal problems:

Lei Zhu and Huaxiang Zhang are corresponding authors.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’19, July 21–25, 2019, Paris, France
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00
https://doi.org/10.1145/3331184.3331217

715

Session 8B: Hashing

SIGIR ’19, July 21–25, 2019, Paris, France

Table 1: Key characteristics of representative multi-modal
hashing methods and the proposed OMH-DQ (n is the number of training samples). The complexity includes both computation and space complexity.

1) Fixed weights. The adopted weighting scheme is essential to
multi-modal feature combining for multi-modal hash code learning.
Nevertheless, all existing methods [7, 12, 13, 18, 19, 21, 28] generate
hash codes for various queries with a fixed weight vector, which is
learned in the offline hash code learning stage. The fixed weights
obviously cannot capture the variations of dynamic queries.
2) Limited semantics or scalability. Most multi-modal hashing
methods [7, 12, 19, 21] are unsupervised. They have not exploited
any semantic labels, which have demonstrated to be very useful
on enhancing the discriminative capability and thus significantly
improve the performance in uni-modal and cross-modal hashing.
As a result, the hash codes learned by unsupervised methods only
contain limited semantics. On the other hand, although existing
supervised multi-modal hashing methods can preserve the semantics, they often consume considerable computation and storage. For
example, Compact Kernel Hashing with Multiple Features (MFKH)
[13] and Discrete Multi-view Hashing (DMVH) [28] need to construct semantic graph, which is an n×n matrix reflecting the similarities between pairs of instances, to model the semantic correlation
for hashing learning, resulting in high time complexity and storage
cost.
3) Optimization challenge. The binary constraint on hash codes
leads to an NP-hard combinatorial optimization problem. Existing
multi-modal hashing methods adopt two kinds of optimization
strategies. The first is a two-step relaxing+rounding relaxed optimization strategy [7, 12, 21, 30], which relaxes the discrete constraints and then obtains binary codes by simply thresholding. As
indicated by recent literature [18, 19], this simplified hash optimization strategy could bring significant quantization errors and thus
lead to sub-optimal solutions. The other one is the bit-by-bit hash
optimization based on Discrete Cyclic Coordinate Descend (DCC)
[19, 28]. This strategy is still time-consuming since only one bit is
optimized in each step and thus learning all hashing bits requires
lots of iterations.
In light of the above analysis, in this paper, we propose an Online Multi-modal Hashing with Dynamic Query-adaption (OMH-DQ)
method to address all these problems. Our method learns discriminative hash codes with pair-wise semantic supervision and efficient
(both computation and storage) discrete optimization. Moreover,
OMH-DQ adaptively generates query hash codes with online query
hashing module to capture the query variations. The main contributions of this paper are:

Methods
CHMIS [30]
MFH [21]
MVAGH [7]
MAH [12]
MVLH [19]
MVDH [18]
MFKH [13]
DMVH [28]
Ours

Weight
fixed
fixed
fixed
fixed
fixed
fixed
fixed
fixed
Query adaptive

Learning
unsupervised
unsupervised
unsupervised
unsupervised
unsupervised
unsupervised
supervised
supervised
supervised

Optimization
relaxation
relaxation
relaxation
relaxation
discrete
discrete
relaxation
discrete
discrete

Complexity
O(n2 )
O(n2 )
O(n2 )
O(n2 )
O(n)
O(n2 )
O(n2 )
O(n2 )
O(n)

challenging symmetric semantic matrix factorization and
storage cost of semantic graph.
• An efficient discrete optimization method is proposed to
directly solve the binary hash codes without relaxing quantization errors. The hash codes are learned in a fast mode
with simple operation, achieving high learning efficiency
and retrieval accuracy. Experimental results on public multimedia retrieval datasets demonstrate the state-of-the-art
performance of the proposed method from various aspects.
The rest of this paper is arranged as follows. Section 2 reviews
the related work of existing multi-modal hashing methods. The
details of the proposed method are introduced in Section 3. Section
4 presents the experiments. Finally, Section 5 concludes the paper.

2

RELATED WORK

Multi-modal hashing takes advantage of multi-modal features to
learn hash codes1 . Most multi-modal hashing methods learn the
hash codes with the unsupervised learning paradigm. They basically model the sample relations in each modality with graph, based
on which the hashing learning is performed. Composite Hashing
with Multiple Information Sources (CHMIS) [30] is one of the pioneering methods of this kind. It integrates multi-modal feature
distribution in multiple pre-constructed graphs into the binary
hash codes by adjusting the weight of each modality-specific graph
to maximize the hashing performance. Multiple Feature Hashing
(MFH) [21] considers to learn the hashing codes by preserving the
local structures with graphs of all multi-modal features. Multi-view
Anchor Graph Hashing (MVAGH) [7] solves nonlinear integrated
binary codes by a subset of eigenvectors of an averaged multi-view
graph. It can improve learning efficiency with a low-rank form
of the averaged similarity matrix induced by multi-view anchor
graph. Multi-view Alignment Hashing (MAH) [12] combines multimodal features to learn hash codes and meanwhile removes feature
redundancies. The authors formulate a multi-graph regularized
nonnegative matrix factorization framework, where hash codes are
learned by uncovering the hidden semantics and capturing the joint
probability distribution of data. The above four multi-modal hashing methods adopt the relaxed optimization strategy to compute
hash codes. This strategy first relaxes the discrete constraints and
then calculates binary codes by a simple thresholding method. As

• Instead of adopting the fixed modality combination weights
to generate online query hash codes as existing multi-modal
hashing methods, we propose a query-adaptive and selfweighted online hashing module to accurately capture the
variations of different queries. Moreover, the online module is parameter-free. It could avoid time-consuming and
inaccurate parameter adjustment in the unsupervised query
hashing process.
• We develop an efficient hash code learning module to simultaneously correlate the learned hash codes with low-level
data distribution and high-level semantics. In particular, this
design not only enhances the discriminative capability of
hash codes with pair-wise semantics, but also avoids the

1 Many approaches are termed as multi-view hashing. They can be considered as multi-

modal hashing by substituting the multi-view features with heterogeneous modality
features.

716

Session 8B: Hashing

SIGIR ’19, July 21–25, 2019, Paris, France

Offline Training

indicated by the recent literature [17], this relaxed hash optimization strategy could bring significant quantization errors and lead
to sub-optimal solutions.
To address the problems, Multi-view Latent Hashing (MVLH)
[19] and Multi-view Discrete Hashing (MVDH) [18] propose to
directly learn discrete multi-modal hash codes. MVLH performs
discrete hash learning in a latent kernel feature space shared by
multiple views. MVDH adopts matrix factorization to directly generate discrete hash codes as the latent representations of multiple
views. Due to the independence on the semantic labels, all these
unsupervised multi-modal hashing still suffer from the limited discriminative capability problem.
A few supervised multi-modal hashing methods have been proposed to exploit the semantic labels as supervision. Compact Kernel Hashing with Multiple Features (MFKH) [13] formulates the
multiple feature hashing as a similarity preserving problem with
optimal linearly-combined multiple kernels. Hash codes in MFKH
are learned with relaxed optimization strategy. Discrete Multi-view
Hashing (DMVH) [28] is a discrete supervised multi-modal hashing
method. It exploits the discriminative semantic labels to directly
learn discrete hash codes supervised with the pre-constructed semantic matrix. It preserves both the local similarity structure and
the semantic similarity of data points into the hash codes. With
the supervision of semantic labels, these supervised multi-modal
hashing methods are reported to achieve superior performance
than unsupervised ones.
All existing multi-modal hashing methods simply adopt fixed
modality combination weights learned from offline hash code learning to generate all query hash codes. The fixed weights cannot
adaptively capture the variations of dynamic out-of-sample queries.
Furthermore, most existing methods construct graph to model similarity or semantic correlation for hashing learning. The graphs
require considerable computation and storage cost. This disadvantage limits their scalability on large-scale multimedia retrieval. The
key characteristics of representative hashing methods and the proposed method are summarized in Table 1.

3

Training Set

Consensus Multi-modal Feature Mapping
Pair-wise Semantic

Nonlinear Mapping
Projection

Image Modality

CNN Feature

Visual


Nonlinear Mapping
Projection

Text Modality

Consensus Multi-modal
Factor

Textual

BoW Feature

-1

1

-1

1

1

-1

-1

-1

-1

Hash code

Asymmetric Supervised Learning

Discrete Hash Code
Optimization
Computation
Efficient

Online Multimedia Retrieval
New Query

Storage
Efficient

Multi-modal Projection Matrix

Multi-modal Features
Nonlinear Mapping

Hash Code
for Database

Visual
Image Modality

Query-adaptive
Online Hashing

CNN Feature



Parameterfree

Dynamic
Weights

Retrieval

Hash Code
for Query

Nonlinear Mapping
Textual
Text Modality

BoW Feature

Figure 1: The basic framework of the proposed OMH-DQ.
to model the multi-modal data information so that the binary projection learning can be performed. Most existing methods [7, 12, 21, 30]
construct graph to accomplish this task. The graph construction
process costs O(n 2 ) computation and storage complexity, which
is practically unacceptable for large-scale multimedia retrieval. In
this paper, we propose an efficient consensus multi-modal feature
mapping to reduce the complexity to O(n).
(m)
(m)
Given the m-th modality feature X(m) = [x1 , ..., xn ] ∈ Rdm ×n ,
(m)

we first obtain the nonlinearly transformed representation φ(xi
(m)
(m)
∥xi −ap ∥F2
2
2σm

(m)
(m)
∥xi −a1 ∥F2
2
2σm

(m)

)

(m)

as [exp(
), ..., exp(
)]T where {a1 , ..., ap }
are p anchors that are randomly selected from the training samples in the m-th modality, σm is the Gaussian kernel parameter.
(m)
(m)
φ(X(m) ) = [φ(x1 ), ..., φ(xn )] ∈ Rp×n preserves the modalityspecific sample correlations by simply characterizing the correlations between the sample and certain anchors. The computation
complexity of this part is O(mnp).
In multimedia retrieval, the heterogeneous modality gap and
inter-modality redundancy in multi-modal data are detrimental
to hashing learning. In this paper, we propose to collaboratively
M
project the nonlinearly transformed representation φ(X(m) )|m=1
r
×n
into a consensus multi-modal factor H ∈ R
(r is the hash code
length) as the hash code learning basis. Motivated by these considerations, we can formulate this part as

THE PROPOSED METHODOLOGY

Throughout this paper, we utilize uppercase letters to represent
matrices and lowercase letters to represent vectors. Suppose that
n is the training dataset, which contains n training
O = {oi }i=1
samples represented with M different modality features. The m-th
(m)
(m)
M , where
modality feature is {X(m) = [x1 , ..., xn ] ∈ Rdm ×n }m=1
dm is the dimensionality of the m-th modality, M is the number of
modalities. Different modalities of one sample oi share the same
semantic, that is, they belong to the same category. S ∈ {−1, 1}n×n
is a pair-wise semantic matrix. Our method is to learn hash code
B ∈ {−1, 1}r ×n to represent multimedia instances, where r is the

M
Õ

min
µ (m) , W(m) , H

µ (m) ∥H − W(m) φ(X(m) ) ∥F2 + ζ ∥µ ∥F2

m=1
(1)
(2)

de f

length of hash code. ∆n = {x ∈ Rn |x i ≥ 0, 1Tx = 1} is the
probabilistic simplex. The basic framework of the proposed OMHDQ is illustrated in Figure 1.

3.1

Online Multi-modal Hashing with Dynamic Query-adaption

Multi-modal Features

(1)

s .t . µ = [µ , µ , ..., µ (M ) ]T, µ ∈ ∆M

where W(m) ∈ Rr ×p , m = 1, .., M is the projection matrix of the
m-th modality, H ∈ Rr ×n is the consensus multi-modal factor, µ (m)
is the weight of the m-th modality and it measures the importance
of modality feature. With weight setting, complementarity of multimodal features can be exploited properly. Similar to MVDH [18],
we formulate a second term in Eq.(1) to smoothen the weight distribution. ζ > 0 is a hyper-parameter that plays the balance between

Discriminative Multi-modal Projection
Learning

3.1.1 Consensus Multi-modal Feature Mapping. To learn effective
binary projection for multi-modal data, the projected features should
comprehensively preserve multi-modal information. The first task is

717

Session 8B: Hashing

SIGIR ’19, July 21–25, 2019, Paris, France

3.1.2 Asymmetric Supervised Learning. The projection matrices
learned from Eq.(2) only exploit low-level multi-modal information
while have not exploited any semantic labels which have already
demonstrated to be able to impressively improve the performance
on uni-modal hashing [17, 34] and cross-modal hashing [10, 23]. In
this paper, we further leverage explicit semantic labels to guide the
projection learning process and thus to enhance the discriminative
capability of hash codes. Specifically, in this paper, we consider the
pair-wise semantic supervision, motivated by its recent success on
uni-modal [16, 36] and cross-modal hashing [27, 35].
Let B ∈ {−1, 1}r ×n denote the hash code to be learned. Intuitively, it can be learned by preserving the high-level semantic
correlations (described by similarity matrix S) into the hash codes

the regularization terms. Specifically, without this regularization
term (or ζ → 0), the weight of the best modality with the minimum
reconstruction loss will be assigned to 1 and that of other modalities
will be 0. On the other hand, when ζ → ∞, an equal weight will be
assigned to every modality. Under such circumstance, the effects of
weights on exploring the complementarity of multi-modal features
are missing. Therefore, it is advisable for this parameter-weighted
hash learning to involve an additional parameter ζ , whose best
value is confirmed to be data related. In practice, it means that more
time will be consumed on parameter adjustment in the offline hash
learning process. Furthermore, the parameter adjustment requirement is also contradictory to the fact that we cannot manually set
a proper parameter for each query in the online retrieval process.
To address the problem, in this paper, we introduce a virtual
weight and propose a new consensus multi-modal feature mapping
which can achieve the same goal as Eq.(1) without additional hyperparameters. Specifically, we formulate this part as
min
W(m) , H

M
Õ

∥H − W(m) φ(X(m) ) ∥F

min ∥r S − BT B ∥F2 , s .t . B ∈ {−1, 1}r ×n

where S ∈
the pair-wise similarity matrix, its element in
the i-th row and j-th column is defined as

1, if xi and x j belong to the same category,
si j =
−1,

(2)

m=1

Theorem 3.1. Eq.(2) is equivalent to
M
Õ

µ ∈∆M ,W(m),H m=1

1
∥H − W(m)φ(X(m) )∥F2
µ (m)

(3)

Proof. Note that,
M
Õ

1

m=1

µ (m)

(m)

W

M
Õ

1

m=1

µ (m)

(a)

φ(X

∥H − W(m) φ(X(m) ) ∥F2 = (
(m)

M
M
Õ
(b) Õ
) ∥F2 )(
µ (m) ) ≥ (
m=1
m=1

∥H − W

min ∥r S − BT RH ∥F2 + β ∥ B − RH ∥F2 , s.t . B ∈ {−1, 1}r ×n , RT R = Ir

B,R,H

∥H−

(m)

φ(X

(m)

2

) ∥F )

ÍM
where (a) holds since m=1
µ (m) = 1 and (b) holds according to
the Cauchy-Schwarz inequality. This equation indicates
(

M
Õ

∥H − W(m) φ(X(m) ) ∥F )2 = min

µ∈∆ M

m=1

M
Õ
m=1

1
∥H − W(m) φ(X(m) ) ∥F2
µ (m)

It is easy to derive
min

M
Õ

W(m) , H m=1

⇔

min

∥H − W(m) φ(X(m) ) ∥F ⇔ min (

M
Õ

W(m) , H m=1

M
Õ

µ∈∆ M , W(m) , H m=1

∥H − W(m) φ(X(m) ) ∥F )2

1
∥H − W(m) φ(X(m) ) ∥F2
µ (m)

which completes the proof.

vice versa

However, directly solving discrete B in Eq.(4) is very challenging
due to the discrete symmetric factorization, not to mention the fast
hashing optimization. Furthermore, storing the elements of S will
consume O(n 2 ), which is unacceptable in large-scale multimedia
retrieval. In this paper, to avoid these problems, we develop an
asymmetric hashing learning module that transfers semantics from
pair-wise semantic matrix S to hash codes. Specifically, we substitute one of B with the rotated consensus multi-modal factor RH
(R ∈ Rr ×r is rotation matrix) and keep their consistency during
the optimization process. The formula is

where ∥ · ∥F is the Frobenius norm of the matrix. We can derive the
following theorem.

min

(4)

B
Rn×n is



This formulation has two advantages: 1) The symmetric matrix
factorization can be obviously avoided. Only one of the decomposed
variable is imposed with discrete constraint. The second regularization term can guarantee the acceptable information loss. 2) The
learned hash codes can not only reflect the low-level multi-modal
data distribution via H, but also involve the high-level semantics in
S. As shown below, with the support of asymmetric hashing learning, the hash codes can be learned with a simple sgn(·) operation
instead of bit-by-bit discrete optimization as existing discrete multimodal hashing methods. In addition, as shown below, the O(n 2 )
storage cost brought by S can be reduced to O(n) when representing
S with the label matrix in our approach.
3.1.3 Overall Objective Formulation. By integrating the above two
parts into a unified learning framework, we derive the overall objective function of hash code learning in OMH-DQ as
M
Õ

min
W(m) , H, B, R

1 can be considered as a virtual weight
As shown in Eq.(3), µ (m)
which acts as the function of real weight. If the m-th modality is
discriminative, then the value of ∥H − W(m)φ(X(m) )∥F should be
1 of the m-th
small, and thus the corresponding virtual weight µ (m)
modality is large. Similarly, a less discriminative modality will be
assigned a small weight.

m=1

∥H − W(m) φ(X(m) ) ∥F + α ∥r S − BT RH ∥F2

+ β ∥B − RH ∥F2 + δ

M
Õ
m=1

∥W(m) ∥F2

(5)

s .t . B ∈ {−1, 1}r ×n , RT R = Ir

where α, β, and δ are balance parameters. The first term performs consensus multi-modal feature mapping to combine multiple modalities, bridge the heterogeneous modality gap, and avoid

718

Session 8B: Hashing

SIGIR ’19, July 21–25, 2019, Paris, France

where
(a) holds since 1T µ = 1 and the equality in (b) holds when
q
(m)
µ (m) ∝ √h ( . Since the right-hand side of Eq.(3.1.4) is constant,

inter-modality redundancy. The second and the third terms perform asymmetric supervised hashing learning. The last term is a
regularization term to avoid over-fitting.

µ m)

the optimal µ (m) in Eq.(8) can be obtained by

3.1.4 Efficient Discrete Hash Code Optimization. Solving hash codes
is actually an NP-hard problem due to the discrete constraint. It is
always a challenging task from the birth of this technique. Most
existing multi-modal hashing methods [7, 12, 13, 21, 30] adopt twostep relaxing+rounding optimization strategy. They basically solve
the relaxed continuous solutions first and then calculate the binary
hash codes by thresholding. However, this simplified strategy will
lead to significant quantization loss. Although there exist discrete
multi-modal hash methods [18], they learn the hash codes bit-by-bit
with DCC, which is still time-consuming.
In this paper, with the support of objective formulation, we propose to directly learn the discrete multi-modal hash code. Moreover,
different from existing multi-modal hashing methods, we avoid explicitly computing the similarity matrix S, which can achieve linear
computation and storage efficiency.
By Theorem 3.1, Eq.(5) is equivalent to the following problem
min

M
Õ

µ (m) , W(m) , R, H, B m=1

h (m)
µ (m) = ÍM
(m)
m=1 h

Step 2: By fixing other variables, update W(m) . We set the
derivative of objective function with respect to W(m) to zero, and
then we have
W(m) = (

µ (m)

M
Õ
m=1

∥W(m) ∥F2

min tr(−2β RT BHT −2αr RT BSHT +α RT BBT RHHT −λRT (ZR −

RT R=Ir

δ

m=1
M
Õ
m=1

µ (m)

(6)

(11)

i 2

i-th column in the matrix L̃. Then we can get the similarity matrix
T
S̃ = L̃ L̃. The semantic similarity matrix S can be calculated as
T

T
S = 2S̃ − E = 2L̃ L̃ − 1n 1n

(12)

where 1n is an all-one column vector with length n, and E is a
matrix with all elements as 1. Then we can get

GR 2
GB 2
λ
( ∥R − ZR +
∥ + ∥B − ZB +
∥ )
2
λ F
λ F

BSHT = 2(BLT )(HLT )T − (B1n )(H1n )T

Thus the calculation of C can be transformed as

s .t . B, ZB ∈ {−1, 1}r ×n , RT R = Ir , ZTR ZR = Ir , µ ∈ ∆M

C = β BHT + α r BSHT = β BHT + 2α r (BLT )(HLT )T + 2α r (B1n )(H1n )T

(7)

which consumes O(n).
Step 4: By fixing other variables, update H. We set the derivative of objective function with respect to H to zero, and then
we have

where GR ∈ Rr ×r and GB ∈ Rr ×n measure the difference between
the target and auxiliary variables, λ > 0 adjusts the balance between
terms. Specifically, we drive the following iterative optimization
steps to solve Eq.(7).
Step 1: By fixing other variables, update µ (m) . For convenience, we denote ∥H − W(m)φ(X(m) )∥F by h (m) . The original problem can be written as:
M
Õ
h (m)2
min
(m)
µ (m) ≥0, 1T µ=1 m=1 µ

))

where C = βBHT + αr BSHT − αBBT ZR HHT + λZR − GR . The
optimal R is defined as R = PQT , where P and Q are comprised of
left-singular and right-singular vectors of C respectively [40].
Note that, the S ∈ Rn×n is included in the term BSHT when updating R. If we compute S explicitly, the computational complexity
is O(n2 ). In this paper, we utilize c × n matrix L̃ (c is the number
of semantic categories) to store the label information instead of
directly calculating S, and can reduce the computational complexity
to O(n). Let L̃ki = ∥llk i∥ , as the element at the k-th row and the

∥H − W(m) φ(X(m) ) ∥F2 + α ∥r S − BT RH ∥F2 + β ∥B − RH ∥F2 +

∥W(m) ∥F2 +

λ

RT R=Ir

Besides, we propose a new and effective optimization algorithm
based on augmented Lagrangian multiplier (ALM) [9, 15] to solve
the problem in Eq.(6). Our idea is to introduce auxiliary variables
to separate constraints, and transform the objective function to
an equivalent one that can be tackled more easily. Formally, we
introduce two auxiliary variables ZR and ZB , and set ZR = R and
ZB = B. Eq.(6) is transformed as
min

GR

We substitute RT BBT RHHT with RT BBT ZR HHT and the above
equation can be transformed with the following equivalent form

s .t . B ∈ {−1, 1}r ×n , RT R = Ir , µ ∈ ∆M

1

(10)

Step 3: By fixing other variables, update R. The objective
function with respect to R can be represented as

∥H − W(m) φ(X(m) ) ∥F2

+ α ∥r S − BT RH ∥F2 + β ∥B − RH ∥F2 + δ

M
Õ

1
1
Hφ T (X(m) ))( (m) φ(X(m) )φ T (X(m) ) + δ Ip )−1
µ (m)
µ

max tr(RT C)

1

(9)

ÍM
H=

1
(m) φ(X(m) ) + β RT B + α r RT BS
m=1 µ (m) W
ÍM
1
m=1 µ (m) + α + β

(13)

where S is also transformed using Eq.(12), then we have
T
T
RT BS = BSRT = 2(BL̃ )(RL̃ )T + (B1n )(R1n )T

(8)

The time complexity of computing RT BS is reduced to O(n).
Step 5: By fixing other variables, update B. The objective
function with respect to B can be presented as

which combining with Cauchy-Schwarz inequality gives
M
M
M
M
Õ
h (m)2 (a) Õ h (m)2 Õ (m) (b) Õ (m) 2
= (
)(
µ ) ≥ (
h )
(m)
(m)
m=1 µ
m=1 µ
m=1
m=1

min

B∈{−1, 1}r ×n

719

tr(−2α r BT RHS + α BT RHHT RT B − 2β BT RH − λBT ZB + GB )

Session 8B: Hashing

SIGIR ’19, July 21–25, 2019, Paris, France

We substitute BT RHHT RT B with BT RHHT RT ZB . Thus the above
equation is equivalent to
min

B∈ {−1,1}r ×n

As Theorem 3.1, Eq.(18) can be shown to be equivalent to

T

T T

1

(m)

Bq ∈{−1, 1}r ×nq , µ∈∆M m=1 µ q(m)

∥Bq − W(m) φ(Xq ) ∥F2

(19)

To solve Eq.(19), we drive the following iterative steps
(m)
Step 1: By fixing Bq , update µq . As Section 3.1.4, we denote

We can obtain the closed solution of B as
B = sgn(2α r RHST − α RHHT RT ZB + 2β RH + λZB − GB )

M
Õ

min

tr(−B (2αr RHS − α RHH R ZB + 2β RH + λ ZB − GB ))
T

(14)

(m)

(m)

(m)

∥Bq − W(m)φ(Xq )∥F by hq . The optimal µq

where S is also transformed using Eq.(12), then we have

is given by

(m)

T

T
RHST = 2RHL̃ L̃ − RH1n 1n

(m)

µq

The time complexity of computing RHST is reduced to O(n).
Step 6: By fixing other variables, update ZB . We can obtain
the closed solution of ZB as
ZB = sgn(−α RHHT RT B + λB + GB )

(m)

Step 2: By fixing µq , update Bq . We can obtain the closed
solution of Bq as

(15)
Bq = sgn(

Step 7: By fixing other variables, update ZR . The objective
function respect to ZR can be represented as
max tr(ZTR Czr )

3.3

(16)

GB = GB + λ(B − ZB )
(17)

Query-adaptive Online Hashing with
Dynamic Weights

In the online retrieval process, we aim to map the new coming
query instances into binary hash codes with the learned hash proM . All existing multi-modal hashing methjection matrix {W(m) }m=1
ods simply adopt equal or fixed weights obtained from offline hash
code learning to combine multi-modal features. However, the fixed
weights obtained from offline hash code learning cannot capture
the variations of the dynamic queries. This motivates us to develop
query-adaptive online hashing. More importantly, we should avoid
bringing any additional parameter in weighting scheme.
In this paper, with the support of the offline hash code learning
framework, we present a parameter-free query-adaptive online
hashing with a self-weighting scheme. The weights are actually
virtual and the hash codes are iteratively updated online without
any parameters by considering the specific query contents. With
this design, we could obtain more accurate query hash codes for
fast multimedia retrieval while relying less on parameter searching.
Specifically, the objective function of our query-adaptive online
hashing process is formulated as
min
Bq ∈{−1, 1}

W(m)

M
Õ
r ×nq

(m)

∥Bq − W(m) φ(Xq ) ∥F

1

(m)

W(m)φ(Xq ))

Discussions

3.3.1 Complexity Analysis. This section provides the complexity
analysis of OMH-DQ. The time complexity of constructing the nonlinear feature mapping φ(X(m) ) of each modality is O(np). It takes
O(prn) for updating µ (m) . The computational complexity of updating W(m) is O(prn). The computational complexity of updating R
is O(r 2n). Updating B requires O(prn). The computational complexity of updating H is O(rn). Then the computational complexity of
optimization process is O(iter × prn), where iter is the number of
iterations. This process scales linearly with n. At online multimedia
retrieval process, it takes O(iter × prn) to generate binary hash
code for a new query.
In addition, in the discrete optimization process, we avoid explicitly computing the pair-wise similarity matrix S, but substituting
it with the expression of L̃. We successfully reduce the space complexity to O(n). In sum, both the computational and the space
complexity of OMH-DQ are linear with the size of the dataset. Our
approach is scalable for large-scale multimedia retrieval.

where C = −αBBT RT HHT + λR + GR . The optimal ZR is defined
T .
as ZR = Pzr Qzr
Step 8: By fixing other variables, update GB and GR . The
update rules are

3.2

M
Õ

(m)
m=1 µ q

ZTR ZR =Ir

GR = GR + λ(R − ZR )

hq
= Í
(m)
M
m=1 hq

3.3.2 Convergence Analysis. The objective functions Eq.(6) is convex to one variable by fixing the others2 . Thus, optimizing one
variable in each step will lead to a lower or equal value of the objective function. Our iterative updating rules will monotonically
decrease the objective function value. After several iterations, the
optimization process eventually achieves a local minimum. In addition, in experiments, we will empirically verify the convergence of
the proposed OMH-DQ on three benchmark datasets.

4 EXPERIMENTS
4.1 Experimental Configuration
4.1.1 Evaluation Datasets. In this paper, we conduct experiments
on three publicly available multimedia retrieval datasets: MIR Flickr
[6], NUS-WIDE [3] and MS COCO [8]. These three datasets are
widely used for performance evaluation of multi-modal hashing
methods [13, 18, 28]. The statistics of the three datasets used in
experiments are summarized in Table 2.

(18)

m=1

Rr ×p

where
∈
is the linear projection matrix from Eq.(5),
(m)
φ(Xq ) ∈ Rp×nq is the nonlinearly projected representation of the
newly coming query instances, Bq ∈ {−1, 1}r ×nq is the binary hash
code of the newly coming query instances, and nq is the number
of samples of the query set.

2 For

720

optimizing the hash code in Step 4, we can directly obtain a closed-form solution.

Session 8B: Hashing

SIGIR ’19, July 21–25, 2019, Paris, France

Table 2: General statistics of three datasets.
Dataset
MIR Flickr [6]
NUS-WIDE [3]
MS COCO [8]

Training Size
5,000
5,000
18,000

Retrieval Size
17,772
193,749
82,783

Query Size
2,243
2,085
5,981

Categories
24
21
80

Visual Modality
CNN (4,096-D) (by Caffe implementation of VGG Net [20])
CNN (4,096-D) (by Caffe implementation of VGG Net [20])
CNN (4,096-D) (by Caffe implementation of VGG Net [20])

Text Modality
BoW (1,386-D)
BoW (1,000-D)
BoW (2,000-D)

Table 3: MAP comparison results on MIR Flickr, NUS-WIDE and MS COCO.
Methods
MFH [21]
MFKH [13]
MAH [12]
MVLH [19]
MVDH [18]
DMVH [28]
Ours

16 bits
0.5795
0.6369
0.6488
0.6541
0.6828
0.7231
0.7988

MIR Flickr
32 bits 64 bits
0.5824 0.5831
0.6128 0.5985
0.6649 0.6990
0.6421 0.6044
0.7210 0.7344
0.7326 0.7495
0.8047 0.8154

MIR Flickr on 32 bits

0.7

0.9

0.6

0.8

0.6

0.5
1000 2000 3000 4000 5000 6000 7000 8000 9000 10000

Number of top retrieved samples

NUS-WIDE on 32 bits

NUS-WIDE on 128 bits

0.9

0.6
0.5

0.8

Precision

Precision

0.7

0.9
MFH
MFKH
MAH
MVLH
MvDH
DMVH
Our method

0.4

0.7
0.6
0.5
0.4

0.3
1000 2000 3000 4000 5000 6000 7000 8000 9000 10000

Number of top retrieved samples

Number of top retrieved samples

MS COCO on 32 bits

MS COCO on 128 bits

0.9

0.7
0.6

0.9
MFH
MFKH
MAH
MVLH
MvDH
DMVH
Our method

0.5

0.8

Precision

0.8

Precision

MFH
MFKH
MAH
MVLH
MvDH
DMVH
Our method

0.3
1000 2000 3000 4000 5000 6000 7000 8000 9000 10000

0.7
0.6

16 bits
0.3948
0.4216
0.3967
0.3993
0.3978
0.4123
0.5059

MS COCO
32 bits 64 bits
0.3966 0.3960
0.4211 0.4230
0.3943 0.3966
0.4012 0.4065
0.3966 0.3977
0.4288 0.4355
0.5148 0.5263

128 bits
0.3980
0.4229
0.3988
0.4099
0.3998
0.4563
0.5488

4.1.4 Implementation Details. The proposed OMH-DQ has several
parameters: α, β, and δ in Eq.(5), and the number of anchors p. In
the experiments, the best performance of OMH-DQ is achieved
when the number of anchors p is set as 500, 1,200 and 1,000 on
MIR Flickr, NUS-WIDE and MS COCO, respectively. α and β are
balance parameters to support asymmetric supervised learning,
and δ is a regularization parameter to avoid over-fitting. The best
performance is achieved when {α = 10−5 , β = 10−1 , γ = 10−3 },
{α = 101 , β = 105 , γ = 10−5 }, and {α = 10−3 , β = 103 , γ = 10−5 }
on MIR Flickr, NUS-WIDE and MS COCO, respectively. We carefully
tune the parameters of all the baselines and finally report their best
results for performance comparison. On three datasets, we conduct
five successive experiments with different randomly partitioned
datasets and report the average results. All our experiments are
conducted on a workstation with a 3.40 GHz Intel(R) Core(TM)
i7-6700 CPU and 64 GB RAM.

0.7

Number of top retrieved samples

128 bits
0.3569
0.3750
0.5477
0.3772
0.6022
0.6285
0.6744

(MAH) [12], Multi-view Latent Hashing (MVLH) [19], Multi-modal
Discrete Hashing (MVDH) [18], Discrete Multi-modal Hashing
(DMVH) [28]. Those methods have been discussed in Section 2.

MFH
MFKH
MAH
MVLH
MvDH
DMVH
Our method

0.5
1000 2000 3000 4000 5000 6000 7000 8000 9000 10000

0.8

NUS-WIDE
32 bits 64 bits
0.3579 0.3629
0.4323 0.3917
0.4936 0.5371
0.3966 0.3751
0.5533 0.5855
0.5856 0.6063
0.6488 0.6608

1
MFH
MFKH
MAH
MVLH
MvDH
DMVH
Our method

Precision

Precision

0.8

16 bits
0.3556
0.4663
0.4608
0.4277
0.5083
0.5665
0.6378

MIR Flickr on 128 bits

1

0.9

128 bits
0.5836
0.5807
0.7114
0.5982
0.7527
0.7641
0.8184

MFH
MFKH
MAH
MVLH
MvDH
DMVH
Our method

0.5

4.2

0.4

0.4

0.3
1000 2000 3000 4000 5000 6000 7000 8000 9000 10000

0.3
1000 2000 3000 4000 5000 6000 7000 8000 9000 10000

Number of top retrieved samples

Number of top retrieved samples

Comparison Results

4.2.1 Retrieval Accuracy Comparison. The MAP values of all compared methods varying with different hash code lengths (16 bits,
32 bits, 64 bits, and 128 bits) on three datasets (MIR Flickr, NUSWIDE and MS COCO) are presented in Table 3. Their corresponding
topK-precision curves with the increasing number of the retrieved
samples on three datasets are shown in Fig. 2.
From these results, we easily find that the proposed OMH-DQ
consistently outperforms all the compared baselines on three datasets.
For example, when the hash code length is fixed as 64 bits, OMH-DQ
obtains the MAP of 0.8154 on MIR Flickr, 0.6608 on NUS-WIDE, and
0.5263 on MS COCO, while the second best performance is 0.7495
on MIR Flickr, 0.6063 on NUS-WIDE, and 0.4355 on MS COCO. The
topK-precision curves on three datasets demonstrate the similar
trend as the MAP results. From these curves, we can find that, with
the number of retrieved samples increasing, the precision of OMHDQ consistently outperforms the baselines by a large margin. These
results clearly validate the superiority of OMH-DQ over the baselines. In addition, we observe that the performance of OMH-DQ
improves as the code length increases from 16 bits to 128 bits, while
that of the baseline methods, such as MFKH and MVLH, degrade

Figure 2: The topK-precision curves on three datasets.
4.1.2 Evaluation Metrics. We adopt two standard evaluation metrics, Mean Average Precision (MAP) [4, 22] and topK-precision
[18, 40], to evaluate the multimedia retrieval performance. In the
retrieval phase, we adopt Hamming distance to measure the similarity between the binary codes of the query instances and the
ones in multimedia database. Two instances are considered to be
semantically similar when they share at least one semantic tag.
For all the queries, we first calculate their APs and then obtain the
average value as MAP as [22]. topK-precision curves reflect the
change of precision with respect to the number of top-ranked K
instances. For both evaluation protocols, larger value indicates the
better retrieval performance.
4.1.3 Evaluation Baselines. In experiments, we compare the proposed method with six state-of-the-art multi-modal hashing methods, including Multiple Feature Hashing (MFH) [21], Multiple Feature Kernel Hashing (MFKH) [13], Multi-modal Alignment Hashing

721

Session 8B: Hashing

SIGIR ’19, July 21–25, 2019, Paris, France

Table 4: Comparison of training and query time (seconds) on MIR Flickr, NUS-WIDE and MS COCO.
Methods
MFH [21]
MFKH [13]
MAH [12]
MVLH [19]
MVDH [18]
DMVH [28]
OMH-DQ-linear
Ours

MIR Flickr
56.91
37.82
107.72
219.81
1774.94
314.01
6.46
1.14

Training time (s)
NUS-WIDE MS COCO
92.04
591.68
56.26
112.84
149.42
102.98
1313.39
444.89
1004.21
7994.75
968.59
453.62
7.04
34.00
2.11
16.99

MIR Flickr
14.48
14.20
204.64
15.06
930.13
14.37
14.72
10.93

Query time (s)
NUS-WIDE MS COCO
130.78
102.95
106.14
148.59
644.12
1070.34
286.05
158.90
3687.26
2041.58
186.74
168.06
153.11
116.83
135.02
103.66

Table 5: MAP comparison with variants of the proposed method on MIR Flickr, NUS-WIDE and MS COCO.
16 bits
0.4598
0.4160
0.3631
0.4843
0.5714
0.6378

NUS-WIDE
32 bits 64 bits
0.4709 0.4733
0.4278 0.4474
0.3640 0.3644
0.4916 0.4976
0.6129 0.6317
0.6488 0.6608

significantly. This result indicates that longer hash codes can bring
more discriminative information in OMH-DQ. Finally, we can find
that OMH-DQ even obtains better performance with shorter code
length than performance of many baselines with longer code length.
For example, on MIR Flickr, the MAP 0.7988 obtained by OMH-DQ
when the code length is 16 bits is even better than the MAP of
0.7641 obtained by DMVH when the code length is 128 bits.
Four advantages of OMH-DQ bring the improved retrieval precision: 1) OMH-DQ learns the consensus multi-modal factor collaboratively from multi-modal data, which can explore complementarity
of multi-modal features to boost the hash performance. 2) We develop an asymmetric supervised hash learning module, where the
learned hash codes are simultaneously correlated with the low-level
consensus multi-modal factor and the high-level semantic labels to
enhance discriminative capability. 3) The discrete hash code optimization solves hash code directly without quantization errors. 4)
With a query-adaptive online hashing strategy, the hash codes are
adaptively learned according to the specific query contents.

128 bits
0.4562
0.4528
0.3648
0.5009
0.6423
0.6744

16 bits
0.3915
0.4892
0.4652
0.4846
0.4870
0.5059

MIR Flickr

MS COCO
32 bits 64 bits
0.3918 0.3923
0.4932 0.5005
0.4795 0.4822
0.4997 0.5020
0.4946 0.5075
0.5148 0.5263

128 bits
0.3941
0.5037
0.4964
0.5066
0.5210
0.5488

NUS-WIDE

1

MS COCO
0.7

0.9
OMH-DQ-fixed
OMH-DQ-equal
Our method

0.9

OMH-DQ-fixed
OMH-DQ-equal
Our method

0.8

OMH-DQ-fixed
OMH-DQ-equal
Our method

0.65
0.6

mAP

0.7
0.55

0.8

mAP

128 bits
0.6599
0.6855
0.6347
0.7270
0.6232
0.8184

0.6

0.7

0.5
0.45

0.5
0.4
0.6

0.4
0.35

0.5

0.3

0.3
16

32

64

128

16

32

Hash code length

64

16

128

32

64

128

Hash code length

Hash code length

Figure 3: Effects of our query-adaptive online hashing.
Adaptive Weight on MIR Flickr

Adaptive Weight on NUS-WIDE

Weight of the 1st modality
Weight of the 2nd modality

0.7

0.5
0.4

0.9
Weight of the 1st modality
Weight of the 2nd modality

0.65

0.6

Weight

Adaptive Weight on MS COCO

0.7

0.8

0.6

0.7

0.55

0.6

0.5
0.45
0.4

0.3

20

40

60

80

The i-th Query Sample

100

0.4

0.2

0.3
0

0.5

0.3

0.35

0.2

Weight of the 1st modality
Weight of the 2nd modality

0.8

Weight

MIR Flickr
32 bits 64 bits
0.6651 0.6623
0.6811 0.6857
0.6336 0.6340
0.7172 0.7183
0.7516 0.6527
0.8047 0.8154

mAP

OMH-DQ-img
OMH-DQ-txt
OMH-DQ-con
OMH-DQ-linear
OMH-DQ-relax
Ours

16 bits
0.6538
0.6714
0.6325
0.6936
0.7718
0.7988

Weight

Methods

0.1
0

20

40

60

80

The i-th Query Sample

100

0

20

40

60

80

100

The i-th Query Sample

Figure 4: Modality weights adapted to dynamic queries.
thus reduces the complexity of hash code learning. 3) The proposed
discrete hash code optimization method efficiently solves hash code
within a simple operation instead of updating hash codes bit by bit.

4.2.2 Run Time Comparison. In this subsection, we conduct experiments to compare the computational efficiency between OMH-DQ
and baselines. Table 4 presents the comparison of training and
query time on three datasets when code length is fixed to 128 bits.
From it we can observe that the proposed OMH-DQ consumes the
least time at the training stage and comparable time at the query
stage. The training time of OMH-DQ is 16.99 seconds on MS COCO,
while that of the second best competitor MAH is 102.98 seconds,
the advantage of our method is very significant. At the online stage,
although OMH-DQ adaptively learns hash codes of the new queries,
this process has not degraded the query efficiency. It can achieve
comparable or better (on MIR Flickr) performance than the second
best ones.
The high efficiency of the OMH-DQ is mainly attributed to three
reasons: 1) With the nonlinear feature mapping, OMH-DQ efficiently preserves the multi-modal features into hash codes with
linear computational complexity. 2) Asymmetric hashing learning
avoids direct decomposition of the pair-wise similarity matrix and

4.3

Ablation Analysis

4.3.1 Is the weighting scheme effective? In our method, query hash
codes are adaptively learned in an online mode. The designed virtual
weights for modality combination can capture the variations of the
queries, and thus boost the retrieval performance. To evaluate its
effectiveness, we design two variants of our method: 1) OMH-DQfixed: It adopts fixed modality combination weights obtained from
the offline learning to generate the query hash codes. 2) OMH-DQequal: It fixes the weight of each modality to 1 at both the offline
learning and online hashing phases. Fig. 3 shows the comparison
of the retrieval performance. From the figures, we can find that the
performance of our method is obviously higher than that of the
other two variants. Moreover, we show that our proposed model
adaptively learns weight according to the dynamic query contents
in Fig. 4.
4.3.2 Is self-weighted multi-modal fusion effective? In this paper,
we propose a self-weighted multi-modal fusion strategy to preserve

722

Session 8B: Hashing

SIGIR ’19, July 21–25, 2019, Paris, France

is fixed to 10-1
0.8

0.6

0.6

0.2

mAP

0.8

0.6
0.4

0.4
0.2

0

0

1e-05
0.001
0.1
10
1000
100000

!

0.4
0.2
0

1e-05
0.001
0.1
100000
10
1000
1000
10
100000
0.1
0.001
1e-05

1e-05
0.001
0.1
100000
10
1000
1000
10
100000
0.1
0.001
1e-05

!

"

obtains binary codes by mean-thresholding. The objective function of OMH-DQ-relax is similar to our method as Eq.(5) and the
difference is on the updating rule of the hash code matrix B as
B = (βIr +αRHHT RT )−1 (βRH +αr RHST ). The final binary hash
codes B are obtained by mean-thresholding. The optimization of
other variables is similar to that of our proposed OMH-DQ. The retrieval performance of OMH-DQ-relax and our method are shown
in Table 5. We can observe that the performance of our proposed
method is obviously better than that of OMH-DQ-relax on three
datasets. With discrete optimization, the quantization loss can be
effectively reduced and thus the retrieval performance can be improved.

is fixed to 10-5

0.8

mAP

mAP

is fixed to 10-3

0.1
0.001
1e-05

!

"

10

100000
1000

"

Figure 5: Parameter variations on MIR Flickr.
1012

2.622

Convergence on MIR Flickr

Convergence on NUS-WIDE

2.616
2.614
2.612

2.578
2.576
2.574
2.572

2.61
2.608
5

10

15

Number of iterations

20

3
2.99
2.98
2.97

2.57
0

Convergence on MS COCO

3.01

Objective value

2.618

1016

3.02

2.58

Objective value

Objective value

1015

2.582

2.62

4.3.5 Convergency and Parameter Sensitivity Analysis. We conduct
empirical experiments to observe the performance variations with
the involved parameters α, β, δ . We report the results on MIR Flickr
when the code length is fixed to 32 bits. Similar results can be found
on other code lengths and datasets. Since α, β and δ are equipped
in the overall objective function, we vary the value of them from
the range of {10−5 , 10−3 , 10−1 , 10, 103 , 105 } while fixing the others.
Detailed experimental results are presented in Fig.5. From Fig.5(a),
(b), (c), we can find that the performance is relatively stable when
α is in the range of {10−5 , 10−3 }, β is in the range of {10−1 , 101 },
and δ is in the range of {10−5 , 10−3 , 10−1 , 101 }.
In addition, we conduct convergency analysis on MIR Flickr,
NUS-WIDE and MS COCO with the code length as 64 bits. The
convergency curves are shown in Fig. 6. The performance on other
code lengths is similar. We can observe from the figures that, the updating of variables monotonically decreases the objective function
value and eventually reaches a local minimum at each iteration.

2.96
0

5

10

15

Number of iterations

20

0

5

10

15

20

Number of iterations

Figure 6: Convergence analysis on three datasets.
the multi-modal data into hash codes while exploring their complementarity. To evaluate its effectiveness, we design three variants of
our method: 1) OMH-DQ-img: It only inputs the visual modality
feature into the hashing model. 2) OMH-DQ-txt: It only inputs the
textual modality feature into the hashing model. 2) OMH-DQ-con:
It simply concatenates multi-modal features as a unified vector
input and then imports it into the hashing model. The performance
comparison is shown in Table 5. We can observe that, on three
datasets, our method performs better than the other three variants. By exploring the complementarity of multi-modal features,
the self-weighted multi-modal fusion can improve the retrieval
performance.
4.3.3 How about the nonlinear feature mapping on the performance
improvement? OMH-DQ nonlinearly projects features of different
modalities into a new representation based on the anchors, which
help to reduce the computational complexity of hash code learning
to O(n). In addition, it can effectively characterize the nonlinear
instance relations and thus has stronger description capability than
original feature. To validate its effectiveness, we design a variant
of our method named OMH-DQ-linear for comparison. OMH-DQlinear simply imports the original features of different modalities
into the hashing model. The retrieval performance comparison
between OMH-DQ-linear and our method is shown in Table 5.
We can find that our method achieves superior retrieval precision
than OMH-DQ-linear. Besides, we compare the training and query
efficiency of OMH-DQ-linear and our method on three datasets
when the code length is fixed on 128 bits and show the results
in Table 4. We can find that without nonlinear feature mapping,
both the training and query efficiency are reduced. On MS COCO,
OMH-DQ-linear spends 116.83 seconds at query phase, which takes
13.17 seconds more than our method. These results demonstrate
that the nonlinear feature mapping can reduce the computational
complexity and improve the retrieval precision.

5

CONCLUSION

In this paper, we propose a novel Online Multi-modal Hashing with
Dynamic Query-adaption (OMH-DQ) to support large-scale multimedia retrieval. We formulate a unified hash code learning model
that can enhance the discriminative capability of hash codes, avoid
the challenging symmetric similarity matrix factorization, and facilitate efficient (computation and storage) discrete hash code optimization. The discrete hash codes are learned directly without
relaxing quantization information loss. More importantly, we design a query-adaptive online hashing module, so that the generated
query hash codes can capture the varied query contents. Experiments on several public multimedia retrieval datasets demonstrate
the superiority of the proposed approach.

6

ACKNOWLEDGMENTS

The work is partially supported by the National Natural Science
Foundation of China (Nos. U1836216, 61802236) and the Key Research and Development Foundation of Shandong Province (Nos.
2017GGX10117, 2017CXGC0703), in part by the Natural Science
Foundation of Shandong, China (No. ZR2019QF002).

4.3.4 Can discrete hash code optimization avoid quantization errors?
We propose an efficient discrete optimization method to directly
learn binary hash codes instead of the relaxing+rounding optimization strategy commonly used in the previous works. To validate its
effects, we design a variant of our method named OMH-DQ-relax
for comparison. During hash code optimization, OMH-DQ-relax
firstly relaxes the discrete constraints to learn hash codes and then

REFERENCES
[1] Micael Carvalho, Rémi Cadène, David Picard, Laure Soulier, Nicolas Thome, and
Matthieu Cord. 2018. Cross-Modal Retrieval in the Cooking Context: Learning
Semantic Text-Image Embeddings. In SIGIR. 135–44.
[2] Suthee Chaidaroon, Travis Ebesu, and Yi Fang. 2018. Deep Semantic Text Hashing
with Weak Supervision. In SIGIR. 1109–1112.

723

Session 8B: Hashing

SIGIR ’19, July 21–25, 2019, Paris, France

SIGMOD. 785–796.
[23] Di Wang, Xinbo Gao, Xiumei Wang, and Lihuo He. 2018. Label Consistent Matrix
Factorization Hashing for Large-Scale Cross-Modal Similarity Search. TPAMI
(2018).
[24] Jingdong Wang, Ting Zhang, Jingkuan Song, Nicu Sebe, and Heng Tao Shen.
2018. A Survey on Learning to Hash. TPAMI 40, 4 (2018), 769–790.
[25] Liang Xie, Jialie Shen, Jungong Han, Lei Zhu, and Ling Shao. 2017. Dynamic
Multi-View Hashing for Online Image Retrieval. In IJCAI. 3133–3139.
[26] Liang Xie, Jialie Shen, and Lei Zhu. 2016. Online Cross-Modal Hashing for Web
Image Retrieval. In AAAI. 294–300.
[27] Erkun Yang, Cheng Deng, Wei Liu, Xianglong Liu, Dacheng Tao, and Xinbo Gao.
2017. Pairwise Relationship Guided Deep Hashing for Cross-Modal Retrieval. In
AAAI. 1618–1625.
[28] Rui Yang, Yuliang Shi, and Xin-Shun Xu. 2017. Discrete Multi-view Hashing for
Effective Image Retrieval. In ICMR. 175–783.
[29] Dongqing Zhang and Wu-Jun Li. 2014. Large-Scale Supervised Multimodal
Hashing with Semantic Correlation Maximization. In AAAI. 2177–2183.
[30] Dan Zhang, Fei Wang, and Luo Si. 2011. Composite hashing with multiple
information sources. In SIGIR. 225–234.
[31] Hanwang Zhang, Fumin Shen, Wei Liu, Xiangnan He, Huanbo Luan, and Tat-Seng
Chua. 2016. Discrete Collaborative Filtering. In SIGIR. 325–334.
[32] Hanwang Zhang, Meng Wang, Richang Hong, and Tat-Seng Chua. 2016. Play
and Rewind: Optimizing Binary Representations of Videos by Self-Supervised
Temporal Hashing. In MM. 781–790.
[33] Hanwang Zhang, Na Zhao, Xindi Shang, Huan-Bo Luan, and Tat-Seng Chua.
2016. Discrete Image Hashing Using Large Weakly Annotated Photo Collections.
In AAAI. 3669–3675.
[34] Peichao Zhang, Wei Zhang, Wu-Jun Li, and Minyi Guo. 2014. Supervised hashing
with latent factor models. In SIGIR. 173–182.
[35] Xi Zhang, Siyu Zhou, Jiashi Feng, Hanjiang Lai, Bo Li, Yan Pan, Jian Yin, and
Shuicheng Yan. 2017. HashGAN: Attention-aware Deep Adversarial Hashing for
Cross Modal Retrieval. CoRR abs/1711.09347 (2017).
[36] Han Zhu, Mingsheng Long, Jianmin Wang, and Yue Cao. 2016. Deep Hashing
Network for Efficient Similarity Retrieval. In AAAI. 2415–2421.
[37] Lei Zhu, Zi Huang, Xiaojun Chang, Jingkuan Song, and Heng Tao Shen. 2017.
Exploring Consistent Preferences: Discrete Hashing with Pair-Exemplar for
Scalable Landmark Search. In MM. 726–734.
[38] Lei Zhu, Zi Huang, Zhihui Li, Liang Xie, and Heng Tao Shen. 2018. Exploring Auxiliary Context: Discrete Semantic Transfer Hashing for Scalable Image
Retrieval. TNNLS 29, 11 (2018), 5264–5276.
[39] Lei Zhu, Jialie Shen, Xiaobai Liu, Liang Xie, and Liqiang Nie. 2016. Learning Compact Visual Representation with Canonical Views for Robust Mobile Landmark
Search. In IJCAI. 3959–3967.
[40] Lei Zhu, Jialie Shen, Liang Xie, and Zhiyong Cheng. 2017. Unsupervised visual
hashing with semantic assistant for content-based image retrieval. TKDE 29, 2
(2017), 472–486.

[3] Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping Luo, and Yantao Zheng. 2009. NUS-WIDE: a real-world web image database from National
University of Singapore. In CIVR. 48.
[4] Guiguang Ding, Yuchen Guo, and Jile Zhou. 2014. Collective matrix factorization
hashing for multimodal data. In CVPR. 2075–2082.
[5] Yunchao Gong, Svetlana Lazebnik, Albert Gordo, and Florent Perronnin. 2013.
Iterative quantization: A procrustean approach to learning binary codes for
large-scale image retrieval. TPAMI 35, 12 (2013), 2916–2929.
[6] Mark J. Huiskes and Michael S. Lew. 2008. The MIR flickr retrieval evaluation. In
SIGMM. 39–43.
[7] Saehoon Kim and Seungjin Choi. 2013. Multi-view anchor graph hashing. In
ICASSP. 3123–3127.
[8] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014. Microsoft coco: Common
objects in context. In ECCV. 740–755.
[9] Zhouchen Lin, Minming Chen, and Yi Ma. 2010. The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices. arXiv preprint
arXiv:1009.5055 (2010).
[10] Zijia Lin, Guiguang Ding, Jungong Han, and Jianmin Wang. 2017. Cross-view
retrieval via probability-based semantics-preserving hashing. TCYB 47, 12 (2017),
4342–4355.
[11] Han Liu, Xiangnan He, Fuli Feng, Liqiang Nie, Rui Liu, and Hanwang Zhang.
2018. Discrete Factorization Machines for Fast Feature-based Recommendation.
In IJCAI. 3449–3455.
[12] Li Liu, Mengyang Yu, and Ling Shao. 2015. Multiview alignment hashing for
efficient image search. TIP 24, 3 (2015), 956–966.
[13] Xianglong Liu, Junfeng He, Di Liu, and Bo Lang. 2012. Compact kernel hashing
with multiple features. In ACM MM. 881–884.
[14] Fuchen Long, Ting Yao, Qi Dai, Xinmei Tian, Jiebo Luo, and Tao Mei. 2018. Deep
Domain Adaptation Hashing with Adversarial Learning. In SIGIR. 725–734.
[15] Katta G. Murty. 2013. Nonlinear Programming: Theory and Algorithms (3rd ed.).
Wiley Publishing.
[16] Fumin Shen, Xin Gao, Li Liu, Yang Yang, and Heng Tao Shen. 2017. Deep
Asymmetric Pairwise Hashing. In ACM MM. 1522–1530.
[17] Fumin Shen, Chunhua Shen, Wei Liu, and Heng Tao Shen. 2015. Supervised
discrete hashing. In CVPR. 37–45.
[18] Xiaobo Shen, Funmin Shen, Liliu, Yunhao Yuan, Weiwei Liu, and Quansen Sun.
2018. Multiview Discrete Hashing for Scalable Multimedia Search. ACM TIST 9,
5 (2018), 53.
[19] Xiaobo Shen, Fumin Shen, Quan-Sen Sun, and Yunhao Yuan. 2015. Multi-view
latent hashing for efficient multimedia search. In ACM MM. 831–834.
[20] Karen Simonyan and Andrew Zisserman. 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition. CoRR abs/1409.1556 (2014).
[21] Jingkuan Song, Yi Yang, Zi Huang, Heng Tao Shen, and Jiebo Luo. 2013. Effective
multiple feature hashing for large-scale near-duplicate video retrieval. TMM 15,
8 (2013), 1997–2008.
[22] Jingkuan Song, Yang Yang, Yi Yang, Zi Huang, and Heng Tao Shen. 2013. Intermedia hashing for large-scale retrieval from heterogeneous data sources. In

724

