Short Research Papers 3A: AI, Mining, and others

SIGIR ’19, July 21–25, 2019, Paris, France

Training Streaming Factorization Machines with Alternating
Least Squares
Xueyu Mao∗

xmao@cs.utexas.edu
The University of Texas at Austin
Austin, TX

Saayan Mitra

smitra@adobe.com
Adobe Research
San Jose, CA

Factorization Machines (FM) have been widely applied in industrial applications for recommendations. Traditionally FM models
are trained in batch mode, which entails training the model with
large datasets every few hours or days. Such training procedure
cannot capture the trends evolving in real time with large volume
of streaming data. In this paper, we propose an online training
scheme for FM with the alternating least squares (ALS) technique,
which has comparable performance with existing batch training
algorithms. We incorporate an online update mechanism to the
model parameters at the cost of storing a small cache. The mechanism also stabilizes the training error more than a traditional online
training technique like stochastic gradient descent (SGD) as data
points come in, which is crucial for real-time applications. Experiments on large scale datasets validate the efficiency and robustness
of our method.

CCS CONCEPTS
• Computing methodologies → Factorization methods.

KEYWORDS
Factorization Machines, Online Learning, Alternating Least Squares
ACM Reference Format:
Xueyu Mao, Saayan Mitra, and Sheng Li. 2019. Training Streaming Factorization Machines with Alternating Least Squares. In Proceedings of the
42nd International ACM SIGIR Conference on Research and Development in
Information Retrieval (SIGIR ’19), July 21–25, 2019, Paris, France. ACM, New
York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331374

INTRODUCTION

Context aware recommender systems [13] are increasingly employed to provide personalized recommendations to users or to
predict the reactions of users to a given subject or situation. Many
such recommender systems employ factorization machines (FM)
[10] which combines the advantages of both a regression model
and matrix factorization. In recent years, many variants of FMs
∗ This

sheng.li@uga.edu
University of Georgia
Athens, GA

have been proposed. Field-aware FM leverages the field information
associated with each feature, and it usually outperforms the original FM in many real-world evaluations [6]. Robust FM utilizes the
robust optimization framework, and achieves stable performance
under interval uncertainty [9]. FMs are typically trained in batch
mode, where the prediction model is trained on large datasets every
few hours or days. In these systems, the models can become stale
and fail to capture trends as they are evolving in real-time. This
can be a particular problem in use cases such as sports or news,
where it is important to react quickly to shifting trends to make
relevant recommendations based on the current viewing patterns
observed over large number of users. Improving the system performance of real-time stream recommendations has become an active
research topic from the database perspective, but mainly focuses on
the item-based collaborative filtering [5]. Another disadvantage of
batch mode training is the significant processing time and storage
required to deal with large scale datasets. To this end, it would be
desirable to have a system that is capable of training and updating
the FM in an efficient, incremental, and continuous fashion, based
on each newly received data point from a stream of data, without the need to perform extensive re-calculations based on large
volumes of previously received historical data. Such a system can
enable the FM to capture dynamic information from streaming data
sources in a real-time fashion by performing a single processing
pass on each data point as soon as it arrives.
Existing FM training methods include: 1) stochastic gradient
descent [11], which can be easily implemented in an online setting, however, the learning rate needs to be tuned and may need
multiple epochs to converge; 2) tuning-free coordinate descent,
which includes (a) alternating least squares (ALS) [12], (b) efficient
training for FM using ANOVA kernel [1] which needs cache scaling
with number of data points in the dataset; 3) MCMC [11], which is
usually time consuming and it is usually hard to pick a consistently
good model as it uses ensembles for prediction. [7] proposed a provable one-pass algorithm for generalized FMs and rank-one matrix
sensing using alternating minimization, however, it has strong assumptions that the data are sampled from random Gaussian vectors
and is computationally expensive. In [2], the authors propose a fast
learning method when FM is used for binary classification, and
the objective function is modified to make it multi-block convex.
Recently, some methods that combine the merits of deep learning
and FM have been proposed, such as DeepFM [3], neural factorization machines [4] and attentional factorization machines [15].
Like many other deep learning approaches, these FM based deep
learning methods mainly use gradient descent for model training.
In this work, we propose an algorithm based on ALS for fast online update of FM model with streaming data. We give the detailed

ABSTRACT

1

Sheng Li∗

work was done while the authors were at Adobe Research.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’19, July 21–25, 2019, Paris, France
© 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00
https://doi.org/10.1145/3331184.3331374

1185

Short Research Papers 3A: AI, Mining, and others

SIGIR ’19, July 21–25, 2019, Paris, France

derivation of the algorithm and validate on large scale simulated
and real industrial data.

Also, it is easy to show that
X
(e (x,y) − θ |ti h (θ ) (x))h (θ ) (x)
(x,y ) ∈S |ti

2 PRELIMINARIES
2.1 Model

*
= −θ |ti −1 ..

We focus on degree-2 FM in this work, which was proposed by
Rendle [10]. The model equation is given by:
ŷ(x) = w 0 +

d
X

wi xi +

i=1

d X
d
X

+
h 2(θ ) (x) + λ (θ ) // + e (x|ti ,y|ti )h (θ ) (x|ti )

, (x,y ) ∈S |ti X
+

(x,y ) ∈S |ti

⟨vi , vj ⟩x i x j ,

(θ |ti −1 h 2(θ ) (x) − θ |ti h 2(θ ) (x)).

(3)

Now, combining Equations (2) and (3), we have,
e (x|ti ,y|ti )h (θ ) (x|ti )
θ |ti =θ |ti −1 − P
2
(x,y ) ∈S |ti h (θ ) (x) + λ (θ )
P
2
2
(x,y ) ∈S |ti (θ |t i −1 h (θ ) (x) − θ |t i h (θ ) (x))
−
P
2
(x,y ) ∈S |ti h (θ ) (x) + λ (θ )

i=1 j=i+1

where, x ∈ Rd is a feature vector representation of a data point.
w 0 ∈ R, w ∈ Rd , V = [v1 , · · · , vd ]T ∈ Rd×k are model parameters,
d is the dimension of the input feature vector, and k ∈ Z+ is the
rank of the factorization. ⟨vi , vj ⟩ is the inner product of two latent
factors, which models the interaction between the i-th and j-th
variable.

2.2

X

≈θ |ti −1 − P

ALS update for FM

e (x|ti ,y|ti )h (θ ) (x|ti )
(x,y ) ∈S |ti

h 2(θ ) (x) + λ (θ )

.

(4)

As pointed in [12], the objective function of the FM model is linear
in each parameter (w 0 , w i , Vi j ) given the others are fixed. Let θ ∈ Θ
be a parameter, then the FM model can be written as: ŷ(x|θ ) =
д(θ ) (x) + θh (θ ) (x). For example, if θ = w 0 , we have h (w 0 ) (x) = 1
P
P P
and д(w 0 ) (x) = di=1 w i x i + di=1 dj=i+1 ⟨vi , vj ⟩x i x j . When we use
P
P
square loss L = (x,y ) ∈S (ŷ(x)−y) 2 + θ λ (θ ) θ 2 , this linear function
has a closed form optimal solution:
P
(x,y ) ∈S (д (θ ) (x) − y)h (θ ) (x)
θ =− P
,
(1)
2
(x,y ) ∈S h (θ ) (x) + λ (θ )

Equation (4) gives the update rule for any parameter θ at time ti
given a new data point (x|ti ,y|ti ). The updates rules for specific
parameters of FM models can then be derived as following:
e (x|ti ,y|ti )
,
(5)
w 0 |ti =w 0 |ti −1 −
S |ti + λ (θ )
e (x|ti ,y|ti )xl |ti
wl |ti =wl |ti −1 − P
,
(6)
2
(x,y ) ∈S |ti x l + λ (w l )

where S is the set of all the data points and λ (θ ) is the regularization
parameter for θ . [12] proposed an alternating least squares based
algorithm for FM training using Equation (1), which forms the basis
for our method.

where |S |ti | denotes the number of data points received at time ti ,
xl is the l-th element of vector x, and
n
X
h (Vl f ) (x) = xl
Vl f xl − xl2Vl f = xl q(x, f ) − xl2Vl f ,
(8)

3

with

Vl f |ti =Vl f |ti −1 − P

e (x|ti ,y|ti )h (Vl f ) (x|ti )
(x,y ) ∈S |ti

h 2(V ) (x) + λ (Vl f )
lf

,

(7)

l =1

ONLINE ALS UPDATE FOR FM

We consider a streaming setting where data points come one by
one: (x|t1 ,y|t1 ), (x|t2 ,y|t2 ), · · · . Let the model parameters at time ti
be w 0 |ti , w|ti , V|ti . Our premise is that the data points can only
be used once for training, and then thrown away. We use S |ti to
denote all the data points that has been seen at time ti .
In an online setting, for normal ALS training, in each epoch, at
step i (time ti ), it is ideal to update the model using all the data
points that have been seen:
P
(x,y ) ∈S |ti (д (θ ) (x) − y)h (θ ) (x)
θ |ti = − P
2
(x,y ) ∈S |ti h (θ ) (x) + λ (θ )
P
(x,y ) ∈S |ti (e (x,y) − θ |t i h (θ ) (x))h (θ ) (x)
=−
,
(2)
P
2
(x,y ) ∈S |ti h (θ ) (x) + λ (θ )

q(x, f ) =

n
X

Vl f xl

(9)

i=l

as shown in [12]. We can see that, in order to update efficiently, we
need to store some cache values. For linear term updates, ∀l ∈ [n],
P
we need al = (x,y ) ∈S xl2 ; for interaction terms, ∀l ∈ [n], f ∈ [k],
P
we need Bl f = (x,y ) ∈S h 2(V ) (x). With these update rules, our
lf

online FM training algorithm is summarized in Algorithm 1. Note
that there is no learning rate required for this algorithm.
It is easy to see that the memory cost for the cache is (k + 1)n,
which is independent of the number of data points. Note that as x
is typically sparse in real data, we can always check if xl is 0 and
skip the iteration if it is so. Hence, the computation cost for each
while loop is O (km), where m is the number of non-zero elements
in x. For batch ALS, as the whole training set will be used for
updating the parameters, the time complexity is O (km S ), where
m S is the number of non-zero elements in the whole training set.
For large scale datasets, m S is much larger than m, so our algorithm
yields fast implementation and saves a huge computation overhead
compared to batch training.

where e (x,y) = ŷ(x) − y, which is the error term for the prediction
of data point x. Note that at step ti−1 , we also have:
P
(x,y ) ∈S |ti −1 (e (x,y) − θ |t i −1 h (θ ) (x))h (θ ) (x)
θ |ti −1 = −
.
P
2
(x,y ) ∈S |ti −1 h (θ ) (x) + λ (θ )

1186

Short Research Papers 3A: AI, Mining, and others

SIGIR ’19, July 21–25, 2019, Paris, France

Algorithm 1 Online-ALS for FM

• Training Error: RMSE for the model at time t N on the entire training set, i.e., use the final model to recalculate the
prediction for each data point

Require: Data stream (x|ti ,y|ti ), i = 1, 2, · · ·
Ensure: FM model parameters w 0 , w, V; cache a, B
1: Initialize w 0 |t 0 , w|t 0 , V|t 0 from Gaussian samples;
2: Initialize cache al and Bl f to 0, ∀l ∈ [n], f ∈ [k]
3: while data point (x|t i ,y|t i ) arrives do
4:
e (x|ti ,y|ti ) = ŷ(x|ti ) − y|ti
5:
Update w 0 using Equation (5)
6:
e (x|ti ,y|ti ) = e (x|ti ,y|ti ) + (w 0 |ti − w 0 |ti −1 )
7:
for l ∈ [n] do
8:
al = al + xl2 |ti
9:
Update wl using Equation (6)
10:
e (x|ti ,y|ti ) = e (x|ti ,y|ti ) + (wl |ti − wl |ti −1 )xl |ti
11:
end for
12:
for f ∈ [k] do
13:
Calculate q(x|ti , f ) using Equation (9)
14:
for i ∈ [n] do
15:
Calculate h (Vl f ) (x|ti ) using Equation (8)
16:
Bl f = Bl f + h 2(V ) (x|ti )

Note that True-training Error is more important for a streaming
setting, as after updating the model we will dispose of the data
point and there is no way to reuse it. It also measures the quality
of recommendations in a real-time system.

4.1

lf

20:
21:
22:

Training Error

0.24
0.22

0.28

Online-ALS
Online-ALS-pretrained
SGD

0.25

0.26

Online-ALS
Online-ALS-pretrained

0.24
0.22

0.2
0.2
0.18
0.16

0.15

0.2
0.18
0.16

0.1
0.14

0.14

0.12

0.12
0.05

0.1

0.1

0.08

0
0

10

20

30

40

50

60

70

80

90

100

0.08
0

10

Percentage of Data used

It is notable that although Algorithm 1 starts with random initialization, if we have a pre-trained FM model, we can skip step 1 in the
algorithm (or even step 2 if the cache values have been saved). This
is useful in a real world setting because one can always use batch
training on a small set of historical data to get a reasonably good
model, and then update the model in a streaming setting to capture
the real-time trends, and get a good accuracy without re-training
using the large dataset. Similarly, a rolling bootstrap dataset from
recent past can be used to initialize from time to time.

4

True Training Error

0.3

Online-ALS
Online-ALS-pretrained
SGD

0.26

RMSE

19:

Test Error
0.28

RMSE

18:

Update Vl f using Equation (7)
e (x|ti ,y|ti ) = e (x|ti ,y|ti ) + (Vl f |ti − Vl f |ti −1 )xl |ti
q(x|ti , f ) = q(x|ti , f ) + (Vl f |ti − Vl f |ti −1 )xl |ti
end for
end for
end while

RMSE

17:

Simulations

We first evaluate our method with simulated data. We generate
the ground-truth FM model with random samples drawn from a
Gaussian, and then simulate data from this FM, with perfect label.
The size of the simulated dataset is 1.5M for training, and 171K for
testing. The minimum target value is -0.986732, and the maximum
is 1.25055. The result is shown in Fig. 1, we can see that SGD has
faster convergence on test error, but Online-ALS-pretrained is much
better because of the pre-trained procedure. In terms of training
errors, Online-ALS converges faster than SGD. And we can see,
pre-training is beneficial for both training and true training error.

20

30

40

50

60

70

80

90

100

Percentage of Data used

0

10

20

30

40

50

60

70

80

90

Percentage of Data used

Figure 1: Simulated Data

4.2

Real data

In this section, we present the results on large scale industrial
datasets. We use the feature selection framework in [8] to select
about 10 representative features from hundreds of them to accelerate the training procedure. The target (y) of the dataset is session
progress introduced in [14], which is a real valued number in [0, 1].
Session progress is an implicit measure of engagement of the user
with the video, which measures how much of the video was watched.
We also use RMSE to measure the accuracy of the prediction.

EXPERIMENTS

In this section, we will present our experimental evaluation of
Algorithm 1. Besides Online-ALS, we also consider Online-ALSpretrained, which uses 10% of data to pre-train the model in batch
fashion, and then do online updating as in Algorithm 1. By default
we use ALS for pre-training. We compare Online-ALS, Online-ALSpretrained and SGD with step size 0.01, which performs consistently
well across datasets we have used.
Our evaluation metric is root-mean-square error (RMSE), suppose there are N data points for evaluation, then it is defined as
v
u
t
N
1 X
RMSE =
(ŷ(x|ti ) − y|ti )) 2
N i=1

Table 1: Statistics of datasets.
Dataset
1
2
3
2-subset

Training size
61K
1.65M
154K
36K

Test size
6.8K
61K
17K
4K

Feature Selection
Yes
Yes
Yes
No

We use real industrial datasets which are time-ordered for both
training and test. Unlike the simulation experiments, the data points
can be dependent with each other and the underlying model may
not necessarily be a perfect FM model. The statistics of datasets
we are using are shown in Table 1 but we omit the names of the
datasets to preserve confidentiality. 2-subset is a subset of dataset
2, but without applying any feature selection technique.

We also use two kind of training error to evaluate the model:
• True-training Error: RMSE of training error without reloading the model, i.e., prediction for data point x|ti will not be
adjusted after ti .

1187

100

Short Research Papers 3A: AI, Mining, and others

Training Error

Test Error
Online-ALS
Online-ALS-pretrained
SGD

0.8

0.7

Table 2: Test errors for training with multiple passes of the
data

True Training Error

0.9

0.85

0.75

SIGIR ’19, July 21–25, 2019, Paris, France

0.85

Online-ALS
Online-ALS-pretrained
SGD

0.8

0.7

Online-ALS
Online-ALS-pretrained

0.8

0.75

0.6
0.55

RMSE

RMSE

RMSE

0.7
0.65

0.6

0.5

0.65

Dataset

0.6

0.55
0.5

0.4
0.5

0.45
0.3

0.45

0.4
0.35

0.2
0

10

20

30

40

50

60

70

80

90

100

1

0.4
0

10

20

Percentage of Data used

30

40

50

60

70

80

90

100

0

10

20

Percentage of Data used

30

40

50

60

70

80

90

100

Percentage of Data used

Figure 2: Dataset 1
Test Error

True Training Error
0.65

0.65

0.6

Online-ALS
Online-ALS-pretrained
SGD

0.55

Online-ALS
Online-ALS-pretrained
SGD

0.6

0.55

3

Online-ALS
Online-ALS-pretrained

0.6

0.55

RMSE

0.45

RMSE

0.5

0.5

RMSE

2

Training Error

0.45

0.4

0.5

2-subset

0.45

0.35

0.4

0.4
0.3
0.35

Iteration
1
80
1
80
1
80
1
80

Batch ALS
0.399962
0.409616
0.353463
0.371093
47.6281
54.5406
0.375003
0.370772

MCMC
0.415421
0.382334
0.358274
0.312613
45.9093
43.1982
0.517774
0.344105

SGD
0.407748
0.401594
0.344176
0.316464
42.6463
43.1189
0.549916
0.470411

0.35
0.25

0.3

0.2
0

10

20

30

40

50

60

70

80

90

100

0.3
0

10

20

Percentage of Data used

30

40

50

60

70

80

90

100

0

10

20

Percentage of Data used

30

40

50

60

70

80

90

100

Percentage of Data used

5

Figure 3: Dataset 2
Test Error

Training Error

Online-ALS
Online-ALS-pretrained
SGD
Online-ALS-pretrained-w-MCMC

RMSE

75
70

85

70

65
60
55

Online-ALS
Online-ALS-pretrained
Online-ALS-pretrained-w-MCMC

80

Online-ALS
Online-ALS-pretrained
SGD
Online-ALS-pretrained-w-MCMC

80

RMSE

80

75
70

RMSE

85

In this paper, we introduced an algorithm for online FM model
training with streaming data. It has comparable results with batch
training and has much more stable training error than SGD. We validated the efficiency and robustness of our algorithm with extensive
experiments on simulated and industrial datasets.

True Training Error

90

90

60

50

65
60
55
50

40

50

45

REFERENCES

30
45

40

40

20
0

10

20

30

40

50

60

70

80

90

100

35
0

10

20

Percentage of Data used

30

40

50

60

70

80

90

100

0

10

20

Percentage of Data used

30

40

50

60

70

80

90

100

90

100

Percentage of Data used

Figure 4: Dataset 3
Test Error

Training Error

0.7

0.6

Online-ALS
Online-ALS-pretrained

0.6
0.55

0.55

0.5

0.45

RMSE

0.5

RMSE

0.6

RMSE

True Training Error

0.7

Online-ALS
Online-ALS-pretrained
SGD

0.65

0.4

0.3

0.2

0.4

Online-ALS
Online-ALS-pretrained
SGD

0.1

0.35
10

20

30

40

50

60

70

80

Percentage of Data used

90

100

0.5

0.45

0.4

0
0

CONCLUSIONS

0.35
0

10

20

30

40

50

60

70

80

90

100

Percentage of Data used

0

10

20

30

40

50

60

70

80

Percentage of Data used

Figure 5: Dataset 2-subset

Figs 2, 3, 4 and 5 show the results. We can see that for real
datasets, Online-ALS is much more stable than SGD, both for training and test errors. Pre-training is not as advantageous as it is on
the simulation data, maybe because the first 10% of training data is
not representative and gives a bad initialization, such as Dataset 3 in
Fig 4. However, pre-training with MCMC [11] gives a better result.
In a real application, we can evaluate which batch training is better
for a specific dataset and pre-train to get a better initialization. Also
in Fig 5, it is very interesting to see that Online-ALS is more stable
when using more context features, where SGD fluctuates a lot. It
is also notable to mention that for batch training (ALS, MCMC,
SGD) with real industrial data, training with multiple passes of the
data does not perform much better than training with one pass of
the data (batch or online), if proper features are selected. This is
shown in Table 2. Hence, online training in a streaming setting can
provide faster results without compromising accuracy in real world
applications. We can also see from Table 2 and Figs 2, 3, 4 and 5 that
test error of batch training for ALS (one or multiple passes of the
training data) is similar or larger than that of Online-ALS with only
one pass of the data. This implies that in real world application,
Online-ALS outperforms the batch ALS in both speed and accuracy.

1188

[1] Mathieu Blondel, Masakazu Ishihata, Akinori Fujino, and Naonori Ueda. 2016.
Polynomial networks and factorization machines: New insights and efficient
training algorithms. In 33rd International Conference on International Conference
on Machine Learning (ICML), Vol. 48. 850–858.
[2] Wei-Sheng Chin, Bo-Wen Yuan, Meng-Yuan Yang, and Chih-Jen Lin. 2018. An
efficient alternating newton method for learning factorization machines. ACM
Transactions on Intelligent Systems and Technology (TIST) 9, 6 (2018), 72.
[3] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.
DeepFM: a factorization-machine based neural network for CTR prediction. In
Proceedings of the 26th International Joint Conference on Artificial Intelligence.
AAAI Press, 1725–1731.
[4] Xiangnan He and Tat-Seng Chua. 2017. Neural factorization machines for sparse
predictive analytics. In Proceedings of the 40th International ACM SIGIR conference
on Research and Development in Information Retrieval. ACM, 355–364.
[5] Yanxiang Huang, Bin Cui, Wenyu Zhang, Jie Jiang, and Ying Xu. 2015. Tencentrec:
Real-time stream recommendation in practice. In Proceedings of the 2015 ACM
SIGMOD International Conference on Management of Data. ACM, 227–238.
[6] Yuchin Juan, Yong Zhuang, Wei-Sheng Chin, and Chih-Jen Lin. 2016. Fieldaware factorization machines for CTR prediction. In Proceedings of the 10th ACM
Conference on Recommender Systems. ACM, 43–50.
[7] Ming Lin and Jieping Ye. 2016. A non-convex one-pass framework for generalized factorization machine and rank-one matrix sensing. In Advances in Neural
Information Processing Systems. 1633–1641.
[8] Xueyu Mao, Saayan Mitra, and Viswanathan Swaminathan. 2017. Feature Selection for FM-Based Context-Aware Recommendation Systems. In 2017 IEEE
International Symposium on Multimedia (ISM). 252–255.
[9] Surabhi Punjabi and Priyanka Bhatt. 2018. Robust Factorization Machines for
User Response Prediction. In Proceedings of the 2018 World Wide Web Conference
on World Wide Web. 669–678.
[10] Steffen Rendle. 2010. Factorization machines. In 2010 IEEE International Conference on Data Mining. IEEE, 995–1000.
[11] Steffen Rendle. 2012. Factorization machines with libfm. ACM Transactions on
Intelligent Systems and Technology (TIST) 3, 3 (2012), 57.
[12] Steffen Rendle, Zeno Gantner, Christoph Freudenthaler, and Lars Schmidt-Thieme.
2011. Fast context-aware recommendations with factorization machines. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. ACM, 635–644.
[13] Paul Resnick and Hal R Varian. 1997. Recommender systems. Commun. ACM 40,
3 (1997), 56–58.
[14] Gang Wu, Viswanathan Swaminathan, Saayan Mitra, and Ratnesh Kumar. 2014.
Online video session progress prediction using low-rank matrix completion. In
Multimedia and Expo Workshops (ICMEW), 2014 IEEE International Conference on.
IEEE, 1–6.
[15] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua.
2017. Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks. In Proceedings of the 26th International Joint
Conference on Artificial Intelligence. AAAI Press.

