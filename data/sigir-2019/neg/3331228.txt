Session 2A: Question Answering

SIGIR ’19, July 21–25, 2019, Paris, France

Adaptive Multi-Attention Network Incorporating Answer
Information for Duplicate Question Detection
Di Liang∗ , Fubao Zhang∗ , Weidong Zhang, Qi Zhang† , Jinlan Fu, Minlong Peng, Tao Gui and
Xuanjing Huang
School of Computer Science, Shanghai Key Laboratory of Intelligent Information Processing
Fudan University, Shanghai, P.R.China 201203
{liangd17,fbzhang17,zhangyd17,qz,fujl16,mlpeng16,tgui16,xjhuang}@fudan.edu.cn

ABSTRACT

Case one:
Q1: What can I do for IAS?
A1: UPSC exam is also called IAS with a low pass rate.
You must take a lot energy.
Q2: How do I start preparation for UPSC?
A2: To prepare for the IAS exam, or more precisely, the UPSC
Civil Services Exam , self-discipline is the first.

Community-based question answering (CQA), which provides a
platform for people with diverse backgrounds to share information and knowledge, has become increasingly popular. With the
accumulation of site data, methods to detect duplicate questions
in CQA sites have attracted considerable attention. Existing methods typically use only questions to complete the task. However,
the paired answers may also provide valuable information. In this
paper, we propose an answer information- enhanced adaptive multiattention network (AMAN) to perform this task. AMAN takes full
advantage of the semantic information in the paired answers while
alleviating the noise problem caused by adding the answers. To
evaluate the proposed method, we use a CQADupStack set and the
Quora question-pair dataset expanded with paired answers. Experimental results demonstrate that the proposed model can achieve
state-of-the-art performance on the above two data sets.

Case two:
Q1: What should I do if I have a slight fever?
A1: Wiping with alcohol may help you, and it is my usual practice.
Q2: What items can remove oil stains?
A2: Alcohol may be a good choice. Try it out.

Figure 1: Two examples from Quora. In case one, the corresponding answers explain that the IAS is equivalent to
UPSC, which is crucial for determining the relationship between the both questions. In case two, the questions have
distinct semantics, but their paired answers are semantically
similar.

CCS CONCEPTS
• Information systems → Near-duplicate and plagiarism detection; Question answering.

1

KEYWORDS
duplicate question detection , adaptive multi-attention, communitybased question answering
ACM Reference Format:
Di Liang∗ , Fubao Zhang∗ , Weidong Zhang, Qi Zhang† , Jinlan Fu, Minlong
Peng, Tao Gui and Xuanjing Huang. 2019. Adaptive Multi-Attention Network Incorporating Answer Information for Duplicate Question Detection.
In Proceedings of the 42nd International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR ’19), July 21–25, 2019, Paris,
France. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3331184.
3331228
∗ Equal

INTRODUCTION

Community-based question answering (CQA) websites such as
Quora and Stack Overflow have grown in popularity in recent
years. However, with the increase of the CQA archives, massive
amounts of duplicate questions have accumulated. A large number
of redundant questions make the maintenance for these sites harder
and seriously affect the user experience. Therefore, it has become
increasingly important to detect duplicate questions. There are
two application scenarios for this technique. The first application
scenario is used as a basic technique for CQA retrieval to judge
whether one queried question is semantically equal to one historical
question [29, 41, 43]. The other scenario is that a CQA forum needs
to judge whether two historical questions are duplicates and then
merge the duplicate historical questions on the site [12, 39, 42]. With
an automatic detection method, the forum can organize questions
and answers more efficiently. In this paper, we present a robust
approach to the latter.
Question duplication is a pervasive issue in CQA, and existing
works have studied various aspects of the detection problem. The
study in [39] uses a distributed index and MapReduce framework to
calculate pairwise similarity and to identify redundant data quickly
and in a scalable manner. Zhang et al. [43] compute four similarity
scores by comparing their titles, descriptions, latent topics, and tags
of each pair of questions to detect duplicate posts in Stack Overflow.

contribution. Alphabetical order of the last name.
author.

† Corresponding

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’19, July 21–25, 2019, Paris, France
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00
https://doi.org/10.1145/3331184.3331228

95

Session 2A: Question Answering

SIGIR ’19, July 21–25, 2019, Paris, France

Zhang et al. [41] leverage continuous word vectors from the deep
learning literature, topic model features, and phrases pairs that
co-occur frequently in duplicate questions mined using machine
translation systems. Hoogeveen et al. [12] find that for misflagged
duplicate detection, meta data features that capture user authority,
question quality, and relational data between questions, outperform
pure text-based methods. In general, there are two major problems
in duplicate detection, namely the lexical gap and essential constituents matching. Distributed representation is an effective way to
tackle the lexical gap problem. Researchers have designed various
similarity features based on word embeddings [30], or acquired
representations of questions via neural networks and then calculated their similarity [7, 19]. And two approaches are proposed
to integrate FrameNet parsing with neural networks to achieve
essential constituents matching in [42].
Despite the above research improves the performance of previous state-of-the-art methods, some issues still have not been well
solved. Due to the relatively short text and lexical gap, in many
cases, the questions do not provide sufficient information. As a
result, all of these methods depending on only questions suffer
from having insufficient information to determine the relationship
between questions. However, answers to questions usually explain
the corresponding question in detail, they can be seen as a complementary information resource. Case one in the Figure 1 shows an
example from Quora. In this case, although Q1 and Q2 share many
words in common, their key concepts (IAS in Q1 and UPSC in Q2)
cannot be linked via the both questions. Without the knowledge
that IAS is equivalent to UPSC, existing methods that use only the
questions will fail to accomplish the task. Defining and labeling
knowledge bases for these rapidly-growing CQA websites is impractical, as this would consume too much time and resources. Answers
often provide crucial information for linking these seemingly different concepts in the questions. However, the information provided
by the answers is not always beneficial. Similar paired answers
can also introduce noise to the detection of semantically different
questions. Case two in Figure 1 illustrates another example from
Quora. Q1 (What should I do if I have a slight fever?) and Q2 (What
can be used to remove oil stains?) have distinct semantics, and previous methods may accurately distinguish the relationship between
the two. Yet the semantics of the both corresponding answers are
very similar. In this case, the introduction of answer information
introduces complications in identifying a solution. Hence, it is nontrivial to incorporate answer information into neural networks with
respect to duplicate question detection in a reasonable way.
In this paper, we propose a novel method to perform this task,
called the adaptive multi-attention network (AMAN). This model integrates external knowledge from paired answers for duplicate identification and filters out the noise introduced by answers adaptively.
To obtain multi-level textual features, we use the concatenation of
word embedding, character embedding, and syntactical features
as the representation. To incorporate answer information and capture the text relevance effectively , we utilize three heterogeneous
attention mechanisms: self-attention, which facilitates modeling
of the temporal interaction in a long sentence; cross attention,
which captures the relevance between questions and the relevance
between answers; and adaptive co-attention, which extracts valuable knowledge from the answers. In an adaptive co-attention block,

question-guided attention and answer-guided attention are combined to capture the semantic interaction between a question and its
paired answer. We propose a gated fusion module to adaptively fuse
the answer-based features. Then, to alleviate the noise introduced
by paired answers, we utilize a filtration gate module as a filter. An
interaction layer enhances the collected local semantic information
of questions and answers. Finally, predictions are calculated based
on the similarity features extracted from the question-answer pairs.
To demonstrate the effectiveness of our model, we evaluate it on
CQADupStack set and Quora question-pair dataset with expanded
paired answers. The experimental results on these two data sets
reveal that our method can achieve better performance than those
of previous methods.
The main contributions of this work can be summarized as follows:
• We take into account the noise problems that may be introduced by adding paired answers and study ways to integrate
answer information into neural network-based methods to
perform a duplicate detection task.
• We propose a novel method that integrates information extracted from paired answers into neural attention model to
complete duplicate detection and alleviates possible noise
introduced by this answer information.
• The experimental results on two data sets demonstrate that
our model can achieve significantly better performance than
those of current state-of-the-art methods.

2

RELATED WORK

Question duplication is a pervasive issue in CQA, and a number of
studies have looked into related problems, including text relevance
and question retrieval.

2.1

Text Relevance

Two categories of neural network-based models have been developed for this problem. The first set of models is sentence encodingbased. The models are developed from Siamese architecture [2]
and aim to find a fixed-length vector representation for each of
two sentences. Using the variant concatenation of the two sentence
vectors, a neural network classifier is then employed to decide the
relationship between the two sentences. The sentence encoder is
usually based on RNN, CNN, or a self-attention network [1, 22, 32].
Sentence vectors produced by sentence encoding-based models
usually generalize for a wide range of tasks. However, this kind
of method doesn’t explore the lower-level semantic interaction
between sentences.
The second set of models uses the cross-sentence feature or
inter-sentence attention from one sentence to another, and is hence
referred to as a matching-aggregation framework. Rocktäschel et al.
[28] are the first to use the attention-based method to improve
the performance of LSTM. Wang et al. [38] try to match words in
different sentences with word-by-word attention. Wang et al. [37]
propose a multiple-perspectives attention mechanism to modeling the semantic matching between two sentences, and achieved
state-of-the-art results on several relevant semantic matching tasks.
Cheng et al. [5] enhance the attention mechanism by a memory
network. Munkhdalai and Yu [23] use a tree structure to improve

96

Session 2A: Question Answering

SIGIR ’19, July 21–25, 2019, Paris, France

Interaction & Prediction

Interaction
& Prediction

q2_new

q1_new
Adaptive
Co-Attention

Adaptive Co-Attention Network
internal structure . Taking as an
example.

Adaptive
Co-Attention

1

1−

Adaptive
Multi-Attention
Cross
Attention

β

Cross
Attention

1

q1

q1

q1

q1

q1

α

1

1

1

Self Attention Layer
Representation

1

Question1

Question2

Answer1

…

1

A1

Answer2

Figure 2: The overall view of our model. The left part is the main framework of this work. The right part is the detailed
structure of the adaptive co-attention network. The thick arrow indicates that information flows from bottom to top.

3

the recurrent or recursive architecture for natural language inference and answer selection. Chen et al. [3] propose ESIM, which
achieved state-of-the-art results on the SNLI dataset.

2.2

APPROACH

We define the duplicate question detection problem as a binary
classification problem. Given two historical question-answer pairs
(Q1-A1 and Q2-A2) on a CQA website, our goal is to judge whether
the two historical questions are semantically equivalent or not. In
this work, we propose an answer information-enhanced adaptive
multi-attention network (AMAN) to incorporate external knowledge extracted from answers to complete this task. The overall
architecture of the model is illustrated on the left part of Figure 2.
Our sentence matching architecture, AMAN , is composed of
the following three components: (1) information representation
layer, (2) adaptive multi-attention layer, and (3) interaction and
prediction layer. The information representation layer combines the
multi-level features as the question and answer representation. The
adaptive multi-attention layer extracts the semantic connections
between the questions and the paired answers. The interaction and
prediction layer is designed to fuse local information for making a
global decision at the sentence level.

Question Retrieval

Most previous research has framed the CQA problem as a semantic matching task [9], and has relied on a number of extracted
features to train models. Various supervised methods that use handcrafted features or templates have been proposed for this task. In
their paper, Wang et al. [36] tackle the similar question matching
problem using syntactic parsing, while Zhou et al. [44] propose
a phrase-based translation model for this task. Although these
methods have shown impressive results, they are restricted in their
modeling of word sequence information [27]. Recently, enabled
by developments in distributed representation and deep learning
methods, many word-embedding-based neural network models
have been successfully used to tackle this problem from different
angles [7, 23, 25, 30, 34]. Many researchers have also considered
the use of different kinds of external resources. Zhou et al. [45]
utilize semantic relations extracted from the global knowledge of
Wikipedia.
In addition, some models for question retrieval are concerned
about the important paired answer part [14, 31, 40]. But these studies ignore that the answer information may also introduce noise
to the retrieval process. Different from the previous works, in this
paper, we propose a novel method that integrates information extracted from paired answers into neural attention model to perform
duplicate detection and to alleviate possible noise introduced by
this answer information.

3.1

Information Representation Layer

The information representation layer converts each word or phrase
in the question-answer pairs into a vector representation and constructs the representation matrix for the sentences. We combine
the multi-level features as the question and answer representation.
Each token is represented as a vector by using the pre-trained word
embedding such as GloVe [26], word2Vec [21], and fasttext [15]. It
can also utilize the preprocessing tool, e.g. part-of-speech recognizer, named entity recognizer, lexical parser etc., to incorporate
more syntactical and lexical information into the feature vector.

97

Session 2A: Question Answering

SIGIR ’19, July 21–25, 2019, Paris, France

For AMAN, we use a concatenation of word embedding, character embedding, and syntactical features as the sentence representation. The word embedding is obtained by mapping token to
high dimensional vector space by pre-trained word vector (300D
Glove 840B), and the word embedding is updated during training. Character-level embedding could alleviate out-of-vocabulary
(OOV) problems and capture helpful morphological information.
As in [16, 18], we filter the character embedding with 1D convolution kernel. The character convolutional feature maps are then max
pooled over the time dimension for each token to obtain a vector.
As in [4], the syntactical features consist of one-hot part-ofspeech (POS) tagging feature and binary exact match (EM) feature.
For one question or answer, the EM value is activated if the same
word is found in the other question or answer.
Next, AMAN adopts bidirectional Long Short-Term Memory network (Bi-LSTM) [10] to model the internal temporal interaction on
both directions of questions and answers. Consider two questionanswer pairs (Q1-A1 and Q2-A2), we have got their multi-level
features representation. Suppose the length of Q1, Q2, A1, and A2
are m, n, p, and l, respectively. These multi-level features representation are then passed to a Bi-LSTM encoder to obtain the contextdependent hidden state matrix, i.e, Q1 = {q1i |q1i ∈ Rd , i =
1, 2, ..., m}, Q2 = {q2i |q2i ∈ Rd , i = 1, 2, ..., n}, A1 = {a1i |a1i ∈
Rd , i = 1, 2, ..., p}, and A2 = {a2i |a2i ∈ Rd , i = 1, 2, ..., l }, where
d is the dimension of Bi-LSTM’s hidden state.

3.2

3.2.2 Cross attention. Cross attention captures the relevance
between both questions and between both answers. For the both
questions, we first compute a co-attention matrix C ∈ Rm×n . Each
element Ci, j ∈ R indicates the relevance between the i-th word
of question one and the j-th word of question two. Formally, the
co-attention matrix could be computed as:
Ci, j = vT tanh(W(q1i ⊙ q2j )),

where W ∈ Rk ×d , v ∈ Rk , and ⊙ denotes the element-wise production operation. Then the attentive matrix for question one and
c and Q2
c:
question two could be formalized as Q1
cqc1 = so f tmax(Ci,: ),
i

c1i = Q2 · cc ,
q
q1
i

3.2.1 Self-attention. In order to further model the temporal interaction between words and tackle the long-term dependency in a
long sentence, we additionally introduce the self-attention mechanism. Formally, for question one, we first compute a self-attention
matrix E ∈ Rm×m :

(3)

(6)

j

c = (q
c11 , q
c12 , ..., q
c1m ),
Q1

(7)

c = (q
c21 , q
c22 , ..., q
c2n ),
Q2

(8)

c = (ac
A1
11 , ac
12 , ..., ac
1p ),

(9)

c = (ac
A2
21 , ac
22 , ..., ac
2l ),

(10)

(11)
(12)

where WA1
c , Wqc1 , Wα i , bqc1 , and bα i are parameters, and WA1
c,
i

i

Wqc1 ∈ Rk ×d and Wα i ∈ R1×2k . In addition, we use ⊕ to denote the
i
concatenation of the answer one feature matrix and word feature
vector of the question one. The concatenation between a matrix
and a vector is performed by concatenating each column of the
matrix by the vector.
Based on the attention distribution α i , which is the weight corresponding to each word of the answer one, the new answer one
vector related to i-th word in the question one can be obtained by:

where ⟨·, ·⟩ denotes the inner production operation. Ei, j indicates
the relevance between the i-th word and j-th word in question one.
Then the self attentive vector for each word can be computed as
follow:

q1i = Q1 · eq1i ,

c2j = Q1 · cc ,
q
q2

c
c
zi = tanh(WA1
c A1 ⊕ (Wqc1i q1i + bqc1i )),
α i = so f tmax(Wα i zi + bα i ),

(1)

(2)

(5)

j

3.2.3 Adaptive Co-attention. Inspired by previous works [20],
adaptive co-attention includes question-guided attention and answerguided attention to capture the semantic interaction between a
question and its paired answer. Taking the time step i as an example, the internal structure of adaptive co-attention is shown on the
right part of Figure 2. We propose the use of a gated fusion module to fuse the features adaptively. Then, to reduce the possibility
of noise introduced by paired answer information, we utilize the
filtration gate to adaptively filter out some of the useless answer
information.
c1i is i-th word feature
Formally, after the cross attention layer, q
c
of the question one, and A1 is the answer one feature matrix. We
feed these through a single layer neural network followed by a
softmax function to generate the attention distribution over the
answer one:

Modeling local semantic information for words and their context is
the basic procedure for determining the semantic relation between
sentences. Generally in neural network methods, this procedure is
achieved with some forms of soft alignment. Answers to questions
usually explain the corresponding question in detail, they can be
seen as a complementary information resource. However, as shown
in Figure 1, the introduction of answer information sometimes may
also have a negative impact on the detection. In this layer , we
utilize three heterogeneous attention mechanisms to incorporate
answer information into question pair matching and adaptively
filter out the noise introduced by adding paired answers.

eq1i = so f tmax(Ei,: ),

cqc2 = so f tmax(C:, j ),

where Q1 = (q11 , q12 , ..., q1m ) and Q2 = (q21 , q22 , ..., q2n ). In
a similar way, we get the attentive matrix for the corresponding
answers:

Adaptive Multi-attention Layer

Ei, j = ⟨q1i , q1j ⟩,

(4)

c · αi,
a1i′ = A1

(13)

Next, we use the new answer one vector a1i′ to conduct the answerbased attention of the question one.

We can similarly derive the self attentive vector for question two,
answer one, and answer two as q2i , a1j , and a2k , respectively.

98

Session 2A: Question Answering

SIGIR ’19, July 21–25, 2019, Paris, France

Table 1: CQADupStack sub-forum statistics.
c ⊕ (W c ′ a1 ′ + b c ′ )),
b
zi = tanh(WQc1 Q1
i
Q 1,a1
Q 1,a1

(14)

β i = so f tmax(Wβi b
zi + bβi ),

(15)

i

i

Then, we acquire a new representation, q1i′ :
c · βi ,
q1i′ = Q1

(16)

where WQc1 , WQc1,a1′ ∈ Rk ×d , and Wβi ∈ R1×2k . We propose a
i

gated fusion to fuse question feature and answer feature:
a1i′′ = tanh(Wa1′i a1i′ + ba1′i ),

(17)

q1i′′ = tanh(Wq1′i q1i′ + bq1′i ),

(18)

gi = σ (Wдi (a1i′′ ⊕ q1i′′ )),

(19)

νi =

gi a1i′′

+ (1 − gi )q1i′′,

(20)

f1i =Wf (q
c
q
q 1i 1i ⊕ u i ),

of duplicates

android
english
gaming
gis
mathematica
physics
programmers
stats
tex
unix
webmasters
wordpress

1,866
5,076
3,531
978
1,302
2,196
2,637
645
4,560
2,466
1,899
864

622
1,692
1,177
326
434
732
879
215
1,520
822
633
282

28,020

9,334

Our model converts the resulting vectors obtained above to a
fixed-length vector with pooling and feeds it to the final classifier to determine the overall relationship. More specifically, we
compute max pooling and mean pooling for Q1v and Q2v . where
Q1v = (q1v1 , q1v2 , ..., q1vm ) and Q2v = (q2v1 , q2v2 , ..., q2vn ). All
these vectors are then concatenated into a fixed-length vector r.
Formally:
rmean
=
Q1

(21)
(22)

rmean
=
Q2

m q1v
Õ
i

m
i=1
n q2v
Õ
j

,

m

v
rmax
Q 1 = max q1i ,
i=1
n

, rmax
q2vj ,
Q 2 = max
j=1
n
j=1
max mean max
r = [rmean
Q 1 ; rQ 1 ; rQ 2 ; rQ 2 ],

(23)

where Ws , qc1 , Wνi ,si , Wνi , Wqf1 , bνi ,si , and bνi are parameters,
i
i
i
u i is the reserved features after filtration gate filter out noise.
We can similarly derive the answer-information-enhanced vector
f2j .
for question two as q

3.3

pairs

all

where σ is the logistic sigmoid activation, gi is the gate applied
to the new answer vector a1i′′ , and ν i is the fusion feature that
incorporates the question information and its paired answer information.
Because the fusion feature contains answer information, and
it may introduce some noise, we use a filtration gate to combine
the fusion feature and the original feature. The filtration gate is a
scalar in the range of [0, 1]. When the fusion feature is helpful to
improve the performance , the filtration gate is 1; otherwise, the
value of the filtration gate is 0. The filtration gate si and the answerf1i of question one are defined as
information-enhanced feature q
follows:
c1i ⊕ (Wνi ,si ν i + bνi ,si )),
si =σ (Ws , qc1 q
i
i
u i =si (tanh(Wνi ν i + bνi )),

Sub-forum

(28)
(29)
(30)

We then put the obtained final global representation r into our
prediction layer to determine whether Q1 and Q2 are semantically
equivalent.
The duplicate question detection task requires the model to
predict whether the given question pair (Q1, Q2) is semantically
identical or not, hence it is a binary classification task. We use a
multi-layer perceptron (MLP) classifier to predict the label:

Interaction and Prediction Layer

Inspired by previous works[3, 22], we further enhance the collected
local semantic information by combining the question representation and corresponding answer-information-enhanced vector of
question. More formally:

v = ReLU (Wr r + br ),

(31)

f
f
f
q1m
i = [q1i ; q1i ; q1i − q1i ; q1i ⊙ q1i ],

(24)

ŷ = so f tmax(Wv v + bv ).

(32)

f
f
f
q2m
j = [q2 j ; q2 j ; q2 j − q2 j ; q2 j ⊙ q2 j ],

(25)

where Wr , br , Wv , and bv are trainable parameters. The entire
model is trained end-to-end, optimizing the standard binary crossentropy loss function.

where [·; ·; ·; ·] refers to the concatenation operation. In the formula,
we first calculate the difference and the element-wise product for
f1i ) as well as for (q2j , q
f2j ).
(q1i , q
Then, BiLSTMs are trained to learn to modeling vectors which
contain the crucial information for judging the relationship between
two sentences:
v
v
q1vi = BiLST M(q1m
i , q1i−1 , q1i+1 ),

v
v
q2vj = BiLST M(q2m
j , q2 j−1 , q2 j+1 ),

4

EXPERIMENT

In this section, we present the evaluation of our model. We first
perform quantitative evaluation, comparing our model with other
competitive models. We then conduct some qualitative analyses to
understand the ability of AMAN to incorporate answer information
and adaptively filter out noise.

(26)
(27)

99

Session 2A: Question Answering

SIGIR ’19, July 21–25, 2019, Paris, France

Table 2: Hyper-parameters configuration.
Hyper-parameters
Word embedding size
character embedding size
convolution kernel size
Initial learning rate
Adam β 1
Adam β 2
Dropout rate
Batch size
LSTM hidden size
MLP hidden size

4.1

Table 3: Overall results on AeQQP.

Value

Model

de = 300
dc = 100
dk = 5
α = 0.001
β 1 = 0.9
β 2 = 0.999
p = 0.2
b = 64
dh x = 300
dh t = 300

InferSent [6]
SSE [24]

84.00
86.62

PWIM [11]
Multi-Perspective-CNN [37]
Multi-Perspective-LSTM [37]
BiMPM [37]
pt-DECATT [35]
ESIM [3]
AF-DMN [8]
DIIN [4]

72.59
78.98
79.12
87.32
86.43
84.35
87.61
88.20

AMAN(ours)

90.07

Dataset

In this work, we introduce two datasets to evaluate our model. In
addition to CQADupStack, we expand the Quora Question Pairs
(QQP) dataset with the paired answers and named it the answerenhanced QQP (AeQQP).
AeQQP : Each sample in the QQP dataset contains two questions
and is annotated with a binary label indicating whether these two
questions are semantically equivalent. The question pairs in the
dataset are not restricted to any subject. In the original dataset,
the answers to the questions are not contained. To evaluate the
effectiveness of our model, we collect the answers from Quora
for question pairs in the QQP dataset. More specifically, in the
QQP dataset, there are a total of 404,302 question pairs formed
by 537,933 distinct questions. We crawl the answer recommended
by Quora (usually the answer with the most upvotes) for each
question in the dataset. In total, we get 290,391 question pairs
where both questions were answered. We construct the AeQQP
dataset with the 290k question pairs with both questions answered
and their corresponding answers. We split the dataset into three
parts: training set, development set, and testing set, which contain
270k, 10k, and 10k question-answer pairs, respectively. Accuracy is
used as the evaluation metric on this dataset.
CQADupStack: This is a benchmark dataset for use in community question-answering (CQA) research [13]. It contains threads
from twelve StackExchange sub-forums, annotated with duplicate
question information. Table 1 gives the total number of the question
pairs with both questions answered and duplicate questions for
the twelve sub-forums. The training, development, and test split
follows a ratio of 8:1:1. For CQADupStack, Precision , Recall, F 1
score, and Accuracy are used as the evaluation metrics in this work.
F 1 score is the harmonic mean of Precision and Recall, wherein
Recall reflects the ability to identify duplicate pairs among the true
duplicate pairs.

4.2

Accuracy

•

•
•

•

•

•

•

•

Models for Comparing
•

To analyze the effectiveness of our model, we evaluate some traditional and state-of-the-art methods as baselines as follows on the
above two data sets:
• InferSent [6] is a sentence encoding-based model. InferSent
adopts Bi-LSTM max-pooling sentence encoder and passes

the independent vector representations of two questions
through an MLP classifier to make the final prediction.
SSE [24] is a simple sequential sentence encoder for multidomain natural language inference and is based on stacked
bidirectional LSTM-RNNs with shortcut connections and
fine-tuning of word embeddings. It enhances multi-layer
Bi-LSTM with a skip connection.
PWIM [11] uses cosine similarity, Euclidean distance, and
dot product to calculate the word-pair interactions.
pt-DECATT [35] is variant of decomposable attention models based on word-level embedding and character-level ngram embedding.
ESIM [3] is a previous state-of-the-art model for the natural
language inference (NLI) task. It is a sequential model that
incorporates the chain LSTM and the tree LSTM to infer
local information between two sentences.
DIIN [4] is a novel class of neural network architectures that
is able to achieve high-level understanding of the sentence
pair by hierarchically extracting semantic features from the
interaction space. The model uses word-by-word dimensionwise alignment tensors to encode the high-order alignment
relationship between sentence pairs.
AF-DMN [8] stacks multiple computational blocks in its
matching layer to learn the interaction of the sentence pair
better.
Multi-Perspective-CNN [37] changes the cosine similarity
calculation layer with multi-perspective cosine matching
function based on "Siamese-CNN" which implements the
sentence encoder with a CNN.
Multi-Perspective-LSTM [37] is an identical idea to the
Multi-Perspective-CNN, but uses "Siamese-LSTM" instead
of "Siamese-CNN".
BiMPM is also proposed in [37]. The model combines the
above two models. All these models employ a multi-perspective
matching mechanism in sentence pair modeling tasks.

The first two models are both sentence encoding-based models,
and all other models use some kind of cross sentence feature.

100

Session 2A: Question Answering

SIGIR ’19, July 21–25, 2019, Paris, France

Table 4: Overall results on CQADupStack.
MODEL

Precision

Recall

F1

Accuracy

Multi-Perspective-CNN [37]
Multi-Perspective-LSTM [37]
BiMPM [37]
ESIM [3]
AF-DMN [8]
DIIN [4]

82.81
83.64
84.78
87.83
89.22
89.46

92.23
94.08
97.21
95.20
93.66
94.60

87.13
87.98
90.04
90.81
90.92
91.36

90.12
90.15
94.14
93.85
94.72
94.73

AMAN(ours)

90.52

97.87

94.05

96.28

Table 5: Sub-forum results of CQADupStack.
ESIM

sub-forum
android
english
gaming
jgis
mathematica
physics
programmers
stats
tex
unix
webmasters
wordpress

4.3

DIIN

F1

Accuracy

F1

Accuracy

F1

Accuracy

84.17
86.70
86.29
82.23
81.58
85.62
85.11
85.36
90.58
86.22
83.28
85.11

90.21
91.46
92.28
88.79
87.11
86.17
91.56
85.90
93.09
90.75
89.86
86.00

89.05
90.60
88.88
87.29
90.94
90.28
87.36
86.90
91.12
88.34
86.84
89.29

92.61
92.26
93.64
89.89
91.96
90.30
89.41
91.01
93.65
91.50
91.01
90.14

89.95
91.97
90.80
90.08
92.23
91.96
89.36
90.75
94.45
89.93
90.02
90.63

93.44
94.00
93.62
92.21
94.01
90.56
91.18
91.59
95.23
92.06
90.86
91.89

Experiment Configurations

As illustrated in Table 3, our model outperforms the baselines
and achieves an accuracy of 90.07% in the test set of the AeQQP
dataset. In Table 3, the first two models InferSent and SSE are both
sentence encoding-based models, and all other compared models
use some kind of cross-sentence feature.
Meanwhile, the results demonstrate that the models utilizing
cross-sentence features achieve more competitive results in this
task. This phenomenon shows that cross-sentence interaction operations, like cross attention, are crucial components for sentence
modeling. We explore this idea in our multi-attention component
to understand logical and semantic relationship between two sentences.
Precision, Recall, F 1 score, and Accuracy are used as the evaluation metrics on the CQADupStack dataset. Table 4 shows the overall results of different models on the test set of the CQADupStack
dataset. Our AMAN model achieves state-of-the-art performance
on the all four evaluation metrics. In addition, we evaluate AMAN
and two other strong baselines (ESIM and DIIN) on twelve subforum datasets separately. The models are trained and tested on
every sub-forum dataset separately. Table 5 shows the performance
of the different models on every sub-forum dataset. Our model
achieves the best performance on the most of sub-forum sets.
The above results demonstrate that the answer information could
be an essential knowledge source for duplicate question detection,
and our model makes effective use of this information.

We adopt the hyper-parameters as shown in Table 2. In this work,
an Adam [17] optimizer with β 1 as 0.9 and β 2 as 0.999 is used to
optimize all trainable parameters. The initial learning rate is set to
0.001 and is halved when the accuracy on the dev set decreases. We
use a batch size of 64. All hidden states of LSTMs and MLPs have 300
dimensions. We also apply dropout [33] on the word embeddings
and all MLPs to avoid over-fitting, and the dropout rate is set to 0.2.
The length of all questions and answers is truncated to 40 and
100 respectively. For initialization, we initialize the word embeddings with a 300D Glove 840B [26], and the out-of- vocabulary
(OOV) words are randomly initialized. All word embeddings are
updated during training. Parameters, including neural network parameters and OOV word embeddings, are initialized with a uniform
distribution between [−0.01, 0.01]. The character embeddings are
randomly initialized with 100D. We crop or pad each token to have
16 characters. And the 1D convolution kernel size for character
embedding is 5.

4.4

AMAN(ours)

Quantitative Results

In this subsection, we compare our model performance to that of
other neural network-based models on the AeQQP and CQADupStack dataset. On the both datasets, our model AMAN uses questionanswer pairs information, while other compared models are only
trained on the question pairs dataset.

101

Session 2A: Question Answering

4.5

SIGIR ’19, July 21–25, 2019, Paris, France

Table 6: Answer information research results.

Answer Information Research

To further explore the impact of the answer information on duplicate question detection, we perform additional experiments on the
AeQQP dataset.
As illustrated in Table 6, we train the ESIM on the answer pairs
dataset of AeQQP train set, and the ESIM achieves an accuracy of
67.82% in the test set. The result demonstrates that this task can be
done pretty well using only the answer information. This verifies
the speculation that answers usually explain the corresponding
questions in detail, and hence they could provide sufficient information for duplicate identification.
In addition, we concatenate the multi-level features representation of the question and the corresponding answer to acquire the
Q-A representation. Then, we feed the the Q-A representation to
the ESIM in the process of training and testing instead of only using
the question representation. With this method, the performance
drops sharply to 77.56% from 84.35%. The result illustrates that
the corresponding answer information may also cause trouble for
the solution of the task, and the naive way to incorporate answer
information could introduce noise to detection.
Generally, the answer information for this task is usually a mixture of valuable information and other redundant information.
Hence, how to incorporate answer information into neural networks for duplicate question detection should be investigated.

4.6

Method
ESIM (Answer Pairs)
ESIM (Q-A) Pairs
ESIM (Question Pairs)
AMAN (ours)

Accuracy
67.82
77.56
84.35
90.07

Table 7: Ablation experiment results on AeQQP.
Method
1. AMAN (- self-att)
2. AMAN (- co-att)
3. AMAN (- adaptive co-att + co-att)
4. AMAN (- adaptive co-att)
5. AMAN (- interaction)
6. AMAN (- char-emb - syntactical fea)
7. AMAN(ours)

Accuracy
89.60
88.42
89.19
87.58
88.13
89.10
90.07

syntactical features and just keep word embedding as the representation. The performance of the model is reduced to 89.10% on the
test set.
In conclusion, due to the effective combination of each component, our model integrates valuable information from paired
answers for duplicate identification and adaptively filters out the
noise introduced by answers.

Ablation Study

We conduct an ablation study on our base model to examine the
effectiveness of each component. We study our model on the AeQQP
dataset. The experimental results are shown in Table 7.
First, we study how self-attention contributes to the system.
After removing the self-attention component, we find that the performance degrades to 89.60% for test accuracy. Simple self-attention
further models the temporal interaction between words and tackles
the long-term dependency in a long sentence to acquire stronger
representation. In the experiment 2, we remove the cross attention
acting on between the both questions and between the both answers, and the performance drops to 88.42%. Cross attention can
capture the relevance between both sentences, and the relevance information is crucial for this task. (-adaptive co-att + co-att) indicates
that we introduce ordinary cross attention to integrate the paired
answers information in instead of adaptive co-attention. Model performance decrease by nearly 0.9 percentage points. Furthermore,
the result of experiment 4 shows that our AMAN, without adaptive
co-attention component, declines to an accuracy of 87.58% in the
test set, which is equal to the removal of the answers feature to only
depend on the question information. The above two experiments
reflect that our adaptive co-attention component integrates the
answer information and alleviates the noise problem effectively.
To show that the interaction layer can enhance the collected local
semantic information and help to determine the overall relationship
between both questions, we remove this component as a comparison in the experiment 5. The result of 88.13% demonstrates that our
interaction component plays a crucial role in achieving competitive
performance. In the last comparative experiment, we explore the
role of multi-level features. We remove character embedding and

4.7

Case Study

To visually demonstrate the validity of the model, we do a qualitative study using the two cases in Figure 1. The qualitative results
are demonstrated in Table 8. Only depending on question pairs
information, the ESIM and DIIN are able to capture the distinct
semantics of Q1 and Q2 in case 2, but they are unconcerned about
the association between IAS and UPSC in case 1. Therefore the
above two methods correctly judge the label of case 2, while determining the label of case 1 as NO. The ESIM(Q-A pairs) is trained
and tested using the concatenation of the question representation
and the corresponding answer representation. With the knowledge
provided by the answer pair that IAS is equivalent to UPSC, the
ESIM(Q-A pairs) makes a correct judgment in case 1. But because
of the interference provided by similar answers, the model fails in
case 2.
Our model AMAN makes the correct predictions in the both
cases. With the filtration gate being set to 1 automatically, the
proposed model AMAN incorporates the information extracted
from the answer pair into the detection and links the key concepts
(IAS in Q1 and UPSC in Q2) in case 1. With the filtration gate as 0
adaptively, the model filters out the noise introduced by the similar
answers, and correctly judges that both questions are different in
case 2.

4.8

Parameter Sensitivity

In this section, we evaluate the impact of the hidden state dimension
of LSTMs and the answer sentence length on the AeQQP.

102

Session 2A: Question Answering

SIGIR ’19, July 21–25, 2019, Paris, France

Table 8: Qualitative results. The ESIM(Q-A pairs) indicates that we feed the Q-A representation to the ESIM instead of only
question representation. Our model is able to effectively utilize the information extracted from answer pairs and adaptively
filter out the resulting noise.
ESIM

ESIM(Q-A pairs)

DIIN

AMAN(ours)

Case 1

Q 1: What can I do for IAS?
A 1: To prepare for IAS exam, or more precisely,
the UPSC Civil Services Exam, self-discipline
is the first.
Q 2: How do I start preparation for UPSC?
A 2: UPSC exam is also called IAS with a low pass
rate. You must take a lot energy.
Label: YES

prediction: NO

prediction: YES

prediction: NO

prediction: YES
filtration gate: 1

Case 2

Q 1: What should I do if I have a slight fever?
A 1: Wiping with alcohol may help you, and it
is my usual practice.
Q 2: What items can remove oil stains?
A 2: Alcohol may be a good choice. Try it out.
Label: NO

prediction: NO

prediction: YES

prediction: NO

prediction: NO
filtration gate: 0

Figure 3: Results of our model influenced by different hidden state dimensions of LSTMs.

Figure 4: Results with answers that are truncated at different
lengths.

First, we investigate the impact of different hidden state dimensions of LSTMs. Figure 3 shows our model’s achieved results for
different dimensions. As shown in the figure, when the hidden state
size is less than 300, the performance of our model is increasing
along with it. This trend indicates that a large hidden state size
could enhance the performance of our model. When the dimension
reaches 400, however, the performance drops on both the dev and
test sets. This may be due to a requirement of more data for fitting
such a large number of parameters. In our work, we get the best
result when the hidden state dimensions of the LSTMs are set to
300.
We further compare the performance of our model with answers
that are truncated at different lengths. As illustrated in Figure 4,
our model achieve the best performance at the truncated length
of answers as 100. As mentioned before, the answer information
for this task is usually a mixture of valuable information and other

redundant information. Therefore, a shorter truncation may cause
the useful information to be lost, while a longer truncation may
introduce more redundant information to aggravate the noise problem.

5

CONCLUSIONS

In this work, we show that the paired answers can provide effective
information for duplicate question detection while they may simultaneously introduce noise to the detection. We propose an adaptive
multi-attention network (AMAN), an effective method integrating
external knowledge from answers for duplicate identification and
filtering out the noise introduced by paired answers adaptively.
This model consists of three layers: the information representation
layer aims to obtain multi-level textual features as sentence representation; the adaptive multi-attention layer incorporates answer
information into the neural attention model and captures the text

103

Session 2A: Question Answering

SIGIR ’19, July 21–25, 2019, Paris, France

relevance; and the interaction and prediction layer enhances the
collected local semantic information of questions and answers to
make a global decision at the sentence level. Experimental results
on two data sets demonstrate that our model can achieve significantly better performance than those of current state-of-the-art
methods.

ACKNOWLEDGMENTS
The authors wish to thank the anonymous reviewers for their helpful comments. This work was partially funded by China National
Key RD Program (No.2018YFC0831105, 2017YFB1002104), National
Natural Science Foundation of China (No.61751201, 61532011), Shanghai Municipal Science and Technology Major Project (No.2018SHZD
ZX01), STCSM (No.16JC1420401, 17JC1420200), ZJLab.

REFERENCES
[1] Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning.
2015. A large annotated corpus for learning natural language inference. arXiv
preprint arXiv:1508.05326 (2015).
[2] Jane Bromley, Isäbelle Guyon, Yann LeCun, Eduard Säckinger, and Roopak Shah.
1994. Signature verification using a" siamese" time delay neural network. In
Advances in neural information processing systems. 737–744.
[3] Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang, and Diana
Inkpen. 2016. Enhanced lstm for natural language inference. arXiv preprint
arXiv:1609.06038 (2016).
[4] Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Diana Inkpen, and Si Wei. 2018. Neural
natural language inference models enhanced with external knowledge. In ACL
2018 (Volume 1: Long Papers), Vol. 1. 2406–2417.
[5] Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016. Long short-term memorynetworks for machine reading. arXiv preprint arXiv:1601.06733 (2016).
[6] Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine
Bordes. 2017. Supervised learning of universal sentence representations from
natural language inference data. arXiv preprint arXiv:1705.02364 (2017).
[7] Cícero Dos Santos, Luciano Barbosa, Dasha Bogdanova, and Bianca Zadrozny.
2015. Learning hybrid representations to retrieve semantically equivalent questions. In ACL-IJCNLP 2015 (Volume 2: Short Papers), Vol. 2. 694–699.
[8] Chaoqun Duan, Lei Cui, Xinchi Chen, Furu Wei, Conghui Zhu, and Tiejun Zhao.
2018. Attention-Fused Deep Matching Network for Natural Language Inference..
In IJCAI. 4033–4040.
[9] Hanyin Fang, Fei Wu, Zhou Zhao, Xinyu Duan, Yueting Zhuang, and Martin
Ester. 2016. Community-based question answering via heterogeneous social
network learning. In Thirtieth AAAI Conference on Artificial Intelligence.
[10] Alex Graves and Jürgen Schmidhuber. 2005. Framewise phoneme classification
with bidirectional LSTM and other neural network architectures. Neural Networks
18, 5-6 (2005), 602–610.
[11] Hua He and Jimmy Lin. 2016. Pairwise word interaction modeling with deep
neural networks for semantic similarity measurement. In NAACL 2016: Human
Language Technologies. 937–948.
[12] Doris Hoogeveen, Andrew Bennett, Yitong Li, Karin M Verspoor, and Timothy Baldwin. 2018. Detecting Misflagged Duplicate Questions in Community
Question-Answering Archives. In Twelfth International AAAI Conference on Web
and Social Media.
[13] Doris Hoogeveen, Karin M Verspoor, and Timothy Baldwin. 2015. CQADupStack:
A benchmark data set for community question-answering research. In Proceedings
of the 20th Australasian Document Computing Symposium. ACM, 3.
[14] Zongcheng Ji, Fei Xu, Bin Wang, and Ben He. 2012. Question-answer topic model
for question retrieval in community question answering. In Proceedings of the
21st ACM international conference on Information and knowledge management.
ACM, 2471–2474.
[15] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. Bag
of tricks for efficient text classification. arXiv preprint arXiv:1607.01759 (2016).
[16] Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. 2016. CharacterAware Neural Language Models.. In AAAI. 2741–2749.
[17] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).
[18] Jason Lee, Kyunghyun Cho, and Thomas Hofmann. 2016. Fully characterlevel neural machine translation without explicit segmentation. arXiv preprint
arXiv:1610.03017 (2016).
[19] Tao Lei, Hrishikesh Joshi, Regina Barzilay, Tommi Jaakkola, Katerina Tymoshenko, Alessandro Moschitti, and Lluis Marquez. 2016. Semi-supervised
question retrieval with recurrent convolutions. NAACL 2016 (2016).

104

[20] Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher. 2017. Knowing when
to look: Adaptive attention via a visual sentinel for image captioning. In CVPR,
Vol. 6. 2.
[21] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality. In
NIPS. 3111–3119.
[22] Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan, and Zhi Jin. 2015. Natural
language inference by tree-based convolution and heuristic matching. arXiv
preprint arXiv:1512.08422 (2015).
[23] Tsendsuren Munkhdalai and Hong Yu. 2017. Neural tree indexers for text understanding. In Proceedings of the conference. Association for Computational Linguistics. Meeting, Vol. 1. NIH Public Access, 11.
[24] Yixin Nie and Mohit Bansal. 2017. Shortcut-stacked sentence encoders for multidomain inference. arXiv preprint arXiv:1708.02312 (2017).
[25] Ankur P Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. 2016. A
decomposable attention model for natural language inference. arXiv preprint
arXiv:1606.01933 (2016).
[26] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove:
Global vectors for word representation. In EMNLP. 1532–1543.
[27] Xipeng Qiu and Xuanjing Huang. 2015. Convolutional Neural Tensor Network
Architecture for Community-Based Question Answering.. In IJCAI. 1305–1311.
[28] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomáš Kočiskỳ,
and Phil Blunsom. 2015. Reasoning about entailment with neural attention. arXiv
preprint arXiv:1509.06664 (2015).
[29] João António Rodrigues, Chakaveh Saedi, Vladislav Maraev, Joao Silva, and António Branco. 2017. Ways of asking and replying in duplicate question detection.
In * SEM 2017. 262–270.
[30] MF Salvador, S Kar, T Solorio, and P Rosso. 2016. Combining lexical and semanticbased features for community question answering. SemEval (2016), 814–821.
[31] Anirban Sen, Manjira Sinha, and Sandya Mannarswamy. 2017. Improving Similar
Question Retrieval using a Novel Tripartite Neural Network based Approach.
In Proceedings of the 9th Annual Meeting of the Forum for Information Retrieval
Evaluation. ACM, 1–5.
[32] Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Sen Wang, and Chengqi Zhang.
2018. Reinforced Self-Attention Network: a Hybrid of Hard and Soft Attention
for Sequence Modeling. arXiv preprint arXiv:1801.10296 (2018).
[33] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from
overfitting. The Journal of Machine Learning Research 15, 1 (2014), 1929–1958.
[34] Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. 2017. A Compare-Propagate Architecture with Alignment Factorization for Natural Language Inference. arXiv
preprint arXiv:1801.00102 (2017).
[35] Gaurav Singh Tomar, Thyago Duque, Oscar Täckström, Jakob Uszkoreit, and
Dipanjan Das. 2017. Neural paraphrase identification of questions with noisy
pretraining. arXiv preprint arXiv:1704.04565 (2017).
[36] Kai Wang, Zhaoyan Ming, and Tat-Seng Chua. 2009. A syntactic tree matching
approach to finding similar questions in community-based qa services. In Proceedings of the 32nd international ACM SIGIR conference on Research and development
in information retrieval. ACM, 187–194.
[37] Zhiguo Wang, Wael Hamza, and Radu Florian. 2017. Bilateral multi-perspective
matching for natural language sentences. arXiv preprint arXiv:1702.03814 (2017).
[38] Zhiguo Wang, Haitao Mi, and Abraham Ittycheriah. 2016. Sentence similarity learning by lexical decomposition and composition. arXiv preprint
arXiv:1602.07019 (2016).
[39] Yan Wu, Qi Zhang, and Xuanjing Huang. 2011. Efficient near-duplicate detection
for q&a forum. In IJCNLP. 1001–1009.
[40] Kai Zhang, Wei Wu, Haocheng Wu, Zhoujun Li, and Ming Zhou. 2014. Question
retrieval with high quality answers in community question answering. In Proceedings of the 23rd ACM International Conference on Conference on Information
and Knowledge Management. ACM, 371–380.
[41] Wei Emma Zhang, Quan Z Sheng, Jey Han Lau, and Ermyas Abebe. 2017. Detecting duplicate posts in programming QA communities via latent semantics
and association rules. In Proceedings of the 26th International Conference on World
Wide Web. International World Wide Web Conferences Steering Committee,
1221–1229.
[42] Xiaodong Zhang, Xu Sun, and Houfeng Wang. 2018. Duplicate Question Identification by Integrating FrameNet with Neural Networks. (2018).
[43] Yun Zhang, David Lo, Xin Xia, and Jian-Ling Sun. 2015. Multi-factor duplicate
question detection in stack overflow. Journal of Computer Science and Technology
30, 5 (2015), 981–997.
[44] Guangyou Zhou, Li Cai, Jun Zhao, and Kang Liu. 2011. Phrase-based translation
model for question retrieval in community question answer archives. In ACL
2011: Human Language Technologies-Volume 1. Association for Computational
Linguistics, 653–662.
[45] Guangyou Zhou, Yang Liu, Fang Liu, Daojian Zeng, and Jun Zhao. 2013. Improving Question Retrieval in Community Question Answering Using World
Knowledge. In IJCAI, Vol. 13. 2239–2245.

