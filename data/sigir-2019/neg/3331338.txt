Short Research Papers 2A: AI, Mining, and others

SIGIR ’19, July 21–25, 2019, Paris, France

Local Matrix Approximation based on Graph Random Walk
Xuejiao Yang and Bang Wang*
Huazhong University of Science and Technology (HUST), Wuhan, China
yxj603@foxmail.com,wangbang@hust.edu.cn

ABSTRACT

1

How to decompose a large global matrix into many small local matrices has been recently researched a lot for
matrix approximation. However, the distance computation
in matrix decomposition is a challenging issue, as no prior
knowledge about the most appropriate feature vectors and
distance measures are available. In this paper, we propose a
novel scheme for local matrix construction without involving
distance computation. The basic idea is based on the application of convergence probabilities of graph random walk.
At first, a user-item bipartite graph is constructed from the
global matrix. After performing random walk on the bipartite graph, we select some user-item pairs as anchors. Then
another random walk with restart is applied to construct
the local matrix for each anchor. Finally, the global matrix
approximation is obtained by averaging the prediction results
of local matrices. Our experiments on the four real-world
datasets show that the proposed solution outperforms the
state-of-the-art schemes in terms of lower prediction errors
and higher coverage ratios.

Recommendation systems have been playing an important
role in the online social networks and e-commerce websites.
Among various recommendation algorithms, the matrix factorization (MF) has gained lots of attentions since its great
success in the Netflix contest [2]. For a user-item rating matrix
R ∈ R𝑀 ×𝑁 with 𝑀 users and 𝑁 items, the MF technique
is to predict the missing values in R by approximating R
^ = UVT , where U ∈ R𝑀 ×𝐾 is a
with another matrix R
user-factor matrix and V ∈ R𝑁 ×𝐾 an item-factor matrix,
with 𝐾 ≪ min(𝑀, 𝑁 ). After performing a global MF on R,
a recommendation list for unrated items can be obtained
according to the predicted ratings for each user.
Recently, a global rating matrix has become very large
and sparse. Many studies have focused on how to convert
the global matrix factorization task into several local matrix
approximation tasks [1, 3, 4, 6]. For example, the DFC algorithm [4] randomly selects a subset of rows and columns
from the global matrix to form many local matrices. It then
performs a local MF on each local matrix and gets the final
predictions by averaging the local approximations of the local
matrices. Another important direction for decomposing the
original rating matrix is based on the idea of anchor selection
and neighborhood inclusion, as done in LLORMA [3] and
CLLORMA [6]. They first select some of rated user-item
pairs as anchors, and then find the neighbor users and items
for each anchor, by which a local matrix is constructed.
Distance computation is needed for comparing the similarity in between users and items in the aforementioned
algorithms when decomposing R. However, engineering an
appropriate feature as a user or item vector representation
may become a challenging task. Although using the global
MF can obtain user latent features from U and item latent
features from V, how to choose an appropriate feature dimensionality 𝐾 again becomes a new challenge. Furthermore,
there exist lots of distance measures, like the Euclidean distance, Pearson correlation, cosine similarity and etc., yet
which one is the most appropriate for what kind of datasets
is still an open problem. On the one hand, we are motivated
from the recent findings for decomposing the original large
matrix into many small local matrices. On the other hand,
we would like to construct local matrices without involving
distance computation.
In this paper, we propose a novel scheme to decompose
the original rating matrix into many local matrices, called
random walk-based local matrix approximation (RWLMA).
We first construct a user-item bipartite graph for the original
matrix R. We then select some user-item pairs as anchors
according to their convergence probabilities after performing
a random walk (RW) on the bipartite graph. Then for each

CCS CONCEPTS
• Information systems → Recommender systems.

KEYWORDS
Matrix factorization; local matrix construction; graph random walk; recommendation
ACM Reference Format:
Xuejiao Yang and Bang Wang. 2019. Local Matrix Approximation
based on Graph Random Walk. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development
in Information Retrieval (SIGIR ’19), July 21–25, 2019, Paris,
France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.
1145/3331184.3331338
*

Xuejiao Yang and Bang Wang are with the School of Electronic,
Information and Communications, HUST. This work is supported in
part by National Natural Science Foundation of China (Grant No:
61771209).

Permission to make digital or hard copies of all or part of this work
for personal or classroom use is granted without fee provided that
copies are not made or distributed for profit or commercial advantage
and that copies bear this notice and the full citation on the first
page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee. Request permissions
from permissions@acm.org.
SIGIR ’19, July 21–25, 2019, Paris, France
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00
https://doi.org/10.1145/3331184.3331338

1037

INTRODUCTION

Short Research Papers 2A: AI, Mining, and others

SIGIR ’19, July 21–25, 2019, Paris, France

SIGIR ’19, July 21–25, 2019, Paris, France

Xuejiao Yang and Bang Wang

anchor, we perform a random walk with restart (RWR) with
the anchor user (item) as the restarting node to obtain the
correlation degrees between this anchor user (item) and other
users (items). Each non-anchor user-item pair is included
into her top anchors’ neighborhood, and for each anchor
and her neighborhood, we construct a local matrix. The
global matrix approximation is obtained by averaging the
approximation results of local matrices. Experiments on four
datasets including three public ones show that the proposed
RWLMA can outperform the state-of-the-art ones in terms
of lower prediction errors and higher coverage ratios.

order, respectively. Then we select the top 𝐴 users and top 𝐴
items from the sorted lists and randomly pair them to form
in total 𝐴 anchors. Let 𝒜 denote the set of selected anchors.

2.2

Neighborhood Construction

For each anchor, we next determine its neighbor users
and neighbor items so as to construct a corresponding local
matrix. To this end, we apply a graph-based random walk
with restart to measure the relations between an anchor and
other non-anchor user-item pairs as follows. Take anchor
(𝑢𝑎 , 𝑣𝑎 ) as an example. We first use the anchor user 𝑢𝑎 as
a restart node to compute its correlation degree with other
user nodes. We use the one-hot vector to initialize the user
probability vector as u(0) . That is, u(0) (𝑖) = 1, if 𝑖 = 𝑢𝑎 .
Otherwise, u(0) (𝑖) = 0. We define the user restart vector as
r𝑈 . For user 𝑢𝑎 , r𝑈 (𝑖) = 1, if 𝑖 = 𝑢𝑎 . Otherwise, r𝑈 (𝑖) = 0.
Furthermore, we randomly initialize the item probability
vector v(0) .
To obtain the convergence probabilities, the RWR algorithm is to iteratively compute the following equations:

2 THE PROPOSED METHOD
2.1 Anchor Selection
We propose to use a graph-based random walk to select
anchors. Let R ∈ R𝑀 ×𝑁 denote a user-item rating matrix
with 𝑀 users and 𝑁 items, where an element 𝑟𝑢𝑣 ∈ R, if
it exists, is a non-negative real value as the rating given
to the 𝑣th item by the 𝑢th user. From R, we construct a
bipartite graph of users and items as follows: If 𝑟𝑢𝑣 exists,
an undirected edge is established between 𝑢 and 𝑣 in the
bipartite graph with the edge weight of 𝑟𝑢𝑣 . Motivated from
the PageRank [5], we use a Markov chain to transform the
anchor importance calculation task into a node convergence
probability computation problem. Let P𝑉 𝑈 be the probability
transition matrix from items to users, which is obtained by
column-normalizing the rating matrix R. Let P𝑈 𝑉 be the
transition matrix from users to items, which is calculated by
column-normalizing RT .
We randomly initialize the probability vector of users and
items as u(0) and v(0) . To obtain the convergence probabilities, the proposed random walk algorithm is to iterative
compute the following equations:
1
(1)
u(𝑡+1) = (1 − 𝛼) · P𝑉 𝑈 v(𝑡) + 𝛼 ·
𝑀
1
v(𝑡+1) = (1 − 𝛼) · P𝑈 𝑉 u(𝑡) + 𝛼 ·
(2)
𝑁
where u(𝑡+1) and v(𝑡+1) , respectively, are the user and item
probability vectors in the 𝑡th iteration. The parameter 𝛼 ∈
(0, 1) denotes the random visit probability. On the one hand,
using 𝛼 is equivalent to adding a very small weight for each
node to connect with other nodes, thus guaranteeing the
connectivity of the constructed bipartite graph. On the other
hand, it is reasonable to assume that a user has some chance
to select one item seemingly not within her previous interests.
For example, in Eq. (1), a user node receives (1 − 𝛼) probability of being visited from its connected item nodes, and
𝛼 probability of being randomly visited by one of 𝑀 users.
Similar analysis applies in Eq. (2) for item transition.
The iteration terminates until the pairwise difference in
between two iteration probability vectors is smaller than a
predefined threshold. After the iteration termination, each
node in the graph can obtain its convergence probability,
which can reflect the importance of this node in the network
to some extent. According to the convergence probabilities,
we sort the list of users and the list of items in a decreasing

u(𝑡+1) = (1 − 𝛽) · P𝑉 𝑈 v(𝑡) + 𝛽 · r𝑈
v

(𝑡+1)

(𝑡)

= P𝑈 𝑉 u

(3)
(4)

where u(𝑡+1) and v(𝑡+1) , respectively, are the probability vectors that user and item nodes are visited in the 𝑡th iteration.
The restart probability 𝛽 denotes that in each iteration, there
is 𝛽 probability to return to the restart node 𝑢𝑎 and 1 − 𝛽
probability to walk from items to users. The iteration terminates until the pairwise difference in between the two iteration
probability vectors is smaller than a predefined threshold. Let
u𝑎 denote the convergence probability vector of user nodes,
which represents the correlation degree between these user
nodes and 𝑢𝑎 .
Similarly, we perform the random walk with restart for
each anchor item 𝑣𝑎 as follows: We use the one-hot vector to
initialize the probability vector of items as v(0) , and randomly
initialize the probability vector of users as u(0) . Let r𝑉 be
the item restart vector: If 𝑖 = 𝑣𝑎 , r𝑉 (𝑖) = 1; Otherwise,
r𝑉 (𝑖) = 0. The iteration process is as follows:
v(𝑡+1) = (1 − 𝛽) · P𝑈 𝑉 u(𝑡) + 𝛽 · r𝑉
(𝑡+1)

u

= P𝑉 𝑈 v

(𝑡)

(5)
(6)

Let v𝑎 denote the convergence probability vector of item
nodes, which represents the correlation degree between these
item nodes and 𝑣𝑎 .
For each anchor (𝑢𝑎 , 𝑣𝑎 ) ∈ 𝒜, we now obtain its convergence probability vectors u𝑎 and v𝑎 , by which we construct
a user convergence matrix C𝑈 ∈ R𝑀 ×𝐴 and an item convergence matrix C𝑉 ∈ R𝑁 ×𝐴 , respectively. The 𝑎th column in
C𝑈 is the user convergence vector u𝑎 for the anchor (𝑢𝑎 , 𝑣𝑎 ),
and the 𝑢th row in C𝑈 is the user 𝑢’s convergence probability
under different restart anchor node 𝑢𝑎 , which can reflect the
relationship between the user 𝑢 and each anchor user. We
define a local matrix scale control parameter 𝜌(0.5 < 𝜌 < 1)
for each user and each item. That is, a user can only be

1038

Short Research Papers 2A: AI, Mining, and others

SIGIR ’19, July 21–25, 2019, Paris, France

Local Matrix Approximation based on Graph Random Walk

SIGIR ’19, July 21–25, 2019, Paris, France

Table 1: Dataset Statistics

assigned into the user neighborhood of 𝜌 × 𝐴 anchors. The
same applies to each item.
For each user 𝑢, we select the top 𝜌 × 𝐴 anchors in the
decreasing order of the element value in the 𝑢th row of C𝑈 .
In the end, we construct a set of neighboring users 𝒰𝑎 for
each anchor 𝑎. Similarly, the 𝑎th column in C𝑉 is the item
convergence vector v𝑎 for the anchor (𝑢𝑎 , 𝑣𝑎 ), and the 𝑣th
row in C𝑉 can reflect the relationship between the item 𝑣
and each anchor item. We assign each item 𝑣 into the top
𝜌 × 𝐴 anchors in the decreasing order of the element value in
the 𝑣th row of C𝑉 and construct a set of neighboring items
𝒱𝑎 for each anchor 𝑎.
Finally, we construct a local matrix for each anchor 𝑎
based on the neighbor set 𝒰𝑎 and 𝒱𝑎 as follows: For each
neighbor user 𝑢 ∈ 𝒰𝑎 , we extract its corresponding 𝑢th row
in the original rating matrix R; And for each neighbor item
𝑣 ∈ 𝒱𝑎 , we extract its corresponding 𝑣th column in R. These
extracted rows and columns then construct the local matrix
R𝑎 ∈ R|𝒰𝑎 |×|𝒱𝑎 | for the anchor 𝑎.
We next discuss the coverage of local matrices. Let 𝒫
and 𝒫𝑎 denote the set of user-item pairs in R and in R𝑎 ,
respectively. For each element 𝑝 ∈ 𝒫, we define I(𝑝) = 1, if
𝑝 appears in at least one of 𝒫𝑎 s; Otherwise, I(𝑝) = 0. The
coverage of local matrices is defined as
1 ∑︁
I(𝑝).
(7)
𝜂=
|𝒫| 𝑝∈𝒫

Ciao
MovieLens-100K
MovieLens-1M
Zhihu Live

Lemma 2.1. Our local matrices can achieve complete coverage of the global rating matrix R.

2.3



Matrix Approximation

In each local matrix R𝑎 , we use the SVD algorithm[2] to
predict ratings. The loss function in each local matrix is:
∑︁
arg min
([U𝑎 V𝑎 𝑇 ]𝑢,𝑣 − R𝑢,𝑣 )2 + 𝜆(||U𝑎 ||2 + ||V𝑎 ||2 )
(U𝑎 ,V𝑎 )

where U𝑎 and V𝑎 , respectively, are the user-factor matrix
and item-factor matrix of R𝑎 after matrix factorization. We
train each local matrix by the gradient descent method, and
compute the prediction rating from user 𝑢 to item 𝑣 as:
R̂𝑎𝑢,𝑣 = [U𝑎 V𝑎 𝑇 ]𝑢,𝑣 (𝑎 = 1, 2, ..., 𝐴)

(8)

𝐴

Finally, we obtain the global rating approximation R̂𝑢,𝑣 by
averaging the results R̂𝑎𝑢,𝑣 from each local matrix to which
(𝑢, 𝑣) belongs.

3

#items
106797
1682
3952
3724

#rating
283119
100000
1000209
60285

density
0.04%
6.3%
3.65%
0.01%

Zhihu Live is a real-time Q&A interactive product launched
by Zhihu, a well-known online Q&A community in China.
We craweled Zhihu Live website to build our dataset. In
all the four datasets, the ratings are chosen from {1,...,5}.
Table. 1 summarizes the statistics of the four datasets, where
the density refers to the proportion of rated user-item pairs
among all user-item pairs.
We compare the performance of the following algorithms:
SVD [2]: It is a kind of global matrix factorization method,
which has been widely used for rating prediction.
LLORMA [3]: It splits the global rating matrix into many local
matrices and obtains the global ratings through averaging
the rating results from local matrix factorizations.
CLLORMA [6]: It improves the LLORMA by modifying its
random anchor selection.
RWLMA: It is our proposed local matrix approximation based
on graph random walk.
For the hyper-parameter settings, we set the learning rate
𝜇 = 0.01, L2-regularization coefficient 𝜆 = 0.001, maximum number of iterations as 50 for the matrix factorization.
The number of anchor points is fixed as 50 according to
LLORMA [3] and CLLORMA [6]. For our scheme, we set the
parameters as follows: 𝛼 = 0.2, 𝛽 = 0.5 and 𝜌 = 0.7. All
the experiment results are averaged through the standard
5-fold cross-validation. To evaluate the performance of rating
prediction, we apply the commonly used metrics: Mean Absolute Error (MAE) and Root Mean Square Error (RMSE).
Obviously, the more accurate the prediction, the lower value
of MAE and RMSE.
We observe from our experiments that for LLORMA and
CLLORMA, some user-item pairs in the test datasets cannot
be included into any of their constructed local matrices. In
this case, we use the SVD to predict the ratings for such
dangling pairs. We define a Coveage (Cvg) performance met𝑖𝑛𝑐 |
ric: 𝐶𝑜𝑣𝑒𝑟𝑎𝑔𝑒 = |𝒟
, where 𝒟𝑡𝑠𝑡 is the test dataset and
|𝒟𝑡𝑠𝑡 |
𝒟𝑖𝑛𝑐 is the set of testing user-item pairs that can be included
into at least one local matrix. Furthermore, as the size of a
local matrix impacts on the local matrix training efficiency,
we define a metric of Normalized Local
Matrix Average Size
∑︀

As we obtain the global predictions for missing values from
those local predictions of local matrices, it is desirable to
have complete coverage of the global rating matrix, that is,
𝜂 = 1. We have the following lemma.

Proof. Proof is omitted due to limited space.

#users
7375
943
6940
4482

|𝒟 𝑎

|

1
(NLMAS) as 𝑁 𝐿𝑀 𝐴𝑆 = |𝒟𝑡𝑟𝑛
· 𝑎=1𝐴 𝑡𝑟𝑛 , where 𝒟𝑡𝑟𝑛 is
|
𝑎
the training set and 𝒟𝑡𝑟𝑛 the set of training user-item pairs
in the local matrix of anchor 𝑎.
Experiment Results: Table 2 presents the experiment
results for the four datasets. We can first observe that the
SVD algorithm performs the worst in almost all cases. This is
not unexpected as it performs a global matrix factorization
over the original user-item rating matrix, which is very sparse
in all the four datasets. All the other methods are based on

EXPERIMENT

Experimental Settings: We conduct our experiments on
four datasets: Ciao, Movielens-100K(ML-100K), Movielens1M(ML-1M) and Zhihu Live(Zhihu). Movielens and Ciao are
public datasets widely used in the rating prediction problem.

1039

Short Research Papers 2A: AI, Mining, and others

SIGIR ’19, July 21–25, 2019, Paris, France

SIGIR ’19, July 21–25, 2019, Paris, France

Xuejiao Yang and Bang Wang

Table 2: Performances comparison on four datasets
Dataset Metrics SVD LLORMA CLLORMA RWLMA

Ciao

MAE 0.7768

0.7636
1.70%
RMSE 1.0341 1.0090
2.43%
Cvg
–
70.63%
NLMAS –
84.34%

0.7631
1.76%
1.0084
2.49%
68.74%
92.66%

0.7528
3.09%
0.9781
5.42%
100%
49.16%

MAE 0.7434

0.7111
4.34%
0.9119
5.04%
98.5%
75.04%

0.7074
4.84%
0.9019
6.08%
100%
49.15%

MAE 0.6740

0.6613
1.88%
0.8487
2.71%
99.5%
51.63%

0.6559
2.69%
0.8391
3.81%
100%
48.82%

0.5739
0.98%
0.8489
1.92%
95.3%
95.09%

0.5778
0.31%
0.8327
3.79%
100%
48.92%

0.7099
4.51%
ML-100K RMSE 0.9603 0.9104
5.20%
Cvg
–
99.2%
NLMAS –
72.92%
0.6600
2.08%
ML-1M RMSE 0.8723 0.8469
2.91%
Cvg
–
99.4%
NLMAS –
51.52%
MAE 0.5796

Zhihu

0.5814
-0.31%
RMSE 0.8655 0.8632
0.27%
Cvg
–
97.7%
NLMAS –
89.14%

(a) Varying 𝜌 of Ciao

(b) Varying 𝜌 of Zhihu Live

(c) Varying 𝜌 of Movielens-100K

(d) Varying 𝜌 of Movielens-1M

Figure 1: Performance for different choices of 𝜌.

4

CONCLUSION

In this paper, we have proposed the RWLMA scheme to
solve the classic matrix approximation problem by constructing many small local matrices. In RWLMA, a bipartite graph
for user-item pairs is first established. Based on the graph,
we have proposed to use the RW for anchor selection and the
RWR for neighborhood construction. We then construct local
matrices for each of anchors and compute local predictions in
each local matrix. Experiments on four datasets have shown
that our proposed scheme can outperform the state-of-the-art
schemes in terms of lower approximation errors and higher
coverage ratios. We notice that the RWLMA is merely based
on the original user-item rating matrix. Our future work shall
investigate how to integrate some other system information
for local matrix construction and factorization.

local matrix construction and factorization, so that the data
sparsity can be alleviated in each smaller local matrix to some
extent. In these local methods, we further observe that the
proposed RWLMA algorithm can achieve the best prediction
performance in almost all cases, with one exception that
its MAE is slightly larger than that of CLLORMA in the
Zhihu Live dataset. The results suggest the superiority of the
proposed anchor selection and neighborhood construction
based on the graph random walk.
As for the NLMAS metric, we observe that the proposed
RWLMA constructs smaller local matrices, compared with the
other local matrix methods LLORMA and CLLORMA, which
can help to reduce computation complexity in local matrix
factorization. Furthermore, for the Coverage of local matrices,
we can observe that the proposed RWLMA achieves 100%
coverage in all cases, yet the LLORMA and CLLORMA cannot
achieve 100% in many cases. They select neighbors from the
perspective of anchors based on the distance between anchors
and non-anchor pairs and have not provided a mechanism to
ensure complete coverage.
Figs. 1 plots the proposed RWLMA performance against
the local matrix scale control parameter 𝜌. It can be observed
that as 𝜌 decreases, the NLMAS becomes smaller, indicating
that local matrices become smaller; Furthermore, the MAE
and RMSE experience a slight degradation in three datasets,
but even perform slightly better in the Ciao dataset. We
argue that this slight degradation might be justified by a
higher computation efficiency, as local training and prediction
can be performed in smaller size local matrices.

REFERENCES
[1] A. Beutel, A. Ahmed, and A. J. Smola, “Accams: Additive coclustering to approximate matrices succinctly,” in Proceedings
of the 24th International Conference on World Wide Web, ser.
WWW ’15, no. 11, 2015, pp. 119–129.
[2] Y. Koren, R. Bell, and C. Volinsky, “Matrix factorization techniques
for recommender systems,” Computer, vol. 42, no. 8, pp. 30–37,
2009.
[3] J. Lee, S. Kim, G. Lebanon, Y. Singer, and S. Bengio, “Llorma:
local low-rank matrix approximation,” The Journal of Machine
Learning Research, vol. 17, no. 1, pp. 442–465, 2016.
[4] L. Mackey, A. Talwalkar, and M. I. Jordan, “Divide-and-conquer
matrix factorization,” in Advances in Neural Information Processing Systems. Curran Associates, Inc., 2011, pp. 1134–1142.
[5] L. Page, S. Brin, R. Motwani, and T. Winograd, “The pagerank
citation ranking: Bringing order to the web,” Stanford InfoLab,
Technical Report 1999-66, 1999.
[6] M. Zhang, B. Hu, C. Shi, and B. Wang, “Local low-rank matrix
approximation with preference selection of anchor points,” in Proceedings of the 26th International Conference on World Wide
Web Companion, ser. WWW ’17 Companion, no. 9, 2017, pp.
1395–1403.

1040

