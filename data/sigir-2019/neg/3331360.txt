Short Research Papers 3A: AI, Mining, and others

SIGIR ’19, July 21–25, 2019, Paris, France

Sparse Tensor Co-Clustering as a Tool for Document
Categorization
Rafika Boutalbi1,2
1 LIPADE

- University of Paris
196 rue Saint Honoré, 75001 Paris
rafika.boutalbi@parisdescartes.fr

2 TRINOV,

Lazhar Labiod1
1 LIPADE

- University of Paris
lazhar.labiod@parisdescartes.fr

ABSTRACT

- University of Paris
mohamed.nadif@parisdescartes.fr

Incorporating this additional information leads us to consider a
tensor representation of the data.
Despite the great interest for co-clustering and the tensor representation, few works tackle the co-clustering from tensor data.
In fact, a large part of works are devoted mainly to popular factorization approaches such as Tucker-decomposition [15] and
PARAFAC [8]. We can nevertheless mention the works related to
our proposal such as the work of [2] based on Minimum Bregman
information (MBI) to find co-clustering of a tensor. Most recently, in
[16] the General Tensor Spectral Co-clustering (GTSC) method for
co-clustering the modes of non-negative tensor has been developed.
In [5] the authors proposed a tensor biclustering algorithm able to
compute a subset of tensor rows and columns whose corresponding trajectories form a low-dimensional subspace. However, the
majority of authors consider the same entities, for both sets of rows
and columns, or do not consider the tensor co-clustering under a
probabilistic approach.
To the best of our knowledge, this is the first attempt to formulate
our objective when both sets -rows and columns- are different and
with model-based co-clustering. To this end, we rely on the latent
block model [7] for its flexibility to consider any data matrices. We
propose a co-clustering model for sparse tensor data which can be
viewed as a multi-way clustering model where each slice of the
third dimension of the tensor represents a relation between two
sets (see Figure 1). The goal is to simultaneously discover the row
and column clusters and the relationship between these clusters
for all slices.

To deal with document clustering, we usually rely on documentterm matrices. However, from additional available information like
keywords, co-authors, citations we might rather exploit a reorganization of the data in the form of a tensor.
In this paper, we extend the use of the Sparse Poisson Latent Block
Model to deal with sparse tensor data using jointly all information
arising from documents. The proposed model is parsimonious and
tailored for this kind of data. To estimate the parameters, we derive a
suitable tensor co-clustering algorithm. Empirical results on several
real-world text datasets highlight the advantages of our proposal
which improves the clustering results of documents.

CCS CONCEPTS
• Unsupervised learning; • Co-clustering → Tensor data; • Text
mining;

KEYWORDS
Co-clustering; Tensor data; Text mining.
ACM Reference Format:
Rafika Boutalbi1, 2 , Lazhar Labiod1 , and Mohamed Nadif1 . 2019. Sparse
Tensor Co-Clustering as a Tool for Document Categorization. In Proceedings
of the 42nd International ACM SIGIR Conference on Research and Development
in Information Retrieval (SIGIR ’19), July 21–25, 2019, Paris, France. ACM,
New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331360

1

Mohamed Nadif1
1 LIPADE

INTRODUCTION

Co-clustering which is a simultaneous clustering of both dimensions of a data matrix has proven to be more useful than traditional
one-sided clustering especially when dealing with sparse data as
is the case with document-term matrices. Generally, in document
clustering, we rely on such matrices where each cell represents the
occurrence of a word on a document. However, there is some additional available information like Keywords, co-authors, citations
which not taken into account and it can improve the clustering
results; two documents that have one or more authors in common
and/or that quote each other, are likely to deal with the same topic.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’19, July 21–25, 2019, Paris, France
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00
https://doi.org/10.1145/3331184.3331360

Figure 1: Goal of co-clustering for Sparse Tensor.

2

LATENT BLOCK MODEL (LBM)

Given an n × d data matrix X = (x i j , i ∈ I = {1, . . . , n}; j ∈ J =
{1, . . . , d }). It is assumed that there exists a partition on I and a partition on J . A partition of I ×J into д×m blocks will be represented by
a pair of partitions (Z, W). The k-th row cluster corresponds to the
set of rows i such that zik = 1 and zik ′ = 0 ∀k ′ , k. Thereby, the

1157

Short Research Papers 3A: AI, Mining, and others

SIGIR ’19, July 21–25, 2019, Paris, France

partition Z can be represented by a matrix of elements in {0, 1}д satÍд
isfying k =1 zik = 1. Similarly, the ℓ-th column cluster corresponds
to the set of columns j and the partition W can be represented by a
Í
matrix of elements in {0, 1}m satisfying m
ℓ=1 w j ℓ = 1. Considering
the Latent Block Model (LBM) [7], it is assumed that each element
x i j of the kℓth block is generated according to a parameterized
probability density function (pdf) f (x i j ; α k ℓ ). Furthermore, in the
LBM the univariate random variables x i j are assumed to be conditionally independent given (Z, W). Thereby, the conditional pdf
Î
of X can be expressed as i, j,k, ℓ { f (x i j ; α k ℓ )}zik w j ℓ . From this
hypothesis, we then consider the latent block model where the two
sets I and J are considered as random samples and the row, and
column labels become latent variables. Therefore, the parameter
of the latent block model is Θ = (π, ρ, α ), with π = (π 1 , . . . , πд )
and ρ = (ρ 1 , . . . , ρm ) where (πk = P(zik = 1), k = 1, . . . , д),
(ρ ℓ = P(w j ℓ = 1), ℓ = 1, . . . , m) are the mixing proportions and
α = (α k ℓ ; k = 1, . . . д, ℓ = 1, . . . , m) where α k ℓ is the parameter of
the distribution of block kℓ. Denoting Z and W the sets of possible
labels Z for I and W for J , the pdf f (X; Θ) of X can be written
Õ
Ö
Ö
Ö w
z
πk ik
ρℓ jℓ ×
{ f (x i j ; α k ℓ )}zik w j ℓ .
(1)
j, ℓ

(z,w )∈Z×W i,k

It maximizes the log-likelihood f (X, Ω) w.r. to Ω iteratively by
maximizing the conditional expectation of the complete data loglikelihood LC (Z, W; Ω) w.r. to Ω, given a previous current estimate
Ω(c) and the observed data X. To solve this problem an approximation using the interpretation of the EM algorithm has been proposed;
see, e.g., [6]. More precisely, the authors rely on the variational
approach which consists in approximating the true likelihood by
another expression using the following independence assumption:
P(zik = 1, w j ℓ = 1|X) = P(zik = 1|X)P(w j ℓ = 1|X) = z̃ik w̃ j ℓ .
Hence, the aim is to maximize the following lower bound of the
log-likelihood criterion:
FC (Z̃, W̃; Ω) = LC (Z̃, W̃, Ω) + H (Z̃) + H (W̃)
(2)
Í
Í
where H (Z̃) = − i,k z̃ik log z̃ik , H (W̃) = − j, ℓ w̃ j ℓ log w̃ j ℓ and
LC (Z̃, W̃, Ω) is the fuzzy complete-data log-likelihood is given by
!
v
Õ
Õ
Õ
Õ
b b
z̃ik log πk +
w̃ j ℓ log ρ ℓ +
z̃ik w̃ j ℓ
log Φ(x i j ; λk ℓ ) .

i, j,k, ℓ

TENSOR LATENT BLOCK MODEL (TLBM)

Hereafter, we propose a novel Latent Block model for tensor data
(TLBM). Few studies have addressed the issue of co-clustering for
tensor data [5, 16]. Unlike classical LBM which considers data matrix X = [x i j ] ∈ Rn×d , TLBM considers 3D data matrix X = [xi j ] ∈
Rn×d×v where n is the number of rows, d the number of columns,
and v the number of covariates (slices). Figure 2 presents a tensor
data with v slices. In this case, we can consider an extension of

TENSOR SPARSE POISSON LBM (TSPLBM)

(the third term of LC (Z̃, W̃, Ω) (3)) takes the following form
!
!
v
v
Õ
Õ
Õ
Õ
z̃ik w̃ jk
log Φ(x ibj ; λbkk ) +
z̃ik w̃ j ℓ
log Φ(x ibj ; λb ) .
i, j,k

n

xi j

v

i, j,k, ℓ,k

b=1

some algebraic calculations and simplifications, (3) becomes (up a
constant)
Õ
Õ
ÕÕ
b
b
b
z̃ik log πk +
w̃ jk log ρ k +
(x kk
log(γkk
) − x kb . x b.k γkk
)

v

Figure 2: Data structure; xi j = (x i1j , . . . , x ibj , . . . , x ivj )

i,k

the latent block model (TLBM). The Φ(xi j ; λk ℓ ) is the pdf function,
where λk ℓ a v × 1 vector formed by (λk1 ℓ , . . . , λbk ℓ , . . . , λvk ℓ ) of the
parameters of Φ. The complete data log-likelihood LC (Z, W, Ω) is
given by
Õ
Õ
Õ
zik log πk +
w j ℓ log ρ ℓ +
zik w j ℓ log(Φ(xi j ; λk ℓ ))
j, ℓ

b=1

For each block k = 1, . . . , д and each slice b, the x ibj ’s are distributed
b ) and outside according P(x b x b γ b ). After
according P(x ib. x b.j γkk
i . .j

d

i,k

b=1

Recently, in [1, 11] the authors proposed a generative mixture
model for co-clustering document-term matrices. With SPLBM,
the authors first assume that for each diagonal block kk the values
Í
Í
x i j ∼ P(λi j ) where λi j = x i . x .j k [zik w jk ]γkk with x i . = j x i j
Í
and x .j = i x i j . Second, they assume that for each block kℓ with
k , ℓ, x i j ∼ P(λi j ) where the parameter λi j takes the following
Í
form : λi j = x i . x .j k, ℓ,k [zik w j ℓ ]γ . Assuming ∀ℓ , k, γk ℓ = γ
leads to suppose that all blocks outside the diagonal share the
same parameter. SPLBM has been designed from the ground up to
deal with data sparsity problems. As a consequence, in addition to
seeking homogeneous blocks, it also filters out homogeneous but
noisy ones due to the sparsity of the data.
In the following we propose to extend SPLBM
Í to deal with tensor
Í
v log Φ(x b ; λb )
data. It is easy to show that i, j,k z̃ik w̃ jk
ij kℓ
b=1

i, j,k, ℓ

j, ℓ

i,k

i, j,k, ℓ

(3)

4

Assuming that the complete data are the vector (X, Z, W), i.e, we
assume that the latent variables Z and W are known, the resulting
complete data log-likelihood of the latent block model LC (Z, W, Θ) =
log f (X, Z, W; Θ) can be written as follows
Õ
Õ
Õ
w j ℓ log ρ ℓ +
zik log πk +
zik w j ℓ log f (x i j ; α k ℓ ).

3

j, ℓ

i,k

+

j,k

Õ

(Nb −

b

=

Õ

Õ
k

i, j,k, ℓ

b
x kk
) log(γ b ) − (Nb2

z̃ik log πk +

i,k

b

Õ

+

Õ Õ
b

−

Õ
k

x kb . x b.k )γ b

!

w̃ jk log ρ k

j,k

"

where Ω is formed by π , ρ and λ = (λ 11 , . . . , λдm ). Assuming
the conditional independence per block which
Í is here a cube, the

Í
third term of LC becomes i, j,k, ℓ zik w j ℓ vb=1 log Φ(x ibj ; λbk ℓ ) .
To estimate Ω, the EM algorithm [3] is a candidate for this task.

k

k

b
x kk

log(

b
γkk

b
) − x kb . x b.k (γkk
γb

b

#

− γ ) + Nb (log(γ ) − Nb γ ) ,

Í
Í
b = Í z̃ w̃ x b and
where x kb . = i z̃ik x ib. , x b.k = j w̃ jk x b.j , x kk
i, j ik jk i j
Í
Nb = i, j x ibj .

1158

!

Short Research Papers 3A: AI, Mining, and others

5

SIGIR ’19, July 21–25, 2019, Paris, France

VARIATIONAL EM ALGORITHM

We use three text datasets DBLP1, DBLP2 and PubMed Diabetes1
to highlight the objective of the proposed algorithm. DBLP1 and
DBLP2 are constructed from DBLP2 , by selecting three journals for
each one. The selected journals for DBLP1 are SIGMOD, STOC, and
SIGIR. The journals selected for DBLP2 are Discrete Applied Mathematics, IEEE software, and SIGIR. For PubMed Diabetes dataset the
papers are categorized into three types, the first one deals with Diabetes mellitus of type 1, the second with Diabetes mellitus of type
2, and the third with Diabetes mellitus Experimental. We extract
from these different datasets information linked documents:
- Co-terms matrix on the title: each cell represents the number of
times that a term is present simultaneously in the title of a pair
of papers,
- Co-terms matrix on the abstract: each cell represents the number of times that a pair of papers share a term extracting from
abstract,
- Co-authors matrix: each cell represents the number of common
authors of a pair of papers,
- Citations matrix: is a binary data matrix where 1 indicates the
presence of a citation between two papers.
The constructed tensor (Paper × Paper × Relation) for each dataset
DBLP1, DBLP2 and PubMed Diabetes has respectively size (2223 ×
2223 × 4), (1949 × 1949 × 4), and (4354 × 4354 × 4) and different rates
of sparsity 0.93, 0.94, and 0.69 respectively. In Figures 3, 4 and 5, the
first plots represent the low-dimensional projection of papers from
tensor data of each dataset using the Multiple Factor Analysis (MFA).
MFA deals with a multiple table where the slices are contingency
tables [10]. We notice that the three datasets have different degree
of complexity. On the other hand, the four other plots of each figure
represent the slices of tensor namely Co-terms matrices on the
titles and abstracts, Co-authors and Citations matrices. In Table 1
are reported the performances of the seven algorithms (cited above)
on the three datasets. In terms of ACC, NMI and ARI, we observe
in most cases, that VEM-ST is better than other algorithms applied
on each slice and those applied on tensor data. We observe that
PubMed Diabets which is the least sparse dataset, we obtain the
lowest results for the three measures ACC, NMI and ARI due to the
complex structure of dataset appearing on figure 5. Further note
that GTSC, less effective than VEM-ST, reaches better results than
PARAFAC and Tucker-decomposition followed by K-means.

In what follows, we detail the Expectation (E) and Maximization
(M) step of the Variational EM algorithm for tensor data. The Estep consists in computing, for all i, j, k the posterior probabilities
z̃ik and w̃ jk maximizing FC given the estimated parameters Ω. As
Í
Í
k z̃ik = 1 and k w̃ jk = 1, using the corresponding Lagrangians,
up to terms which are not function of z̃ik and w̃ jk leads to
v
Õ
γb ª
©Õ
z̃ik ∝ πk exp ­ w̃ jk
x ibj log( kk )® ,
γb
b=1
« j
¬
!
b
v
Õ
Õ
γ
w̃ jk ∝ ρ k exp
x ibj log( kk ) .
z̃ik
γb
i
b=1

Given the previously computed posterior probabilities Z̃ and W̃, the
M-step consists in updating , ∀k, the parameters πk , ρ k , γ bkk and
γ b maximizing FC (Z̃, W̃, Ω). The estimated parameters are defined
Í
as follows. First, taking into account the constraints
k πk =Í1 and
Í
Í
j w̃ jk
i z̃ ik
ρ
=
1,
it
is
easy
to
show
that
π
=
and
ρ
.
k
k =
k k
n
d
Secondly, it is easy to derive
Í
b
xb
i, j z̃ik w̃ jk x i j
b
γ kk = Í
= kk and,
Í
b
b
x kb . x b.k
i z̃ik x i . j w̃ jk x .j
Í
Í b
Nb − i, j,k z̃ik w̃ jk x ibj
Nb − k x kk
b
γ =
=
.
Í Í
Í
Í
Nb2 − k i z̃ik x ib. j w̃ jk x b.j
Nb2 − k x kb . x b.k
The proposed algorithm for sparse tensor (ST) data, referred to
as VEM-ST in Algorithm 1, alternates the two previously described
steps Expectation-Maximization. At the convergence, a hard coclustering is deduced from z̃ik ’s and w̃ jk ’s using the maximum a
posteriori principle.
Algorithm 1: VEM-ST
Input: X, д.
Initialization (Z, W) randomly, compute Ω
repeat
E-Step : Compute z̃ik and w̃ jk


Í
Í
γb
• z̃ik ∝ πk exp j w̃ jk vb=1 x ibj log( γkbk )


Í
Ív
γ kbk
b
• w̃ jk ∝ ρ k exp i z̃ik b=1 x i j log( γ b )

7

M-Step : Update Ω
until convergence;
return Z, W

6

CONCLUSION

To deal with document categorization with additional information like key-words, co-authors and citations, we proposed a data
structure including all information and a novel model-based coclustering on tensor data. The proposed model is an extension of
the latent block model (LBM). We have derived a suitable sparse
tensor Poisson co-clustering model (STLBM). To estimate the parameters, a variational EM algorithm is developed, referred to as
VEM-ST. VEM-ST does a better job than GTSC, and other competitive
algorithms applied on each slice of tensor data.
Due to the flexibility of the proposed model, our findings open
up interesting opportunities for other applications like relational

EXPERIMENTS

In our experiments, we aim to evaluate VEM-ST in terms of document clustering leading to measure the impact of additional information. Thereby, we compare VEM-ST with Spherical K-means, Itcc
[4], and VEM-STb applied on each slice b of tensor and three other
algorithms applied on tensor data namely Tucker-decomposition
[9], PARAFAC [9] and GTSC [16]. Note that the first two are used with
ranks number equals to 10 and followed by K-means. We perform 30
random initialisations, and compute the Accuracy (ACC), Adjusted
Rand index (ARI) [13] and Normalized Mutual Information (NMI)
[14] metrics.

1 https://linqs.soe.ucsc.edu/data
2 https://aminer.org/citation

1159

Short Research Papers 3A: AI, Mining, and others

SIGIR ’19, July 21–25, 2019, Paris, France

Figure 3: DBLP 1

Figure 4: DBLP 2

Figure 5: PubMed Diabets
Table 1: Evaluation of documents clustering in terms of ACC, NMI and ARI for the three datasets

PubMed Diabets

DBLP2

DBLP1

Datasets

Metrics
Algorithms
Spherical K-means
Itcc
VEM-STb
PARAFAC1
Tucker-decomp1
GTSC2
VEM-ST
Spherical K-means
Itcc
VEM-STb
PARAFAC1
Tucker-decomp1
GTSC2
VEM-ST
Spherical K-means
Itcc
VEM-STb
PARAFAC1
Tucker-decomp1
GTSC2
VEM-ST

Titles
0.75 ± 0.0
0.72 ± 0.0
0.72± 0.0

0.52 ± 0.0
0.43 ± 0.0
0.45 ± 0.0

0.54 ± 0.0
0.55 ± 0.0
0.54 ± 0.0

ACC
Abstracts Co-authors
0.79 ± 0.0
0.4 ± 0.0
0.79 ± 0.0
0.41 ± 0.0
0.72 ± 0.0
0.43 ± 0.0
0.54 ± 0.01
0.55 ± 0.0
0.87 ± 0.0
0.89 ± 0.0
0.69 ± 0.0
0.37 ± 0.0
0.80 ± 0.0
0.37 ± 0.0
0.80 ± 0.0
0.38 ± 0.0
0.65 ± 0.0
0.62 ± 0.0
0.55 ± 0.0
0.81 ± 0.0
0.64 ± 0.0
0.38 ± 0.0
0.64 ± 0.0
0.37 ± 0.0
0.61 ± 0.0
0.37 ± 0.0
0.54 ± 0.0
0.53 ± 0.0
0.53 ± 0.0
0.64 ± 0.0

Citations
0.43 ± 0.0
0.41 ± 0.0
0.42 ± 0.0

Titles
0.38 ± 0.0
0.36 ± 0.0
0.38 ± 0.0

0.41 ± 0.0
0.39 ± 0.0
0.39 ± 0.0

0.13 ± 0.0
0.05 ± 0.0
0.05 ± 0.0

0.43 ± 0.0
0.43 ± 0.0
0.44 ± 0.0

0.18 ± 0.0
0.18 ± 0.0
0.19 ± 0.0

NMI
Abstracts Co-authors
0.41 ± 0.0
0.02 ± 0.0
0.41 ± 0.0
0.02 ± 0.0
0.39 ± 0.0
0.02 ± 0.0
0.10 ± 0.0
0.11 ± 0.0
0.61 ± 0.0
0.61 ± 0.0
0.30 ± 0.0
0.01 ± 0.0
0.45 ± 0.0
0.03 ± 0.0
0.47 ± 0.0
0.01 ± 0.0
0.16 ± 0.0
0.14 ± 0.0
0.27 ± 0.0
0.55 ± 0.0
0.29 ± 0.0
0.01 ± 0.0
0.3 ± 0.0
0.01 ± 0.0
0.3 ± 0.0
0.0 ± 0.0
0.14 ± 0.0
0.10 ± 0.0
0.26 ± 0.0
0.33 ± 0.0

Citations
0.03 ± 0.0
0.02 ± 0.0
0.02 ± 0.0

Titles
0.45 ± 0.0
0.42 ± 0.0
0.43 ± 0.0

0.05 ± 0.0
0.04 ± 0.0
0.02 ± 0.0

0.09 ± 0.0
0.04 ± 0.0
0.05 ± 0.0

0.0 ± 0.0
0.0 ± 0.0
0.0 ± 0.0

0.17 ± 0.0
0.16 ± 0.0
0.18 ± 0.0

ARI
Abstracts Co-authors
0.48 ± 0.0
0.02 ± 0.0
0.49 ± 0.0
0.02 ± 0.0
0.43 ± 0.0
0.02 ± 0.0
0.08 ± 0.0
0.04 ± 0.0
0.65 ± 0.0
0.70 ± 0.0
0.28 ± 0.0
0.01 ± 0.0
0.5 ± 0.0
0.01 ± 0.0
0.5 ± 0.0
0.01 ± 0.0
0.11 ± 0.0
0.07 ± 0.0
0.25 ± 0.0
0.56 ± 0.0
0.26 ± 0.0
0.01 ± 0.0
0.28 ± 0.0
0.0 ± 0.0
0.28 ± 0.0
0.0 ± 0.0
0.10 ± 0.0
0.05 ± 0.0
0.24 ± 0.0
0.31 ± 0.0

Citations
0.02 ± 0.0
0.01 ± 0.0
0.01 ± 0.0

0.01 ± 0.0
0.01 ± 0.0
0.0 ± 0.0

0.0 ± 0.0
0.0 ± 0.0
0.0 ± 0.0

1 PARAFAC and Tucker-decomp followed by K-means,
2 GTSC with appropriate parameters to obtain the desired number of co-clusters.

[8] Harshman, R. A., and Lundy, M. E. Parafac : parallel factor analysis. Computational statistics and data analysis 18 (1994), 39–72.
[9] Kossaifi, J., Panagakis, Y., Anandkumar, A., and Pantic, M. Tensorly: Tensor
learning in python. CoRR abs/1610.09555 (2018).
[10] Pagès, J. Multiple factor analysis by example using R. Chapman and Hall/CRC,
2014.
[11] Role, F., Morbieu, S., and Nadif, M. Coclust: A python package for co-clustering.
Journal of Statistical Software 88, 7 (2019), 1–29.
[12] Salah, A., and Nadif, M. Directional co-clustering. Advances in Data Analysis
and Classification (2018), 1–30.
[13] Steinley, D. Properties of the hubert-arabie adjusted rand index. Psychological
methods 9, 3 (2004), 386.
[14] Strehl, A., and Ghosh, J. Cluster ensembles - a knowledge reuse framework
for combining multiple partitions. Journal of Machine Learning Research 3 (2002),
583–617.
[15] Tucker, L. R. Some mathematical notes on three-mode factor analysis. Psychometrika 31, 3 (1966), 279–311.
[16] Wu, T., Benson, A. R., and Gleich, D. F. General tensor spectral co-clustering
for higher-order data. In Advances in Neural Information Processing Systems 29.
Curran Associates, Inc., 2016, pp. 2559–2567.

learning. Furthermore, other block models relying on other appropriate distributions can be thought for tensor data; see, e.g., [12].

REFERENCES
[1] Ailem, M., Role, F., and Nadif, M. Sparse poisson latent block model for
document clustering. IEEE Transactions on Knowledge and Data Engineering 29, 7
(2017), 1563–1576.
[2] Banerjee, A., Krumpelman, C., Ghosh, J., Basu, S., and Mooney, R. J. Modelbased overlapping clustering. In Proceedings of the Eleventh ACM SIGKDD (2005),
pp. 532–537.
[3] Dempster, A. P., Laird, N. M., and Rubin, D. B. Maximum likelihood from
incomplete data via the EM algorithm. Journal of the Royal Statistical Society:
Series B 39 (1977), 1–38.
[4] Dhillon, I. S., Mallela, S., and Modha, D. S. Information-theoretic co-clustering.
In Proceedings of the Ninth ACM SIGKDD (2003), pp. 89–98.
[5] Feizi, S., Javadi, H., and Tse, D. Tensor biclustering. In Advances in Neural
Information Processing Systems 30 (2017), Curran Associates, Inc., pp. 1311–1320.
[6] Govaert, G., and Nadif, M. An EM algorithm for the block mixture model. IEEE
Transactions on Pattern Analysis and machine intelligence 27, 4 (2005), 643–647.
[7] Govaert, G., and Nadif, M. Co-clustering. Wiley-IEEE Press, 2013.

1160

