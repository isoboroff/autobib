Short Research Papers 1C: Search

SIGIR ’19, July 21–25, 2019, Paris, France

Multi-Level Matching Networks for Text Matching
Chunlin Xu, Zhiwei Lin, Shengli Wu, Hui Wang∗
Faculty of Computing, Engineering and Built Environment, Ulster University
{xu-c,z.lin,s.wu1,h.wang}@ulster.ac.uk

ABSTRACT

relationship based on the two extracted representations, taking
no account of interaction between two texts [1, 3]. The second
approach is based on the attention mechanism [2, 4, 14], in which
words in two texts are matched firstly. Then these word level matching results are aggregated into a fixed-size vector for making final
text level matching decision. Combining with bidirectional long
short term memory (BiLSTM) [6], previous models based on attention mechanisms have achieved the state-of-the-art performances
for text matching [2, 14].
However, the problem with the second approach lies in the fact
that previous models only use the final representaions of words
to obtain the word level matching results for text level matching
decision without considering other levels of word representations.
For example, the state-of-the-art model ESIM [2], firstly uses the
low level pre-trained word embeddings [10] as inputs to a BiLSTM
layer to generate high level contextualized word representations
for representing words and their contextual information. Then an
attention mechanism is employed to conduct word level matching
solely based on high level contextualized word representations without considering low level representations, which can not capture
sufficient information for modeling complex matching relations.
For example, obviously, “I went to London yesterday” and “I went
to Beijing yesterday” have different meanings and they should not
be matched. However, because the contextual information of ‘London’ and ‘Beijing’ are very similar, the high level contextualized
representations of these two words generated by BiLSTM layer
will be very close in word representation space, which may not
be sufficient to differentiate the two words, thus leading to incorrect matching decision. If the low-level word embeddings of these
two words, which may be far from each other in embedding space,
are also considered, the model would be aware of the difference
between words ‘London’ and ‘Beijing’, which is helpful for making correct matching decision. Therefore, it is important to have
multiple levels of matching to capture more matching information,
hence yielding correct matching decision.
In order to address the above limitation, this paper presents a
multi-level matching network (MMN) for text matching, which
utilizes multiple levels of word representations to obtain multiple
word level matching results for final text level matching decision.
In each matching level, an attention mechanism is firstly used
to learn the attention-aware representation of each word in two
texts and to make word level matching at current level. Next, a
fusion gate is used to combine the attention-aware representation
with original representation of each word for word representation
refinement. Then, a BiLSTM encoder is employed to generate new
word representations which will be used as the inputs for next
matching level. The above process is repeated for k times. Finally,
the matching results of k matching levels are aggregated for final
decision. The contributions of this paper are summarized as follows:

Text matching aims to establish the matching relationship between
two texts. It is an important operation in some information retrieval
related tasks such as question duplicate detection, question answering, and dialog systems. Bidirectional long short term memory
(BiLSTM) coupled with attention mechanism has achieved state-ofthe-art performance in text matching. A major limitation of existing
works is that only high level contextualized word representations
are utilized to obtain word level matching results without considering other levels of word representations, thus resulting in incorrect
matching decisions for cases where two words with different meanings are very close in high level contextualized word representation
space. Therefore, instead of making decisions utilizing single level
word representations, a multi-level matching network (MMN) is
proposed in this paper for text matching, which utilizes multiple levels of word representations to obtain multiple word level matching
results for final text level matching decision. Experimental results
on two widely used benchmarks, SNLI and Scaitail, show that the
proposed MMN achieves the state-of-the-art performance.

KEYWORDS
text matching, attention, multi-level matching network
ACM Reference Format:
Chunlin Xu, Zhiwei Lin, Shengli Wu, Hui Wang. 2019. Multi-Level Matching
Networks for Text Matching . In Proceedings of the 42nd International ACM
SIGIR Conference on Research and Development in Information Retrieval
(SIGIR ’19), July 21–25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages.
https://doi.org/10.1145/3331184.3331276

1

INTRODUCTION

Text matching is to compare two texts in order to establish their relationship – such as whether the two texts share the same meaning
or not. It is an important operation in some information retrieval
related tasks such as question duplicate detection [16], question
answering [13], and dialog systems [15].
Deep learning has been widely applied to text matching in recent years [1, 2, 9] and the existing deep models for text matching
can be categorized into two approaches. The first approach models
two texts by encoding each text separately and then predicts their
∗ Corresponding

Author

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’19, July 21–25, 2019, Paris, France
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00
https://doi.org/10.1145/3331184.3331276

949

Short Research Papers 1C: Search

SIGIR ’19, July 21–25, 2019, Paris, France

k ) and (hk , . . . , hk ). A word-by-word
matching level: (p1k , . . . , pm
n
1
matching layer is firstly employed to obtain the word level matching
results at current level.
Word-by-Word Matching. To perform word level matching,
co-attention matrix Ak = (α ikj )m×n between two texts are computed
by the following equations:

• A new multi-level matching network (MMN) is proposed for
text matching. The model can capture more matching information by utilizing multiple levels of word representations.
• An attention aware representation fusion (AARF) layer is
devised to refine word representaions in each matching level.
• The model is evaluated on two popular benchmarks, SNLI
and Scaitail. Experimental results show that the model outperforms state-of-the-art baselines.

2

T

α ikj = pik · hkj
where α ikj

THE MMN MODEL

(1)

indicates the relevance between the i-th
word hkj

word pik

of text

T
α ikj

P and j-th
of text H ,
=
otherwise. Next, for
each word in one text, the relevant semantics in the other text is
extracted and composed based on the co-attention matrices α ikj and

The overall framework of the proposed MMN model is shown in
Fig. 1. It consists of five layers:
(1) the input layer for a pair of texts;
(2) the word embedding layer for representing each word in the
two texts as a vector;
(3) the multi-level matching layer whose architecture is shown
in Fig. 2;
(4) the aggregation layer, where matching results from all matching levels are aggregated into a fixed-size vector;
(5) the prediction layer.

k
and α ji

k by the following equations:
α ji

p ki

k

hj

 
exp α ikj
  hkj
=
Ín
k
exp
α ir
j=1 r =1
 
k
m
exp α ji
Õ
  pik
=
Ím
k
i=1 r =1 exp α r j
n
Õ

(2)

(3)

k

where p ki and h j are the attention-aware representations of pik
and hkj , representing the contents in {hkj }nj=1 related to pik and the
m related to hk , respectively. Then we use a vector
contents in {pik }i=1
j
k

matching function on each pair of < pik , p ki > and < hkj , h j > to
obtain the word level matching results at current level between two
texts.
p
(4)
tki = pik ⊙ p ki
k

tkhj = hkj ⊙ h j

(5)

p
operation, tki

where ⊙ is the element-wise product
and tkhj are the
matching results at k-th level of matching P against H and matching
H against P, respectively.
Attention Aware Representaion Fusion (AAFR). This layer
is utilized to refine the word representation vectors. In this paper,
a fusion gate is used to incorporate the attention-aware representations into original representations of each word in two texts for
word representation refinement.

Figure 1: The framework of the proposed MMN.

2.1

Word Embedding Layer
p

p

Given a pair of texts P = (w 1 , . . . , wm ) with m words and H =
(w 1h , . . . , w nh ) with n words, the purpose of this layer is to convert
each word in text P and text H into a d-dimensional vector denoted
as pi ∈ Rd and h j ∈ Rd . The d-dimensional column vector is
composed of two parts: a word-level embedding and a characterlevel embedding. The word-level embedding is obtained from a
pre-trained word embedding matrix Glove [10]. Then we feed each
character within a word into a BiLSTM, and the last time-step
output of the BiLSTM is used as the character-level embedding,
which is the same as [14].

2.2

Fp = siдmoid(Wp1pik + Wp2p ki + bp )
k

(6)

Fh = siдmoid(Wh1hkj + Wh2h j + bh )

(7)

peik = Fp ⊙ pik + (1 − Fp ) ⊙ p ki

(8)

hekj

=

k
+ (1 − Fh ) ⊙ h j
Rdl ×dl and bp , bh

Fh ⊙ hkj

(9)

where Wp1,Wp2,Wh1,Wh2 ∈
∈ Rdl are the learable parameters of the fusion gate. ⊙ is the element-wise product
operation. peik , hekj ∈ Rdl are the refined word representation vectors
after fusion.
BiLSTM Encoder. Next, a BiLSTM encoder layer is employed
to encode the contexutal information into the above refined representation vectors to generate new word representations.

Multi-level Matching Layer

This layer obtains the matching results based on different levels of
word representations. During the k-th matching level, given the
representations of two texts P and H computed in the previous

k+1 k +1
pik +1 = BiLST M(e
pik , pi−1
, pi+1 )

950

(10)

Short Research Papers 1C: Search

SIGIR ’19, July 21–25, 2019, Paris, France

Figure 2: Architecture of multi-level matching layer (taking k = 3 as an example).
+1 k+1
hkj +1 = BiLST M(hekj , hkj−1
, h j+1 )

where pik +1, hk+1
j

contradiction, neutral. Scitail is constructed from science domain,
which contains about 27K sentence pairs. Unlike the SNLI dataset,
SciTail uses only two labels: entailment, neutral.
In this paper, word embeddings are initialized with the 300d
GloVe word vectors [10]. The dimensions of the BiLSTM encoders
are set as 400 in multi-level matching layer and 600 for aggregation
layer. The number of aggregation BiLSTM layers is set as 2. The
number of matching levels is tuned from [1, 4]. Batch sizes are 32
for Scitail dataset and 128 for SNLI dataset. The Adam optimizer
[8] is used for training, and the initial learning rate is set as 0.001.
To avoid overfitting, we apply dropout to all layers of the model
and the dropout ratio is set as 0.2 .

(11)

Rdl

∈
will be used as the inputs for next matching
level.
Finally, The matching results from all matching levels are concatenated and used as the outputs of the multi-level matching layer.
p

p
si , s hj

p

p

si = [t 1i ; . . . ; tki ]

(12)

h
s hj = [t 1j
; . . . ; tkhj ]

(13)

Rkdl

where
∈
are the concatenated matching results of
matching P against H and matching H against P, respectively.

2.3

Aggregation and Prediction Layer

3.2

To aggregate all the matching results into a fixed-size vector, we
pass the above concatenated matching results into another BiLSTM.
p
ui
u hj

=
=

p p
p
BiLST M(si , ui−1, ui+1 )
BiLST M(sih , u hj−1, u hj+1 )

The accuracy metric is used to evaluate the performance of the proposed MMN and baseline models on datasets SNLI and Scitail. The
performance of all baseline models come from respective papers.
SNLI. Table 1 shows the results of different models on the training and test sets of SNLI. DecompAtt [9] divides the text matching
task into several sub-tasks using soft attention. BiMPM [14] performs matching at multi-perspective and two directions. ESIM [2]
enhances the local inference procedure and achieves the state-ofthe-art performance. DIIN [5] and CIN[4] are two advanced models
based on CNN. From Table 1 we can see that the proposed MMN
achieves an accuracy of 88.2% in the test sets, which outperforms
all the baselines and achieves the state-of-the-art performance.

(14)
(15)

Then a mean pooling method is used to obtain the fixed-size vectors.
m
1 Õ p
ap =
u
(16)
m i=1 i
ah =

n
1Õ h
u
n j=1 j

Experimental Results

(17)

Finally, the above fixed-size vectors ap and ah are concatenated and
then passed to a MLP classifier which includes a tanh activation
and softmax output layer to obtain the final prediction.

Table 1: Performances on the SNLI dataset
Model
DecompAtt [9]
BiMPM [14]
ESIM [2]
DIIN [5]
CIN[4]
MMN

3 EXPERIMENTS AND RESULTS
3.1 Dataset and Experimental Setup
We evaluate our model on two datasets: SNLI [1], and SciTail dataset
[7]. SNLI contains over 570K human annotated sentence pairs,
each labeled with one of the following relationships: entailment,

951

Train
89.5
90.9
92.6
91.2
93.2
89.3

Test
86.3
87.5
88.0
88.0
88.0
88.2

Short Research Papers 1C: Search

SIGIR ’19, July 21–25, 2019, Paris, France

Scitail. Table 2 shows results of the proposed MMN model and
baselines on the SciTail dataset. DGEM is the decomposed graph
entailment model proposed in [7]. HCRN [12] obtain the attention matrix using the complex-valued inner product (Hermitian
products). CAFE [11] utilizes the word level matching results for
augmentation of the base word representation instead of aggregating them for prediction. From Table 2 we can see our model
MMN outperforms all baselines and achieves the state-of-the-art
performance with an accuracy of 84.8%. Comparing with ESIM
which achieves the state-of-the-art performance on SNLI dataset,
the proposed MMN outperforms it by a large margin over 14%.
Table 2: Performances on the Scitail dataset
Model
DecompAtt[9]
ESIM[2]
DGEM[7]
HCRN [12]
CAFE [11]
MMN

Accuary
72.3
70.6
77.3
80.0
83.3
84.8

Figure 3: Effect of number of matching levels.
work, we will extend the proposed MMN to address other tasks,
such as question answering and machine reading comprehension.

5
3.3

Ablation Study

We performe an ablation study on the MMN model to examine the
effectiveness of each major component. Table 3 shows the ablation
study results on SciTail and SNLI datasets. First, if we remove
the attention aware representation fusion (AARF) layer from the
model, the performance of the model has dropped slightly on two
datasets, from 84.8% to 84.24% on Scaitail, and from 88.2% to 87.9%
on SNLI. This indicates the AARF layer is helpful for improving the
performance of the model. Second, if we do not use multiple levels
of word representations (only use the final word representaions to
get word level matching results), the accuracy drops by over 1% on
both datasets. According to the results, all of the components are
effective for performance improvement.

REFERENCES
[1] Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning.
2015. A large annotated corpus for learning natural language inference. In EMNLP.
632–642.
[2] Qian Chen, Xiaodan Zhu, Zhenhua Ling, Si Wei, Hui Jiang, and Diana Inkpen.
2017. Enhanced LSTM for Natural Language Inference. In ACL, Vol. 1. 1657–1668.
[3] Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine
Bordes. 2017. Supervised Learning of Universal Sentence Representations from
Natural Language Inference Data. In EMNLP. 670–680.
[4] Jingjing Gong, Xipeng Qiu, Xinchi Chen, Dong Liang, and Xuanjing Huang. 2018.
Convolutional Interaction Network for Natural Language Inference. In EMNLP.
1576–1585.
[5] Yichen Gong, Heng Luo, and Jian Zhang. 2017. Natural language inference over
interaction space. ICLR.
[6] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural
computation 9, 8 (1997), 1735–1780.
[7] Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018. SciTail: A textual entailment dataset from science question answering. In AAAI.
[8] Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In ICLR.
[9] Ankur P Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. 2016.
A Decomposable Attention Model for Natural Language Inference. In EMNLP.
2249–2255.
[10] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove:
Global vectors for word representation. In EMNLP. 1532–1543.
[11] Yi Tay, Anh Tuan Luu, and Siu Cheung Hui. 2018. Compare, Compress and
Propagate: Enhancing Neural Architectures with Alignment Factorization for
Natural Language Inference. In EMNLP.
[12] Yi Tay, Anh Tuan Luu, and Siu Cheung Hui. 2018. Hermitian Co-Attention
Networks for Text Matching in Asymmetrical Domains. In IJCAI.
[13] Nam Khanh Tran and Claudia Niedereée. 2018. Multihop Attention Networks
for Question Answer Matching. In SIGIR. 325–334.
[14] Zhiguo Wang, Wael Hamza, and Radu Florian. 2017. Bilateral multi-perspective
matching for natural language sentences. In IJCAI. 4144–4150.
[15] Liu Yang, Minghui Qiu, Chen Qu, Jiafeng Guo, Yongfeng Zhang, W Bruce Croft,
Jun Huang, and Haiqing Chen. 2018. Response ranking with deep matching
networks and external knowledge in information-seeking conversation systems.
In SIGIR. 245–254.
[16] Wei Emma Zhang, Quan Z Sheng, Zhejun Tang, and Wenjie Ruan. 2018. Related
or Duplicate: Distinguishing Similar CQA Questions via Convolutional Neural
Networks. In SIGIR. 1153–1156.

Table 3: Ablation study on SciTail and SNLI datasets
Model
MMN
- AARF
- Multi-level matching

3.4

Scitail
84.8
84.24
83.34

SNLI
88.2
87.9
87.8

Effect of Number of Matching levels

Fig. 3 shows the effect of number of matching levels on SNLI and Scitail datasets. We observe that the optimal performance is 3 matching
levels for SNLI. However, the performance of SNLI declines after 3
matching levels. Similarly, Scitail achieves its best performance at
level 2 and then declines after 2 matching levels.

4

ACKNOWLEDGMENTS

This work is partially funded by the EU Horizon 2020 under Grant
690238 for DESIREE Project, under Grant 700381 for ASGARD
project, by the UK EPSRC under Grant EP/P031668/1.

CONCLUSION

In this paper, we have presented a novel multi-level matching network (MMN) for text matching, which obtains word level matching
results based on multiple levels of word representations to capture
more matching information. The MMN model achieves the stateof-the-art performance on two datasets: SNLI and Scitail. In future

952

