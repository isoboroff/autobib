Short Research Papers 1B: Recommendation and Evaluation

SIGIR ’19, July 21–25, 2019, Paris, France

One-Class Collaborative Filtering with the
Queryable Variational Autoencoder
Ga Wu∗

Mohamed Reda Bouadjenek

Scott Sanner∗

University of Toronto
wuga@mie.utoronto.ca

University of Toronto
mrb@mie.utoronto.ca

University of Toronto
ssanner@mie.utoronto.ca

ABSTRACT
Variational Autoencoder (VAE) based methods for Collaborative
Filtering (CF) demonstrate remarkable performance for one-class
(implicit negative) recommendation tasks by extending autoencoders with relaxed but tractable latent distributions. Explicitly
modeling a latent distribution over user preferences allows VAEs
to learn user and item representations that not only reproduce observed interactions, but also generalize them by leveraging learning
from similar users and items. Unfortunately, VAE-CF can exhibit
suboptimal learning properties; e.g., VAE-CFs will increase their
prediction confidence as they receive more preferences per user,
even when those preferences may vary widely and create ambiguity in the user representation. To address this issue, we propose a
novel Queryable Variational Autoencoder (Q-VAE) variant of the
VAE that explicitly models arbitrary conditional relationships between observations. The proposed model appropriately increases
uncertainty (rather than reduces it) in cases where a large number
of user preferences may lead to an ambiguous user representation. Our experiments on two benchmark datasets show that the
Q-VAE generally performs comparably or outperforms VAE-based
recommenders as well as other state-of-the-art approaches and is
generally competitive across the user preference density spectrum,
where other methods peak for certain preference density levels.
Keywords: One-Class Collaborative Filtering; Variational Autoencoder; Conditional Inference.

Figure 1: In this experiment, we show the average standard
deviation of the diagonal Gaussian latent embeddings for
VAE-CF and Q-VAE across 500 users. At the top, we first measure this embedding uncertainty after sampling 20 real interactions from each user’s data and at the bottom we add in
80 random (fake) interactions. While Q-VAE increases its uncertainty, VAE-CF oddly becomes more certain in user preferences after observing this incoherent random data.
a conventional Autoencoder recommender tends to be unsatisfactory as latent representations are likely to overfit and memorize
individual observations [2]. Indeed, an Autoencoder-based model
for CF may be overly sensitive to individual user-item interactions,
and thus may significantly change the latent representation of a
user even with a single interaction update. Several prior works
have noted this unsatisfactory representation issue [2, 3] and thus
Denoising Autoencoders [4] have been developed to mitigate this
issue. Unfortunately, denoising can hurt the prediction performance
when the data is very sparse as we show later in the experiments.
Recently, Variational Autoencoders (VAEs) [5] – which model
distributions over latent representations – have been used and extended by Liang et al. [6] for CF recommendation (VAE-CF) and
showed remarkable prediction performance improvement over previous Autoencoding methods. The prediction performance improvement arising from the generalization of VAEs over non-probabilistic
Autoencoders is due to two key reasons: (i) VAEs relax the latent
distribution from a (deterministic) Delta function to a Gaussian
distribution allowing for explicit representation of user and item
uncertainty, and (ii) VAEs regularize the latent distribution through
Kullback-Leibler (KL) divergence with a tractable standard Gaussian distribution leading to learning stability (i.e., less sensitivity to
individual data). Despite their remarkable prediction performance,
VAEs exhibit the undesirable property of being over-confident when
users express a large number of preferences. We argue in this paper
that this property is particularly problematic because VAE-CF tends
to recommend items to users with high certainty if a user has a
considerable number of observed interactions even when these
preferences may vary widely and increase ambiguity in the latent
user representation as demonstrated in Figure 1.
To address the issue mentioned above, we propose the Queryable
Variational Auto-encoder (Q-VAE) for one-class (implicit negative)

ACM Reference Format:
Ga Wu, Mohamed Reda Bouadjenek, and Scott Sanner. 2019. One-Class
Collaborative Filtering with the Queryable Variational Autoencoder. In
Proceedings of the 42nd International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR ’19), July 21–25, 2019, Paris,
France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.
3331292

1

INTRODUCTION

Autoencoder-based Collaborative Filtering (CF) algorithms make
predictions by embedding user preferences into a latent space that
enables generalization to unobserved user preferences [1]. However,
∗ Affiliate

to Vector Institute of Artificial Intelligence, Toronto

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’19, July 21–25, 2019, Paris, France
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00
https://doi.org/10.1145/3331184.3331292

921

Short Research Papers 1B: Recommendation and Evaluation

SIGIR ’19, July 21–25, 2019, Paris, France

lower-bound of log p(y|x) as follows:
log p(y|x) ≥ Eq(z|x,y) [log p(y|z)] − KL[q(z|x, y)||p(z|x)]
x
(0, 0)
<latexit sha1_base64="w2y21lA1/aySPyIRT6dyd9IfEJg=">AAAB7HicbVBNSwMxEJ31s9avqkcvwSJUkJIVQY8FLx4ruG2hXUo2zbah2WRJskJZ+hu8eFDEqz/Im//GtN2Dtj4YeLw3w8y8KBXcWIy/vbX1jc2t7dJOeXdv/+CwcnTcMirTlAVUCaU7ETFMcMkCy61gnVQzkkSCtaPx3cxvPzFtuJKPdpKyMCFDyWNOiXVSUMOX+KJfqeI6ngOtEr8gVSjQ7Fe+egNFs4RJSwUxpuvj1IY50ZZTwablXmZYSuiYDFnXUUkSZsJ8fuwUnTtlgGKlXUmL5urviZwkxkySyHUmxI7MsjcT//O6mY1vw5zLNLNM0sWiOBPIKjT7HA24ZtSKiSOEau5uRXRENKHW5VN2IfjLL6+S1lXdx3X/4brawEUcJTiFM6iBDzfQgHtoQgAUODzDK7x50nvx3r2PReuaV8ycwB94nz8PVY13</latexit>

y

<latexit sha1_base64="hMUN0ytn0IYshdQKp5n0VmIOIPA=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLgxmUF+8A2lMl00g6dTMLMjVBD/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wSTm9zvPHJtRKzucZpwP6IjJULBKFrpoR9RHAdh9jQbVGtu3Z2DrBKvIDUo0BxUv/rDmKURV8gkNabnuQn6GdUomOSzSj81PKFsQke8Z6miETd+Nk88I2dWGZIw1vYpJHP190ZGI2OmUWAn84Rm2cvF/7xeiuG1nwmVpMgVW3wUppJgTPLzyVBozlBOLaFMC5uVsDHVlKEtqWJL8JZPXiXti7rn1r27y1rDLeoowwmcwjl4cAUNuIUmtICBgmd4hTfHOC/Ou/OxGC05xc4x/IHz+QP62ZEM</latexit>
<latexit

z

µ

x,y

µx

<latexit sha1_base64="JvHu4rC8XllXsmazALvOLBRbAp0=">AAAB8HicbVBNS8NAEJ3Ur1q/qh69LBbBg5REBD0WvHisYD+kjWWz3bRLdzdhdyOG0F/hxYMiXv053vw3btMctPXBwOO9GWbmBTFn2rjut1NaWV1b3yhvVra2d3b3qvsHbR0litAWiXikugHWlDNJW4YZTruxolgEnHaCyfXM7zxSpVkk70waU1/gkWQhI9hY6b4vkofs6SydDqo1t+7mQMvEK0gNCjQH1a/+MCKJoNIQjrXueW5s/Awrwwin00o/0TTGZIJHtGepxIJqP8sPnqITqwxRGClb0qBc/T2RYaF1KgLbKbAZ60VvJv7n9RITXvkZk3FiqCTzRWHCkYnQ7Hs0ZIoSw1NLMFHM3orIGCtMjM2oYkPwFl9eJu3zuufWvduLWsMt4ijDERzDKXhwCQ24gSa0gICAZ3iFN0c5L8678zFvLTnFzCH8gfP5A/l5kHM=</latexit>

<latexit sha1_base64="qsusuHrCcQP6PgRTAJbCz2Q+12c=">AAAB8XicbVBNS8NAFHypX7V+VT16WSyCp5KIoMeCF48VbCu2oWy2L+3SzSbsboQQ+i+8eFDEq//Gm//GTZuDtg4sDDPvsfMmSATXxnW/ncra+sbmVnW7trO7t39QPzzq6jhVDDssFrF6CKhGwSV2DDcCHxKFNAoE9oLpTeH3nlBpHst7kyXoR3QsecgZNVZ6HETUTIIwz2bDesNtunOQVeKVpAEl2sP612AUszRCaZigWvc9NzF+TpXhTOCsNkg1JpRN6Rj7lkoaofbzeeIZObPKiISxsk8aMld/b+Q00jqLAjtZJNTLXiH+5/VTE177OZdJalCyxUdhKoiJSXE+GXGFzIjMEsoUt1kJm1BFmbEl1WwJ3vLJq6R70fTcpnd32Wi5ZR1VOIFTOAcPrqAFt9CGDjCQ8Ayv8OZo58V5dz4WoxWn3DmGP3A+fwD5VJEL</latexit>

<latexit sha1_base64="ihlpTIJSSoeYfdWYzsyV9rf2BzA=">AAAB7nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKoMeAF48RzAOSNcxOJsmQmdllplcMSz7CiwdFvPo93vwbJ8keNLGgoajqprsrSqSw6PvfXmFtfWNzq7hd2tnd2z8oHx41bZwaxhsslrFpR9RyKTRvoEDJ24nhVEWSt6LxzcxvPXJjRazvcZLwUNGhFgPBKDqp1VXpQ/Y07ZUrftWfg6ySICcVyFHvlb+6/ZilimtkklrbCfwEw4waFEzyaambWp5QNqZD3nFUU8VtmM3PnZIzp/TJIDauNJK5+nsio8raiYpcp6I4ssveTPzP66Q4uA4zoZMUuWaLRYNUEozJ7HfSF4YzlBNHKDPC3UrYiBrK0CVUciEEyy+vkuZFNfCrwd1lpebncRThBE7hHAK4ghrcQh0awGAMz/AKb17ivXjv3seiteDlM8fwB97nD6+Fj7o=</latexit>

<latexit sha1_base64="tli2lKeB1EcNpYeDPxK69ZODRog=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi1GXBjcsK9oFtKJPpTTt0MgkzE6GE/oUbF4q49W/c+TdO2iy09cDA4Zx7mXNPkAiujet+O6WNza3tnfJuZW//4PCoenzS0XGqGLZZLGLVC6hGwSW2DTcCe4lCGgUCu8H0Nve7T6g0j+WDmSXoR3QsecgZNVZ6HETUTIIwU/NhtebW3QXIOvEKUoMCrWH1azCKWRqhNExQrfuemxg/o8pwJnBeGaQaE8qmdIx9SyWNUPvZIvGcXFhlRMJY2ScNWai/NzIaaT2LAjuZJ9SrXi7+5/VTE974GZdJalCy5UdhKoiJSX4+GXGFzIiZJZQpbrMSNqGKMmNLqtgSvNWT10nnqu65de/+utZ0izrKcAbncAkeNKAJd9CCNjCQ8Ayv8OZo58V5dz6WoyWn2DmFP3A+fwDusZEE</latexit>
<latexit

r

(a)

<latexit sha1_base64="C+ewVegs3+gvFEDylyVVHLjR7ZE=">AAAB+XicbVDLSsNAFL3xWesr6tJNsAiuSiKCLgtuXFawD2hCmUwn7dDJJMzcFErIn7hxoYhb/8Sdf+OkzUJbDwwczrmXe+aEqeAaXffb2tjc2t7Zre3V9w8Oj47tk9OuTjJFWYcmIlH9kGgmuGQd5ChYP1WMxKFgvXB6X/q9GVOaJ/IJ5ykLYjKWPOKUoJGGtu1PCOZ+THASRrkqiqHdcJvuAs468SrSgArtof3ljxKaxUwiFUTrgeemGOREIaeCFXU/0ywldErGbGCoJDHTQb5IXjiXRhk5UaLMk+gs1N8bOYm1nsehmSwj6lWvFP/zBhlGd0HOZZohk3R5KMqEg4lT1uCMuGIUxdwQQhU3WR06IYpQNGXVTQne6pfXSfe66blN7/Gm0XKrOmpwDhdwBR7cQgseoA0doDCDZ3iFNyu3Xqx362M5umFVO2fwB9bnD0d3lAI=</latexit>

r̂

<latexit sha1_base64="x7haAD4KZfAZXBOps7rjOuMRL+Q=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBahXkoigj0WvHisaD+gDWWy3bRLN5uwuxFK6E/w4kERr/4ib/4bt20O2vpg4PHeDDPzgkRwbVz32ylsbG5t7xR3S3v7B4dH5eOTto5TRVmLxiJW3QA1E1yyluFGsG6iGEaBYJ1gcjv3O09MaR7LRzNNmB/hSPKQUzRWeqji5aBccWvuAmSdeDmpQI7moPzVH8Y0jZg0VKDWPc9NjJ+hMpwKNiv1U80SpBMcsZ6lEiOm/Wxx6oxcWGVIwljZkoYs1N8TGUZaT6PAdkZoxnrVm4v/eb3UhHU/4zJJDZN0uShMBTExmf9NhlwxasTUEqSK21sJHaNCamw6JRuCt/ryOmlf1Ty35t1fVxr1PI4inME5VMGDG2jAHTShBRRG8Ayv8OYI58V5dz6WrQUnnzmFP3A+fwCFqY1A</latexit>

(b)
<latexit sha1_base64="2U3/wNlaT7QGwVjsUqiBa6qyVt8=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBahXkoigj0WvHisaD+gDWWz3bRLN5uwOxFK6E/w4kERr/4ib/4bt20O2vpg4PHeDDPzgkQKg6777RQ2Nre2d4q7pb39g8Oj8vFJ28SpZrzFYhnrbkANl0LxFgqUvJtoTqNA8k4wuZ37nSeujYjVI04T7kd0pEQoGEUrPVSDy0G54tbcBcg68XJSgRzNQfmrP4xZGnGFTFJjep6boJ9RjYJJPiv1U8MTyiZ0xHuWKhpx42eLU2fkwipDEsbalkKyUH9PZDQyZhoFtjOiODar3lz8z+ulGNb9TKgkRa7YclGYSoIxmf9NhkJzhnJqCWVa2FsJG1NNGdp0SjYEb/XlddK+qnluzbu/rjTqeRxFOINzqIIHN9CAO2hCCxiM4Ble4c2Rzovz7nwsWwtOPnMKf+B8/gCHLo1B</latexit>

Figure 2: Proposed Q-VAE model. (a) The joint likelihood
log p(x) and conditional likelihood log p(y|x) objectives share
the VAE network parameters that form a structured regularization. (b) KL[q(z|x, y)||q(z|x)] restricts the user representation from severe changes with additional observations y.

log p(y|x) ≥ Eqψ (z|x,y) [log pϑ (y|z)] − KL[qψ (z|x, y)||qϕ (z|x)] (4)
where ψ and ϑ are respectively encoder and decoder coefficients of
the second VAE. The naive combination of the two VAE objectives
from Equations 2 and 4 is impractical due to the need to maintain
two VAE parameter sets and obtain the conditional prior qθ (z|x)
before training the second VAE network.
We mitigate the above problem by sharing the parameter sets
from the two VAE objectives and training the two networks simultaneously. Specifically, Q-VAE optimizes a combined objective
function on a single VAE network structure as follows:

recommendation tasks. The key contribution of the Q-VAE is to
reformulate the variational lower-bound of the joint observation
distribution to support arbitrary conditional queries over observed
user interactions. We show that our model can accurately measure
the uncertainty of user latent representations (cf. Figure 1), thus
preventing the model from performing poorly for users with a large
number of interactions. Finally, we empirically demonstrate that QVAE outperforms VAE-CF in terms of prediction performance and
is also competitive w.r.t. several state-of-the-art recommendation
algorithms across the user preference density spectrum.

2

log p(x, y)
≥ Eqϕ (z|x) [log pθ (x|z)dz] − KL[qϕ (z|x)||p(z)]

We begin with notation: we denote the observed preferences of
user i as a set ri of items preferred by the user (assuming binary
preference with only positive observations for the one-class case).
We denote partial preference observation subsets as xi and yi ,
where xi , yi ⊆ ri , xi ∩ yi = ∅, and xi ∪ yi ⊆ ri . In the following
context, we omit the user subscript i to reduce notational clutter.
We propose the Queryable Variational Auto-encoder (Q-VAE)
to model the joint probability of a user’s preferences p(r) and the
conditional probability p(y|x) of some subset of preferences given
the others, which allows the model to treat the recommendation
as a conditional inference (i.e., a query) problem with an arbitrary
evidence set of user preference observations.
Instead of directly modeling the lower-bound of the log joint
probability p(r) as other VAE-based recommender systems do, we
propose to model the joint probability of any arbitrary partition of
x and y as follows:

where the two sub-objective functions form a mutually structured
regularizer as demonstrated in Figure 2(a).
Arbitrary Combination of Evidence and Query Variables: Instead of fixing the split of variables x and y as in CVAE and BCDE,
Q-VAE randomly splits variables during training through a dropout
method as shown in Equation 6 that is detailed later. Such random
dropout training enables the model to do arbitrary conditional inference with a different set of evidence or query variables without
retraining a new model. Specifically, we can obtain random x ∪ y,
x and y as follows:
x∪y = Dropout(r, ρ);

x = Dropout(x∪y, ρ);

y = x∪y−x, (6)

where ρ indicates the dropout ratio that randomly samples dropout
percentages uniformly from the range [0.1, 0.5].
KL Divergence: The objective function in Equation 5 introduces
one additional KL divergence term that regularizes posterior distributions qϕ (z|x) and qϕ (z|x, y). Since both posterior distributions
are approximated as diagonal Gaussian distributions that are parameterized by mean µ and standard derivation σ , the KL divergence
computation is in closed-form and is computed as follows:
"
#
x,y
x,y
Õ
(σ )2 +(µ k − µ kx )2 1
σkx
KL[q(z|x, y)||q(z|x)] =
log x,y
+ k
−
2
2(σkx )2
σ

(1)

where log p(y|x) estimates user preference for some items given
the user’s historical interactions, and log p(x) estimates how well
the model can reproduce the historical interactions.
Maximizing both log p(y|x) and log p(x) for a given user is intractable due to the unknown relations between their interactions.
We therefore optimize the lower-bounds log p(x), which has been
derived for VAE [6–8] as follows:
log p(x) ≥ Eqϕ (z|x) [log pθ (x|z)] − KL[qϕ (z|x)||p(z)]

(5)

+ Eqϕ (z|x,y) [log pθ (y|z)dz] − KL[qϕ (z|x, y)||qϕ (z|x)],

QUERYABLE-VAE FOR RECOMMENDATION

log p(x, y) = log p(y|x) + log p(x)

(3)

However, we note that we cannot form Equation 3 into an Autoencoder since the distribution p(z|x) is unknown. While it is possible to relax p(z|x) by p(z) as it has been done in both CVAE [4]
and BCDE [9], in this work, we require a variational approximation
q(z|x, y) with additional observations y as close as possible to p(z|x)
to ensure that recommendations align with observed preferences x.
We address this excessive relaxation problem by approximating
the prior distribution p(z|x) with its lower-bound qϕ (z|x) learned
from Equation 2. Thus, Equation 3 can represent a second VAE
objective function as follows:

<latexit sha1_base64="MtaA25cfKxHazUZoK9PQJjvGprs=">AAAB8XicbVDLSsNAFL2pr1pfVZduBovgqiQi6LLgxmUF+8A2lMn0ph06mYSZiVhC/8KNC0Xc+jfu/BsnbRbaemDgcM69zLknSATXxnW/ndLa+sbmVnm7srO7t39QPTxq6zhVDFssFrHqBlSj4BJbhhuB3UQhjQKBnWByk/udR1Sax/LeTBP0IzqSPOSMGis99CNqxkGYPc0G1Zpbd+cgq8QrSA0KNAfVr/4wZmmE0jBBte55bmL8jCrDmcBZpZ9qTCib0BH2LJU0Qu1n88QzcmaVIQljZZ80ZK7+3shopPU0CuxknlAve7n4n9dLTXjtZ1wmqUHJFh+FqSAmJvn5ZMgVMiOmllCmuM1K2JgqyowtqWJL8JZPXiXti7rn1r27y1rDLeoowwmcwjl4cAUNuIUmtICBhGd4hTdHOy/Ou/OxGC05xc4x/IHz+QP3z5EK</latexit>
<latexit

k

k

(7)
where k is the index of latent dimension. The KL divergence suggests to keep expectation of the user preference µ x,y as close as
possible to µ x when the model observes more interactions y as
demonstrated in Figure 2(b).

(2)

where ϕ and θ are respectively encoder and decoder coefficients,
and z is a user latent representation. Similarly, we can define the

922

Short Research Papers 1B: Recommendation and Evaluation

SIGIR ’19, July 21–25, 2019, Paris, France

Table 1: Results of Movielens-1M dataset with 95% confidence interval. Hyper-parameters are chosen from the validation set.
α: loss-weighting. λ: L2-regularization. ρ: corruption rate.
model

rank

α

λ

epochs

ρ

R-Precision

MAP@5

MAP@50

Precision@5

Precision@50

Recall@5

Recall@50

PureSVD
BPR
WRMF
CDAE
VAE-CF
AutoRec

50
200
200
200
200
200

0
0
10
0
0
0

1
1e-5
100
1e-5
1e-5
1e-5

10
30
10
300
200
300

0
0
0
0.5
0.4
0

0.092±0.0024
0.0933±0.0025
0.097±0.0026
0.0941±0.0025
0.0892±0.0025
0.0945±0.0025

0.1212±0.0052
0.1192±0.0052
0.1235±0.0053
0.1297±0.0056
0.1066±0.0048
0.1254±0.0054

0.0987±0.0024
0.1002±0.0025
0.1039±0.0025
0.1032±0.0028
0.093±0.0022
0.1017±0.0026

0.116±0.0043
0.1141±0.0043
0.1198±0.0045
0.1226±0.0047
0.1054±0.0039
0.1194±0.0045

0.0852±0.0018
0.0875±0.0019
0.091±0.002
0.0891±0.0021
0.0827±0.0017
0.0877±0.0019

0.0383±0.002
0.0375±0.002
0.0411±0.0022
0.035±0.0018
0.0376±0.002
0.0377±0.002

0.2383±0.0052
0.2426±0.0052
0.2668±0.0058
0.2177±0.0047
0.2449±0.0054
0.2398±0.0052

Q-VAE

200

0

0.1

200

0

0.1±0.0026

0.1306±0.0055

0.1066±0.0026

0.125±0.0046

0.0917±0.0020

0.0404±0.0021

0.2504±0.0054

Table 2: Results of Netflix dataset with 95% confidence interval. Hyper-parameters are chosen from the validation set.
model

rank

α

λ

epochs

ρ

R-Precision

MAP@5

MAP@50

Precision@5

Precision@50

Recall@5

Recall@50

PureSVD
BPR
WRMF
CDAE
VAE-CF
AutoRec

50
50
200
50
100
50

0
0
10
0
0
0

1
1e-5
1e4
1e-5
1e-4
1e-5

10
30
10
300
300
300

0
0
0
0.2
0.5
0

0.0994±0.0003
0.0757±0.0002
0.0985±0.0003
0.0797±0.0003
0.1017±0.0003
0.0876±0.0003

0.159±0.0007
0.1197±0.0006
0.1531±0.0007
0.1251±0.0006
0.1559±0.0007
0.14±0.0006

0.118±0.0003
0.096±0.0003
0.117±0.0003
0.0979±0.0003
0.1176±0.0003
0.1074±0.0003

0.146±0.0005
0.115±0.0005
0.1447±0.0006
0.1198±0.0005
0.1465±0.0005
0.1324±0.0005

0.0953±0.0003
0.0816±0.0002
0.096±0.0003
0.0832±0.0002
0.0957±0.0003
0.0894±0.0003

0.0445±0.0003
0.0291±0.0002
0.045±0.0003
0.0323±0.0002
0.0467±0.0003
0.0361±0.0002

0.2188±0.0006
0.1859±0.0006
0.2325±0.0007
0.1788±0.0006
0.2309±0.0006
0.1958±0.0006

Q-VAE

100

0

1e-5

200

0

0.0976±0.0003

0.1593±0.0007

0.1194±0.0003

0.1488±0.0006

0.0972±0.0003

0.0429±0.0003

0.2303±0.0006

Table 3: Summary of datasets used in evaluation.

3

Dataset

#Users

#Items

|r i, j > ϑ |

Sparsity

MovieLens-1m
Netflix Prize

6,038
2,649,430

3,533
17,771

575,281
56,919,190

2.69 × 10−2
1.2 × 10−3

• AutoRec [1]: Autoencoder based recommendation system with
one hidden layer, Relu activation, and sigmoid cross entroy loss.
• CDAE [4]: Collaborative Denoising Autoencoder, which is specifically optimized for implicit feedback recommendation tasks.
• VAE-CF [6]: Variational Autoencoder for CF. A state-of-the-art
metric learning based recommender system.

EXPERIMENTS AND EVALUATION

Ranking Performance Evaluation: Tables 1 and 2 show the
general performance comparison of Q-VAE with the six baselines
using R-Precision, MAP, Precision@K, and Recall@K metrics. From
the results obtained, we make the following observations: (i) In
general, Q-VAE achieves competitive prediction performance w.r.t.
the state-of-the-art recommendation algorithms such as WRMF
and VAE-CF. (ii) Also, Q-VAE outperforms all candidates on MAP
and Precision@K, at the expense of Recall@K as a trade-off.

In this section, we evaluate Q-VAE by comparing it to a variety of
scalable state-of-the-art One-Class Collaborative Filtering (OC-CF)
algorithms on two different benchmark datasets. The comparison
includes: (i) recommendation precision performance, (ii) latent representation uncertainty evaluation, and (iii) convergence speed.
Datasets: We evaluate the candidate algorithms on two publicly
available datasets: Movielens-1M1 and Netflix Prize2 where in both
datasets, ratings rages from 1 to 5. For both datasets, we binarize
the ratings based on a threshold ϑ = 3, defined to be the upper half
of the range of the ratings. Hence, a rating r i j > ϑ is considered as a
positive feedback, otherwise, it’s considered as a negative feedback.

Performance vs. User Interaction Level: We investigate conditions under which Q-VAE achieves a significantly higher prediction
performance than the baselines. To this end, we categorize users
based on their number of interactions in the training set into 4
categories. The categories come from the 25%, 50%, 75% quartiles
of the number of training interactions, which indicate how often
the user rated items in the training set.
Figure 3 shows the performance comparison for different user
categories. We note that CDAE, comparing to AutoRec, performs
poorly for users with sparse historical interactions. It reflects our
intuition that simple random corruption of inputs (i.e., "Denoising")
hurts the performance for users with sparse observations. WRMF
and VAE-CF both perform well with sparse user interactions but
poorly for users with many interactions. In comparison, Q-VAE
shows relatively stable and good performance over all four user
categories and significant prediction performance improvement
over VAE-CF, especially with a large number of user interactions.

Evaluation Metrics: We evaluate the recommendation performance using five different metrics: Precision@K, Recall@K, MAP@K,
R-Precision, and B-NDCG.
Candidate Methods: We compare Q-VAE with six different CF
algorithms, ranging from classical Matrix Mactorization to the latest
VAE for CF. These algorithms are:
• PureSVD [10]: A method that constructs a similarity matrix
through randomized SVD decomposition of implicit matrix R.
• BPR [11]: Bayesian Personalized Ranking. One of the first recommender that explicitly optimize the pairwise ranking.
• WRMF [12]: Weighted Regularized Matrix Factorization. A Matrix Factorization that was designed for OC-CF.

User Representation Uncertainty: Both VAE-CF and Q-VAE explicitly model the user latent representation distributions. Hence, in

1 https://grouplens.org/datasets/movielens/1m/
2 https://www.kaggle.com/netflix-inc/netflix-prize-data

923

Short Research Papers 1B: Recommendation and Evaluation

SIGIR ’19, July 21–25, 2019, Paris, France

Figure 3: Average NDCG comparison for different quantiles of user activity (number of ratings as binned into the ranges shown
in [·, ·]) for MovieLens-1m. Error bars show the standard deviation of the NDCG across users in that bin.

Figure 4: Average standard deviation of the diagonal Gaussian latent representations of users (averaged over users having a given number of ratings) for VAE-CF and Q-VAE on
Movielens-1m; VAE-CF is overconfident with a high number
of user ratings while Q-VAE shows more uncertainty.

Figure 5: NDCG versus training epochs for the four Autoencoder based recommendation algorithms.

REFERENCES
[1] Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie. Autorec:
Autoencoders meet collaborative filtering. In Proceedings of the 24th International
Conference on World Wide Web, pages 111–112. ACM, 2015.
[2] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and PierreAntoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of machine
learning research, 11(Dec):3371–3408, 2010.
[3] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol.
Extracting and composing robust features with denoising autoencoders. In
Proceedings of the 25th international conference on Machine learning, pages 1096–
1103. ACM, 2008.
[4] Yao Wu, Christopher DuBois, Alice X Zheng, and Martin Ester. Collaborative
denoising auto-encoders for top-n recommender systems. In Proceedings of
the Ninth ACM International Conference on Web Search and Data Mining, pages
153–162. ACM, 2016.
[5] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. 2013.
[6] Dawen Liang, Rahul G Krishnan, Matthew D Hoffman, and Tony Jebara. Variational autoencoders for collaborative filtering. arXiv preprint arXiv:1802.05814,
2018.
[7] Xiaopeng Li and James She. Collaborative variational autoencoder for recommender systems. In Proceedings of the 23rd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pages 305–314. ACM, 2017.
[8] Yifan Chen and Maarten de Rijke. A collective variational autoencoder for top-n
recommendation with side information. In Proceedings of the 3rd Workshop on
Deep Learning for Recommender Systems, pages 3–9. ACM, 2018.
[9] Rui Shu, Hung H. Bui, and Mohammad Ghavamzadeh. Bottleneck conditional
density estimation. In Proceedings of the 34th International Conference on Machine
Learning - Volume 70, ICML’17, pages 3164–3172. JMLR.org, 2017.
[10] Paolo Cremonesi, Yehuda Koren, and Roberto Turrin. Performance of recommender algorithms on top-n recommendation tasks. In Proceedings of the fourth
ACM conference on Recommender systems, pages 39–46. ACM, 2010.
[11] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.
Bpr: Bayesian personalized ranking from implicit feedback. In Proceedings of
the twenty-fifth conference on uncertainty in artificial intelligence, pages 452–461.
AUAI Press, 2009.
[12] Yifan Hu, Yehuda Koren, and Chris Volinsky. Collaborative filtering for implicit
feedback datasets. In Data Mining, 2008. ICDM’08. Eighth IEEE International
Conference on, pages 263–272. Ieee, 2008.

this experiment, we analyze the latent representation uncertainty
of users vs. their number of ratings. As shown in Figure 4, VAECF tends to provide high certainty to users with a large number
of interactions, even though a large number of interactions often
requires more uncertainty to cover the range of preferences. The
caveat of this over-certainty is reflected in our observation of Figure 3, where VAE-CF performs poorly for users with a large number
of interactions as compared to Q-VAE (rightmost chart).
Convergence Profile: We track the convergence progress among
the four Autoencoder based recommenders in Figure 5. It shows
that VAE-based algorithms converge faster than the original Autoencoder approaches (which tend to overfit). Q-VAE benefits from
relatively fast and smooth convergence without overfitting due to
the mutually structured regularization of its two objectives.

4

CONCLUSION

In this paper, we proposed the Queryable Variational Auto-encoder
(Q-VAE) as a way to explicitly condition recommendations in oneclass collaborative filtering on observed user preferences to better
model latent uncertainty of user preferences. Our experiments show
that the Q-VAE not only converges faster, but also outperforms several state-of-the-art Auto-encoder based recommendation models.
Also, we showed that Q-VAE avoids over-confidence with a large
number of user preferences leading to strong recommendation
performance across the user preference density spectrum.

924

