Session 9A: Fashion Match

SIGIR â€™19, July 21â€“25, 2019, Paris, France

Interpretable Fashion Matching with Rich Attributes
Xun Yang

Xiangnan He

Xiang Wang

National University of Singapore
xunyang@nus.edu.sg

University of Science and Technology
of China
xiangnanhe@gmail.com

National University of Singapore
xiangwang1223@gmail.com

Yunshan Ma
Fuli Feng
National University of Singapore
yunshan.ma@u.nus.edu
fulifeng93@gmail.com

Meng Wang

Tat-Seng Chua

Department of Computer Science
Hefei University of Technology
eric.mengwang@gmail.com

National University of Singapore
dcscts@nus.edu.sg

ABSTRACT

ACM Reference Format:
Xun Yang, Xiangnan He, Xiang Wang, Yunshan Ma, Fuli Feng, Meng Wang,
and Tat-Seng Chua. 2019. Interpretable Fashion Matching with Rich Attributes. In Proceedings of the 42nd International ACM SIGIR Conference on
Research and Development in Information Retrieval (SIGIR â€™19), July 21â€“25,
2019, Paris, France. ACM, New York, NY, USA, 10 pages. https://doi.org/10.
1145/3331184.3331242

Understanding the mix-and-match relationships of fashion items
receives increasing attention in fashion industry. Existing methods
have primarily utilized the visual content to learn the visual compatibility and performed matching in a latent space. Despite their
effectiveness, these methods work like a black box and cannot reveal
the reasons that two items match well. The rich attributes associated with fashion items, e.g., off-shoulder dress and black skinny jean,
which describe the semantics of items in a human-interpretable
way, have largely been ignored.
This work tackles the interpretable fashion matching task, aiming to inject interpretability into the compatibility modeling of
items. Specifically, given a corpus of matched pairs of items, we
not only can predict the compatibility score of unseen pairs, but
also learn the interpretable patterns that lead to a good match, e.g.,
white T-shirt matches with black trouser. We propose a new solution
named Attribute-based Interpretable Compatibility (AIC) method,
which consists of three modules: 1) a tree-based module that extracts decision rules on matching prediction; 2) an embedding module that learns vector representation for a rule by accounting for
the attribute semantics; and 3) a joint modeling module that unifies
the visual embedding and rule embedding to predict the matching
score. To justify our proposal, we contribute a new Lookastic dataset
with fashion attributes available. Extensive experiments show that
AIC not only outperforms several state-of-the-art methods, but also
provides good interpretability on matching decisions.

1

INTRODUCTION

Fashion is a rapidly growing industry, which has motivated various research topics in the fashion domain, such as recommendation [42, 43], search [25], and dialogue systems [24], etc. In this
paper, we focus on a newly-emerged topic of Mix-and-match-based
fashion recommendation [13, 22, 30â€“32, 39], for which the goal is
to predict the compatibility between fashion items. For example,
when a user views/buys an item (e.g., a red floral maxi dress), the
system matches it with the compatible fashion items from a complementary category (e.g., high-heel sandals). The key to solving this
problem is how to effectively model the item-item compatibility
relationships.
Existing methods have primarily leveraged the images of fashion
items to model the notion of visual compatibility and performed
matching in a latent visual space [7, 16, 26, 31, 33]. A common
assumption is that a pair of compatible items should stay close
with each other in the latent space . Then, the matching problem
is solved under a metric learning paradigm: first collect a corpus
of matched/unmatched item pairs, and then train a parameterized
similarity function that enforces the matched pairs to have higher
similarity scores than that of unmatched pairs. Despite their effectiveness, existing methods mainly exploit the visual information
that comprises of low-level signals, while forgoing the modeling of
rich attributes associated with fashion items, e.g., off-shoulder dress
and black skinny jean. They just work like a black box and cannot
interpret the reasons that two items match well; this has been found
to be insufficient to support downstream applications. We argue
that the rich attributes, which describe the semantics of items in a
human-interpretable way, should be carefully taken into account
to improve both the matching accuracy and interpretability.
Recent works have tried to alleviate the above-mentioned limitations by augmenting the visual features of items with textual
descriptions [31], or refining pairwise visual compatibility with
category-category complementary relationships [32, 39]. However,
the textual description of items is directly encoded as a dense vector

CCS CONCEPTS
â€¢ Information systems â†’ Specialized information retrieval.

KEYWORDS
Multimedia recommendation; clothing matching; fashion compatibility learning
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR â€™19, July 21â€“25, 2019, Paris, France
Â© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00
https://doi.org/10.1145/3331184.3331242

775

Session 9A: Fashion Match

SIGIR â€™19, July 21â€“25, 2019, Paris, France

23:
::
2 4:2 - :2- 4 2
00 :
22 2- 2 3 4
2
2 : 20 2
2
- 2- 2

without language parsing, making it hard to reveal which attributes
contribute the most to a match. The category-category relationships only use coarse-grained categories to bridge two items from
complementary categories, which results in limited interpretability.
In summary, the semantics of rich attributes associated with fashion
items have not been fully explored in fashion matching.
This paper addresses the interpretable fashion matching task,
which is a new topic in this field. Our aim is to inject interpretability into the compatibility modeling of fashion items by leveraging
the rich fashion attributes. Specifically, given a corpus of matched
pairs of items, we learn the interpretable matching patterns that
lead to a good match, e.g., white T-shirt matches with black trouser,
which is termed as attribute cross (analogous to feature cross [9])
in this work. Towards this end, we propose a new solution named
Attribute-based Interpretable Compatibility (AIC) method, which
discovers informative attribute crosses in an explicit and interpretable way. Specifically, we first automatically extract decision
rules on matching prediction by using a decision tree method. Then,
we design an embedding module to explicitly learn the vector representation for each rule by preserving the semantics of attributes
in the rule. We further propose a joint modeling module that unifies the visual embedding and attribute-based rule embedding to
predict the matching score. To enhance the interpretability, we design an attention network to select the most informative matching
patterns, making the overall prediction process easy-to-interpret.
To the best of our knowledge, this is the first work to develop an
interpretable fashion matching framework that can explicitly learn
attributed-based matching patterns.
Our contributions are summarized as follows.

:
0:

A

04

4:

Tank
Multi-color
Viscose
Dry-clean
Relaxed
stripes
Sleeveless
Summer
School

Sandals
Polyester
Red
High-heels
Open-toed
Casual
Dating

2

(b)

Figure 1: An illustration of the mix-and-match relationship
(Left) and rich fashion attributes associated with fashion
items (Right). Fashion items are usually described by a diverse set of attributes that carry rich semantics of items,
which have been largely ignored by existing fashion matching methods.
Traditional methods primarily leverages the visual content of
item images to learn the compatibility in a latent visual space. However, item images just describe the implicit and low-level visual
content. In addition to item images, fashion items on most fashion e-commercial websites are usually described by a diverse set
of attributes, which have been largely ignored by most existing
methods. For example, the item of ID 001 in Figure 1(a) has diverse
categorical attributes about category (midi-dress), pattern (floral),
color (natural-white), neckline (V-Neck), and style (causal), etc. The
attributes not only provide good semantic description of items, but
also have the potential to explicitly reveal the intra-connectivity
between items. They can help to explain why two fashion items
can be grouped together for a fashionable outfit based on a set of
attribute crosses[9, 35], such as [Fullbody: pattern=floral] & [Fullbody: category=Midi-dresses] & [Footwear: category=Sandals]. Each
attribute cross reflects a particular matching pattern.1
This paper aims to address the task of interpretable fashion match|A |
ing. We denote a and A = {ak }k=1 as an item attribute and the
whole attribute set, respectively. For a given item x i , we construct
its attribute set as Ai âŠ‚ A. Then, we can formally define this new
task as:
â€¢ Inputs: A corpus of fashion items with rich attributes and pairwise matching relationships {X, A, Y}.
â€¢ Outputs: (1) A pairwise ranking function for each pair of items
(x i , x j ), i.e., f : X Ã— X â†’ R which maps a pair of items to
a compatibility score value by jointly considering the visual
correlations and attribute correlations; and (2) a set of secondorder attribute crosses {ap &aq } or higher-order attribute crosses:
{ap &aq & Â· Â· Â· al } that explicitly reveals which attributes in x i and
x j dominate the matching process.

PROBLEM FORMULATION
|X |

Given a corpus of fashion items X = {x i }i with binary pair labels
Y = {yi j }, defined by

1 if(x i , x j ) âˆˆ C
yi j =
,
(1)
0
Otherwise
where C denotes the pairwise compatibility relationship (i.e., if x i is
compatible with x j , then yi j = 1), the basic goal of fashion matching
is to build a predictive model that estimates the compatibility score
between x i and x j :
yÌ‚i j = f (x i , x j ),

04

2

(a)

â€¢ We present an attribute-based interpretable compatibility framework that not only can predict the compatibility score of unseen
pairs, but also learn interpretable matching patterns that lead to
a good match.
â€¢ We propose to capture the semantics of decision rules by modeling attribute interaction, and unify the strengths of visual embedding and attribute-based rule embedding.
â€¢ We contribute a dataset with fashion attributes available to justify
the effectiveness of AIC on interpretable fashion matching. Extensive experiments show that AIC not only outperforms several
state-of-the-art methods, but also provides good interpretability
on matching decisions.

2

1

2

Midi-dresses Skinny jeans
Short-sleeve High-rise
V-neckline
Light-blue
Summer
Cotton
Floral
Denim
Viscose
Street-style
Natural-white
Casual
Beach

3

OUR PROPOSED APPROACH

This paper proposes to address the interpretable fashion matching
task, aiming to inject interpretability into the compatibility modeling of fashion items. The keys to tackling such interpretable fashion
matching task are 1) how to extract the self-interpretable attribute

(2)

where f denotes the predictive model, and yÌ‚i j denotes the predicted
compatibility score of a pair of items.

1 Note that

in this work we express the matching pattern as a attribute cross, which is a
combination of multiple attributes. We use them exchangeable without specification.

776

Session 9A: Fashion Match

BPR

SIGIR â€™19, July 21â€“25, 2019, Paris, France

Training

ğ‘¦ğ‘¦ï¿½ğ‘–ğ‘–ğ‘–ğ‘–

T:M=Wool

Prediction Score

yes

Joint Modeling

ğ«ğ«ğ‘–ğ‘–ğ‘–ğ‘– =

yes

1
âˆ‘ ğ‘¤ğ‘¤ ğ«ğ« ğ‘¡ğ‘¡
ğ‘‡ğ‘‡ ğ‘¡ğ‘¡ ğ‘–ğ‘–ğ‘–ğ‘–ğ‘–ğ‘– ğ‘–ğ‘–ğ‘–ğ‘–

xğ‘—ğ‘—
CNN

ğ«ğ«1

ğ«ğ«2

0.56

Attribute-based Decision Rule Embedding

yes

no
yes
no

Tree 1

â‹¯

no

yes

yes
Tree 2

Item ğ‘–ğ‘– attributes

no

yes

yes

Attribute nodes
Decision nodes

no

no

1.83

yes

no

yes

-0.96 -0.25

0.60

no

yes

-0.51 -1.01

no
Leaf
0.01 nodes

ï‚§
ï‚§
ï‚§

Top (T)
Bottom (B)
Material (M)

Pattern (P)
Category (Ca)
Lower Body Length (LBL)

Figure 3: A simple decision tree for a Top-Bottom matching.
no

no
Tree 3

â‹¯

XGBoost [8], for automatically constructing self-interpretable attribute crosses from categorical item attributes, in order to achieve
the self-interpretability and scalability. As shown in Figure 3, a
simple decision tree with binary node splits can be represented as
Q = {V, D, E}, where V denotes two types of nodes: one is internal/root nodes that represent features (attributes) and the other
is leaf nodes that represent outcomes for prediction; D denotes
binary decision nodes (yes, or no); and E denotes the edge connecting two nodes. The paths from root to leaf represent the decision
rules, revealing the reasoning procedure. By training the decision
tree using one-hot-encoded categorical attributes as inputs, each
derived decision rule can be seen as a high-order attribute cross
(i.e., matching pattern). As shown in Figure 3, the path from root
node to the second leaf node on the left side represents a threeorder attribute cross [Top:Material=Wool]&[Bottom:Material=Wool]
& [Top:Category,Blazers]. When the last decision is changed from
yes to no, the rule [Top:Material=Wool] & [Bottom:Material=Wool]
& [Top:Category=Blazers] still has high prediction score. It reveals
that sometimes the most dominant matching pattern may be a
second-order attribute cross. In this work, we adopt the boosted
tree model, e.g., GBDT [10], which is defined as an ensemble of T
Ã
decision trees Tt=1 Qt . Then, given the one-hot-encoded categorical attributes Ai j = (Ai , A j ) of (x i , x j ) as inputs, the boosted tree
module will return T decision rules {r i1j , Â· Â· Â· , r itj , Â· Â· Â· , r iTj }, where
r itj (1 â‰¤ t â‰¤ T ) denotes the t-th decision rule returned by its corresponding decision tree. Since a decision rule is directed and has
different decision states between two attribute nodes, for clarity,
we describe a decision rule in a path-like form

â‹¯

Item ğ‘—ğ‘— attributes

Figure 2: An illustration of our Attribute-based Interpretable Fashion Compatibility (AIC) framework.
crosses from data; 2) how to learn the representation of the derived
attribute crosses; and 3) how to unify the strengths of attribute
crosses and item images for joint prediction.
We address these three problems by developing an attributebased interpretable compatibility (AIC) framework, as shown in
Figure 2, which mainly consists of three modules:
â€¢ A Tree-based decision rule extraction module that automatically
derives a set of self-interpretable decision rules, in which each
decision rule can be seen as a high-order attribute cross or a set
of second-order attribute crosses.
â€¢ An embedding module that learns vector representations of decision rules by accounting for the attribute semantics.
â€¢ A joint modeling module that unifies the visual embedding and
attribute-based rule embedding in the same space to predict the
compatibility score.

3.1

no

Decision rule
ï‚§
ï‚§
ï‚§

R2.4 R3.1 R3.2

R1.1 R1.2 R1.3 R2.1 R2.2 R2.3
yes

ğ‘¥ğ‘¥ğ‘–ğ‘– ğ‘¥ğ‘¥ğ‘—ğ‘—
Item images

yes

â‹¯

ğ«ğ«3

T:P=Embellished

T:Ca=Blazers B:P= Ripped B:LBL=Calf_length B:M=Satin

Attentive Decision Rules Reweighting

xğ‘–ğ‘–

no

B:M=Wool

Tree-based Decision Rule Extraction

The main goal of the interpretable fashion matching framework
is to infer attribute-based matching patterns, i.e., attribute crosses.
Thus, the first problem is to extract the attribute crosses. A popular
solution in industry is to manually craft all the feature crosses, and
learn the weight of all feature crosses. Obviously, such a straightforward solution is not scalable when we model higher-order attribute
interactions on a large scale attribute set. Another solution is to
manually define a set of matching rules [24, 28, 30] based on the item
attributes, such as White shirt & black trousers. However, manually
defining matching rules usually needs strong domain knowledge
and may not be expressive enough to capture the complex matching
patterns. It is highly desirable to infer the rich matching patterns
from data automatically.
Motivated by recent works[35, 45] in recommendation domain,
we propose to leverage tree-based models, e.g., CART[3], GBDT[10],

s 1t

s 2t

s Zt

r itj : at1 âˆ’âˆ’â†’at2 âˆ’âˆ’â†’ Â· Â· Â· atZ âˆ’âˆ’â†’,

(3)

where atz (1 â‰¤ z â‰¤ Z ) denotes the z-th attribute in rule r itj , szt
denotes the binary decision state of attribute atz , and Z denotes the
number of attributes and decisions in rule r itj . The leaf node is not
shown in Eq. (3).
Note that we only utilize the GBDT model to automatically
extract the decision rules and do not use its prediction scores on the
leaf nodes for prediction, since it suffers from poor generalization
ability[35]. For unseen attribute vector inputs Ai j , it would return
a decision rule with all no decisions, such as the path from the root
node to the first leaf node on the right side in Figure 3.

777

Session 9A: Fashion Match

3.2

SIGIR â€™19, July 21â€“25, 2019, Paris, France

Attribute-based Decision Rule Embedding

After extracting a set of decision rules via the boosted tree model,
the next question is how to transform the decision rules into vector
representations for predicting the compatibility score. Since each
rule has a unique leaf node which corresponds to a unique ID, prior
work [35] proposed to encode rule ID as a vector, while ignoring
the semantics of decision rules. To be more specific, such ID embedding method fails to model the semantic correlation between
similar rules. To address this problem, we propose to embed the
semantics of each rule into a low-dimensional vector by taking the
attribute interactions into consideration. We elaborate this solution
as follows:
Attribute and Decision Embedding. Recall that each rule is composed by attributes, decisions, and edges that connect two nodes,
as show in Figure 3 and 4. To represent the attribute, we first set
up a lookup layer to transform the one-hot encodings of all the
|A |
attributes {ak }k=1 into low-dimensional dense embedding vectors

yes

ğšğ‘¡ğ‘§

2(t )

âˆ’a t âŠ— â†’
âˆ’a t ,
=â†’
z
z+1

z

yes

yes

ğ‘¡
ğšğ‘§+1

3-order
Attribute Crosses

Rule Embedding

Attribute

Embedding

Decision

Embedding

Attribute Cross Embedding

Figure 4: An illustration of the proposed attribute-based decision rule Embedding.
âˆ’a t & â†’
âˆ’a t & Â· Â· Â· &â†’
âˆ’a t
â€¢ Higher-order attribute cross â†’
z
z+1
z+oâˆ’1 , which
o(t ) â†’
âˆ’
â†’
âˆ’
â†’
âˆ’
t
t
t
is represented by v
= a âŠ— a
âŠ— Â·Â·Â· âŠ— a
,
z

o(t )
vz

z

z+1

z+oâˆ’1

Rd

where
âˆˆ
(2 â‰¤ o â‰¤ Z ) denotes the embedding vector of the
z-th high-order attribute cross. The vz2 is a specific form of voz when
o = 2. The âŠ— operator denotes the element-wise multiplication, i.e.,
Hadamard Product. Finally, the embedding of the rule r t is defined
as the linear aggregation of all the attribute crosses embedding with
an average pooling operation
rit j =

O Z âˆ’o+1
1 Ã• Ã• o(t )
v ,
N o=2 z=1 z

(5)

where rit j âˆˆ Rd , and N is the number of all attribute crosses in the
decision rule.

3.3

Visual-Rule Joint Modeling

This section describes how to jointly model visual embedding of
items and attribute-based rule embedding for predicting fashion
compatibility. It mainly consists of three submodules: 1) learning
low-dimensional visual embeddings of item images with a pretrained CNN, 2) reweighting the embeddings of decision rules with
an attention network, and 3) jointly leveraging visual embedding
and attribute-based rule embedding for compatibility prediction.
Deep Visual Embedding of Items. The deep visual embedding
learning module on the bottom left side of Figure 2 has been widely
used in existing visual compatibility learning models due to the
strong transferability of deep features. This work adopts a pretrained deep CNN (e.g., ResNet-50[14]) to extract visual features
from item images. Given an image of item x i , the output of a prec nn
trained CNN is xcnn
âˆˆ Rd
where xcnn
is a high-dimensional
i
i
visual feature representation of item x i . Then we apply a one-layer
feed forward network to transform the high-dimensional output of
CNN into a d-dimensional visual embedding xi âˆˆ Rd :

xi = Ğ´ xcnn
= WĞ´ xcnn
+ bĞ´ ,
(6)
i
i

k

âˆ’a denotes the translated embedding vector of a . For simwhere â†’
k
k
âˆ’a to denote the attribute a with decision state
plicity, we use â†’
k
k
âˆ’a denotes its vector representation. In this way, we only
sk , then â†’
k
need to optimize (2 + |A|) embedding vectors while preserving the
exclusive relationship between attribute and its opposite.
Rule Embedding. After injecting the embedding vectors of binary decision states into the attribute embeddings by Eq. (4), we
âˆ’a t â†’â†’
âˆ’a t â†’ Â· Â· Â· â†’
âˆ’a t , which is a
can reformulate Eq. (3) as r itj : â†’
1
2
Z
sequence of inner-connected attributes. Then, the popular pooling
operation, such as max-pooling or average-pooling, can be used
to compute the embedding vector of decision rules based on the
attribute embeddings. But this approach does not explicitly model
the second-order or higher-order attribute interactions, and also
cannot identify which attribute cross in the decision rule is the
most informative one.
To address this issue,we propose to learn the representation of
decision rule based on the interaction of attribute crosses in the rule.
As shown in Figure 4, the second-order and higher-order attribute
crosses in the rule are respectively described and represented by
âˆ’a t & â†’
âˆ’a t , which is represented
â€¢ Second-order attribute cross â†’
by vz

no

Average Pooling

|A |

k

Decision Rule

2-order
Attribute Crosses

{ak }k =1 âˆˆ RdÃ—| A | . While, as shown in Figure 3, each attribute
could have two decision states (yes and no) in two mutually exclusive decision edges. One question here is how to model such
decision states into the attribute representation. A simple way is
to directly treat the attribute (e.g., [Top:Material=Wool]) and its
opposite [Top:Material,Wool]) as two independent attributes. Then,
we need to optimize 2 Ã— |A| attribute embedding vectors. However, such a solution ignores the exclusive relationship between
[Top:Material=Wool] and [Top:Material,Wool]. To address this limitation, we propose to embed the two decision states as the same
dimensional vector representations sk âˆˆ Rd with the attribute
embeddings ak via the look up operation. To model the exclusive
relationship, we propose to combine the attribute embedding and its
corresponding decision embedding by a simple vector translating
operation[2]:
â†’
âˆ’a = a + s ,
(4)
k

ğ¬ğ‘§ğ‘¡+1

ğ¬ğ‘§ğ‘¡

where Ğ´(Â·) is a one-layer feed forward network with weight paramc nn
eters WĞ´ âˆˆ RdÃ—d
and bĞ´ âˆˆ Rd . The visual embedding module
enables our framework to generalize to unseen fashion items.
Attentive Decision Rules Re-weighting. Given inputs (Ai , A j )
of (x i , x j ), our boosted tree module (GBDT) returns T decision rules

z+1

778

Session 9A: Fashion Match

SIGIR â€™19, July 21â€“25, 2019, Paris, France

The third part is equal to hT3 (xi âŠ— ri j ) + hT3 (xj âŠ— ri j ), which
transforms the interaction of ri j and xi and the interaction of ri j
and xj into the compatibility scores, respectively. The third part
aims to capture the complex interaction between low-level visual
concept and high-level semantic concept (i.e., attributes) in a joint
space. It refines the item-item visual compatibility with the intraconnectivity between the two items, which enables the second-time
message passing between visual space and attribute-based rule
space in a mutually enhanced way.

[r i1j , Â· Â· Â· , r itj , Â· Â· Â· , r iTj ]. Note that not every rule has equal contribution to (x i , x j ), and some rules may also be invalid. Therefore, it is
necessary to design an attention module to modulate the contribution of each rule. Inspired by the recent work [6, 30, 35], we apply
a multi-layer perceptrons (MLPs) to learn the attentive weight of
each derived rule:
w â€²i jt = wT Ïƒ (W([(xi + rit j ) âŠ— xj , rit j ]) + b),

(7)

exp(w â€²i jt )
w i jt = ÃT
â€²
t exp(w i jt )

(8)

3.4

where w i jt denotes the weight of the t-th rule corresponding to
(x i , x j ), W âˆˆ RdÃ—2d and b denotes the weight matrix and bias
vector of the hidden layer in our attention module, and w âˆˆ RdÃ—1
is the weight vector of the regression layer. Also [Â·, Â·] denotes the
concatenation operation of two vectors, and Ïƒ is the non-linear
activation function ReLu. In Eq. (7), we project (xi + rit j ) âŠ— xj into
the attention module, which aims to directly capture the interaction
xi âŠ— xj and r itj âŠ— xj in the same embedding space. Note that Eq. (7) is
implemented in an asymmetrical form by considering the directed
matching order (e.g., Top-Bottom[30, 31] and Top-Footwear) in
the fashion matching task. Then, we can obtain a unified vector
representation of all the derived decision rules corresponding to
(x i , x j ):
ri j =

T
1Ã•
w i jt rit j
T t =1

We formulate the fashion matching task as a ranking problem and
minimize the Bayesian Personalized Ranking (BPR) objective [27]
which forces the prediction score of a matched pair (x i , x j ) âˆˆ C to
be larger than that of unmatched pair (x i , x k ) < C:
Ã•


L=
âˆ’ ln Ïƒ f x i , x j , Ai j âˆ’ f (x i , x k , Aik ) ,
(12)
T

where Ïƒ (Â·) is the widely-used logistic sigmoid function. The regularization term has been omitted for clarity. T denotes a training set
of 5-tuples : (x i , x j , x k , Ai j , Aik )|(x i , x j ) âˆˆ C, (x i , x k ) < C . The
matched pair (x i , x j ) is extracted from the same outfit. The negative item x k is randomly selected from a different category with
x i , which has not matched with x i before. Note that our tree-based
module is first trained and then fixed as a decision rule extractor.

(9)

3.5




f x i , x j , Ai j = hT1 xi âŠ— xj + hT2 ri j + hT3 (xi + xj ) âŠ— ri j ,
| {z } |{z} |
{z
}
Rul e

Discussion

3.5.1 Interpretability. The main goal of the interpretable fashion
matching task is to learn self-interpretable attribute crosses for
revealing the reasons behind each matching decision. Our proposed
AIC method injects interpretability into the fashion compatibility
modeling, which is able to provide two levels of interpretation.
â€¢ Given a pair of items x i and x j from different categories, the tree
module first returns a set of decision rules. Then, our attention
model re-weights each rule embedding and selects informative
decision rules by the importance w i jt to x i and x j as the first-level
interpretation. (Rule-based)
â€¢ Given a selected decision rule r itj , our predictive model in Eq.
(11) can identify which attribute cross in the rule dominates this
matching. (Attribute cross-based)
In summary, we not only can yield a decision rule to explain the
matching process, but can also identify the most dominant attribute
cross in the rule. We have conducted a case study in section 4.4 on
the interpretability of AIC.

The attention module enables the first-time message passing
between visual space and attribute-based rule space for learning
the importance of decision rules. Note that the attention module
endows our framework with interpretability. For each matching
pair, we can return the most informative decision rule to explain
the matching result.
Joint Prediction. Given the visual embedding vectors xi and xj
of items x i and x j , and the unified rule embedding vector ri j , we
design a joint modeling solution that can enable the visual part and
rule part perform separately and mutually. The complete predictive
function is defined by

V isual

Learning

V isual âˆ’Rul e

(10)
where h1 âˆˆ RdÃ—1 , h2 âˆˆ RdÃ—1 , and h3 âˆˆ RdÃ—1 denote the weight
parameters of three regression layers, respectively, which yields
compatibility predictions from three parts: the first is visual compatibility (h1 ), the second is rule-based compatibility (h2 ), and the third
is visual-rule joint compatibility (h3 ). To identify the contribution
of each attribute cross in a decision rule, the second term can be
rewritten as

3.5.2 Relation to Tree-enhanced Embedding (TEM). Our proposed
AIC has a similar two-way (embedding + tree) architecture as that
of TEM[35]. The key difference lies in the decision rule embedding
module. TEM simply encodes ID information as a dense vector to
represent a rule, while ignoring the semantics of rules. To be more
specific, TEM treats all rules independently and fails to explicitly
model the semantic correlation between rules. Moreover, its parameter size is linear with respect to the scale of decision rules, which
easily leads to overfitting when the tree number is large (as verified
in section 4.3.2). AIC overcomes the limitation of TEM by linearly
modeling the attribute interactions into semantics-preserving rule
embedding, thus can not only achieve better performance than

T
T O Z âˆ’o+1
1Ã•
1 Ã•Ã• Ã•
o(t )
w i jt hT2 rit j =
w i jt hT2 vz ,
T t =1
T Ã— N t =1 o=2 z=1
(11)
o(t )
where the w i jt (hT2 vz ) is the prediction score contributed by the
âˆ’a t & â†’
âˆ’a t & Â· Â· Â· &â†’
âˆ’a t
attribute cross â†’
z
z+1
z+oâˆ’1 in the t-th decision rule.

hT2 ri j =

779

Session 9A: Fashion Match

SIGIR â€™19, July 21â€“25, 2019, Paris, France

TEM, but also provides higher interpretability. Besides, AIC enforces interaction between visual embedding and rule embedding
in the prediction layer, which yields better performance.
In summary, 1) AIC learns the attribute-based rule embedding
while TEM only learns ID-based rule embedding, 2) AIC not only
provides decision rules as an interpretation but can also identify
the most informative attribute cross as the second-level interpretation, while TEM only provides rule-level interpretation, and 3) AIC
models the interaction of visual embedding and rule embedding in
the same embedding space.

To justify the effectiveness of AIC, we conduct extensive experiments to answer the following questions:
â€¢ RQ1: Can our AIC framework outperform the state-of-the-art
approaches?
â€¢ RQ2: How do different modules of our AIC (e.g., the attributebased rule embedding module) contribute to the overall performance?
â€¢ RQ3: How can our AIC provide easy-to-interpret fashion matching results?

and report the average performance of all methods on the testing
set with significance test. For each matched item-item pair in the
training set, we pair it with three randomly sampled negative items
from a different category. Each query item and its negative items
must not co-occur in the same outfit. For each matched pair in the
testing set, we pair it with 500 negative items. Then each method
outputs prediction scores for these 501 items. If not otherwise
mentioned, all negative items are sampled from the whole dataset
except from the category of the query item.
To evaluate the prediction performance of a ranked list, we use
three widely-used information retrieval metrics: the Mean Reciprocal Rank (MRR), Hit Ratio at rank K (hit@K), and Normalized
Discounted Cumulative Gain at rank K (ndcg@K). The MRR is the
average of the reciprocal ranks of results for a sample of queries.
The hit@K intuitively measures whether the test item is present
on the top-K list, and the ndcg@K accounts for the position of
the hit by assigning higher scores to hits at top-K list. A higher
MRR, hit@K, or ndcg@K score denotes a better performance. We
calculate all metrics for each test query item and reported the average score. Without special mention, we truncate the ranked list at
K = 5 and K = 10 for hit@K and ndcg@K

4.1

4.2.2 Baselines. We compare our proposed AIC with the following
baseline methods to justify its effectiveness:

4

EXPERIMENTS

Dataset Description

The most popular fashion matching dataset is the Polywore [13, 31,
39]. However, this dataset does not have fashion attribute annotation. To the best of our knowledge, there is no available dataset
for this fashion matching task, due to the absence of fine-grained
attribute annotations. To effectively evaluate our AIC framework,
we collect a large outfit dataset from a personal outfit recommendation website Lookastic 2 which provides diverse and fashionable
outfit collections with detailed product attribute annotations. We
collected 30,790 fashionable outfits from the website, in which both
male and female outfits are collected. Each outfit contains a set of
items from multiple complementary categories (e.g., Top, Outwear,
Bottom, Footwear).
Following the setting in [12, 31], we extract matched item pairs
that are co-occurring in the same outfit as the ground truth for
training, and filter out some improper or incomplete pairs. Finally,
we obtain 124,665 matched pairs for men with 5,069 items, 158,755
matched pairs for women with 10,016 items. Apart from the attributes provided by Lookastic, we also use the Visenze3 API to
extract more item attributes and filter out overlapping attributes.
This final dataset has diverse item attribute annotations consisting
of 65 colors, 38 materials, 40 patterns, 253 fine-grained categories, 11
styles, and 114 category-specific attributes.
We evaluate our proposed AIC with baseline methods on LookasticMen, and Lookastic-Women, respectively. We randomly split the
dataset by 70% for training, 20% for testing, and 10% for validation.
The validation set is used to tune hyper-parameters and the final
comparison is conducted on the test set.

4.2

- Siamese Nets[33] (SiaNet). It measures the visual compatibility
using â„“2 -normalized Euclidean distance. (Image only)
- BPR-DAE[31]. This work models the pairwise visual compatibility as the inner-product of item embeddings. (Image only)
- TransNFCM[39]. It is a state-of-the-art fashion matching method
that leverages category-level complementary relationships to refine the item-item compatibility. (Image + coarse category)
- VBPR[15]. It is a strong baseline for visually-aware user-item interaction modeling. It fuses visual information and ID embedding
to enhance the item representation. (Image + ID)
- Neural Factorization Machines[17] (NFM). It is a state-of-theart embedding-based learning method that implicitly models
higher-order feature interaction in a nonlinear way. We implement it by encoding all item attributes and item images with
embedding vectors. (Image + attributes)
- TEM[35]. It is a state-of-the-art embedding-based learning method
that combines the strength of traditional embedding-based models and the tree-based models. Different with AIC, it learns the
ID embedding to represent rule. (Image + attributes)
Note that we use the same deep visual embeddings of item images
for all baselines. The ID embeddings of items in TEM are replaced
by visual embeddings of images for a fair comparison. We only
use the visual modules of BPR-DAE and TransNFCM in our experiments, due to the absence of textual descriptions in our dataset.
We implement all the baseline methods, using the same BPR loss,
except SiaNet4 .
4.2.3 Parameter Settings. We implement AIC by stochastic gradient descent (SGD) using Pytorch5 . The pretrained ResNet-50[14]
model is applied to extract visual feature of item images using the

Experimental Settings

4.2.1 Evaluation Protocols. To evaluate the effectiveness of our
model more fairly, we repeat the random dataset split for five times
2 https://lookastic.com/

4 We

3 https://www.visenze.com/automated-product-tagging

5 https://pytorch.org

780

empirically found that SiaNet performs much better with margin ranking loss

Session 9A: Fashion Match

SIGIR â€™19, July 21â€“25, 2019, Paris, France

Table 1: Overall Performance Comparison (%) with baseline methods. * and ** denote the statistical significance for
pvalue < 0.05 and pvalue < 0.01, respectively, compared to
the best baseline. RI denotes the relative improvement on
the best baseline.
Dataset
Methods

MRR

BPR-DAE
Siamese
TransNFCM
VBPR
NFM
TEM
AIC
RI
Dataset
Methods

23.35
23.05
26.14
28.32
28.92
29.10
30.74**
5.6%

30.97
31.37
34.94
36.83
37.49
37.88
39.51**
4.3%

23.69
24.00
29.88
29.46
30.49
31.63
33.19**
4.9%

32.97
33.71
41.01
39.32
40.90
42.35
43.83*
3.4%

MRR(%)

29
28
1

5

10

50

100

ndcg@10

24.02
24.25
30.70
30.06
31.15
32.32
33.94*
5.0%

27.02
27.65
33.96
32.98
34.29
35.55
37.01**
4%

33
32
31

AIC (ID)
AIC (Attri.)

26.17
26.12
29.30
31.34
32.02
32.27
33.88**
4.9%

Lookastic-Women

34

30

â€¢ BPR-DAE and SiaNet, which merely rely on visual information,
achieve poor performance. TransNFCM and VBPR perform much
better, since TransNFCM exploits the category-level complementary relationship as the connection between compatible items
and VBPR combines the ID embedding of items and visual embedding for feature augmentation. It indicates the necessity of
exploiting the side information for modeling the complex fashion
compatibility beyond the visual information, since visual embeddings of items just comprise of low-level signals, which cannot
effectively capture the complex interaction patterns.
â€¢ NFM and TEM achieve competitive performance, which can be
attributed to the utilization of feature interaction. NFM exploits
high-order feature interaction with a multi-layer MLPs in a nonlinear way, which consistently outperforms the strong baseline
VBPR. While TEM uses a tree-based model to automatically derive higher-order feature crosses with an attention mechanism.
It slightly outperforms NFM on both datasets, especially the
Lookastic-Women dataset where there are more diverse item-item
interactions. It indicates the effectiveness of modeling the highorder feature interactions.
â€¢ Our proposed AIC substantially outperforms the state-of-the-art
methods, NFM and TEM, on both datasets. This demonstrates
the effectiveness of AIC. It not only integrates the predictions
from both visual space and attribute-based rule space in the
prediction layer, but also explicitly learns the semantics of decision rules based on the attribute interaction in the rule. Such
semantics-preserving rule embedding is jointly modeled with
visual information in a unified space, which leads to better performance and also reveals the complex matching patterns in a
more explicit way.

ndcg@10

23.28
23.04
26.28
28.57
29.16
29.33
31.06**
5.8%

42.25
44.23
51.08
48.33
50.60
52.33
53.09**
1.4%

Lookastic-Men

31

27

30.90
40.92
44.27
45.40
46.37
46.97
48.23**
2.6%

Lookastic-Women
hit@5 hit@10 ndcg@5

MRR

BPR-DAE
Siamese
TransNFCM
VBPR
NFM
TEM
AIC
RI

MRR(%)

Lookastic-Men
hit@5 hit@10 ndcg@5

4.3.1 Overall Comparison. (R1) Table 1 presents the performance
comparison w.r.t. MRR, hit@K (K=5, 10), and ndcg@K (K=5, 10)
among the baseline methods on the Lookastic-Men and LookasticWomen datasets. We have the following findings:

30

AIC (ID)
AIC (Attri.)

1

Tree Number (T)

5

10

50

Tree Number (T)

1
0
0

Figure 5: Comparison (MRR (%)) of the attribute-based (AIC
(Attri.)) and ID-based (AIC (ID)) rule embeddings.

4.3.2 Effect of Attribute-based Decision Rules Embedding. (R2) One
of the contributions of AIC is that it learns the semantics of decision
rules by explicitly modeling the attribute interaction. While, the
prior work [35] proposes to learn the ID embedding of each rule
without considering the content of each rule. To justify the effect of
our attribute-based rule embedding, we compare the performance
of this two rule embeddings in Table 2 and Figure 5, which are
termed as AIC(Attri.) and AIC(ID), respectively. Note that we fix
the maximum depth of tree as 6 and vary the number of decision
trees T âˆˆ [1, 5, 10, 50, 100] to generate different tree structures for
comparison. We have the following observations from Table 2 and
Figure 5.
Overall, the attribute-based rule embedding consistently outperforms the ID-based rule embedding. When the tree number is 5 or
10, AIC (ID) performs comparable to AIC (Attri.). However, when
the tree number is increased to 50 or 100, the performance of AIC
(ID) drops significantly. It reflects that the AIC (ID) is sensitive to
the tree numbers. It easily suffers from overfitting when the tree
number is large, since its parameter size is linear with the scale of
all the leaf nodes in GBDT. While AIC(Attri.) directly optimizes the
attribute embedding and could thus effectively capture the semantic

output of the pool5 layer. The size of hidden layer for learning lowdimensional visual embedding is set to d = 64 as well as the latent
embedding size of item attributes. The mini-batch size is set to
1024 and the learning rate Î· is searched in {0.001, 0.01, 0.05, 0.1 } on
the validation set. We use XGBoost6 to generate the tree-structure
where the number of trees and the maximum depth of trees are
searched in {1, 10, 30, 50, 80, 100 } and {4, 6, 8, 10} on the validation set, respectively. Unless otherwise mentioned, the number and
maximum depth of trees are fixed as 10 and 6 on the testing set,
respectively. We employ SGD to optimize all methods with momentum factor of 0.9. We run all methods until convergence and drop
the learning rate Î· to Î·/10 at every 10 epochs.

4.3

Performance Comparison

We first compare the performance of all the methods. We then justify how our method can effectively learn the semantics of decision
rule for enhancing the compatibility modeling.
6 https://xgboost.readthedocs.io/en/latest/

781

hit@5

Session 9A: Fashion Match

SIGIR â€™19, July 21â€“25, 2019, Paris, France

Lookastic-Men
Lookastic-Women

T=1
T=5
T=10
T=50
T=100

Datasets
Methods
AIC (Attri.)
AIC (ID)
AIC (Attri.)
AIC (ID)
AIC (Attri.)
AIC (ID)
AIC (Attri.)
AIC (ID)
AIC (Attri.)
AIC (ID)

Lookastic-Men
hit@5 ndcg@5
37.16
28.92
35.99
27.60
39.34
30.88
39.05
30.59
39.51
31.06
39.25
30.83
39.32
30.77
38.85
30.33
39.45
30.90
37.88
29.55

Lookastic-Women
hit@5 ndcg@5
42.05
32.22
41.05
31.27
43.66
33.80
43.57
33.69
43.83
33.94
43.46
33.78
43.81
33.97
42.85
33.16
43.87
34.06
41.99
32.38

MRR

hit@5

AIC (Rule only)
AIC (VRI only)
AIC (without VRI)
AIC (with VRI)

18.90
29.22
30.38
30.74

25.17
38.03
39.13
39.51

Dataset
Methods

MRR

hit@5

AIC (Rule only)
AIC (VRI only)
AIC (without VRI)
AIC (with VRI)

23.40
33.12
32.73
33.18

30.97
43.64
43.19
43.83

Lookastic-Men
hit@10 ndcg@5
34.11
46.98
47.92
48.23

18.37
29.49
30.68
31.06

Lookastic-Women
hit@10 ndcg@5
39.82
53.28
52.62
53.09

23.30
33.83
33.43
33.94

45

36

43

34

41
39
37
Lookastic-Men

Attention

Max Pooling

Lookastic-Women
Average Pooling

Average Pooling
37.696
42.687

32
30
28
26
Lookastic-Men
Attention

Lookastic-Women

Max Pooling

Average Pooling

Figure 6: Ablation study on the effect of the attention network using hit@5 (Left) and ndcg@5 (Right).
4.3.4 Effect of the Attention Network. (R2) As mentioned in section
3.3, we design an attention network to re-weight the decision rule
embeddings. This section investigates how this attention network
improve the performance. We replace the attention module with
average pooling/max-pooling and then compare the performance
of AIC with the two variants. As shown in Figure 6, the attention
network consistently outperforms the average pooling and maxpooling operations in terms of hit@5 and ndcg@5. It indicates that
some derived rules are invalid. It will degrade the performance by
simply aggregating all the rule embedding with average pooling.
Although the max-pooling operation obtains better performance
than average pooling, it is an element-wise nonlinear operation,
which makes the matching process hard-to-interpret. Overall, the
attention network not only makes the item-item matching easy-tointerpret but also further improves the performance.

Table 3: Ablation study on the effect of visual-rule interaction (VRI) term.
Dataset
Methods

Max Pooling
38.01
42.98

ndcg@5(%)

TreeNum

hit@5 (%)

Table 2: Comparison (hit@5, ndcg@5, %) of the attributebased (AIC (Attri.)) and ID-based (AIC (ID)) rule embeddings.

Attention
39.512
43.834

ndcg@10
21.25
32.38
33.52
33.88
ndcg@10
26.16
36.95
36.49
37.00

4.4

Case Study on Interpretation (R3)

To demonstrate the interpretability of AIC, we visualize two itemitem matching cases on Lookastic-Women in Figure 7. Figure 7 (a) is a
Top-Bottom case, and Figure 7 (b) is a Fullbody-Footwear case. Each
item-item matching pair is sampled on the testing set (positive).
For simplicity, the maximum depth of GBDT is set to 4 and only
second-order attribute crosses are computed. As shown in Figure
7, the abbreviations of attributes are shown on the right side of
each decision rule and the normalized score of each second-order
attribute cross is shown on the left side.
For the first case in Figure 7 (a), the input is a navy coat paired
with low rise gray jeans. We observe that the first decision rule encodes some common sense matching patterns, such as Sophisticated
knee length top doesnâ€™t match with shorts, and Sophisticated knee
length top matches with low rise bottom. In most cases, high rise
bottom is more likely to match with short body length top, which
could more clearly highlight womenâ€™s beautiful waist curve. By
our proposed AIC, we also identify the most dominant attribute
cross in a decision rule. The second-order attribute cross with the
highest score in the first rule is [Bottom: Rise Type=Low rise]&[Top:
Style=Sophisticated]. For the second decision rule, it still cares about
the clothing length. The most dominant attribute cross is [Top: Sleeve
Length=Long] &[Bottom: Lower Body Length=7/8], which can be
explained as long sleeve top goes with long body length bottom. For
the Fullbody-Footwear case, the input is a white sleeveless cutout
dress paired with white heels. The first decision rule is mainly dominated by the attribute cross [Fullbody: Color=White]&[Footwear:
Color=White], which is a common matching pattern. The second
decision rule is dominated by the attribute cross [Fullbody: Pattern=Cutout]&[Footwear: Heel Type=Common heels].

correlation between similar rules. The performance comparison
justifies the effectiveness of AIC on the semantic encoding of rules.
4.3.3 Effect of Visual-Rule Joint Modeling. (R2) As shown in Eq.
(10), AIC not only predicts the visual compatibility and semantic
compatibility with two regression vectors (h1 and h2 ) respectively,
but also transforms the visual-rule interaction (VRI) (xi + xj ) âŠ— Ri j
into a compatibility score with the regression vector h3 . This section
investigates how AIC perform with/without the VRI term, and how
AIC perform with the rule term or VRI term only.
The performance comparison is shown in Table 3. If only using
the rule term, AIC achieves poor performance, since tree-based
module has poor generalization ability [35]. If only using the VRI
term, AIC achieves comparable performance to the combination
of the other two predictions (h1 (Â·) + h2 (Â·)) on the Lookastic-Men
dataset, and even performs better on the Lookastic-Women dataset
which has richer item-item interactions. It yields 38.03% hit@5 score
and 43.64% hit@5 score, respectively, on the two datasets, which
outperforms most of the baseline methods in Table 1. When the VSI
term is integrated with the other two terms, it effectively improves
the prediction from 39.13% to 39.51% on Lookastic-Men and from
43.19% to 43.83% on Lookastic-Women in terms of hit@5 score. On
Lookastic-Women, the VRI term has dominated the prediction. It
shows the necessity and effectiveness of modeling the interaction
of visual embedding and rule embedding in a shared embedding
space.

782

Session 9A: Fashion Match

SIGIR â€™19, July 21â€“25, 2019, Paris, France

B::RT=Low_rise

0.09

0.54
0.42

(a)

0.04

T::S=Sophisticated

0.80

T::UBL=Knee length

0.10

0.11

(b)
ï‚§
ï‚§
ï‚§
ï‚§
ï‚§
ï‚§
ï‚§
ï‚§

0.21

B::LBL=7/8
T::SL=Long

Decision rule #2

Fb::Co=White

0.75

Fw::Co=White

0.12

Fw::HTâ‰ Flat form
Fw::SDâ‰ Other
Decision rule #1

Top (T)
Bottom (B)
Rise Type (RT)
Style (S)
Fullbody (Fb)
Footwear (Fw)
Heel Type (HT)
Shoe Decoration (SD)

for cross modal retrieval [23], interactive fashion search [11, 44],
classification [25, 29], and fashion trend prediction [1]. Unlike prior
work, this paper prefers to use the rich product attributes associated with fashion items to design an interpretable fashion matching
framework. Current visual analysis methods can facilitate our work
when the attribute annotation is unavailable.
User-item Recommendation. Personalized recommendation [18,
19, 34, 36, 37] has also been applied to fashion industry [42]. Its core
is to estimate how likely a user will adopt a fashion item based on
the historical interactions and visual apperance of items [6, 38, 42].
Our work is related to the personalized multimedia recommendation methods [6, 38, 42], which leverage the ID information and
visual information of items to model user-item interaction. In this
work, we only focus on cross-category item matching without considering the user information. While, the user attributes can be
easily incorporated into AIC for personalized compatibility modeling, which is left for our future work.

B::DWCâ‰ Dark

B::Caâ‰ Shorts
Decision rule #1

0.68

T::Coâ‰ White

ï‚§
ï‚§
ï‚§
ï‚§
ï‚§
ï‚§
ï‚§
ï‚§

0.13

Fb::P=Cutout
Fw::HT=Common heels
Fw::Sâ‰ Minimalism
Fb::SL=Sleeveless
Decision rule #2

Pattern (P)
Category (Ca)
Color (Co)
Lower Body Length (LBL)
Upper Body Length (UBL)
Sleeve Length (SL)
Denim Wash Color (DWC)
Shoe Type (SD)

6

Figure 7: Visualization of the derived decision rules and the
normalized prediction score of each second-order attribute
cross in the rule. The highest score is marked in red. Note
that the binary decision state has been merged with its corresponding attribute for simplicity.
Overall, the derived matching patterns are consistent with the
given matched pairs, and the discovered second-order attribute
crosses have higher readability and are also easy-to-interpret. The
two matching cases demonstrate the capability of AIC in providing
more informative and easy-to-interpret matching patterns.

5

CONCLUSION

In this paper, we developed an attribute-based interpretable compatibility (AIC) method, which aims to inject interpretability into
the pairwise compatibility modeling. Specifically, we devised a twoway compatibility architecture. Given a matched pair of items, it
first automatically extracts a set of decision rules from a boosted
tree model and learns the semantics-preserved rule embedding by
explicitly modeling the attribute interaction. Then, it leverages a
joint modeling module to unify the strengths of visual information
and attribute-based rule information in a shared embedding space,
which facilitates the information propagation between visual space
and rule space in a mutually-enhanced way. By such a two-way
architecture, AIC could not only predict the compatibility score of
unseen pairs, but also derive self-interpretable matching patterns to
reveal the reasons behind each matching decision. In summary, this
work contributes a self-interpretable approach for fashion compatibility modeling by deriving the intra-connectivity between items
based on rich fashion attributes.
As future work, we will work towards discovering informative
extra-connectivity between items by constructing a domain-specific
knowledge graph [4, 5, 34] which could encode richer information,
e,g, designer, celebrity, fashion show, country, religion, etc, to further enrich the interpretability of AIC. We are also interested in
incorporating the user profile, such as age, occupy, gender, city,
social relationships, etc., into AIC for personalized fashion matching and personalized outfit composition. We will also extend AIC
to facilitate other attribute-based visual matching/retrieval tasks
[20, 24].

RELATED WORK

Fashion Matching. Existing works can be mainly classified into
two groups: one is outfit creation [13, 22] aiming to automatically
compose fashion outfits , and the other one is item-item compatibility [7, 16, 26, 30â€“32, 39], which is close to our work. Most existing
methods of the second group cast fashion matching as a metric
learning [40, 41] problem by assuming that a pair of matched items
should be close to each other in a latent space. Earlier works model
the pairwise compatibility with data-independent interaction functions, e.g., inner-product[31], or Euclidean distance[7, 26], which
are improved by data-dependent interaction function, such as probabilistic mixtures of non-metric embeddings [16], and categoryaware conditional similarity [32, 39]. Our work is related to the
second direction but addresses the new and challenging task of
interpretable fashion matching, where we not only predict compatibility of unseen pairs but also aim to learn self-interpretable
matching patterns to uncover the reasons behind each matching
decision. Our work is different from the recent work [30] which
first manually constructs a set of matching rules and then use these
rules to guide the item embedding learning. The main limitation of
[30] is that manually constructing matching rules usually rely on
strong domain knowledge, thus resulting in poor scalability.
Fashion Attributes. In recent years, substantial works [1, 12, 21,
23, 25, 29] have been devoted to extract and analyse visual descriptive attributes from fashion images or related textual descriptions

7

ACKNOWLEDGMENTS

This research is part of NExT++ research and also supported in part
by the Thousand Youth Talents Program 2018, and in part by the
National Natural Science Foundation of China (NSFC) under Grant
61725203 and Grant 61732008. NExT++ research is supported by the
National Research Foundation, Prime Ministerâ€™s Office, Singapore
under its IRC@SG Funding Initiative.

783

Session 9A: Fashion Match

SIGIR â€™19, July 21â€“25, 2019, Paris, France

REFERENCES

[24] Lizi Liao, Yunshan Ma, Xiangnan He, Richang Hong, and Tat-Seng Chua. 2018.
Knowledge-aware Multimodal Dialogue Systems. In ACM MM. ACM, 801â€“809.
[25] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. 2016. Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In
CVPR. 1096â€“1104.
[26] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel.
2015. Image-based recommendations on styles and substitutes. In SIGIR. ACM,
43â€“52.
[27] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.
2009. BPR: Bayesian personalized ranking from implicit feedback. In UAI. AUAI
Press, 452â€“461.
[28] Amrita Saha, Mitesh M Khapra, and Karthik Sankaranarayanan. 2018. Towards
Building Large Scale Multimodal Domain-Aware Conversation Systems. In AAAI.
[29] Edgar Simo-Serra and Hiroshi Ishikawa. 2016. Fashion style in 128 floats: joint
ranking and classification using weak data for feature extraction. In CVPR. 298â€“
307.
[30] Xuemeng Song, Fuli Feng, Xianjing Han, Xin Yang, Wei Liu, and Liqiang Nie.
2018. Neural Compatibility Modeling with Attentive Knowledge Distillation. In
SIGIR. New York, USA, 5â€“14.
[31] Xuemeng Song, Fuli Feng, Jinhuan Liu, Zekun Li, Liqiang Nie, and Jun Ma. 2017.
Neurostylist: Neural compatibility modeling for clothing matching. In ACM MM.
ACM, 753â€“761.
[32] Mariya I Vasileva, Bryan A Plummer, Krishna Dusad, Shreya Rajpal, Ranjitha
Kumar, and David Forsyth. 2018. Learning Type-Aware Embeddings for Fashion
Compatibility. In ECCV. 390â€“405.
[33] Andreas Veit, Balazs Kovacs, Sean Bell, Julian McAuley, Kavita Bala, and Serge
Belongie. 2015. Learning visual clothing style with heterogeneous dyadic cooccurrences. In ICCV. IEEE, 4642â€“4650.
[34] Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. 2019. KGAT:
Knowledge Graph Attention Network for Recommendation. In KDD.
[35] Xiang Wang, Xiangnan He, Fuli Feng, Liqiang Nie, and Tat-Seng Chua. 2018. Tem:
Tree-enhanced embedding model for explainable recommendation. In WWW.
1543â€“1552.
[36] Xiang Wang, Xiangnan He, Liqiang Nie, and Tat-Seng Chua. 2017. Item silk road:
Recommending items from information domains to social users. In SIGIR. ACM,
185â€“194.
[37] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.
Neural Graph Collaborative Filtering. In SIGIR. ACM.
[38] Qidi Xu, Fumin Shen, Li Liu, and Heng Tao Shen. 2018. GraphCAR: Contentaware Multimedia Recommendation with Graph Autoencoder. In SIGIR. ACM,
981â€“984.
[39] Xun Yang, Yunshan Ma, Lizi Liao, Meng Wang, and Tat-Seng Chua. 2019.
TransNFCM: Translation-Based Neural Fashion Compatibility Modeling. In
AAAI.
[40] Xun Yang, Meng Wang, and Dacheng Tao. 2018. Person Re-Identification With
Metric Learning Using Privileged Information. IEEE Transactions on Image Processing 27, 2 (2018), 791â€“805.
[41] Xun Yang, Peicheng Zhou, and Meng Wang. 2018. Person Reidentification via
Structural Deep Metric Learning. IEEE Transactions on Neural Networks and
Learning Systems 99 (2018), 1â€“12.
[42] Wenhui Yu, Huidi Zhang, Xiangnan He, Xu Chen, Li Xiong, and Zheng Qin. 2018.
Aesthetic-based clothing recommendation. In WWW. 649â€“658.
[43] Xishan Zhang, Jia Jia, Ke Gao, Yongdong Zhang, Dongming Zhang, Jintao Li, and
Qi Tian. 2017. Trip outfits advisor: Location-oriented clothing recommendation.
IEEE Transactions on Multimedia 19, 11 (2017), 2533â€“2544.
[44] Bo Zhao, Jiashi Feng, Xiao Wu, and Shuicheng Yan. 2017. Memory-augmented
attribute manipulation networks for interactive fashion search. In CVPR. 1520â€“
1528.
[45] Qian Zhao, Yue Shi, and Liangjie Hong. 2017. Gb-cent: Gradient boosted categorical embedding and numerical trees. In WWW. 1311â€“1319.

[1] Ziad Al-Halah, Rainer Stiefelhagen, and Kristen Grauman. 2017. Fashion Forward:
Forecasting Visual Style in Fashion. In ICCV. 388â€“397.
[2] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational
data. In NIPS. 2787â€“2795.
[3] Leo Breiman. 2017. Classification and regression trees. Routledge.
[4] Yixin Cao, Lei Hou, Juanzi Li, and Zhiyuan Liu. 2018. Neural Collective Entity
Linking. In COLING. 675â€“686.
[5] Yixin Cao, Lei Hou, Juanzi Li, Zhiyuan Liu, Chengjiang Li, Xu Chen, and Tiansi
Dong. 2018. Joint Representation Learning of Cross-lingual Words and Entities
via Attentive Distant Supervision. In EMNLP. 227â€“237.
[6] Jingyuan Chen, Hanwang Zhang, Xiangnan He, Liqiang Nie, Wei Liu, and TatSeng Chua. 2017. Attentive collaborative filtering: Multimedia recommendation
with item-and component-level attention. In SIGIR. ACM, 335â€“344.
[7] Long Chen and Yuhang He. 2018. Dress Fashionably: Learn Fashion Collocation
With Deep Mixed-Category Metric Learning. In AAAI. 2103â€“2110.
[8] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system.
In SIGKDD. ACM, 785â€“794.
[9] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al.
2016. Wide & deep learning for recommender systems. In Proceedings of the 1st
Workshop on Deep Learning for Recommender Systems. ACM, 7â€“10.
[10] Jerome H Friedman. 2001. Greedy function approximation: a gradient boosting
machine. Annals of statistics (2001), 1189â€“1232.
[11] Xiaoxiao Guo, Hui Wu, Yu Cheng, Steven Rennie, Gerald Tesauro, and Rogerio
Feris. 2018. Dialog-based interactive image retrieval. In Advances in Neural
Information Processing Systems. 678â€“688.
[12] Xintong Han, Zuxuan Wu, Phoenix X Huang, Xiao Zhang, Menglong Zhu, Yuan
Li, Yang Zhao, and Larry S Davis. 2017. Automatic spatially-aware fashion
concept discovery. In ICCV. IEEE, 1472â€“1480.
[13] Xintong Han, Zuxuan Wu, Yu-Gang Jiang, and Larry S Davis. 2017. Learning
fashion compatibility with bidirectional lstms. In ACM MM. ACM, 1078â€“1086.
[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In CVPR. 770â€“778.
[15] Ruining He and Julian McAuley. 2016. VBPR: Visual Bayesian Personalized
Ranking from Implicit Feedback.. In AAAI. AAAI Press, 144â€“150.
[16] Ruining He, Charles Packer, and Julian McAuley. 2016. Learning compatibility
across categories for heterogeneous item recommendation. In ICDM. IEEE, 937â€“
942.
[17] Xiangnan He and Tat-Seng Chua. 2017. Neural factorization machines for sparse
predictive analytics. In SIGIR. ACM, 355â€“364.
[18] Xiangnan He, Zhenkui He, Jingkuan Song, Zhenguang Liu, Yu-Gang Jiang, and
Tat-Seng Chua. 2018. NAIS: Neural Attentive Item Similarity Model for Recommendation. EEE Transactions on Knowledge and Data Engineering 30, 12 (2018),
2354â€“2366.
[19] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng
Chua. 2017. Neural collaborative filtering. In WWW. 173â€“182.
[20] R. Hong, L. Li, J. Cai, D. Tao, M. Wang, and Q. Tian. 2017. Coherent SemanticVisual Indexing for Large-Scale Image Retrieval in the Cloud. IEEE Transactions
on Image Processing 26, 9 (2017), 4128â€“4138.
[21] Wei-Lin Hsiao and Kristen Grauman. 2017. Learning the latent â€œlookâ€: Unsupervised discovery of a style-coherent embedding from fashion images. In ICCV.
[22] Wei-Lin Hsiao and Kristen Grauman. 2018. Creating capsule wardrobes from
fashion images. In CVPR. 7161â€“7170.
[23] Lizi Liao, Xiangnan He, Bo Zhao, Chong-Wah Ngo, and Tat-Seng Chua. 2018.
Interpretable Multimodal Retrieval for Fashion Products. In ACM MM. ACM,
1571â€“1579.

784

