Short Research Papers 2B: Recommendation and Evaluation

SIGIR ’19, July 21–25, 2019, Paris, France

Normalized Query Commitment Revisited
Haggai Roitman
IBM Research - Haifa
haggai@il.ibm.com
0.8

We revisit the Normalized Query Commitment (NQC) query performance prediction (QPP) method. To this end, we suggest a scaled
extension to a discriminative QPP framework and use it to analyze
NQC. Using this analysis allows us to redesign NQC and suggest
several options for improvement.

0.6

0.8

R² = 0.2934

AP@1000

0.7

R² = 0.3611

0.7
0.6

0.5

scaled

0.4
0.3

AP@1000

ABSTRACT

0.5
0.4
0.3
0.2

0.2

0.1

0.1

0

0
0

0.1

Clarity

0.2

0

0.3

0.2

0.4

Clarity

0.6

0.8

AP@1000 Histogram (Topics 351-400)

ACM Reference Format:
Haggai Roitman. 2019. Normalized Query Commitment Revisited. In Proceedings of the 42nd International ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR ’19), July 21–25, 2019, Paris,
France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331334

1

Frequency

20
15
10
5
0
0.0024143990.1092984850.2161825710.3230666560.4299507420.5368348280.643718914
Bin

More

Figure 1: Example of scaling Clarity’s values (TREC Robust
queries 351-400, γ = 0.3)

INTRODUCTION

Query performance prediction (QPP) is a core IR task whose primary goal is to assess retrieval quality in the absence of relevance
judgements [1]. In this work, we revisit Shtok et al.’s Normalized
Query Commitment (NQC) QPP method [10]. NQC is a state-ofthe-art post-retrieval QPP method [1], based on document retrieval
scores variance analysis. Nowadays, the NQC method serves as a
common competitive baseline in many QPP works [4, 6, 8, 11].
We first present the NQC method and shortly discuss the motivation behind it. We then shortly present Roitman et al.’s discriminative QPP framework [8]. Using a scaled extension to this
framework, we analyze the NQC method, “deconstructing” it into
its most basic parts. Based on this analysis, we revise NQC’s design
and suggest several options for improvement. Using an extensive
evaluation over common TREC corpora, we demonstrate that, by
redesigning NQC, where we extend it with more proper calibration
and scaling, we are able to significantly improve its prediction accuracy.

further normalized by the corpus score1 s(C), formally:
q Í
2
1
def
k d ∈D (s(d) − µ̂ D )
,
N QC(D|q) =
s(C)

(1)

Í
def
where µ̂ D = k1 d ∈D s(d) is D’s mean document score. The
key idea behind NQC is the assumption that the mean score µ̂ D
may serve as a pseudo-ineffective reference (score) point of the retrieval. The more the retrieval scores deviate from this point, the
less chance is assumed for a query-drift within D; and hence, a
more qualitative retrieval will be predicted [10]. The corpus score
s(C) further serves as a query-sensitive normalization term, allowing to compare NQC values across different queries [10].

2.2

Discriminative QPP

In this work, we build on top of Roitman et al.’s discriminative QPP
framework [8]. In [8], the authors have shown that many of the
previously suggested post-retrieval QPP methods (e.g., Clarity [2],
WIG [12] and SMV [11]) share the following basic form:
def 1 Õ
W PM(D|q) =
s(d) · ϕ F (d),
(2)
k

2 BACKGROUND
2.1 Normalized Query Commitment
Let q denote a query and let D denote the respective ranked-list
of top-k documents in corpus C with the highest retrieval scores.
Let s(d) further denote document d’s (∈ D) retrieval score with respect to q. The NQC method estimates the query’s performance according to the standard-deviation of D’s document retrieval scores,

def

where ϕ F (d) =

Î
j

f j (d)

αj

d ∈D

is a Weighted Product Model (WPM)

discriminative calibrator; with f j (d) represents some retrieval quality feature and α j ≥ 0 denotes its relative importance [8]. Within
this framework, ϕ F (d) calibrates each document d’s (∈ D) retrieval
score s(d) according to the likelihood of d being a relevant response
to query q [8]. To this end, ϕ F (d) may encode various retrieval quality properties, such as properties of q, C, D and the document d
itself. As Roitman et al. have pointed out, some of such properties
may be complementing each other (e.g., query vs. corpus quality

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’19, July 21–25, 2019, Paris, France
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00
https://doi.org/10.1145/3331184.3331334

1 Such

a score is obtained by treating the corpus as a single (large)
document.

1085

Short Research Papers 2B: Recommendation and Evaluation

SIGIR ’19, July 21–25, 2019, Paris, France

effects), and therefore, tradeoffs in the design of general QPP methods should be properly modeled [8]. ϕ F (d), therefore, models such
tradeoffs (i.e., using the weights α j ).

3.1

2.3

this end, we “break” f 2 (d) into two main parts. The first, given
by s(d) − µ̂ D , measures the difference between a given document
d’s (∈ D) score and that of the pseudo-ineffective reference (score)
point, considered by NQC as the mean document score µ̂ D . The
larger the difference is, the more will document d’s own score be
estimated as an informative evidence for a true view of its relevance (i.e., s(d)p> µ̂ D ) or irrelevance (i.e., s(d) < µ̂ D ). The second
part, given by s(d), basically serves as a scaler, allowing to measure score differences with respect to some comparable scale. In
NQC’s case, scaling is simply performed using the document score
itself.

s(d )

Scaled Weighted Product Model

While most of existing QPP methods are designed to predict a
given quality measure (with AP@1000 being the most commonly
used measure [1]), the relationship between a given predictor’s estimates and actual quality numbers may not be necessarily linear.
As an example, the bottom part of Figure 1 depicts an histogram
of the AP@1000 values obtained by a query-likelihood (QL) basedretrieval method, which was applied over TREC Robust topics 351400. As we can observe, there is a high variability in quality values,
having query difficulty non-uniformly distributed.
To address such variability during prediction, we now suggest
a simple, yet effective extension to [8], which scales the calibratedmean estimator defined in Eq. 2, as follows:
def

SW PM(D|q) =

!γ
1 Õ
s(d) · ϕ F (d) ,
k

3.2

(3)

where sneд represents some pseudo-ineffective reference (score)
point for comparison, and Zd denotes some scaler value. Therefore,
for the original NQC method we have the configuration of sneд =
µ̂ D and Zd = s(d).
We next suggest several other configurations, based on alternative sneд and Zd instantiations. Starting with Zd , as a first alterna-

where γ ≥ 0 is a scaling parameter. Note that, whenever γ < 1,
higher variability in prediction values is encouraged. Going back to
Figure 1, its upper part further illustrates the relationship between
Clarity’s [2] predicted values and actual quality numbers before
and after applying scaling (γ = 0.3). As we can observe, after scaling, Clarity’s prediction accuracy has significantly improved.

def

tive, we suggest smax = maxd ∈D s(d). As a second alternative,
motivated by Ozdemiray et al.’s ScoreRatio QPP method [6], we

NQC REVISITED

def

also suggest smin = mind ∈D s(d). As a third alternative, we sug-

We now show that, the NQC method can be derived as a scaled
calibrated-mean estimator. To this end, we first rewrite NQC (defined in Eq. 1), as follows:
! 2  0.5
 Õ


1
1 2 s(d) − µ̂ D 

N QC(D|q) = 
s(d)
p

s(C)
s(d)

 k d ∈D


def

Redesigning f 2 (d)

We now generalize the basic form of f 2 (d), and reformulate it as
follows:
def s(d) − sneд
,
(5)
f 2 (d) =
p
Zd

d ∈D

3

Analysis of f 2 (d)

Using the new NQC formulation, we now try to explain the mos(d )− µ̂
tivation behind its second calibration feature f 2 (d) = √ D . To

def

gest a range-sensitive scaler: smax − smin + ϵ, where ϵ = 10−10
is used to avoid dividing by zero. Noting that both smax and smin
may actually represent outlier values, as a fourth and final alternative, which tries to reduce such outliers, we suggest the following
inter-percentile range scaler Q .95 − Q .05 + ϵ. Here, Q .x represents
the value of the x% percentile of D’s observed score distribution.
Next, we suggest several alternative estimates of sneд . As a first
alternative, we suggest smin , which represents the document with
the lowest relevance score, and hence, presumably the least effective sample reference. As a second alternative, following [4] we
consider sneд to be the mean score of the documents at the lower

(4)

Using Eq. 4, it now becomes apparent that, NQC may be treated
as an instance of the generic scaled estimator defined in Eq. 3. In
1 and α = 2, f (d) = s(d
√)− µ̂ D
NQC’s case, we have: f 1 (d) = s(C)
1
2
s(d )

and α 2 = 2, and a scaling parameter γ = 0.5.
Reformulating the NQC method as a scaled calibrated-mean estimator has two main advantages. First, being represented as a discriminative QPP method, we can potentially improve its prediction
accuracy by applying supervised-learning to better tune its calibration (i.e., α 1 and α 2 ) and scaling (i.e., γ ) parameters. As we shall
later demonstrate, such a more fine-tuned calibration may indeed
result in a better prediction quality.
Second, and more important, similar to [8], such a representation allows to analyze the existing design of NQC’s calibration features (i.e., f 1 (d) and f 2 (d)), potentially even redesigning some of
them. Among these two features, f 1 (d) was previously examined
in [8], where it was shown to be utilized as a calibration feature
within the SMV method [11]. We, therefore, next focus our attention on the second calibration feature of f 2 (d).

def

ranks of D. Formally, for a given l ≤ k, we estimate µ̂ D neд =
1 Ík
k−l i=l +1 s(di ), where di denotes the document that is ranked at
position i in D.
Our third and final alternative is motivated by Diaz’s Autocorrelation QPP method [3]. Using the opposite of the logic2 that
was suggested in [3], which is based on the Cluster-Hypothesis in
IR [5], we expect an informative document score s(d) to be quite
different from those scores of documents that are the most dissimilar to d. Formally, let dist(d, d ′ ) denote the distance between two
documents in D and let KF N (d) be the set of K-farthest neighbors
2 The Autocorrelation method treats similar documents as pseudo-effective
reference points, and hence, the score of a document is actually supposed
to be similar to the scores of its closest neighbors.

1086

Short Research Papers 2B: Recommendation and Evaluation

SIGIR ’19, July 21–25, 2019, Paris, France

of d in D according to dist(d, d ′ ). We then define sneд as the mean
score of documents in KF N (d).

further evaluated a combined predictor (SC-NQC), where all the
three parameters were tuned. Finally, we evaluated a pre-tuned SCNQC employed with the best f 2 (d) configuration learned for the
non-calibrated/non-scaled NQC versions, denoted SC-NQC(best).

4 EVALUATION
4.1 Experimental setup
4.1.1 Datasets. For our evaluation we have used the following TREC corpora and topics: TREC4 (201-250), TREC5 (251-300),
AP (51-150), WSJ (151-200), ROBUST (301-450, 601-700), WT10g
(451-550) and GOV2 (701-850). These datasets were used by many
previous QPP works [1–3, 7–12]. Titles of TREC topics were used
as queries, except for TREC4, where we used topic descriptions instead [8]. We used the Apache Lucene3 open source search library
for indexing and searching documents. Documents and queries
were processed using Lucene’s English text analysis (i.e., tokenization, lowercasing, Porter stemming and stopwords). For retrieval,
we used Lucene’s language-model-based cross-entropy scoring with
Dirichlet smoothed document language models, where the smoothing parameter was set to 1000 [8, 10].
4.1.2 Baseline predictors. We compared the original NQC method
(see again Eq. 1) and its proposed extensions to several different
baseline QPP methods. As a first line of baselines, we compared
with the following state-of-the-art post-retrieval QPP methods:
• Clarity [2]: estimates query performance relatively to the divergence between the relevance model induced from a given (retrieved) list and the corpus background model.
• WIG [12]: estimates query performance according to the difference between the average document retrieval score in a given list
and that of the corpus s(C).
• WEG [4]: a WIG-like alternative that uses µ̂ D neд as the pseudoineffective reference point instead of s(C).
• Autocorrelation [3]: based on the Cluster-Hypothesis in IR, it
estimates query performance according to the Pearson’s-r correlation between D’s original document scores and those estimated
by interpolating the scores of each document d’s (∈ D) K-nearest
neighbors relatively to that document’s similarity with each neighbor.
• ScoreRatio [6]: simply estimates query performance according
to the ratio ssmax
.
min
• SMV [11]: is a direct alternative to NQC that also considers score
magnitude and variance, estimated as:
Õ
def
1
s(d)
SMV (D|q) =
s(d) ln
.
k · s(C)
µ̂ D
d ∈D

4.1.3 Setup. On each setting, we predicted the performance of
each query with respect to its top-1000 retrieved documents [1].
We assessed prediction over queries quality according to the correlation between the predictor’s values and the actual average precision (AP@1000) values calculated using TREC’s relevance judgments [1]. To this end, we report the Pearson’s-r (P-r ) and Kendall’sτ (K-τ ) correlations, which are the most commonly used measures [1].
Most of the methods that we have evaluated required to tune
some free parameters. First, following the common practice [1],
for each QPP method, we tuned k – the number of documents used
for prediction4 ; with k ∈ {5, 10, 20, 50, 100, 150, 200, 500, 1000}. For
Clarity, we induced a relevance model using the top-m (∈ {5, 10, 20,
50, 100}) documents in D and further applied clipping at the top-n
terms cutoff, with n ∈ {10, 20, 50, 100}. For the Autocorrelation
and NQC(KF N (K), ·) baselines, we tuned K ∈ {3, 5, 10, 20, 30}, further using the Bhattacharyya distance between the unsmoothed
language models of the documents in D as the (dis)similarity measure dist(·) of choice. To realize µ̂ D neд in SMV and NQC, we further tuned l ∈ {5, 10, 20, 50, 100, 200, 500}.
To learn the calibration feature weights (i.e., α 1 and α 2 ) and scaling parameter (i.e., γ ) of the NQC variants, following [7–9], we
used a Coordinate Ascent approach. To this end, we selected the
feature weights over the grid of [0, 5] × [0, 5] × [0, 1], in steps of 0.1
per dimension. Following [7–10], we trained and tested all methods using a holdout (2-fold cross validation) approach. On each
dataset, we generated 30 random splits of the query set; each split
had two folds. We used the first fold as the (query) train set. We
kept the second fold for testing. We recorded the average prediction quality over the 30 splits. Finally, we measured statistical significant differences of prediction quality using a two-tailed paired
t-test with p < 0.05 computed over all 30 splits.

4.2

Results

We report our evaluation results in Table 1. As a “stand-alone”
QPP method, even the original NQC method (i.e., NQC(µ̂ D ,s(d)))
already provides highly competitive prediction quality results compared to the other state-of-the-art QPP methods. We, therefore,
next evaluate the impact of our proposed NQC extensions.

Using our derivation of NQC as a scaled calibrated QPP method
(see Section 3), we further evaluated various alternatives of NQC,
as follows. We first evaluated various f 2 (d) configurations within
NQC, i.e., NQC(sneд ,Zd ). To this end, we instantiated sneд and Zd
according to our proposed alternatives.
Next, using the original NQC configuration (i.e., sneд = µˆD and
Zd = s(d)), we also evaluated a calibration-only version of NQC (CNQC), where we only tuned its α 1 and α 2 parameters, while still
fixing the scaling parameter to γ = 0.5. In a similar manner, we
evaluated a scaled-only version of NQC (S-NQC), where we fixed
α 1 = 2 and α 2 = 2 and only tuned the scaling parameter γ . We

4.2.1 Evaluation of various f 2 (d) configurations. We start with
a qualitative examination of the relative contribution of each of
the two parts of f 2 (d) (i.e., sneд and Zd ). To this end, we count the
relative number of cases, per dataset and quality measure (i.e., P-r
or K-τ ), in which using a specific configuration part has resulted
in a better prediction accuracy than that of the original NQC’s configuration. Overall, in 136 out of the 14 × 19 = 266 possible cases,
utilizing one of the alternative configurations has resulted in a better prediction.
Among the three alternative sneд options, the relative preference was: KF N (d) (45/70), µ̂ D neд (38/70) and s min (29/70). This

3 http://lucene.apache.org

4 All

1087

NQC variants were tuned with the same value of k .

Short Research Papers 2B: Recommendation and Evaluation

SIGIR ’19, July 21–25, 2019, Paris, France

Table 1: Evaluation results. “greenish” and “reddish” colored values represent an improvement or a decline in prediction accuracy compared
to the original NQC method. A statistical significant difference between a given NQC variant and the original NQC are marked with ∗ (p < .05).

Autocorrelation
ScoreRatio
Clarity
WIG
WEG
SMV
NQC( µ̂ D ,s(d)) (Shtok et al. [10])
NQC( µ̂ D ,max)
NQC( µ̂ D ,min)
NQC( µ̂ D ,max − min)
NQC( µ̂ D ,Q .95 − Q .05)
NQC( µ̂ D neд ,s(d))
NQC( µ̂ D neд ,max)
NQC( µ̂ D neд ,min)
NQC( µ̂ D neд ,max − min)
NQC( µ̂ D neд ,Q .95 − Q .05)
NQC(min,s(d))
NQC(min,max)
NQC(min,min)
NQC(min,max − min)
NQC(min,Q .95 − Q .05)
NQC(K F N (d),s(d))
NQC(K F N (d),max)
NQC(K F N (d),min)
NQC(K F N (d),max − min)
NQC(K F N (d),Q .95-Q .05)
C-NQC( µ̂ D ,s(d))
S-NQC( µ̂ D ,s(d))
SC-NQC( µ̂ D ,s(d))
SC-NQC(best)

TREC4
P-r
K-τ
.456
.366
.420
.444
.401
.357
.533
.502
.349
.454
.458
.386
.491
.383
.484
.371
.486
.403
.436*
.325*
.451*
.335*
.482
.393
.474*
.381
.483
.412*
.415*
.320*
.443*
.344*
.466*
.393
.459*
.374
.484
.425*
.380*
.311*
.407*
.338*
.475*
.422*
.470*
.418*
.488
.434*
.415*
.349
.448*
.371
.492
.412*
.496
.383
.504*
.412*
.510*
.457*

TREC5
P-r
K-τ
.188
.136
.344
.204
.314
.208
.347
.252
.296
.201
.414
.332
.454
.275
.455
.277
.468
.265
.417*
.301*
.406*
.297*
.503*
.304*
.495*
.314
.518*
.306*
.427*
.299*
.438*
.336*
.506*
.312*
.498*
.312*
.522*
.317*
.430*
.304*
.442*
.330*
.524*
.297*
.519*
.306*
.530*
.286*
.474*
.317*
.479*
.330*
.455
.310*
.473*
.275
.473*
.310*
.560*
.317*

AP
P-r
.348
.231
.413
.613
.499
.536
.619
.628
.592*
.645*
.643*
.625*
.632*
.615
.635*
.678*
.602*
.617
.591*
.630*
.674*
.638*
.643*
.635*
.646*
.688*
.622*
.620
.623*
.691*

K-τ
.226
.166
.265
.417
.357
.379
.405
.413
.390*
.414*
.386*
.423*
.422*
.407
.409
.406
.423*
.430*
.424*
.412*
.412*
.445*
.449*
.428*
.452*
.453*
.412*
.405
.412*
.454*

5

demonstrates that, a better choice of a pseudo-ineffective reference point within NQC should be one that is more sensitive to the
document in mind d (i.e., KF N (d)). This in comparison to a point
that is more generally estimated (i.e., µ̂ D neд and s min ). Moreover,
among the latter two alternatives, considering more than one sample from the lower ranks of D is a better choice. In a similar manner,
among the four alternative Zd options, the relative preference was:
s max (34/56), s min (29/56), s max − s min (25/56) and lastly Q .95 − Q .05
(23/56). This further demonstrates that, a better choice for a scaler
is one that depends on a single point (having s max a better choice
than s min ), rather than a range of values. Finally, quantitatively,
by more proper configuration of f 2 (d), NQC’s prediction accuracy
has improved in all datasets, up to 38% and 46% improvement in
P-r and K-τ , respectively. This demonstrates the usefulness of analyzing NQC using our scaled extension to Roitman et al.’s discriminative QPP framework [8].

WSJ
P-r
K-τ
.586
.496
.620
.411
.500
.355
.685
.463
.696
.482
.713
.484
.722
.510
.723
.494
.689*
.486*
.706*
.460*
.715
.496
.729*
.504
.734*
.513
.705*
.499*
.734*
.496*
.736*
.502*
.715
.496
.723
.494
.714
.500
.706*
.460*
.715
.496
.730*
.561*
.739*
.561*
.691*
.523*
.760*
.558*
.744*
.582*
.737*
.510
.728*
.502*
.737*
.510
.770*
.567*

ROBUST
P-r
K-τ
.385
.321
.405
.370
.404
.329
.560
.399
.562
.400
.534
.391
.580
.422
.582
.424
.540
.425
.585*
.405*
.440*
.409*
.567*
.412*
.568*
.409*
.541*
.420*
.551*
.377*
.465*
.394*
.538*
.393*
.536*
.388*
.529*
.408*
.493*
.329*
.457*
.353*
.580
.413*
.583
.414
.556*
.422
.572*
.394*
.504*
.403*
.580
.412
.580
.422
.580
.422
.585*
.405*

WT10g
P-r
K-τ
.299
.198
.226
.275
.289
.203
.221
.323
.462
.383
.466
.279
.522
.330
.518
.324
.538*
.337*
.407*
.237*
.384*
.274*
.523
.333
.516
.325
.539*
.334*
.399*
.233*
.383*
.273*
.406*
.277*
.391*
.265*
.429*
.291*
.245*
.176*
.255*
.205*
.506*
.333*
.500*
.324*
.528*
.351*
.385*
.253*
.380*
.272*
.559*
.377*
.522
.330
.559*
.377*
.561*
.380*

GOV2
P-r
K-τ
.247
.198
.295
.246
.206
.164
.498
.352
.419
.337
.419
.310
.378
.253
.386*
.256
.350*
.240*
.460*
.320*
.421*
.294*
.416*
.292*
.427*
.300*
.384*
.280*
.507*
.355*
.504*
.363*
.445*
.306*
.456*
.318*
.412*
.296*
.528*
.358*
.532*
.367*
.432*
.327*
.443*
.327*
.400*
.310*
.515*
.374*
.509*
.373*
.507*
.360*
.398*
.253
.527*
.360*
.566*
.401*

CONCLUSIONS

We introduced a simple, yet highly effective, extension to Roitman
et al.’s discriminative QPP framework [8]. Our main focus was on
the NQC method, where using our proposed extension, we were
able to redesign it and suggest several options for improvement.

REFERENCES
[1] David Carmel and Oren Kurland. Query performance prediction for ir. In Proceedings of SIGIR ’12.
[2] Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft. Predicting query performance. In Proceedings of SIGIR ’02.
[3] Fernando Diaz. Performance prediction using spatial autocorrelation. In Proceedings of SIGIR ’07.
[4] Ahmad Khwileh, Andy Way, and Gareth J. F. Jones. Improving the reliability of
query expansion for user-generated speech retrieval using query performance
prediction. In CLEF, 2017.
[5] Oren Kurland. The cluster hypothesis in information retrieval. In Advances in
Information Retrieval, pages 823–826. Springer International Publishing, 2014.
[6] A. M. Ozdemiray and Ismail S. Altingovde. Query performance prediction for
aspect weighting in search result diversification. Proceedings of CIKM ’14.
[7] Haggai Roitman. An extended query performance prediction framework utilizing passage-level information. In Proceedings of ICTIR, pages 35–42, New York,
NY, USA, 2018. ACM.
[8] Haggai Roitman, Shai Erera, Oren Sar-Shalom, and Bar Weiner. Enhanced mean
retrieval score estimation for query performance prediction. In Proceedings of
ICTIR ’17.
[9] Haggai Roitman, Shai Erera, and Bar Weiner. Robust standard deviation estimation for query performance prediction. In Proceedings of ICTIR ’17.
[10] Anna Shtok, Oren Kurland, David Carmel, Fiana Raiber, and Gad Markovits.
Predicting query performance by query-drift estimation. ACM Trans. Inf. Syst.,
30(2):11:1–11:35, May 2012.
[11] Yongquan Tao and Shengli Wu. Query performance prediction by considering
score magnitude and variance together. In Proceedings of CIKM ’14.
[12] Yun Zhou and W. Bruce Croft. Query performance prediction in web search
environments. In Proceedings of SIGIR ’07.

4.2.2 Effect of calibration and scaling. The four bottom rows
in Table 1 further report the effect of NQC’s calibration and scaling. First, we observe that, by better tuning of NQC’s calibration
features within C-NQC(µ̂ D ,s(d)), its prediction accuracy has improved in most cases (up to 42% better). Next, scaling (i.e.,
S-NQC(µ̂ D ,s(d))) by itself also improves NQC’s accuracy (up to 5%).
Combining both calibration and scaling (i.e., SC-NQC(µ̂ D ,s(d))),
has resulted in most cases in a further improved accuracy (up to
42%). Finally, further using the best f 2 (d) configuration together
with calibration and scaling (i.e., SC-NQC(best)) provides the best
prediction strategy for NQC (up to 60% improvement).

1088

