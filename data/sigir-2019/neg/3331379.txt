Tutorial

SIGIR ’19, July 21–25, 2019, Paris, France

Building Economic Models and Measures of Search
Leif Azzopardi

Alistair Moffat

University of Strathclyde
Glasgow, UK
leifos@acm.org

The University of Melbourne
Melbourne, Australia
ammoffat@unimelb.edu.au

Paul Thomas

Guido Zuccon

Microsoft
Canberra, Australia
pathom@microsoft.com

The University of Queensland
Brisbane, Australia
g.zuccon@uq.edu.au
to provide: (i) the underpinnings of retrieval models (e.g., the probability ranking principle [19], etc.); (ii) the development of new
retrieval models (e.g., portfolio theory [23], facilities location analysis [27], etc.); (iii) the measurement of ranked lists (e.g., discounted
cumulative gain [11], expected utility [12], etc.); and (iv) the modelling of user behaviors (e.g., information foraging theory [17],
production theory [1], etc.). By exploring how economic theory
has permeated the field of IR, the goal of this tutorial is to provide
the audience with an overview of the key developments, showing
how the underlying economics concepts can be applied to: (1) build
models of search behavior; and (2) develop metrics to search performance. By using economic models it is possible to reason about
interaction, make predictions about how changes to the system affect behavior, and measure the performance of people’s interactions
with the system.

ABSTRACT
Economics provides an intuitive and natural way to formally represent the costs and benefits of interacting with applications, interfaces and devices. By using economic models it is possible to reason
about interaction, make predictions about how changes to the system will affect behavior, and measure the performance of people’s
interactions with the system. In this tutorial, we first provide an
overview of relevant economic theories, before showing how they
can be applied to formulate different ranking principles to provide
the optimal ranking to users. This is followed by a session showing
how economics can be used to model how people interact with
search systems, and how to use these models to generate hypotheses
about user behavior. The third session focuses on how economics has been used to underpin the measurement of information
retrieval systems and applications using the C/W/L framework
(which reports the expected utility, expected total utility, expected
total cost, and so on) – and how different models of user interaction
lead to different metrics. We then show how information foraging
theory can be used to measure the performance of an information
retrieval system – connecting the theory of how people search
with how we measure it. The final session of the day will be spent
building economic models and measures of search. Here sample
problems will be provided to challenge participants, or participants
can bring their own.

2

OBJECTIVES

From this tutorial a student should learn how to:
Describe how economics has influenced developments in IR;
Compare and contrast the different ranking principles;
Describe different models of user behavior;
Create an economic model of search;
Explain the C/W/L framework and the different measurements
it incorporates;
• Design a metric given the C/W/L framework; and
• Infer, hypothesize and predict user behaviors and performance.
•
•
•
•
•

ACM Reference Format:
Leif Azzopardi, Alistair Moffat, Paul Thomas, and Guido Zuccon. 2019.
Building Economic Models and Measures of Search. In Proceedings of the
42nd International ACM SIGIR Conference on Research and Development in
Information Retrieval (SIGIR ’19), July 21–25, 2019, Paris, France. ACM, New
York, NY, USA, 2 pages. https://doi.org/10.1145/3331184.3331379

3
3.1

1

COURSE FORMAT

The full day tutorial at SIGIR 2019 consists of four sessions.

MOTIVATION

Session 1 – Economics in IR

The first session focuses on providing a grounding in economics
and optimization models [10, 16]. It explains why we need such
models, and how they can be used to gain insights about search
performance and search behavior. We then provide a high level
overview of how different economics concepts and theories have
been utilized in IR, in terms of: (1) ranking documents (e.g., ranking
principles); (2) modelling user behavior (e.g., user models); and (3)
measuring performance (e.g., metrics and measurements).
The remainder of the session takes a deeper dive into the economics behind ranking principles [9, 19, 25, 26]. Starting from the
Probability Ranking Principle (PRP), we show how this decisiontheoretic approach ensures that documents are ranked optimally

Economics provides an intuitive and natural way to formally represent the costs and benefits of interaction. Over the years, various
economic concepts have been used with Information Retrieval (IR)
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
SIGIR ’19, July 21–25, 2019, Paris, France
© 2019 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00
https://doi.org/10.1145/3331184.3331379

1401

Tutorial

SIGIR ’19, July 21–25, 2019, Paris, France

people interact with the system, or to evaluate how well people
perform using the system.

(given a number of assumptions). We then explain how the PRP’s
assumptions about user behavior can be relaxed and how it can be
extended to be more realistic, by detailing the Interactive Probability Ranking Principle [9] and the Generalized PRP, that is, the Card
Model [24, 25]. In summary, this session provides the foundations
for understanding how economics has guided and underpinned the
development of ranking models.

3.2

REFERENCES
[1] Leif Azzopardi. The economics in interactive information retrieval. In Proc. of
the 34th International ACM SIGIR Conference, pages 15–24, 2011.
[2] Leif Azzopardi. Modelling interaction with economic models of search. In Proc.
of the 37th International ACM SIGIR Conference, pages 3–12, 2014.
[3] Leif Azzopardi and Guido Zuccon. An analysis of the cost and benefit of search
interactions. In Proc. of the 2016 ACM International Conference on the Theory of
Information Retrieval, ICTIR ’16, pages 59–68, 2016.
[4] Leif Azzopardi and Guido Zuccon. Two scrolls or one click: A cost model for
browsing search results. In Proc. of the 38th European Conference on IR Research,
pages 696–702, 2016.
[5] Leif Azzopardi, Paul Thomas, and Nick Craswell. Measuring the utility of search
engine result pages: An information foraging based measure. In Proc. of the 41st
International ACM SIGIR Conference, SIGIR ’18, pages 605–614, 2018.
[6] Leif Azzopardi, Paul Thomas, and Alistair Moffat. cwl_eval: An evaluation tool
for information retrieval. In Proc. of the 42nd International ACM SIGIR Conference,
SIGIR ’19, 2019.
[7] Eric L. Charnov. Optimal foraging, the marginal value theorem. Theoretical
Population Biology, 9(2):129–136, 1976.
[8] Michael D. Cooper. A cost model for evaluating information retrieval systems.
Journal of the American Society for Information Science, pages 306–312, 1972.
[9] Norbert Fuhr. A probability ranking principle for interactive information retrieval.
Information Retrieval, 11(3):251–265, 2008.
[10] Frederick S. Hillier and Gerald J. Lieberman. Introduction to Operations Research.
McGraw-Hill, NY, USA, 2001.
[11] Kalervo Järvelin and Jaana Kekäläinen. Cumulated gain-based evaluation of IR
techniques. ACM Trans. Inf. Syst., 20(4):422–446, October 2002.
[12] Alistair Moffat and Justin Zobel. Rank-biased precision for measurement of
retrieval effectiveness. ACM Trans. Inf. Syst., 27(1):2:1–2:27, 2008.
[13] Alistair Moffat, Paul Thomas, and Falk Scholer. Users versus models: What
observation tells us about effectiveness metrics. In Proc. of the ACM International
Conference on Information and Knowledge Management, pages 659–668, 2013.
[14] Alistair Moffat, Peter Bailey, Falk Scholer, and Paul Thomas. INST: An adaptive
metric for information retrieval evaluation. In Proc. of Australasian Document
Computing Symposium, pages 5:1–5:4, 2015.
[15] Alistair Moffat, Peter Bailey, Falk Scholer, and Paul Thomas. Incorporating user
expectations and behavior into the measurement of search effectiveness. ACM
Trans. Inf. Syst., 35(3):24:1–24:38, 2017.
[16] Katta G. Murty. Optimization Models For Decision Making. University of Michigan,
Ann Arbor, 2003.
[17] Peter Pirolli and Stuart Card. Information foraging. Psychological Review, 106:
643–675, 1999.
[18] Howard L. Resnikoff. The Illusion of Reality. Springer-Verlag New York, 1989.
[19] Stephen E. Robertson. The probability ranking principle in IR. Journal of Documentation, 33(4):294–304, 1977.
[20] Daniel M. Russell, Mark J. Stefik, Peter Pirolli, and Stuart K. Card. The cost
structure of sensemaking. In Proc. of the INTERACT and SIGCHI, pages 269–276,
1993.
[21] Pamela Effrein Sandstrom. An optimal foraging approach to information seeking
and use. The Library Quarterly, pages 414–449, 1994.
[22] D. W. Stephens and J. R. Krebs. Foraging Theory. Princeton University Press,
1986.
[23] Jun Wang and Jianhan Zhu. Portfolio theory of information retrieval. In Proc. of
the 32nd International ACM SIGIR Conference, pages 115–122. ACM, 2009.
[24] ChengXiang Zhai. Towards a game-theoretic framework for information retrieval.
In Proc. of the 38th International ACM SIGIR Conference, pages 543–543, 2015.
[25] Yinan Zhang and ChengXiang Zhai. Information retrieval as card playing: A
formal model for optimizing interactive retrieval interface. In Proc. of the 38th
International ACM SIGIR Conference, SIGIR ’15, pages 685–694, 2015.
[26] Guido Zuccon, Leif A Azzopardi, and Keith Rijsbergen. The quantum probability
ranking principle for information retrieval. In Proc. of ICTIR, pages 232–240,
2009.
[27] Guido Zuccon, Leif Azzopardi, Dell Zhang, and Jun Wang. Top-k retrieval using
facility location analysis. In Proc. of the 34th European Conference on IR Research,
pages 305–316. 2012.

Session 2 – Economic Models of Search

In session two, we change focus, and examine models that are userfocused. That is, we provide an overview of economic models that
aim to explain and predict user search behavior [1–3, 17]. To this
end, we describe Information Foraging Theory [17, 18, 20, 21] (IFT),
which applies economics to how people find and forage for information (based on Optimal Foraging Theory [22] from ecology). We
explain how IFT can more generally be used to model how people
search and forage for information by trying to maximize their rate
of gain over time (e.g., Charnov’s Marginal Value Theorem [7]).
While IFT has been used to motivate experimentation at the high
level, we explain more concretely how IFT can be used to derive
testable hypotheses about how people interact with an information
system, and how their behavior adapts depending on the circumstances. We then continue working through different economic
models that have been developed to represent different interfaces,
features and scenarios. This session puts the theory into practice
showing how different costs and benefits impact usage, and performance in a number of contexts [2–4, 8]). By using economic models
it is possible to draw actionable insights regarding how to improve
the system and to identify and understand the trade-offs between
interactions (to explain when and why one option is preferred or
used over another).

3.3

Session 3 – Economic Measures of Search

In the third session, we examine how economics can be used to
evaluate the user-system interactions. Most existing metrics essentially report the expected utility (also called the rate of gain)
given the ranked list and model of user behavior. Here we link
the models of search behavior with how we measure search performance by describing and explaining the C/W/L framework,
which utilizes economics theory to provide the theoretical basis
for measuring Expected Utility [13, 15], as well as a series of other
related economic measures [5]. We then take a deeper dive into
the C/W/L framework, and explain how different metrics, encode
different user models (e.g., P@k, DCG, RBP, and INST [5, 13–15]) –
and as such how to design and build your own metric [6]. Finally,
we conclude the session by showing how models of search behavior
(i.e., IFT) can be used to define theoretically underpinned metrics
within the C/W/L framework – connecting “how we model search”
with “how we measure search”.

3.4

Session 4 – Practical Session

The final session of the tutorial is dedicated to a hands-on practical session where we challenge participants to apply economics
modelling techniques to different scenarios – where they will need
to model user behavior in order to hypothesize in regard to how

1402

