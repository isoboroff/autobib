Session 6A: Social Media

SIGIR ’19, July 21–25, 2019, Paris, France

Online User Representation Learning
Across Heterogeneous Social Networks
Weiqing Wang

Hongzhi Yin∗

Xingzhong Du

teresa.wang@monash.edu
Monash University

h.yin1@uq.edu.au
The University of Queensland

xingzhong.du@coles.com.au
Coles Advanced Analytics Centre

Wen Hua

Yongjun Li

Quoc Viet Hung Nguyen

w.hua@uq.edu.au
The University of Queensland

lyj@nwpt.edu.cn
Northwestern Polytechnical
University

quocviethung1@gmail.com
Griffith University

ABSTRACT

KEYWORDS

Accurate user representation learning has been proven fundamental
for many social media applications, including community detection,
recommendation, etc. A major challenge lies in that, the available
data in a single social network are usually very limited and sparse.
In real life, many people are members of several social networks in
the same time. Constrained by the features and design of each, any
single social platform offers only a partial view of a user from a
particular perspective. In this paper, we propose MV-URL, a multiview user representation learning model to enhance user modeling
by integrating the knowledge from various networks. Different
from the traditional network embedding frameworks where either
the whole framework is single-network based or each network
involved is a homogeneous network, we focus on multiple social
networks and each network in our task is a heterogeneous network. It’s very challenging to effectively fuse knowledge in this
setting as the fusion depends upon not only the varying relatedness of information sources, but also the target application tasks.
MV-URL focuses on two tasks: user account linkage (i.e., to predict
the missing true user account linkage across social media) and user
attribute prediction. Extensive evaluations have been conducted
on two real-world collections of linked social networks, and the experimental results show the superiority of MV-URL compared with
existing state-of-art embedding methods. It can be learned online,
and is trivially parallelizable. These qualities make it suitable for
real world applications.

user modelling, social networks, multiview, representation learning,
heterogeneous networks
ACM Reference Format:
Weiqing Wang, Hongzhi Yin, Xingzhong Du, Wen Hua, Yongjun Li, and Quoc
Viet Hung Nguyen. 2019. Online User Representation Learning Across Heterogeneous Social Networks. In Proceedings of the 42nd International ACM
SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’19), July 21–25, 2019, Paris, France. ACM, New York, NY, USA, 10 pages.
https://doi.org/10.1145/3331184.3331258

1

CCS CONCEPTS
• Information systems → Collaborative and social computing systems and tools; • Human-centered computing → Collaborative and social computing; • Applied computing →
Computer forensics.
∗ This

INTRODUCTION

The blossom of various social services has boosted many business
intelligence by leveraging the big social data. In particular, people
wonder how to gain a deeper and better understanding of each
individual user from the vast amount of social data out there.
Most existing work focus on modeling users with their behavior
data from a single social platform [13, 15, 32, 34]. Unfortunately,
information of a user from a single platform are usually very limited and sparse (e.g., the sparsity of user-item behavior data on a
single social network is usually higher than 99.9% [41]). The key
to unleashing the true power of social media analysis is to link
up all the data of the same user across different social platforms
[17, 36, 41], which is an important topic covered in the broad learning task [37, 39, 42]. The benefit of integrating the information from
multiple social platforms for user profiling are three-fold [17]. The
first benefit is to complete users’ information by offering different
views of a user. Constrained by the features and design of a single
platform, it can only offer a partial view of a user from a particular
perspective. For example, many people use Facebook to communicate with their acquaintances, but post their real-time information
on Twitter. The second advantage lies in that multiple networks
can help in correcting the possible false or conflicting information
provided by users on a single social platform. The last well-being
is to integrate useful user information from those social platforms
that have become less popular or even abandoned over time.
Data across multiple social networks can be very difficult to deal
with due to their complex structures (containing heterogeneous
links and nodes). Traditional machine learning algorithms face
great challenges in handling such network data as they usually
take feature vectors as the input. Thus, a general representation of
users by fusing information from heterogeneous networks as feature vectors is desired for many user-related knowledge discovery
on such complex-structured data, including community detection,

is the Corresponding Author

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’19, July 21–25, 2019, Paris, France
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00
https://doi.org/10.1145/3331184.3331258

545

Session 6A: Social Media

SIGIR ’19, July 21–25, 2019, Paris, France

SIGIR ’19, July 21–25, 2019, Paris, France

Wang and Yin, et al.

recommendation, etc. Representing users with low dimension feature vectors is also known as user embedding. In this paper, we
focus on effective user embedding by jointly learning users’ related
information across multiple social platforms.
Many works have been proposed to embed the online social
network data into a low-dimensional feature space. However, most
existing social network embedding methods are proposed for homogeneous networks [11, 22, 30], which learn the feature vectors
for user nodes merely based on the social connections among them.
Recently, several research work have been proposed to embed the
heterogeneous networks [1, 4, 33]. But different from our research
task which focuses on multiple networks, they are single-network
based. Technically, the research task in this paper is actually to
embed users with “multiple heterogeneous networks”. More specifically, we study multiple networks and every network involved is a
heterogeneous network containing different links (e.g., User-User,
User-Item, etc) and diverse nodes.
Qu et al. propose a multi-view network representation learning
in [25], which integrates the information from multiple types of
relationships between the same group of nodes. The structure of
the studied network by Qu et al. is actually multiple homogeneous
networks. Specifically, these networks are coupled by the same
group of nodes, and the links and nodes inside each network are
homogeneous. Thus, the research task in [25] is different from our
task in this paper. The research work that is most related to our
focused network structure is DIME (i.e., Deep alLgned autoencoder
based eMbEdding), proposed in [39]. DIME is designed for multiple
heterogeneous social networks. However, the aim of DIME is to
transfer the user proximity information from a dense network to a
sparse network, and a set of meta paths need to be predefined to
represent the general correlation among users.
In this paper, we propose a general user representation learning framework to jointly and automatically learn and fuse users’
related information across multiple heterogeneous social networks.
It’s very challenging to construct such a framework to automatically and effectively integrate relevant knowledge across various
platforms as this integration depends upon not only the different
relatedness of these information sources, but also the target application problems. In this paper, we focus on two application tasks:
user account linkage and user attribute prediction. Most existing
work on user account linkage [5, 17, 19] focus on pair-wise user
linkage between two platforms and most existing work on users’
attributes prediction [2, 23, 24] only leverage the data from single
social platforms. Our proposed framework is able to achieve these
two tasks with the information from multiple social platforms.
To model the various relatedness of these information sources,
we design different schemes in integrating sources of information.
We assume that all the social networks share the same semantic
space by restraining that a content word has the same latent representations across different social networks. For the coupled graphs
inside of one single social network, we assume that they have higher
homogeneity compared with the graphs across social networks.
Thus, we introduce different schemes to model the heterogeneity inside of one single social platform and across different social
platforms. Specifically, to reconcile the heterogeneities of different
latent spaces on single networks, we use a harmonious embedding
matrix [4] to further embed the embeddings from one latent space to

another latent space. On the other hand, a co-regularization scheme,
which has been widely used in multi-view learning [9, 25, 27], is
introduced to promote the collaboration of different networks.
As we need to learn a new harmonious embedding matrix for
each extra coupled graph on one single social platform, which is really expensive and easy to lead to over fitting when training data is
sparse in the real world, MV-URL constrains the harmonious matrix
to be diagonal, to reduce the number of parameters to be learned.
Different sampling methods are applied in MV-URL. Specifically, hierarchical sampling method is adopted to sample positive instances
and bi-direction strategy is used to sample negative instances for
each positive instance. A two-step optimization method (i.e., alternating optimization algorithm is adopted for the initialization
of node vectors on single networks because of the convex objective functions, and a combination of Stochastic Gradient Descent
and back-propagation algorithm is used to optimize the collaborations across social networks) is proposed to learn the parameters
of MV-URL. To further ensure that the proposed model is suitable
for real world applications, we also discuss how to parallelize the
optimization and how to extend it to the streaming scenarios.
Extensive experiments have been conducted on two collections
of real world linked networks. One collection is publicly available
containing two linked networks while another one is contributed by
this paper which contains three social networks sharing users. The
evaluation results show that MV-URL outperforms state-of-the-art
embedding approaches of both individual networks and multiple
networks.

2

PROBLEM FORMULATION

As we aim at user representation, for each social network, we only
model user-related information. Specifically, we focus on these
information: users’ social information (User-User graph) and users’
preference information (User-Item graph and User-Word graph).
Definition 2.1. (User-User Graph) The User-User graph on a
social network is defined as Gu = (U, Eu , Wu ) where U are vertexes representing users, Eu are edges within Gu denoting the
social relationships between users and Wu are edge weights.
Definition 2.2. (User-Item Graph) The User-Item graph on a
social network is defined as a graph Guv = (U ∪ V, Euv , Wuv )
where V defines the items, Euv are edges within Guv denoting
users’ interaction information with items and Wuv correspond to
edge weights.
Definition 2.3. (User-Word Graph) The User-Word graph on
a social network is defined as a graph Guc = (U ∪ C, Euc , Wuc )
where C represents content words, Euc are edges within Guc denoting users’ interaction information with content words and Wuc
correspond to edge weights.
Definition 2.4. (Social Network, View) A social network or
view is denoted as G = (Gu , Guv , Guc ) = (U, V, C, Eu , Euv , Euc ,
Wu , Wuv , Wuc ) containing User-User graph, User-Item graph and
User-Word graph, integrating both users’ social information and
preference information on this social network.
Figure 1 shows the structure of multiple linked heterogeneous
networks that we study in this paper. A user may have accounts on
different social networks, our aim in this paper is to learn robust low

546

Session 6A: Social Media

SIGIR ’19, July 21–25, 2019, Paris, France

Online User Representation Learning
Across Heterogeneous Social Networks

SIGIR ’19, July 21–25, 2019, Paris, France

ࡳ૛࢛࢛

ࡳ૚࢛࢜
࢛૚૚

࢜૚૛

࢛૚૛

࢛૚૚

૛
ࡳ࢛࢜

࢛૛૛

࢛૛૚

࢜૚૚

࢛૚૜

࢜૛૚

࢛૛૚

࢛૛૜

࢜૛૛
ࡳ૛࢛ࢉ

ࡳ૚࢛ࢉ

ࢉ૛૛

࢛૛૚

࢛૚૚

ࢉ૚૛

There are multiple graphs on each single social network, which
are coupled together with shared user set U. There are embedding
methods available for individual graph [11, 30, 31]. However, it is
not straightforward to extend them to coupled graphs. The major
challenge is imposed by heterogeneous characteristics of multiple
graphs, which would result in heterogeneous latent spaces. As a
result, embeddings of different graphs cannot be directly matched.
MV-URL embeds the embeddings from one latent space to another by harmonious embedding matrices [33]. However, it’s very
challenging to effectively learn harmonious embedding matrices
because of the data sparsity problem. More specifically, there are
multiple graphs (i.e., this paper focuses on three graphs on each social network but MV-URL aims at solving the problem of embedding
across multiple heterogeneous networks and thus is expected to be
capable of easily extending to multiple graphs), and thus we need
to learn multiple harmonious embedding matrix. This can easily
lead to over-fitting problem especially when data is sparse, which
is usually the case on social networks. To deal with this challenge,
MV-URL constrains the matrix to be diagonal.
Specifically, MV-URL enables the transformation between social
spaces (i.e., User-User graph) and preference spaces (i.e., User-Item
graph and User-Word graph) by multiplying two diagonal matriuv
uc
uc
ces: diaд(muv
1 , . . . , md ) and diaд(m 1 , . . . , md ), where d is the
dimension of latent features. Here, we take the transformation from
User-User graph to User-Item graph for an example. The probability of linkage between vertexes of User-Item graph Guv can be
quantified as follows:

EĞƚǁŽƌŬϮ;ࡳ૛ Ϳ

EĞƚǁŽƌŬϭ;ࡳ૚ Ϳ
ࡳ૚࢛࢛

ࢉ૛૚

ࢉ૚૚
ŶĐŚŽƌ>ŝŶŬƐĐƌŽƐƐEĞƚǁŽƌŬƐ

Figure 1: The Structure of Multiple Linked Heterogeneous
Networks. a) Note that, in this paper, we assume that users and
content words both might be shared across social networks, which
is usually the case in the real world. We argued user sharing and it’s
also natural for the content sharing as most social media share the
very similar semantic space. b) uik , vik and c ik represent the i th user,
item and content word on k t h social network (i.e., G k ), respectively
dimensional vector representations for users, which are consistent
across different social networks.
Problem 1. (Multi-view User Embedding) Given K (K > 1) sok ,
cial networks G = (G 1 , G 2 , . . . , G k , . . . , G K ) where G k = (Guk , Guv
k
k
k
k
k
k
k
k
k
k
Guc ) = (U , V , C , Eu , Euv , Euc , Wu , Wuv , Wuc ) is the k th
social network, and the linkage information across them (not all
users are shared and the users can be shared by two or more networks), the problem of multi-view social network embedding is to
learn a low dimensional vector representation for each user in U =
U1 ∪ U2 ∪ . . . ∪ UK .

3

p (u i , v j ) =

MULTI-VIEW USER EMBEDDING

O (Gu ) = −

(1)

1 + exp {−u iT u j }

(h, k )Eu

loд(1 − p (uh .u k ))

(4)

O (G k ) =

The aim of this framework is to capture the vertex proximity encoded in each social network and meanwhile integrate them to vote
for the robust embeddings across social networks.
3.1.1 Embedding on Single Networks. MV-URL aims at maintaining
the proximities between vertexes directly connected by edges on
each social network. The probability of linkage measured in latent
space is quantified by the following equation:
p (u i , u j ) =



(wu )i j × loд(p (u i , u j )) −

To ensure the scalability of MV-URL on large-scale and sparse
social networks, negative sampling scheme [25, 29, 33] is adopted.
In this paper, we adopt the bi-direction strategy to sample negative
instances for each positive instance. For example,
 for
 an edge (i, j) ∈
k , we first fix user u and randomly sample N negative items,
Euv
i
  2
and then fix item v j and randomly sample N2 negative users. The
loss functions for Guv , and Guc are all defined in this way. To enable
the complementary information inside each graph affecting each
other, we jointly optimize these loss functions for each individual
graph as follows:

Collaboration Embedding Framework

1



(i, j )∈Eu

where Oco is the objective function of the collaboration framework,
in which we aim to learn the node proximities in individual social
networks and meanwhile vote for the robust node representations.
O at is the goal function for weight learning of each social network.

3.1

(3)

The loss function on each individual graph is as Equation 4 (e.g.,
take Gu as an example).

In this paper, we follow the common symbolic notation, where
upper case bold letters denote matrices, and lower case bold letters
denote column vectors. To solve the Problem 1, our approach first
mines the social information and preference information encoded in
single views (Section 3.1.1). After that, we further integrate different
views to vote for more robust user representations (Section 3.1.2).
During voting, we automatically learn the voting weights of views
through an attention based approach (Section 3.2).
The overall objective of our approach is summarized below:
O = O co + O at

1
uv
1 + exp {−u iT diaд(m uv
1 , . . . , md )v j }

−



(wuk )i j × [loд(p (u ik , u jk )) +

k
(i, j )∈Eu

−



N

 , j  )E k
(i n
u
n

k ) × [loд(p (u k , v k )) +
(wuv
ij
i
j

k
(i, j )∈Euv

−


k
(i, j )∈Euc

(2)

k ) × [loд(p (u k , c )) +
(wuc
ij
j
i

loд(1 − p (u k .u k ))]
i
j
n

N

 , j  )E k
(i n
uv
n
N


 , j  )E k
(i n
uc
n

n

loд(1 − p (u k .v k ))]
i
j
n

(5)

n

loд(1 − p (u k .c j  ))]
in

n

where N is the number of negative data instances sampled for each
positive data instance, uik and vik are the embeddings for i th user
and item on k th platform respectively, and c j is the latent vector

where ui and u j are column vectors of embedding for vertexes
indexed by i and j in the User-User graph Gu , respectively.

547

Session 6A: Social Media

SIGIR ’19, July 21–25, 2019, Paris, France

SIGIR ’19, July 21–25, 2019, Paris, France

Wang and Yin, et al.

for j t h content word. In this Equation, the regularization norms are
omitted and we will illustrate them in the following part.
Note that, in this paper, we assume that all social platforms
share same content space while different user and item spaces. This
is reasonable as one single content word generally has the same
semantic meaning even on different social platforms while different
platforms focus on varying aspects for the same users or items [41]
(e.g., Facebook reflect users’ interaction with acquaintances in daily
life while Youtube focus on users’ preferences on videos).

3.2

3.1.2 Collaboration Embedding. By minimizing the Equation 5
K are
on each platform, the view-specific representations {U k }k=1
able to preserve the structure information encoded in individual
platforms. In this part, we focus on promoting the collaboration of
different platforms to learn a robust representations for users. We
introduce the co-regularization scheme [25] to achieve this.
The problem setting in [25] is that all users have to be shared
across all views , but this is usually not the case in real world. A user
might have accounts on one single social platform or a subset of
social platforms. These two types of users are modeled in different
ways in MV-URL. Assume that there are S users shared across a
subset of social platforms, We use a set U s to define these shared
users. For each shared user u, the subset of social platforms having
u’s account is defined as Su . Note that, |Su | is not necessarily
equal to K. We introduce the following regularization term for each
shared user u:

co =
Ru

k ∈Su

k  u k − u 2
λu
2

(6)

λ ik = 
kn

k ∈Su

k uk
λu

K


(7)

O (G k ) + R

(8)

k =1

R = α Ru + β

k
K N
v


k =1 n=1

 v nk 22 +γ

Nc


 c n 22 +δ

n=1

Ru =


u ∈U s

co +
Ru

K

k =1

k
K N
s̃


k =1 n=1

k  2 +ϵ
 Muv
2

 u nk 22

K

k =1

(11)

exp (z k  · u iC )

where zk is a feature vector of platform k, describing which users
will consider platform k as informative, and uiC is the concatenation
of all the platform-specific representations of user ui . An observation is that for users who have similar embeddings on each platform,
they will have similar weights over each platform, which allows us
to leverage the learned embeddings in each social platform.
To make the weight learning task-aware, the weights of social
networks for each user are automatically learned with the backpropagation algorithm based on the predictive error in different
tasks. Assume that the set of labeled users is L. For user linkage
prediction, L is the collection of linked user pairs between G and
G K +1 and the loss function is defined as Equation 12, where ui is the
robust representation of user ui from G and u j is the view-specific
representation of user u j in G K +1 . For user attribute prediction,
L is the set of users with labeled attribute values and the loss
function is defined as Equation 13. In this equation, ui is the robust
representation for user ui , and li is the attribute value label vector
of ui , in which the dimension j is set to 1 if ui ’s attribute is j th value
and set as 0 otherwise, and w is the parameter set of the classifier.

The final objective of our collaboration embedding framework can
be summarized below:
O co =

exp (z k · u iC )

k  =k 1

where λuk is the weight of platform k in voting for the robust representation for the shared user u, u is the robust representation
for u, and u k is the representation for u on k t h social platform.
Intuitively, by learning proper weights λuk for each shared user u,
MV-URL focuses on the most informative views in learning u’s embedding. We will introduce how MV-URL automatically learns such
weights in Section 3.2. By minimizing Equation 6, representations
on each social platform will vote for the robust representations
based on Equation 7. This vote process is quite intuitive. Naturally,
the robust embeddings are calculated as the weighted combinations
of the view-specific representations with coefficients as the voting
weights of each view.

u=

View Weight Learning Model

In this part, we present an attention-based method to automatically
compute the weights for each social platform in voting the robust
node representations for the shared users. The weight learning
should be both task-specific and user-specific. First, the weights
learning depends on application tasks. For example, one user may
prefer Facebook over Twitter for communicating with daily acquaintance while use Twitter more frequently instead for sharing
real-time status. On the other hand, the weight learning should be
personalized [41] as users usually play different roles and get different levels of influence from their neighbors in different networks.
We focus on two applications: user attribute prediction and user
linkage prediction. We treat the prediction of one user attribute as a
multi-label classification task. For user account linkage prediction,
we focus on this scenario: given G = {G 1 , G 2 , . . . , G K } where K ≥ 2
and the user account linkage information inside of G are known and
another social network G K +1 which shares users with G but the
linkage information between G and G K +1 are partially given, the
task is to predict the unknown user linkage information between
G and G K +1 . In other words, we already know the user linkage
information on two or more existing social networks (i.e., G), and
then we receive a new social network which also share some users
with the existing networks but the linkage information is only
partially known and to be predicted for the remaining.
To make weight learning user aware, personalized softmax units
are used to define the weight of view k for user ui as follows:

k  2 (9)
 Muc
2

l ink = −
O at



[loд(p (u i , u j )) +

(ui ,u j )∈L
at t r =
O at

(10)

3.3

where α, β, γ , δ and ϵ ∈ R are regularization coefficients, Nuk
and Nvk are the numbers of users and items in k t h social platform
respectively, Nc is the number of content words, and Ns̃k is the
number of users who have accounts on k t h platform only and are
not shared across platforms.


u i ∈L

N


(ui  ,u j  )L
n
n

 w u i − l i 22

loд(1 − p (u i  .u j  ))]
n
n

(12)

(13)

Model Optimization

Alternating optimization algorithm is adopted for the initialization
of view-specific node representations on each social platform. Then,
MV-URL optimizes the collaborations across different social platforms with a combination of the well known Mini-Batch Stochastic

548

Session 6A: Social Media

SIGIR ’19, July 21–25, 2019, Paris, France

Online User Representation Learning
Across Heterogeneous Social Networks

SIGIR ’19, July 21–25, 2019, Paris, France

Algorithm 1: Optimization Algorithm
Input: K social networks G = (G 1, G 2, . . . , G K ) , a set of labeled data L , number of
samples M , number of negative samples N ;
Output: Robust latent representations for all the users on G ;

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

Node

k = 1;

Initialize C randomly;
while k <= K do
Initialize U k and V k randomly;
while not converge do
Fixing V k and C , find the optimal U k with gradient descent w.r.t. Equ. 5;

Link

Property

Twitter

Foursquare

Users
Items
Words
User-User
User-Item
User-Word

5,223
122,047
123,534
164,920
615,515
12,209,481

5,392
38,921
28,411
76,972
48,756
292,561

Table 1: Statistics of Graphs on TW-FS

Fixing U k and C , find the optimal V k with gradient descent w.r.t. Equ. 5;

computation of first step is naturally parallelizable (i.e., from Line
3 to Line 10). For the second step, inspired by [7], asynchronous
stochastic gradient descent (ASGD) can be used to parallelize the
model optimization procedure on GPUs to accelerate the training
process. This is based on the observation that social graphs are
usually very sparse and when different threads sample different
positive instances for model updating on a sparse dataset, the elements of the sampled instances in different threads seldom overlap,
i.e., the latent vectors of the elements (e.g., users, items and words)
usually do not conflict across different threads. It has been shown
experimentally in [7] that, on social networks, the speed up ratio
is nearly linear to the number of threads and there is almost no
loss of performance in running models with ASGD. MV-URL is
implemented with ASGD for the evaluation in this paper.
Studying MV-URL under streaming scenarios is interesting as
it offers a solution to online embedding of heterogeneous streams.
In streaming scenario, the optimization should be implemented
without knowledge of entire graphs. For Algorithm 1, we discuss
how to extend the part between Line 11 and Line 23 to streaming
scenario, as the initialization of MV-URL between Line 1 and Line
10 is usually done offline. One change is that, batch-based stochastic
gradient decent is used to update the weight of each social media
from Line 20 to Line 22. For the collaboration embedding part (i.e.,
between Line 13 and Line 19), we discuss two possible scenarios.
If the window size is large and there are data from various social
platforms in a window, we still use the hierarchy sampling strategy
in Algorithm 1 to update the embeddings online. This is to ensure
that every view and every graph will get updated if the data distribution is unbalanced. If the window size is small and a window
only contains the data from one single social platform, we adopt the
existing edge-based sampling strategy for single graphs [11, 30].

Fixing U k and V k , find the optimal C with gradient descent w.r.t. Equ. 5;
end
end
while not converge do
it er = 0;
while it er <M do
Randomly select a view, denoted as k ;

17
18
19
20
21
22
23 end

Randomly select a graph from G k ;
Sample an edge with Equation 14 from the selected graph and sample N
negative edges with bi-direction strategy;
Update U k , V k or C w.r.t. Equ. 1;
it er = it er + 1;
end
Optimize the parameters of the softmax unit with gradient decent w.r.t. Equ. 12 or 13;
Update the weights of social platforms for each user according to Equ. 11;
Compute robust user representations across social platforms U according to Equ. 7;

Gradient Descent (SGD) [25] and the back-propagation algorithm.
The overall optimization algorithm is summarized in Algorithm 1.
Minimizing the loss function for each social platform in Equation
5 is a convex optimization problem. Thus, we can employ alternating optimization algorithms for pre-training the node vectors (i.e.,
users, items and words) on each social platform (Lines from 1 to
10). This pre-training is for not propagating negative influences to
other social platforms during the optimization for the collaborations
across related social media.
After initialization of vectors on each social platform, we optimize the collaborations across platforms with a combination of
Mini-Batch SGD and back-propagation algorithm (Lines from 11
to 23). For SGD (Lines from 12 to 19), we fix the weights of social
platforms for each user and control the learning rate using AdaGrad
[7]. Hierarchical sampling strategy is used to sample positive edges
(Line 14 to 18). Specifically, we first sample a view randomly and
then sample a positive edge (i, j) from this view according to the
probability defined in Equation 14. For this positive edge, we set
corresponding negative edges following existing work [25, 29, 30],
and update the view-specific representations and shared word representations with respect to Equation 1. For the back-propagation
algorithm (Line 20 to Line 22), by fixing the entities’ representations on each social platform, we compute the parameter vectors
of social platforms (i.e., zk ) with the labeled data using back propagation algorithm (Line 20), and update the voting weights of social
platforms (i.e., λuk ) for different users (Line 21). Finally, different
view-specific user representations will be integrated to vote for the
robust representations U based on the learned weights (Line 22).
p (i, j ) = 

3.4

wi j

(i , j  )∈E

wi  j 

4

EVALUATION

The experiment is to evaluate the effectiveness of the latent features
learned by MV-URL on two tasks: user account linkage across social
platforms and user attribute prediction.

4.1

Experimental Setups

4.1.1 Datasets and Graph Construction. In this paper, two collections of real-world linked heterogeneous networks are used: TW-FSFB and TW-FS. TW-FS has been widely used in previous research
work [36, 38, 40] while TW-FS-FB is a new collection of threelinked heterogeneous networks. There are three real-world social
networks involved in these two collections of datasets: Twitter
(TW), Foursquare (FS) and Facebook (FB).
TW-FS-FB contains the related information of a group of users
on these three social networks. There are 2, 990 users in total on this
dataset. 2, 328 users have records on both Foursquare and Twitter
and 1, 755 of them have activities on all three platforms. TW-FS

(14)

Further Discussion

There are two major steps in Algorithm 1: independent training
on each social platform and joint training across networks. The

549

Session 6A: Social Media

SIGIR ’19, July 21–25, 2019, Paris, France

SIGIR ’19, July 21–25, 2019, Paris, France

Node

Link

Wang and Yin, et al.

Property

Twitter

Foursquare

Facebook

Users
Items
Words
User-User
User-Item
User-Word

2,502
14,517
20,028
61,991
48,103
767,994

2,990
39,364
12,591
180,247
39,364
194,768

2,041
3487
20,341
14,347
80,613
1,812,013

merged graphs for both TW-FS-FB dataset and TW-FS dataset as
the input to EOE.
As we argued before, our model is able to model different levels
of heterogeneity. To experimentally validate the benefits brought by
restraining shared semantic space across different social networks,
using a diagonal harmonious embedding matrix to reconcile the heterogeneities inside single networks and introducing co-regulation
scheme to promote the collaboration of different networks, respectively, we also compare with three variant versions of our MV-URL
model: MV-URL-V1, MV-URL-V2 and MV-URL-V3.
MV-URL-V1 is the variant version of MV-URL model assuming
that different social networks have their own content space. More
specifically, this model assumes that the embeddings for the same
words on different social networks are different and uses c kj instead
of c j to represent the word embeddings in Equation 5.
MV-URL-V2 is the variant version of MV-URL model which
does not use the diagonal harmonious embedding matrix inside
single networks. Specifically, this model uses the following Equation
instead of Equation 3 as the measurement of the closeness between
vertexes of User-Item graph.
1
p(ui , v j ) =
1 + exp{−uTi v j }

Table 2: Statistics of Graphs on TW-FS-FB
contains the activities of another group of users on Foursquare and
Twitter [38]. The number of shared users is 3, 388.
We construct User-User graph, User-Item graph and User-Word
graph for each social network on both datasets. For Facebook, we
extract “Likes” information for each user and the items in “Likes” are
used to construct User-Item graph. For both Twitter and Foursquare,
the check-in locations are used as the items in User-Item graph.
Users’ comments and post information on all three social platforms
are used to extract the words in User-Word graph. More specifically,
we first extract all the words based on the stop words listed in “NLTK
3.3” (Natural Language Toolkit)1 , which is a leading platform for
building Python programs to work with human language data, then
remove the words which have been used less than 20 times, and at
last remove the words whose length is less than 3.
We assume that, for User-User graphs, all the edges are equally
weighted. On Facebook, all edges in User-Item graph are equally
weighted. On Twitter and Foursquare, the weight of euv in the
User-Item graphs is computed as 1 + loд(Nuv ), where Nuv is the
number of check-ins associated with user u and item v. Similarly,
the weight of an edge euc in User-Word graphs is computed as
1 + loд(Nuc ), where Nuc is the numbers of activities associated
with u and word c. Specifically, Nuc is the number of items that are
linked with u in User-Item graph and have content containing the
word c. The statistics of constructed graphs on the two collections
of networks are presented in Table 1 and Table 2 respectively.
4.1.2 Comparison Methods. We compare MV-URL with the following four graph embedding methods:
LINE and Node2Vec: Both LINE [30] and Node2Vec [11] are
designed for single networks. To feed our datasets to these two
models, we merge the edges of different social networks into a
unified network and then embed the unified network with these
models. We follow the rules that the items are not shared while
both users and content words are shared when do the merging for
both LINE and Node2Vec.
MVE: MVE [25] treats each social network as a single graph and
uses a single-view based embedding method to encode the nodes
on each social network. Thus, we first merge the User-User Graph,
User-Item Graph and User-Word Graph on each social network into
one single graph. As a result, we have three merged graphs on
TW-FS-FB dataset and two merged graphs on TW-FS dataset. Then
MVE is used to embed the merged graphs by treating each merged
graph on a social network as a single view.
EOE: According to the defined input of EOE [33], we first merge
the User-User Graphs, User-Item Graphs and User-Word Graphs on
different social networks into a unified User-User Graph, User-Item
Graph and User-Word Graph respectively. As a result, we have three

MV-URL-V3 is the variant version of MV-URL model which
omits the co-regulation scheme and trains the representations of
each social network independently. MV-URL-V3 uses the average
of a user’s representations on multiple social networks as his/her
personal representation. More specifically, we use the following
Equation instead of Equation 7 to compute robust representations
based on the results on multiple social networks.
 1
uk
u=
|Su |
k ∈Su

4.1.3 Evaluation Method. In this part, we will introduce the evaluation methods for both tasks.
User Linkage Prediction. As described in Section 3.2, MV-URL
predicts the user linkage between G = {G 1 , G 2 , . . . , G K } and G K +1 ,
where K ≥ 2 as there is no meaning in learning weight for one
graph (i.e., K = 1), thus we need at least three datasets for this task.
So we only conduct the evaluation of this task on TW-FS-FB. We
evaluate three scenarios where G is set to {TW , FS }, {TW , F B} and
{FS, F B} respectively. For the shared users between G and G K +1 ,
we randomly sample 50% users as the labeled data Xt r ain and use
the other 50% users as the test data Xt est .
There are many linked user pairs (ui , u j ) in Xt est where ui ∈ G
and u j ∈ G K +1 . For each ui ∈ G in the test set, we want to predict
his/her linked user in G K +1 by ranking the users in G K +1 according
to the scores defined in Equation 2. Let rank (u j ) denote the position
of u j in the ranking list. To evaluate the user linkage prediction
thoroughly, we employ two widely-used metrics Hits@n and MRR
(i.e., Mean Reciprocal Rank) [3, 20] in top-n prediction tasks. While
Hits@n indicates the probability that the models put the actually
linked users in the top-n predictions, MRR measures the ranking
quality which assigns higher scores to the test cases where the
ground-truth users are at higher ranking positions.
The computation of Hits@n proceeds as follows. We form a
top-n prediction list by picking the n top ranked users from the
ranked list. If rank (u j ) ≤ n, we have a hit. Otherwise, we have a

1 https://www.nltk.org/

550

Session 6A: Social Media

SIGIR ’19, July 21–25, 2019, Paris, France

Online User Representation Learning
Across Heterogeneous Social Networks

SIGIR ’19, July 21–25, 2019, Paris, France

(a) MacroF1

```

``` Attributes Hometown
```
Methods
`
MV-URL
MVE
EOE
Node2Vec
LINE

``` Attributes Hometown
```
Methods
`

UserLocation

0.333971
0.305339
0.210579
0.094384
0.031329

(b) MicroF1

```

0.326309
0.306698
0.203255
0.091984
0.019743

MV-URL
MVE
EOE
Node2Vec
LINE

UserLocation

0.451953
0.375307
0.277918
0.112893
0.130575

0.418762
0.359624
0.265274
0.094906
0.095707

Table 3: User Attribute Prediction Results
7

MV−URL
MVE
EOE

8

Node2Vec
LINE

7

5

7

Node2Vec
LINE

6

4

3

3

2

2

1

1

0

0

5

10

15

n

20

25

30

(a) Foursquare

Node2Vec
LINE

4

4

3

MV−URL
MVE
EOE

5

Hits@n(%)

6

5

Hits@n(%)

6

MV−URL
MVE
EOE

Hits@n(%)

8

2
1
5

10

15

n

20

25

0

30

5

(b) Twitter

10

15

n

20

25

30

(c) Facebook

Figure 2: Hits@n of User Account Linkage Prediction

XXX

XXTest Sets Foursquare
XXX

miss. We define hit@n for a single test case as either 1, if we have
a hit for this case, or 0, if otherwise. The overall Hits@n is defined
by averaging over all test cases:
#hit @n
H it s@n =
| Xt es t |

Methods

MV-URL
MVE
EOE
Node2Vec
LINE

(15)

where #hit@n denotes the number of hits in the test set, and |Xt est |
is the number of test cases.

M RR =

1
| Xt es t |

(ui ,u j )∈Xt es t

1
r ank (u j )

(16)

0.014417
0.010999
0.010959
0.008727
0.007544

Twitter

Facebook

0.014387
0.009340
0.011901
0.008936
0.011578

0.016667
0.008564
0.011253
0.009178
0.009737

Table 4: MRR of User Account Linkage Prediction

MRR is defined as Equation 16. It is an average of the reciprocal
rank of a positive example, and a good prediction model should
have a bigger MRR value. In contrast to mean rank, MRR is less
sensitive to outliers.
User Attribute Prediction. We evaluate this task on TW-FS as detailed attribute information has been provided on this dataset. More
specifically, we evaluate the prediction of the attribute Hometown
on Foursquare and UserLocation on Twitter. We choose these two
attributes for two reasons. First, many real applications can benefit
from predicting these attributes. For example, we can recommend
points of interests (e.g., restaurants, hotels, etc) to a specific user
if we know his/her current location. Second, these two attributes
have reasonable granularities (i.e., not too coarse like Gender which
makes the prediction trivial, and not too fine like UserName which
makes the prediction hard and meaningless). There are 121 and 161
classes for Hometown and UserLocation respectively.
The prediction of one attribute is actually a multi-label classification task. We use the user representations learned by different
algorithms as input features, and train one-vs-rest linear classifiers
using the LibLinear package [8]. 50% labeled data are used to learn
the weights of different social platforms. The results are evaluated
with a pair of widely used metrics in multi-class classification tasks:
MacroF1 and MicroF1 [18, 25]. We will not report the details of
these two metrics considering the limited space.
Hyper-parameter Optimization. For the hyper parameters in MVURL, we perform cross-validation. Specifically, we use the grid
search algorithm to obtain the optimal hyper parameter setup on the

validation dataset: α, β, γ , δ , and ϵ. We will selectively present the
parameter sensitivity analysis for some important hyper-parameters.

4.2

Prediction Effectiveness

In this part, we present the overall performance of all embedding
methods with well-tuned parameters. For all approaches, the dimension of node representation is set as 100. For LINE, EOE, MVE
and MV-URL, the number of negative samples N is set to 5, and
the initial learning rate is set to 0.025, as suggested in [25, 30]. For
node2vec, the window size is set to 10, the walk length is set as 40,
parameters p and q are selected based on the labeled data, following
[11]. For MVE, parameter η is set to 0.05, as suggested in [25].
Table 3 demonstrate the user attribute prediction results on
TW-FS. Some observations can be made from Table 3: 1) MV-URL
outperforms all the other models consistently on this task. More
specifically, compared with the second best model MVE, MV-URL
brings 9.38% (MacroF1) and 20.42% (MicroF1) improvements for the
prediction of users’ hometown, and also brings 6.39% (MacroF1)
and 16.44% (MicroF1) improvements for the prediction of users’
location; 2) MVE performs the second best consistently for this task.
The possible reason is that both MV-URL and MVE are supervised
learning methods and they are capable in taking advantage of the
user attribute information other than the graph information in this
task while the other methods are not.
For user linkage prediction task, we present both Hits@n where
n = 5, 10, 15, 20, 25, 30 and MRR when the social network to be
linked (i.e., denoted as G K +1 ) is set to Facebook, Foursquare and

551

Session 6A: Social Media

SIGIR ’19, July 21–25, 2019, Paris, France

SIGIR ’19, July 21–25, 2019, Paris, France

```

Wang and Yin, et al.

(a) MacroF1

``` Attributes Hometown
```
Methods
`
MV-URL
MV-URL-V1
MV-URL-V2
MV-URL-V3

0.333971
0.325669
0.326109
0.293468

UserLocation

```

(b) MicroF1

``` Attributes Hometown
```
Methods
`

0.326309
0.290353
0.287120
0.291086

MV-URL
MV-URL-V1
MV-URL-V2
MV-URL-V3

0.451953
0.438136
0.443412
0.410801

UserLocation
0.418762
0.396818
0.406480
0.381989

Table 5: Impact of Factors in User Attribute Prediction

XX

XXXTest Sets Foursquare
XXX

Methods

MV-URL
MV-URL-V1
MV-URL-V2
MV-URL-V3

0.014417
0.012885
0.013637
0.009305

Twitter

Facebook

0.014387
0.013774
0.012391
0.010630

0.016667
0.015611
0.012149
0.008452

Sensitivity Analysis. In this experiment, we investigate the sensitivity of MV-URL w.r.t. two important hyper-parameters: the dimension of node representations D and the number of negative
samples for each positive instance N . Considering the limited space,
for user account linkage prediction, the sensitivity analysis is done
only when the target is Facebook.
Figure 4 contain the impact of varying D on user attribute prediction and user linkage prediction. We observe that the performance
in both tasks first improves by increasing D and then the increment becomes small. The reason is that D represents the model
complexity. Thus, when D is too small, the model has limited ability
to describe the data. However, when D exceeds a threshold (e.g.,
D = 60 for user linkage task measured with MRR), the model is
complex enough to handle the data. At this point, it is less helpful
to improve the model performance by increasing D.
Figure 5 show how MV-URL performs with various N on user
attribute prediction and user linkage prediction. This figure shows
MV-URL gains a significant improvement by increasing N from 1
to 2 for all the tasks. Then the model’s performance becomes steady.
This demonstrates that our proposed model does not require too
many negative samples to reach a good performance (i.e., MVURL is able to achieve a very good performance with N = 2). A
smaller number of negative samples means much less training time.
For example, assuming that the training time of the model with 5
negative samples is t, the training time is roughly t/2 for the model
with 2 negative samples according to our experiments.

Table 6: Impact of Factors in User Account Linkage Prediction Measured by MRR
Twitter respectively. Figure 2 show methods’ performance measured by Hits@n, while Table 4 show the results of MRR. From
Figure 2 and Table 4, we can see that the improvements brought
by MV-URL is significant compared with the comparison methods.
For Hits@n, we take top-20 prediction as an example. Compared
with the second performers, MV-URL has brought 27.19%, 54.27%,
and 44.63% improvements on Foursquare, Twitter and Facebook,
respectively. This indicates that, the proposed MV-URL has much
larger probability in putting the actually linked users across social
media in the top-20 predictions. From Table 4, we can see that, compared with the second best methods, MV-URL brings 31.08%, 20.89%
and 48.11% improvements on Foursquare, Twitter and Facebook,
respectively. This demonstrates that MV-URL prefers to rank the
ground-truth user in the top positions. Note that, different from the
user attribute prediction task where MV-URL and MVE are capable
in using user attribute information in addition to graph information
while the other methods are not, all the methods have used the
same amount of information in user linkage prediction.

4.3

5

Impact of Different Factors

RELATED WORK

There are two lines of work that are most related to our work in
this paper: user modeling on social media and network embedding.
User Modeling on Social Media. User modeling on social media has drawn much research interest, ranging from link prediction
containing user linkage prediction [5, 6, 19] and friendship prediction [13, 15, 36, 40], user attribute prediction [2, 23, 24], community
detection [14, 35, 37] to item recommendation [21, 36, 42]. Our work
focus on two application tasks: user account linkage prediction and
user attribute prediction. The essential idea of user account linkage
prediction is building users’ profiles with the information on social
media and then computing the similarities between users based on
the built users’ profiles. The users with highest similarities on two
platforms are identified as the same user. Most existing work in this
area focus on pair-wise user linkage between two platforms only
[5, 12, 17, 19] while our work focus on user account linkage across
multiple platforms. User attribute prediction is actually a classification task and most existing work only leverage the information on
one social platform to train a classification model [2, 23, 24]. Our
research work is able to leverage information from multiple social
platforms in predicting the users’ attributes.

Analysis of Various Heterogeneity Reconciling Strategies. To evaluate
key components of MV-URL, we implement and compare with three
variant versions: MV-URL-V1, MV-URL-V2 and MV-URL-V3. The
results are demonstrated in Table 5, 6 and Figure 3.
Several observations can be made from the results in Table 5, 6
and Figure 3: 1) MV-URL outperforms MV-URL-V1, MV-URL-V2
and MV-URL-V3 on all tasks, which evaluate the benefit brought
by sharing the same semantic space, using the diagonal harmonious embedding matrix to reconcile the heterogeneities inside of
single networks and adopting co-regulation scheme in promoting
the collaboration of different networks, respectively; 2) MV-URLV3 performs worst compared with the other two variant versions.
Essentially, the possible reason is that MV-URL-V3 is a late fusion method, where the user linkage information is used in fusing
the representation learned from different social networks while the
other methods use this linkage information in training the representations. This performance discrepancy shows that in our problem
setting, user representation learning usually benefits more from
the early fusion methods compared to late fusion methods.

552

Session 6A: Social Media

SIGIR ’19, July 21–25, 2019, Paris, France

Online User Representation Learning
Across Heterogeneous Social Networks

SIGIR ’19, July 21–25, 2019, Paris, France

MV−URL
MV−URL−V1

7

8

MV−URL−V2
MV−URL−V3

MV−URL
MV−URL−V1

7

5

6

4

3

3

2

2

1

1

0

0

5

10

15

n

20

25

30

MV−URL−V2
MV−URL−V3

4

4

3

MV−URL
MV−URL−V1

5

Hits@n(%)

6

5

Hits@n(%)

6

7

MV−URL−V2
MV−URL−V3

Hits@n(%)

8

2
1
5

10

(a) Foursquare

15

n

20

25

0

30

5

10

(b) Twitter

15

20

n

25

30

(c) Facebook

Figure 3: Impact of Factors in User Account Linkage Prediction Measured by Hits@n
0.4

0.3

0.3

F1

0.4

0.2

0.2

0.1

0.1
20

40

60

80
D

100

120

140

2.0

MV−URL

40

60

80
D

100

120

4.0
3.5

140

MV−URL

1.8

4.5

1.6
1.4
1.2

3.0
20

(a) User Location Prediction

5.0

MacroF1 of MV−URL
MicroF1 of MV−URL

0.5

MRR(%)

0.5

F1

0.6

MacroF1 of MV−URL
MicroF1 of MV−URL

Hits@20(%)

0.6

20

40

60

80

D

100

120

140

160

20

(c) User Linkage (H it s@20)

(b) User Hometown Prediction

40

60

80

D

100

120

140

160

(d) User Linkage (M RR )

Figure 4: Impact of Latent Dimensions

0.3

0.4
0.3

0.2

2.0

MV−URL

MV−URL

1.8

6.0
MRR(%)

0.4

7.0

MacroF1 of MV−URL
MicroF1 of MV−URL

0.5
F1

F1

0.6

MacroF1 of MV−URL
MicroF1 of MV−URL

0.5

Hits@20(%)

0.6

5.0
4.0

1.6
1.4
1.2

3.0
1

2

3

N

4

5

(a) User Location Prediction

6

1

2

3

N

4

5

6

1

2

3

N

4

5

6

(c) User Linkage (H it s@20)

(b) User Hometown Prediction

1

2

3

N

4

5

6

(d) User Linkage (M RR )

Figure 5: Impact of Negative Samples
Network Embedding. Network embedding has become a very
hot research problem in recent years and has been proven fundamental for many network based applications [10, 26, 28]. The aim
is to project a graph-structured data into the feature vector representations automatically. With these feature vector representations,
many machine learning algorithms can be applied.
One major line of research work treat the relations as the translations between the entities [1, 16]. In recent years, some embedding
works have been proposed based on random walk [11] and deep
learning models [22], which extend the techniques in language
modeling and deep learning from sequences of words to graphs by
treating walks as the equivalent of sentences.
Most the above techniques are based on single networks. However, many real systems can be naturally projected to multiple
linked networks [26], like movie knowledge library entries can be
found from both IMDB2 and Douban Movie sites3 . Broad Learning
[37, 39] is a new type of learning task, which focuses on fusing
multiple information sources together and carrying out data mining
tasks across these fused sources in one unified analytic. The current
research work in Broad Learning can be categorized into two major types: learning over multiple networks with same entities but
different relations (e.g., multi-view learning, multi-source learning
and multi-model learning) [9, 25], and transferring the information
from other networks to improve the learning on one specific network [38]. Different from these existing work, our work focuses on

joint representation learning by integrating multiple heterogeneous
networks where each network involved is a heterogeneous network
containing different links and diverse nodes.

6

CONCLUSION

In this paper, we proposed a novel user representation learning
model, MV-URL, to jointly and automatically embed users over
multiple linked heterogeneous social networks, which effectively
overcomes the challenges arising from modeling heterogeneous
information sources. MV-URL assumes that all the social networks
share the same semantic space by restraining the content words
share the same latent representation across different social networks. To reconcile the heterogeneities of different latent spaces
inside of single networks, MV-URL uses a diagonal harmonious embedding matrix to project the embeddings from one latent space to
another latent space. On the other hand, a co-regulation scheme is
introduced to promote the collaborations of different networks. Extensive experiments have been conducted on two collections of real
world linked networks. The evaluation results show that MV-URL
outperforms both state-of-the-art approaches for user embedding
with individual networks and other competitive approaches with
multiple networks.

ACKNOWLEDGMENTS
This work was supported by ARC (Australian Research Council) Discovery Project (No. DP190101985 and DP170103954), ARC Discovery Early Career Researcher Award (No. DE160100308), and Shanxi
Provincial Natural Science Foundation in China (No. 2018JM6063).

2 http://www.imdb.com/
3 https://www.douban.com/

553

Session 6A: Social Media

SIGIR ’19, July 21–25, 2019, Paris, France

SIGIR ’19, July 21–25, 2019, Paris, France

Wang and Yin, et al.

REFERENCES

[21] Wei Pan, Nadav Aharony, and Alex Pentland. 2011. Composite Social Network
for Predicting Mobile Apps Installation. In AAAI.
[22] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: online learning
of social representations. In SIGKDD. 701–710.
[23] Daniel Preotiuc-Pietro, Wei Xu, and Lyle H. Ungar. 2016. Discovering User
Attribute Stylistic Differences via Paraphrasing. In AAAI. 3030–3037.
[24] Minghui Qiu, Yanchuan Sim, Noah A. Smith, and Jing Jiang. 2015. Modeling User
Arguments, Interactions, and Attributes for Stance Prediction in Online Debate
Forums. In SIAM. 855–863.
[25] Meng Qu, Jian Tang, Jingbo Shang, Xiang Ren, Ming Zhang, and Jiawei Han.
2017. An Attention-based Collaboration Framework for Multi-View Network
Representation Learning. In CIKM. 1767–1776.
[26] Chuan Shi, Yitong Li, Jiawei Zhang, Yizhou Sun, and Philip S. Yu. 2017. A Survey
of Heterogeneous Information Network Analysis. TKDE 29, 1 (2017), 17–37.
[27] Ajit Paul Singh and Geoffrey J. Gordon. 2008. Relational learning via collective
matrix factorization. In SIGKDD. 650–658.
[28] Yizhou Sun and Jiawei Han. 2012. Mining heterogeneous information networks:
a structural analysis approach. SIGKDD Explorations 14, 2 (2012), 20–28.
[29] Jian Tang, Meng Qu, and Qiaozhu Mei. 2015. PTE: Predictive Text Embedding
through Large-scale Heterogeneous Text Networks. In SIGKDD. 1165–1174.
[30] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei.
2015. LINE: Large-scale Information Network Embedding. In WWW. 1067–1077.
[31] Lei Tang and Huan Liu. 2011. Leveraging social media networks for classification.
Data Min. Knowl. Discov. 23, 3 (2011), 447–478.
[32] Weiqing Wang, Hongzhi Yin, Ling Chen, Yizhou Sun, Shazia Wasim Sadiq, and
Xiaofang Zhou. 2015. Geo-SAGE: A Geographical Sparse Additive Generative
Model for Spatial Item Recommendation. In SIGKDD. 1255–1264.
[33] Linchuan Xu, Xiaokai Wei, Jiannong Cao, and Philip S. Yu. 2017. Embedding of
Embedding (EOE): Joint Embedding for Coupled Heterogeneous Networks. In
WSDM. 741–749.
[34] Hongzhi Yin, Bin Cui, Xiaofang Zhou, Weiqing Wang, Zi Huang, and Shazia W.
Sadiq. 2016. Joint Modeling of User Check-in Behaviors for Real-time Point-ofInterest Recommendation. TOIS 35, 2 (2016), 11:1–11:44.
[35] Hongzhi Yin, Zhiting Hu, Xiaofang Zhou, Hao Wang, Kai Zheng, Nguyen
Quoc Viet Hung, and Shazia Wasim Sadiq. 2016. Discovering interpretable
geo-social communities for user behavior prediction. In ICDE. 942–953.
[36] Jiawei Zhang, Jianhui Chen, Shi Zhi, Yi Chang, Philip S. Yu, and Jiawei Han. 2017.
Link Prediction across Aligned Networks with Sparse and Low Rank Matrix
Estimation. In ICDE. 971–982.
[37] Jiawei Zhang, Limeng Cui, Philip S. Yu, and Yuanhua Lv. 2017. BL-ECD: Broad
Learning based Enterprise Community Detection via Hierarchical Structure
Fusion. In CIKM. 859–868.
[38] Jiawei Zhang, Xiangnan Kong, and Philip S. Yu. 2014. Transferring heterogeneous
links across location-based social networks. In WSDM. 303–312.
[39] Jiawei Zhang, Congying Xia, Chenwei Zhang, Limeng Cui, Yanjie Fu, and Philip S.
Yu. 2017. BL-MNE: Emerging Heterogeneous Social Network Embedding Through
Broad Learning with Aligned Autoencoder. In ICDM. 605–614.
[40] Jiawei Zhang, Philip S. Yu, and Zhi-Hua Zhou. 2014. Meta-path based multinetwork collective link prediction. In SIGKDD. 1286–1295.
[41] Erheng Zhong, Wei Fan, Junwei Wang, Lei Xiao, and Yong Li. 2012. ComSoc:
adaptive transfer of user behaviors over composite social network. In SIGKDD.
696–704.
[42] Junxing Zhu, Jiawei Zhang, Lifang He, Quanyuan Wu, Bin Zhou, Chenwei Zhang,
and Philip S. Yu. 2017. Broad Learning based Multi-Source Collaborative Recommendation. In CIKM. 1409–1418.

[1] Antoine Bordes, Nicolas Usunier, Alberto García-Durán, Jason Weston, and Oksana Yakhnenko. 2013. Translating Embeddings for Modeling Multi-relational
Data. In NIPS. 2787–2795.
[2] Nina Cesare, Christan Grant, and Elaine O. Nsoesie. 2017. Detection of User
Demographics on Social Media: A Review of Methods and Recommendations for
Best Practices. CoRR abs/1702.01807 (2017).
[3] Hongxu Chen, Hongzhi Yin, Weiqing Wang, Hao Wang, Quoc Viet Hung Nguyen,
and Xue Li. 2018. PME: Projected Metric Embedding on Heterogeneous Networks
for Link Prediction. In SIGKDD. 1177–1186.
[4] Ting Chen and Yizhou Sun. 2017. Task-Guided and Path-Augmented Heterogeneous Network Embedding for Author Identification. In WSDM. 295–304.
[5] Wei Chen, Hongzhi Yin, Weiqing Wang, Lei Zhao, Wen Hua, and Xiaofang Zhou.
2017. Exploiting Spatio-Temporal User Behaviors for User Linkage. In CIKM.
517–526.
[6] Wei Chen, Hongzhi Yin, Weiqing Wang, Lei Zhao, and Xiaofang Zhou. 2018. Effective and Efficient User Account Linkage across Location Based Social Networks.
In ICDE. 1085–1096.
[7] John C. Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient
Methods for Online Learning and Stochastic Optimization. Journal of Machine
Learning Research 12 (2011), 2121–2159.
[8] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008. LIBLINEAR: A Library for Large Linear Classification. Journal of Machine
Learning Research 9 (2008), 1871–1874.
[9] Jing Gao, Jiawei Han, Jialu Liu, and Chi Wang. 2013. Multi-View Clustering via
Joint Nonnegative Matrix Factorization. In SIAM. 252–260.
[10] Ming Gao, Leihui Chen, Xiangnan He, and Aoying Zhou. 2018. BiNE: Bipartite
Network Embedding. In SIGIR. 715–724.
[11] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable Feature Learning for
Networks. In SIGKDD. 855–864.
[12] Xiangnan Kong, Jiawei Zhang, and Philip S. Yu. 2013. Inferring anchor links
across multiple heterogeneous social networks. In CIKM. 179–188.
[13] Vincent Leroy, Berkant Barla Cambazoglu, and Francesco Bonchi. 2010. Cold
start link prediction. In SIGKDD. 393–402.
[14] Jure Leskovec, Kevin J. Lang, and Michael W. Mahoney. 2010. Empirical comparison of algorithms for network community detection. In WWW. 631–640.
[15] Ryan Lichtenwalter, Jake T. Lussier, and Nitesh V. Chawla. 2010. New perspectives
and methods in link prediction. In SIGKDD. 243–252.
[16] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning
Entity and Relation Embeddings for Knowledge Graph Completion. In AAAI.
2181–2187.
[17] Siyuan Liu, Shuhui Wang, Feida Zhu, Jinbo Zhang, and Ramayya Krishnan. 2014.
HYDRA: large-scale social identity linkage via heterogeneous behavior modeling.
In SIGMOD. 51–62.
[18] Mohamed Morchid, Mohamed Bouallegue, Richard Dufour, Georges Linarès,
Driss Matrouf, and Renato De Mori. 2015. Compact Multiview Representation of
Documents Based on the Total Variability Space. IEEE/ACM Trans. Audio, Speech
& Language Processing 23, 8 (2015), 1295–1308.
[19] Xin Mu, Feida Zhu, Ee-Peng Lim, Jing Xiao, Jianzong Wang, and Zhi-Hua Zhou.
2016. User Identity Linkage by Latent User Space Modelling. In SIGKDD. 1775–
1784.
[20] Maximilian Nickel, Lorenzo Rosasco, and Tomaso A. Poggio. 2016. Holographic
Embeddings of Knowledge Graphs. In AAAI. 1955–1961.

554

