Session 7B: Multilingual and Cross-modal Retrieval

SIGIR ’19, July 21–25, 2019, Paris, France

Cross-Modal Interaction Networks for Query-Based Moment
Retrieval in Videos
Zhu Zhang

Zhijie Lin

Zhejiang University
Hangzhou, China
zhangzhu@zju.edu.cn

Zhejiang University
Hangzhou, China
linzhijie@zju.edu.cn

Zhou Zhao∗

Zhenxin Xiao

Zhejiang University
Hangzhou, China
zhaozhou@zju.edu.cn

Zhejiang University
Hangzhou, China
alanshawzju@gmail.com

ABSTRACT

nsubj

Query-based moment retrieval aims to localize the most relevant
moment in an untrimmed video according to the given natural
language query. Existing works often only focus on one aspect
of this emerging task, such as the query representation learning,
video context modeling or multi-modal fusion, thus fail to develop
a comprehensive system for further performance improvement. In
this paper, we introduce a novel Cross-Modal Interaction Network
(CMIN) to consider multiple crucial factors for this challenging task,
including (1) the syntactic structure of natural language queries; (2)
long-range semantic dependencies in video context and (3) the sufficient cross-modal interaction. Specifically, we devise a syntactic
GCN to leverage the syntactic structure of queries for fine-grained
representation learning, propose a multi-head self-attention to capture long-range semantic dependencies from video context, and
next employ a multi-stage cross-modal interaction to explore the
potential relations of video and query contents. The extensive experiments demonstrate the effectiveness of our proposed method.

nmod

dobj

dobj

det

Ground Truth:

case

nmod

cc

det
conj

60.10s

case

det
nmod

case det

82.90s

Figure 1: Query-Based Moment Retrieval in Video
France. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3331184.
3331235

1

INTRODUCTION

Multimedia information retrieval is an important topic in information retrieval systems. Recently, query-based video retrieval [21, 26,
43] has been well-studied, which searches the most relevant video
from large collections according to a given natural language query.
However, in practical applications, the untrimmed videos often contain multiple complex events that evolve over time, where a large
part of video contents are irrelevant to the query and only a small
clip satisfies the query description. Thus, as a natural extension,
the query-based moment retrieval aims to automatically localize
the start and end boundaries of the target moment semantically
corresponding to the given query within an untrimmed video. Different from retrieving an entire video, moment retrieval offers more
fine-grained temporal localization in a long video, which avoids
manually searching for the moment of interests.
However, localizing the precise moment in a continuous, complicated video is more challenging than simply selecting a video from
pre-defined candidate sets. As shown in Figure 1, the query “A man
throws the ball and hits the boy in the face before landing in a cup”
describes two successive actions, corresponding to complex object
interactions within the video. Hence, the accurate retrieval of the
target moment requires sufficient understanding of both video and
query contents by cross-modal interactions.
Most existing moment retrieval works [6, 10, 13, 14, 22, 23, 42]
only focus on one aspect of this emerging task, such as the query
representation learning [23], video context modeling [10, 22] and
cross-modal fusion [6, 42], thus fail to develop a comprehensive
system to further improve the performance of query-based moment

CCS CONCEPTS
• Information systems → Novelty in information retrieval;
Video search.

KEYWORDS
Query-based moment retrieval; syntactic GCN; multi-head selfattention; multi-stage cross-modal interaction
ACM Reference Format:
Zhu Zhang, Zhijie Lin, Zhou Zhao, and Zhenxin Xiao. 2019. Cross-Modal
Interaction Networks for Query-Based Moment Retrieval in Videos. In
Proceedings of the 42nd International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR ’19), July 21–25, 2019, Paris,
∗ Zhou

det

Query：A man throws the ball and hits the boy in the face before landing in a cup.

Zhao is the corresponding author.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’19, July 21–25, 2019, Paris, France
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00
https://doi.org/10.1145/3331184.3331235

655

Session 7B: Multilingual and Cross-modal Retrieval

SIGIR ’19, July 21–25, 2019, Paris, France

• We propose the syntactic GCN to leverage the syntactic
structure of queries for fine-grained representation learning,
and adopt a multi-head self-attention method to capture
long-range semantic dependencies from video context.
• We employ a multi-stage cross-modal interaction to further
exploit the potential relation of video and query contents,
where an attentive aggregation method extracts relevant
syntactic-aware query representations for each frame, a cross
gate emphasizes crucial contents and a low-rank bilinear
fusion method learn cross-modal semantic representations.
• The proposed CMIN method achieves the state-of-the-art
performance on ActivityCaptions and TACoS datasets.

retrieval. In this paper, we consider multiple crucial factors for highquality moment retrieval.
Firstly, the query description often contains causal temporal
actions, thus it is fundamental and crucial to learn fine-grained
query representations. Existing works generally adopt widely-used
recurrent neural networks, such as GRU networks, to model natural
language queries. However, these approaches ignore the syntactic
structure of queries. As shown in Figure 1, the syntactic relation
implies dependencies of word pairs, helpful for query semantic understanding. Recently, the graph convolution networks (GCN) have
been proposed to model the graph structure [18, 41], including the
visual relationship graph [44] and syntactic dependency graph [25].
Inspired by these works, we develop a syntactic GCN to exploit
the syntactic structure of queries. Concretely, we first build the
syntactic dependency graph as shown in Figure 1, and then pass
information along the dependency edges to learn syntactic-aware
query representations. In detail, we consider the direction and label
of dependency edges to adequately incorporate syntactic clues.
Secondly, the target moments of complex queries generally contain object interactions over a long time interval, thus there exist
long-range semantic dependencies in video context. That is, each
frame is not only relevant to adjacent frames, but also associated
with distant ones. Existing approaches often apply RNN-based
temporal modeling [6], or propose R-C3D networks to learn spatiotemporal representations from raw video streams [42]. Although
these methods are able to absorb contextual information for each
frame, they still fail to build direct interactions between distant
frames. To eliminate the local restrictions, we propose a multihead self-attention mechanism [40] to capture long-range semantic
dependencies from video context. The self-attention method can
develop the frame-to-frame interaction at arbitrary positions and
the multi-head setting ensures the sufficient understanding of complicated dependencies.
Thirdly, query-based moment retrieval requires the comprehensive reasoning of video and query contents, thus the crossmodal interaction is necessary for high-quality retrieval. Early approaches [10, 13, 14] ignore this factor and only simply combine the
query and moment features for correlation estimations. Although
recent methods [6, 22, 23, 42] have developed a cross-modal interaction by widely-used attention mechanism, they still remain in the
rough one-stage interaction, for example, highlighting the crucial
context information of moments by the guidance of queries [22].
Different from previous works, we adopt a multi-stage cross-modal
interaction method to further exploit the potential relation of video
and query contents. Specifically, we first adopt a normal attention
method to aggregate syntactic-aware query representations for
each frame, then apply a cross gate [9] to emphasize crucial contents and weaken inessential parts, and next develop the low-rank
bilinear fusion to learn a cross-modal semantic representation.
In summary, the key contributions of this work are four-fold:

The rest of this paper is organized as follows. We briefly review
some related works in Section 2. In Section 3, we introduce our
proposed method. We then present a variety of experimental results
in Section 4. Finally, Section 5 concludes this paper.

2

RELATED WORK

In this section, we briefly review some related works on image/video
retrieval, temporal action localization and query-based moment
retrieval.

2.1

Image/Video Retrieval

Given a set of candidate images/videos and a natural language
query, image/video retrieval aims to select the image/video that
matches this query. Karpathy et al. [16] propose a deep visualsemantic alignment (DVSA) model for image retrieval, which uses
the BiLSTM to encode query features and R-CNN detector [11] to
extract object representations. Sun et al. [36] advise an automatic
visual concept discovery algorithm to boost the performance of
image retrieval. Moreover, Hu et al. [15] and Mao et al [24] regard
this problem as natural language object retrieval. As for video
retrieval, some methods [26, 43] incorporate deep video-language
embeddings to boost retrieval performance, similar to the imagelanguage embedding approach [34]. And Lin et al. [21] first parse the
query descriptions into a semantic graph and then match them to
visual concepts in videos. Different from these works, query-based
moment retrieval aims to localize a moment within an untrimmed
video, which is more challenging than simply selecting a video from
pre-defined candidate sets.

2.2

Temporal Action Localization

Temporal action localization is a challenging task to localize action
instances in an untrimmed video. Shou et al. [32] develop three
segment-based 3D ConvNets with localization loss to explicitly
explore the temporal overlap in videos. Singh et al. [33] propose a
multi-stream bi-directional RNN with two additional streams on
motion and appearance to achieve the fine-grained action detection.
To leverage the context structure of actions, Zhao et al. [45]
advise a structured segment network to model the structure of
action instances by a structured temporal pyramid. And Chao et
al. [5] boost the action localization performance by imitating the
Faster RCNN object detection framework [29]. Although these
works have achieved promising performance, they still are limited
to a pre-defined list of actions. And query-based moment retrieval
tackles this problem by introducing the natural language query.

• We design a novel cross-modal interaction networks for
query-based moment retrieval, which is a comprehensive
system to consider multiple crucial factors of this challenging
task: (1) the syntactic structure of natural language queries;
(2) long-range semantic dependencies in video context and
(3) the sufficient cross-modal interaction.

656

Session 7B: Multilingual and Cross-modal Retrieval

BiGRU

…

𝑞)

BiGRU

+

…

𝑣%
Video

Linear
Linear

×𝐻
Multi-Head Self-Attention

k-1

…

BiGRU
+

BiGRU

1

k

2

BiGRU

…

c) Multi-Stage Cross-modal Interaction Module

BiGRU

……

…

BiGRU

Linear

𝑣$

Linear

Concat

𝑣#

Scaled Dot-Product
Attention

𝑣"

……

a) Syntactic GCN Module

Cross Gate

Attentive
Aggregation

Query

Bilinear Fusion

BiGRU

...

𝑞$

…

BiGRU

……

𝑞#

BiGRU

Syntactic GCN

BiGRU

Syntactic GCN

𝑞"

SIGIR ’19, July 21–25, 2019, Paris, France

BiGRU
BiGRU

d) Moment Retrieval Module

b) Multi-Head Self-Attention Module

Figure 2: The Framework of Cross-Modal Interaction Networks for Query-Based Moment Retrieval. (a) The syntactic GCN
module leverages the syntactic structure to learn syntactic-aware query representations. (b) The multi-head self-attention
module captures long-range semantic dependencies from context. (c) The multi-stage cross-modal interaction module explores
the intrinsic relations between video and query. (d) The moment retrieval module localizes the boundaries of target moments.

2.3

3

Query-Based Moment Retrieval

Query-based moment retrieval is to detect the target moment depicting the given natural language query in an untrimmed video. Early
works study this task in constrained settings, including the fixed
spatial prepositions [21, 38], instruction videos [1, 31, 35] and ordering constraint [4, 37]. Recently, unconstrained query-based moment
retrieval has attracted a lot of attention [6, 10, 13, 14, 22, 23, 42].
These methods are mainly based on a sliding window framework,
which first samples candidate moments and then ranks these moments. Hendricks et al. [13] propose a moment context network
to integrate global and local video features for natural language
retrieval, and the subsequent work [14] considers the temporal
language by explicitly modeling the context structure of videos.
Gao et al. [10] develop a cross-modal temporal regression localizer
to estimate the alignment scores of candidate moments and textual
query, and then adjust the boundaries of high-score moments. With
the development of attention mechanism in the field of vision and
language interaction [2, 46], Liu et al. [22] advise a memory attention to emphasize the visual features and simultaneously utilize the
context information. And similar attention strategy [23] is designed
to highlight the crucial part of query contents. From the holistic
view, Chen et al. [6] capture the evolving fine-grained frame-byword interactions between video and query. Xu et al. [42] introduce
a multi-level model to integrate visual and textual features earlier
and further re-generate queries as an auxiliary task.
Unlike these previous methods, we propose a novel cross-modal
interaction network to consider three critical factors for querybased moment retrieval, including the syntactic structure of natural
language queries, long-range semantic dependencies in video context and the sufficient cross-modal interaction.

CROSS-MODAL INTERACTION NETWORKS

As Figure 2 illustrates, our cross-modal interaction networks consist of four components: 1) the syntactic GCN module leverages the
syntactic structure to enhance the query representation learning; 2)
the multi-head self-attention module captures long-range semantic
dependencies from video context; 3) the multi-stage cross-modal interaction module aggregates syntactic-aware query representations
for each frame, emphasizes crucial contents and learns cross-modal
semantic representations; 4) the moment retrieval module finally
localizes the boundaries of target moments.

3.1

Problem Formulation

n ∈ V , where
We present a video as a sequence of frames v = {vi }i=1
vi is the feature of the i-th frame and n is the frame number of
the video. Each video is associated with a natural language query,
m ∈ Q, where q is the feature of the i-th
denoted by q = {qi }i=1
i
word and m is the word number of the query. The query description
corresponds to a target moment in the untrimmed video and we
denote the start and end boundaries of the target moment by τ =
(s, e) ∈ A. Thus, given the training set {V , Q, A}, our goal is to
learn the cross-modal interaction networks to predict the boundary
τ̂ = (ŝ, ê) of the most relevant moment during inference.

3.2

Syntactic GCN Module

In this section, we introduce the syntactic GCN module based on a
syntactic dependency graph. By passing information along the dependency edges between relevant words, we learn syntactic-aware
query representations for subsequent cross-modal interactions.
We first extract word features for the query using a pre-trained
Glove word2vec embedding [27], denoted by q = (q1, q2, . . . , qm ),

657

Session 7B: Multilingual and Cross-modal Retrieval

SIGIR ’19, July 21–25, 2019, Paris, France

where qi is the feature of the i-th word. After that, we develop a
bi-directional GRU networks (BiGRU) to learn the query semantic
representations. The BiGRU networks incorporate contextual information for each word by combining the forward and backward
GRU [7] . Sepcifically, we input the sequence of word features to
the BiGRU networks, and obtain the contextual representation of
each word, given by
f

f

f

hi = GRUq (qi , hi−1 ),
hbi = GRUbq (qi , hbi+1 ),
q
hi

=

f
[hi

(1)

Syntactic Dependency Graph

; hbi ],

1) along the edge direction

f

where GRUq and GRUbq represent the forward and backward GRU
q
networks, respectively. And the contextual representation hi is
the concatenation of the forward and backward hidden state at
the i-th step. Thus, we get the query semantic representations
q q
q
hq = (h1 , h2 , . . . , hm ).
Although the BiGRU networks have encoded temporal context
of word sequences, they still ignore the syntactic information of
natural language, which implies underlying dependencies between
word pairs. So we then advise the syntactic graph convolution
networks to leverages the syntactic dependencies for better query
understanding. We first build the syntactic dependency graph by an
NLP toolkit, where each word is regarded as a node and each dependency relation is presented as a directed edge. Formally, we denote
a query by a graph G = (V, E), where the node set V contains all
words and edge set E contains all directed syntactic dependencies
of word pairs. Note that we add the self-loop for each node into
the edge set. As the dependency relations have different types, the
directed edges also correspond to different labels, where the selfloop is given a unique label. The original GCN regard the syntactic
dependency graph as an undirected graph, denoted by
© Õ
ª
q
gi1 = ReLU ­
Wд hj + bд ®
«j ∈N(i)
¬

2) opposite to the edge direction

Figure 3: Three Directions of Information Passing.
types of information passing, which correspond to three transforд
д
д
mation matrix W1 ,W2 and W3 , respectively. On the other hand,
the lab(i, j) represents the label of edge (i, j) to select a distinct bias
vector for each type of dependencies. Next, we employ a residual
connection [12] to keep the original representation of each node,
given by
q

oi1 = gi1 + hi .
(4)
Furthermore, we stack a multi-layer syntactic GCN to adequately
explore the syntactic structure as follows.

 g1 = synGCN(hq ), o1 = g1 + hq


 g2 = synGCN(o1 ), o2 = g2 + o1

(5)
...



 gl = synGCN(ol −1 ), ol = gl + ol −1


(2)

where Wд is the transformation matrix, bд is the bisa vector and
ReLU is the rectified linear unit. The N (i) represents the set of
nodes with a dependency edge to node i or from node i (including
q
self-loop). And the hj is the original representation of node j from
the preceding modeling.
Although the original GCN enhances the word semantic representation by aggregating the clues from its neighbors, it fails to
leverage the direction and label information of edges. Thus, we
consider a syntactic GCN to exploit the directional and labeled
dependency edges between nodes, given by
© Õ
ª
д
q
д
gi1 = ReLU ­
Wdir (i,j) hj + bl ab(i,j) ®
«j ∈N(i)
¬

3) self-loop

By the syntactic GCN with l layers, we obtain the syntactic-aware
l ).
query representation ol = (ol1, ol2, . . . , om

3.3

Multi-Head Self-Attention Module

In this section, we present the multi-head self-attention module to
capture long-range semantic dependencies from video context. By
the self-attention method, each frame is able to interact not only
with adjacent frames but also with distant ones. And the multihead setting is beneficial to sufficiently understand the complicated
dependencies.
We first extract frame features from the untrimmed video by a
pre-trained 3D-ConvNet [39], denoted by v = (v1, v2, . . . , vn ). The
vi is the visual feature of the i-th frame. We then introduce the
multi-head self-attention based on the scaled dot-product attention,
which originally is proposed in the field of machine translation [40].
Scaled dot-product attention. We assume the input of the
scale dot-product attention is a sequence of queries Q ∈ Rdk ×nq ,

(3)

where dir (i, j) indicates the direction of edge (i, j): 1) a dependency
edge from node i to j; 2) a dependency edge from node j to i; 3)
a self-loop if i = j. Since there is no reason to assume the information transmits only along the syntactic dependency arcs [25],
we also allow the information to transmit in the opposite direction
of directed dependencies here. The Figure 3 describes the three

658

Session 7B: Multilingual and Cross-modal Retrieval

SIGIR ’19, July 21–25, 2019, Paris, France

keys K ∈ Rdk ×nk and values V ∈ Rdv ×nk , where nq , nk and nk
represent the number of queries, keys and values, and dk , dk and
dv are respective dimensions. The scaled dot-product attention is
then calculated by

each row of M, given by
exp(Mi j )
,
Mirjow = Ím
exp(Mik )
k=1

where Mirjow represents the correlation of the i-th frame and j-th
word. Next, we extract the crucial query clues for each frame based
on M r ow , given by
Í
r ow l
hsi = m
(11)
j=1 Mi j o j ,

⊤

Q K ⊤
Attention(Q, K, V) = Softmax( p )V ,
dk

(6)

where the Softmax operation is performed on every row. The values
are aggregated for each query according to the dot-product score
between the query and the corresponding key of values.
Multi-head attention. The multi-head attention consists of H
paralleled scaled dot-product attention layers. For each independent attention layer, the input queries, keys and values are linearly
projected to dk , dk and dv dimenstions. Concretely, the result of
multi-head attention is given by
MultiHead(Q, K, V) = WO Concat(head1, . . . , headH )
Q

V
where headi = Attention(Wi Q, WK
i K, Wi V)

where the hsi represents the aggregated query representation relevant to the i-th frame.
Cross-gated interaction. With the aggregated query representation hsi and frame semantic representation hvi , we then apply a
cross gate [9] to emphasize crucial contents and weaken inessential
parts. In the cross gate method, the gate of query representation
depends on the frame representation, and meanwhile the frame representation is also gated by its corresponding query representation,
denoted by

(7)

gvi = σ (Wv hvi + bv ),

Q

where Wi ∈ Rdk ×d1 , WiK ∈ Rdk ×d2 , WVi ∈ Rdv ×d3 , WO ∈
Rd4 ×H dv are linear projection matrices. The d 1 , d 2 ,d 3 are the initial
input dimensions and d 4 is the output dimension.
When the queries Q, keys K and values V are set to the same
video feature matrix V = [v1 ; v2 ; . . . ; vn ] ∈ Rd×n and the input
dimension is equal to the output dimension, we get a multi-head
self-attention method. Based on it, we obtain the self-attentive video
representation Vs = [vs1 ; vs2 ; . . . ; vsn ], given by
Vs = MultiHead(V, V, V) + V,

e
hsi = hsi ⊙ gvi ,
gsi = σ (Ws hsi + bs ),

(12)

e
hvi = hvi ⊙ gsi ,
where Wv , Ws are parameter matrices, bv and bs are the bias vectors, σ is the sigmoid function, and ⊙ represents element-wise multiplication. If the aggregated query representation hsi is irrelevant to
the frame semantic representation hvi , both the two representations
are filtered to decrease their influences on subsequent networks.
On the contrary, the cross gate can further enhance the effects of
relevant frame-query pairs.
Bilinear fusion. After the attentive aggregation and cross gate,
we propose a low-rank bilinear fusion method [17] to further exploit the cross-modal interaction between e
hsi and e
hvi . The original
bilinear fusion method is written by

(8)

where a residual connection is applies similar to syntactic GCN.
Here we establish frame-to-frame correlation among video sequences and the multi-head setting allows the attention operation
to aggregate information from different representation subspaces.
But the temporal modeling is still critical for video semantic understanding, we cannot only depend on the multi-head self-attention
and ignore contextual representation learning. Hence, we next
employ another BiGRU to learn the self-attentive video semantic
representations hv = (hv1 , hv2 , . . . , hvn ).

3.4

(10)

⊤
f s
f
fi j = e
hvi Wj e
hi + bj ,

(13)

where fi j represents the j-th dimension of the bilinear output at the
f
f
time step i, and the fi is the fusion result of e
hs and e
hv . the W , b

Multi-Stage Cross-Modal Interaction
Module

i

i

j

j

are the parameter matrix and the bias vectors for the j-th dimension.
We can note that the original bilinear fusion method requires too
many parameters and suffers from the heavy computation cost.
Thus, we replace it with the low-rank version [17], given by

In this section, we introduce the cross-modal interaction module to
exploit the potential relations of video and query contents, which
consists of the attentive aggregation, cross-gated interaction and
low-rank bilinear fusion.
Attentive aggregation. Given the syntactic-aware query reprel ) and self-attentive video semantic
sentations ol = (ol1, ol2, . . . , om
v
v
v
representations h = (h1 , h2 , . . . , hvn ), we apply a typical attention
mechanism to aggregate the query clues for each frame. Concretely,
we first compute the attention score between each pair of frame
and word, and obtain a video-to-query attention matrix M ∈ Rn×m .
The attention score of the i-th frame and j-th word is given by

fi = Pf (σ (Wv e
hvi ) ⊙ σ (Ws e
hsi )) + bf ,

(14)

where the fi is the biliner fusion result at the time step i.
Eventually, by the attentive aggregation, cross-gated interaction
and low-rank bilinear fusion, we obtain the cross-modal semantic
representations for each frame, denoted by f = (f1, f2, . . . , fn ).

3.5

Moment Retrieval Module

In this section, we present the moment retrieval module to simultaneously score a set of candidate moments with multi-scale windows
at each time step, and further adopt a temporal boundaries regression mechanism to adjust the moment boundaries.

v
m l
m
M(hvi , olj ) = w⊤tanh(Wm
(9)
1 hi + W2 o j + b ),
m
m
where Wm
1 , W2 are parameter matrices, b is the bias vector and
the w⊤ is the row vector. We then apply the softmax operation for

659

Session 7B: Multilingual and Cross-modal Retrieval

SIGIR ’19, July 21–25, 2019, Paris, France

Table 1: Summaries of ActivityCaption and TACoS Datasets,
including number of samples, average video duration, average target moment duration and average query length.

By the cross-modal interaction module, we get the the crossmodal semantic representations f = (f1, f2, . . . , fn ). To absorb the
contextual evidences, we likewise develop another BiGRU networks
f f
f
to learn the final semantic representations hf = (h1 , h2 , . . . , hn ).
We then pre-define a set of candidate moments with multi-scale
windows at each time step i, denoted by Ci = {(ŝi j , êi j )}kj=1 , where
(ŝi j , êi j ) = (i − w j /2, i + w j /2) are the start and end boundaries
of the j-th candidate moment at time i, w j is the width of j-th
moment and k is the number of moments. Note that we set the
fixed window width w j for j-th candidate moment at every time
step. Thus, we can simultaneously produce the confidence scores
for these moments at time i by a fully connected layer with sigmoid
nonlinearity, given by
f

csi = σ (Wc hi + bc )

(15)

where the csi ∈ Rk represents the confidence scores of k moments
at time i and csi j corresponds to the j-th moment. Likewise, we
produce the predicted offsets for these moments by
f
δˆi = Wo hi + bo

(16)

Query Len
13.48
13.58
12.02
13.16
Query Len
8.69
9.12
9.00
8.86

(20)

where α is a hyper-parameter to control the balance of two losses.
During inference, we simply choose the candidate moment with
the highest confidence score. If we need to select multiple moments
(i.e. Top K), we first rank all candidates according to their confidence
scores and adopt a non-maximum suppression (NMS) to select
moments in order.

(17)

4 EXPERIMENTS
4.1 Datasets

where we consider all candidate moments during alignment training, and apply the concrete IoU score rather than set 0 or 1 according
to a threshold value for every candidate. This setting is helpful for
distinguishing high-score candidates.
Regression loss. As these multi-scale temporal windows have
fixed widths, our candidate moments are restricted to discrete
boundaries. To go beyond this limitation, we apply a boundary
regression mechanism to adjust the temporal boundaries of highscore moments. Concretely, we fine-tune the localization offsets of
high-score moments by a regression loss. First, we define a set Ch of
high-score moments which IoU scores are larger than a high-score
threshold γ , then compute the start and end offset values for those
high-score moments as follows:
δs = s − ŝ, δ e = e − ê,

Number
10,146
4,589
4,083
18,818

L = Lal iдn + α Lr eд ,

Li j = (1 − IoUi j ) · loд(1 − csi j ) + IoUi j · loд(csi j )
n k
1 ÕÕ
Li j
nk i=1 j=1

Train
Valid
Test
All

ActivityCaption
Video Time Target Time
117.30
35.45
118.23
37.73
118.21
40.25
117.74
37.14
TACoS
Video Time Target Time
224.16
5.70
387.46
6.23
367.70
6.96
296.21
6.10

where Ch is the set of high-score moments, N is the size of Ch and
R represents the smooth L1 function.
With the alignment loss and regression loss, we eventually propose a multi-task loss to train the cross-modal interaction networks
in an end-to-end manner, denoted by

where the δˆi ∈ R2k represents the predicted offsets of k moments
at time i and δˆi j = (δˆs , δˆe ) corresponds to the j-th moment.
Alignment loss. We first adopt an alignment loss to make the
moment aligned to the target moment have high confidence scores
and the misaligned moment have low confidence scores. Formally,
we first compute the IoU (i.e. Intersection over Union) score IoUi j
of each candidate moment Ci j = (ŝi j , êi j ) with the target moment
(s, e). If the IoU score of a candidate is less than a clearing threshold
λ, we reset it to 0. Next, we calculate the alignment loss by

Laliдn = −

Train
Valid
Test
All

Number
37,421
17,505
17,031
71,957

We first introduce two public datasets for query-based moment
retrieval.
ActivityCaption [20]: The ActivityCaption dataset is originally
developed for the task of dense video caption, which contains 20,000
untrimmed videos and each video includes multiple natural language descriptions with temporal annotations. The video contents
of this dataset are diverse and open. For query-based moment retrieval, each description is regarded as a query and corresponds
to a target moment. Since the caption annotations of test data of
ActivityCaption are not publically available, we take the val_1 as
the validation set and val_2 as test data. The details of the ActivityCaption dataset are summarized in Table 1.
TACoS [28]: The TACoS dataset is developed onMPII Compositive [30] and only contains 127 videos. But each video of TACoS has
a large amount of temporal textual annotations. The contents of
TACoS are limited to cooking scenes, thus lack the diversity. Moreover, the videos of TACoS are longer but the target moments are
shorter than ActivityCaption, which make the query-based moment
retrieval harder. The details of this dataset are also summarized in
Table 1.

(18)

where (s, e) are the boundaries of the target moment, and (ŝ, ê) are
the boundaries of a high-score moment in Ch . Thus, the (δs , δ e )
denote its ground truth offsets and the predicted offsets (δˆs , δˆe )
are given by preceding fully connected layer. Next, we design the
regression loss as follows:
1 Õ
Lr eд =
(R(δs − δˆs ) + R(δ e − δˆe ))
(19)
N
Ch

660

Session 7B: Multilingual and Cross-modal Retrieval

SIGIR ’19, July 21–25, 2019, Paris, France

Table 2: Performance Evaluation Results on the ActivityCaption Dataset (n ∈ {1, 5} and m ∈ {0.3, 0.5, 0.7}).
Method
MCN
VSA-RNN
VSA-STV
CTRL
ACRN
QSPN
CMIN

4.2

R@1
IoU=0.3
39.35
39.28
41.71
47.43
49.70
52.13
63.61

R@1
IoU=0.5
21.36
23.43
24.01
29.01
31.67
33.26
43.40

R@1
IoU=0.7
6.43
9.01
8.92
10.34
11.25
13.43
23.88

R@5
IoU=0.3
68.12
70.84
71.05
75.32
76.50
77.72
80.54

R@5
IoU=0.5
53.23
55.52
56.62
59.17
60.34
62.39
67.95

Table 3: Performance Evaluation Results on the TACoS
Dataset (n ∈ {1, 5} and m ∈ {0.1, 0.3, 0.5}).

R@5
IoU=0.7
29.70
32.12
34.52
37.54
38.57
40.78
50.73

Implementation Details

MCN
VSA-RNN
VSA-STV
CTRL
ACRN
QSPN
CMIN

R@1
IoU=0.1
3.11
8.84
15.01
24.32
24.22
25.31
32.48

R@1
IoU=0.3
1.64
10.77
10.77
18.32
19.52
20.15
24.64

R@1
IoU=0.5
1.25
4.78
7.56
13.30
14.62
15.23
18.05

R@5
IoU=0.1
3.11
19.05
32.82
48.73
47.42
53.21
62.13

R@5
IoU=0.3
2.03
13.90
23.92
36.69
34.97
36.72
38.46

R@5
IoU=0.5
1.25
9.10
15.50
25.42
24.88
25.30
27.02

moments of the query qi has IoU > m, and Nq is the total number
of testing queries.

In this section, we introduce some implementation details of our
CMIN method, including the data preprocessing and model setting.
Data Preprocessing. We first resize every frame of videos to
112 × 112 and extract the visual features by a pre-trained 3DConvNet [39]. Specifically, we define continuous 16 frames as a
unit and each unit overlaps 8 frames with adjacent units. We then
input the units to the pre-trained 3D-ConvNet and obtain 4,096
dimension features for each unit. We next reduce the dimensionality of features from 4,096 to 500 using PCA, which is helpful for
decreasing model parameters. These 500-d features are used as the
frame features of our CMIN. Since some videos are overlong, we
uniformly downsample their feature sequences to 200.
For natural language queries, we first extract the syntactic dependency graph using the library of NLTK [3] and employ the
pre-trained Glove word2vec [27] to extract the embedding features
for each word token. The dimension of word features is 300.
Model Setting. In our CMIN, we sample k candidate moments
with multi-scale windows at each time step. Concretely, we set
7 window widths of [16, 32, 64, 96, 128, 160, 196] for the ActivityCaption dataset and 4 window widths of [8, 16, 32, 64] for TACoS.
Thus, we have 1,400 samples for each video on ActivityCaption and
800 samples on TACoS. Note that we cut off candidate examples
that are beyond the boundaries of videos. We then set the clearing
threshold λ to 0.3, the high-score threshold γ to 0.7, and the balance
hyper-parameter α to 0.001. Moreover, the dimension of the hidden
state of BiGRU networks is set to 512 (256 for one direction). The
dimensions of the linear matrice in the multi-head self-attention
and bilinear fusion are also set to 512. During training, we adopt an
adam optimizer [8] to minimize the multi-task loss and the learning
rate is set to 0.001. We employ a mini-batch method and the batch
size is 128.

4.3

Method

4.4

Performance Comparisons

We compare our proposed CMIN method with some existing stateof-the-art methods to verify the effectiveness.
• MCN [13]: The MCN method adopts a moment context network to integrate local and global moment features for querybased moment retrieval.
• VSA-RNN and VSA-STV [10]: The two methods are the
extensions of the DVSA model [16]. They both simply transforms the visual features of candidate moments and query
features into a common space, and then estimate the correlation scores to select the most relevant one. The VSA-RNN
applies a LSTM network to encode queries and VSA-STV
adopts the off-the-shelf skip-thought [19] feature extractor.
• CTRL [10]: The CTRL method proposes a cross-modal temporal regression localizer to estimate the alignment scores
of candidate moments and textual queries by leveraging contextual contents of these moments, and then adjust the start
and end boundaries of high-score moments.
• AMRN [22]: The AMRN method emphasizes the visual moment features by attentive contextual contents and develops
a cross-modal feature representation.
• QSPN [42]: The QSPN method introduces a multi-level model
to integrate visual and textual features earlier with an attention mechanism, learn spatio-temporal visual representations and further re-generate queries as the auxiliary task.
The former three approaches only focus on the visual features
within each moment and ignore the contextual information. And
the latter three approaches incorporate the contextual evidence to
improve the retrieval performance, where the AMRN utilizes the
attention mechanism to filter irrelevant context and QSPN further
develops an early interaction strategy for cross-modal features.
Table 2 and Table 3 show the overall performance evaluation
results of our CMIN and all baselines on ActivityCaption and TACoS
datasets, respectively. We choose the evaluation criteria “R@n,
IoU=m” with n ∈ {1, 5}, m ∈ {0.3, 0.5, 0.7} for ActivityCaption
and n ∈ {1, 5} , m ∈ {0.1, 0.3, 0.5} for TACoS. Note that we report
the baseline performance based on either their original paper or
our implementation by selecting the higher one. The experimental
results reveal a number of interesting points:

Evaluation Criteria

To measure the retrieval performance of our CMIN and baselines,
we adopt the “R@n, IoU=m” as evaluation criteria, which are proposed in [10]. Concretely, we first calculate the IoU (i.e. Intersection
over Union) between the selected moment and ground-truth moment, and the “R@n, IoU=m” means the percentage of at least one
of top-n selected moments having IoU larger than m. The metric
is on the query level, so the overall performance is the average
ÍNq
among all the queries, denoted by R(n, m) = N1q i=1
r (n, m, qi ),
where the r (n, m, qi ) represents whether one of the top-n selected

661

Session 7B: Multilingual and Cross-modal Retrieval

SIGIR ’19, July 21–25, 2019, Paris, France

Table 4: Performance Evaluation Results of Ablation Model
on the ActivityCaption dataset.

• While modeling moment features, the MCN applies a meanpooling operation to aggregate all features of video sequences
as context of the current moment, which may introduce
noises into the moment representations and degrade the
retrieval accuracy. Thus, the MCN achieves the worst performance on all criteria.
• The context based methods CTRL, ACRN, QSPN and CMIN
outperform the simple SVA-STV and VSA-RNN, which suggest the context modeling is crucial for high-quality moment
retrieval. And the performance of the VSA-STA is slightly
better than VSA-RNN, demonstrating the skip-thought feature extractor is helpful for query understanding.
• The QSPN adopts an attention mechanism to fuse the visual and textual features, which develop the early interactions between moments and queries. The fact that the QSPN
achieves better performance than CTRL and ACRN verifies
the cross-modal interaction is critical for query-based moment retrieval.
• On all the criteria of two datasets, the CMIN not only outperforms all previous state-of-the-art baselines, but also
achieves tremendous improvements, especially on ActivityCaption. These results verify the effectiveness of our syntactic GCN, multi-head self-attention and multi-stage crossmodal interaction.

Method
w/o. GCN
w/o. SA
w/o. CG
w/o. BF
full

R@1
IoU=0.5
40.84
41.56
41.21
41.89
43.40

R@1
IoU=0.7
21.79
22.36
22.01
22.12
23.88

R@5
IoU=0.3
78.23
79.43
78.62
79.27
80.54

R@5
IoU=0.5
65.67
66.91
65.99
66.21
67.95

R@5
IoU=0.7
45.43
48.12
46.89
47.92
50.73

Table 5: Performance Evaluation Results of Ablation Model
on the TACoS dataset.
Method
w/o. GCN
w/o. SA
w/o. CG
w/o. BF
full

R@1
IoU=0.1
30.54
30.21
31.96
32.01
32.48

R@1
IoU=0.3
23.22
23.02
23.59
24.79
24.64

R@1
IoU=0.5
10.03
16.87
17.47
17.61
18.05

R@5
IoU=0.1
57.69
55.54
61.87
61.59
62.13

R@5
IoU=0.3
37.12
36.6
38.11
38.23
38.46

R@5
IoU=0.5
26.16
25.37
26.79
26.75
27.02

The ablation results on ActivityCaption and TACoS datasets are
shown in Table 4 and Table 5, respectively. By analyzing the ablation
results, we can find several interesting points:

Moreover, we can find that the overall experimental results on
TACoS are lower than ActivityCaption, and meanwhile, the CMIN
can only achieve a smaller improvement on TACoS. That is because
the videos are longer and the target moments are shorter on TACoS
as shown in Table 1, which increase the number of potential candidate moments and make this task harder. Moreover, the invariant
cooking scenes and shorter query description may also improve the
retrieval difficulty. Since invariant scenes require better discrimination ability and the short query is hard to describe a moment
clearly.

4.5

R@1
IoU=0.3
60.12
61.22
60.57
61.32
63.61

• The CMIN(full) outperforms all ablation models on both ActivityCaption and TACoS datasets, which demonstrates the
syntactic GCN, multi-head self-attention, cross gate and lowrank bilinear fusion are all helpful for query-based moment
retrieval.
• The CMIN(w/o. GCN) achieves the worst performance on
AcitivityCaption, and also have poor results on TACoS, indicating the utilization of syntactic structure is critical for
query semantic understanding and subsequent modeling.
• All ablation models still yield better results than all baselines. This fact demonstrates that our comprehensive retrieval framework is suitable for this task and the excellent
performance does not only depend on a key component.

Ablation Study

To prove the contribution of each component of our CMIN method,
we next conduct some ablation studies on the syntactic GCN, multihead self-attention and multi-stage cross-modal interaction. Concretely, we discard one component at a time to generate an ablation
model as follows.
• CMIN(w/o. GCN): We first remove the syntactic GCN layer
from the query representation learning and take the query
representations from BiGRU networks as the input of multistage cross-modal interaction module.
• CMIN(w/o. SA): We then discard the multi-head self-attention
from the video representation learning to validate the importance of long-range semantic dependency modeling.
• CMIN(w/o. CG): We next remove the cross gate in the multistage cross-modal interaction module, directly applying the
bilinear fusion for frame representations and aggregation
query representations.
• CMIN(w/o. BF): We finally replace the low-rank bilinear
fusion method with a simple concatenation for query and
video features.

662

Moreover, for the syntactic GCN module, the number of stacked
layers is a crucial hyper-parameter. Therefore, We further explore
the effect of this hyper-parameter by varying the number of layers from 1 to 5. Figure 4 and Figure 5 shows the impact of layer
number on ActivityCaption and TACoS datasets. Here we select
“R@1,IoU=0.3” and “R@1,IoU=0.5” as evaluation criteria. From the
tables, we note that the CMIN achieves the best performance while
the number of layers is set to 2, and stacking too many or too few
layers will both affect the performance of query-based moment
retrieval. Because only one syntactic GCN layer cannot sufficiently
leverage the syntactic dependencies of natural language queries
and too many syntactic GCN layers will result in over-smoothing,
that is, each word representation converges to the same value.

4.6

Qualitative Analysis

To qualitatively validate the effectiveness of the CMIN method, we
display several typical examples of query-based moment retrieval.
Figure 6 and Figure 7 show the retrieval results of the CMIN method

44

44

43.5

43.5

R@1 IoU=0.5

R@1 IoU=0.3

Session 7B: Multilingual and Cross-modal Retrieval

43
42.5
42

CMIN

41.5
41

SIGIR ’19, July 21–25, 2019, Paris, France

Query : The boy drops the cloths and takes the iron away before the baby can pick it up.

43

Ground Truth :

42.5

QSPN :

41.5

CMIN :

41
1

2

3

4

117.4s

114.12s

CMIN

42

5

1

2

GCN layer

3

4

116.12s

112.42s
113.56s

117.85s

Query : The female athlete jumped over the pole and wave at everyone.

5

GCN layer

Figure 4: Effect of the Number of Stacked Syntactic GCN layers on the ActivityCaption Dataset.

Ground Truth :

24.5

R@1 IoU=0.5

R@1 IoU=0.3

117.45s

26.98s

18.5

CMIN

24
23.5
23
22.5

18

Figure 6: Examples on the ActivityCaption dataset.

CMIN

17.5

Query : After getting out the juicer, he juices the ﬁrst orange half.

17
16.5
16

1

2

3

4

GCN layer

5

1

2

3

4

5

Ground Truth :

GCN layer

120.34s

74.56s

QSPN :

129.31s

79.1s

CMIN :

Figure 5: Effect of the Number of Stacked Syntactic GCN layers on the TACoS Dataset.

117.78s

72.32s

Query : She washes herb stems in the sink before placing them on the cuttingboard.

and the best baseline QSPN on ActivityCaption and TACoS datasets,
respectively. We can find that natural language queries are very
diverse and often contain successive temporal actions. By intuitive
comparison, the CMIN can retrieve more accurate boundaries of
target moments than QSPN. Moreover, the retrieval precision on
TACoS is lower than ActivityCaption, which is consistent with
previous qualitative evaluations.
Furthermore, as the fundamental component of our multi-stage
cross-modal interaction module, the video-to-query attentive aggregation builds a bridge between video and query information.
Thus, we demonstrate how the attentive aggregation mechanism
works to further understand the interaction process. As shown in
Figure 8, the video-to-query attention results are visualized using a
thermodynamic diagram, where the darker color means the higher
correlation of the pair of frame and word representations. We note
that each frame can attend the semantically related words and ignore these irrelevant words. For example, the word “line” has the
highest attention score over the query for the fourth frame. This
suggests the attentive aggregation strategy effectively establishes
the relationship between visual and textual information, and is
helpful for high-quality moment retrieval.

5

114.12s

21.56s

CMIN :
25

117.4s

25.01s

QSPN :

Ground Truth :
QSPN :
CMIN :

127.62s

126.29s

120.1s

133.34s
130.72s

123.04s

Figure 7: Examples on the TACoS dataset.
They

end

their

routine

and

line

up

against

the

wall

Figure 8: The Video-to-Query Attention Results in the MultiStage Cross-Modal Interaction Module

CONCLUSION
to explore the potential relations of video and query contents. The
extensive experiments on ActivityCaption and TACoS datasets
demonstrate the effectiveness of our proposed method.

In this paper, we propose a novel cross-modal interaction network
for query-based moment retrieval, which considers three critical
factors of this task, including the syntactic structure of natural language queries, long-range semantic dependencies in video context
and the fine-grained cross-modal interaction. Specifically, we advise
a syntactic GCN to leverage the syntactic structure of queries for
fine-grained representation learning, then propose a multi-head
self-attention to capture long-range semantic dependencies from
video context, and employ a multi-stage cross-modal interaction

ACKNOWLEDGMENTS
This work was supported by the National Natural Science Foundation of China under Grant No.61602405, No.U1611461, No.61751209
and No.61836002, Sponsored by Joint Research Program of ZJU and
Hikvision Research Institute.

663

Session 7B: Multilingual and Cross-modal Retrieval

SIGIR ’19, July 21–25, 2019, Paris, France

REFERENCES

[24] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille,
and Kevin Murphy. 2016. Generation and comprehension of unambiguous object
descriptions. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. 11–20.
[25] Diego Marcheggiani and Ivan Titov. 2017. Encoding sentences with graph convolutional networks for semantic role labeling. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing.
[26] Mayu Otani, Yuta Nakashima, Esa Rahtu, Janne Heikkilä, and Naokazu Yokoya.
2016. Learning joint representations of videos and sentences with web image
search. In Proceedings of the European Conference on Computer Vision. Springer,
651–667.
[27] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove:
Global vectors for word representation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. 1532–1543.
[28] Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt
Schiele, and Manfred Pinkal. 2013. Grounding action descriptions in videos.
Transactions of the Association of Computational Linguistics 1 (2013), 25–36.
[29] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn:
Towards real-time object detection with region proposal networks. In Advances
in neural information processing systems. 91–99.
[30] Marcus Rohrbach, Michaela Regneri, Mykhaylo Andriluka, Sikandar Amin, Manfred Pinkal, and Bernt Schiele. 2012. Script data for attribute-based recognition
of composite activities. In Proceedings of the European Conference on Computer
Vision. Springer, 144–157.
[31] Ozan Sener, Amir R Zamir, Silvio Savarese, and Ashutosh Saxena. 2015. Unsupervised semantic parsing of video collections. In Proceedings of the IEEE International
Conference on Computer Vision. 4480–4488.
[32] Zheng Shou, Dongang Wang, and Shih-Fu Chang. 2016. Temporal action localization in untrimmed videos via multi-stage cnns. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. 1049–1058.
[33] Bharat Singh, Tim K Marks, Michael Jones, Oncel Tuzel, and Ming Shao. 2016.
A multi-stream bi-directional recurrent neural network for fine-grained action
detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. 1961–1970.
[34] Richard Socher, Andrej Karpathy, Quoc V Le, Christopher D Manning, and Andrew Y Ng. 2014. Grounded compositional semantics for finding and describing
images with sentences. Transactions of the Association of Computational Linguistics 2, 1 (2014), 207–218.
[35] Young Chol Song, Iftekhar Naim, Abdullah Al Mamun, Kaustubh Kulkarni, Parag
Singla, Jiebo Luo, Daniel Gildea, and Henry A Kautz. 2016. Unsupervised Alignment of Actions in Video with Text Descriptions. In Proceedings of the International Joint Conference on Artificial Intelligence. 2025–2031.
[36] Chen Sun, Chuang Gan, and Ram Nevatia. 2015. Automatic concept discovery
from parallel text and visual corpora. In Proceedings of the IEEE International
Conference on Computer Vision. 2596–2604.
[37] Makarand Tapaswi, Martin Bauml, and Rainer Stiefelhagen. 2015. Book2movie:
Aligning video scenes with book chapters. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition. 1827–1835.
[38] Stefanie Tellex and Deb Roy. 2009. Towards surveillance video search by natural
language query. In Proceedings of the ACM International Conference on Image and
Video Retrieval. ACM, 38.
[39] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri.
2015. Learning spatiotemporal features with 3d convolutional networks. In
Proceedings of the IEEE International Conference on Computer Vision. 4489–4497.
[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information processing systems. 5998–6008.
[41] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
Lio, and Yoshua Bengio. 2018. Graph attention networks. In Proceedings of the
International Conference on Learning Representations.
[42] Huijuan Xu, Kun He, L Sigal, S Sclaroff, and K Saenko. 2019. Multilevel Language
and Vision Integration for Text-to-Clip Retrieval. In Proceedings of the American
Association for Artificial Intelligence, Vol. 2. 7.
[43] Ran Xu, Caiming Xiong, Wei Chen, and Jason J Corso. 2015. Jointly Modeling
Deep Video and Compositional Text to Bridge Vision and Language in a Unified
Framework. In Proceedings of the American Association for Artificial Intelligence,
Vol. 5. 6.
[44] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. 2018. Exploring visual relationship
for image captioning. In Proceedings of the European Conference on Computer
Vision. 684–699.
[45] Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xiaoou Tang, and Dahua
Lin. 2017. Temporal action detection with structured segment networks. In
Proceedings of the IEEE International Conference on Computer Vision.
[46] Zhou Zhao, Zhu Zhang, Shuwen Xiao, Zhou Yu, Jun Yu, Deng Cai, Fei Wu, and
Yueting Zhuang. 2018. Open-Ended Long-form Video Question Answering via
Adaptive Hierarchical Reinforced Networks.. In Proceedings of the International
Joint Conference on Artificial Intelligence. 3683–3689.

[1] Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Josef Sivic, Ivan
Laptev, and Simon Lacoste-Julien. 2016. Unsupervised learning from narrated
instruction videos. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition. 4575–4583.
[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson,
Stephen Gould, and Lei Zhang. 2018. Bottom-Up and Top-Down Attention for
Image Captioning and Visual Question Answering. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition.
[3] Steven Bird and Edward Loper. 2004. NLTK: the natural language toolkit. In
Proceedings of the ACL 2004 on Interactive poster and demonstration sessions.
Association for Computational Linguistics, 31.
[4] Piotr Bojanowski, Rémi Lajugie, Edouard Grave, Francis Bach, Ivan Laptev, Jean
Ponce, and Cordelia Schmid. 2015. Weakly-supervised alignment of video with
text. In Proceedings of the IEEE International Conference on Computer Vision.
4462–4470.
[5] Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Seybold, David A Ross, Jia
Deng, and Rahul Sukthankar. 2018. Rethinking the Faster R-CNN Architecture for
Temporal Action Localization. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. 1130–1139.
[6] Jingyuan Chen, Xinpeng Chen, Lin Ma, Zequn Jie, and Tat-Seng Chua. 2018.
Temporally Grounding Natural Sentence in Video. In Proceedings of the Conference
on Empirical Methods in Natural Language Processing. ACL, 162–171.
[7] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014.
Empirical evaluation of gated recurrent neural networks on sequence modeling.
In Advances in neural information processing systems.
[8] John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods
for online learning and stochastic optimization. Journal of Machine Learning
Research 12, Jul (2011), 2121–2159.
[9] Yang Feng, Lin Ma, Wei Liu, Tong Zhang, and Jiebo Luo. 2018. Video relocalization. In Proceedings of the European Conference on Computer Vision. 51–66.
[10] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. 2017. TALL: Temporal
Activity Localization via Language Query. In Proceedings of the IEEE International
Conference on Computer Vision. IEEE, 5277–5285.
[11] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. 2014. Rich
feature hierarchies for accurate object detection and semantic segmentation. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
580–587.
[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. 770–778.
[13] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell,
and Bryan Russell. 2017. Localizing moments in video with natural language. In
Proceedings of the IEEE International Conference on Computer Vision. 5803–5812.
[14] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and
Bryan Russell. 2018. Localizing Moments in Video with Temporal Language. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing.
ACL, 1380–1390.
[15] Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko, and
Trevor Darrell. 2016. Natural language object retrieval. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. 4555–4564.
[16] Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-semantic alignments for
generating image descriptions. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. 3128–3137.
[17] Jin-Hwa Kim, Kyoung-Woon On, Woosang Lim, Jeonghee Kim, Jung-Woo Ha,
and Byoung-Tak Zhang. 2017. Hadamard product for low-rank bilinear pooling.
In Proceedings of the International Conference on Learning Representations.
[18] Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with
graph convolutional networks. In Proceedings of the International Conference on
Learning Representations.
[19] Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun,
Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in
neural information processing systems. 3294–3302.
[20] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles.
2017. Dense-Captioning Events in Videos. In Proceedings of the IEEE International
Conference on Computer Vision. 706–715.
[21] Dahua Lin, Sanja Fidler, Chen Kong, and Raquel Urtasun. 2014. Visual semantic
search: Retrieving videos via complex textual queries. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition. 2657–2664.
[22] Meng Liu, Xiang Wang, Liqiang Nie, Xiangnan He, Baoquan Chen, and TatSeng Chua. 2018. Attentive moment retrieval in videos. In Proceedings of the
International ACM SIGIR Conference on Research and Development in Information
Retrieval. ACM, 15–24.
[23] Meng Liu, Xiang Wang, Liqiang Nie, Qi Tian, Baoquan Chen, and Tat-Seng Chua.
2018. Cross-modal Moment Localization in Videos. In Proceedings of the ACM
International Conference on Multimedia. ACM, 843–851.

664

