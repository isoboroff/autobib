Short Research Papers 1A: AI, Mining, and others

SIGIR ’19, July 21–25, 2019, Paris, France

Encoding Syntactic Dependency and Topical Information for
Social Emotion Classification
Chang Wang, Bang Wang, Wei Xiang and Minghua Xu*
Huazhong University of Science and Technology (HUST), Wuhan, China
wang chang,wangbang,xiangwei,xuminghua@hust.edu.cn

ABSTRACT
Social emotion classification is to estimate the distribution
of readers’ emotion evoked by an article. In this paper, we
design a new neural network model by encoding sentence
syntactic dependency and document topical information into
the document representation. We first use a dependency embedded recursive neural network to learn syntactic features
for each sentence, and then use a gated recurrent unit to
transform the sentences’ vectors into a document vector. We
also use a multi-layer perceptron to encode the topical information of a document into a topic vector. Finally, a gate layer
is used to compose the document representation from the
gated summation of the document vector and the topic vector.
Experiment results on two public datasets indicate that our
proposed model outperforms the state-of-the-art methods in
terms of better average Pearson correlation coefficient and
MicroF1 performance.

Figure 1: The framework of our proposed model.

CCS CONCEPTS

1

• Information systems → Sentiment analysis.

INTRODUCTION

Some early methods have been proposed for social emotion
classification, including the word-emotion models [2], which
discover the direct relations between words and emotions
based on the features of individual words. However, those
word-emotion models cannot distinguish different emotions
of a same word in different contexts. Some topic-emotion
models [1, 6] utilize topic models to discover topical information of a document and model the relations between topics
and emotions. However, they treat each word separately, yet
ignoring some inter-word information like semantics.
Recently, some neural network-based models have been
proposed for social emotion classification [4]. These models
can automatically learn text features and generate a semantical document representation based on the convolutional
neural network (CNN) [4] or recurrent neural network (RNN) [9]. But they have not fully utilized the syntactic feature
of a sentence and the topical information of a document.
We argue that the syntactic dependency relations between
words in a sentence are important for social emotion classification. Furthermore, we also support the claim in the existing
work that the document topical information would help to
distinguish the emotion of a same word in different contexts.
So we design a new neural network model to include the both
in document representation for social emotion classification.
Specifically, our proposed model includes a document encoding component and a topic encoding component. Fig. 1
presents the overall network framework. For document encoding, we design a dependency embedded recursive neural
network (DERNN) to encode each syntactic dependency

KEYWORDS
Social emotion classification; recursive neural network;
dependency embedding; topic model
ACM Reference Format:
Chang Wang, Bang Wang, Wei Xiang and Minghua Xu. 2019. Encoding Syntactic Dependency and Topical Information for Social
Emotion Classification. In Proceedings of the 42nd International
ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’19), July 21–25, 2019, Paris, France.
ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/
3331184.3331287
*

Chang Wang, Bang Wang and Wei Xiang are with the School of
Electronic, Information and Communications, HUST; Minghua Xu
is with the School of Journalism and Information Communication,
HUST. This work is supported in part by National Natural Science
Foundation of China (Grant No. 61771209)

Permission to make digital or hard copies of all or part of this work
for personal or classroom use is granted without fee provided that
copies are not made or distributed for profit or commercial advantage
and that copies bear this notice and the full citation on the first
page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee. Request permissions
from permissions@acm.org.
SIGIR ’19, July 21–25, 2019, Paris, France
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00
https://doi.org/10.1145/3331184.3331287

881

Short Research Papers 1A: AI, Mining, and others

SIGIR ’19, July 21–25, 2019, Paris, France

embedding 𝑥 and the dependency embeddings 𝑞1 and 𝑞2
are input into the parent DERNN unit whose output is the
hidden state ℎ of the parent node. The modeling process
starts from the bottom leaf nodes till the root node, and we
treat the hidden state of the root node as the sentence vector.
Let 𝑤𝑛 denote the 𝑛th word in a sentence and x𝑛 ∈ R𝑑𝑤 is
its word embedding, 𝐶(𝑤𝑛 ) is the child node set of 𝑤𝑛 . The
transition functions of the DERNN are as follows.
h̃𝑛

=

∑︁

h𝑗

(1)

q𝑗

(2)

𝑗∈𝐶(𝑤𝑛 )

q̃𝑛

𝑗∈𝐶(𝑤𝑛 )

Figure 2: The left part is an example of dependency
tree. The right part is the internal structure of the
DERNN unit.

f𝑗

relation in between words of a sentence into the sentence
vector in the lower layer, and we use a gated recurrent unit
(GRU) to encode a document vector in the upper layer. For
topic encoding, we apply a multi-layer perceptron (MLP) to
transform the document topical distribution obtained from
the Latent Dirichlet Allocation (LDA) model into a topic
vector. Afterwards, to let the network adaptively decide the
importance of the document vector and the topic vector, we
propose to use a gate layer to obtain the final document representation from them yet with a gate controlling mechanism.
Finally, we feed the document representation into a softmax
layer to generate the predicted social emotion distribution.
Experiment results on two public datasets have validated the
superiority of our proposed model over the state-of-the-art
ones in terms of higher average Pearson correlation coefficient
and MicroF1 performance.

2

=

∑︁

=

𝜎(W

(𝑓 )
(𝑖)

(𝑓 )

x𝑛 + U

i𝑛

=

𝜎(W

x𝑛 + U

u𝑛

=

𝑡𝑎𝑛ℎ(W

h𝑛

=

(𝑖)

(𝑓 )

h𝑗 + D

(𝑖)

h̃𝑛 + D
(𝑢)

(𝑢)

(𝑓 )

q𝑗 + b

(𝑖)

q̃𝑛 + b
(𝑢)

)

(3)

)

(4)
(𝑢)

x𝑛 + U h̃𝑛 + D q̃𝑛 + b
∑︁
𝑡𝑎𝑛ℎ(i𝑛 ⊙ u𝑛 +
f𝑗 ⊙ h𝑗 )

)

(5)
(6)

𝑗∈𝐶(𝑤𝑛 )

where 𝑗 ∈ 𝐶(𝑤𝑛 ) in Eq. (3), W(𝑓,𝑖,𝑢) , U(𝑓,𝑖,𝑢) , D(𝑓,𝑖,𝑢) , b(𝑓,𝑖,𝑢)
are learnable parameters and h𝑛 ∈ R𝑑ℎ is the hidden state
of 𝑤𝑛 . q𝑗 ∈ R𝑑𝑞 is the dependency embedding of the dependency relation between the 𝑗th child node and the parent
node. Because leaf nodes have no child nodes, they share an
additional dependency embedding. All dependency embeddings are xavier uniform initialized and updated during the
network training. The input gate i𝑛 and the forget gate f𝑗
are both dependent on the dependency embeddings. This
allows the DERNN to forget the child words with unimportant dependency relations (like the punctuation relation)
and remember the words with important relations (like the
subject-predicate relation), to compose a sentence vector.
Regarding a document as a sequence of sentences, we use
a gated recurrent unit to obtain a document vector from sentence vectors. The GRU transition functions are as follows.

THE PROPOSED METHOD

z𝑚

1) Document Encoding: Since a document consists of
one or more sentences, we first encode the syntactic dependency information in one sentence into the sentence vector in the
lower layer network and then compose the document vector
from sentence vectors in the upper layer network. We design
a dependency embedded recursive neural network (DERNN)
to model a sentence. It learns an embedding representation
for each dependency relation (called dependency embedding)
and encodes the syntactic dependency information into the
sentence vector.
Before we input words into the network, we adopt a pretrained word2vec model to transform each word into a word
embedding, which is a low dimensional dense vector with
real values. To construct the DERNN network, we use a
dependency analysis tool for each sentence to obtain its
dependency tree, where each node represents a word and
each edge represents a dependency relation. The left part in
Fig. 2 presents an example of dependency tree. Each parent
node is connected with one or more child nodes and the
dependency relations between them are marked above the
edges. We model each node as a DERNN unit and its internal
structure is illustrated in the right part of Fig. 2. We can
see that the child hidden states ℎ1 and ℎ2 , the parent word

=

𝜎(W

(𝑧)
(𝑟)

s𝑚 + U
s𝑚 + U

(𝑧)

(𝑟)

(𝑧)

)

(7)

(𝑟)

)

(8)

h𝑚−1 + b

r𝑚

=

𝜎(W

u𝑚
h𝑚

=
=

𝑡𝑎𝑛ℎ(W
s𝑚 + U
(r𝑚 ⊙ h𝑚−1 ) + b
(1 − z𝑚 ) ⊙ h𝑚−1 + z𝑚 ⊙ u𝑚

(𝑢′ )

h𝑚−1 + b
(𝑢′ )

(𝑢′ )

)

(9)
(10)

where s𝑚 is the sentence vector of the 𝑚th sentence and
h𝑚 ∈ R𝑑ℎ′ is its hidden state, and 𝑑ℎ = 𝑑ℎ′ in our work. The
hidden state of the last sentence is viewed as the document
vector d.
2) Topic Encoding: Considering that social emotion is
often related to the document topic, we propose to encode the
document topical information into the document representation based on the LDA model, which regards a document
as a mixture over various latent topics. After training a LDA model, we obtain the topic probability distribution of
each document, denoted as p = {𝑝𝑟(𝑘)}𝐾
𝑘=1 , where 𝐾 is the
number of topics and 𝑘 the topic index. p is treated as the
original topic representation and then fed into a multi-layer
perceptron followed by a 𝑡𝑎𝑛ℎ activation layer to get the
topic vector t, formulated as follows:
t = 𝑡𝑎𝑛ℎ(W(𝑝) p + b(𝑝) )
𝑝

𝑑𝑡 ×𝐾

(11)

where W ∈ R
, 𝑑𝑡 is the dimension of the topic vector.
Notice that in our work, 𝑑𝑡 = 𝑑ℎ . The MLP layer is used to
discover the topic feature in the original topic distribution p.

882

Short Research Papers 1A: AI, Mining, and others

SIGIR ’19, July 21–25, 2019, Paris, France

provided by Google3 . Stanford Parser4 is used to construct
dependency trees. Since there are too many kinds of dependency relations in the parsing result of Stanford Parser, to
balance the occurrence number of each relation, we map all
relations into 9 categories according to universal dependencies v15 . Other experiment parameters are setting as follows:
For SinaNews and ISEAR, the topic number 𝐾 in LDA is 30
and 10, 𝑑ℎ = 𝑑𝑞 = 𝑑𝑡 are 200 and 100, and the learning rate
is both 0.001, the batch size is both 20.
Evaluation Metrics: Considering label distributions are
very imbalanced in the datasets, we adopt the Micro-averaged
F1 score (𝑀 𝑖𝑐𝑟𝑜𝐹 1) to reflect the accuracy of the predicted
top-ranked emotion label [3]. And the average Pearson correlation coefficient (𝐴𝑃 ) is used to measure the divergence
between the predicted emotion probability distribution and
the ground truth distribution [3].
{︃
∑︀
1, if 𝑒ˆ𝑑𝑡𝑜𝑝 = 𝑒𝑑𝑡𝑜𝑝 ,
𝑑∈𝒟𝑡𝑒𝑠𝑡 I𝑑
, I𝑑 =
𝑀 𝑖𝑐𝑟𝑜𝐹 1 =
,
|𝒟𝑡𝑒𝑠𝑡 |
0,
otherwise,
(17)
where 𝑒𝑑𝑡𝑜𝑝 is the actual top-ranked emotion triggered by an
article 𝑑 and 𝑒ˆ𝑑𝑡𝑜𝑝 is the predicted one. 𝒟𝑡𝑒𝑠𝑡 denotes the
testing set. 𝐴𝑃 between the predicted emotion distribution
ŷ and the ground truth distribution y:
∑︀
y, y)
𝑐𝑜𝑣(^
y, y)
𝒟𝑡𝑒𝑠𝑡 𝑟(^
, 𝑟(^
y, y) = √︀
. (18)
𝐴𝑃 =
|𝒟𝑡𝑒𝑠𝑡 |
𝑣𝑎𝑟(^
y)𝑣𝑎𝑟(y)

3) Gate Layer: After obtaining the document vector d
and the topic vector t, a straightforward strategy is to sum
them to get the final document representation. But we argue
that the syntactic dependency and topical information are not
always the same important for social emotion classification.
So we design a gate layer which has a document gate and
a topic gate to combine the two vectors. The transition
functions of the gate layer are computed as follows.
gd
gt
v

=
=
=

𝜎(W

(𝑔𝑑 )
(𝑔𝑡 )

(𝑔𝑑 )

d+U

(𝑔𝑡 )

(𝑔𝑑 )

t+b

(𝑔𝑡 )

𝜎(W
d+U
t+b
𝑡𝑎𝑛ℎ(gd ⊙ d + gt ⊙ t)

)

)

(12)
(13)
(14)

where gd is the document gate, gt is the topic gate and
W(𝑔𝑑 ,𝑔𝑡 ) , U(𝑔𝑑 ,𝑔𝑡 ) , b(𝑔𝑑 ,𝑔𝑡 ) are the learnable parameters. ⊙
represents element-wise multiplication. The gate layer allows
the network to adaptively assign different importance to the
document vector and the topic vector, when composing the
final document representation v ∈ R𝑑𝑣 , 𝑑𝑣 = 𝑑𝑡 = 𝑑ℎ .
4) Social Emotion Classification: In this part, we first
add a linear layer to transform the document representation
v into a label vector whose dimension is the number of the
emotion labels 𝐸. Then the label vector is fed into a softmax
layer to get the predicted probability vector ŷ.
ŷ = 𝑠𝑜𝑓 𝑡𝑚𝑎𝑥(W(𝑙) v + b(𝑙) )

(15)

where W𝑙 ∈ R𝐸×𝑑𝑣 and b𝑙 ∈ R𝐸 are the parameters in the
linear layer.
Considering the evaluation objective in social emotion
classification is an emotion probability distribution, other
than a single most likely emotion label. So for the network
training, we use the Kullback-Leibler divergence between
the gold emotion distribution y and the predicted emotion
distribution ŷ as the loss function:

where 𝑟 denotes the Pearson correlation coefficient and 𝑐𝑜𝑣
denotes the covariance operation.
Comparison models: We compare our proposed model,
Gated DR-G-T (Gated DERNN-GRU-Topic), with the following models. The first group contains the baseline models from
𝐸
1 ∑︁
the literature, including the SWAT [2], Emotion Topic Model
𝐿𝑜𝑠𝑠(^
y, y) =
𝑦𝑐 (log(𝑦𝑐 ) − log(ˆ
𝑦𝑐 ))
(16)
(ETM) [1], Contextual Sentiment Topic Model (CSTM) [6],
𝐸 𝑐=1
Social Opinion Mining model (SOM) [5], 1-HNN-BTM [3],
Finally, we train the whole network with the Adam optimizer.
and CNN and CNN-SVM [5].
The second group is the LSTM models implemented by
3 EXPERIMENT
us.Hierarchical LSTM (H-LSTM) is a hierarchical structure
Datasets: We experiment on two public datasets: SinaNews [5] of two LSTM networks which are used for sentence modeling
and ISEAR [7]. SinaNews is a Chinese dataset which contains
and document modeling, respectively. Child-Sum Tree-LSTM
5,258 hot news collected from the social channel of the news
- LSTM (T-LSTM) uses the Child-Sum Tree-LSTM (a treewebsite (www.sina.com). To be consistent with the baseline
structured LSTM proposed by [8]) to model sentences and
methods [5], we use 3,109 articles as the training set and 2,149
the LSTM to model the document.
articles as the testing set. ISEAR is an English dataset which
The third group is the variants of Gated DR-G-T for modconsists of 7,666 samples. Each sample is a paragraph of text
el analysis. The RNN-GRU (R-G) uses a recursive neural
tagged by an emotion label. Like [3], we randomly select 60%
network in the lower layer, which removes dependency emof the samples as the training set and the remaining 40% as
beddings compared to DERNN. The GRU is used to model
the testing set.
the document. The DERNN-GRU (DR-G) uses the DERNSettings: For the Chinese dataset SinaNews, we train a
N proposed in this work to model sentences and the GRU
100-dimensional word2vec model with the Chinese Wikipedito model the document. Compared to Gated DR-G-T, the
a corpus1 . The LTP toolkit2 provided by HIT is used for
DERNN-GRU-Topic (DR-G-T) does not use the gate layer.
dependency parsing. There are in total 15 dependency relaExperimental Results: Table 1 presents the experiment
tions predefined in the LTP, and we define one dependency
results, where our proposed Gated DR-G-T model outperembedding for each relation. For the English dataset ISEAR,
forms all the other models. Specifically, compared to the best
we directly use the 300-dimensional English word2vec model
3

https://code.google.com/archive/p/word2vec/
https://nlp.stanford.edu/software/lex-parser.shtml
5
http://universaldependencies.org/docsv1/u/dep/all.html

1

4

https://dumps.wikimedia.org/
2
https://www.ltp-cloud.com/

883

Short Research Papers 1A: AI, Mining, and others

SIGIR ’19, July 21–25, 2019, Paris, France

Table 1: Experiment results on the two datasets.
Models
SWAT [2]
ETM [1]
CSTM [6]
1-HNN-BTM [3]
CNN [5]
CNN-SVM [5]
SOM [5]

SinaNews
MicroF1 AP
38.97
0.40
54.19
0.49
40.74
0.43
51.23
52.63
58.59
0.64

than R-G on SinaNews, where the latter does not concern
the specific dependency relations. This indicates that encoding syntactic dependency contributes to the performance
improvement. But the result of DR-G is worse than R-G in
ISEAR. A possible reason is that there are too many short
texts in ISEAR and the number of sentences is not enough
to well train the dependency embedding. Another observation is that although DR-G-T uses the topical information,
it does not show an obvious advantage over DR-G on the
two datasets. Although the dependency feature and topical
information enrich the semantics of a document representation, they should be carefully integrated. The straightforward
summation might add some noises into the final document
representation. Since the proposed Gated DR-G-T applies
a gate layer to control their weights in the final document
representation, the results reveal that it can perform better
than the naive DR-G-T.

ISEAR
MicroF1 AP
26.29
0.21
48.79
0.35
28.23
0.19
51.21
0.40
-

H-LSTM
T-LSTM

61.69
62.57

0.68
0.68

59.13
59.98

0.56
0.57

R-G
DR-G
DR-G-T

63.55
64.48
64.39

0.69
0.70
0.70

60.04
59.55
60.06

0.57
0.56
0.57

Gated DR-G-T

65.20

0.71

60.44

0.57

4

CONCLUSION

In this paper, we have proposed a new neural network
model for social emotion classification. For document encoding, the DERNN encodes syntactic dependency relations into
sentence vectors and the GRU transforms sentence vectors
into a document vector. For topic encoding, the MLP transforms the document topic distribution into a topic vector.
Finally, a gate layer is used to compose the final document
representation from the gated summation of document vector
and topic vector. Experiments on two public datasets show
that compared with the state-of-the-art models, the proposed
model can improve the classification performance in terms of
higher MicroF1 and AP on two public datasets.

Figure 3: Comparison between the proposed Gated
DR-G-T, its variants, the best LSTM Model and the
best model from the literature. The left vertical axis
and the bar stand for 𝑀 𝑖𝑐𝑟𝑜𝐹 1; The right vertical
axis and the line stand for 𝐴𝑃 .

REFERENCES
[1] S. Bao, S. Xu, L. Zhang, R. Yan, Z. Su, D. Han, and Y. Yu,
“Mining social emotions from affective text,” IEEE transactions
on knowledge and data engineering, vol. 24, no. 9, pp. 1658–1670,
2012.
[2] P. Katz, M. Singleton, and R. Wicentowski, “Swat-mp: the semeval2007 systems for task 5 and task 14,” in Proceedings of the 4th
international workshop on semantic evaluations. Association
for Computational Linguistics, 2007, pp. 308–313.
[3] X. Li, Y. Rao, H. Xie, R. Y. K. Lau, J. Yin, and F. L. Wang, “Bootstrapping social emotion classification with semantically rich hybrid
neural networks,” IEEE Transactions on Affective Computing,
vol. 8, no. 4, pp. 428–442, 2017.
[4] X. Li, Y. Rao, H. Xie, X. Liu, T.-L. Wong, and F. L. Wang, “Social
emotion classification based on noise-aware training,” Data &
Knowledge Engineering, 2017.
[5] X. Li, Q. Peng, Z. Sun, L. Chai, and Y. Wang, “Predicting social emotions from readers perspective,” IEEE Transactions on
Affective Computing, no. 1, pp. 1–1, 2017.
[6] Y. Rao, “Contextual sentiment topic model for adaptive social
emotion classification,” IEEE Intelligent Systems, no. 1, pp. 41–
47, 2016.
[7] K. R. Scherer and H. G. Wallbott, “Evidence for universality
and cultural variation of differential emotion response patterning.”
Journal of personality and social psychology, vol. 66, no. 2, p. 310,
1994.
[8] K. S. Tai, R. Socher, and C. D. Manning, “Improved semantic representations from tree-structured long short-term memory networks,”
arXiv preprint arXiv:1503.00075, 2015.
[9] X. Zhao, C. Wang, Z. Yang, Y. Zhang, and X. Yuan, “Online
news emotion prediction with bidirectional lstm,” in International
Conference on Web-Age Information Management. Springer,
2016, pp. 238–250.

model from the literature (cf., Fig. 3), Gated DR-G-T improves MicroF1 by 6.61% and AP by 0.07 on SinaNews, and
improves MicroF1 by 9.23% and AP by 0.17 on ISEAR. We
attribute the improvements to the leverage of encoding the
syntactic dependency and topical information into document
representation. We note that the performance of the two LSTM models performs better than the baseline models from the
literature. A possible reason is that they adopt a hierarchical
structure and can learn the word-level and sentence-level
semantic features respectively. Although the T-LSTM model
considers the syntactic dependency structure of a sentence,
it ignores the specific dependency relation in between words.
On the one hand, the DERNN in our model can encode each
specific dependency relation into sentence vectors. On the
other hand, our model also values the topical information
via LDA-based topic encoding. Furthermore, with the help
of the gate layer, the final document representation can well
balance the syntactic dependency and topical information
for social emotion classification.
Fig. 3 compares the results between the Gated DR-G-T
and its variants. We first observe that DR-G performs better

884

