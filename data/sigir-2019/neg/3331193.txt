Session 5A: Conversation and Dialog

SIGIR ’19, July 21–25, 2019, Paris, France

EnsembleGAN: Adversarial Learning for Retrieval-Generation
Ensemble Model on Short-Text Conversation
Jiayi Zhang∗

Chongyang Tao∗

Zhenjing Xu

Turing Robot
zhangjiayi@uzoo.cn

ICST, Peking University
chongyangtao@pku.edu.cn

Turing Robot
xuzhenjing@uzoo.cn

Qiaojing Xie

Wei Chen

Rui Yan†

Turing Robot
xieqiaojing@uzoo.cn

Turing Robot
weichen@uzoo.cn

ICST, Peking University
ruiyan@pku.edu.cn

ABSTRACT

ACM Reference Format:
Jiayi Zhang, Chongyang Tao, Zhenjing Xu, Qiaojing Xie, Wei Chen, and Rui
Yan. 2019. EnsembleGAN: Adversarial Learning for Retrieval-Generation
Ensemble Model on Short-Text Conversation. In Proceedings of the 42nd
International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’19), July 21–25, 2019, Paris, France. ACM, New York,
NY, USA, 10 pages. https://doi.org/10.1145/3331184.3331193

Generating qualitative responses has always been a challenge for
human-computer dialogue systems. Existing dialogue systems generally derive from either retrieval-based or generative-based approaches, both of which have their own pros and cons. Despite
the natural idea of an ensemble model of the two, existing ensemble methods only focused on leveraging one approach to enhance
another, we argue however that they can be further mutually enhanced with a proper training strategy. In this paper, we propose
ensembleGAN, an adversarial learning framework for enhancing
a retrieval-generation ensemble model in open-domain conversation scenario. It consists of a language-model-like generator, a
ranker generator, and one ranker discriminator. Aiming at generating responses that approximate the ground-truth and receive high
ranking scores from the discriminator, the two generators learn to
generate improved highly relevant responses and competitive unobserved candidates respectively, while the discriminative ranker
is trained to identify true responses from adversarial ones, thus
featuring the merits of both generator counterparts. The experimental results on a large short-text conversation data demonstrate
the effectiveness of the ensembleGAN by the amelioration on both
human and automatic evaluation metrics.

1

CCS CONCEPTS
• Information systems → Retrieval models and ranking; Web
applications; • Computing methodologies → Natural language
processing;

KEYWORDS
Generative adversarial network, short-text conversation, ensemble
method, retrieval-based conversation, generation-based conversation
∗ Equal

INTRODUCTION

Natural language human-computer conversation has long been an
attractive but challenging task in artificial intelligence (AI), for it
requires both language understanding and reasoning [17]. While
early works mainly focused on domain-specific scenarios such as
ticket booking, the open-domain chatbot-human conversations has
gained popularity recently, not only for their commercial values
(e.g., Xiaoice1 from Microsoft), but for the rapid growth of online
social media as well, along with tremendous data available for datadriven deep learning methods to be proved worthwhile. Current
conversation systems could be generally divided into two different
categories, namely the retrieval-based and the generative-based
approach.
Given an user input utterance (also called a query), a retrievalbased system usually retrieves a number of response candidates
from a pre-constructed index, and then selects the best matching
one as a response to a human input using semantic matching [25, 29,
33]. The retrieved responses usually have various expressions with
rich information and language fluency. However, limited by the
capacity of the pre-constructed repository, the selected response
might seem less customized for unobserved novel queries .
Meanwhile the generative conversation system works differently,
for it generates responses token by token according to conditional
probabilistic language models (LM) such as seq2seq with attention [1], which generates appropriate and tailored responses to
most queries, but often suffers from the lack of language fluency
and the problem of universal responses (e.g., “I don’t know" and “Me
too") due to statistical model incapabilities [2]. Various ameliorations have been proposed to enrich the generation, either by better
exploring internal features such as mutual-information-based objective function [9], dynamic vocabularies [30] and diverse beam
search [23], or by incorporating external knowledge, such as topic
information [31], cue words [15, 37], dialog acts [40], and common
sense knowledge [38].

contribution.
author: Rui Yan (ruiyan@pku.edu.cn).

† Corresponding

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’19, July 21–25, 2019, Paris, France
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00
https://doi.org/10.1145/3331184.3331193

1 http://www.msxiaoice.com/

435

Session 5A: Conversation and Dialog

SIGIR ’19, July 21–25, 2019, Paris, France

On the other hand, studies seeking for an ensemble of both
retrieval and generative approaches show great improvement to
dialogue generation performance. Song et al. [18] proposed MultiSeq2Seq model that focuses on leveraging responses generated by
the retrieval-based dialog systems to enhance generation-based
dialog systems, thus synthesizing a more informative response.
Similarly, Weston et al. [27] designed a retrieval-and-refine model
which treats the retrieval as additional context for sequence generator to avoid universal issues such as producing short sentences with
frequent words. Wu et al. [28] introduced a prototype-then-edit paradigm for their conversation system by building a retrieval-based
prototype editing with a seq2seq model that increases the diversity
and informativeness of the generation results.
Despite the performance gain of an ensemble compared with either retrieval or generative model, previous works only focused on
ameliorating one approach based on the other, still leaving great potentials for making further progress by allowing both methods to be
mutually enhanced. Inspired by adversarial learning [5], we propose
a generative adversarial framework for improving an ensemble on
short-text conversation, which is called EnsembleGAN throughout
the paper. Particularly, EnsembleGAN consists of two generators
and a discriminator. The LM-like generator (G 1 ) is responsible for
synthesizing tailored responses via a sequence-to-sequence framework, while the ranking-based generator (G 2 ) aims at selecting
highly competitive negative responses from a pre-retrieval module
and G 1 , and finally the ranking-based discriminator (D) endeavors
to distinguish the ground-truth and adversarial candidates provided
by pre-retrieval module and two generators (G 1 and G 2 ).
The motivation behind is that through adversarial learning, with
G 1 generating improved highly relevant responses, and G 2 providing enriched and fluent unobserved as well as synthetic candidates,
the discriminative ranker could be further trained to identify responses that are highly correlated, informative and fluent, thus absorbing the merits of both its generative counterparts. The proposed
EnsembleGAN framework is intuitively suited for improving a combination of any neural-based generative and retrieval approaches
towards better global optimal results. The main contribution of this
paper is three-folded and it’s summarized as follows:

conversation system [22, 29, 33–35]. Besides, with the success of
generative adversarial networks (GANs) [5] on computer vision
such as image translation [41] and image captioning [3], studies
of GAN applications also start to emerge in the domain of natural
language processing (NLP), such as dialogue generation [10, 32],
machine translation [36] and text summarization [13], all demonstrating the effectiveness of GAN mechanism in the domain of
NLP. With respect to dialogue generation framework, the GANrelated researches could also be generally categorized as the GAN
on generative-based and retrieval-based models.
As for sequence generation models, also regarded as sequential decision making process in reinforcement learning, Yu et al.
[39] proposed seqGAN framework that bypasses the differentiation
problem for discrete token generation by applying Monte Carlo rollout policy, with recurrent neural network (RNN) as generator and
binary classifier as discriminator. What follows are RankGAN [11]
which treats the discrimination phase as a learning-to-rank optimization problem as opposed to binary classification, dialogueGAN [10] that adapts the GAN mechanism on a seq2seq model
for dialogue generation scenario with its discriminator capable of
identifying true query-response pairs from fake pairs, as well as
DPGAN [32] that promotes response diversity by introducing an
LM-based discriminator that overcomes the saturation problem for
classifier-based discriminators. Nevertheless, even state-of-the-art
generative approaches couldn’t achieve comparable performance
as retrieval-based approaches in terms of language fluency and
diversity of response generation.
As for retrieval-based models, Wang et al. [26] proposed IRGAN
framework that unifies both generative and discriminative rankingbased retrieval models through adversarial learning. While the
generator learns the document relevance distribution and is able
to generate (or select) unobserved documents that are difficult for
discriminative ranker to rank correctly, the discriminator is trained
to distinguish the good matching query-response pair from the
bad. However effective IRGAN is, in a conversation scenario, a
pure retrieval system would always be limited by the constructed
query-response repository. The adversarial responses, observed or
not, might not be suitable for novel queries after all, which is a
common problem for retrieval-based conversation system that is
beyond IRGAN’s capability.
While previous GAN-related studies only focused on the improvement of either generative-based or retrieval-based single approach, our work in this paper could be categorized as a unified
GAN framework of the aforementioned GAN mechanism on both
retrieval model and sequence generation model of an ensemble,
which is constructed with each of its modules getting involved in
adversarial learning with different roles. While being most related
to rankGAN and IRGAN, our work has the following differences:

• We introduce a novel end-to-end generative adversarial framework that aims to mutually enhance both generative and
retrieval module, leading to a better amelioration of a dialogue ensemble model.
• We make extensive studies on ensembles of various generators and discriminators, providing insights of global and
local optimization from the ensemble perspective through
both quantitative and qualitative analysis.
• We demonstrate the effectiveness of the proposed EnsembleGAN by performing experiments on a large mixed STC
dataset, the gain on various metrics confirms that the ensemble model as well as each of its modules could all be
enhanced by our method.

2

1) RankGAN only trains a language model through point-wise
ranking of independent human-written and synthetic sentences, while EnsembleGAN trains a generative seq2seq
model (G1 ) through pair-wise ranking (D) of ground-truth
and negative responses, with both G1 and D conditioned on
the user’s query, let alone the existence of another strong
competitor G2 providing negative adversarial samples.

RELATED WORK

Open-domain dialogue systems have been attracting increasing
attention in recent years. Researchers have made various progress
on building both generative-based [15, 17, 20] and retrieval-based

436

Session 5A: Conversation and Dialog

SIGIR ’19, July 21–25, 2019, Paris, France

where σ is the sigmoid function, and д(·, ·) is the ranker’s scoring
function defined by any matching model. We train the ranker to
rank the ground-truth response r pos higher than a sampled negative
candidate r neg , with the pair-wise ranking loss L rank defined as a
hinge function [6]:

2) While IRGAN allows for both generative and discriminative
retrieval model to compete against each other, EnsembleGAN allows for both rankers G2 and D to compete against
each other as ensembles, with the constant involvement of
response generation module G1 included in a more delicate
three-stage sampling strategy.
3) EnsembleGAN unifies both GAN mechanism with a shared
overall learning objective among all generators and discriminator, enhancing an ensemble of generative and retrievalbased approaches towards better global optimal results.

3

Lrank =

Before diving into details of our EnsembleGAN Framework, we
first introduce the generation-based conversation model and the
retrieval-based conversation model, which is the basis of our Ensemble model.

4 ENSEMBLEGAN FRAMEWORK
4.1 Model Overview

Response Generation Model. An LM-based probabilistic conversation model usually employs the seq2seq encoder decoder
framework, where in general the encoder learns the query representation and the decoder generates the response sequence token
by token based on encoder output [17]. For an RNN-based seq2seq
model with attention mechanism [1], the generation probability
of the current word w t of the response given query q of length Tq
could be generally modeled as follows:

Figure 1 illustrates the overall architecture of our proposed EnsembleGAN framework. Given a set of user queries Q = {q 1 , q 2 , · · · , q N },
the original ensemble applies both its generation and pre-retrieval
module to synthesize and retrieve response candidates {r˜1 , · · · , r˜M1 }
and {rˆ1 , · · · , rˆH } for each q ∈ Q, respectively. All candidates are
ranked together based on the scoring function д(q, r ) of the ranking
module.
1) Generative seq2seq model. G θ 1 (r˜ |q), which inherits from the
generation module of the ensemble, is responsible for synthesizing
response candidates {r˜1 , · · · , r˜M1 } given query q, as depicted in
Eq.(7), with the application of Monte Carlo (MC) roll-out policy. By
combining with the ground-truth response r , we directly generate
M1
negative response pairs {⟨r , r˜m ⟩}m=1
aiming at receiving high ranking scores from discriminator, the process of which is also noted as
G θ 1 (⟨r , r˜⟩|q) for formulation coherence.

p(w t |w t −1 , · · · , w 1 , q) = f de (st , w t −1 , c t )
T

(1)

hi = f en (w i , hi−1 )
where f en and f de are the recurrence functions. hi ∈ Rd1 and
st ∈ Rd2 represent the hidden state of the encoder and decoder,
and c t the context vector obtained by attention mechanism based
Tq
on f att , which often takes the form of a weighted sum of {hi }i=1
.
The weight factor is generally computed as a similarity between st
Tq
and each hi ∈ {hi }i=1
, allowing the decoder to attend to different
part of contexts at every decoding step. The cross entropy loss
P
Lce = − yt log p(w t ) is often applied for the model training,
with yt the ground-truth corresponding word.

2) Generative ranking model. G θ 2 (⟨r , rˆ⟩|q), which inherits from
the response ranking model of the ensemble, learns to approximate
the true relevance distribution over response pairs ptrue (⟨r, rˆ⟩|q).
Hence with the true response r , we generates highly competitive
H as specified in Eq.(9) so as to challenge
negative samples {⟨r , rˆh ⟩}h=1
the discriminator.

Response Ranking Model. Given a query q and some candidate
provided by a fast pre-retrieval module1 , the ranking model learns
to compute a relevance score between each candidate and the query
q. Instead of the absolute relevance of individual responses (a.k.a,
point-wise ranking), we train the model through pair-wise ranking,
for a user’s relative preference on a pair of documents is often
more easily captured [26]. Hence, the probability of a response
pair ⟨r 1 , r 2 ⟩ with r 1 more relevant than r 2 (noted as r 1 ≻ r 2 ) being
correctly ranked can be estimated by the normalized distance of
their matching relevance to q as:

3) Discriminative ranking model. D ϕ (⟨r , r neg ⟩|q), which inherits
from the same ranking model as G 2 , endeavors however to distinguish the true response pairs from adversarial candidates provided
by both generators (G 1 and G 2 ). After the adversarial training, all
G 1 , G 2 and D could be used alone as single model, or we could also
form an improved ensemble consisting of a generation model G 1
and a ranking model (either G 2 or D) as described previously.

4.2

exp(д(q, r 1 ) − д(q, r 2 ))
1 + exp(д(q, r 1 ) − д(q, r 2 ))

Adversarial Training for the Ensemble

4.2.1 Overall Objective. In our generative adversarial framework for the ensemble, both generators try to generate fake samples that get high ranking scores so as to fool the discriminator,
the discriminator on the contrary is expected to distinguish the
good samples from the bad by ranking more precisely as well as
scoring down negative samples. We summarize the minimax game
among generators G 1 ,G 2 and the discriminator D with the objective

p(⟨r 1 , r 2 ⟩|q) = σ (д(q, r 1 ) − д(q, r 2 ))
=

(3)

where N is the number of (q, ⟨r pos , r neg ⟩) training samples and
δ denotes the margin allowing for a flexible decision boundary.
While both the response generation and ranking model could be
used alone as single model, they form an ensemble when the latter
reranks both pre-retrieved candidates and generated responses and
finally selects the response of the top ranking.

PRELIMINARIES

q
c t = f att (st , {hi }i=1
)

N
1 X
max(0, δ + д(q, r neg ) − д(q, r pos ))
N i =1

(2)

1 We

apply Lucene (https://lucenenet.apache.org/) to index all query-response pairs
and use the built-in TF-IDF method to retrieve candidates, following Song et al. [18].

437

Session 5A: Conversation and Dialog

SIGIR ’19, July 21–25, 2019, Paris, France

Figure 1: Illustration of EnsembleGAN Architecture (best viewed in color): generators G 1 , G 2 , discriminator D as well as three1 and ⃝
2 denote the trainstage sampling strategy are represented by blue, green, grey and orange colored blocks respectively. ⃝
ing phase of G 1 -steps and G 2 -steps respectively, which is defined in algorithm 1.
D ϕ for every time step. With the true response r , the expected end
reward of a response pair o ′ = ⟨r , r˜⟩ is defined as follows:

function L as follows:
L = min max L(G θ 1 , G θ 2 , D ϕ ) = min max ( L1 + L2 )
θ 1 ,θ 2

L1 =
L2 =

N
X
n=1
N
X
n=1

θ1, θ2

ϕ

ϕ

Eo∼ptrue (o |qn ) [log(D ϕ (o |q n )]

Eo ′ ∼G θ

1, θ2

(o ′ |q n ) [log(1 −

Jθ 1 (o ′ |q) =
(4)
=

D ϕ (o ′ |q n ))]

4.2.2 Optimizing Discriminative Ranker. As shown in Eq.(2) previously, we design D ϕ (⟨r, r neg ⟩|q) = pϕ (⟨r , r neg ⟩|q) to evaluate the
probability of a response pair ⟨r , r neg ⟩ being correctly ranked given
query q. Combining the ground-truth responses with the fake ones
generated by both current G 1 and G 2 , the optimal parameters of
D ϕ are obtained as follows:
ϕ

′
′ ∼G (o ′ |q ) [log D ϕ (om |q) |w 1:t −1 ]
Eom
θ1 m
t =1
T X
X
Gθ
G θ 1 (w t |w 1:t −1, q)Q D 1 (w 1:t −1, w t
ϕ
t =1 w t

(6)
πr
|q, MCm
)
1

where w 1:t −1 is the current state with t −1 tokens already generated
′ = ⟨r , w
in r˜, w 0 the initial token. The response pair om
1:Tm ⟩, where
w 1:Tm is the completed Tm -length sequence rolled out from current
πr
w 1:t −1 according to m 1 -time MC roll-out policy πr (noted as MCm
),
1
resulting in the action-value function defined as follows:

where E denotes the mathematical expectation, N the number of
′ ⟩ are the true and
training samples, o = ⟨r , r neg ⟩ and o ′ = ⟨r , r neg
generated response pair, respectively.

ϕ ∗ = argmax ( L1 + L2 )

T
X

Gθ
Q D 1 (w 1:t −1,
ϕ

w t |q,

πr
MCm
)
1

m

1 X1

′


log D ϕ (om
|q),


=  m 1 m=1


 log D (o ′ |q),
ϕ


for t < T
for t = T
(7)

Hence, the instant reward for time step t is calculated as the aver′ }m 1
age ranking scores from D ϕ of all sampled response pairs {om
m=1
obtained by repeatedly rolling out w 1:t −1 for m 1 times based on
πr
MCm
. We note M 1 = m 1 ∗T as the total number of generations for
1
r˜ of length T . In contrast to the original rankGAN, both generator
G θ 1 (w t |w 1:t −1 , q) and discriminator D ϕ (o ′ |q) are conditioned on
the given query q, which is a necessary adaptation in the case of
dialogue generation. Note that such a configuration could also be
referred to as conditionalGAN framework [14].

(5)

where L1 and L2 are defined in Eq.(4), such an optimization problem is usually solved by gradient descent as long as D ϕ is differentiable with respect to ϕ. When training generators, D ϕ is used
to provide reward of generated negative samples, which will be
detailed later in this section.
4.2.3 Optimizing Generative Seq2Seq. At the first stage, we enhance the generative seq2seq model G θ 1 through discriminative
ranker D ϕ . When given a user query q, the generation of a sequence
r˜ = {w 0 , w 1 , ..., wT } could be regarded as a series of decision making at T time steps by policy π = G θ 1 (w t |w 1:t −1 , q) as defined in
Eq.(1). However, since D ϕ only provides the reward for a complete
sequence, the lack of intermediate reward for every time step leads
to the ignorance of long term reward causing the model to be shortsighted. We hence apply MC roll-out policy [11, 39] to tackle with
the problem, which repeatedly rolls out incomplete sequences until
the end-of-sequence token so as to get an expected reward from

4.2.4 Optimizing Generative Ranker. The second stage involves
the amelioration of generator G 2 through discriminator D ϕ , with
the objective function defined as below:
Jθ 2 |θ 1 (q) = Eo ′ ∼G θ |θ (o ′ |q) [log(1 − D ϕ (o ′ |q))]
2 1

(8)

where θ 2 |θ 1 denotes that this second stage is actually based on the
first stage discussed above, with G θ 1 fixed as a result. Inheriting
from the same ranking model as D ϕ , we train G θ 2 to generate competitive negative response pairs that receive high ranking scores
from D ϕ , where both ranking-based generative and discriminative

438

Session 5A: Conversation and Dialog

SIGIR ’19, July 21–25, 2019, Paris, France

Algorithm 1 EnsembleGAN Minimax Game

models could get improved [26]. More precisely, when given a true
(q, r ) pair and a scoring function дθ 2 , the chance of G θ 2 selecting a
negative sample oh′ = (r, rˆh ) according to the relevance distribution
of response pairs {⟨r , rˆh ⟩|rˆh ≻ r, rˆh ∈ R M } is defined by a softmax
function as follows:
G θ 2 (oh′ |q) = P

exp(дθ 2 (q, r̂ h ) − дθ 2 (q, r ))
exp(дθ 2 (q, r̂ h ) − дθ 2 (q, r ))

r̂ h ∈R M

exp(дθ 2 (q, r̂ h ))
= Pθ 2 (r̂ h |q)
r̂ h ∈R M exp(дθ 2 (q, r̂ h ))

Require:
Generators G θ 1 ,G θ 2 , and discriminator D ϕ ;
Training data Ds2s , Drank and retrieval database Dret ;
Three-stage sampling approach R M as in Eq.(10);
M 1 ,H the sampling size of G θ 1 and G θ 2 respectively.
Ensure:
Ensemble of seq2seq model G θ 1 and ranker model G θ 2 , D ϕ
1: Initialize G θ 1 , D ϕ with random weights θ 1 ,ϕ;
2: Pretrain G θ 1 , D ϕ on Ds2s , Drank respectively
3: for G 1 -steps do
4:
G θ 1 (· |q) generates M 1 samples for each (q, r ) ∈ Ds2s ;
5:
Update G θ 1 via policy gradient defined in Eq.(11);
6: end for
7: for G 2 -steps do
8:
for each (q, r ) ∈ Drank do
9:
G θ 1 (· |q) generates M 1 samples
10:
G θ 2 (·, r |q) generates H samples via R M ;
11:
end for
12:
Update G θ 2 via policy gradient defined in Eq.(11);
13: end for
14: for D-steps do
15:
G θ 1 (· |q) generates M 1 samples for each (q, r ) ∈ Drank ;
16:
G θ 2 (·, r |q) generates H samples via R M and combine with positive
samples from Drank ;
17:
Train discriminator D ϕ according to Eq.(5)
18: end for

(9)

= P

where R M represents the M-sized candidate pool with groundtruth responses excluded. Despite other possible configurations as
observed in Wang et al. [26], we follow G θ 2 (oh′ |q) = Pθ 2 (rˆh |q) as
directly being the relevance distribution of an individual response
rˆh , not only for the simplicity, but for the coherence of both G θ 1 and
G θ 2 being able to sample responses independently of the groundtruth response, as it’s the real usage case after training.
The candidate pool R M is of importance for the capability of G θ 2
to sample H unobserved as well as highly competitive responses.
In addition to the random sampling strategy that generates Mr random responses (R random
) from the database as the original IRGAN,
Mr
we apply both the pre-retrieval module to retrieve Mp candidates
(R retrieval
) similar to ground-truth responses regardless of queries,
Mp
and also G θ 1 to synthesize M 1 relevant responses, all of which are
summarized as a three-stage sampling strategy:
M1
R M (Mr , Mp , M 1 ) = R random
∪ R retrieval
∪ {r˜m }m=1
(10)
Mr
Mp
The design of R M not only compensates for the ineffectiveness
of random sampling for generating competitive responses from a
huge dialogue database in our case, it also enables the generator
G 2 to work as an ensemble with the response generation model G 1 ,
thus always considering the cooperation of both generative-based
and retrieval-based approaches during adversarial learning.

4.2.7 Overall Algorithm. We summarize the ensembleGAN algorithm in Algorithm 1, where all the generators G 1 , G 2 and discriminator D are initialized by a pretrained ensemble, with G 2 and
D sharing the same parameter initialization.
Despite the very existence of Nash equilibrium between generator and discriminator for their minimax game, it remains an
open problem of how they could be trained to achieve the desired
convergence [5]. In our empirical study, we confirm that both the
ranker D ϕ and generator G 1 are enhanced by ensembleGAN, while
the ranker generator G 2 encounters a loss of performance after
adversarial training, as also observed in Wang et al. [26].

4.2.5 Policy Gradient. Following Sutton et al. [19], we apply policy gradient to update generators’ parameters through feedback of
D ϕ , for the sampling process of both generators are non-differential.
Hence, with D ϕ fixed, for each query q with true-negative response
′ ), the minimization of L in Eq.(4) with respect to
pair o ′ = (r, r neg
θ 1 ,θ 2 could be deduced as follows [11, 26]:
min L = max

θ 1 ,θ 2

θ1

N
X

n=1
T X
X

∇θ 1 Jθ 1 (o ′ |q n ) ≃
∇θ 2 Jθ 2 (q n ) ≃

Eo ′ ∼G θ Jθ 1 (o ′ |q n ) − max

t =1 w t

1

θ 2 |θ 1

N
X

n=1

5

Jθ 2 |θ 1 (q n )
Gθ

∇θ 1 log G θ 1 (w t |w 1:t −1, q n )Q D

1
ϕ

EXPERIMENTS

In this section, we compare our EnsembleGAN with several representative GAN mechanism on a huge dialogue corpus. The goal of
our experiments is to 1) evaluate the performance of our generation
module and retrieval module for response generation and selection,
and 2) evaluate the effectiveness of our proposed EnsembleGAN
framework from the ensemble perspective.

(11)

H
1 X
∇θ 2 log G θ 2 (oh′ |q n ) log D ϕ (oh′ |q n )
H
h=1

where Jθ 1 , Jθ 2 are defined in Eq.(6) and Eq.(8) respectively. ∇ is the
differential operator, T the generated sequence length by G θ 1 and
H the negative sampling size of G θ 2 .
4.2.6 Reward Setting. Normally, we would consider that the
reward R ≡ log D ϕ (r, r neg |q). It’s however problematic that the
logarithm leads to instability of training [5]. We thus follow Wang
et al. [26] with the advantage function of reward implementation
defined as below:
R = 2 · D ϕ (r , r neg |q) − 1
(12)
= 2 · [σ (дϕ (q, r ) − дϕ (q, r neg ))] − 1

5.1

Dataset

We conduct our experiments on a large mixed dialogue dataset
crawled from online Chinese forum Weibo1 and Toutiao2 containing millions of query-response pairs. For data pre-processing, we
remove trivial responses like "wow" as well as the responses after
first 30 ones for topic consistency following Shang et al. [17]. We
use Jieba3 , a common Chinese NLP tool, to perform Chinese word
1 https://www.weibo.com/
2 https://www.toutiao.com/
3 https://github.com/fxsjy/jieba

439

Session 5A: Conversation and Dialog

SIGIR ’19, July 21–25, 2019, Paris, France

Table 1: The Statistics of Mixed Short-Text Conversation
Dataset. Resp. is response for short, # Sent, # Vocab and
Avg_L denote the number of sentences, vocabularies and the
average sentence length, respectively.
Dataset
Features
Corpus
# Sent
Post # Vocab
Avg_L
# Sent
Resp. # Vocab
Avg_L
Pair # Pairs

Retrieval Pool

Ranking Set

Generation Set

Test Set

Weibo+Toutiao
2,065,908
251,523
11.4
5,230,048
628,254
8.7
6,000,000

Weibo
30,000
29,272
13.1
360,000
28,000
9.8
360,000

Toutiao
1,000,000
120,996
9.3
1,000,000
121,763
7.1
1,000,000

Toutiao
2,000
5,642
10.1
2,000
4,544
7.7
2,000

single component (generator or discriminator) as well as the derived ensemble (Generation + Ranking) for each GAN mechanism,
resulting in various combinations which will be detailed later.
Response Generation Models (S2SA). We compare with the
attention-based seq2seq model (S2SA) [1], which has been widely
adopted as a baseline model in recent studies [10, 17]. As a result,
we have three derived adversarial sequence generators, namely
the dialogueGAN-G, DPGAN-G, RankGAN-G that compete against
ensGAN-G1 . Besides, We include mutual information enhanced
seq2seq model (MMI-S2SA) [9] as another generative baseline
method.
Pre-Retrieval Module (TF-IDF). The pre-retrieval module, as
the basis of retrieval approach, first calculates similarities among
utterances (queries) based on simple TF-IDF scores and then retrieve
the corresponding responses [18]. We report the Top-{1,2} responses,
noted as TF-IDF-{1,2}, respectively.
Response Ranking Models (Ranking). The pure retrieval system is consisted with a pre-retrieval module and a ranking (matching) model, where the pre-retrieved candidates is reranked by the
ranker, for which we apply state-of-the-art attentive conv-RNN
model [24] for our ranker baseline. Therefore, we have 5 derived
adversarial rankers based on the same original ranker, namely the
RankGAN-D, IRGAN-G and IRGAN-D that compete against our
ensGAN-G2 and ensGAN-D.
Ensemble Models (Generation+Ranking). Ensemble models are constructed with a generation model, a pre-retrieval module and a ranking model. When given a query, the generative
model (e.g., S2SA, RankGAN-G and ensGAN-G1 ) synthesizes candidate responses. Then the ranking model (e.g., conv-RNN, IRGAND, RankGAN-D and ensGAN-D) is required to rerank both preretrieved candidates and synthetic responses, and select the top
one response in the end. Besides, following Song et al. [18] and Wu
et al. [28], we also consider Multi-Seq2Seq + GBDT reranker and
Prototype-Edit as two baseline ensemble models.

segmentation on all sentences. Each query and reply contain on average 10.2 tokens and 8.44 tokens, respectively. From the remaining
query-response pairs, we randomly sampled 6,000,000 pairs as retrieval pool for the pre-retrieval module, 1,000,000 and 50,000 pairs
for training and validating the sequence generation model, 360,000
and 2000 pairs for training and validating the ranking model (we
apply three-stage sampling strategy to generate 11 negative samples for 30,000 true query-response pairs), and finally 2,000 pairs as
test set for both models. We make sure that all test query-response
pairs are excluded in training and validation sets. More detailed
data statistics are summarized in Table 1.

5.2

Baselines

We introduce baseline models and GAN competitors on three levels,
namely the generation approach, the retrieval approach and the
ensemble approach. We note GAN-G (D) for the generator (discriminator) of a GAN mechanism in this section. EnsembleGAN is
represented by ensGAN for ease of demonstration.
DialogueGAN. We consider dialogueGAN [10] as our GAN
competitor for the generation part, with a seq2seq generator and a
binary-classifier-based discriminator that is trained to distinguish
the true query-response pairs from the fake ones. In order to eliminate structure biases for a fair comparison, we adopt the very same
deep matching model structure as our ranking model (which will
be detailed later) for its discriminator, instead of the hierarchical
recurrent architecture applied by the original paper.
DPGAN. We consider diversity-promoting GAN (DPGAN) [32]
as a second GAN competitor for the generation part, with a seq2seq
generator and a language-model-based discriminator that is trained
to assign higher probability (lower perplexity) for true responses
than fake ones. The LM-based discriminator is consisted with a
uni-directional LSTM [7] as the original paper.
RankGAN. We consider RankGAN as another GAN competitor.
The original RankGAN [11] is an unconditional language model
that is unsuitable for dialogue generation scenario as discussed
previously, we hence modify RankGAN to consist of a seq2seq
generator and a pairwise discriminative ranker, which could be
considered as ensGAN without getting generator G2 involved.
IRGAN. We also consider IRGAN [26] as a GAN competitor.
Similarly, this could be considered as ensGAN without any involvement with seq2seq generator or the three-stage sampling strategy.
All GAN mechanism are applied on exactly the same pre-trained
generation or ranking model for a fair comparison, and we evaluate

5.3

Implementation Details

The seq2seq model is trained with a word embedding size of 300
for source and target vocabulary of 30,000 most frequent tokens
of queries and responses in the generation training set, covering
97.47% and 97.22% tokens that appear in queries and responses
respectively. The rest tokens are treated as "UNK" as unknown
tokens. We set the hidden size of the encoder and decoder to 512.
The adversarial sampling size m 1 = 20 during G1 training steps.
The conv-RNN ranker is trained with 200-dimensional word embedding for a shared vocabulary of 40,000 tokens, covering 93.54%
words in the retrieval pool. The size of GRU is set to 200. The window size of the convolution kernel is set to (2, 3, 4, 8), with number
of filters equal to (250, 200, 200, 150), following Wang et al. [24].
We pretrain the ranker to rank the ground-truth response to the
top from k = 11 negative samples including 5 random samples,
the top 5 pre-retrieved candidates and a synthetic one generated
by seq2seq model. During adversarial training, the ranker generator G 2 generates H = 8 negative samples from a candidate pool
R M (100, 10, 10) according to the three-stage sampling strategy.
We use dropout of 0.2 for all models, and Adam optimizer [8]
with a mini-batch of 50. The learning rate of S2SA and conv-RNN

440

Session 5A: Conversation and Dialog

SIGIR ’19, July 21–25, 2019, Paris, France

are respectively 0.0002 and 0.001 during pre-training, 2 × 10−6 and
1 × 10−5 during adversarial learning.

Evaluation Metrics

We adopt multiple automatic evaluation criteria as well as human
evaluation for a comprehensive comparison.
BLEU. BLEU [16] evaluates the word-overlap between the proposed and the ground-truth responses. Typically, we use BLEUn
(n = 1, 2, 3, 4) to calculate their n-grams-overlap, where BLEUn
denotes the BLEU score considering n-grams of length n.
Embedding-based metrics (EA, GM, VE). Following Liu et al.
[12], we alternatively apply three heuristics to measure the similarity between the proposed and ground-truth response based on
pre-trained word embeddings1 , including Embedding Average (EA),
Greedy Matching (GM), and Vector Extrema (VE).
Semantic Relevance (RUBERA , RUBERG ). Together with
the embedding similarity, Tao et al. [21] evaluates the semantic relatedness between a response and its query based on neural matching
models. Following the original paper, we report the arithmetic and
geometric mean of embedding similarity and semantic relatedness,
denoted as RUBERA and RUBERG , respectively.
Retrieval Precision (P@1). We evaluate pure ranking-based
retrieval systems by precision at position 1 (P@1), which calculates the ratio of relevant responses (in our case, the ground-truth
response) within top-1 reranked responses.
Human evaluation. We also conduct human evaluations for
generation and ensemble models since automatic metrics might
not be consistent with human annotations [12, 21]. Following previous studies [18, 21, 31], we invited 3 well educated volunteers to
judge the quality of 100 randomly generated responses by different
models2 , based on the following criteria: a score of 0 indicates a
bad response that is either dis-fluent or semantically irrelevant;
+1 means a relevant but universal response; +2 indicates a fluent,
relevant and informative response. We report the proportion of
each score (0, +1, +2) for each model. Fleiss’ kappa [4] scores are
also reported.

5.5

Eens

IRGAN-D
RankGAN-D
ensGAN-D

0.38

Contribution of Generation V.S. Retrieval
Generation

conv-RNN
IRGAN-G
ensGAN-G2

0.40

Precision

5.4

P@1 for Retrieval System
0.42

E1 + 1
ERank

37.5%(*)

EIR
E1

0.36

E0
0.34

Song
0

1

2

(a)

3

4

5

0.0

Retrieval

44.9%
ensGAN-G1
46.1%
RankGAN-G
52.5%
RankGAN-G
28.5%
S2SA
29.3%
S2SA
17.8%
S2SA
44.8%
Multi-seq2seq
0.2

55.1%
ensGAN-D
53.9%
IRGAN-D
47.5%
RankGAN-D
71.5%
IRGAN-D
70.7%
ensGAN-D
82.2%
conv-RNN
55.2%
GBDT
0.4

0.6

0.8

1.0

(b)

Figure 2: (a) P@1 scores for various ranker-based retrieval
systems. * denotes significant precision improvement (compared with conv-RNN) according to the Wilcoxon signedrank test; and (b) The final response contribution of generation and ranking modules for ensembles.
1) As for generation module, we first notice that GAN-enhanced
seq2seq models achieve plausible improvement on most evaluation metrics, outperforming S2SA and MMI-S2SA baselines. Both
RankGAN-G and ensGAN-G1 aim at synthesizing responses that
approximate true responses with higher ranking scores, which is
demonstrated by the obvious gain of their contribution ratios for
ensembles shown in Figure 2(b), with more than 40% contribution
for both RankGAN ensemble (ERank ) and ensGAN ensemble (Eens ).
Their comparable enhancement to RUBER scores indicates better
generations in terms of the semantic relevance. Despite the outperforming word overlap and embedding average of RankGAN-G,
ensGAN-G1 is not only better at improving the GM and VE metrics,
indicating more generation of key words with important information that are semantically similar to those in the ground-truth [12],
but it’s also capable of generating more satisfying responses with
fewer 0 human scores according to Table 3.
2) As for retrieval methods, we see that they often achieve advantageous higher order BLEU scores (e.g., BLEU3 and BLEU4 )
than generative approaches, since generating responses of better
language fluency (hence higher n-gram overlaps to some extent)
is undoubtedly their strong points. They are however inferior to
generative methods in terms of RUBER scores, for the latter are
generally better at generating more tailored responses of high semantic relatedness [18], similar results are also obtained by Tao et al.
[21]. Together with P@1 scores in Figure 2(a), all discriminative
rankers of GAN approaches (IRGAN-D, RankGAN-D, ensGAND) are generally ameliorated on various aspects, with generative
rankers (IRGAN-G, ensGAN-G2 ) somehow deteriorated, which is
also confirmed by Wang et al. [26]. Similarly, one possible explanation might be the sparsity of the positive response distribution
compared with negative ones during training, making it hard for
a generative ranker to get positive feedbacks from discriminator.
Without any generation module, IRGAN outperforms others on
enhancing a pure retrieval system, notably achieving the highest
P@1 score. On the other hand however, the P@1 score for all methods remains low compared with common QA task [24, 26], which
might be explained by a more complicated and chaotic nature of
STC dataset [25].
3) As for the ensembles, they commonly outperform previous
single approaches, for the scores in the third block (Ensemble)

Results and Analysis

5.5.1 Overall Performance. Our evaluation is divided into three
parts, namely the evaluation for pure generation module, pure retrieval module and the ensemble. Table 2 summarizes the general
dialogue generation performance including various automatic metrics of word overlap, embedding similarity and semantic relevance.
Figure 2 shows the P@1 scores for different retrieval systems as
well as the study of contribution of two modules that consist of an
ensemble, together with Table 3 of human evaluation results for
representative models. The human agreement is validated by the
Kappa with a value range of 0.4 to 0.6 indicating “moderate agreement" among annotators. A higher value denotes a higher degree
of agreement, such as 0.65 for S2SA which is probably because it
generates more dis-fluent or irrelevant responses that are easy to
recognize. We first make several observations as follows:
1 We

apply pre-trained Chinese word embedding which is available at
https://github.com/Embedding/Chinese-Word-Vectors.
2 Due to numerous generation + ranking possibilities and space limitations, we only
asked annotators to evaluate representative models with high automatic metric scores.

441

Session 5A: Conversation and Dialog

SIGIR ’19, July 21–25, 2019, Paris, France

Table 2: Overall performance of baselines and GAN competitors. Ranking(M) means that candidate responses (generated by
the pre-retrieval module) are re-ranked by the ranking model M. Bold scores denote the highest score within each block. The
RUBER scores for ground-truth are 0.815, 0.798 for RUBERA and RUBERG , respectively.
Automatic Metrics
Modules

Generation

Retrieval

Ensemble

S2SA
MMI-S2SA
DialogGAN-G
DPGAN-G
RankGAN-G
ensGAN-G1
TF-IDF-1 (pre-retrieval)
TF-IDF-2 (pre-retrieval)
Ranking (conv-RNN)
Ranking (RankGAN-D)
Ranking (IRGAN-G)
Ranking (IRGAN-D)
Ranking (ensGAN-G2 )
Ranking (ensGAN-D)
Multi-Seq2Seq + GBDT [18]
Prototype-Edit [28]
S2SA + conv-RNN
RankGAN-G + conv-RNN
ensGAN-G1 + conv-RNN
RankGAN-G + IRGAN-D
S2SA + IRGAN-D
S2SA + ensGAN-D
RankGAN-G + RankGAN-D
ensGAN-G1 + ensGAN-D

BLEU1
7.334
8.468
9.465
8.578
10.033
9.530
7.026
7.120
7.242
7.441
7.225
7.451
7.057
7.452
7.542
7.926
7.630
7.755
7.570
8.827
8.375
8.535
8.715
9.339

Word Overlap
BLEU2 BLEU3
2.384
0.987
2.464
0.956
2.483
0.912
2.474
0.922
2.545
0.967
2.487
0.872
2.175
0.928
2.108
0.990
2.213
0.933
2.194
0.945
2.166
0.867
2.362
1.012
2.129
0.897
2.320
1.004
2.173
0.993
2.334
1.120
2.299
1.125
2.275
0.889
2.168
0.871
2.693
1.234
2.850
1.232
2.749
1.297
2.501
1.075
2.876
1.277

+1

0

Kappa

0.12
0.14
0.16
0.21
0.22
0.25
0.28
0.30
0.37

0.40
0.49
0.39
0.33
0.35
0.35
0.35
0.36
0.38

0.48
0.36
0.45
0.47
0.43
0.40
0.36
0.35
0.26

0.65
0.55
0.43
0.52
0.47
0.49
0.46
0.53
0.45

Cosine similarity

+2

Embedding Similarity
EA
GM
VE
0.503 0.154 0.332
0.526 0.149 0.342
0.533 0.161 0.344
0.535 0.165 0.345
0.560 0.145 0.343
0.531 0.163 0.347
0.537 0.152 0.337
0.538 0.153 0.338
0.543 0.151 0.339
0.547 0.152 0.341
0.540 0.152 0.337
0.553 0.156 0.343
0.539 0.150 0.338
0.548 0.153 0.341
0.540 0.152 0.338
0.557 0.164 0.346
0.544 0.153 0.341
0.549 0.150 0.340
0.544 0.156 0.339
0.560 0.152 0.348
0.558 0.162 0.347
0.547 0.159 0.345
0.561 0.154 0.347
0.559 0.178 0.352

RUBER
RUBERA RUBERG
0.550
0.500
0.557
0.521
0.560
0.533
0.588
0.557
0.602
0.580
0.598
0.584
0.541
0.486
0.539
0.499
0.558
0.519
0.571
0.535
0.560
0.518
0.573
0.542
0.549
0.516
0.579
0.539
0.592
0.568
0.610
0.587
0.564
0.535
0.572
0.543
0.568
0.540
0.608
0.577
0.600
0.573
0.595
0.569
0.615
0.591
0.621
0.605

Word Distribution Comparison for Generation Module

Table 3: Results of human evaluation for generation and ensemble models. “Kappa" means Fleiss’ kappa.
Score
Model
S2SA
ensGAN-G1
RankGAN-G
S2SA + conv-RNN
S2SA + IRGAN-D
S2SA + ensGAN-D
RankGAN-G + RankGAN-D
RankGAN-G + IRGAN-D
ensGAN-G1 + ensGAN-D

BLEU4
0.340
0.404
0.349
0.385
0.436
0.352
0.460
0.581
0.488
0.490
0.409
0.528
0.460
0.527
0.569
0.571
0.555
0.432
0.423
0.716
0.637
0.715
0.580
0.763

S2SA
MMI
DialogueGAN

0.8
0.6

DPGAN
RankGAN-G
ensGAN-G1

0.4
0.2

[1-300]

[301,600]

[601-1500]

Figure 3: Cosine similarity between ground-truth and synthetic word distribution by various generative models on
test data for different word frequency level (e.g., top 300 frequent words). EnsGAN achieves satisfying performance especially when considering words of lower frequency.
accounts more for the ensemble’s final selection, the ensGAN-D
learns to rank (select) responses featuring advantages of both generative and retrieval approach as expected, with the help of another
strong negative sampler G2 during adversarial training.

are generally better than the first two blocks (Generation and Retrieval), which is especially true for those GAN-enhanced ensembles. Among various combinations of generation + ranking, the
ensGAN ensemble (ensGAN-G1 + ensGAN-D) outperforms both IRGAN (S2SA + IRGAN-D) and RankGAN (RankGAN-G + RankGAND) ensembles with the largest gain on almost all metrics, as well as
achieving the most +2 and the fewest 0 scores for human judgement.
While RankGAN and IRGAN bring specific enhancement to the
generative and retrieval module respectively, the ensGAN improves
the whole ensemble by allowing each its module to compete against
each other, which might be regarded as seeking for a global optimum compared with other GAN that searches for local optimum
of a single approach. While the ensGAN-G1 generation module

5.5.2 Discussion. In addition to previous observations, we’d also
like to provide further insights of the EnsembleGAN framework on
several interesting aspects in this section.
Ranking versus LM versus Binary-Classification. As for the
amelioration of generative seq2seq model, while DialogueGAN
uses a binary classifier as the discriminator, DPGAN utilizes an
LM-based discriminator, and both RankGAN and ensGAN apply
ranking-based discriminator. As a result, the superiority of adversarial ranking over binary-classification is not only observed in our
experiment, but confirmed in [11] as well. The LM-based discriminator (DPGAN-D) on the other hand, by addressing the saturation

442

Session 5A: Conversation and Dialog

SIGIR ’19, July 21–25, 2019, Paris, France

conv-RNN
IRGAN-G
ensGAN-G2

0.38

IRGAN-D
RankGAN-D
ensGAN-D

0.25

0.20

S2SA+conv-RNN
S2SA+IRGAN-D
RankGAN-G+RankGAN-D
RankGAN-G+IRGAN-D
ensGAN-G1+ensGAN-D

0.37

Loss

Precision

Table 4: Response generation case study. The final decision
√
of rankers are marked by . White and gray cells denote
valid and inaccessible candidates for a ranker when combined with its corresponding generative module as an ensemble. The original ranker is noted D O , and D IR , D R , D E ,
D T denote IRGAN-D, RankGAN-D, ensGAN-D and GBDT respectively.

Ensemble Ranking Loss

Retrieval P@1 Learning curves

0.39

0.15

0.36

0.10
0.35

Response generation cases

0.05
0.34

0

5

10

15

Training epochs

(a)

20

Retrieval
Generation
rank
rank

Overall
rank

(b)

Figure 4: (a) Ranker P@1 learning curves; and (b) Error bars
of mean and standard deviation of ranking loss for different
modules of ensembles. Results are calculated on test set.
issue of binary classification [32], brings comparable improvement
as adversarial ranking, all of which help generate responses of
higher quality as observed previously, as well as achieving better
cosine similarity of word distributions (Figure 3). In particular, we
apply the adversarial ranking in our work for it’s the very bridge
that connects the adversarial training of both generative-based and
retrieval-based methods in the EnsembleGAN framework.
1+1,2 for Ensemble Approach. Although it’s unreasonable to
exhaustively study all possible generation + ranking combinations,
it’s however interesting to directly combine the seemingly best
two modules of their separate worlds, namely the RankGAN-G +
IRGAN-D, to see how such an ensemble performs compared with
ensGAN. Apart from the overall results in Table 2 which already
indicate that these two "best winners" do not get along as well as
ensGAN-G1 + ensGAN-D to some extent, a further evidence lies
in the analysis on the ranking module shown in Figure 4. On one
hand, the P@1 adversarial learning curves (Figure 4(a)) show that
the IRGAN is better at enhancing a pure retrieval system, while
RankGAN-D encounters a higher oscillation which is probably due
to its concentration on ranking the synthetic responses to the top,
making its P@1 pure retrieval performance unpredictable. On the
other hand, the ensemble of ensGAN-G1 + ensGAN-D turns out to
be clearly advantageous in terms of the ranking loss (Figure 4(b))
defined in Eq.(2) among ensemble approaches. More specifically,
we calculate the module-wise ranking loss for the final chosen reat ion )
sponses (considered as r neg ) from the generation (LrGener
ank
Ret
r
ieval
or the pre-retrieval module (Lr ank
), the overall ranking loss
all ) is computed as the weighted sum of the two losses
(LrOver
ank
based on the module contribution. We see that ensGAN-D generally achieves the lowest ranking loss with moderate variance,
which clearly demonstrates that EnsembleGAN is indeed more inclined towards global optimum without unilaterally enhancing a
single module and thus is more adapted for an ensemble of multiple
modules, especially when we note that the direct combination of
the two "best winners" RankGAN-G + IRGAN-D does not result in
the lowest overall ranking loss (not even close).

q:
r:
TF-IDF-1:
TF-IDF-2:
S2SA:
MMI-S2SA:
DialogueGAN:
DPGAN:
RankGAN-G:
ensGAN-G1:
Multi-Seq2Seq:

I can’t play sniper.
You’re messing up with me.
Really can’t play this.
I really can’t play games.
Have you ever played?
Right, I’ve been playing for a year.
I’ve played once, I don’t know.
They’re my men, what’re you thinking?
You’ll know once you’ve played.
What I played was real.
I’ve played this game, and you?

q:
r:
TF-IDF-1:
TF-IDF-2:
S2SA:
MMI-S2SA:
DialogueGAN:
DPGAN:
RankGAN-G:
ensGAN-G1:
Multi-Seq2Seq:

Looking for the title of this song.
It’s the theme song from Rudy.
It’s Faith.
Faith by Xinzhe Zhang.
UNK.
I’d like to know where you’re from.
UNK.
Yeah!
UNK by UNK.
Thanks for your love.
UNK.

q:
r:
TF-IDF-1:
TF-IDF-2:
S2SA:
MMI-S2SA:
DialogueGAN:
DPGAN:
RankGAN-G:
ensGAN-G1:
Multi-Seq2Seq:

You’re pretty, with or without fringe.
Well, I’ve got a big face actually.
Hahahaha.
UNK.
Thanks.
Haha, thanks.
I think so.
I’m UNK.
Haha, I’m also UNK.
Thank you my friend.
Haha.

q:
r:
TF-IDF-1:
TF-IDF-2:
S2SA:
MMI-S2SA:
DialogueGAN:
DPGAN:
RankGAN-G:
ensGAN-G1:
Multi-Seq2Seq:

Who’s called hot pepper?
It’s a girl we saw previously.
Our favourite fast food.
You don’t even know hot pepper?
I know you’re male.
I know you’re male, I’m female.
Let me guess who you are.
I mean that I’m actually her.
Are you talking about your joke?
I knew it was you!
Yeah, me too.

DO

D IR

√

√

DR

DE

DT

√

√
√

√

√
√

√

√

√
√

√
√
√

√

√
√

√

√

conv-RNN, suggesting the very importance of a re-ranker for a
dialogue ensemble, which is reasonable because all candidates have
to be reranked by this final decision maker. Hence, despite the trend
on the amelioration of generative approaches, it’s also plausible to
concentrate on the research of retrieval or ensemble methods so as
to improve the open domain human-computer conversation.
5.5.3 Case Study. Table 4 shows several example response generation by ensembles, together with various baselines. It’s obvious that an ensemble becomes plausible for selecting one final
response from multiple candidates in case a single approach fails to
respond correctly, just as the second and third case, corresponding
to generative-failure and retrieval-failure respectively. We could
also observe that as for generation module, most enhanced seq2seq
models are better than S2SA in terms of both language fluency and

The Merits of the Ranking Module. In addition, we find that
there also exists a clear performance gap among the ensembles
themselves. As shown in Table 2, the combinations of original
S2SA + GAN-enhanced rankers generally bring better ameliorations
compared with the combinations of GAN-enhanced S2SA + original

443

Session 5A: Conversation and Dialog

SIGIR ’19, July 21–25, 2019, Paris, France

informativeness. Moreover, the GAN-enhanced seq2seq models
are generally better than MMI-S2SA which generates irrelevant
responses like "I know you’re male" given the query "who’s called
hot pepper?" in the last case. Among GAN-based generators, all
DPGAN, RankGAN and ensGAN achieve similar performances in
terms of the generation enrichness, which seem slightly better than
dialogueGAN. Besides, while the original GBDT ranker and IRGAND mostly prefer the retrieved candidates, RankGAN-D however
largely favors synthetic responses, conforming with their respective
GAN initiatives. In contrast, the ensGAN-D is able to perform more
balanced and logical selections between its generation module and
pre-retrieval module, demonstrating its ability to leverage both advantages of single retrieval-based and generation-based approach
in dialogue generation scenarios.

6

[12] Chia-Wei Liu, Ryan Lowe, Iulian Serban, Michael Noseworthy, Laurent Charlin,
and Joelle Pineau. 2016. How NOT To Evaluate Your Dialogue System: An
Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response
Generation. In EMNLP. 2122–2132.
[13] Linqing Liu, Yao Lu, Min Yang, Qiang Qu, Jia Zhu, and Hongyan Li. 2018. Generative Adversarial Network for Abstractive Text Summarization. In AAAI.
[14] Mehdi Mirza and Simon Osindero. 2014. Conditional Generative Adversarial
Nets. CoRR abs/1411.1784 (2014).
[15] Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang, and Zhi Jin. 2016. Sequence to
Backward and Forward Sequences: A Content-Introducing Approach to Generative Short-Text Conversation. In COLING. 3349–3358.
[16] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a
method for automatic evaluation of machine translation. In ACL. 311–318.
[17] Lifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neural Responding Machine
for Short-Text Conversation. In ACL. 1577–1586.
[18] Yiping Song, Cheng-Te Li, Ming Zhang, Dongyan Zhao, and Rui Yan. 2018. An
Ensemble of Retrieval-Based and Generation-Based Human-Computer Conversation Systems. In IJCAI. 4382–4388.
[19] Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour.
1999. Policy Gradient Methods for Reinforcement Learning with Function Approximation. In NIPS. 1057–1063.
[20] Chongyang Tao, Shen Gao, Mingyue Shang, Wei Wu, Dongyan Zhao, and Rui
Yan. 2018. Get the Point of My Utterance! Learning Towards Effective Responses
with Multi-head Attention Mechanism. In IJCAI. 4418–4424.
[21] Chongyang Tao, Lili Mou, Dongyan Zhao, and Rui Yan. 2018. RUBER: An
Unsupervised Method for Automatic Evaluation of Open-Domain Dialog Systems.
In AAAI. 722–729.
[22] Chongyang Tao, Wei Wu, Can Xu, Wenpeng Hu, Dongyan Zhao, and Rui Yan.
2019. Multi-Representation Fusion Network for Multi-Turn Response Selection
in Retrieval-Based Chatbots. In WSDM. 267–275.
[23] Ashwin K. Vijayakumar, Michael Cogswell, Ramprasaath R. Selvaraju, Qing Sun,
Stefan Lee, David J. Crandall, and Dhruv Batra. 2018. Diverse Beam Search for
Improved Description of Complex Scenes. In AAAI. 7371–7379.
[24] Chenglong Wang, Feijun Jiang, and Hongxia Yang. 2017. A Hybrid Framework
for Text Modeling with Convolutional RNN. In SIGKDD. 2061–2069.
[25] Hao Wang, Zhengdong Lu, Hang Li, and Enhong Chen. 2013. A Dataset for
Research on Short-Text Conversations. In EMNLP. 935–945.
[26] Jun Wang, Lantao Yu, Weinan Zhang, Yu Gong, Yinghui Xu, Benyou Wang, Peng
Zhang, and Dell Zhang. 2017. IRGAN: A Minimax Game for Unifying Generative
and Discriminative Information Retrieval Models. In SIGIR. 515–524.
[27] J. Weston, E. Dinan, and A. H. Miller. 2018. Retrieve and Refine: Improved
Sequence Generation Models For Dialogue. CoRR abs/1808.04776 (2018).
[28] Yu Wu, Furu Wei, Shaohan Huang, Zhoujun Li, and Ming Zhou. 2018. Response
Generation by Context-aware Prototype Editing. CoRR abs/1806.07042 (2018).
[29] Yu Wu, Wei Wu, Chen Xing, Ming Zhou, and Zhoujun Li. 2017. Sequential
matching network: A new architecture for multi-turn response selection in
retrieval-based chatbots. In ACL. 496–505.
[30] Yu Wu, Wei Wu, Dejian Yang, Can Xu, and Zhoujun Li. 2018. Neural Response
Generation With Dynamic Vocabularies. In AAAI. 5594–5601.
[31] Chen Xing, Wei Wu, Yu Wu, Jie Liu, Yalou Huang, Ming Zhou, and Wei-Ying Ma.
2017. Topic Aware Neural Response Generation. In AAAI. 3351–3357.
[32] Jingjing Xu, Xu Sun, Xuancheng Ren, Junyang Lin, Bingzhen Wei, and Wei
Li. 2018. DP-GAN: Diversity-Promoting Generative Adversarial Network for
Generating Informative and Diversified Text. In EMNLP. 3940–3949.
[33] Rui Yan, Yiping Song, and Hua Wu. 2016. Learning to Respond with Deep Neural
Networks for Retrieval-Based Human-Computer Conversation System. In SIGIR.
55–64.
[34] Rui Yan and Dongyan Zhao. 2018. Coupled context modeling for deep chit-chat:
towards conversations between human and computer. In SIGKDD. 2574–2583.
[35] Rui Yan, Dongyan Zhao, and Weinan E. 2017. Joint learning of response ranking
and next utterance suggestion in human-computer conversation system. In SIGIR.
685–694.
[36] Zhen Yang, Wei Chen, Feng Wang, and Bo Xu. 2018. Improving Neural Machine
Translation with Conditional Sequence Generative Adversarial Nets. In NAACL.
[37] Lili Yao, Yaoyuan Zhang, Yansong Feng, Dongyan Zhao, and Rui Yan. 2017.
Towards Implicit Content-Introducing for Generative Short-Text Conversation
Systems. In EMNLP. 2190–2199.
[38] T. Young, E. Cambria, I. Chaturvedi, M. Huang, H. Zhou, and S. Biswas. 2018.
Augmenting End-to-End Dialogue Systems with Commonsense Knowledge. In
AAAI. 4970–4977.
[39] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. SeqGAN: Sequence
Generative Adversarial Nets with Policy Gradient. In AAAI. 2852–2858.
[40] Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi. 2017. Learning Discourse-level
Diversity for Neural Dialog Models using Conditional Variational Autoencoders.
In ACL. 654–664.
[41] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. 2017. Unpaired
Image-to-Image Translation using Cycle-Consistent Adversarial Networks. In
ICCV, Vol. 2223–2232.

CONCLUSION AND FUTURE WORK

In this paper, we proposed a novel generative adversarial framework
that aims at enhancing a conversation retrieval-generation ensemble model by unifying GAN mechanism for both generative and
retrieval approaches. The ensembleGAN enables the two generators
to generate responses getting higher scores from the discriminative
ranker, while the discriminator scores down adversarial samples
and selects responses featuring merits of both generators, allowing
for both generation and retrieval-based methods to be mutually
enhanced. Experimental results on a large STC dataset demonstrate
that our ensembleGAN outperforms other GAN mechanism on
both human and automatic evaluation metrics and is capable of
bringing better global optimal results.

ACKNOWLEDGMENTS
We would like to thank the anonymous reviewers for their constructive comments. This work was supported by the National Key Research and Development Program of China (No. 2017YFC0804001),
the National Science Foundation of China (NSFC Nos. 61672058
and 61876196).

REFERENCES
[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine
Translation by Jointly Learning to Align and Translate. In ICLR.
[2] Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang Tang. 2017. A Survey on
Dialogue Systems: Recent Advances and New Frontiers. SIGKDD Explorations
19, 2 (2017), 25–35.
[3] Bo Dai, Sanja Fidler, Raquel Urtasun, and Dahua Lin. 2017. Towards Diverse and
Natural Image Descriptions via a Conditional GAN. In ICCV. 2989–2998.
[4] Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters.
Psychological Bulletin 76, 5 (1971), 378–382.
[5] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David WardeFarley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. 2014. Generative
Adversarial Nets. In NIPS. 2672–2680.
[6] R Herbrich. 2008. Large margin rank boundaries for ordinal regression. Advances
in Large Margin Classifiers 88 (2008).
[7] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory.
Neural Computation 9, 8 (1997), 1735–1780.
[8] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In ICLR.
[9] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A
Diversity-Promoting Objective Function for Neural Conversation Models. In
NAACL. 110–119.
[10] Jiwei Li, Will Monroe, Tianlin Shi, Sébastien Jean, Alan Ritter, and Dan Jurafsky.
2017. Adversarial Learning for Neural Dialogue Generation. In EMNLP. 2157–
2169.
[11] Kevin Lin, Dianqi Li, Xiaodong He, Ming-Ting Sun, and Zhengyou Zhang. 2017.
Adversarial Ranking for Language Generation. In NIPS. 3158–3168.

444

