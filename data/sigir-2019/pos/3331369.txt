Short Research Papers 3C: Search

SIGIR ’19, July 21–25, 2019, Paris, France

Query Performance Prediction for Pseudo-Feedback-Based
Retrieval
Haggai Roitman

Oren Kurland

IBM Research – Haifa
haggai@il.ibm.com

Technion – Israel Institute of Technology
kurland@ie.technion.ac.il

ABSTRACT

that the final result list will be effective regardless of the querymodel induction approach employed. Accordingly, our novel approach for QPP for pseudo-feedback-based retrieval accounts for
the presumed effectiveness of the initially retrieved list, its association with the final retrieved list and properties of the latter.
Empirical evaluation shows that the prediction quality of our approach substantially transcends that of state-of-the-art prediction
methods adopted for the pseudo-feedback-based retrieval setting
— the practice in prior work on QPP for pseudo-feedback-based
retrieval [6, 14, 15].

The query performance prediction task (QPP) is estimating retrieval
effectiveness in the absence of relevance judgments. Prior work
has focused on prediction for retrieval methods based on surface
level query-document similarities (e.g., query likelihood). We address the prediction challenge for pseudo-feedback-based retrieval
methods which utilize an initial retrieval to induce a new query
model; the query model is then used for a second (final) retrieval.
Our suggested approach accounts for the presumed effectiveness
of the initially retrieved list, its similarity with the final retrieved
list and properties of the latter. Empirical evaluation demonstrates
the clear merits of our approach.

2

ACM Reference Format:
Haggai Roitman and Oren Kurland. 2019. Query Performance Prediction
for Pseudo-Feedback-Based Retrieval. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information
Retrieval (SIGIR ’19), July 21–25, 2019, Paris, France. ACM, New York, NY,
USA, 4 pages. https://doi.org/10.1145/3331184.3331369

1

RELATED WORK

In prior work on QPP for pseudo-feedback-based retrieval, existing
predictors were applied either to the final retrieved list [6, 14] or to
the initially retrieved list [15]. We show that our prediction model,
which incorporates prediction for both lists and accounts for their
association, substantially outperforms these prior approaches.
The selective query expansion task (e.g., [1, 5, 13]) is to decide
whether to use the pseudo-feedback-based query model, or stick
to the original query. In contrast, we predict performance for a list
retrieved using the query model.
In several prediction methods, a result list retrieved using a pseudofeedback-based query model is used to predict the performance of
the initially retrieved list [15, 20]. In contrast, our goal is to predict the effectiveness of the final result list; to that end, we also use
prediction performed for the initial list.

INTRODUCTION

The query performance prediction task (QPP) has attracted much
research attention [2]. The goal is to evaluate search effectiveness
with no relevance judgments. Almost all existing QPP methods are
(implicitly or explicitly) based on the assumption that retrieval is
performed using (only) document-query surface-level similarities
[2]; e.g., standard language-model-based retrieval or Okapi BM25.
We address the QPP challenge for a different, common, retrieval
paradigm: pseudo-feedback-based retrieval [3]. That is, an initial
search is performed for a query. Then, top-retrieved documents,
considered pseudo relevant, are utilized to induce a query model
(e.g., expanded query form) that is used for a second (final) retrieval.
Thus, in contrast to the single-retrieval setting addressed in almost all prior work on QPP, here the effectiveness of the final result
list presented to the user depends not only on the retrieval used to
produce it (e.g., properties of the induced query model), but also
on the initial retrieval using which the query model was induced.
A case in point, if the initial retrieval is poor, it is highly unlikely

3

PREDICTION FRAMEWORK

Suppose that some initial search is applied in response to a query
q over a document corpus D. Let D init be the list of the k most
highly ranked documents. Information induced from the top documents in D init is used for creating a new query model (e.g., expanded query) used for a second retrieval; i.e., these documents
are treated as pseudo relevant. We use D scnd to denote the result
list, presented to the user who issued q, of the k documents most
highly ranked in the second retrieval.
Our goal is to predict the effectiveness of D scnd . To this end,
we appeal to a recently proposed query performance prediction
(QPP) framework [16]. Specifically, the prediction task amounts to
estimating the relevance likelihood of D scnd , p(D scnd |q, r ), where
r is a relevance event1 .
We can use reference document lists to derive an estimate for
p(D scnd |q, r ) [16]:

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’19, July 21–25, 2019, Paris, France
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00
https://doi.org/10.1145/3331184.3331369

1 This

is post-retrieval prediction which relies on analyzing the retrieved list. The relevance of a retrieved list is a notion that generalizes that for a single document [16].
At the operational level, a binary relevance judgment for a list can be obtained by
thresholding any list-based evaluation measure.

1261

Short Research Papers 3C: Search

def

p̂(D scnd |q, r ) =

Õ

SIGIR ’19, July 21–25, 2019, Paris, France

p̂(D scnd |q, L, r )p̂(L|q, r );

p̂ D (·) is a language model induced from the corpus. Further details
about language model induction are provided in Section 4.1.

(1)

L

L is a document list retrieved for q; herein, p̂ is an estimate for p.
The underlying idea is that strong association (e.g., similarity) of
D scnd with reference lists L (i.e., high p̂(D scnd |q, L, r )) which are
presumably effective (i.e., high p̂(L|q, r )) attests to retrieval effectiveness.
It was shown that numerous existing post-retrieval prediction
methods can be instantiated from Equation 1 where a single reference list is used. Similarly, here we use D init as a reference list:
p̂(D sc nd |q, r ) ≈ p̂(D sc nd |q, D ini t , r )p̂(D ini t |q, r ).

4 EVALUATION
4.1 Experimental setup
4.1.1 Datasets. We used for evaluation the following TREC corpora and topics: WT10g (451-550), GOV2 (701-850), ROBUST (301450, 601-700) and AP (51-150). These datasets are commonly used
in work on QPP [2]. Titles of TREC topics were used as queries.
We used the Apache Lucene2 open source search library for indexing and retrieval. Documents and queries were processed using Lucene’s English text analysis (i.e., tokenization, lowercasing,
Porter stemming and stopping). For the retrieval method — both
the initial retrieval and the second one using the induced query
model — we use the language-model-based cross-entropy scoring
(Lucene’s implementation) with Dirichlet smoothed document language models where the smoothing parameter was set to 1000.

(2)

That is, by the virtue of the way D scnd is created — i.e., using
information induced from D init — we assume that D init is the
most informative reference list with respect to D scnd ’s effectiveness. A case in point, an expanded query constructed from a poor
initial list (i.e., which mostly contains non-relevant documents) is
not likely to result in effective retrieval.

4.1.2

3.1

Instantiating Predictors

def

Equation 2 can be instantiated in various ways, based on the choice
of estimates, to yield a specific prediction method. To begin with,
any post-retrieval predictor, P, can be used to derive p̂(D init |q, r )
[16].
For p̂(D scnd |q, D init , r ) in Equation 2, we use logarithmic interpolation:

def

def

p̂(D sc nd |q, D ini t , r ) = p̂ [P] (D sc nd |q, r )α p̂(D sc nd |D ini t , r )(1−α ) ;
(3)

α (∈ [0, 1]) is a free parameter. The estimate p̂ [P] (D scnd |q, r ) corresponds to the predicted effectiveness of D scnd , where the prediction, performed using the post-retrieval predictor P, ignores the
knowledge that D scnd was produced using information induced
from D init .
The estimate p̂(D scnd |D init , r ) from Equation 3, of the association between D scnd and D init , is usually devised based on some
symmetric inter-list similarity measure sim(D scnd , D init ) [16]. However, as Roitman [11] has recently suggested, a more effective estimate can be derived by exploiting the asymmetric co-relevance relationship between the two lists (cf., [10]); that is, p̂(D scnd |D init , r )
is the likelihood of D scnd given that a relevance event has happened and D init was observed:
def

p̂(D scnd |D init , r ) =
Õ
p̂(D scnd |D init )

d ∈D scnd

note x’s Dirichlet-smoothed language model, where p D (w) =
c D (w )
| D | . For a query q and a set of pseudo-relevant documents F ⊆
D init , p F (·) denotes a pseudo-feedback-based query model.
We use three state-of-the-art pseudo-feedback-based (PRF) querymodel induction methods. All three incorporate query anchoring
as described below. The first is the Relevance Model [8] (RM):
def Õ [0]
[µ]
p F (w) =
(6)
pd (w)pq (d),
d ∈F

[µ]

def

where pq (d) =

[µ ]

pd (q)
Í [µ ]
p̂d ′ (q)

[µ]

def

and pd (q) =

d ′ ∈F

Í

w ∈q

[µ]

cd (w) log pd (w).

The second is the Generative Mixed Model [19] (GMM) which is
estimated using the following EM algorithm iterative update rules:
def

t (n) (w ) =

(n−1)

(1−β )p F

(w )

,
(n−1)
(1−β )p F
(w )+β p D (w )

(n)

def

p F (w ) =

Í

(n) (w )
dÍ
∈F c d (w )t
′ (n) (w ′ ) .
d ∈F c d (w )t

Í

w ′ ∈V

The third is the Maximum-Entropy
Divergence Minimization Model
[9]


Í
[µ]
[0]
(MEDMM): p F (w) ∝ exp γ1
p̂q (d) log pd (w) − γν p D (w) .
d ∈F

def

p̂(d |D scnd )

p̂(d, r |D init )
;
p̂(d |D init )p̂(r |D init )

(4)
def

d is a document. Following Roitman [11], we use p̂(D scnd |D init ) =
sim(D scnd , D init ). Similarly to some prior work [7, 11], for p̂(r |D init )
we use the entropy of the centroid (i.e., the arithmetic mean) of the
language models of documents in D init . We further assume that
p̂(d |D scnd ) and p̂(d |D init ) are uniformly distributed over D scnd
and D init , respectively. Finally, to derive p̂(d, r |D init ), we follow
Roitman [11] and use the corpus-based regularized cross entropy
(CE) between a relevance model, R [Dinit ], induced from D init , and
a language model, pd (·), induced from d:
def

Pseudo-feedback based retrieval. Let c x (w) denote the oc-

currence count of a term w in a text (or text collection) x; let |x | =
Í
def c x (w )+µp D (w )
[µ]
dew ∈x c x (w) denote x’s length. Let px (w) =
|x |+µ

p̂(d, r |D init ) = CE(R [Dinit ] ||pd (·)) − CE(R [Dinit ] ||p D (·));

(5)

We applied query anchoring [8, 9, 19] to all three models: p F, λ (w) =
λpqM LE (w) + (1 − λ)p F (w); pqM LE (w) is the maximum likelihood estimate of w with respect to q and λ ∈ [0, 1].
We used the n most highly ranked documents in the initial retrieval for query-model induction (i.e., inducing p F (·)). Then, a second query q f was formed using the l terms w assigned the highest
p F (w). We resubmitted q f to Lucene3 to obtain D scnd .
4.1.3 Baseline predictors. As a first line of baselines, we use
Clarity [4], WIG [20] and NQC [17], which are commonly used
post-retrieval QPP methods [2]. These baselines are also used for
P in Eq. 3. Clarity [4] is the divergence between a language model
induced from a retrieved list and that induced from the corpus.
2 http://lucene.apache.org
3 Expressed in Lucene’s query parser syntax as: w

1262

1ˆp F (w 1 ) w 2ˆp F (w 2 ) .

. . w lˆp F (w l ).

Short Research Papers 3C: Search

SIGIR ’19, July 21–25, 2019, Paris, France

Table 1: Prediction quality. Boldface: best results per basic QPP method and query-model induction method. Underlined: best
results per query-model induction method. ’∗’ marks a statistically significant difference between PFR-QPP and either the
second best predictor (in case PFR-QPP is the best) or the best predictor (in case PFR-QPP is not).

Method
ListSim
NQC(D sc nd |q f )
NQC(D sc nd |q)
NQC(D ini t |q)
RefList(NQC)
PFR-QPP(NQC)
Clarity(D sc nd |q f )
Clarity(D sc nd |q)
Clarity(D ini t |q)
RefList(Clarity)
PFR-QPP(Clarity)
WIG(D sc nd |q f )
WIG(D sc nd |q)
WIG(D ini t |q)
RefList(WIG)
PFR-QPP(WIG)
WEG(D sc nd |q f )
WEG(D sc nd |q)
WEG(D ini t |q)
RefList(WEG)
PFR-QPP(WEG)

RM
.442
.293
.071
.483
.535
.513∗
.292
.327
.363
.481
.408∗
.270
.253
.237
.338
.370∗
.231
.141
.353
.443
.456∗

WT10g
GMM MEDMM
.532
.337
.228
.182
.051
.092
.397
.424
.531
.415
.557∗
.410
.325
.316
.227
.368
.350
.314
.567
.388
.557∗
.398∗
.307
.388
.105
.153
.221
.224
.384
.311
.466∗
.353∗
.205
.331
.134
.239
.311
.313
.483
.371
.575∗
.436∗

RM
.490
.599
.437
.486
.517
.596
.230
.278
.282
.480
.615∗
.263
.583
.562
.581
.630∗
.585
.513
.532
.527
.660∗

GOV2
GMM MEDMM
.432
.410
.545
.353
.418
.283
.414
.414
.486
.457
.549
.550∗
.157
.130
.200
.084
.261
.264
.469
.414
.497∗
.490∗
.301
.448
.424
.276
.498
.498
.562
.480
.603∗
.575∗
.548
.432
.504
.390
.470
.409
.481
.427
.562∗
.481∗

WIG [20] and NQC [17] are the corpus-regularized4 mean and
standard deviation of retrieval scores in the list, respectively. We
further compare with the Weighted Expansion Gain (WEG) [6] method
– a WIG alternative which regularizes with the mean score of documents at low ranks of the retrieved list instead of the corpus.
We use three variants of each of the four predictors described
above. The first two directly predict the effectiveness of the final
retrieved list D scnd using either (i) the original query q (denoted
P(D scnd |q)), or (ii) the query q f (denoted P(D scnd |q f )) which
was induced from D init as described above (cf., [15, 20]). The third
variant (denoted P(D init |q)) is based on predicting the performance
of D scnd by applying the predictor to D init as was the case in [15].
To evaluate the impact of our inter-list association measure in
Eq. 4, we use two additional baselines. The first, denoted ListSim
[16], uses sim(D scnd , D init ) to predict the performance of D scnd .
The second, denoted RefList(P) [7, 16], treats D init as a pseudoeffective list of D scnd and estimates D scnd ’s performance by:
def

p̂Ref List (r |D scnd , q) = sim(D scnd , D init )p̂ [P] (D init |q, r ),
where P is one of the four basic QPP methods described above.
There are two important differences between our proposed method
and RefList. First, we use the query q in the list association measure in Eq. 3. Second, we use an asymmetric co-relevance measure
between the two lists in Eq. 4 compared to the symmetric one used
in RefList.
4.1.4 Setup. Hereinafter, we refer to our proposed QPP method
from Eq. 2 as PFR-QPP: Pseudo-Feedback based Retrieval QPP.
4 To

this end, the corpus is treated as one large document.

1263

RM
.543
.653
.475
.635
.654
.671∗
.450
.412
.452
.582
.589∗
.424
.651
.649
.660
.665∗
.661
.566
.635
.654
.688∗

ROBUST
GMM MEDMM
.528
.436
.637
.622
.492
.620
.605
.602
.631
.621
.661∗
.642∗
.393
.409
.350
.349
.441
.401
.575
.535
.607∗
.566∗
.361
.381
.455
.430
.618
.578
.638
.637
.682∗
.648∗
.656
.693
.571
.674
.619
.616
.633
.632
.664∗
.688∗

RM
.537
.655
.574
.550
.607
.670∗
.313
.236
.320
.589
.652∗
.159
.414
.554
.639
.650∗
.627
.560
.526
.580
.675∗

AP
GMM
.343
.617
.479
.536
.530
.640∗
.408
.350
.456
.519
.585∗
.281
.281
.614
.580
.634∗
.562
.491
.474
.467
.552∗

MEDMM
.407
.454
.530
.502
.572
.650∗
.339
.270
.308
.511
.651∗
.285
.226
.505
.608
.643∗
.575
.575
.518
.555
.664∗

PFR-QPP(P) is a specific predictor instantiated using the base predictor P. We predict for each query the effectiveness of the 1000
documents (i.e., k = 1000) most highly ranked in the final result
list D scnd . Prediction quality is measured using the Pearson correlation between the ground truth AP@1000 (according to TREC’s
relevance judgments) and the values assigned to the queries by a
prediction method.
Most prediction methods described above incorporate free parameters. Following the common practice [2], we set m ≤ k – the
number of documents in a given list (i.e., either D scnd or D init )
used for calculating a given predictor’s value; with m ∈ {5, 10, 20, 50,
100, 150, 200, 500, 1000}. We applied term clipping with l terms (l ∈
{10, 20, 50, 100}) to the relevance model used in Clarity and PFRQPP. Following [16], we realized the ListSim, RefList and PFRQPP baselines using Rank-Biased Overlap (RBO(p)) [18] as our listsimilarity measure sim(·) (with p = 0.95, further following [16]).
For our PFR-QPP method, we set α ∈ {0, 0.1, 0.2, . . . , 0.9, 1.0}.
Query models are induced from the n = 50 top documents. For the
GMM and MEDMM models, we set their free-parameters to previously recommended values, i.e., GMM (β = 0.5) [19] and MEDMM
(γ = 1.2, ν = 0.1) [9]. Unless stated otherwise, the query anchoring and clip-size parameters in all models were fixed to λ = 0.9 and
l = 20, respectively. The prediction quality for other (λ, l) settings
is studied in Section 4.2.
Following [12, 17], we trained and tested all methods using a
2-fold cross validation approach. Specifically, in each dataset, we
generated 30 random splits of the query set; each split had two
folds. We used the first fold as the (query) train set. We kept the
second fold for testing. We recorded the average prediction quality

Short Research Papers 3C: Search

Clarity
0.6

0.5
0.4
0.3

0.5
0.4

0.2

0.4

0.6

0.8

0

0.2

WIG

0.6

0.5
0.4
0.3
0.4

0.6

0.8

30

λ

50

70

0.5
0.4
0.4

10

30

0.6

λ

(a) Query anchoring (λ).

0.8

50

70

90

l

WEG

5

0.7

0.6
0.5
0.4
0.3

0.2

0.4

90

WIG

0.7

0.6

0

0.5

l

0.3
0.2

0.6

0.3
10

0.8

Pearson

Pearson

Pearson

0.4

WEG

0.7

0.6

0

0.4

λ

λ
0.7

0.5

Pearson

0

0.7

0.6

0.3

0.3

more “verbose”, with less emphasis on the original query q. Indeed,
a recent study showed that existing QPP methods are less robust
for long queries [12]. Finally, we see that for any value of λ and l,
PFR-QPP outperforms the baselines.

Clarity

NQC

0.7

Pearson

0.7

0.6

Pearson

0.7

Pearson

Pearson

NQC

SIGIR ’19, July 21–25, 2019, Paris, France

0.6
0.5

We addressed the QPP task for pseudo-feedback-based retrieval,
where the final retrieved list depends on an initially retrieved list –
e.g., via a query model induced from the latter and used to produce
the former. Our approach accounts for the predicted effectiveness
of each of the two lists as well as to their asymmetric co-relevance
relation. Empirical evaluation showed that our approach significantly outperforms a variety of strong baselines.

0.4
0.3

10

30

50

l

70

90

10

30

50

70

90

l

(b) Number of terms (l ).

Figure 1: Sensitivity to free-parameter values of the relevance model used for query-model induction.

ACKNOWLEDGEMENT
We thank the reviewers for their comments. This work was supported in part by the Israel Science Foundation (grant no. 1136/17)

over the 30 splits. Finally, we measured statistically significant differences of prediction quality using a two-tailed paired t-test with
p < 0.05 computed over all 30 splits.

4.2

CONCLUSIONS

REFERENCES
[1] Giambattista Amati, Claudio Carpineto, and Giovanni Romano. Query difficulty,
robustness, and selective application of query expansion. In Proceedings of ECIR,
pages 127–137, 2004.
[2] David Carmel and Oren Kurland. Query performance prediction for ir. In Proceedings of SIGIR, pages 1196–1197, New York, NY, USA, 2012. ACM.
[3] Claudio Carpineto and Giovanni Romano. A survey of automatic query expansion in information retrieval. ACM Comput. Surv., 44(1):1:1–1:50, January 2012.
[4] Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft. Predicting query performance. In Proceedings of SIGIR, pages 299–306, New York, NY, USA, 2002.
ACM.
[5] Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft. A framework for selective query expansion. In Proceedings of CIKM, pages 236–237, New York, NY,
USA, 2004. ACM.
[6] Ahmad Khwileh, Andy Way, and Gareth J. F. Jones. Improving the reliability of
query expansion for user-generated speech retrieval using query performance
prediction. In CLEF, 2017.
[7] Oren Kurland, Anna Shtok, Shay Hummel, Fiana Raiber, David Carmel, and Ofri
Rom. Back to the roots: A probabilistic framework for query-performance prediction. In Proceedings of CIKM, pages 823–832, New York, NY, USA, 2012. ACM.
[8] Victor Lavrenko and W. Bruce Croft. Relevance based language models. In
Proceedings of SIGIR.
[9] Yuanhua Lv and ChengXiang Zhai. Revisiting the divergence minimization feedback model. In CIKM ’14.
[10] Fiana Raiber, Oren Kurland, Filip Radlinski, and Milad Shokouhi. Learning asymmetric co-relevance. In Proceedings of ICTIR, pages 281–290, 2015.
[11] Haggai Roitman. Enhanced performance prediction of fusion-based retrieval. In
Proceedings of ICTIR, pages 195–198, New York, NY, USA, 2018. ACM.
[12] Haggai Roitman. An extended query performance prediction framework utilizing passage-level information. In Proceedings of ICTIR, pages 35–42, New York,
NY, USA, 2018. ACM.
[13] Haggai Roitman, Ella Rabinovich, and Oren Sar Shalom. As stable as you are:
Re-ranking search results using query-drift analysis. In Proceedings of HT, pages
33–37, New York, NY, USA, 2018. ACM.
[14] Harrisen Scells, Leif Azzopardi, Guido Zuccon, and Bevan Koopman. Query
variation performance prediction for systematic reviews. In Proceedings of SIGIR,
pages 1089–1092, New York, NY, USA, 2018. ACM.
[15] Anna Shtok, Oren Kurland, and David Carmel. Using statistical decision theory
and relevance models for query-performance prediction. In Proceedings of SIGIR,
pages 259–266, New York, NY, USA, 2010. ACM.
[16] Anna Shtok, Oren Kurland, and David Carmel. Query performance prediction
using reference lists. ACM Trans. Inf. Syst., 34(4):19:1–19:34, June 2016.
[17] Anna Shtok, Oren Kurland, David Carmel, Fiana Raiber, and Gad Markovits.
Predicting query performance by query-drift estimation. ACM Trans. Inf. Syst.,
30(2):11:1–11:35, May 2012.
[18] William Webber, Alistair Moffat, and Justin Zobel. A similarity measure for
indefinite rankings. ACM Trans. Inf. Syst., 28(4):20:1–20:38, November 2010.
[19] Chengxiang Zhai and John Lafferty. Model-based feedback in the language modeling approach to information retrieval. In CIKM ’01.
[20] Yun Zhou and W. Bruce Croft. Query performance prediction in web search
environments. In Proceedings of SIGIR, pages 543–550, New York, NY, USA, 2007.
ACM.

Results

Table 1 reports the prediction quality of our method and the baselines. We can see that in the vast majority of cases, our PFR-QPP
approach statistically significantly outperforms the baselines.
Applying basic QPP methods. We first compare the three variants of the four basic QPP methods. We observe that, in most cases,
utilizing the PRF-induced query q f for predicting the performance
of the final list D scnd (P(D scnd |q f )), yields better prediction quality than using the original query q (P(D scnd |q)). In addition, predicting the performance of D scnd by applying the base predictor
to the initially retrieved list D init (P(D init |q)) yields high prediction quality — sometimes even higher than applying the predictor
to D scnd . These findings provide further support the motivation
behind PFR-QPP: integrating prediction for the initially retrieved
list and the final retrieved list and accounting for their asymmetric
co-relevance relation.
PFR-QPP vs. reference-list based alternatives. First, in line with
previous work [7, 15, 16], the high prediction quality of ListSim
and RefList in our setting shows that the similarity between the
two lists is an effective performance indicator. Moreover, combining prediction for the performance of the initial list with its similarity with the final list (i.e., RefList) yields prediction quality
that transcends in most cases that of using only the similarity (i.e.,
ListSim). Finally, our PFR-QPP method which uses prediction for
both the initial and final lists, and accounts for their asymmetric
co-relevance relationship, outperforms both ListSim and RefList
in most cases, and often to a statistically significant degree.
Sensitivity to query-model induction tuning. Using the ROBUST
dataset and the relevance model (RM), Figure 1 reports the effect
on prediction quality of varying the value of the query anchoring
parameter (λ; while fixing l = 20) and the number of terms used
after clipping (l; while fixing λ = 0) in the query model, and hence,
in q f . As can be seen, decreasing λ or increasing l decreases the
prediction quality of all methods. With reduced query anchoring
or when using more terms, the induced queries (q f ) tend to become

1264

