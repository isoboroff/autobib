Short Research Papers 2A: AI, Mining, and others

SIGIR ’19, July 21–25, 2019, Paris, France

A study on the Interpretability of Neural Retrieval Models using
DeepSHAP
Zeon Trevor Fernando

Jaspreet Singh

Avishek Anand

L3S Research Center
Hannover, Germany
fernando@l3s.de

L3S Research Center
Hannover, Germany
singh@l3s.de

L3S Research Center
Hannover, Germany
anand@l3s.de

ABSTRACT

NNs in the wild and having them work in tandem with humans.
Explanations can help debug models, determine training data bias
and understand decisions made in simpler terms in order to foster
trust. Recently in IR, models such as DRMM [5], MatchPyramid [10],
PACRR-DRMM [8] and others have shown great promise in ranking
for adhoc text retrieval. While these models do improve state-ofthe-art on certain benchmarks, it is sometimes hard to understand
why exactly these models are performing better. With the increased
scrutiny on automated decision making systems, including search
engines, it is vital to be able to explain decisions made. In IR however,
little to no work has been done on trying to explain the output of
complex neural ranking models.
In the ML community, several post-hoc non-intrusive methods
have been suggested recently which enable us to train highly accurate and complex models while also being able to get a sense of
their rationale. One of the more popular approaches to producing
explanations is to determine the input feature attributions for a
given instance and it’s prediction according to a given model. The
output of such a method is typically visualized as a heat map over
the input words/pixels. Several approaches have been proposed in
this direction for image and text classification but their applicability to adhoc text retrieval and ranking remains unexplored. In
this paper we study the applicability of one such method designed
specifically for neural networks – DeepSHAP [7], to explain the
output of 3 different neural retrieval models. DeepSHAP is a modification of the DeepLift [15] algorithm to efficiently estimate the
shapley values over the input feature space for a given instance.
The shapley value is a term coined by Shapley [14] in cooperative
game theory to refer to the contribution of a feature in a prediction.
More specifically, shapley values explain the contribution of an
input feature towards the difference in the prediction made vs the
average prediction value.
The objective of our work is to utilize DeepSHAP to explain
NRMs which should ideally be a trivial pursuit since they are standard neural networks. However, in our experiments, we found that
DeepSHAP’s explanations are highly dependent on a reference
input which is used to compute the average prediction. This is
inline with recent work that suggests approaches like DeepLIFT
lack robustness [4]. In this work, we ponder on the question, what
makes a good reference input distribution for neural rankers? In
computer vision, a plain black image is used as the reference input but what is the document equivalent of such an image in IR?
Furthermore, we found that explanations produced by the model
introspective DeepSHAP are considerably different from the model
agnostic approach – LIME [12]. Although both models produce
local explanations, the variability is concerning.

A recent trend in IR has been the usage of neural networks to
learn retrieval models for text based adhoc search. While various
approaches and architectures have yielded significantly better performance than traditional retrieval models such as BM25, it is still
difficult to understand exactly why a document is relevant to a
query. In the ML community several approaches for explaining
decisions made by deep neural networks have been proposed –
including DeepSHAP which modifies the DeepLift algorithm to
estimate the relative importance (shapley values) of input features
for a given decision by comparing the activations in the network for
a given image against the activations caused by a reference input.
In image classification, the reference input tends to be a plain black
image. While DeepSHAP has been well studied for image classification tasks, it remains to be seen how we can adapt it to explain the
output of Neural Retrieval Models (NRMs). In particular, what is a
good “black” image in the context of IR? In this paper we explored
various reference input document construction techniques. Additionally, we compared the explanations generated by DeepSHAP to
LIME (a model agnostic approach) and found that the explanations
differ considerably. Our study raises concerns regarding the robustness and accuracy of explanations produced for NRMs. With this
paper we aim to shed light on interesting problems surrounding
interpretability in NRMs and highlight areas of future work.
ACM Reference Format:
Zeon Trevor Fernando, Jaspreet Singh, and Avishek Anand. 2019. A study
on the Interpretability of Neural Retrieval Models using DeepSHAP. In
Proceedings of the 42nd International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR ’19), July 21–25, 2019, Paris,
France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.
3331312

1

INTRODUCTION

Deep neural networks have achieved state of the art results in
several NLP and computer vision tasks in the last decade. Along
with this spurt in performance has come a new wave of approaches
trying to explain decisions made by these complex machine learning
models. Explainablilty and interpretability are key to deploying
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’19, July 21–25, 2019, Paris, France
© 2019 Association for Computing Machinery.
ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00
https://doi.org/10.1145/3331184.3331312

1005

Short Research Papers 2A: AI, Mining, and others

2

SIGIR ’19, July 21–25, 2019, Paris, France

RELATED WORK

the query-document instance to be explained and experiment with
various reference inputs for the document. The intuition behind
doing so is to gain an average reference output in the locality of
the query.
The various document reference inputs that we considered in
our experiments are:

There are two main approaches to interpretability in machine learning models: model agnostic and model introspective approaches.
Model agnostic approaches [12, 13] generate post-hoc explanations for the original model by treating it as a black box by learning
an interpretable model on the output of the model or by perturbing
the inputs or both. Model introspective approaches on one hand
include “interpretable” models such as decision trees [6], attentionbased networks [20], and sparse linear models [19] where there
is a possibility to inspect individual model components (path in a
decision tree, feature weights in linear models) to generate useful
explanations. On the other hand, there are gradient-based methods
like [16] that generates attributions by considering the partial derivative of the output with respect to the input features. Following
this, there were many works [1, 2, 7, 15] that generate attributions
by inspecting the neural network architectures.
Interpretability in ranking models Recently there have been
few works focused on interpretability [17, 18] and diagnosis of neural IR models [9, 11]. In the diagnostic approaches, they use the formal retrieval constraints (“axioms”) defined for traditional retrieval
models to find the differences between neural IR and learningto-rank approaches with hand-crafted features through a manual
error analysis [9] or build diagnostic datasets based on the axioms
to empirically analyse these models [11]. In [18] they built a explainable search system (EXS) that adapts a local model agnostic
interpretability approach (LIME [12]) to explain the relevance of
a document for a query for various neural IR models. In [17] they
propose an approach that understands the query intent encoded by
NRMs by learning a simple ranking model with an expanded query
that approximates the original ranking.
To the best of our knowledge, this is the first work that looks at
model introspective interpretability specifically for NRMs.

3

OOV The reference document consists of ‘OOV’ tokens. For,
DRMM and MatchPyramid models the embedding vector
for ‘OOV’ comprises of all-zeros which is similar to the
background image used for MNIST. But for PACRR-DRMM, it
is the average of all the embedding vectors in the vocabulary.
IDF lowest The reference document is constructed by sampling
words with low IDF scores. These words are generally stopwords or words that are similar to stop-words so they should,
in general, be irrelevant to the query.
QL lowest The reference document comprises of sampled words
with low query-likelihood scores that are derived from a language model of the top-1000 documents.
COLLECTION rand doc The reference document is randomly
sampled from the rest of the collection minus the top-1000
documents retrieved for the query.
TOPK LIST rand doc from bottom The reference document is
randomly sampled from the bottom of the top-1000 documents retrieved.
These variants were designed based on the intuition that the reference input document would comprise of words that are irrelevant
to the query and thus DeepSHAP should be able to pick the most
important terms from the input document that explain relevance
to the query.

4

EXPERIMENTAL SETUP

In our experiments, we aim to answer the following research questions:

DEEPSHAP FOR IR

• Are DeepSHAP explanations sensitive to the type of reference input in the case of NRMs?
• Can we determine which reference input produces the most
accurate local explanation?

DeepSHAP is a local model-introspective interpretability method
to approximate the shapley values using DeepLIFT [15]. DeepLIFT
explains the difference in output/prediction from some ‘reference’
output with respect to the difference of the input (to explain) from a
‘reference’ input. The authors define a function analogous to partial
derivatives to compute the feature importance scores and use the
chain rule to backpropagate the activation differences from the
output layer to the original input. The choice of reference input
depends on domain specific knowledge; For example, in digit classification task on the MNIST dataset, they use a reference input
of all-zeros as that is the background of the images. For object
detection in images, a plain black image is often used.
In the context of IR, DeepSHAP can be used to explain why
a document is relevant to query (according to a given NRM) by
computing the shapley values for words in the document. The words
with high shapley values indicate that they are important towards
this prediction of relevance. However, to accurately compute the
shapley values using DeepSHAP a reference input is needed. What
makes a good background image in the context of IR?
Unlike classification tasks, in ranking we have at least 2 inputs
which are in most cases the query and document tokens. In this
work we fix the reference input for the query to be same as that of

To this end, we describe the experimental setup we used to address these questions. We describe the various NRM’s we considered
and how we used LIME to evaluate the correctness of explanations
produced by DeepSHAP.

4.1

Neural Retrieval Models

DRMM [5] This model uses a query-document term matching
count histogram as input into a feed forward neural network (MLP)
to output a relevance score along with a gating mechanism that
learns query term weights.
MatchPyramid [10] This model uses a query-document interaction matrix as input to a 2D CNN to extract matching patterns.
The output is then fed into a MLP to get a relevance score.
PACRR-DRMM [8] This model creates a query-document interaction matrix that is fed into multiple 2D CNNs with different
kernel sizes to extract n-gram matches. Then, after k-max pooling
across each q-term, the document aware q-term encodings are fed
into a MLP, like in DRMM to obtain a relevance score.

1006

Short Research Papers 2A: AI, Mining, and others

SIGIR ’19, July 21–25, 2019, Paris, France

Figure 1: Confusion matrices of various DeepSHAP background document methods comparing Jaccard similarities.

4.2

Evaluation

representation and parts of the model architecture. Table 3 shows
explanations for DRMM across variants. Once again we see how
the explanations can differ significantly if we are not careful in
selecting the reference input. For IR, finding the background image
seems to be a much harder question.
Our results show how explanations are highly sensitive to the
reference input for NRMs chosen in our experiments. This is also
indication that a single reference input method may not be the best
for every NRM.

To conduct the experiments, we used the Robust04 test collection
from TREC. We used Lucene to index and retrieve documents. Next,
we trained the NRMs using their implementations in MatchZoo1 [3].
All the hyparameters were tuned using the same experimental
setup as described in the respective papers. We chose to study
explanations for the distinguished set of hard topics2 (50) from the
TREC Robust Track 2004. We generate the explanations from LIME
and DeepSHAP for the top-3 documents retrieved for each query
and use only these for our quantitative experiments.
Evaluating explanations Since no ground truth explanations
are available for a neural model, we use LIME based explanations
as a proxy. We found that it can approximate the locality of a query
document pair well, using a simple linear model that achieved an accuracy of over 90% across all NRMs considered in our experiments.
To produce the explanations from LIME we used the implementation found in 3 along with the score-based modification suggested
in [18]. The primary parameters for training a LIME explanation
model are the number of perturbed samples to be considered and
the number of words for the explanation. The number of perturbed
samples is set to 5000 and the number of words is varied based on
the experiment. We used the DeepSHAP implementation provided
here 4 . Note that we ignore the polarity of the explanation terms
provided by both methods in our comparison since the semantics
behind the polarities in LIME and DeepSHAP are different. We are
more interested in the terms chosen as the explanations in both
cases.

5.2

Accuracy of reference input methods

To help identify which reference input method is most accurate in
explaining a given query-document pair, we compared the LIME
explanations for the same against it’s corresponding DeepSHAP
explanations. In general we found that DeepSHAP produces more
explanation terms whereas LIME’s L1 regularizer constrains the
explanations to only the most important terms. Additionally, the
discrepancy between the explanations can be attributed to LIME
being purely local, whereas DeepSHAP has some global context
since it looks at activations for the whole network which may have
captured some global patterns. Hence, to estimate which reference
input surfaces the most ‘ground truth’ explanation terms we only
computed recall at top 50 and 100 (by shapley value magnitude)
DeepSHAP explanation terms (in Table 1).
The first interesting insight is that some NRMs are easier to explain whereas others are more difficult. PACRR-DRMM consistently
has a recall less than 0.7, whereas the DeepSHAP explanations of
DRMM effectively capture almost all of the LIME explanation terms.
When comparing reference input variants within each NRM we
find that there is no consistent winner. For DRMM, QL is the best
which indicates that sampling terms which are relatively generic
for this query in particular is a better ‘background image’ than
sampling generic words from the collection (IDF).
In the case of MatchPyramid, TOPK LIST is the worst performing
but it is more difficult to distinguish between the approaches here.
The best approach surprisingly is OOV. This can be attributed to
how MatchPyramid treats OOV terms. The OOV token is represented by an all-zeros embedding vector that is used for padding
the input interaction matrix whereas in DRMM, OOV tokens are
filtered out. These preprocessing considerations prove to be crucial
when determining the right input reference. Moving on to PACRRDRMM, we once again find that QL is the best method even though
DeepSHAP struggles to find all the LIME terms.

5 RESULTS AND DISCUSSION
5.1 Effect of reference input document
Figure 1 illustrates the overlap between the explanation terms produced when varying the reference input. Immediately we observe
that the overlap between explanations produced is low; below 50%
in most cases and consistently across all NRMs. Each reference
input method has its own distinct semantics and this is reflected
by the low overlap scores. We also find that there is no consistent
trend across NRMs. For MatchPyramid, OOV and QL have highest
overlap whereas for PACRR-DRMM its OOV and COL that have
highest overlap even though both models share similarities in input
1 https://github.com/NTMC-Community/MatchZoo/tree/1.0
2 https://trec.nist.gov/data/robust/04.guidelines.html
3 https://github.com/marcotcr/lime
4 https://github.com/slundberg/shap

1007

Short Research Papers 2A: AI, Mining, and others

SIGIR ’19, July 21–25, 2019, Paris, France

Table 1: Comparison of recall measures at top-k (50, 100) terms from DeepSHAP without polarity against the top-k (10, 20, 30)
ground-truth terms from LIME for ROBUST04 difficult queries (50)
DRMM
top-10

MatchPyramid

top-20

top-30

top-10

top-20

top-10

top-20

top-30

SHAP
variants

recall
@50

recall
@100

recall
@50

recall
@100

recall
@50

recall
@100

recall
@50

recall
@100

recall
@50

recall
@100

recall
@50

recall
@100

recall
@50

recall
@100

recall
@50

recall
@100

recall
@50

recall
@100

OOV

0.789

0.905

0.672

0.845

0.615

0.812

0.793

0.843

0.656

0.726

0.566

0.640

0.582

0.582

0.388

0.388

0.299

0.299

IDF

0.830

0.917

0.723

0.871

0.658

0.841

0.795

0.832

0.653

0.711

0.565

0.633

0.633

0.633

0.446

0.446

0.362

0.362

QL

0.894

0.955

0.754

0.892

0.670

0.856

0.765

0.821

0.638

0.711

0.556

0.636

0.643

0.643

0.462

0.462

0.367

0.367

COLLECTION

0.760

0.881

0.673

0.841

0.620

0.815

0.783

0.824

0.639

0.709

0.552

0.630

0.621

0.621

0.429

0.429

0.343

0.343

TOPK LIST.

0.639

0.821

0.606

0.794

0.578

0.788

0.759

0.811

0.624

0.702

0.545

0.627

0.625

0.625

0.425

0.425

0.340

0.340

Table 2: Comparison of mean squared error (MSE) and accuracy (ACC) of LIME’s linear model across various NRMs.
Low MSE and high accuracy shows that it is able to fit and
generalize in the query-document locality.

NRM

TRAIN MSE

DRMM
MatchPyramid
PACRR-DRMM

0.00631
0.01827
0.00165

Linear Regression
TEST MSE TRAIN ACC
0.00633
0.01839
0.00160

This work was supported by the Amazon research award on
‘Interpretability of Neural Rankers’.

REFERENCES
[1] Leila Arras, Franziska Horn, Grégoire Montavon, Klaus-Robert Müller, and Wojciech Samek. 2017. "What is relevant in a text document?": An interpretable
machine learning approach. PLOS ONE 12 (2017), 1–23.
[2] Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen,
Klaus-Robert Müller, and Wojciech Samek. 2015. On Pixel-Wise Explanations
for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation. PLOS
ONE 10 (2015), 1–46.
[3] Yixing Fan, Liang Pang, Jianpeng Hou, Jiafeng Guo, Yanyan Lan, and Xueqi Cheng.
2017. MatchZoo: A Toolkit for Deep Text Matching. (2017). arXiv:1707.07270
[4] Amirata Ghorbani, Abubakar Abid, and James Y. Zou. 2019. Interpretation of
Neural Networks is Fragile. In AAAI ’19.
[5] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. 2016. A Deep Relevance
Matching Model for Ad-hoc Retrieval. In CIKM ’16. ACM, 55–64.
[6] Benjamin Letham, Cynthia Rudin, Tyler H. McCormick, and David Madigan.
2015. Interpretable classifiers using rules and Bayesian analysis: Building a better
stroke prediction model. The Annals of Applied Statistics 9, 3 (2015), 1350–1371.
[7] Scott M Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model
Predictions. In Advances in Neural Information Processing Systems 30. 4765–4774.
[8] Ryan McDonald, George Brokos, and Ion Androutsopoulos. 2018. Deep Relevance
Ranking Using Enhanced Document-Query Interactions. In EMNLP ’18.
[9] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, and Xueqi Cheng. 2017. A Deep
Investigation of Deep IR Models. arXiv preprint (2017). arXiv:1707.07700
[10] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Shengxian Wan, and Xueqi Cheng.
2016. Text Matching As Image Recognition. In AAAI’16. 2793–2799.
[11] Daan Rennings, Felipe Moraes, and Claudia Hauff. 2019. An Axiomatic Approach
to Diagnosing Neural IR Models. In ECIR ’19. 489–503.
[12] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "Why Should
I Trust You?": Explaining the Predictions of Any Classifier. In KDD ’16. ACM,
1135–1144.
[13] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Anchors: HighPrecision Model-Agnostic Explanations. In AAAI ’18.
[14] Lloyd S Shapley. 1953. A value for n-person games. Contributions to the Theory
of Games 2, 28 (1953), 307–317.
[15] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning Important Features Through Propagating Activation Differences. arXiv preprint
(2017). arXiv:1704.02685
[16] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep Inside
Convolutional Networks: Visualising Image Classification Models and Saliency
Maps. ICLR Workshop (2014).
[17] Jaspreet Singh and Avishek Anand. 2018. Interpreting search result rankings
through intent modeling. arXiv preprint (2018). arXiv:1809.05190
[18] Jaspreet Singh and Avishek Anand. 2019. EXS: Explainable Search Using Local
Model Agnostic Interpretability. In Proceedings of the Twelfth ACM International
Conference on Web Search and Data Mining (WSDM ’19). ACM, 770–773.
[19] Berk Ustun and Cynthia Rudin. 2016. Supersparse Linear Integer Models for
Optimized Medical Scoring Systems. Machine Learning 102, 3 (2016), 349–391.
[20] Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan
Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. Show, Attend and
Tell: Neural Image Caption Generation with Visual Attention. In International
Conference on Machine Learning - Volume 37 (ICML’15). 2048–2057.

TEST ACC

0.92662
0.90367
0.98857

0.92654
0.90387
0.98980

Table 3: An example of words selected by LIME and SHAP
methods for the query ‘cult lifestyles’ and document ‘FBIS3-843’
which is about clashes between cult members and student
union’s activists at a university in Nigeria. Words unique to a
particular explanation method are highlighted in bold.
LIME

OOV

IDF

QL

COL.

TOPK

cult
style
followers
elite
saloon
student
home
members
march
september

cult
followers
black
fraternities
degenerate
sons
academic
american
tried
household

cult
style
followers
suspects
belong
reappearing
household
black
fraternities
degenerate

cult
style
elite
saloon
final
march
friday
september
arms
closed

cult
black
fraternities
degenerate
sons
followers
style
home
household
avoid

cult
numbers
english
college
university
fallouts
buccaneers
feudings
activists
troubles

6

PACRR-DRMM
top-30

CONCLUSION AND FUTURE WORK

In this paper we suggested several reference input methods for
DeepSHAP that take into account the unique semantics of document ranking and relevance in IR. Through quantitative experiments we found that it is indeed sensitive to the reference input.
The distinct lack of overlap in most cases was surprising but in line
with recent works on the lack of robustness in interpretability approaches. We also tried to evaluate which reference method is more
accurate by comparing against LIME. Here we found that reference
input method selection is highly dependent on the model at hand.
We believe that this work exposes new problems when dealing
with model introspective interpretability for NRMs. A worthwhile
endeavor will be to investigate new approaches that explicitly take
into account the discreteness of text and the model’s preprocessing
choices when generating explanations.

1008

