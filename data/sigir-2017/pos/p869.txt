Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

An Enhanced Approach to Query Performance Prediction Using
Reference Lists
Haggai Roitman
IBM Research - Haifa
Haifa, Israel 31905
haggai@il.ibm.com

ABSTRACT

log-odds of the relevance of a given target list for prediction. Yet,
as the authors have pointed out, they were not able to find an educated way for generating PIE-RLs and, therefore, they could not
fully validate their proposal [8].

We address the problem of query performance prediction (QPP) using reference lists. To date, no previous QPP method has been fully
successful in generating and utilizing several pseudo-effective and
pseudo-ineffective reference lists. In this work, we try to fill the
gaps. We first propose a novel unsupervised approach for generating and selecting both types of reference lists using query perturbation and statistical inference. We then propose an enhanced
QPP approach that utilizes both types of selected reference lists.

1

2

TOWARDS AN ENHANCED APPROACH

In this work we address two main problems:
(1) How to automatically generate PE-RLs and PIE-RLs given a
target list for performance prediction?
(2) How to utilize both types of RLs for QPP?

BACKGROUND

We address the problem of query performance prediction (QPP)
using reference lists. We focus on post-retrieval QPP [2]. Given a
query, a corpus and a retrieval method that evaluates the query, a
post-retrieval QPP method predicts the effectiveness of the query’s
retrieved result list [2].
While existing post-retrieval QPP methods may seem different
at first glance, as Kurland et al. [4] have pointed out, many of
them are actually built on the same grounds. Common to such
methods is the usage of a single list that acts as a pseudo-effective
(PE for short) or pseudo-ineffective (PIE for short) reference list
(RL for short) for predicting the performance of a given target
list [4]. Example methods that utilize a single PE-RL are the Query
Feedback [12] (QF) and the Utility Estimation Framework [7] (UEF)
methods. Example methods that utilize a single PIE-RL are the
Weighted Information Gain [12] (WIG) and the Normalized Query
Commitment [9] (NQC) methods. Given a target list for prediction
and a RL, the former’s performance is typically predicted according to the similarity between the two lists [4].
Few previous works have further tried to utilize several RLs for
QPP [4, 5, 7, 8, 11]. Yet, these works have either manually selected1 RLs [4, 7] or generated RLs with no type distinction
(i.e., PE or PIE) [8, 11]. In addition, works that did consider the RL
types have either used only PE-RLs [5, 7] or linearly combined two
RLs, one of each type [4]. Lately, Shtok et al. [8] have suggested to
combine an arbitrary number of PE-RLs and PIE-RLs based on the

To address the first problem, we propose a novel unsupervised
approach for generating and selecting both types of RLs based on
query perturbation and statistical inference. To this end, we utilize
two state-of-the-art QPP methods as sample moments’ estimators.
To address the second problem, we suggest an enhanced QPP
approach that combines the selected PE-RLs and PIE-RLs together
based on the weighted mean of their predicted qualities. For that,
we utilize only the most significant PE-RLs and PEI-RLs and weigh
them using a similarity measure that was never applied before
for QPP. Our evaluation demonstrates that, overall, using our proposed RL-based QPP approach significantly improves prediction.

3 FRAMEWORK
3.1 Preliminaries
Let D denote a top-k (ranked) list of documents, retrieved from
corpus C by some (retrieval) method M in response to query q.
Let s M (d |q) denote the score assigned to a document d ∈ C by
method M given query q. Let s M (C |q) further denote the score
assigned to the whole collection. The post-retrieval QPP task is to
predict the retrieval quality (performance) of the target list D given
query q [2]. We hereinafter denote such prediction Q̂(D|q).

3.2

1 For

example, Kurland et al. [4] have manually selected relevant documents for “generating” a PE-RL.

Candidate RLs Generation

As the first step, we generate a pool of candidate RLs based on
query-perturbation. Query perturbation is implemented so as to
generate queries that are more or less relevant to the same information need of query q. To this end, let w denote a single term in
the vocabulary V . Given query q, we first induce a relevance model
p(w |R) using the target list D’s top-m ranked documents (denoted
as D [m] ) as pseudo-relevance feedback [6]. We next consider only
the top-n terms w (∈ V ) with the highest likelihood p(w |R). For each
considered term w, let qw denote the corresponding perturbed version of query q, obtained by expanding q with a single additional

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’17, August 7–11, 2017, Shinjuku, Tokyo, Japan.
© 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00
DOI: http://dx.doi.org/10.1145/3077136.3080665

869

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

disjunctive term2 w. Using query qw and method M we then retrieve the corresponding (top-k) RL Dw and add it as a candidate
to the pool.

3.3

Using the fact that for any given variable x and real constant
def

number c , 0, V ar ( c1 x) = c12 V ar (x), we now make another obdef
servation that, the NQC predictor [12], defined by Q̂ N QC (D ′ |q ′ ) =
r
1 Í s̃ (d |q ′ ) − s̃ (D ′ |q ′ ) 2 (where k ′ ≤ k and s̃ (D ′ |q ′ )
M
M
M
k′

RLs Selection

As in any query expansion setting, we anticipate that some expansion terms may improve query q; while some others may drift the
query and, therefore, have a negative effect on the retrieval’s quality [6]. Using the target list D as the baseline for comparison, we
further wish to select only those RLs Dw whose performance is
(presumably) significantly better (i.e., PE) or worse (i.e., PIE) than
D’s. Let Dr+ef and Dr−ef denote the set of PE-RLs and PIE-RLs that
are selected, respectively.
Given query q, we now assume that method M’s scores for documents in the corpus s M (d |q) are drawn from some (unknown)
probability distribution. Hence, the target list D document scores
are actually samples from this distribution. Using this fact in mind,
we next describe two unsupervised schemes for selecting RLs Dw
from the candidate pool to be assigned to either Dr+ef or Dr−ef .
Both schemes are based on an initial step of score transformation,
followed by the selection step that is based on statistical inference.

d ∈D ′

further denotes the sample mean of D ′ ’s documents transformed
scores) is actually the sample estimator of the standard deviation of
Eq. 2 transformed scores’ distribution. Similar to the WIG case, we
utilize a statistical test for selecting RLs from the candidate pool.
To this end, as the null hypothesis H 0 we shall assume that, the two
s̃ M (·|q) and s̃ M (·|qw ) score distributions have an equal variance
(which is estimated by taking Q̂ N QC (D ′ |q ′ ) to the power of two).
The rest of the decision, whether to reject a RL Dw or assign it to
either Dr+ef or Dr−ef , is done in a similar way as was described
for the WIG case. To validate this hypothesis, we use the BrownForsythe test [1] for equality of variances with 95% of confidence.

3.4

An Enhanced QPP Approach

Here we propose an enhanced QPP approach based on both types
of selected RLs (hereinafter termed the Reference-Lists Selectionbased QPP method or RLS for short). For a given selected RL Dw
(∈ Dr+ef ∪ Dr−ef ), let pv(Dw ) denote the p-value of the statistical
test [1, 10] used to validate its selection. Thus, the lower pv(Dw ) is,
+[l ]
the more confidence we have in Dw ’s selection. Let Dr ef denote

3.3.1 WIG-based selection. Given any query q ′ (i.e., either q or
qw ), we first transform the scores of the documents in the corresponding (result) list D ′ (i.e., either D or Dw ) that was retrieved by
method M as follows:

def
1
s M (d |q ′ ) − s M (C |q ′ ) ,
(1)
s̃ M (d |q ′ ) = p
′
|q |

the l PE-RLs Dw ∈ Dr+ef with the lowest pv(Dw ). In a similar way,
−[l ]
we define Dr ef . We now propose to predict the performance of
a given target list D based on the weighted mean of the predicted
+[l ]
−[l ]
qualities of its RLs3 Dw ∈ Dr ef ∪ Dr ef , as follows:

def

where |q ′ | denotes query q ′ length (note that, |qw | = |q| +
1). We now make the observation that, the WIG predictor [12],
Í
def
defined by Q̂W I G (D ′ |q ′ ) = k1′
s̃ M (d |q ′ ) (where k ′ ≤ k),
d ∈D ′

[bas e ]

Q̂ R LS

is actually the sample estimator of the mean of Eq. 1 transformed
scores’ distribution. Using this observation, we now use statistical
inference for identifying (and selecting) both types of RLs.
For a large enough sample size (e.g., k ′ ≥ 30), under the assumption that method M document scores are i.i.d, according to
the Central Limit Theorem, Q̂W I G (D ′ |q ′ ) (approximately) follows
a normal distribution. Therefore, the decision whether a given RL
Dw performs significantly better or worse than the target list D
may be validated by a statistical hypothesis test for the equality
of (normal) means. As the null hypothesis H 0 we shall assume
that, the two s̃ M (·|q) and s̃ M (·|qw ) score distributions have an
equal mean. Whenever we can accept H 0 , then we reject Dw . On
the other hand, assuming that H 0 is rejected and Q̂W I G (D|q) <
Q̂W I G (Dw |qw ), then we assume that Dw is a PE-RL of D and add it
to Dr+ef . Similarly, assuming that H 0 is rejected and Q̂W I G (D|q) >

2 This

s M (d |q )
s M (C |q ′ )

φ w · Q̂ [bas e ] (D w |qw )
Í

,

φw

(3)

where Q̂ [base] (Dw |qw ) is either the WIG or NQC base QPP method;
depending on which RLs selection approach we use. φw further
denotes the weight (“importance”) of RL Dw and is calculated as
follows:
(
def

φw =

+[l ]

sim(D, D w ),

D w ∈ Dr e f

1 − sim(D, D w ),

D w ∈ Dr e f

−[l ]

(4)

sim(D, Dw ) denotes the similarity between the target list D and
a given RL Dw [5, 8]. Therefore, according to Eq. 3, the similar the
+[l ]
target list D is to the PE-RLs in Dr ef and the dissimilar it is from
−[l ]

3.3.2 NQC-based selection. We now consider an alternative score
transformation as follows:
′
def

(D |q) =

Dw

Q̂W I G (Dw |qw ), then we assume that Dw is a PIE-RL of D and add it
to Dr−ef . To validate this hypothesis, we use the Welch’s t-test [10]
for equality of means with 95% of confidence.

s̃ M (d |q ′ ) =

Í

d e f Dw

(2)

the PIE-RLs in Dr ef , the better its performance is predicted to be.
Finally, for measuring list-wise similarity, we adopt the ConsistencyIndex measure, which was previously proposed in the context of
sequential forward (feature) selection [3]. As was noted in [3], the
intersection size between two random subsets of a given set follows a hypergeometric distribution. The Consistency-Index, therefore, measures the level of agreement between the expected and
the observed intersection sizes between two given subsets [3]. The
normalized-[0, 1] Consistency-Index based similarity is calculated
as follows:
3 Whenever

is simply done by concatenating w to q’s text.

870

+[l ]

−[l ]

Dr e f ∪ Dr e f = ∅, we simply use Q̂ [bas e ] (D |q).

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

1 |D ∩ Dw | · nC − k 2
+
,
(5)
2
2k(nC − k)
where nC denotes the number of documents in corpus C. Note
that, whenever |D ∩ Dw | = k, then sim(D, Dw ) = 1; while whenever |D ∩ Dw | = 0, then lim
sim(D, Dw ) = 0.
n
def

prediction) to 5 for WIG [12] and 150 for NQC [9]. We further
evaluated the QF method as an alternative QPP method. To this
end, we first run an expanded query based on a selection of the
top-100 terms w (∈ V ) with the highest contribution to the KL divergence between pRM 1 (w |R) and p(w |C) [12]; Let D R denote the
corresponding retrieved RL [12]. The QF prediction is then simply given by the number of documents that are shared among the
top-50 documents in D and RL D R [12].
We further evaluated state-of-the-art QPP alternatives [4, 5, 7, 8]
that have also utilized one or more PE-RLs or PIE-RLs. The first alternative is the UEF method [7]. For a given target list D, UEF
utilizes a single PE-RL obtained by reranking D according to a
relevance model induced from the pseudo-relevance feedback set

sim(D, Dw ) =

k→

C
2

4 EVALUATION
4.1 Datasets and Setup
Corpus
SJMN
WSJ
AP
ROBUST
WT10g
GOV2

#documents
90,257
173,252
242,918
528,155
1,692,096
25,205,179

Queries
51-150
151-200
51-150
301-450, 601-700
451-550
701-850

Disks
3
1-2
1-3
4&5-{CR}
WT10g
GOV2

[base]

Table 1: TREC data used for experiments.

The TREC corpora and queries used for the evaluation are specified in Table 1. These benchmarks were used by previous QPP
works [2], especially those that utilized reference lists (e.g., [4, 5,
7, 8]). Titles of TREC topics were used as queries. The Apache
Lucene4 open source search library was used for indexing and
searching documents. Documents and queries were processed using Lucene’s English text analysis (i.e., tokenization, Poter stemming, stopwords, etc.). As the underlying retrieval method M we
used Lucene’s query-likelihood implementation with the Dirichlet
smoothing parameter fixed to µ = 1000 (following [4, 5, 7, 8]).
For each query, we predicted the performance of the target list
D based on its top-1000 retrieved documents (i.e., k = 1000). Following the common practice [2], we measured the prediction over
queries quality by the Pearson correlation between the predictor’s
values and the actual average precision (AP@1000) values calculated using TREC’s relevance judgments.
To generate candidate RLs, we used the Relevance Model 3 (RM3) [6],
where we chose the top-100 (i.e., n = 100) terms w ∈ V with the
highest pRM 3 (w |R). The RM3 parameters were further fixed as follows: m = 10 (i.e., the number of pseudo-relevance feedback documents) and λ = 0.9 (i.e., query anchoring) [6]. This has left us with
only two parameters to tune: k ′ ∈ {30, 50, 100, 150, 200
, 500, 1000} – the sample size used for deriving the WIG and NQC
RLs based selection methods; and l ∈ {1, 2, . . . , 100} – the number
of the lowest p-valued RLs in Dr+ef and Dr−ef to be used for the
actual prediction. To this end, we used the SJMN corpus for training; with (k ′ = 100, l = 5) and (k ′ = 150, l = 3) tuned for the WIG
and NQC based selection methods, respectively. We used the rest
of the corpora for testing.

4.2

Baselines

def

D [m] . UEF prediction is calculated as follows: Q̂U E F (D|q) =
sim(D, π R (D))Q̂ [base] (D [m] |q), where π R (D) further denotes the
the reranking (permutation) of D according to the (RM1) relevance
model pRM 1 (w |R) (i.e., λ = 0, following [7]). Following [7], sim(D, π R (D))
was measured by Pearson’s correlation (on document scores) and
setting m = 5 and m = 150 for the WIG and NQC base predictors [7]. Next, we evaluated the RefList [5] method, an extended
approach of UEF [5], “designed” to utilize several RLs. To this
end, we follow [5] and generate 10 RLs D µ by varying the smoothing parameter µ used for QL scoring of documents5 in the corpus given query q. RefList prediction is calculated as follows [5]:
def Í
[base]
Q̂ Ref List (D|q) =
sim(D, D µ )Q̂ [base] (D µ |q µ ). sim(D, D µ ) is
µ

again measured by Pearson’s correlation [5]. We also implemented
Kurland et al.’s [4] approach (denoted hereinafter as PE-PIE) which
predicts the quality based on a single PE-RL (D α+ ) and a single
def

PIE-RL (D α− ) as follows: Q̂ P E−P I E (D|q) = αsim(D, D α+ ) − (1 −
α)sim(D, D α− ); with α ∈ [0, 1] [4]. We further chose D α+ and D α−
to be the most significant PE-RL and PIE-RL (i.e., according to
pv(Dw )) in Dr+ef and Dr−ef , respectively. The α smooth parameter was tuned using the SJMN dataset; yielding α = 0.5. Finally,
we further implemented the LogOdds approach that was recently
proposed by Shtok et al. [8]. LogOdds extends RefList with a
PE-PIE inspired PE-RLs and PIE-RLs
utilization, calculated as:
Í
sim(D, Dw )Q̂ [base] (Dw |qw )
[base]

def

Q̂ LoдOdds (D|q) = log

D w ∈ Dr+e f

Í

D w ∈ Dr−e f

sim(D, Dw )Q̂ [base] (Dw |qw )

Following [8], sim(D, Dw ) was measured using the rank-biased
overlap measure (RBO), with its free parameter set to 0.95 [8]. It is
worth noting again that, Shtok et al. [8] could not find an educated
way to obtain PIE-RLs, and therefore, their approach has not been
fully validated until now [8]. Finally, we used base ∈ {W IG, N QC}
for instantiating Q̂ [base] (·|·) in all the evaluated methods. Statistical difference in correlations was further measured.

4.3

Results

We compared our proposed QPP approach with the following baselines. First, we evaluated both basic QPP methods (i.e., WIG and
NQC) as “standalone” methods. Following previous recommendations, we set k ′ (the number of high ranked documents used for

The results of our evaluation are summarized in Table 2. First, comparing RLS side-by-side with the WIG and NQC base methods, we
observe that, RLS boosts the laters’ performance (significantly in

4 http://lucene.apache.org

5µ

871

∈ {100, 200, 500, 800, 1500, 2000, 3000, 4000, 5000, 10000} [5].

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

WSJ
AP
ROBUST WT10g GOV2
WIG
.677
.617r
.528r
.407r
.487r
QF
.502br
.575br
.435br
.451br
.368br
PE-PIE[WIG]
.097br
.115br
.265br
.188br
.362br
UEF[WIG]
.607br
.655br
.576br
.425r
.475r
RefList[WIG]
.668
.651r
.523br
.398r
.516br
LogOdds[WIG] -.065br
-.121br
-.202br
-.130br
-.125br
RLS[WIG]
.702
.678b
.591b
.472b
.533b
(a) Methods comparison based on WIG as the base method

WSJ
AP
ROBUST WT10g GOV2
NQC
.727
.602r
.557r
.496r
.348r
QF
.502br
.575br
.435br
.451br
.368br
PE-PIE[NQC]
.153br
.147r
.301br
.257br
.303r
UEF[NQC]
.712r
.625r
.581r
.533br
.372br
RefList[NQC]
.725
.630r
.552r
.491r
.430b
LogOdds[NQC] -.012br
-.188br
-.223r
-.058br
-.053br
RLS[NQC]
.748
.653b
.619b
.553b
.424b
(b) Methods comparison based on NQC as the base method

Table 2: Pearson’s correlation to AP per corpus and evaluated QPP method. Numbers denoted with b and r further represent
a statistical significant difference with the base QPP method (i.e., either WIG or NQC) and RLS, respectively.

of the RLS approach. We believe that, our results shade new light
on the problem of QPP using RLs and the challenges that the designers of such methods may face.

most cases); with an average of +10.2 ± 2% and +11.2 ± 3% improvement over WIG and NQC, respectively. Next, comparing
RLS side-by-side with the other alternatives, we further observe
that, RLS in all cases but one, provides better prediction quality
(again, significantly in most cases); with an average improvement
of +5.2±1.6% and +3±1.4% over the best alternative, when WIG
and NQC are used as the underlying base method, respectively.
We next make the observation that, while RLS provided a consistent improvement over WIG and NQC, when the later were
used as the underlying base methods, the other alternatives do not
share the same behavior. Closer examination of the results of these
alternatives across the corpora shades some light. Focusing on the
UEF method, we can observe that, only in 7 out of the 10 cases
it managed to improve the base method. This may be attributed
to the RL π R (D) utilized by UEF for prediction, where for some
corpora such RLs may not comply with UEF’s PE-RL assumption
(e.g., due to possible query-drift [6]). Hence, measuring the similarity with such RLs actually result in performance degradation. This
argument is further supported by examining the performance of
the RefList method (an extension of UEF [5]), where in only 4
out of the 10 cases it managed to improve over the base method.
Such additional performance degradation may be attributed to the
fact that, RefList aggregates over several RLs with no distinction
of their type, and, therefore, it may accumulate even more error.
Closer examination of the two alternative methods that do distinguish between both types of RLs, PE-PIE and LogLoss, further
reveals an interesting trend. First, we observe that, PE-PIE in 9
out of 10 cases has resulted in much worse performance than that
of the base method used to obtain the two (PE and PIE) RLs. Therefore, it seems that, a simple linear interpolation of a PE-RL with a
PIE-RL as proposed by the PE-PIE method, where the dissimilarity from the PIE-RL is calculated by substraction, does not actually
work well. Similar to the UEF vs. RefList case, a further comparison of the PE-PIE with LogLoss supports this argument. The
summation over several PIE-RLs using a similar (log) substraction
approach only results in further performance degradation due to
more accumulated error. It is worth noting again that, both PE-PIE
and LogLoss methods were never evaluated with automatically
selected RLs. To remind, in [4], the PE-PIE method was tested
with a manual selection of the PE-RL and the PIE-RL; whereas,
the LogLoss method was not fully validated [8].
To conclude, among the various alternatives we have examined,
none has exhibited a robust performance prediction similar to that

ACKNOWLEDGEMENT
I would like to thank the reviewers for their fruitful comments.
Special thanks to my wife Janna and daughters Inbar and Einav
for their endless love and support.

REFERENCES
[1] Morton B Brown and Alan B Forsythe. Robust tests for the equality of variances.
Journal of the American Statistical Association, 69(346):364–367, 1974.
[2] David Carmel and Oren Kurland. Query performance prediction for ir. In Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’12, pages 1196–1197, New York, NY, USA,
2012. ACM.
[3] Ludmila I. Kuncheva. A stability index for feature selection. In Proceedings
of the 25th Conference on Proceedings of the 25th IASTED International MultiConference: Artificial Intelligence and Applications, AIAP’07, pages 390–395,
Anaheim, CA, USA, 2007. ACTA Press.
[4] Oren Kurland, Anna Shtok, David Carmel, and Shay Hummel. A unified framework for post-retrieval query-performance prediction. In Proceedings of the
Third International Conference on Advances in Information Retrieval Theory, ICTIR’11, pages 15–26, Berlin, Heidelberg, 2011. Springer-Verlag.
[5] Oren Kurland, Anna Shtok, Shay Hummel, Fiana Raiber, David Carmel, and
Ofri Rom. Back to the roots: A probabilistic framework for query-performance
prediction. In Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM ’12, pages 823–832, New York, NY, USA,
2012. ACM.
[6] Victor Lavrenko and W. Bruce Croft. Relevance based language models. In
Proceedings of the 24th Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR ’01, pages 120–127, New York,
NY, USA, 2001. ACM.
[7] Anna Shtok, Oren Kurland, and David Carmel. Using statistical decision theory and relevance models for query-performance prediction. In Proceedings of
the 33rd International ACM SIGIR Conference on Research and Development in
Information Retrieval.
[8] Anna Shtok, Oren Kurland, and David Carmel. Query performance prediction
using reference lists. ACM Trans. Inf. Syst., 34(4):19:1–19:34, June 2016.
[9] Anna Shtok, Oren Kurland, David Carmel, Fiana Raiber, and Gad Markovits.
Predicting query performance by query-drift estimation. ACM Trans. Inf. Syst.,
30(2):11:1–11:35, May 2012.
[10] Bernard L Welch. The generalization of student’s’ problem when several different population variances are involved. Biometrika, 34(1/2):28–35, 1947.
[11] Elad Yom-Tov, Shai Fine, David Carmel, and Adam Darlow. Learning to estimate query difficulty: Including applications to missing content detection and
distributed information retrieval. In Proceedings of the 28th Annual International
ACM SIGIR Conference on Research and Development in Information Retrieval.
[12] Yun Zhou and W. Bruce Croft. Query performance prediction in web search
environments. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’07, pages
543–550, New York, NY, USA, 2007. ACM.

872

