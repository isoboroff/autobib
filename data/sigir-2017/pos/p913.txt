Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Neural Network based Reinforcement Learning for Real-time
Pushing on Text Stream
Haihui Tan

Ziyu Lu

Wenjie Li

The Hong Kong Polytechnic
University
Kowloon, Hong Kong
tanhaihui92@gmail.com

The Hong Kong Polytechnic
University
Kowloon, Hong Kong
luziyuhku@gmail.com

The Hong Kong Polytechnic
University
Kowloon, Hong Kong
cswjli@comp.polyu.edu.hk

ABSTRACT

the Twitter stream with respect to user interest profiles. However,
pushing real-time text streams is not a short-term process. It is a
dynamic forward decision process in which the current action will
affect further decisions (dependency) and further streaming texts
generate uncertainty on the current decision. [5] treated real-time
stream summarization as a sequential decision making problem
and adopted a locally optimal learning to search algorithm to learn
a policy for selecting or skipping a sentence in the text stream,
by imitating a heuristic reference policy. However, the heuristic
reference policy is difficult to construct for real-time streaming text
and it needs massive human interventions. The real-time decision
process needs to maximize the long-term rewards by taking both
history dependencies and future uncertainty into consideration.
In this paper, we define the real-time pushing on text stream as
a long-term optimization problem and propose a neural network
based reinforcement learning algorithm as the solution. A novel
Q-Network is defined to learn a long-term policy for sequential
decision making. When receiving text streams, the Q-network predicts the values for taking each action under the current state and
chooses the action with the higher value. A long short-term memory (LSTM) layer is integrated into the Q-Network to represent
the high-level abstraction of streaming text by considering both semantic and temporal dependencies of previously pushed texts. The
Q-Network is continuously updated according to the observation
of interrelationship between actions and rewards, and a long-term
policy is explored and exploited to make real-time decisions on text
stream. The main contributions of our paper is as follows:
• We formulate real-time pushing on text streams as a longterm optimization problem, by considering both history
dependencies and future uncertainty of text stream.
• A Neural Network based Reinforcement learning (NNRL)
algorithm is proposed to maximize long-term rewards and
obtain the long-term policy for real-time decision making.
• Experimental evaluations on the real tweet stream show
that our method has superior performance over the stateof-the-art methods.

The massive amount of noisy and redundant information in text
streams makes it a challenge for users to acquire timely and relevant
information in social media. Real-time notification pushing on text
stream is of practical importance. In this paper, we formulate the
real-time pushing on text stream as a sequential decision making
problem and propose a Neural Network based Reinforcement Learning (NNRL) algorithm for real-time decision making, e.g., push or
skip the incoming text, with considering both history dependencies
and future uncertainty. A novel Q-Network which contains a Long
Short Term Memory (LSTM) layer and three fully connected neural
network layers is designed to maximize the long-term rewards.
Experiment results on the real data from TREC 2016 Real-time Summarization track show that our algorithm significantly outperforms
state-of-the-art methods.

CCS CONCEPTS
• Information systems → Document filtering;

KEYWORDS
Text Stream, Real-Time Pushing, Deep Reinforcement Learning

1

INTRODUCTION

With the development of social media, text streams such as Twitter
posts, news articles or user reviews are being generated in fastgrowing volumes. The explosive amount of noisy and redundant
streaming texts are overwhelming and makes it difficult to find
useful information. Pushing or Summarizing the real-time streaming text for users is of practical importance. In recent decades,
the real-time pushing on text streams has attracted increasing attention at Text Retrieve Conferences (TREC), e.g., the Microblog
track in 2015 [4] and the Real-time Summarization track in 2016
[6]. For example, [9] which has achieved the best performance for
the real-time filtering task at TREC 2015 Microblog track, explored
dynamic emission strategies to maintain appropriate thresholds
for pushing relevant tweets. Fan et al. [2] proposed an adaptive
evolutionary filtering framework to filter interesting tweets from

2

METHOD

Problem Definition: Given a text stream S = {t 1 , t 2 , · · · , tn , · · · },
Our model makes real-time decisions from an action set A = {ap , as }
for each incoming text t. ap is to push t while as is to skip it. As
our target is to maximize the long-term rewards, we adopt a reinforcement learning framework to define and solve this problem.
In reinforcement learning methods, the agent model selects an
action from the action set A and passes it to the environment ϒ
which changes its inner state s and the reward score r . Given the

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR’17, August 7–11, 2017, Shinjuku, Tokyo, Japan
© 2017 ACM. ISBN 978-1-4503-5022-8/17/08. . . $15.00.
DOI: http://dx.doi.org/10.1145/3077136.3080677

913

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

state st = [x 1 , a 1 , x 2 , a 2 , · · · , at −1 , x t ] which is a sequence of observations and actions a, the model arrives at an optimal action-value
function Q(s, a)∗ which is the maximum expected return with a
policy π .

on the loss function defined in Equation 1 (line 10). The target y j is
set as Equation 2:

rj
terminal
yj =
(2)
r j + γ maxa 0 Q(s j+1 , a j ; θ i )
non − terminal

2.1

If it is the terminal of the episode, y j equals to the reward r . Otherwise, y j equals to r j + γ maxa 0 Q(s j+1 , a j ; θ i ). γ is the discounted
factor. After training, the Q-network is a neural network approximation function for real-time decision making.

Neural Network-based Reinforcement
Learning (NNRL)

In our problem, x is the feature representation of a text t, the action
set A = {ap , as }. Similar to Deep Q-Network [8], we propose a
neural network-based reinforcement learning (NNRL) algorithm
where a neural network approximation function as Q-Network is
used to estimate the action-value function Q(s, a, θ ) = Q ∗ (s, a). θ is
the weight parameters in Q-Network. The Q-Network is trained by
minimizing the loss function L(θ ) as Equation 1.
L(θ ) = Es,a∼ρ(s,a) [y − Q(s, a; θ )]
0

2

2.2

Q-Network
Q value for push Q value for skip
fully connected
fully connected

(1)

0

in which y = Es ∼ϒ [r + γmax a Q(s , a ; θ )] is the target and ρ(s, a)
is the behavior distribution over s and a.
Algorithm 1 outlines our proposed neural network-based reinforcement learning (NNRL) algorithm. N is the number of iterations
0

0

fully connected
LSTM
A sequence of vectors

Q-Network

Algorithm 1 Neural Network based Reinforcement Learning
1: Initialize θ in the action-value function Q
2: for n = 1, 2, · · · , N do
3:
for i = 1, 2, · · · , M do
4:
S i = [t 1, t 2, · · · , t n , · · · ]
5:
initialize s 1 = {X̂ 1 }, X̂ 1 is appended representation for x 1
6:
for j = 1, 2, · · · , |S | do
7:
Compute: Q (s j , ap ; θ ) and Q (s j , a s ; θ )
8:

9:

Tweet

Tweet

Tweet

Skip

Push

Skip

……

Tweet

Tweet

Push

Push

Timeline

Tweet

?

……

Now

Figure 1: Q-Network

Action: Randomly select an action for a j with ϵ probability;
0
Otherwise a j = maxa 0 ∈A (Q (s j , a ; θ )). Execute a j .
Observe: reward r j and x j+1 ; generate the appended

In this section, we demonstrate the design of our proposed QNetwork. Figure 1 shows the architecture of our Q-Network which
consists of four layers. The first layer is a single Long Short Term
Memory (LSTM) [3] layer which generates the high-level abstraction of the input sequence. Both semantic and temporal dependencies of the text stream are considered into LSTM by taking the last k
previously selected text feature representation. Therefore the input
for LSTM is the representation X̂ = [x 1 , x 2 , · · · , xT ]. T is the length
of X̂ . LSTM computes the hidden vector H = [h 1 , h 2 , · · · , hT ] by
iterating the following equations:

(k )

representation X̂ j+1 = [X̂ j , x j +1 ]; set state s j+1 =
[s j , a j , X̂ j +1 ].
Update Q-Network by performing a gradient descent step
on the loss function in Equation 1.
11:
end for
12:
end for
13: end for

10:

i t = σ (Wx i x t + Whi ht −1 + Wci c t −1 + bi )
ft = σ (Wx f x t + Whf ht −1 + Wc f c t −1 + bf )

and M is the number of text streams (text episodes). Each text stream
for one topic, e.g. S = {t 1 , t 2 , · · · , tn , · · · }, has one episode training.
|S | is the number of texts in each text episode S. Firstly, it initializes the start state s 1 and the (high-level) representation X̂ 1 . For
each text t in the episode, it executes four steps. In the first step,
compute Q function values for two actions in A (push or skip)
(Line 7). They are Q(s j , ap ; θ ) and Q(s j , as ; θ ) respectively. At the
action step (line 8): with ϵ probability, a random action is chosen
0
for a j ; otherwise, a j = maxa 0 ∈A (Q(s j , a ; θ )). Then execute a j . In
the third step (line 9), observe the reward r j 1 and x j+1 ; generate

c t = ft c t −1 + i t tanh(Wxc x t + Whc ht −1 + bc )
ot = σ (Wxo x t + Who ht −1 + Wco + bo )
ht = ot tanh(c t )
in which σ is the logistic sigmoid function, and i, f , o and c are
respectively the input gate, forget gate, output gate and cell activation vectors, all of which are the same size as the hidden vector h.
W terms denote weight matrices, e.g. Wx i is the input-input gate
weight matrix and Whi is the hidden-input gate matrix. The b terms
denote bias vectors, e.g. bh is the hidden bias vector.
LSTM is connected with three fully connected neural networks.
The output of LSTM hT (the last item in the hidden vector H ) acts
as the input for the later fully connected neural network layers.
Each fully connected neural network can be formated as follows:

(k )

the representation X̂ j+1 = [X̂ j , x j+1 ] by appending the text feature vector x j+1 and the last k text feature vector from previously
selected texts in X̂ j ; set the next state s j+1 = [s j , a j , X̂ j+1 ]. Finally,
update the Q-Network by performing a gradient descent step [1]
1 we

use expected gain (EG) as the reward. Only when the current text is the terminal
of this episode, r equal to the EG value. Otherwise r is zero.

Z i = ŷi Wi ;

914

ŷi+1 = F (Z i )

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

where each output of previous (the i layer) layer ŷ will be the input
of the later layer (the i + 1 layer). Here, i indexes from 1 to 3. ŷ1
is the output of LSTM hT . W are the weight matrix of each fully
connected neural network. F is the activation function and for
the previous two fully connected neural networks, the activation
function is ReLU and we use a linear activation function for the
last fully connected neural network.

generate a text vector (dimension=300) for a tweet by averaging
the word embedding vectors in a tweet text. The word embedding
representation is obtained from a pre-trained Google News corpus
word vector model [7]. And we compute the relevance score from a
SVM regression for each topic and the cosine similarity score. Also
we use two boolean flags (is_link and is_redudant) about whether
URL exists or it is redundant. We normalize all features, and concatenate them into one single vector as the basic presentation x for
each tweet. The dimension of the basic presentation is 311.

3 EXPERIMENTS
3.1 Dataset

3.3

We use TREC 2016 Microblog Track 2 as data set to evaluate our
proposed approach. The dataset consists of the Twitter’s sample
tweet streams ( approximately 1% of public tweets) during the official evaluation period from August 2, 2016 to August 11, 2016. 56
judged interest profiles (topics) of tweets have been given. Each
interest profile (topic) consists of title, description and narrative description. Each sample tweet has a given label for its corresponding
topic: highly relevant, relevant and non-relevant. We pre-process
the dataset by removing the non-English tweets and tweets which
have fewer than 5 words. Also we filter out very irrelevant tweets,
e.g. tweets which have no interest profile keywords. After preprocessing, there is a total of 57,419 tweets for 56 topics. Each topic has
a tweet stream. The average number of tweets per topic is about
96.40. We randomly choose 85% of topics (48 topics) of tweets as
training set and the remaining as test set.

We use two TREC 2016 Real-time Summarization Track official
evaluation metrics. They are expected gain (EG) and Normalized
Cumulative Gain (nCG). The expected gain (EG) is defined:
1 Õ
G(t)
EG =
N
where N is the number of tweets returned and G(t) is the gain of
each tweet: Not relevant tweets receive a gain of 0; relevant tweets
receive a gain of 0.5; Highly-relevant tweets receive a gain of 1.0.
Normalized Cumulative Gain (nCG) is defined:
1Õ
nCG =
G(t)
Z
where Z is the maximum possible gain (given the ten tweet per
day limit). In order to differentiate the performances in salient days
when there are no relevant tweets, there are two variants for each
metric, respectively EG-0, EG-1, nCG-0 and nCG-1. For salient days,
the EG-1 and nCG-1 gives a score of one if not pushing any tweets,
or zero otherwise. In the EG-0 and nCG-0 variants, for a silent day,
the gain is zero [9].

Table 1: Features used for basic representation
Features

Statistic

Name

Description

N_{title}
N_{Narr}
N_{exp}
N_{word}
N_{hashtag}
Time

Number of terms in "Title"
Number of terms in "Narrative"
Number of terms in "Description"
Number of words in tweet text
Number of hashtag in tweet text
the current time in one day (ms)
time interval between the incoming
tweet and the last selected tweet
Flag about whether it is redundant
Flag about whether a URL exists.
SVM regression score
Cosine similarity score
Word embedding representation

Temporal
Time_interval

Semantic

3.2

Is_redundant
is_link
Relevance score
Cosine Score
Text vector

Evaluation Metric

3.4

Compared Methods

We compared the following methods:
• Query similarity (QS): it pushes the tweet whose relevance
score is higher than a fixed threshold α. Cosine similarity
is used to measure the relevance score between the topic
title and the tweet text.
• YoGosling [9]: implements simple dynamic emission strategies to maintain appropriate dynamic thresholds for pushing updates. It achieved the best performance at the TREC
2015 Microblog Track Real-time Filtering task [4].
• Features+StaticThreshold (FST) [6]:It develops a relevance
estimation model based on both lexical and non-lexical
features, and set a static threshold to push tweets with
their manual observation. It achieved the best performance
at the TREC 2016 Real-time Summarization track [6].
• NNRL: This is our proposed algorithm in Section 2.

Features

Before we input the tweet stream into the Q-Network, we firstly
extract some features as the basic presentation x for each tweet.
The extracted features are listed in Table 1. There are three groups:
respectively statistic features, temporal features and semantic features. For each tweet, we compute some statistics as features, including number of hashtag, number of terms and number of terms
appearing in the title, narrative description and description. Also,
two temporal features are extracted. They are the time in one day
(milliseconds since 00:00 of one day) and the time interval between
this tweet and the last selected tweet. As semantic features, we

3.5

Implementation Setting

In our algorithm, the learning rate for gradient descent is 0.001
and the discount factor is 1. The ϵ is annealed linearly from 1 to
0, then fix it at 0 until converging to a suboptimal policy. Then
ϵ is annealed linearly from 0.1 to 0; fix at 0 until converging to
another suboptimal. Repeat the ϵ exploration step thereafter. In
our Q-network, the hidden state size of LSTM is 512. The size of
appended sequence from the previously selected text sequence k is

2 http://trecrts.github.io/

915

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

up to 10 due to LSTM training efficiency. Both the first and second
fully-connected neural network layer have 256 hidden units. The
output layer is a fully-connected linear layer with a single output
for each valid action.

pushes the relevant one while the same text was skipped previously.
For example, the first tweet was skipped but the fourth tweet with
the same content is pushed.
Table 3: An exemplary case for "Hiroshima atomic bomb"

0.05

EG-0 performance over different thresholds

0.30

Max: 0.2483

0.25

Max: 0.0362

0.20

0.03

EG-1

EG-0

0.04

EG-1 performance over different thresholds

0.02

0.15
0.10

0.01

0.05

0.00
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

0.00
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9

Threshold (®)

Threshold (®)

(a) EG-0

(b) EG-1

Tweet Text

08-02
04:21:18
08-09
17:10:15
08-09
17:15:11
08-09
21:59:15

Obama At Hiroshima: A World Without Nuclear
Weapons – Ours - American Thinker
RT @HistoryToLearn: Hiroshima,
one year after the atomic bomb blast, 1946.
RT @HistoryToLearn: Hiroshima,
one year after the atomic bomb blast, 1946.
Obama At #Hiroshima: A World Without
Nuclear Weapons – Ours - American Thinker
I dropped the bomb but now I have
the proof so now I can drop the atomic bomb.
I’ve never been so excited
RT @AJENews: Nagasaki marks
71st anniversary of atomic bombing
Nagasaki Marks 71st Anniversary
of Atomic Bombing.

08-10
00:36:05

Figure 2: EG results with varying QS threshold α

3.6

Time

08-10
02:14:43
08-11
03:41:12

Experimental Results

Action
Skip
Push
Skip
Push

Skip

Skip
Push

Table 2: Evaluation results for all compared methods
Method
QS (α = 0.45)
YoGosling
FST
NNRL

EG-1
0.2483
0.2289
0.2698
0.2816

EG-0
0.0322
0.0253
0.0483
0.0691

nCG-1
0.2325
0.0253
0.2909
0.2971

4

nCG-0
0.0164
0.0253
0.0695
0.0846

CONCLUSIONS

In this paper, we propose a neural network based reinforcement
learning algorithm to address real-time pushing on text stream. A
novel Q-Network is designed to approximate the maximum longterm rewards. Experiment results on real data from TREC 2016
Real-time Summarization track demonstrate that our algorithm
is superior to all compared methods for all the official evaluation
metrics and has the ability to make real-time decisions. In future
work, we plan to study the case without specific query topic.

Table 2 shows the evaluation results. We can see that our proposed NNRL outperforms the other competitors for all evaluation
metrics. Our superior performances lie on that we format this problem as a sequential decision making process and deal with both
history dependencies and future uncertainty while the other three
compared methods mainly adopt the static or dynamic threshold
to filter out the incoming tweet. Figure 2 shows the EG evaluation
results (similar results with nCG; we omit it due to space limitation) with varying the threshold α of the QS method. There are
big differences in EG performance when using different thresholds.
Therefore, static methods like QS and FST are inappropriate for
the real-time environment which requires a dynamic and adaptive
mechanism to consider future uncertainty. Although YoGosling proposed some strategies for obtaining dynamic threshold, it ignores
future uncertainty. We use an exemplary case for topic "Hiroshima
atomic bomb" to demonstrate the adaptive ability of our method
(NNLR) to address the potential future uncertainty. Table 3 shows
a snippet of the tweet sequence of 7 tweets for topic "Hiroshima
atomic bomb" with each tweet’s time, raw text and the selected action in our method. At the start of the tweet sequence, our method
pushes the second tweet rather than the first one because the second
one is more specific and relevant to the topic. Our method decides
to skip the first tweet and waits for the potential better tweet (e.g.
the second one) in the future. After pushing the second one, it skips
the following relevant but redundant tweets. When it has pushed
some highly-relevant tweets and time elapses, the pushing condition might change as very few relevant tweets come. Therefore, it

5

ACKNOWLEDGMENTS

The work in this paper was supported by Research Grants Council of Hong Kong (PolyU 152094/14E), National Natural Science
Foundation of China (61272291) and The Hong Kong Polytechnic
University (4-BCB5).

REFERENCES
[1] Leemon Baird and Andrew Moore. Gradient Descent for General Reinforcement
Learning. In NIPS’98.
[2] Feifan Fan, Yansong Feng, Lili Yao, and Dongyan Zhao. Adaptive Evolutionary
Filtering in Real-Time Twitter Stream. In CIKM ’16.
[3] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory.
Neural Comput. 9, 8 (1997), 1735–1780.
[4] Y. Wang G. Sherman J. Lin, M. Efron and E. Voorhees. Overview of the TREC-2015
Microblog Track. In TREC 2015.
[5] Chris Kedzie, Fernando Diaz, and Kathleen McKeown. Real-Time Web Scale
Event Summarization Using Sequential Decision Making. In IJCAI ’16.
[6] Jimmy Lin, Adam Roegiest, Luchen Tan, Richard McCreadie, Ellen Voorhees, and
Fernando Diaz. Overview of the TREC-2016 Microblog Track. In TREC 2016.
[7] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed Representations of Words and Phrases and Their Compositionality. In
NIPS’13.
[8] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
Antonoglou, Daan Wierstra, and Martin A. Riedmiller. 2013. Playing Atari
with Deep Reinforcement Learning. CoRR (2013).
[9] Luchen Tan, Adam Roegiest, Charles L.A. Clarke, and Jimmy Lin. Simple Dynamic
Emission Strategies for Microblog Filtering. In SIGIR ’16.

916

