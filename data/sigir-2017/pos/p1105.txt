Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Word Embedding Causes Topic Shifting; Exploit Global Context!
Navid Rekabsaz, Mihai Lupu, Allan Hanbury∗

Hamed Zamani†

Information & Software Engineering Group
TU WIEN
rekabsaz/lupu/hanbury@ifs.tuwien.ac.at

Center for Intelligent Information Retrieval
University of Massachusetts Amherst
zamani@cs.umass.edu
has shown not only to be competitive to the local approaches but
also that combining the approaches brings further improvements
in comparison to each of them alone [12, 16, 17].
Word embedding methods provide vector representations of
terms by capturing the co-occurrence relations between the terms,
based on an approximation on the likelihood of their appearances
in similar window-contexts. Word embedding is used in various
IR tasks e.g. document retrieval [11, 12, 18], neural network-based
retrieval models [4, 6, 8], and query expansion [16].
In all of these studies, the concept of “term similarity” is defined as the geometric proximity between their vector representations. However, since this closeness is still a mathematical approximation of meaning, some related terms might not fit to the
retrieval needs and eventually deteriorate the results. For instance,
antonyms (cheap and expensive) or co-hyponyms (schizophrenia
and alzheimer, mathematics and physics, countries, months) share
common window-context and are therefore considered as related
in the word embedding space, but can potentially bias the query to
other topics.
Some recent studies aim to better adapt word embedding methods to the needs of IR. Diaz et al. [5] suggest training separate
word embedding models on the top retrieved documents per query,
while Rekabsaz et al. [13] explore the similarity space and suggest a general threshold to filter the most effective related terms.
While the mentioned studies rely on the context around the terms,
in this work we focus on the effect of similarity achieved from
global context as a complementary to the window-context based
similarity.
In fact, similar to the earlier studies [9, 14], we assume each
document to be a coherent information unit and consider the cooccurrence of terms in documents as a means of measuring their
topical relatedness. Based on this assumption, we hypothesize that
to mitigate the problem of topic shifting, the terms with high word
embedding similarities also need to share similar global contexts. In
other words, if two terms appear in many similar window-contexts,
but they share little global contexts (documents), they probably
reflect different topics and should be removed from the related
terms.
To examine this hypothesis, we analyze the effectiveness of each
related term, when added to the query. Our approach is similar to
that of Cao et al. [2] on pseudo-relevance feedback. Our analysis
shows that the set of related terms from word embedding has a high
potential to improve state-of-the-art retrieval models. Based on
this motivating observation, we explore the effectiveness of using
word embedding’s similar term when filtered by global context
similarity on two state-of-the-art IR models. Our evaluation on
three test collections shows the importance of using global context,
as combining both the similarities significantly improves the results.

ABSTRACT

Exploitation of term relatedness provided by word embedding has
gained considerable attention in recent IR literature. However, an
emerging question is whether this sort of relatedness fits to the
needs of IR with respect to retrieval effectiveness. While we observe
a high potential of word embedding as a resource for related terms,
the incidence of several cases of topic shifting deteriorates the final
performance of the applied retrieval models. To address this issue,
we revisit the use of global context (i.e. the term co-occurrence in
documents) to measure the term relatedness. We hypothesize that
in order to avoid topic shifting among the terms with high word
embedding similarity, they should often share similar global contexts as well. We therefore study the effectiveness of post filtering
of related terms by various global context relatedness measures.
Experimental results show significant improvements in two out of
three test collections, and support our initial hypothesis regarding
the importance of considering global context in retrieval.

KEYWORDS
Word embedding; term relatedness; global context; word2vec; LSI
ACM Reference format:
Navid Rekabsaz, Mihai Lupu, Allan Hanbury and Hamed Zamani. 2017.
Word Embedding Causes Topic Shifting; Exploit Global Context!. In Proceedings of SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan, 4 pages.
DOI: http://dx.doi.org/10.1145/3077136.3080733

1

INTRODUCTION

The effective choice of related terms to enrich queries has been
explored for decades in information retrieval literature and approached using a variety of data resources. Early studies explore
the use of collection statistics. They identify the global context of
two terms either by directly measuring term co-occurrence in a context (i.e. document) [9] or after applying matrix factorization [3].
Later studies show the higher effectiveness of local approaches
(i.e. using pseudo-relevant documents) [15]. More recently, the
approaches to exploit the advancement in word embedding for IR
∗ Funded

by: Self-Optimizer (FFG 852624) in the EUROSTARS programme, funded by
EUREKA, BMWFW and European Union, and ADMIRE (P 25905-N23) by FWF. Thanks
to Joni Sayeler and Linus Wretblad for their contributions in SelfOptimizer.
† was supported in part by the Center for Intelligent Information Retrieval.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.
978-1-4503-5022-8/17/08. . . $15.00
DOI: http://dx.doi.org/10.1145/3077136.3080733

1105

Short Research Paper

2

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 1: Test collections used in this paper

BACKGROUND

To use word embedding in document retrieval, recent studies extend the idea of translation models in IR [1] using word embedding
similarities. Zuccon et al. [18] use the similarities in the language
modeling framework [10] and Rekabsaz et al. [12] extend the concept of translation models to probabilistic relevance framework.
In the following, we briefly explain the translation models when
combined with word embedding similarity.
In principle, a translation model introduces in the estimation
of the relevance of the query term t a translation probability PT ,
defined on the set of (extended) terms R(t), always used in its conditional form PT (t |t 0 ) and interpreted as the probability of observing
term t, having observed term t 0 . Zuccon et al. [18] integrate word
embedding with the translation language modeling by using the
set of extended terms from word embedding:

Name
TREC Adhoc 1&2&3
TREC Adhoc 6&7&8
Robust 2005

# Queries
150
150
50

# Documents
740449
556028
1033461

Table 2: The percentage of the good, bad and neutral terms.
#Rel averages the number of related terms per query term.
Collection
TREC 123
TREC 678
Robust 2005
ALL

4

#Rel
8.2
8.8
10.3
8.1

Threshold 0.60
Good
Neutral
7%
84%
9%
78%
8%
77%
8%
81%

Bad
9%
14%
15%
11%

#Rel
1.3
1.2
1.1
1.2

Threshold 0.80
Good
Neutral
19%
68%
34%
48%
39%
44%
27%
58%

Bad
13%
18%
17%
15%

PRELIMINARY ANALYSIS

We start with an observation on the effectiveness of each individual
c model as it has shown
related term. To measure it, we use the LM
 model [12]. Similar to Cao et
slightly better results than the BM25
al. [2], given each query, for all its corresponding related terms, we
repeat the evaluation of the IR models where each time R(t) consists
of only one of the related terms. For each term, we calculate the
differences between its Average Precision (AP) evaluation result
and the result of the original query and refer to this value as the
retrieval gain or retrieval loss of the related term.
Similar to Cao et al. [2], we define good/bad groups as the terms
with retrieval gain/loss of more than 0.005, and assume the rest
with smaller gain or loss values than 0.005 as neutral terms. Table 2
summarizes the percentage of each group. Due to the lack of space,
we only show the statistics for the lowest (0.6) and highest (0.8)
threshold. The average number of related terms per query term is
shown in the #Rel field. As expected, the percentage of the good
terms is higher for the larger threshold, however—similar to the observation on pseudo-relevance feedback [2]—most of the expanded
terms (58% to 81%) have no significant effect on performance.
Let us imagine that we had a priori knowledge about the effectiveness of each related term and were able to filter terms with
negative effect on retrieval. We call this approach Oracle postfiltering as it shows us the maximum performance of each retrieval
model. Based on the achieved results, we provide an approximation
of this approach by filtering the terms with retrieval loss.
Figures 1a and 1b show the percentage of relative MAP improvec and BM25
 models with and without post-filtering
ment of the LM
with respect to the original LM and BM25 models. In the plot, ignore the Gen and Col results as we return to them in Section 6. The
results are aggregated over the three collections. In each threshold
the statistical significance of the improvement with respect to two
baselines are computed: (1) against the basic models (BM25 and
LM), shown with the b sign and (2) against the translation models
without post filtering, shown with the ρ sign.
As reported by Rekabsaz et al. [13], for the thresholds less than
0.7 the retrieval performance of the translation models (without
post filtering) decreases as the added terms introduce more noise.
However, the models with the Oracle post filtering continue to
improve the baselines further for the lower thresholds with high
margin. These demonstrate the high potential of using related terms
from word embedding but also show the need to customize the
set of terms for IR. We propose an approach to this customization
using the global-context of the terms in the following.

Ö© Õ
ª
PT (t |t 0 )P(t 0 |Md )®
(1)
­
0
t ∈q «t ∈R(t )
¬
Rekabsaz et al. [12] extend the idea into four probabilistic relevance
frameworks. Their approach revisits the idea of computing document relevance based on the occurrence of concepts. Traditionally,
concepts are represented by the words appear in the text, quantified
by term frequency (t f ). Rekabsaz et al. posit that we can have a
t f value lower than 1 when the term itself is not actually appear,
but another, conceptually similar term occurs in the text. Based on
it, they define the extended t f of a query word t in a document as
follows:
Õ
tc
fd = t fd +
PT (t |t 0 )t fd (t 0 )
(2)
c d) = P(q|Md ) =
LM(q,

t 0 ∈R(t )

However, in the probabilistic models, a series of other factors are
computed based on t f (e.g. document length). They therefore
propagate the above changes to all the other statistics and refer to
the final scoring formulas as Extended Translation model. Among
the extended models, as BM25 is a widely used and established

model in IR, we use the extended BM25 translation model (BM25)
in our experiments. Similar to the original papers in both models,
the estimation of PT is based on the Cosine similarity between two
embedding vectors.

3

Collection
Disc1&2
Disc4&5
AQUAINT

EXPERIMENT SETUP

We conduct our experiments on three test collections, shown in
Table 1. For word embedding vectors, we train the word2vec skipgram model [7] with 300 dimensions and the tool’s default parameters on the Wikipedia dump file for August 2015. We use the Porter
stemmer for the Wikipedia corpus as well as retrieval. As suggested
by Rekabsaz et al. [13], the extended terms set R(t) is selected from
the terms with similarity values of greater than a specific threshold.
Previous studies suggest the threshold value of around 0.7 as an
optimum for retrieval [12, 13]. To explore the effectiveness of less
similar terms, we try the threshold values of {0.60, 0.65…, 0.80}.
Since the parameter µ for Dirichlet prior of the translation language model and also b, k 1 , and k 3 for BM25 are shared between
the methods, the choice of these parameters is not explored as part
of this study and we use the same set of values as in Rekabsaz et
al. [12]. The statistical significance tests are done using the two
sided paired t-test and significance is reported for p < 0.05. The
evaluation of retrieval effectiveness is done with respect to Mean
Average Precision (MAP) as a standard measure in ad-hoc IR.

1106

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

½b
20

½b

15
10
5

½b
½b
b

½b

½
½

0
−5
−10

0.6

LM

d
LM

5
0.6

d Gen
LM
d Col
LM

0.7

d Oracle
LM

5
0.7

½b

0.90

15
10
5

½b
½b

½b

½
½

0
−5
−10

BM25

d

BM25

0.6

Threshold

(a) LM

d
BM25
dCol

BM25 Gen

5
0.6

d

BM25 Oracle

0.7

Threshold

5
0.7

Word2Vec Similarity

½b
% Improvement (MAP)

% Improvement (MAP)

20

0.85
0.80
0.75
0.70
0.65
0.60
0.5

(b) BM25

0.6

0.7
0.8
LSI Similarity

0.9

1.0

(c) Retrieval Gain/Loss

Figure 1: (a,b) The percentage of relative MAP improvement to the basic models, aggregated on all the collections. The b
and ρ signs show the significance of the improvement to the basic models and the extended models without post filtering
respectively (c) Retrieval gain or loss of the related terms for all the collection. The red (light) color indicate retrieval loss and
the green (dark) retrieval gain.

5

by the following optimization problem:
N
F
Õ
Ù

argmin
1
x j > θ j дi

GLOBAL-CONTEXT POST FILTERING

Looking at some samples of retrieval loss, we can observe many
cases of topic shifting: e.g. Latvia as query term is expanded
with Estonia, Ammoniac with Hydrogen, Boeing with Airbus, and
Alzheimer with Parkinson. As mentioned before, our hypothesis is that for the terms with high window-context similarity (i.e.
word2vec similarity) when they have high global context similarity
(i.e. co-occurrence in common documents), they more probably
refer to a similar topic (e.g. USSR and Soviet) and with low global
context similarity to different topics (e.g. Argentina and Nicaragua).
To capture the global context similarities, some older studies use
measures like Dice, Tanimoto, and PMI [9]. Cosine similarity has
been used as well, considering each term a vector with dimensionality of the number of documents in the collection, with weights given
either as simple incidence (i.e. 0/1), or by some variant of TFIDF.
Cosine can also be used after first applying Singular Value Decomposition on the TFIDF weighted term-document matrix, resulting
in the well known Latent Semantic Indexing (LSI) method [3] (300
dimensions in our experiments). To compute these measures, we
consider both the collection statistics and Wikipedia statistics, resulting in 12 sets of similarities (Dice, Tanimoto, PMI, Incidence
Vectors, TFIDF Vectors, LSI Vectors)×(collection, Wikipedia). We
refer to these similarity value lists as global context features.
Let first observe the relationship between LSI and word2vec
similarities of the terms. Figure 1c plots the retrieval gain/loss of
the terms of all the collections based on their word2vec similarities
as well as LSI (when using test collection statistics). The size of the
circles shows their gain/loss as the red color (the lighter one) show
retrieval loss and green (the darker one) retrieval gain. For clarity,
we only show the terms with the retrieval gain/loss of more than
0.01. The area with high word2vec and LSI similarity (top-right)
contains most of the terms with retrieval gain. On the other hand,
regardless of the word2vec similarity, the area with lower LSI tend
to contain relatively more cases of retrieval loss. This observation
encourages the exploration of a set of thresholds for global context
features to post filter the terms retrieved by embedding.
To find the thresholds for global context features, we explore
the highest amount of total retrieval gain after filtering the related
terms with similarities higher than the thresholds. We formulate it

Θ

i=1

(3)

j=1

where 1 is the indicator function, N and F are the number of terms
and features respectively, Θ indicates the set of thresholds θ j , x j the
value of the features, and finally д refers to the retrieval gain/loss.
We consider two approaches to selecting the datasets used to
find the optimum thresholds: per collection, and general. In the
per collection scenario (Col), for each collection we find different
thresholds for the features. We apply 5-fold cross validation by first
using the terms of the training topics to find the thresholds (solving
Eq. 3) and then applying the thresholds to post filter the terms of
the test topics. To avoid overfitting, we use the bagging method by
40 times bootstrap sampling (random sampling with replacement)
and aggregate the achieved thresholds.
In the general approach (Gen), we are interested in finding a
‘global’ threshold for each feature, which is fairly independent of
the collections. As in this approach the thresholds are not specific
for each individual collection, we use all the topics of all the test
collections to solve the optimization problem.

6

RESULTS AND DISCUSSION

To find the most effective set of features, we test all combinations
of features using the per collection (Col) post-filtering approach.
Given the post-filtered terms with each feature set, we evaluate the
c and BM25
 models. Our results show the superior effectiveness
LM
of the LSI feature when using the test collections as resource in
comparison with the other features as well as the ones based on
Wikipedia. The results with the LSI feature can be further improved
by combining it with the TFIDF feature. However, adding any of
the other features does not bring any improvement and therefore,
in the following, we only use the combination of LSI and TFIDF
features with both using the test collections statistics.
c and LM
c with post filterThe evaluation results of the original LM
ing with the general (Gen) and per collection (Col) approaches are
 is very similar
shown in Figure 2. The general behavior of BM25
and therefore no longer shown here. As before, statistical significance against the basic models is indicated by b and against the
translation models without post filtering, by ρ.

1107

Short Research Paper

0.29

b½

0.32

0.28

b½

0.30

½
½

b½
b½

b

MAP

MAP

0.31

0.29

0.24

b½
b½

0.23

b½

0.22

b½

0.26

0.21

½

0.25

½
½

0.20

0.28

0.24

0.19

0.27

0.23

0.18

0.26
0.6

0.22
0.6

5

0.7

0.6

5

0.7

b½

0.27
MAP

0.33

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

5
0.6

TREC 123

5

0.7

0.7

½

0.17
0.6

TREC 678
LM

d
LM

d Gen
LM

5

0.6

b½
½

0.7

5

0.7

Robust 2005

d Col
LM

d Oracle
LM

c with/without post filtering. The b and ρ signs show the significance of the improvement to
Figure 2: Evaluation results of LM
c
LM and LM without post filtering respectively.
c models with postTable 3: MAP of the translation models when terms filtered
The results show the improvement of the LM
c
with word embedding threshold of 0.7 and post filtered with
filtering in comparison with the original LM. The models with postthe Gen and Col approach.
filtering approaches specifically improve in lower word embedding
thresholds, however similar to the original translation models, the
Model Basic Tran. Tran.+Gen Tran.+Col
Collection
best performance is achieved on word embedding threshold of 0.7.
LM
0.275 0.283
0.290
0.295 bρ
TREC 123
c and BM25
 models with word embedding
The results for both LM
BM25 0.273 0.285
0.288
0.290 b
threshold of 0.7 are summarized in Table 3. Comparing the postLM
0.252 0.259
0.262
0.261
TREC 678
filtering approaches, Col shows better performance than Gen as
BM25 0.243 0.255
0.257
0.256 b
with the optimum word embedding threshold, it achieves significant
LM
0.183 0.204
0.208
0.209 bρ
Robust 2005
improvements over both the baselines in two of the collections.
BM25 0.181 0.203
0.207
0.209 bρ
Let us look back to the percentage of relative improvements,
demonstrate the importance of global context as a complementary
aggregated over the collections in Figures 1a and 1b. In both IR
to the window-context similarities.
models, while the Col approach has better results than Gen, their
REFERENCES
results are very similar to the optimum word embedding thresh[1] Adam Berger and John Lafferty. 1999. Information Retrieval As Statistical Transold (0.7). This result suggests to use the Gen approach as a more
lation. In Proc. of SIGIR.
[2] Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and Stephen Robertson. 2008. Selecting
straightforward and general approach for post filtering. In our
good expansion terms for pseudo-relevance feedback. In Proc. of SIGIR.
experiments, the optimum threshold value for the LSI similarities
[3] Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and
(as the main feature) is around 0.62 (vertical line in Figure 1c).
Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the
American society for information science (1990).
As a final point, comparing the two IR models shows that despite
[4] Mostafa Dehghani, Hamed Zamani, A. Severyn, J. Kamps, and W Bruce Croft.
c models, the BM25

the generally better performance of the LM
2017. Neural Ranking Models with Weak Supervision. In Proc. of SIGIR.
[5] Fernando Diaz, Bhaskar Mitra, and Nick Craswell. 2016. Query expansion with
models gain more. We speculate that it is due to the additional
locally-trained word embeddings. Proc. of ACL (2016).
modification of other statistics (i.e. document length and IDF) in
[6] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. 2016. A deep relevance

matching model for ad-hoc retrieval. In Proc. of CIKM.
the BM25 model and therefore it is more sensitive to the quality of
[7] Tomas Mikolov, Kai Chen, G. Corrado, and J. Dean. 2013. Efficient estimation of
the related terms. However an in-depth comparison between the
word representations in vector space. arXiv preprint arXiv:1301.3781 (2013).
models is left for future work.
[8] Bhaskar Mitra, F. Diaz, and N. Craswell. 2017. Learning to Match Using Local

7

and Distributed Representations of Text for Web Search. In Proc. of WWW.
[9] Helen J Peat and Peter Willett. 1991. The limitations of term co-occurrence data
for query expansion in document retrieval systems. Journal of the American
society for information science (1991).
[10] Jay M. Ponte and W. Bruce Croft. 1998. A Language Modeling Approach to
Information Retrieval. In Proc. of SIGIR.
[11] Navid Rekabsaz, Ralf Bierig, Bogdan Ionescu, Allan Hanbury, and Mihai Lupu.
2015. On the use of statistical semantics for metadata-based social image retrieval.
In Proc. of CBMI Conference.
[12] Navid Rekabsaz, Mihai Lupu, and Allan Hanbury. 2016. Generalizing Translation
Models in the Probabilistic Relevance Framework. In Proc. of CIKM.
[13] Navid Rekabsaz, Mihai Lupu, and Allan Hanbury. 2017. Exploration of a threshold
for similarity based on uncertainty in word embedding. In Proc. of ECIR.
[14] G Salton and MJ MacGill. 1983. Introduction to modern information retrieval.
McGraw-Hill (1983).
[15] Jinxi Xu and W Bruce Croft. 1996. Query expansion using local and global
document analysis. In Proc. of SIGIR.
[16] Hamed Zamani and W Bruce Croft. 2016. Embedding-based query language
models. In Proc. of ICTIR.
[17] Hamed Zamani and W Bruce Croft. 2017. Relevance-based Word Embedding. In
Proc. of SIGIR.
[18] Guido Zuccon, Bevan Koopman, Peter Bruza, and Leif Azzopardi. 2015. Integrating and evaluating neural word embeddings in information retrieval. In Proc. of
Australasian Document Computing Symposium.

CONCLUSION

Word embedding methods use (small) window-context of the terms
to provide dense vector representations, used to approximate term
relatedness. In this paper, we study the effectiveness of related
terms, identified by both window-based and global contexts, in
document retrieval. We use two state-of-the-art translation models to integrate word embedding information for retrieval. Our
analysis shows a great potential to improve retrieval performance,
damaged however by topic shifting. To address it, we propose the
use of global context similarity, i.e. the co-occurrence of terms in
larger contexts such as entire documents. Among various methods to measure global context, we identify LSI and TFIDF as the
most effective in eliminating related terms that lead to topic shifting. Evaluating the IR models using the post-filtered set shows a
significant improvement in comparison with the basic models as
well as the translation models with no post-filtering. The results

1108

