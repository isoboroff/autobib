Session 1A: Evaluation 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

The Probability That Your Hypothesis Is Correct,
Credible Intervals, and Effect Sizes for IR Evaluation
Tetsuya Sakai

Waseda University, Tokyo, Japan
tetsuyasakai@acm.org

ABSTRACT

KEYWORDS

Using classical statistical significance tests, researchers can only
discuss P(D + |H ), the probability of observing the data D at hand or
something more extreme, under the assumption that the hypothesis
H is true (i.e., the p-value). But what we usually want is P(H |D),
the probability that a hypothesis is true, given the data. If we use
Bayesian statistics with state-of-the-art Markov Chain Monte Carlo
(MCMC) methods for obtaining posterior distributions, this is no
longer a problem. That is, instead of the classical p-values and 95%
confidence intervals, which are often misinterpreted respectively
as “probability that the hypothesis is (in)correct” and “probability
that the true parameter value drops within the interval is 95%,” we
can easily obtain P(H |D) and credible intervals which represent
exactly the above. Moreover, with Bayesian tests, we can easily
handle virtually any hypothesis, not just “equality of means,” and
obtain an Expected A Posteriori (EAP) value of any statistic that
we are interested in. We provide simple tools to encourage the
IR community to take up paired and unpaired Bayesian tests for
comparing two systems. Using a variety of TREC and NTCIR data,
we compare P(H |D) with p-values, credible intervals with confidence intervals, and Bayesian EAP effect sizes with classical ones.
Our results show that (a) p-values and confidence intervals can
respectively be regarded as approximations of what we really want,
namely, P(H |D) and credible intervals; and (b) sample effect sizes
from classical significance tests can differ considerably from the
Bayesian EAP effect sizes, which suggests that the former can be
poor estimates of population effect sizes. For both paired and unpaired tests, we propose that the IR community report the EAP, the
credible interval, and the probability of hypothesis being true, not
only for the raw difference in means but also for the effect size in
terms of Glass’s ∆.

Bayesian hypothesis tests; confidence intervals; credible intervals;
effect sizes; Hamiltonian Monte Carlo; Markov Chain Monte Carlo;
p-values; statistical significance

1

INTRODUCTION

In March 2016, the American Statistical Association (ASA) published an official statement about the limitations of classical significance tests and p-values, in response to their continued misuse
and misinterpretations [33]. While ASA’s main statement does not
contain anything new (e.g., “A p-value, or statistical significance,
does not measure the size of an effect or the importance of a result”),
the document mentions some alternatives to classical significance
tests, including Bayesian methods. It goes on to say: “All these [alternative] measures and approaches rely on further assumptions, but
they may more directly address the size of an effect (and its associated
uncertainty) or whether the hypothesis is correct.” In the IR community, similar warnings against classical significance tests have been
given by Carterette [4] and Sakai [22], amongst others.
The above quotations from the ASA statement may be paraphrased as follows. Classical significance tests can only give us
P(D + |H ), the probability of observing the data D at hand or something more extreme under the assumption that the hypothesis H
is true (i.e., the p-value). But what we usually want is P(H |D), the
probability that a hypothesis is true, given the data. Theoretically,
the Bayesian framework proposed in the 18th century [2] can give
us exactly this, but it was heavily criticised during the 19th and
20th centuries (See Section 2.1). However, with the recent advent
of effective and efficient sampling algorithms for obtaining posterior distributions known as Markov Chain Monte Carlo (MCMC)
methods [15], Bayesian approaches to statistical testing are rapidly
gaining popularity among statisticians [30, 31]. Thus, as alternatives to the classical p-values and 95% confidence intervals which
are often misinterpreted respectively as “probability that the hypothesis is (in)correct” and “probability that the true parameter
value drops within the interval is 95%,” we can employ Bayesian
tests to easily obtain P(H |D) and credible intervals, which represent
exactly the above. Moreover, with Bayesian tests, we can easily
handle virtually any hypothesis, not just “equality of means,” as we
shall demonstrate later.
We provide simple tools to encourage the IR community to take
up paired and unpaired Bayesian tests for comparing two systems;
our tools are based on a state-of-the-art MCMC method called
Hamiltonian Monte Carlo and a recently-proposed variant called
No-U-Turn Sampler [13]. Using a variety of TREC1 and NTCIR2

CCS CONCEPTS
•Information systems → Retrieval effectiveness; Presentation of retrieval results;

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’17, August 7–11, 2017, Shinjuku, Tokyo, Japan.
© 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.
978-1-4503-5022-8/17/08. . . $15.00
DOI: http://dx.doi.org/10.1145/3077136.3080766

1 http://trec.nist.gov/

2 http://research.nii.ac.jp/ntcir/

25

Session 1A: Evaluation 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Fisher responsible for this5 . Second, even if the p-value is reported,
this is a function not only of the effect size (the magnitude of the
difference that we are interested in; See Section 3.6) but also the
sample size. That is, a small p-value (i.e., a statistically highly significant result) may just reflect a large sample size (e.g., number
of topics used for computing mean retrieval effectiveness scores)
rather than a large effect size [22]. Moreover, as was mentioned
in Section 1, the outcomes of classical significance tests are often
misinterpreted. We believe that it is time for the IR community to
start using Bayesian statistics regularly, perhaps along with classical significance tests if the community feels reluctant to let the
latter go. Since the Bayesian approach is not only highly intuitive
and flexible but now also computationally feasible, there really is
no reason to reject it.
In the field of psychology, Kruschke [14] argues that the Bayesian
approach is superior to the (unpaired) t-test: “Some people may
wonder which approach, Bayesian or NHST,6 is more often correct.
This question has limited applicability because in real research we
never know the ground truth; all we have is a sample of data. [. . .]
the relevant question is asking which method provides the richest,
most informative, and meaningful results for any set of data. The
answer is always Bayesian estimation.” In the present study, we
empirically demonstrate the relationships between paired/unpaired
t-tests and the corresponding state-of-the-art Bayesian methods
using a variety of real IR evaluation data.

data, we compare P(H |D) with p-values, credible intervals with
confidence intervals, and Bayesian EAP effect sizes with classical
ones. Our results show that (a) p-values and confidence intervals
can respectively be regarded as approximations of what we really
want, namely, P(H |D) and credible intervals; and (b) sample effect
sizes from classical significance tests can differ considerably from
the Bayesian EAP effect sizes, which suggests that the former can
be poor estimates of population effect sizes. For both paired and
unpaired tests, we propose that the IR community report the EAP,
the credible interval, and the probability of hypothesis being true,
not only for the raw difference in means but also for the effect size
in terms of Glass’s ∆.

2 RELATED WORK
2.1 Frequentists Versus Bayesians
It is well known that Ronald A. Fisher heavily and persistently
criticised the Bayesian statistics since the 1960s: “the theory of
inverse probability [i.e., Bayesian statistics] is founded upon an error,
and must be wholly rejected” [9] (p.9). During the 20th century, the
de facto standard in statistical analysis was indeed the “frequentist”
approach founded upon Fisher’s views, and the Bayesian approach
was largely neglected.
The Bayesian approach had two weaknesses. The first, which
has not been resolved completely even to this day, is the fact that
it relies on prior probabilities which nobody knows and therefore
must be set based on researchers’ subjective decisions or beliefs.
However, given the lack of knowledge about priors, noninformative
priors such as those that obey a uniform distribution can always be
used, although this too is a subjective decision which may or may
not reflect the true nature of the phenomenon under study3 . In the
present study, we simply follow the standard Bayesian practice of
employing uniform distributions for obtaining priors.
The second weakness in the original Bayesian approach was
that, despite the theoretical beauty of Bayes’ Theorem, it was often
difficult to obtain the posterior distributions as this often involves
computationally infeasible integrations. However, this second problem has actually been solved, with the recent advent of effective
and efficient sampling algorithms known as Markov Chain Monte
Carlo (MCMC) methods [15]. Because of this, Bayesian statistics
is now gaining popularity rapidly: for example, according to Toyoda [30], over one-half of Biometrika4 papers published in 2014
utilised Bayesian statistics. In the present study, we utilise a stateof-the-art MCMC method called Hamiltonian Monte Carlo [17] and
a recently-proposed variant called No-U-Turn Sampler [13], which
come with easy-to-use implementations.
Meanwhile, the classical significance testing approach of the frequentists have also received many criticisms over the past decades
(e.g. [12]). Some even consider significance tests to be harmful.
First, the practice of using the significance criterion α instead of
the p-value often leads to dichotomous thinking: “Is the difference
statistically significant, or not?” Ziliak and McCloskey [35] hold

2.2

Classical Significance Tests in IR

The limitations of classical significance tests have been pointed out
in the field of IR as well. Carterette argues: “we still believe p-values
from paired t-tests provide a decent rough indicator that is useful
for many of the purposes they are currently used for. We only argue
that p-values and significance test results in general should be taken
with a very large grain of salt, and in particular have an extremely
limited effect on publication decisions and community-wide decisions
about “interesting” research directions.” Sakai [22] encourages IR
researchers to report not only the p-values but also effect sizes and
confidence intervals. However, Sakai’s more recent examination
of over 1,000 SIGIR and TOIS papers [23] shows that about 30% of
the entire papers lack significance testing, while about 65% of the
papers with significance testing neither report p-values nor test
statistics.
While computer-based, distribution-free alternatives to classical
significance tests, namely, the bootstrap [20] and the randomisation
test [27], have been advocated for IR evaluation, they have not been
used as widely as the classical tests [23]. Moreover, these tests
address the same limited question as the classical tests: what is the
p-value?

2.3

Bayesian Inferences for IR

We are already beginning to see Bayesian approaches to IR evaluation. Carterette [3, 5] have proposed to evaluate IR systems by
modelling binary and graded relevance judgments directly instead

3

5

In 2005, Efron remarked: “What looks uninformative enough often turns out to subtly
force answers in one direction or another” while arguing that a combination of Bayesian
and frequentist ideas is needed to handle modern problems [8].
4 This is the journal in which William S. Gosset (or “Student”) published the famous
paper on the t -test in 1908 [29]. http://biomet.oxfordjournals.org/

“Ronald A. Fisher would say, “The potash manures are not statistically significant.
Disregard them. [ . . . ] William S. Gosset would say, “[ . . . ] If you want to know about
the potash manures you have to consider their pecuniary value compared to the barley
you’re trying to make money with” [35].
6 Null Hypothesis Significance Testing

26

Session 1A: Evaluation 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

of using evaluation measure scores as the atomic unit. Using three
different TREC data sets, he compared t-test p-values with the
posterior probabilities of his four Bayesian models to argue the
advantages of the latter. Carterette uses JAGS (Just Another Gibbs
Sampler), an open-source implementation of BUGS (Bayesian inference Using Gibbs Sampling) and its R interface rjags to conduct
MCMC simulations.
Our present work is in a sense less ambitious than that of Carterette
in that we are adhering to using evaluation measure scores as the
atomic unit (just like Carterette’s “Model 2”): we would like the
IR community to take up the habit of using Bayesian approaches,
and to do that, we believe that we need to move cautiously while
clarifying how the transition from classical significance testing will
affect our research findings. Carterette demonstrates that the t-test
(i.e., his “Model 1”) p-values and his Model 2 posterior probabilities
are strinkingly similar; we generalise this in several ways, by using
diverse data sets from NTCIR and TREC, and by comparing confidence intervals with credible intervals, and classical sample effect
sizes with their Bayesian counterparts.
Zhang et al. [34] propose to replace the use of classical significance tests with Bayesian tests with probabilistic graphical models
tailored to a specific problem in information retrieval, namely, text
classification. Following Kruschke [14, 15], they propose to make a
decision about two text classifers by comparing the High Density
Interval (HDI) and the Region of Practical Importance (ROPE) of
the performance difference δ . Zhang et al. use Metropolis-Hastings
(MH) sampling for their MCMC simulations.
Regarding the implementation of MCMC, we employ the stateof-the-art Hamiltonian Monte Carlo (HMC) [17] and its variant NoU-Turn Sampler (NUTS) [13] using stan and its R interface rstan,
which are gaining popularity. According to Hoffman and Gelman [13], HMC’s features “allow it to converge to high-dimentional
target distributions much more quickly than simpler methods such as
random walk Metropolis or Gibbs sampling”; NUTS automatically
sets a required parameter for HMC, namely the number of steps
L for the leap-frog method (See Section 3.2). Also, according to
Kruschke [15] (pp.399-400), “HMC can be more effective than the
various samplers in JAGS and BUGS, especially for large complex
models. [. . .] However, Stan is not universally faster or better (at this
stage in its development).”

ensures:

∫ +∞
1
f (x |θ )f (θ )dθ = 1 .
(2)
f (x) −∞
−∞
Hence, Eq. 1 implies that the property of the posterior distribution
f (θ |x) is governed by f (x |θ )f (θ ), i.e., the kernel.
If somehow the posterior distribution f (θ |x) has been obtained,
a point estimate of the population parameter θ can be obtained as
an expected a posteriori (EAP)7 :
∫
∫
f (x |θ )f (θ )
θˆEAP = E[θ |x] =
θ f (θ |x)dθ =
θ
dθ .
(3)
f (x)
Moreover, how the random variable θ moves around θˆEAP can be
quantified by the posterior variance (or its square root, posterior
standard deviation):
∫
V [θ |x] = E[(θ − θˆEAP )2 |x] = (θ − θˆEAP )2 f (θ |x)dθ .
(4)

3.2

HMC and NUTS

In the above discussion, we assumed that f (θ |x) can be computed
as defined in Eq. 1. However, in practice, it is usually not feasible
to do this analytically. That is where MCMC methods, which try to
sample θ repeatedly according to f (θ |x), come into play. MCMC
methods construct Markov Chains of parameter values so that the
underlying distribution eventually reaches a stationary distribution:
after a burn-in period (B), all of the values in the chain obey the
target distribution f (θ |x). Thus, if we collect T 0 values sequentially
and throw away the intial B values, the remaining T = T 0 −B values
can be regarded as realisations of θ that obey f (θ |x). Suppose that
we managed to obtain T = 100, 000 realisations of θ ; then, θˆEAP
(Eq. 3) can be obtained by simply averaging the T values. Similarly,
the posterior variance (Eq. 4) can be obtained by averaging8 the
squared difference from θˆEAP . A 95% credible interval for θ can be
obtained by sorting the T values in ascending order and then taking
the 2,500th and 97,500th values as the lower and upper limits. The
EAP values of other statistics such as effect sizes (See Section 3.6)
can be computed similarly. Moreover, for virtually any hypothesis
H (e.g., “mean 1 is higher than mean 2,” “mean 1 is at least 0.2 points

To discuss Bayesian tests, let us start with the famous Bayes’ rule:
f (x |θ )f (θ )
f (x |θ )f (θ )
= ∫ +∞
,
f (x)
f (x |θ )f (θ )dθ

f (θ |x)dθ =

We can also obtain an interval estimate of θ by removing an α/2%
area from either side of f (θ |x). This is the 100(1 − α)% credible
interval (or Bayesian confidence interval), which is highly intuitive:
the probability that the random variable θ lies within the interval is
100(1 − α)%. Recall that confidence intervals (CIs) used in classical
statistics do not represent this probability: in the classical paradigm,
θ is a constant, and when (say) 100 CIs are created from 100 different
samples, 100(1 −α)% of them are expected to contain that particular
θ.
As we do not know f (θ ), we employ a non-informative prior
distribution to avoid subjectivity to the best of our ability. Our
choice is to use a uniform distribution, in which case f (θ |x) is
governed solely by the likelihood f (x |θ ) in Eq. 1 and therefore the
EAP reduces to the maximum likelihood estimate (MLE). That is, in
our setting, the EAP of any parameter θ is not directly affected by
our subjective choice of f (θ ).

3 BAYESIAN TESTS
3.1 Bayesian Basics

f (θ |x) =

∫ +∞

(1)

−∞

where f (θ ) is the prior probability distribution of a random variable
θ (e.g., a population mean), f (x |θ ) is the likelihood of data x given
θ , and f (θ |x) is the posterior probability distribution of θ having
observed x. Note that this view is already strikingly different from
that of classical statistics: in classical significance testing, population parameters (θ ’s) are constants; in Bayesian statistics, they are
random variables that form distributions. Since f (θ |x) is a probability distribution, f (x) can be viewed as a normalising constant that

7

Alternatives would be a maximum a posteriori or a posterior median. These three estimates represent the mean, mode and median of the posterior distribution, respectively.
8 Dividing by T suffices since T is large; there is no need for bias correction.

27

Session 1A: Evaluation 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

higher than mean 2,” or “the effect size is greater than 0.5”), the
probability that H is correct can be obtained by just counting the
instances in which H holds among the T realisations. It is clear that
this approach is more versatile than classical significance tests.
Hamiltonian Monte Carlo (HMC) [17] and its variant No U-Turn
Sampler (NUTS) [13] are state-of-the-art MCMC methods which
we utilise for obtaining realisations of θ from f (θ |x). For details
of HMC, we refer the reader to recent books on this topic [15, 17].
However, it would help for us IR researchers to have a general idea
about its basic principles. In physics, Hamiltonian is the sum of
potential energy and kinetic energy; given a curved surface in an
ideal physical world, any object would move on the surface while
keeping the Hamiltonian constant. The path of the moving object
is governed by Hamilton’s equations of motion, which can be solved
by the leap-frog method with parameters ϵ (stepsize) and L (leapfrog
steps). To achieve sampling from a posterior distribution (which is
our “curved surface”), an intuitive explanation of what HMC does
is as follows: put an object somewhere on the surface and give it a
push; after L units of time, let it halt and record its position; give it
another push, and so on, until we have recorded T 0 positions.
Briefly, for a set of d parameters θ = (θ 1 , θ 2 , . . . , θd ), the HMC
algorithm looks like this:

can be assured that the chains have reached stationary distributions.
MCMC produces Markov Chains and therefore the values in them
are correlated to one another. If the chains produce very similar
values repeatedly, then that is highly inefficient from the sampling
point of view. Effective Sample Size (N eff ) is a measure available
in stan for quantifying sampling efficiency, which means “you
have obtained a sample of size T , but that is worth a sample of
size (approximately) N eff obtained when there is zero correlation
within the chain.” Hence, we should also check that N eff is a large
value. Both of the above measures are computed after removing
the burn-in’s from the chains. As our Bayesian test tools that we
introduce in Section 3.7 rely on stan and its R interface rstan, the
tools output R̂ and N eff on the R console. Since all experiments
reported in the paper use sufficiently large sample sizes, namely
T = 100, 000, we do not discuss these indicators henceforth.

3.4

As was discussed in Section 3.2, Bayesian tests are versatile. However, the present study focusses on the problems of comparing two
means from paired and unpaired data, since these are the most
common and basic problems in IR evaluation. While two-sided
tests that ask “are the two systems equally effective or not?” are
often recommended in classical significance tests given lack of prior
knowledge as to which system might be better, this is not a very
useful question from the Bayesian point of view, as two different
systems are, by definition, different. What is more practical to
consider is the probability that System 1 is better than System 2,
P(S1 > S2|D) (or, alternatively, P(S1 < S2|D) = 1 − P(S1 > S2|D)).
Hence we compare Bayesian tests with classical one-sided tests. To
be more specific, given two systems, we let S2 be the less effective
system according to the sample data and consider P(S1 < S2|D) (i.e.,
the probability of the less likely hypothesis) based on the Bayesian
test, while setting the classical null and alternative hypotheses as
H 0 : S1 = S2 and H 1 : S1 > S2 so that the p-value represents
“P(D + |S1 = S2).”

(1) Set θ (1) , ϵ, L,T 0, B (where T = T 0 − B); Let t = 1;
(2) Generate d independent values p (t ) = (p1t , p2t , . . . , pdt ) from
a standard normal distribution;
(3) Obtain candidates θ (a) , p (a) using the leap-frog method;
(4) Let θ (t +1) = θ (a) (i.e., accept the transition candidate) with
probability min(1, r ); otherwise θ (t +1) = θ (t ) (i.e., reject
the candidate and stay at the current position);
(5) End if T 0 = t; otherwise let t = t + 1 and go to Step 2.
The r in Step 4, which governs how often we can accept new candidates, is given by [30]:
r=

f (θ (a) , p (a) |x)
,
f (θ (t ) , p (t ) |x)

Classical versus Bayesian Tests for
Comparing Two Means

(5)

where f (θ, p|x) is a joint distribution of f (θ |x) and an independent
standard normal distribution f (p). One strength of HMC is that r is
often close to one, and therefore we can achieve efficient sampling
through many successful transitions in Step 4 while preserving the
Hamiltonian.
NUTS is a variant of HMC that automatically determines an appropriate value of L [13]; both HMC and NUTS utilise an algorithm
called dual averaging to automatically set ϵ [18]; hence, from the
viewpoint of the users of HMC and NUTS for Bayesian tests, we
do not have to worry about setting these parameters ourselves.

3.5

Statistical Models

For unpaired data, we assume that S1’s scores obey N (µ 1 , σ12 ), while
S2’s scores obey N (µ 2 , σ22 ), for both Bayesian and classical tests.
Hence, the classical test we employ is the Welch’s t-test, which does
not assume homoscedasticity (i.e., equal variances). It is known that
Student’s and Welch’s t-tests yield virtually identical p-values when
the two sample sizes are equal [16, 24]; the experiments reported
in this paper satisifes this condition and therefore our classical
test results can be regarded as representing both types of unpaired
t-test.
For paired data, the classical test we apply is the paired t-test,
which relies on the same normal assumptions as described above
and therefore the score differences obey N (µ 1 − µ 2 , σ12 + σ22 ). As
for the Bayesian test, we can easily consider a bivariate normal

3.3 R̂ and Effective Sample Size
For MCMC algorithms, methods for checking whether the values
have converged to a stationary distribution and for measuring the
sampling efficiency are available.
R̂, a measure for checking convergence, assumes that the MCMC
algorithm produces multiple Markov Chains, and compares the
variance across the multiple chains with the variance within the
chains9 . According to Gelman [10], if R̂ is less than 1.1 or 1.2, we

obey the desired distribution and we prefer to do so. As for the number of chains, since
R̂ can be computed even for a single chain, by breaking it into multiple chains, we
provide sample scripts for handling both multiple and single chains. The Bayesian test
results are almost the same either way, but we report those based on multiple chains.

9 Zhang

et al. [34] remark that “it is perfectly right to do a single long sampling run and
keep all samples.” However, it is very easy to throw away burn-in’s that may not yet

28

Session 1A: Evaluation 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

distribution [31]:
f (x 1 , x 2 |µ 1 , µ 2 , σ12 , σ22 , ρ)

1
e −q/2 ,
=
2πσ1 σ2 (1 − ρ)

realisations, to encourage researchers to use credibile intervals not
only for the raw difference in means but also for effect sizes. The
scripts and sample data are available from our website11 . R with
the rstan package12 and Rtools13 must be installed first in order
to use these scripts.
Figure 1 shows our sample R script for comparing paired data.
It reads a stan file which describes the aforementioned bivariate
normal distribution as well as generated quantities (the absolute
difference and Glass’s ∆’s), and a system score file written in R, and
generates five csv files that correspond to five Markov chains.
Figure 2 shows an output of our UNIX shell script that summarises the Bayesian test results by reading the csv files. Here, the
first argument is “1032” because this is the line number in each
csv file where the realisations start after the burn-in; the second
argument is the number of realisations contained in the file (excluding the burn-in’s); the third argument is the number of chains (i.e.,
number of csv files); the remaining arguments are the aforementioned Markov chain csv files. This script works for both multiple
and single chains, by setting the arguments appropriately14 . The
screenshot provides the following information about the paired
data from run1 and run2:
• The EAP for the difference in means is 0.042, and the 95%
credibile interval is [0.009, 0.074]. The probability that
µ 1 − µ 2 is greater than the specified threshold (which is set
to 0 by default within the script) is 99.4%;
• The EAP for Glass2 (with run2 taken as the baseline) is
0.189 (i.e., about 19% of run2’s standard deviation), and
the 95% credible interval is [0.043, 0.345]. The probability
that this effect size is greater than the specified threshold
(which is set to 0.2 by default) is 43.3%. Similar results with
run1 taken as the baseline are also presented.
• The EAP for the correlation (i.e., ρ in Eq. 6) between the
scores of run1 and run2 is 0.857, and the 95% credible interval is [0.767, 0.920]. The probability that the correlation
is greater than the specified threshold (which is set to 0.9
by default) is 12.2%.
Note that thresholds can be altered arbitrarily within the shell script
according to researchers’ practical needs.
Figures 3 and 4 provide similar screenshots of our scripts for
comparing unpaired data.

(6)

where
q=

x1 − µ1 x2 − µ2
x2 − µ2 2
x1 − µ1 2
1
) − 2ρ(
)(
)+(
) ] . (7)
[(
σ1
σ1
σ2
σ2
1 − ρ2

Here, ρ is the population correlation coefficient for x 1 ’s and x 2 ’s.
Thus, for paired data, we can discuss hypotheses about the correlation between the two systems just as well as those about means
and effect sizes if we are interested in that aspect.

3.6

Effect Sizes

Sakai [22] stresses the importance of reporting effect sizes and
confidence intervals in the context of classical significance testing as
a small p-value may just reflect a large sample size (See Section 2.2).
When comparing two means, the effect size is usually given as
the difference between the two measured in standard deviation
units. We argue that Bayesian test results in IR should also be
accompanied with effect sizes as well as credible intervals.
While there are several choices of effect sizes for comparing
two means, we choose to avoid relying on the homoscedasticity
assumption, to be consistent with the statistical models described in
Section 3.5. One of the simplest effect size in such a case is Glass’s
∆ [19]. That is, if System 1 is taken as the baseline run (or “control
group,”) then its standard deviation is probably representative of
an “ordinary world” before the advent of System 2, and therefore:
µ1 − µ2
Glass∆1 =
.
(8)
σ1
Thus, Glass’s ∆ measures the absolute difference in “ordinary” standard deviation units. Alternatively, if System 2 is taken as the
baseline:
µ1 − µ2
Glass∆2 =
.
(9)
σ2
In our experiments, we focus on Eq. 9 since the second system is
the less effective one (i.e., “baseline”) according to the sample data.
For convenience, we will hereafter refer to this version of effect
size simply as “Glass2.”
We can easily obtain the EAP and credible intervals for Glass2,
as well as the probability of a hypothesis about the effect size
being true, in exactly the same way as described in Section 3.2.
For example, the EAP for Glass2 can be obtained by computing
Eq. 9 T times using T realisations of µ 1 , µ 2 , σ2 and then averaging
them. Hence we propose that the IR community report the EAP,
the credible interval and the probablity of hypothesis being true
not only for the raw difference in means but also for effect sizes.
Note that this is applicable to both paired and unpaired tests.

3.7

4

EXPERIMENTS

To encourage IR researchers to transition comfortably from the
classical significance test paradigm to the Bayesian one for comparison of means, we now report on experiments that compare
Bayesian results against classifical test results using actual IR data.
More specifically, we compare the probability that System X is
better than System Y with p-values; credible intervals with classical
confidence intervals, as well as effect sizes computed based on both
approaches. We show that (a) p-values and confidence intervals
can respectively be regarded as approximations of what we really
want, namely, P(H |D) and credible intervals; and (b) sample effect

Implementation

Here, we briefly describe our Bayesian test tools for comparing
two means. The sample R scripts are based on those developed
by Hideki Toyoda [30]10 . We have sample scripts for generating
both multiple and single chains but discuss only the former here.
We have added a few shell scripts for postprocessing the Bayesian

11

http://www.f.waseda.jp/tetsuya/tools.html
https://cran.r-project.org/
13 https://cran.r-project.org/bin/windows/Rtools/
14 By typing the command without arguments, suggested sets of arguments are
displayed.
12

10

Toyoda’s original scripts are available from http://www.asakura.co.jp/G 27 2.php?
id=200. The present author is solely responsible for the modifications and any errors
introduced thereby.

29

Session 1A: Evaluation 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 1: IR data sets and evaluation measures used in this study
Data set name
NTCIR7IR4QA
NTCIR9INTENT
NTCIR12STC
TREC03robust
TREC11webD
TREC15TS

year
2008
2011
2016
2003
2011
2015

track/task
IR for question answering [25]
INTENT [28] (web diversity task)
short text conversation [26] (tweet retrieval)
robust track [32]
web track diversity task [7]
temporal summarisation track, task 2 [1]

language
Chinese
Chinese
Chinese
English
English
English

measure
Q-measure
D]-nDCG@10
nERR@10
nDCG@1000
α -nDCG@10
H

tool
NTCIREVAL
NTCIREVAL
NTCIREVAL
NTCIREVAL
ndeval
-

#teams
9
7
16
16
9
9

#runs
40 (20)
24 (20)
44 (20)
78 (20)
25 (20)
22 (20)

#topics
97
100
100
50
50
21

Figure 3: A sample R script for comparing unpaired data.
Figure 1: A sample R script for comparing paired data.

Figure 4: A shell script for obtaining the EAP, credible intervals, and the probability that the hypothesis is correct
(paired data) for the absolute difference, and Glass’s ∆ (unpaired data).
official measures of that track/task. Also, for each data set, we considered only the top 20 runs for pairwise comparisons as measured
by that particular evaluation measure, which gives us 190 run pairs.
For TREC11webD, α-nDCG was computed using their official evaluation script ndeval16 ; For TREC15TS, the values of H (which is
basically like a nugget-based F-measure defined over a timeline [1])
were obtained from the official results of the track; other evaluation
measures were computed using NTCIREVAL17 , with the exponential
gain value setting (See, for example, [26]).
For each data set, we conducted Bayesian and classical tests for
every system pair (where System 1 outperforms System 2 for the
sample data), using both paired and unpaired tests. For classical
paired and unpaired tests, common sample effect sizes were obtained
by substituting sample means and System 2’s sample standard
deviations into Eq. 9. For Bayesian paired and unpaired tests, the
EAP values of µ 1 , µ 2 , σ2 were used with Eq. 9, under the two models
described in Section 3.5, respectively.

Figure 2: A shell script for obtaining the EAP, credible intervals, and the probability that the hypothesis is correct
(paired data) for the absolute difference, Glass’s ∆, and correlation coefficient (paired data).
sizes from classical significance tests can differ considerably from
the Bayesian EAP effect sizes, which suggests that the former can
be poor estimates of population effect sizes.
Table 1 provides a brief summary of the six data sets we used for
our experiments. To strengthen the generalisability of our experimental results, we tried to cover diverse IR tasks from both TREC
and NTCIR. For each data set (i.e., test collection with its submitted
runs), we chose one particular commonly-used evaluation measure:
with the exception of TREC03robust15 , we chose from one of the
15

We chose nDCG rather than Average Precision (AP) for TREC03robust as AP
cannot handle graded relevance even though the data set comes with graded relevance
assessments.

16
17

30

http://trec.nist.gov/data/web/11/ndeval.c
http://research.nii.ac.jp/ntcir/tools/ntcireval-en.html

Session 1A: Evaluation 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 5: Paired Bayesian vs classical tests: comparisons with NTCIR data.

4.1

(L) (i.e., the rightmost column) compare the Bayesian credible interval upper limit with the confidence interval upper limit. Figure 6
provides similar information for TREC03robust, TREC11webD, and
TREC15TS.
With the exception of Figure 6(J), i.e., the effect size results
for TREC15TS which we shall discuss in Section 4.3, the results in
Figures 5 and 6 are highly consistent across the diverse NTCIR and
TREC data sets and the messages are clear:

Paired Test Results

Here we compare the paired Bayesian test (based on NUTS) with
the classical paired t-test. We compare: (I) the paired Bayesian
P(S1 < S2|D) (i.e., the probability of the less likely hypothesis)
with the classical one-sided paired p-value (See Section 3.4); (II) the
paired 95% credible interval with the classical paired 95% confidence
interval; (III) the paired Bayesian EAP Glass2 with the classical
Glass2 based on sample statistics. The margin of error for the
p classical paired 95% confidence interval is given by t(n − 1; 0.05) V /n,
where n is the sample size, V is the sample variance of the score
differences, and t(ϕ; α) is the two-sided critical t value18 .
Figure 5 visualises the results of comparing the Bayesian and
classical paradigms for paired data with NTCIR7IR4QA (graphs (A)(D)), NTCIR9INTENT (graphs (E)-(H)), and NTCIR12STC (graphs (I)(L)). Graphs (A), (E) and (I) (i.e., the leftmost column) compare the
Bayesian P(S1 < S2|D) with the p-value; graphs (B), (F), and (J)
compare the Bayesian EAP Glass2 with the sample Glass2; graphs
(C), (G) and (K) compare the Bayesian credible interval lower limit
with the confidence interval lower limit; and graphs (D), (H) and
18

(1) Graphs (A), (E), and (I) in Figures 5 and 6 show that the
Bayesian P(S1 < S2|D) and the classical p-values are very
highly correlated, echoing an earlier observation by
Carterette [5] (See Section 2.3). Thus, while P(S1 < S2|D)
is what we usually want, it appears that the one-sided pvalue, which represents P(D + |S1 = S2), can be considered
as a reasonable approximation of P(S1 < S2|D).
(2) Graphs (C), (D), (G), (H), (K), (L) in Figures 5 and 6 show
that the Bayesian 95% credible intervals and the classical
95% confidence intervals are also very similar, despite the
fundamental differences in what they represent. Thus, the
confidence interval can be considered as an approximation
to the credible interval, which is what we really want.

T.INV.2T(P, α ) with Microsoft Excel.

31

Session 1A: Evaluation 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 6: Paired Bayesian vs classical tests: comparisons with TREC data.
(3) Graphs (B) and (F) in Figures 5 and 6 suggest that, while
the Bayesian EAP effect sizes generally align with the sample effect sizes (Glass2), if the sample effect size is small
(e.g., less than 0.2), that may be an underestimation of the
population effect size. For example, Figure 6(B) indicates
an instance (with a baloon) where the sample effect size is
0.070 even though the Bayesian EAP effect size, which we
believe to be more accurate, is 0.113.

P(S1 < S2|D) (i.e., the probability of the less likely hypothesis)
with the classical one-sided unpaired p-value (See Section 3.4);
(II) the unpaired 95% credible interval with the classical unpaired
95% confidence interval; (III) the unpaired Bayesian EAP Glass2
with the classical Glass2 based on sample statistics. The margin of
error for the
p classical unpaired 95% confidence interval is given by
t(ϕ ∗ ; 0.05) V1 /n 1 + V2 /n 2 , where n 1 , n 2 are the sample sizes, V1 , V2
are the sample variances, and the approximated degrees of freedom
ϕ ∗ is given by [24]:

Observations (1) and (2) suggest that, even though the IR community may have relied on p-values and confidence intervals for
decades, sometimes with incorrect interpretations, switching to
Bayesian approaches would not turn all the experimental results in
the literature upside down. As for Observation 3, even though we
lack the ground truth for population effect sizes, we would like to
repeat Kruschke’s argument [14]: “the relevant question is asking
which method provides the richest, most informative, and meaningful
results for any set of data [. . .]” (See Section 2.1).

4.2

ϕ∗ = (

V1 V2 2 (V1 /n 1 )2 (V2 /n 2 )2
+ ) /{
+
}.
n1 n2
n1 − 1
n2 − 1

(10)

In fact, since our unpaired test experiments merely regard the
paired data from NTCIR and TREC (i.e., two systems evaluated
with a common topic set) as unpaired data, n 1 = n 2 holds in our
case.
Figure 7 visualises the results of comparing the Bayesian and
classical paradigms for unpaired TREC data, similarly to Figure 6.
The unpaired results with the NTCIR data, which look very much
like the corresponding paired test results with the NTCIR data
shown in Figure 5, are omitted in this paper due to lack of space.

Unpaired Test Results

Here we compare the unpaired Bayesian test (based on NUTS) with
the classical Welch’s t-test. We compare: (I) the unpaired Bayesian

32

Session 1A: Evaluation 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 7: Unpaired Bayesian vs classical tests: comparisons with TREC data.
Note that the classical sample effect sizes used in Graphs (B), (F),
and (J) are the same as the ones used in Figure 6. For example, we
have observed from Figure 6(B) that while the sample effect size
for a system pair was 0.070, the Bayesian paired model gives us
an EAP Glass2 of 0.113; in contrast, Figure 7(B) indicates that the
Bayesian unpaired model gives us an EAP Glass2 of 0.161 for the
same system pair.
It is clear that Observations 1-3 listed up in Section 4.1 also
hold for the unpaired tests as well, which further strengthens our
findings. However, just like Figure 6(J), Figure 7(J) shows and
anomalous result for TREC15TS: we therefore discuss these results
separately in the next section.

4.3

than EAPs. For example, for a system pair whose sample effect size
is 11.39, the paired Bayesian EAP is 12.10 (Figure 6(J)), while the unpaired Bayesian EAP is 12.37 (Figure 7(J)), as indicated by baloons.
Similarly, for a system pair whose sample effect size is 16.19, the
paired Bayesian EAP is 12.55, while the unpaired Bayesian EAP
is 12.76. This is in contrast to the aforementioned Obsevation 3
(Section 4.1) for the other data sets.
While the other evaluation measures used in our experiments
have been studied quite extensively (e.g., [6, 20, 21]), we are not
aware of any work in the literature that validated H in a similar
way, and we believe that an investigation is in order. For example,
while the actual nDCG scores from our TREC03robust data range
between 0 and 0.9816 (i.e., almost fully covering the theoretical
range of [0, 1]), the actual H scores from TREC15TS range between
0 and 0.4021: that is, the actual range width is about 0.4 rather
than 1. Moreover, the standard deviations of H for each system
are very low compared to the other measures, causing very large
effect sizes (See Eq. 9): hence, note that the axis scales of Figure 6(J)
and 7(J) are very different from the other effect size graphs. While
studying the properties of a new measure is not the focus of this

On the Anomalous Behaviour of H

The nugget-based H measure, the primary measure from the TREC
2015 Temporal Summarisation track, demonstrates a strange behaviour in Figure 6(J) and Figure 7(J), where the Bayesian EAP
effect sizes and sample effect sizes are compared. The graphs look
quite different from Graphs (B) and (F) of Figures 5-7. More specifically, for a small set of system pairs, EAP values are larger than
sample effect sizes; for a few others, sample effect sizes are larger

33

Session 1A: Evaluation 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

study, these anomalous results with H may deserve attention from
other researchers such as the TREC track coordinators.

5

of ACM WSDM 2011. 75–84.
[7] Clarles L. A. Clarke, Nick Craswell, Ian Soboroff, and Ellen M. Voorhees. 2012.
Overview of the TREC 2011 Web Track. In Proceedings of TREC 2011.
[8] Bradley Efron. 2005. Bayesians, Frequentists, and Scientists. J. Amer. Statist.
Assoc. 100, 469 (2005), 1–5.
[9] Ronald A. Fisher. 1970. Statistical Methods for Research Workers (14th Edition).
Oliver & Boyd.
[10] Andrew Gelman. 1996. Inference and Monitoring Convergence. In Markov Chan
Monte Carlo in Practice, W. R. Gilks, S. Richardson, and D. J. Spiegelhalter (Eds.).
Chapman & Hall/CRC, Chapter 8.
[11] Andrew Gelman, Jennifer Hill, and Masanao Yajima. 2008. Why We (Usually)
Don’t Have to Worry about Multiple Comparisons. Technical Report.
[12] Lisa L. Harlow, Stanley A. Mulaik, and James H. Steiger (Eds.). 2016. What If
There Were No Significance Tests (Classic Edition). Routledge.
[13] Matthew D. Hoffman and Andrew Gelman. 2014. The No-U-Turn Sampler:
Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. Journal of Machine
Learning Research 15 (2014), 1351–1381.
[14] John K. Kruschke. 2013. Bayesian Estimation Supersedes the t test. Journal of
Experimental Psychology: General 142, 2 (2013), 573–603.
[15] John K. Kruschke. 2015. Doing Bayesian Data Analysis (Second Edition). Elsevier.
[16] Yasushi Nagata. 1996. How to Understand Statistical Methods (in Japanese). Nikkagiren.
[17] Radford M. Neal. 2011. MCMC Using Hamiltonian Dynamics. In Handbook of
Markov Chain Monte Carlo, Steve Brooks, Andrew Gelman, Galin L. Jones, and
Xiao-Li Meng (Eds.). 113–162.
[18] Yurii Nesterov. 2009. Primal-Dual Subgradient Methods for Convex Problems.
Mathematical Programming 120 (2009), 221–259.
[19] Matia Okubo and Kensuke Okada. 2012. Psychological Statistics to Tell Your Story:
Effect Size, Confidence Interval, and Power. Keiso Shobo.
[20] Tetsuya Sakai. 2006. Evaluating Evaluation Metrics based on the Bootstrap. In
Proceedings of ACM SIGIR 2006. 525–532.
[21] Tetsuya Sakai. 2011. Evaluating Diversified Search Results Using Per-Intent
Graded Relevance. In Proceedings of ACM SIGIR 2011. 1043–1052.
[22] Tetsuya Sakai. 2014. Statistical Reform in Information Retrieval? SIGIR Forum
48, 1 (2014), 3–12.
[23] Tetsuya Sakai. 2016. Statistical Significance, Power, and Sample Sizes: A Systematic Review of SIGIR and TOIS, 2006-2015. Proceedings of ACM SIGIR 2016 (2016),
5–14.
[24] Tetsuya Sakai. 2016. Two Sample T-tests for IR Evaluation: Student or Welch?.
In Proceedings of ACM SIGIR 2016. 1045–1048.
[25] Tetsuya Sakai, Noriko Kando, Chuan-Jie Lin, Teruko Mitamura, Hideki Shima,
Donghong Ji, Kuang-Hua Chen, and Eric Nyberg. 2008. Overview of the NTCIR-7
ACLIA IR4QA Task. In Proceedings of NTCIR-7. 77–114.
[26] Lifeng Shang, Tetsuya Sakai, Zhengdong Lu, Hang Li, Ryuichiro Higashinaka,
and Yusuke Miyao. 2016. Overview of the NTCIR-12 Short Text Conversation
Task. In Proceedings of NTCIR-12. 473–484.
[27] Mark D. Smucker, James Allan, and Ben Carterette. 2007. A Comparison of
Statistical Significance Tests for Information Retrieval Evaluation. In Proceedings
of ACM CIKM 2007. 623–632.
[28] Ruihua Song, Min Zhang, Tetsuya Sakai, Makoto P. Kato, Yiqun Liu, Miho
Sugimoto, Qinglei Wang, and Naoki Orii. 2011. Overview of the NTCIR-9
INTENT Task. In Proceedings of NTCIR-9. 82–105.
[29] Student. 1908. The Probable Error of a Mean. Biometrika 6, 1 (1908), 1–25.
[30] Hideki Toyoda (Ed.). 2015. Fundamentals of Bayesian statistics: Practical Getting
Started by Hamiltonian Monte Carlo Method (in Japanese). Asakura Shoten.
[31] Hideki Toyoda. 2016. An Introduction to Statistical Data Analysis: Bayesian
Statistics for ‘post p-value era’ (in Japanese). Asakuha Shoten.
[32] Ellen M. Voorhees. 2004. Overview of the TREC 2003 Robust Retrieval Track. In
Proceedings of TREC 2003.
[33] Ronald L. Wasserstein and Nicole A. Lazar. 2016. The ASA’s Statement on
P-values: Context, Process, and Purpose. The American Statistician (2016).
[34] Dell Zhang, Jun Wang, Emine Yilmaz, Xiaoling Wang, and Yuxin Zhou. 2016.
Bayesian Performance Comparison of Text Classifiers. In Proceedings of ACM
SIGIR 2016. 15–24.
[35] Stephen T. Ziliak and Deirdre N. McCloskey. 2008. The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives. The University of
Michigan Press.

CONCLUSIONS AND FUTURE WORK

Using diverse data sets from TREC and NTCIR, we compared,
under both paired and unpaired data settings, (I) the Bayesian
P(S1 < S2|D) (i.e., the probability of the less likely hypothesis) with
the classical one-sided p-value; (II) the 95% credible interval with
the classical 95% confidence interval; (III) the Bayesian EAP Glass2
with the classical Glass2 based on sample statistics. Our results
showed that (a) p-values and confidence intervals can respectively
be regarded as approximations of what we really want, namely,
P(H |D) and credible intervals; and (b) sample effect sizes from classical significance tests can differ considerably from the Bayesian
EAP effect sizes, which suggests that the former can be poor estimates of population effect sizes. Fortunately, our results suggest
that Bayesian statistics will not turn all experimental results in the
IR literature upside down; however, we hope that these results,
as well as our tools, will help IR researchers to take up Bayesian
hypothesis testing approaches. For both paired and unpaired tests,
we propose that the IR community report the EAP, the credible
interval, and the probability of hypothesis being true, not only for
the raw difference in means but also for Glass’s ∆.
The present study focussed on the problem of comparing two
systems, and did not address the multiple comparison and familywise
error rate problems [11]. If a researcher is interested in the p-value
of every system pair, one effective way to obtain them would be to
employ the randomised Tukey HSD test [4, 22], which is completely
distribution-free. In future work, we would like to explore Bayesian
alternatives to this test and validate them.

ACKNOWLEDGEMENTS
We thank Professor Hideki Toyoda (Waseda University) for letting
us modify his R code and distribute it, and Dr. Matthew EkstrandAbueg (Google) for providing the TREC temporal summarisation
track results.

REFERENCES
[1] Javed Aslam, Fernando Diaz, Matthew Ekstrand-Abueg, Richard McCreadie,
Virgil Pavlu, and Tetsuya Sakai. 2016. TREC 2015 Temporal Summarization
Track. In Proceedings of TREC 2015.
[2] Thomas Bayes. 1763. An Essay towards Solving a Problem in the Doctrine of
Chances. Philosophical Transactions of the Royal Society of London 53 (1763),
370–418.
[3] Ben Carterette. 2011. Model-Based Inference About IR Systems. In Proceedings
of ICTIR 2011 (LNCS 6931). 101–112.
[4] Ben Carterette. 2012. Multiple testing in statistical analysis of systems-based
information retrieval experiments. ACM TOIS 30, 1 (2012).
[5] Ben Carterette. 2015. Bayesian Inference for Information Retrieval Evaluation.
In Proceedings of ACM ICTIR 2015. 31–40.
[6] Charles L.A. Clarke, Nick Craswell, Ian Soboroff, and Azin Ashkan. 2011. A Comparative Analysis of Cascade Measures for Novelty and Diversity. In Proceedings

34

