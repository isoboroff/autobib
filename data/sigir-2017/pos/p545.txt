Session 5A: Retrieval Models and Ranking 3

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Learning to Diversify Search Results via Subtopic Attention
Zhengbao Jiang1,2 , Ji-Rong Wen1,2,3 , Zhicheng Dou1,2 ,
Wayne Xin Zhao1,2 , Jian-Yun Nie4 , Ming Yue1,2
1 School

of Information, Renmin University of China
Key Laboratory of Big Data Management and Analysis Methods, China
3 Key Laboratory of Data Engineering and Knowledge Engineering, MOE, China
4 DIRO, Université de Montréal, Québec
rucjzb@163.com, jirong.wen@gmail.com, dou@ruc.edu.cn,
batmanfly@gmail.com, nie@iro.umontreal.ca, yomin@ruc.edu.cn
2 Beijing

ABSTRACT

i.e. the following document should be “different” from the former
ones based on some similarity measures. Instead, explicit approaches [1, 12, 13, 16, 27, 35] model intents (or subtopics) explicitly.
They aim to improve intent coverage, i.e. the following document
should cover the intents not satisfied by previous ones. Intents or
subtopics can be determined by techniques such as query reformulation [2, 14, 34, 38] and query clustering based on query logs
and other types of information. Existing studies showed that explicit approaches have better performance [12, 13, 16, 27, 35] than
implicit approaches due to several reasons: on the one hand, they
provide a more natural way to handle subtopics than implicit approaches; on the other hand, their ranking functions are closer to
the diversity evaluation metrics which are mostly based on explicit subtopics. Furthermore, most similarity measures used in the
implicit approaches, e.g., those based on language model or vector
space model, are determined globally on the whole documents, regardless of possible search intents. This might be problematic for
search result diversification: two documents could contain similar
words and considered globally similar, but this similar part may be
unrelated to underlying search intents.
To avoid heuristic and handcrafted functions and parameters, a
new family of research work using supervised learning is proposed. They try to learn a ranking function automatically. Their major focus lies in the modeling of diversity, including structural prediction [36], rewarding functions for novel contents [39], measurebased direct optimization [32], and neural network based method
[33]. Regardless of diversity modeling and optimization methods,
all these solutions inherit the spirit of MMR which is an implicit
approach and do not take intents into consideration. Although the
learning methods may result in a better similarity measure, they
are hindered by the gap between reducing document redundancy
and improving intent coverage. They suffer from similar problems
with implicit unsupervised approaches. Without modeling subtopics explicitly, they can’t directly improve intent coverage. Hence,
there is a need to incorporate explicit subtopic modeling into supervised diversification methods.
To address the above issue, we propose to model subtopics in a
general supervised learning framework. Our framework combines
the strengths of both explicit unsupervised approaches and (implicit) supervised approaches. First, subtopics are explicitly modeled,
allowing us to improve intent coverage in a proactive way. Second, it automatically learns the diversification ranking function,
and is able to capture complex interaction among documents and

Search result diversification aims to retrieve diverse results to satisfy as many different information needs as possible. Supervised
methods have been proposed recently to learn ranking functions
and they have been shown to produce superior results to unsupervised methods. However, these methods use implicit approaches
based on the principle of Maximal Marginal Relevance (MMR). In
this paper, we propose a learning framework for explicit result diversification where subtopics are explicitly modeled. Based on the
information contained in the sequence of selected documents, we
use attention mechanism to capture the subtopics to be focused on
while selecting the next document, which naturally fits our task
of document selection for diversification. The framework is implemented using recurrent neural networks and max-pooling which
combine distributed representations and traditional relevance features. Our experiments show that the proposed method significantly outperforms all the existing methods.

KEYWORDS
search result diversification; subtopics; attention

1

INTRODUCTION

In real search scenario, queries issued by users are usually ambiguous or multi-faceted. In addition to being relevant to the query, the
retrieved documents are expected to be as diverse as possible in order to cover different information needs. For example, when users
issue “apple”, the underlying intents could be the IT company or
the fruit. The retrieved documents should cover both topics to increase the chance to satisfy users with different information needs.
Traditional approaches to search result diversification are usually unsupervised and adopt manually defined functions with empirically tuned parameters. Depending on whether the underlying
intents (or subtopics) are explicitly modeled, they can be categorized into implicit and explicit approaches [28]. Implicit approaches [6] do not model intents explicitly. They emphasize novelty,
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full
citation on the first page. Copyrights for components of this work owned by others
than the author(s) must be honored. Abstracting with credit is permitted. To copy
otherwise, or republish, to post on servers or to redistribute to lists, requires prior
specific permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR’17, August 7–11, 2017, Shinjuku, Tokyo, Japan
© 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-5022-8/17/08. . . $15.00
DOI: http://dx.doi.org/10.1145/3077136.3080805

545

Session 5A: Retrieval Models and Ranking 3

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 2: Categorization of diversification approaches.

Table 1: Subtopic relevance example.
doc\subtopic
d1
d2
d3
d4

i1
√
√

i2
√
√

×
×

×
√

i3

unsupervised

×
×
√

implicit MMR

×

explicit

subtopics. We call this framework Document Sequence with Subtopic Attention (DSSA). More specifically, to select the next document, we first model the sequence of selected documents in order to capture their contents as well as their relationship with the
subtopics. Then based on the information contained by previous
documents, attention mechanism is used to determine the undercovered subtopics to which we have to pay attention in selecting
the next document. Attention mechanism has been successfully
used to deal with various problems in image understanding [24]
and NLP [3, 21]. This mechanism corresponds well to the document selection problem in search result diversification: attention
on subtopics changes along with the addition of a document in the
result list. For example. Assume that we have 3 subtopics and 4 documents whose relevance judgments are shown in Table 1. Given
that we have selected d 1 and d 2 , which cover subtopics i 1 and i 2 ,
the attention for next choice should incline to i 3 which is not covered, thus d 3 is a better choice than d 4 at this position. We will show
that the DSSA framework is general enough to cover the ideas of
previous unsupervised explicit methods.
We then propose a specific implementation of DSSA using recurrent neural networks (RNN) and max-pooling to leverage both distributed representations and traditional relevance features, which
we call DSSA-RNNMP. Experimental results on TREC Web Track
data show that our method outperforms the existing methods significantly. To our knowledge, this is the first time that a supervised
learning framework with attention mechanism is used to model
subtopics explicitly for search result diversification.

2.2

IA-Select, xQuAD, PM2, TxQuAD, DSSA
TPM2, HxQuAD, HPM2, 0-1 MSKP (our approach)

Explicit Diversification Approaches

Explicit approaches model subtopics underlying a query, aiming
at returning documents covering as many subtopics as possible.
These approaches leverage external resources to explicitly represent information needs in subtopics. IA-Select [1] uses classified
topical categories based on ODP taxonomy. xQuAD [27] is a probabilistic framework that uses query reformulations as intent representations. PM2 [13] tackles search result diversification problem
from the perspective of proportionality. TxQuAD and TPM2 [12]
represent intents by terms and transform intent coverage to term
coverage. Hu et al. [16] proposed to use a hierarchical structure for
subtopics instead of a flat list, which copes with the inherent interaction among subtopics. Two specific models, namely HxQuAD
and HPM2, were proposed using hierarchical structure. Yu et al.
[35] formulated diversification task as a 0-1 multiple subtopic knapsacks (0-1 MSKP) problem where documents are chosen like filling
up multiple subtopic knapsacks. To tackle this NP-hard problem,
max-sum belief propagation is used.
As summarized in Table 2, all existing explicit approaches are
unsupervised and the functions and parameters are defined heuristically. In this paper, we use supervised learning to model the
interaction among documents and subtopics simultaneously.

The basic assumption of implicit diversification approaches is that
dissimilar documents are more likely to satisfy different information needs. The most representative approach is MMR [6]:
dj ∈ C

SVM-DIV, R-LTR,
PAMM, NTN

al. [39] proposed relational learning-to-rank model (R-LTR) which
learns to score a document based on both relevance and novelty
automatically, in order to maximize the probability of optimal rankings. Based on R-LTR score function, Xia et al. [32] proposed
a perceptron algorithm using measures as margins (PAMM) to directly optimize evaluation metrics by enlarging the score margin
of positive and negative rankings. They further proposed to use
a neural tensor network (NTN) [33] to measure document similarity automatically from document representations, which avoids
the burden to define handcrafted diversity features.
The above supervised approaches are shown to outperform the
unsupervised counterparts. However, they are all implicit approaches without using subtopics. In this paper, we propose a learningbased explicit approach which models subtopics explicitly.

2 RELATED WORK
2.1 Implicit Diversification Approaches

S MMR (q, d, C) = (1 − λ)S rel (d, q) − λ max S div (d, d j ),

supervised

(1)

where S rel and S div model document d’s relevance to the query q
and its similarity to a selected documents d j respectively. To gain
high ranking score, a document should not only be relevant, but
also be dissimilar from the selected documents. The definition of
measures for relevance and document similarity is crucial, which
is done manually in this approach.
Recently, machine learning methods have been leveraged to learn score functions. Yue and Joachims [36] proposed SVM-DIV
which uses structural SVM to learn to identify a document subset with maximum word coverage. However, word coverage may
be different from intent coverage. Optimizing the former may not
necessarily lead to optimizing the latter. Similar to MMR, Zhu et

2.3

RNN with Attention Mechanism

RNN can capture the interdependency between elements in a sequence. Attention mechanism, which is usually built on RNN, mimics human attention behavior focusing on different local region
of the object (an image, a sentence, etc) at different times. In computer vision, Google DeepMind [24] used RNN with attention to extract information from an image by adaptively selecting a sequence
of the most informative regions instead of the whole image. In NLP,
attention mechanism is typically used in neural machine translation (NMT). Traditional encoder-decoder models encode the source

546

Session 5A: Retrieval Models and Ranking 3

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

sentence into a fixed-length vector from which the target sentence
is decoded. Such fixed-length vector may not be powerful enough
to reflect all the information of the source sentence. An attentionbased model [3] was proposed to automatically pay unequal and
varied attention to source words during decoding process. In particular, to decide the next target word, not only the fixed-length
vector, but also the hidden states corresponding to source words
relevant to the target word are used. Luong et al. [21] generalized
the idea and proposed two classes of attention mechanism, namely
global and local approaches. In this paper, attention mechanism is
used on subtopics, which guides the model to emphasize different
intents at different positions.
In the following section, we will first propose a general framework, then instantiate it with a specific implementation.

3

d1

d3

subtopic
attention
subtopics

d4

d5

i1

i2

diversity
scoring

candidate
documents

score

d6

query

relevance
scoring

q

Figure 1: Illustration of DSSA framework.

Table 3: Notations in DSSA.

Given a query set Q, a document set Dq and a subtopic set Iq for
each query q ∈ Q, the goal of explicit methods is to learn a ranking
function f (q, Dq , Iq ) which is expected to output a ranking of documents in Dq that is both relevant and diverse. The loss function
could be written in the following general form:
∑
L(f (q, Dq , Iq ), Yq ),
(2)

Notation Definition

q ∈Q

where L measures the quality gap between the ranking outputted
by f and the best ranking Yq . Different from traditional retrieval
tasks, diversity has to be considered in the ranking and evaluation
process. Theoretically, diversity ranking is NP-hard [1, 7]. Hence,
a common strategy is to make greedy selections [6, 27]: at the t-th
position, we assume that t − 1 documents have been selected and
formed a document sequence Ct −1 . The task is to select a locally optimal document dt from the remaining candidate documents based
on a score function S(q, dt , Ct −1 , Iq ). Note that implicit supervised
methods correspond to the case where Iq is an empty set.
To motivate our approach, we start with the ideas of the unsupervised explicit approaches, which can be formulated as the following general form:

r , dt

a ranking, the t-th document.

q, i k

the query, the k-th subtopic.

vd t

representation of the document at the t-th position.

vq

representation of the query.

vi k

representation of the k-th subtopic.

ht

hidden state of previous t documents.

at,k

attention on the k-th subtopic at the t-th position.
∑K
a
= 1, at,k ∈ [0, 1] where K is the number
k =1 t,k
of subtopics. A large value means that this subtopic
is less satisfied by previous t − 1 documents and thus
needs more attention at the t-th position.

sd t

the final score of the document at the t-th position.

don’t model the selected documents as a sequence. In addition, the
functions and parameters are heuristically defined, which may not
best fit the final goal.
To tackle the above problems, we extend Equation (3) to the
following general learning framework:

S unsupervised (q, dt , Ct −1 , Iq ) =
⇒ relevance
⇒ diversity

d2

document
sequence
representation

subtopic
attention
distribution

DOCUMENT SEQUENCE WITH SUBTOPIC
ATTENTION FRAMEWORK

(1 − λ)S rel (dt , q)+
∑
λ
S div (dt , i k ) A(Ct −1 , Iq )k ,
| {z }
i k ∈Iq

hidden
state

sequence of
selected
documents

(3)

S DSSA (q, dt , Ct −1 , Iq ) = sdt =
(1 − λ)S rel (vdt , vq )+
(
(
))
λS div vdt , vi (·) , A H ([vd1 , ..., vdt −1 ]), vi (·) ,
|
{z
}

subtopic weights

where i k ∈ Iq is the k-th subtopic of q and S rel and S div calculate
document dt ’s relevance to a query and to a subtopic respectively.
The essence of diversity lies in the function A which calculates the
weights for subtopics Iq based on previous document sequence
∏
Ct −1 . For xQuAD, A(Ct −1 , Iq )k = P(i k |q) d j ∈ Ct −1 (1 − P(d j |i k ))
where P(i k |q) is the initial importance of subtopic i k , P(d j |i k ) is
the probability that d j is relevant to i k . The weight of a subtopic
is determined by the likelihood that previous documents are not
relevant to this subtopic. PM2 mimics seats allocation of competing political parties to adjust subtopic weights after each selection,
i.e. A(Ct −1 , Iq ) is estimated according to the difference between
the subtopic’s distributions in Ct −1 and in Iq . All these methods

⇒ relevance
⇒ diversity

subtopic attention

(4)
where documents, queries, and subtopics are denoted by their representations, as explained in Table 3. In this paper, we focus
on learning a ranking function only and assume that these representations are given and will not be modified. There are three
main components, namely (1) document sequence representation component H , (2) subtopic attention component A, and
(3) scoring component S rel and S div , which are also illustrated
in Figure 1. This framework is inspired from the attention models

547

Session 5A: Retrieval Models and Ranking 3

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

used in image understanding [24] and neural machine translation
[3, 21], however adapted to our diversification task.
Next, we briefly describe the three components. The document
sequence representation component H encodes the information
contained in document sequence Ct −1 into a fixed-length hidden
state ht −1 , which could consider the interaction and dependency
among these documents. ht −1 could be viewed as a comprehensive
and high-level representation of Ct −1 . The subtopic attention at,(·)
is calculated by the subtopic attention component A using ht −1
and subtopic representations vi (·) . The attention evolves from the
first to the last ranking position, driving the model to emphasize
different subtopics based on previous document sequence. Finally,
the scoring components S rel and S div calculate relevance and diversity scores respectively. Notice that S div is not limited to be a
weighted sum over all subtopics as Equation (3). It can incorporate
more complex interaction among subtopics.
The essence of this framework can be summarized as follows.
Along with the selection of more documents, we encode the information of previous document sequence, and the attention mechanism will monitor the degree of satisfaction for each subtopic.
High scores are assigned to the documents relevant to less covered subtopics. Finally, multiple subtopics would be well covered
by adaptively learning the attention. In this way, our framework
builds an intuitive approach to explicitly model subtopics. We
name the framework Document Sequence with Subtopic Attention (DSSA). DSSA is a unified architecture that takes both relevance and diversity into consideration, and diversity is achieved
by modeling the interaction among documents and subtopics.

query/subtopic
representation

RNN
h1

ht −1

at ,(•) subtopic

attention

document
representation

e i2

max-pooling signals

eq
ed t −1

ed1

hidden state

e i1

edt

s ddivt
diversity
score

max-pooling

xd1 ,i1

xdt−1 ,i1

xd t ,i1

xd1 ,i2

xdt−1 ,i2

xd t ,i2

xd1 , q

xdt−1 ,q

xd t , q

s drelt
relevance
score

Figure 2: Architecture of DSSA-RNNMP. Previous t − 1 documents are encoded into ht −1 from distributed representations ed1 , ..., edt −1 . Attention on the k-th subtopic at,k is
then calculated based on (1) hidden state ht −1 and subtopic representation ei k (2) max-pooling on relevance features
xd1,i k , ..., xdt −1,i k .

Table 4: Parameters in DSSA-RNNMP.
Notation

Definition

RESULT DIVERSIFICATION USING DSSA

W n , bn

parameters of RNN with vanilla cell.

In this section, we instantiate DSSA to a concrete form and articulate the training and prediction algorithms. The main idea of DSSA
is to dynamically capture accumulative relevance information of
previous document sequence, so as to calculate subtopic attention.
Inspired by the recent progress on sequence data modeling, we
adapt RNN to capture the information of previous document sequence based on distributed representations of documents. However, the effectiveness of distributed representation heavily depends
on a large amount of training data. Typically, the representation is
built automatically using the data to optimize an objective function
[17]. We do not have such large data and we can only use unsupervised methods (e.g. doc2vec) to create representation, of which the
effectiveness could be suboptimal. Indeed, our preliminary experiments using only the distributed representation created by unsupervised methods yield low effectiveness. To compensate this weakness, we also use traditional relevance features such as BM25
score, which are proven useful, to calculate subtopic attention and
final score. Such a combination of distributed representations and
features has been used in several previous works [29, 33]. In addition to RNN, we also adopt the way using max-pooling [33], which
has been shown effective, to implement subtopic attention mechanism. We call this model DSSA-RNNMP (DSSA model using RNN
and Max-Pooling), as illustrated in Figure 2. In addition, we also
propose a list-pairwise approach for optimization, which is different from the existing studies.

W a , wp

parameters used in subtopic attention.

W s , wr

parameters used in scoring.

4

4.1 A Neural Network Implementation
We first describe the constitution of representations, namely vdt ,
vq , and vi k , then elaborate how we implement document sequence representation, subtopic attention, and scoring components. The parameters to be learned are listed in Table 4.
vdt : the representation of a document is composed of two parts:
distributed representations and relevance features. Distributed representation can be constructed in different ways. In this paper,
we consider three methods: SVD, LDA [4], and doc2vec [18]. Relevance features are those used in traditional IR, such as BM25 score
etc. Suppose that we have a distributed representation of size Ed ,
K subtopics, and R relevance features, the total size of vdt would
be Ed + R + KR. We use edt ∈ REd , xdt ,q and xdt ,i k ∈ RR to denote distributed representation, relevance features for a query and
a subtopic respectively.
vq , vi k : we first retrieve top Z documents using some basic retrieval model (such as BM25). These documents are concatenated
as a pseudo document, then similar to edt , a distributed representation of size Eq is generated. For consistency, we also use eq and
ei k ∈ REq to represent these representations.

548

Session 5A: Retrieval Models and Ranking 3

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

4.1.1 Document Sequence Representation. H is instantiated using
RNN to encode the information of previous document sequence.
Several types of RNN cell can be used, ranging from the simple vanilla cell, GRU cell [9], to LSTM cell [15]. For simplicity, we only
show the vanilla cell here. At the t-th position, we derive the (accumulative) document sequence representation as follows:
ht = tanh(W n [ht −1 ; edt ] + b n ),

(5)

We adopt an addictive way to integrate both parts and then use
softmax to produce (normalized) attention distribution:
′

′

′′

at,k = A (ht −1 , ei k ) + A (xd1,i k , ..., xdt −1,i k ),
′

w i k exp(at,k )
at,k = ∑K
′
j=1 w i j exp(a t, j )

4.1.3 Scoring. The final score consists of relevance score sdrel
t
and diversity score sddiv , which are combined by a coefficient λ:
t

sdt = (1 − λ)sdrel + λsddiv
t

t

′

|

(9)

|

sdrel = S (edt , eq ) + xd ,q · w r ,
t
t

 S ′ (ed , ei 1 ) + x | · w r 
t


d t ,i 1


|
..
,
sddiv = at,(·) · 

.
t
 ′
r 
S (edt , ei K ) + x |
·
w
d ,i


t

(10)

K

∈
and at,(·) is the attention derived from subtopic attention component. The diversity score is calculated as a weighted
combination of the document’s relevance to each subtopic by attention distribution. We use the same way to calculate document’s relevance to a query and to its subtopics using both distributional representations and relevance features, although different ways can
be used. Specifically, dt ’s relevance to a query q (or a subtopic i k )
is calculated based on both the similarity between two distributed
′
′
representations S (edt , eq ) (or S (edt , ei k )) and relevance features
′
xdt ,q (or xdt ,i k ). S intends to produce a matching score between
two representations and w r linearly combines features. Similar to
′
′
A , S could also be implemented as:
{ |
e W s ei k , (general)
′
S (edt , ei k ) = d|t
(11)
ed · e i k ,
(dot)

where w r

(6)

where W a ∈ RU ×Eq . The “general” operation uses bilinear tensor
product to relate two vectors multiplicatively through its nonlinearity [30]. The “dot” product requires both vectors to be in the same
space. Similar ht −1 and ei k mean that previous documents are likely to satisfy this subtopic, and thus a lower attention score will
be attributed to it. The above way mainly relies on distributed representations, which may not always be effective, especially under
limited data.
Hence, we further leverage relevance features to enhance the
subtopic attention. xdt ,i k directly reflects the degree of satisfaction
for a subtopic-document pair and is combined linearly using w p to
form an explicit signal. To derive the accumulative information of
the document sequence, we adopt commonly used max-pooling to
select the most significant signal from previous documents:
′′

(0 ≤ λ ≤ 1).

The relevance score and diversity score are calculated as follows:

4.1.2 Subtopic Attention. By looking at ht −1 which stores the
information of previous t − 1 documents and ei (·) which represents
the meaning of each subtopic, we are capable of discovering which
intents are not satisfied and thus need to be emphasized at the t′
th position. To capture this idea, we use A (ht −1 , ei k ) to measure
the (unnormalized) importance of the k-th subtopic at the t-th position, which could be implemented in many ways. We consider
the following two ways similar to [21]:
{
|
h W a ei k , (general)
A (ht −1 , ei k ) = t −1
|
−ht −1 · ei k , (dot)

(8)

softmax is modified to include the initial subtopic importance w i k ,
which encodes our intuition that an important subtopic is more
likely to gain attention than unimportant ones.

where W n ∈ RU ×(U +Ed ) (U is the size of the hidden state), b n ∈
RU and [; ] is a concatenation. The cell transforms previous hidden
layer ht −1 and current document distributed representation edt to
another space, where a bias b n is added and a non-linear activation
(i.e. tanh) then happens, producing the next hidden layer ht . h 0
is initialized as a vector of zeros. The vanilla cell can be easily
replaced by GRU and LSTM cells, whose results will be report in
Section 6.2.

′

(w i j ≥ 0, ∀j).

RR

t

REd ×Eq .

∈
Then the score of a ranking r is calculated
by summing up all the |r | documents’ scores:

where W s

sr =

|r |
∑
t =1

sd t .

′

|

A (xd1,i k , ..., xdt −1,i k ) = max([xd ,i · w p , ..., xd ,i · w p ]), (7)
1 k
t −1 k
′′

where A (xd1,i k , ..., xdt −1,i k ) measures the degree of satisfaction
of the k-th subtopic based on relevance features through max-pooling.
Lower value indicates that the previous documents are more likely
to be relevant to this subtopic. Note that if we view the signals produced by max-pooling (i.e. the vectors in “max-pooling” section of
Figure 2) as a part of the general hidden states, our concrete implementation fit in DSSA framework.

549

(12)
′

Vector interaction operations A and S could be implemented
using more complex models, such as multilayer perceptron (MLP),
to model the interaction between two vectors more accurately. We
could also use convolutional neural network (CNN) instead of RNN
to model the interaction among a sequence of documents and encode their information. We deliberately choose to use simple mechanisms in this implementation in order to show that the general
framework is capable of capturing the essence of diversification
even without complex operations. More complex implementations
will be examined in future work.

Session 5A: Retrieval Models and Ranking 3

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Algorithm 1 A List-pairwise Approach For Optimization

d1

d3

d1

d3

d2

d4

rank1

d4

d1

d2

d4

d3

rank2

1:

d2
2:

(a) list-pairwise

3:

(b) PAMM

4:

Figure 3: Pair sample examples of (a) list-pairwise and (b)
PAMM. Both samples are positive.

5:

4.2

7:

6:

A List-pairwise Approach for Optimization

Liu [19] classifies LTR approaches into three categories: pointwise,
pairwise, and listwise. Search result diversification is naturally a
listwise problem because the score of a document depends on the
previous documents. Take Table 1 as an example, under no previous documents, d 2 is better than d 3 because d 2 covers one more
subtopic (subtopics are of equal weight). However, given that we
have selected d 1 , which is similar to d 2 while dissimilar to d 3 , d 3
becomes superior because it provides additional information.
4.2.1 List-pairwise Training. We propose a list-pairwise training
approach. We call it list-pairwise because a sample in our algorithm consists of a pair of rankings (r 1 , r 2 ): r 1 and r 2 are totally
identical except the last document. The sample can be written
as (C, d 1 , d 2 ), where C is the shared previous document sequence.
The pairwise preference ground-truth is generated based on an evaluation metric M, such as α-nDCG. If M(r 1 ) > M(r 2 ), it is positive,
otherwise it is negative. Our approach is similar to pairwise approaches because it aims to compare a pair of documents, but this is
done within some context. Similarly to pairwise, the loss function
can be defined as binary classification logarithmic loss:

8:
9:
10:
11:
12:

procedure List-pairwise Training
input: loss function L, learning rate r , epochs V , query set Q,
document set D, evaluation metric M, random permutation
count N
output: DSSA with trained parameters θ
initialize θ
for i from 1 to V do
for batch b ∈ GetSamples(Q, D, M, N ) do
д ← GetGradient(L(b, θ ))
θ ← θ − rд
return DSSAθ
procedure GetSamples
input: query set Q, document set Dq for each query q, evaluation metric M, random permutation count N
output: a set of ranking pairs with weight and preference
(1) (1)
(2) (2)
{(q (1) , C (1) , d 1 , d 2 , w (1) , y (1) ), (q (2) , C (2) , d 1 , d 2 , w (2) , y (2) ), ...}
include: GetPerms(Dq , l, N , M) return a best ranking (under metric M) and N random permutations of length l.
GetPairs(q, Dq , C, M) samples pairs of documents (d 1 , d 2 )
from Dq \ C under context C if and only if they lead to different metric scores. Let r 1 ← [C, d 1 ], r 2 ← [C, d 2 ], w =
|M(r 1 ) − M(r 2 )| and y = JM(r 1 ) > M(r 2 )K.
R←∅
for query q in Q do
for l from 0 to |Dq | − 1 do
for perm C in GetPerms(Dq , l, N , M) do
R ← R ∪ GetPairs(q, Dq , C, M)
return R
L PAMM =

∑ ∑
q ∈Q

r q+,r q−

JP(rq+ ) − P(rq− ) ≤ M(rq+ ) − M(rq− )K,

(16)

where JconditionK is 1 if the condition is satisfied, 0 otherwise,
L list-pairwise =
MLE maximizes the probability of positive rankings, and PAMM
(
(
)
(
))
∑ ∑
enlarges the probability margin between positive and negative ran(o) (o)
(o) (o)
(o) (o)
(o)
w
y log P(r 1 , r 2 ) + (1 − y ) log 1 − P(r 1 , r 2 ) ,
kings according to an evaluation metric. For MLE, the number of
q ∈Q o ∈ Oq
best rankings is usually small if we only have hundreds of que(13)
ries, which may not be enough to train adequately the parameters.
(o)
where Oq is all the pair samples of query q, y = 1 indicates posiPAMM uses preferences between very different rankings that are
(o) (o)
tive and 0 for negative, and P(r 1 , r 2 ) is the probability of being
not comparable (see Figure 3(b)). In contrast, list-pairwise method
positive calculated by 1+exp(s 1 −s ) . To enhance effectiveness,
only allows the last document to be different (Figure 3(a)). This cor(o)
(o)
r2
r1
responds better to the decision-making situation in which we have
(o)
(o)
we weight pairs with w (o) = |M(r 1 ) − M(r 2 )|, which means that
to choose a document under a given context. It is expected that
the bigger the metric score gap, the more important the pair.
such a pair sample allows us to better train the ranking function.
Because DSSA calculates document d’s score sdC based on previExperiments will show that our approach works better.
ous document C, we could also use Maximum Likelihood EstimaAs shown in Figure 2, our architecture is a unified neural nettion (MLE) or PAMM to optimize our model. We use Plackett-Luce
work and the attention function is continuous, so the gradient of
model [22] to estimate the probability of a ranking r :
the loss function can be backpropagated directly to train the model.
We
use mini-batch gradient descent to facilitate training process.
r
[:i−1]
|r |
exp(sd
)
∏
i
Unfortunately,
it is impossible to acquire all the list-pairwise
,
(14)
P(r ) =
∑ |r |
r [:i−1]
samples, which has in total |Dq |! (|Dq | is the number of candidate
)
i=1 j=i exp(sd
j
documents) different permutations. So we develop a sampling strawhere r [: i − 1] means the top i − 1 documents of ranking r . Then
tegy similar to negative sampling [23] as described in Algorithm 1:
the loss functions could be written as:
for each query q, we sample a large number of pairs of rankings,
∑
whose length ranges from 1 to |Dq |. We first obtain some conL MLE =
− log(P(rq+ )),
(15)
texts C from both best rankings and randomly sampled negative
q ∈Q

550

Session 5A: Retrieval Models and Ranking 3

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

rankings (rankings that are not optimal). Then under each C, a pair
of documents (d 1 , d 2 ) are sampled from the remaining documents
Dq \ C if and only if they lead to different metric scores.

Table 5: Relevance features. Each of the first 3 features is applied to body, anchor, title, URL, and the whole documents.

4.2.2 Prediction. In prediction stage, for each query, we sequentially and greedily choose the document with the highest score and
append it to the ranking list. Specifically, the first document is
selected under initial subtopic importance from the whole candidate set Dq . Once the top t − 1 documents have been selected (i.e.
|C| = t − 1), we feed each document in Dq \ C into DSSA at the
t-th position one by one and choose the one with the highest sdt .
This process continues until all the documents in Dq are ranked.
4.2.3 Time Complexities. The training time complexity with vanilla cell and “general” operation is O(V · |Q| · Γ · |Dq | · Θ) where
V is the number of iterations, |Q| is the number of training queries, Γ = N · |Dq | 2 is the number of sampled pairs where N is the
number of random permutations, |Dq | is the number of candidate
documents, and Θ is the complexity for each position:
Θ=

U (U + Ed )
| {z }
document sequence
representation

+ KU Eq + KR + KEd Eq + KR ,
| {z } | {z }
subtopic
attention

Description

#Features

TF-IDF
BM25
LMIR

the TF-IDF model
BM25 with default parameters
LMIR with Dirichlet smoothing

5
5
5

PageRank
#inlinks
#outlinks

PageRank score
number of inlinks
number of outlinks

1
1
1

Table 6: Diversity features. Each feature is extracted over a
pair of documents.

(17)

scoring

where the dominating terms are KU Eq and KEd Eq which are proportional to the number of subtopics K. How to efficiently handle
a large number of subtopics is our future work. The prediction
complexity is O(|Dq | 2 Θ) for each query. We can limit |Dq | to a
small number (say 50), so the prediction time can be reasonable.

Name

Description

subtopic diversity
text diversity
title diversity
anchor text diversity
link-based diversity
URL-based diversity

euclidean distance based on SVD
cosine-based distance on term vector
text diversity on title
text diversity on anchor
link similarity of document pair
URL similarity of document pair

5.3 Baseline Models
We compare DSSA2 to various unsupervised and supervised diversification methods. The non-diversified baseline is denoted as Lemur. We use xQuAD [27], PM2 [13], TxQuAD, TPM2 [12], HxQuAD, and HPM2 [16] as our unsupervised baselines. We use
ListMLE [31], R-LTR [39], PAMM [32], and NTN [33] as our
supervised baselines. Top 20 results of Lemur are used to train supervised methods. Top 50 (i.e. |Dq |) results of Lemur are used for
diversity re-ranking. To construct the representation of a query or
a subtopic, we use the top 20 (Z ) documents. We use 5-fold cross
validation to tune the parameters in all experiments based on αnDCG@20, which is one of the most widely used metrics. A brief
introduction to these baselines is as follows:
Lemur. We use the same non-diversified results as [16] for fair
comparison. They are produced by language model and retrieved
using the Lemur service3 of which the spams are filtered. These
results are released by Hu et al. [16] on the website1 .
ListMLE. ListMLE is a representative listwise LTR method without considering diversity.
xQuAD, PM2, TxQuAD, TPM2, HxQuAD, and HPM2. These
are competitive unsupervised explicit diversification methods, as
introduced in Section 2.2. All these methods use λ to control the
importance of relevance and diversity. HxQuAD and HPM2 use an
additional parameter α to control the weight of each layer of the
hierarchical structure. Both λ and α are tuned using cross validation. They all require a prior relevance function to fulfill diversification re-ranking. Following [39], we use ListMLE.
R-LTR, PAMM, and NTN. For PAMM, we use α-nDCG@20 as
the optimization metric. We optimize NTN based on both R-LTR
and PAMM, denoted as R-LTR-NTN and PAMM-NTN respectively.

5 EXPERIMENTAL SETTINGS
5.1 Data Collections
We use the same dataset as [16] which consists of Web Track dataset from TREC 2009 to 2012. There are 198 queries (query #95
and #100 are dropped because no diversity judgments are made
for them), each of which includes 3 to 8 subtopics identified by
TREC assessors. The relevance rating is given in a binary form at
subtopic level. All experiments are conducted on ClueWeb09 [5]
collection.
We use query suggestions of Google search engine as subtopics,
which are released by Hu et al. [16] on their website1 . For DSSA,
we only use the first level subtopics and leave the exploration of
hierarchical subtopics to future work. Following the existing work
[16], we simply use uniform weights for these subtopics.

5.2

Name

Evaluation Metrics

We use ERR-IA [8], α-nDCG [10], and NRBP [11], which are official diversity evaluation metrics used in Web Track. They measure
the diversity by explicitly rewarding novelty and penalizing redundancy. D♯-measures [26], the primary metric used in NTCIR Intent
[25] and IMine task [20], is also included. In addition, we also use
traditional diversity measures Precision-IA (denoted as Pre-IA) [1]
and Subtopic Recall (denoted as S-rec) [37]. Consistent with existing works [32, 33, 39] and TREC Web Track, all these metrics are
computed on top 20 results of a ranking. We use two-tailed paired
t-test to conduct significance testing with p-value < 0.05.

2 data
1 hierarchical

and code available at: http://www.playbigdata.com/dou/DSSA/
service: http://boston.lti.cs.cmu.edu/Services/clueweb09 batch/

3 Lemur

search result diversification: http://www.playbigdata.com/dou/hdiv

551

Session 5A: Retrieval Models and Ranking 3

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

To achieve optimal results, for R-LTR and PAMM, we tune the
relational function h S (R) from minimal, maximal, and average. For
PAMM, we tune the number of positive rankings τ + and negative
rankings τ − per query. For NTN, the number of tensor slices is
tuned from 1 to 10. LDA is used to generate distributed representations of size 100 for NTN and DSSA. For all these supervised
methods, the learning rate r is tuned from 10−7 to 10−1 . For DSSA,
we have different settings possible. In our first set of results, we
will use “general” as the implementation of vector interaction ope′
′
rations A and S , LSTM with hidden size of 50 as the cell of RNN.
We set random permutation count as 10 in list-pairwise sampling.
Similarly, λ of DSSA is tuned by cross validation. We also test the
impact of different model settings and permutation counts on performance in Section 6.2 and Section 6.3 respectively.
Similar to [39], we implement 18 relevance features and 6 diversity features, as listed in Table 5 and 6 respectively. We collect the
candidate and retrieved documents of all queries and subtopics to
generate the distributed representations.

Table 7: Performance comparison of all methods. The best
result is in bold. Statistically significant differences between
DSSA and baselines are marked with various symbols. H indicates significant improvement over all baselines.
ERR-IA α-nDCG NRBP D♯-nDCG Pre-IA S-rec

LemurÀ
ListMLE¶

.271
.287

.369
.387

.232
.249

.424
.430

.153
.157

.621
.619

xQuADÁ
TxQuADÂ
HxQuADÃ
PM2Ä
TPM2Å
HPM2Æ

.317
.308
.326
.306
.291
.317

.413
.410
.421
.411
.399
.420

.284
.272
.294
.267
.250
.279

.437
.441
.441
.450
.443
.455

.161
.155
.158
.169
.161
.172

.622
.634
.629
.643
.639
.645

.403
.411
.415
.417
.456H

.267 .441
.271 .450
.275 .451
.272 .457
.326H .473H

.164
.168
.166
.170
.185H

.631
.643
.644
.648
.649ÀÁ
¶

R-LTR·
.303
PAMM¸
.309
R-LTR-NTN¹ .312
PAMM-NTNº .311
DSSA
.356H

6 EXPERIMENTAL RESULTS
6.1 Overall Results
The overall results are shown in Table 7. We find that DSSA significantly outperforms all implicit and explicit baselines, including
both unsupervised and supervised. The improvements are statistically significant (two-tailed paired t-test) for all metrics, except
S-rec. The results clearly show the superiority of DSSA.
(1) DSSA vs. unsupervised explicit methods. DSSA outperforms unsupervised explicit methods (xQuAD, PM2, TxQuAD,
TPM2, HxQuAD, and HPM2) on all the measures. The relative
improvement over HxQuAD and HPM2, the best unsupervised explicit approaches, is up to 8.3% and 8.6% respectively in terms of
α-nDCG. This comparison shows the great advantage of using supervised method for learning the ranking function.
(2) DSSA vs. supervised implicit methods. DSSA also outperforms supervised implicit methods (R-LTR, PAMM, R-LTRNTN, and PAMM-NTN) by quite large margins. The improvement over R-LTR-NTN and PAMM-NTN, the best supervised implicit approaches is up to 9.9% and 9.4% respectively on α-nDCG.
This result demonstrates the utility of taking into account subtopics explicitly in supervised approaches. The improvements are similar to those observed between explicit approaches and implicit
approaches in unsupervised framework [12, 13, 16, 27]. The combination of the two observations suggests that explicit modeling
of subtopics can improve result diversification, whether it is in a
supervised or unsupervised framework.

6.2

Methods

Effects of Different Settings

We conduct experiments with different settings of DSSA to investigate whether the performance is sensitive to these settings. Different aspects of settings are listed follow. For simplicity, when
investigating the impact of each aspect, we keep other aspects the
same as the settings specified in Section 5.3.
(1) Representation generation methods: SVD, LDA, and doc2vec
with window size of 5.
′
(2) Implementation of vector interaction operations A and
′
S : “general” and “dot”.

552

Table 8: Effects of different settings.
Methods

ERR-IA α-nDCG NRBP D♯-nDCG Pre-IA S-rec

SVD
LDA
doc2vec

.348
.356
.351

.450
.456
.452

.315
.326
.318

.470
.473
.471

.184 .646
.185 .649
.184 .646

general
dot

.356
.347

.456
.450

.326
.314

.473
.470

.185 .649
.184 .647

vanilla
GRU
LSTM

.354
.357
.356

.454
.457
.456

.322
.326
.326

.471
.473
.473

.184 .649
.185 .649
.185 .649

DSSA-RNN
DSSA-RNNMP

.342
.356

.445
.456

.306
.326

.466
.473

.172 .657
.185 .649

(3) RNN cell: vanilla, GRU, and LSTM cell.
(4) Dimensionality: we test several representative settings on
the size of distributed representations Ed and Eq , the size
of hidden state U as (25, 10), (50, 25), (100, 50), (200, 100).
(5) Max-pooling: we experiment without using max-pooling
(denoted as DSSA-RNN) in subtopic attention component.
The results are reported in Table 8. We can observe that DSSA
does not heavily rely on specific settings. As for different representation generation methods, LDA has slightly better results. doc2vec
could have been more appropriate if we had large datasets with
more queries. The “general” operation yields slightly better results.
A possible reason is that it is bilinear and thus is more powerful
than “dot” to model the interaction. GRU and LSTM cells yield
slightly better results than vanilla cell because of their ability of
modeling long-term dependency. The difference is however small.
This may be due to that with a limited number of training data,
a model is unable to take advantage of its higher complexity to
capture the fine-grained subtlety. Results with different size of distributed representation and hidden state shown in Figure 4(a) also

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

0.47

0.47

DSSA

PAMM-NTN

DSSA

PAMM-NTN

0.46

0.46

i1 i2 i3 i4 i5

i1 i2 i3 i4 i5

i1 i2 i3 i4

i1 i2 i3 i4

α-nDCG

α-nDCG

Session 5A: Retrieval Models and Ranking 3

0.45

d1
d2
d3
d4
d5

0.45

0.44

0.44

0.43
(25,10) (50,25) (100,50) (200,100)

0.43
0

dimensionality

5

10

15

20

#random permutation

(a) α -nDCG w.r.t. different size

(a) ranking of query #58

(b) α -nDCG w.r.t. different random
permutation count

Figure 5: Case study for DSSA and PAMM-NTN. White means relevant and black means irrelevant.

Figure 4: Performance tendency of different settings.
Table 9: Effects of different optimization methods.
Methods
MLE
PAMM
list-pairwise

d1

.446
.445
.456

.315
.315
.326

.462
.463
.473

.176
.175
.185

d4

d5

z3. quit smoking calculator (i1)
subtopics
z
from Google 4. quit smoking help (i1)
z5. quit smoking benefits (i2)
z6. quit smoking cold turkey (i3)

official
subtopics

z7. quit smoking hypnosis (i4)
i1. What are the ways you
can quit smoking?
i2. What are the benefits of
quitting smoking?
i3. Can you quit smoking using
the cold turkey method?
i4. How can hypnosis help
someone quit smoking?

Figure 6: Subtopic attention variation of query #182. The top
part is attention and the bottom part is relevance judgment.

Effects of Different Optimization Methods

Results in Table 9 shows that list-pairwise is more effective than
MLE and PAMM. This confirms our earlier intuition that list-pairwise
optimization corresponds better to the situation of diversification
ranking than the two other methods. Note that even using MLE
or PAMM as optimization methods, DSSA could also achieve stateof-the-art performances, which confirms the effectiveness of our
explicit learning framework from another perspective.
We vary the number of random permutations used in list-pairwise
sampling from 0 to 20 to investigate its effect. As depicted in Figure 4(b), the performance does not heavily rely it. The best performance is achieved around 10. More permutations lead to lower
effectiveness, which could be explained by model overfitting.

6.4

d3

z2. quit smoking app (i1)

.644
.644
.649

indicate no strong correlation between performance and settings.
α-nDCG remains above 0.45 using different sizes. The best performance is achieved using 100-dimensional representation and 50dimensional hidden state. This suggests that high dimensionality
may result in overfitting. Without using max-pooling, α-nDCG
drops to 0.445, which demonstrates the usefulness of using maxpooling to enhance subtopic attention. The small differences between different settings suggest that DSSA is a stable and robust
framework. Note that we use both distributed representations and
relevance features, which are complementary to each other. This
may be one of the reasons of the stability.

6.3

d2

z1. quit smoking tips (i1)

ERR-IA α-nDCG NRBP D♯-nDCG Pre-IA S-rec
.349
.348
.356

(b) ranking of query #182

Visualization and Discussion

We visualize the ranking results of DSSA and the variation of subtopic attention to better understand why DSSA performs well.
We show the top 5 ranking results of query #58 and #182 in Figure 5 to illustrate why DSSA outperforms implicit learning methods. We choose PAMM-NTN as comparison method, which is the
best existing learning method. In Figure 5, white means relevant
and black means irrelevant. For query #58, DSSA ranks a document relevant to subtopics i 3 and i 4 first and a document relevant
to i 1 and i 2 at the second position, while the first two documents
of PAMM-NTN cover the same subtopics. Note that there is no

553

document covering i 5 in the candidate set. For query #182, DSSA
successively chooses documents that cover i 1 , i 3 , i 2 , and i 4 . One additional intent is satisfied at every position. PAMM-NTN, however,
just covers i 1 and i 2 by top 5 documents, which is obviously not
optimal. We see that the unequal and varied subtopic attention is
capable of discovering unsatisfied subtopics at different positions,
which eventually leads to more subtopic coverage.
To study attention mechanism, we further visualize the variation of subtopic attention of top 5 documents of query #182, namely “quit smoking”, which has 4 official subtopics (i 1 to i 4 ), as
shown in Figure 6. The top part is subtopic attention variation and
the bottom part is relevance judgment. For attention part, the darker the cell is, the lower the attention (weight) on this subtopic is.
Note that we actually leverage query suggestions of Google (z 1 to
z 7 ) to serve as subtopics, which do not match official ones exactly.
We manually align subtopics mined from Google to official ones.
At the beginning, all the subtopics have equal attention. The first
selected document d 1 is relevant to i 1 , i.e. to the Goggle subtopics
z 1 , z 2 , z 3 and z 4 . We see that the attention to these latter decreases
at second position. Then the document d 2 is selected, which is relevant to uncovered i 3 . We see that the attention to the corresponding z 6 begins to diminish from the third position. d 3 and d 4 satisfy additional i 2 and i 4 respectively, which leads to the reduction

Session 5A: Retrieval Models and Ranking 3

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

of attention on z 5 and z 7 at the following position. The subtopic
attention, initialized as uniform distribution, ends up with more
emphasis on z 4 , z 6 , and z 7 . This example illustrates how the unequal and varied attention drives the model to emphasize different
subtopics at different positions, which is crucial in explicit diversification. This example also shows a potential problem inherent
for any method using automatically discovered subtopics: those
topics may be different from the ones defined by human assessors.
Equal distribution is assumed on all the subtopics zi . However,
this implies an unequal distribution among the manually defined
subtopics (more emphasis is put on i 1 ). Assuming an equal distribution at the beginning may not necessarily be the best approach.
We will deal with this problem in our future work.

7

[6] Jaime Carbonell and Jade Goldstein. 1998. The Use of MMR, Diversity-based
Reranking for Reordering Documents and Producing Summaries. In SIGIR.
[7] Ben Carterette. 2009. An Analysis of NP-Completeness in Novelty and Diversity
Ranking. In ICTIR.
[8] Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. 2009. Expected Reciprocal Rank for Graded Relevance. In CIKM.
[9] Junyoung Chung, Çaglar Gülçehre, KyungHyun Cho, and Yoshua Bengio. 2014.
Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. CoRR abs/1412.3555 (2014).
[10] Charles L.A. Clarke, Maheedhar Kolla, Gordon V. Cormack, Olga Vechtomova,
Azin Ashkan, Stefan Büttcher, and Ian MacKinnon. 2008. Novelty and Diversity
in Information Retrieval Evaluation. In SIGIR.
[11] Charles L. A. Clarke, Maheedhar Kolla, and Olga Vechtomova. 2009. An Effectiveness Measure for Ambiguous and Underspecified Queries. In ICTIR.
[12] Van Dang and Bruce W. Croft. 2013. Term Level Search Result Diversification.
In SIGIR.
[13] Van Dang and W. Bruce Croft. 2012. Diversity by Proportionality: An Electionbased Approach to Search Result Diversification. In SIGIR.
[14] Amac Herdagdelen, Massimiliano Ciaramita, Daniel Mahler, Maria Holmqvist,
Keith Hall, Stefan Riezler, and Enrique Alfonseca. 2010. Generalized Syntactic
and Semantic Models of Query Reformulation. In SIGIR.
[15] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation 9, 8 (1997).
[16] Sha Hu, Zhicheng Dou, Xiaojie Wang, Tetsuya Sakai, and Ji-Rong Wen. 2015.
Search Result Diversification Based on Hierarchical Intents. In CIKM.
[17] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry P.
Heck. 2013. Learning deep structured semantic models for web search using
clickthrough data. In CIKM.
[18] Quoc V. Le and Tomas Mikolov. 2014. Distributed Representations of Sentences
and Documents. In ICML.
[19] Tie-Yan Liu. 2009. Learning to rank for information retrieval. Foundations and
Trends in Information Retrieval 3, 3 (2009).
[20] Yiqun Liu, Ruihua Song, Min Zhang, Zhicheng Dou, Takehiro Yamamoto, Makoto P Kato, Hiroaki Ohshima, and Ke Zhou. 2014. Overview of the NTCIR-11
IMine Task.. In NTCIR. Citeseer.
[21] Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective Approaches to Attention-based Neural Machine Translation. In EMNLP.
[22] John I Marden. 1996. Analyzing and modeling rank data. CRC Press.
[23] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean.
2013. Distributed Representations of Words and Phrases and their Compositionality. In NIPS.
[24] Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. 2014.
Recurrent Models of Visual Attention. In NIPS.
[25] Tetsuya Sakai, Zhicheng Dou, Takehiro Yamamoto, Yiqun Liu, Min Zhang, Ruihua Song, MP Kato, and M Iwata. 2013. Overview of the NTCIR-10 INTENT-2
Task.. In NTCIR.
[26] Tetsuya Sakai and Ruihua Song. 2011. Evaluating Diversified Search Results
Using Per-intent Graded Relevance. In SIGIR.
[27] Rodrygo L.T. Santos, Craig Macdonald, and Iadh Ounis. 2010. Exploiting Query
Reformulations for Web Search Result Diversification. In WWW.
[28] Rodrygo L.T. Santos, Craig Macdonald, Iadh Ounis, and others. 2015. Search
result diversification. Foundations and Trends® in Information Retrieval (2015).
[29] Aliaksei Severyn and Alessandro Moschitti. 2015. Learning to Rank Short Text
Pairs with Convolutional Deep Neural Networks. In SIGIR.
[30] Richard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng. 2013.
Reasoning With Neural Tensor Networks for Knowledge Base Completion. In
NIPS.
[31] Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. 2008. Listwise
Approach to Learning to Rank: Theory and Algorithm. In ICML.
[32] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2015. Learning
Maximal Marginal Relevance Model via Directly Optimizing Diversity Evaluation Measures. In SIGIR.
[33] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2016. Modeling
Document Novelty with Neural Tensor Network for Search Result Diversification. In SIGIR.
[34] Jeonghee Yi and Farzin Maghoul. 2009. Query Clustering Using Click-through
Graph. In WWW.
[35] Hai-Tao Yu and Fuji Ren. 2014. Search Result Diversification via Filling Up
Multiple Knapsacks. In CIKM.
[36] Yisong Yue and Thorsten Joachims. 2008. Predicting Diverse Subsets Using
Structural SVMs. In ICML.
[37] Cheng Xiang Zhai, William W. Cohen, and John Lafferty. 2003. Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval. In
SIGIR.
[38] Zhiyong Zhang and Olfa Nasraoui. 2006. Mining Search Engine Query Logs for
Query Recommendation. In WWW.
[39] Yadong Zhu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng, and Shuzi Niu. 2014. Learning for Search Result Diversification. In SIGIR.

CONCLUSIONS

In this paper, we propose a general learning framework DSSA to
model subtopics explicitly for search result diversification. Based
on the sequence of selected documents, unequal and varied subtopic attention is calculated, driving the model to emphasize different
subtopics at different positions. This is the first time that attention
mechanism is used to model the process. We further instantiate
DSSA using RNN and max-pooling to handle both distributed representations and relevance features, which outperforms significantly the existing approaches. The results confirm that modeling
subtopics explicitly in a learning framework is beneficial and effective and this also avoids heuristically defined functions and parameters. However, accurately modeling the interaction among
documents and subtopics is still challenging. There are many other
more complex implementations besides our particular way, which
will be investigated in future work. The proposed model contains
a number of parameters to be learned. This requires a large number of training data. Collecting more training data to fully unlock
the potential of the model is another direction. Finally, this work
only deals with the learning of a ranking function, assuming that
document and query representations have already been created.
In practice, learning these representation is another interesting aspect, which could be incorporated into our framework, provided
with sufficient training data.

ACKNOWLEDGMENTS
Zhicheng Dou is the corresponding author. This work was funded
by the National Natural Science Foundation of China under Grant
No. 61502501 and 61502502, the National Key Basic Research Program (973 Program) of China under Grant No. 2014CB340403, and
the Beijing Natural Science Foundation under Grant No. 4162032.

REFERENCES
[1] Rakesh Agrawal, Sreenivas Gollapudi, Alan Halverson, and Samuel Ieong. 2009.
Diversifying Search Results. In WSDM.
[2] Ricardo A. Baeza-Yates, Carlos A. Hurtado, and Marcelo Mendoza. 2004. Query
Recommendation Using Query Logs in Search Engines. In Current Trends in
Database Technology EDBT 2004 Workshops.
[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural Machine
Translation by Jointly Learning to Align and Translate. CoRR abs/1409.0473
(2014).
[4] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet
Allocation. Journal of Machine Learning Research 3 (2003).
[5] Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. 2009. Clueweb09 data set.
http://boston.lti.cs.cmu.edu/Data/clueweb09/. (2009).

554

