Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

An Extended Relevance Model for Session Search
Nir Levine∗

Haggai Roitman

Doron Cohen

Technion - Israel Institute of
Technology
Haifa, Israel 32000
levin.nir1@gmail.com

IBM Research - Haifa
Haifa, Israel 31905
haggai@il.ibm.com

IBM Research - Haifa
Haifa, Israel 31905
doronc@il.ibm.com

ABSTRACT
The session search task aims at best serving the user’s information need given her previous search behavior during the session.
We propose an extended relevance model that captures the user’s
dynamic information need in the session. Our relevance modelling
approach is directly driven by the user’s query reformulation (change)
decisions and the estimate of how much the user’s search behavior
affects such decisions. Overall, we demonstrate that, the proposed
approach significantly boosts session search performance.

1

INTRODUCTION

We propose an extended relevance model for session search. Relevance models aim at identifying terms (words, concepts, etc) that
are relevant to a given (user’s) information need [5]. Within a session, user’s information need, expressed as a sequence of one or
more queries [1], may evolve over time. User’s search behavior
during the session may be utilized as an additional relevance feedback source by the underlying search system [1]. Given user’s session history (i.e., previous queries, result impressions and clicks),
the goal of the session search task is to best serve the user’s newly
submitted query in the session [1].
We derive a relevance model that aims at “tracking” the user’s
dynamic information need by observing the user’s search behavior so far during the session. To this end, the proposed relevance
model is driven by the user’s query reformulation decisions. Our
relevance modelling approach relies on previous studies that suggest that user query change decisions may (at least partially) be
explained by the previous user search behavior in the session [4,
9, 12]. We utilize the derived relevance model for re-ranking the
search results that are retrieved for the current user information
need in the session. Overall, we demonstrate that, our relevance
modeling approach can significantly boost session search performance compared to many other alternatives that also utilize session data.

2

3 APPROACH
3.1 Session model
Session search is a multi-step process, where at each step t, the
user may submit a new query qt . The search system then retrieves
[k]
the top-k documents Dqt from a given corpus D that best match
1
the user’s query . The user may then examine the results list; each
result usually includes a link to the actual content and is accompanied with a summary snippet. The user may also decide to click
on one or more of the results in the list in order to examine their
actual content. Let Cqt denote the corresponding set of clicked re[k]

sults in Dqt . In case the user decides to continue and submits a
subsequent query, step t ends and a new step t + 1 begins. Let Sn−1
represent the session history (i.e., user queries, retrieved result documents, and clicked results) that was “recorded” prior to the current (latest) submitted user query qn . On each step 1 ≤ t ≤ n − 1,
the session history is represented by a tuple S t = hQt , Dt , Ct i.
Qt = (q 1 , q 2 , . . . , qt ) is the sequence of queries submitted by the
[k] [k]
[k]
user up to step t. Dt = (Dq1 , Dq2 , . . . , Dqt ) is the corresponding
sequence of (top-k) retrieved result lists. Ct = (Cq1 , Cq2 , . . . , Cqt )
further represents the corresponding sequence of user clicks.

RELATED WORK

Few previous works have also utilized the session context (i.e., previous queries, retrieved results and clicks) as an implicit feedback
∗ Work

source for refining the user’s query [3, 8, 10, 11]. To this end,
the query language model was either combined with the language
models of previous queries [11] or retrieved (clicked) results [8, 10].
In addition, different query score aggregation strategies for session
search were explored [3]. Yet, none of these previous works have
actually considered the user’s query change process itself as a possible implicit feedback source.
Several recent works have studied various query reformulation
(change) behaviors during search sessions [4, 9, 12]. Among the
various features that were studied, word-level features were found
to best explain the changes in user queries during search sessions [4,
9]. A notable feature was found to be the occurrence of query
(changed) words in the contents of results that the user previously
viewed or clicked [4, 9, 12].
Few previous works have also utilized query change for the session search task (e.g., [6, 12]). Common to such works is the modeling of user queries and their change as states and actions within
various Reinforcement Learning inspired query weighting and aggregation schemes [7]; In this work we take a rather more “traditional” approach, inspired by the relevance model framework [5].

was done during a summer internship in IBM Research - Haifa.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’17, August 7–11, 2017, Shinjuku, Tokyo, Japan.
© 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00
DOI: http://dx.doi.org/10.1145/3077136.3080664

1 With

k = 10 in the TREC session track benchmarks [1] that we use later
on in our evaluation.

865

Short Research Paper

3.2

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Information need dynamics

γt further controls the relative importance we assign to model
exploitation (i.e., θ St −1 ) versus model exploration (i.e., θ Ft ). γt parameter is dynamically determined based on the relevance model’s
self-clarity at step t [2]. Self-clarity estimates how much the prior
model θ St −1 already “covers” the feedback model θ Ft ; formally:

The session search task is to best answer the current user’s query
qn while considering Sn−1 [1]. Let I denote the user’s (hidden)
information need in the session. The goal of our relevance modelling approach is, therefore, to better capture the user’s information need I which may evolve during the session. In order to capture such dynamics, let It further represent the user’s information
need at step t. We now assume that, It depends both on the previous (dynamic) information need It −1 prior to query qt submission

def

γt = γ · exp−D K L (θ Ft kθ St −1 ) ,

where γ ∈ [0, 1] and D K L (θ Ft k θ St −1 ) is the Kullback-Leibler
divergence between the two (un-smoothed) language models [13].
Finally, given qn , the current user’s query in the session, we derive the relevance model θ Sn by inductively applying Eq. 1 (with

def

and the possible change in such need ∆It = It −1 → It ; ∆It is
assumed be to implied by the change the user has made to her previous query qt −1 to obtain query qt .

3.3

def

θ S0 = 0). We next derive the feedback model θ Ft .

3.5

Query change as relevance feedback

1≤j ≤t

define a pseudo information need Qt′ . Qt′ represents a (crude) estimate of the user’s (dynamic) information need up to step t and
is obtained by concatenating the text of all observed queries in Qt
(with each query having the same importance, following [11]). We
Ð
then define Ft as the set of top-m results in
Dq j with the
1≤j ≤t

highest query-likelihood given Qt′ (representing pseudo-clicks). Let
def t f (w,x )+µ

Similar to previous works on relevance models [5], our goal is to
discover those terms w (∈ V ) that are the most relevant to the
user’s information need In ; To this end, given the user’s current
query qn and session history Sn−1 , let θ Sn denote our estimate of
the relevance (language) model. On each step 1 ≤ t ≤ n, such estimation is given by the following first-order autoregressive model:
def

t f (w,D)

|D|
p [µ] (w |θ x ) =
now denote the Dirichlet smoothed
|x |+µ
language model of text x with parameter µ [13]. Inspired by the
RM1 relevance model [5], we estimate θ Ft as follows:
def Õ [0]
©Õ
ª
p(w |θ Ft ) =
p (w |θd ) · ­ p(d |θ ∆qt )p(∆qt )® , (3)
d ∈F t
«∆qt
¬

Relevance model derivation

p(w |θ St ) = γt p(w |θ St −1 ) + (1 − γt )p(w |θ Ft ),

Feedback model derivation

Our estimate of θ Ft aims at discovering those terms (in qt , qt −1 or
others in V ) that are most relevant to the change in user’s dynamic
information need from It −1 to It (i.e., ∆It ). Given queries qt and
qt −1 , we first classify their occurring terms w ′ according to their
role in the query change. Let ∆qt further denote the set of terms
w ′ that are classified to the same type of query change (i.e., ∆qt ∈
{∆qt ↔ , ∆qt + , ∆qt − }).
Our relevance model now relies on the fact that query changed
terms may also occur within the contents of results that were previously viewed (or clicked) by the user [4, 9, 12]. Therefore, on
each step t, let Ft denote the set of results that are used for (implicit) relevance feedback. We determine the set of results to be
included in Ft as follows. If up to step t < n there is at least one
Ð
Cq j . Otherwise, we first
clicked result, then we assign Ft =

We utilize the user’s query reformulation (change) process during
the session as an implicit relevance feedback for estimating the
change in the user’s information need ∆It . As been suggested by
previous works [4, 9, 12], user’s query changed terms may actually
occur in the contents of previously viewed (clicked) search results
in S t −1 . This, therefore, may (partially) explain how the user decided to reformulate her query from qt −1 to qt [4, 9, 12]. Our proposed relevance model aims at exploiting such query changed term
occurrences within the contents of previously viewed (clicked) results so as to discover those terms w (over some vocabulary V ) that
are the most relevant to the current user’s information need In . As
a consequence, such terms may be used for query expansion aiming
to better serve the current user’s information need In .
Given query qt , compared to the previous query qt −1 , there can
be three main query change types, namely term retention, addition
and removal [4, 9, 12]. User term retention, given by the set of
terms that appear in both query qt and qt −1 and denoted ∆qt↔ ,
usually represent the (general) thematic aspects of the user’s information need [4, 9, 12]. Added terms (denoted ∆qt+ ) are those terms
that the user added to query qt −1 to obtain query qt . A user may
add new related terms that were encountered in previous results
so as to improve the chance of finding relevant content [4]. On the
other hand, a user may remove terms from a previous query qt −1
(further denoted ∆qt− ) in order to terminate a subtask or trying to
improve bad performing queries [4].

3.4

(2)

where p(∆qt ) denotes the (prior) likelihood that, while reformulating query qt −1 into qt , the user will choose to either add (i.e.,
∆qt + ), remove (i.e., ∆qt − ) or retain (i.e., ∆qt ↔ ) terms. Such likelihood can be pre-estimated [9] (i.e., parameterized); e.g., similarly
to the QCM approach [12]. Yet, for simplicity, in this work we assume that every user query change action has the same odds (i.e.,
p(∆qt ) = 31 ). Please note that, the main difference between our
estimate of θ Ft and the RM1 model is in the way the later scores
documents in Ft . Such score in RM1 is based on a given query
qt [5], with no further distinction between the role that each query
term plays or the fact that some of the terms are actually removed
terms that appeared in the previous query qt −1 . Similar to RM1,
p(∆q t |θ d )
we further estimate p(d |θ ∆qt ) ∝ Í p(∆q
.
|θ ′ )

(1)

where θ Ft now denotes the feedback model which depends on
the user’s (reformulated) query qt . While θ S t −1 estimates the dynamic information need prior to step t (i.e., It −1 ), θ Ft captures the
relative change in such need at step t (i.e., ∆It ).

d ′ ∈F t

866

t

d

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

4.2

User added or retained terms are those terms that are preferred
to be included in the feedback documents Ft . On the other hand,
removed terms are those terms that should not appear in Ft [4]. In
accordance, we define:
Î


p [µ] (w ′ |θd ),
∆qt ∈ {∆qt↔ , ∆qt + }

 w ′ ∈∆qt
def 
Í
p(∆qt |θd ) =

1−
p [0] (w ′ |θd ),
∆qt = ∆qt −


′ ∈∆q −
w
t

(4)
In order to avoid query drift, on each step t, we further anchor
the feedback model θ Ft to the query model θqt [5] as follows:
def

′
p(w |θ F
) = (1 − λt )p [0] (w |θqt ) + λt p(w |θ Ft ),
t

(5)

def

where λt = λ · sim(qt , qn ) is a dynamic query anchoring
parameter, λ ∈ [0, 1] and sim(qt , qn ) is calculated using the (idfboosted) Generalized-Jaccard similarity measure; i.e.:
def

sim(q t , q n ) =

Í

Í

w ′ ∈q t ∩q n
w ′ ∈q t ∪q n

min (t f (w ′, q t ), t f (w ′, q n )) · id f (w ′ )
max (t f (w ′, q t ), t f (w ′, q n )) · id f (w ′ )

(6)

According to λt definition, the similar query qt is to the current
query qn , the more relevant is the query change in user’s information need ∆It (modelled by θ Ft ) is assumed to be to the current
user’s information need In ; Therefore, less query anchoring effect
is assumed to be needed using query qt .

4 EVALUATION
4.1 Datasets

Sessions
Sessions
Queries/session
Topics
Sessions/topic
Judged docs/topic
Collection
Name
#documents

2011
(train)

2012
(test)

2013
(test)

76
3.7±1.8

98
3.0±1.6

87
5.1±3.6

1.2±0.5
313±115

2.0±1.0
372±163

2.2±1.0
268±117

ClueWeb09B
28,810,564

ClueWeb09B
28,810,564

ClueWeb12B
15,700,650

Baselines

We compared our proposed relevance modelling approach (hereinafter denoted SRM3 ) with several different types of baselines.
This includes state-of-the-art language modeling methods that utilize session context data (i.e., previous queries, viewed or clicked
results); namely FixedInt [8] (with α = 0.1, β = 1.0 following [8])
and its Bayesian extension BayesInt [8] (with µ = 0.2, ν = 5.0, following [8]) – both methods combine the query qn model with the
history queries Qn−1 and clicks Cn−1 centroid models; BatchUp [8]
(with µ = 2.0, ν = 15.0, following [8]) which iteratively interpolates the language model of clicks that occur up to each step t using a batched approach; and the Expectation Maximization (EM)
based approach [10] (hereinafter denoted LongTEM with λq = 0,
σC = 20 and σ N C = 1, following [10]), which first interpolates
each query qt model with its corresponding session history model
[k]
(based on both clicked (C) and non-clicked (NC) results in Dqt );
the (locally) interpolated query models are then combined based
on their relevant session history using the EM-algorithm [10].
Next, we implemented two versions of the Relevance Model [5].
The first is the basic RM3 model, denoted RM3(qn ), learned using
the last query qn and the top-m retrieved documents as pseudo relevance feedback. The second, denoted RM3(Qn′ ), uses the pseudo
information need Qn′ (see Section 3.5) instead of qn . We also implemented two query aggregation methods, namely: QA(uniform)
which is equivalent to submitting Qn′ as the query [11]; the second, denoted QA(decay), further applies an exponential decay approach to prefer recent queries to earlier ones (with decay parameter γ = 0.92, following [3, 12]). We further implemented three versions of the Query Change Model (QCM) – an MDP-inspired query
weighting and aggregation approach [12]. Following [12] recommendation, QCM’s parameters were set as follows α = 2.2, β = 1.8,
ϵ = 0.07, δ = 0.4 and γ = 0.92. The three QCM versions are the basic QCM approach [12]; QCM(SAT) which utilizes only “satisfied”
clicks (i.e., clicks whose dwell-time is at least 30 seconds [12]); and
QCM(DUP) which ignores duplicate session queries [12].
Finally, in order to evaluate the relative effect of the query-change
driven feedback model (i.e., θ Ft ), we implemented a variant of
SRM by replacing the query-change driven score of Eq. 3 with the
RM1 document score (i.e., p(d |qn )). Let SRM(QC) and SRM(RM1)
further denote the query-change and “RM1-flavoured” variants of
SRM, respectively. It is important to note that, SRM(RM1) still relies on the dynamic relevance model updating formula (see Eq. 1)
and the dynamic coefficients γt and λt – both further depend on
the session dynamics (captured by θ S t −1 and θ Ft ).

Table 1: TREC session track benchmarks

4.3

Our evaluation is based on the TREC 2011-2013 session tracks [1]
(see benchmarks details in Table 1). The Category B subsets of the
ClueWeb09 (2011-2012 tracks) and ClueWeb12 (2013 track) collections were used. Each collection has nearly 50M documents. Documents with spam score below 70 were filtered out. Documents
were indexed and searched using the Apache Solr2 search engine.
Documents and queries were processed using Solr’s English text
analysis (i.e., tokenization, Poter stemming, stopwords, etc).

Setup

Our evaluation is equivalent to the TREC 2011-2012 RL4 and TREC
2013 RL2 sub-tasks [1]. To this end, given each session’s (last)
query qn , we first retrieved the top-2000 documents with the highest query likelihood (QL) score4 to qn . Documents were then reranked using the various baselines by multiplying their (initial)

3 Stands

for “Session-Relevance Model”.

4 For this we used Solr’s LMSimilarity with Dirchlet smoothing parameter
2 http://lucene.apache.org/solr/

µ = 2500 which is similar to Indri’s default parameter.

867

Short Research Paper

Method
Initial retrieval
FixInt [8]
BayesInt [8]
BatchUp [8]
LongTEM [10]
RM3(q n ) [5]
RM3(Qn′ )
QA(uniform) [11]
QA(decay) [3]
QCM [12]
QCM(SAT) [12]
QCM(DUP) [12]
SRM(RM1)
SRM(QC)

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

nDCG@10
0.249r q
0.333r q
0.334r q
0.320r q
0.332r q
0.311r q
0.305r q
0.301r q
0.303r q
0.329r q
0.298r q
0.299r q
0.348q
0.356r

TREC 2012
nDCG nERR@10
0.256r q
0.302r q
r
q
0.296
0.380r q
0.297r q
0.382r q
0.288r q
0.368r q
r
q
0.295
0.389r q
0.284r q
0.369r q
r
q
0.284
0.354r q
0.282r q
0.352r q
0.284r q
0.353r q
r
q
0.262
0.306r q
0.281r q
0.347r q
r
q
0.281
0.350r q
0.300
0.395q
0.304
0.405r

MRR
0.594r q
0.679r q
0.674r q
0.664r q
0.667r q
0.654r q
0.647r q
0.646r q
0.645r q
0.574r q
0.635r q
0.631r q
0.699q
0.716r

nDCG@10
0.113r q
0.165r q
0.171r q
0.181r q
0.167r q
0.134r q
0.153r q
0.160r q
0.163r q
0.158r q
0.158r q
0.160r q
0.188q
0.193r

TREC 2013
nDCG
nERR@10
0.105r q
0.140r q
r
q
0.132
0.209r q
0.131r q
0.208r q
0.134
0.233r q
r
q
0.131
0.205r q
0.122r q
0.161r q
r
q
0.129
0.203r q
0.130r q
0.204r q
0.131r q
0.207r q
r
q
0.129
0.201r q
0.129r q
0.202r q
r
q
0.130
0.208r q
0.137
0.240q
0.138
0.248r

MRR
0.390r q
0.544r q
0.527r q
0.581r q
0.530r q
0.422r q
0.553r q
0.546r q
0.550r q
0.535r q
0.545r q
0.559r q
0.601q
0.612r

Table 2: Evaluation results. The r and q superscripts denote significant difference with SRM(RM1) and SRM(QC), respectively (p < 0.05).

We further observe that, compared to the baseline methods that
implement various query aggregation and scoring schemes (i.e.,
QA and QCM variants), a query-expansion strategy based on the
user’s dynamic information need (such as the one implemented by
SRM variants) provides a much better alternative; with at least
+18.5%, +6.1%, +15.1% and +9.5% improvement in nDCG@10, nDCG,
nERR@10 and MRR, respectively, for both test benchmarks.
Finally, comparing the two SRM variants side-by-side, it becomes even more clear that, using the query-change as an additional relevance feedback source is the better choice; with at least
+2.3%, +1.0%, +2.5% and +1.8% improvement in nDCG@10, nDCG,
nERR@10 and MRR, respectively, for both test benchmarks. Please
recall that, SRM(QC) was trained with a fixed and equal-valued
priors p(∆qt ). Hence, a further improvement may be obtained by
better tuning of these priors.

QL score with the score determined by each method. The document scores of the various language model baselines (i.e., FixInt, BayesInt, BatchUp, LongTEM and the variants of RM3 and
SRM ) were further determined using the KL-divergence score [13];
where each baseline’s learned model was clipped using a fixed cutoff of 100 terms [13]. The TREC session track trec eval tool5 was
used for measuring retrieval performance. Using this tool, we measured the nDCG@10, nDCG (@2000), nERR@10 and MRR of each
baseline. Finally, we tuned the RM3 and SRM’s free parameters6
using the TREC 2011 track as a train set. The parameters were optimized so as to maximize MAP. The TREC 2012-2013 tracks were
used as the test sets.

4.4

Results

The evaluation results are summarized in Table 2. The first row reports the quality of the initial retrieval. Overall, compared to the
various alternative baselines, the two SRM variants provided significantly better performance; with at least +6.6%, +2.4%, +4.1%
and +5.3% better performance in nDCG@10, nDCG, nERR@10 and
MRR, respectively, for both test benchmarks. The results clearly
demonstrate the dominance of the session-context sensitive language modeling approaches (and the two SRM variants among
them) over the other alternatives we evaluated. Furthermore, SRM’s
consideration of the user’s query-change process as an additional
relevance feedback source results in a more accurate estimate of
the user’s information need.
Next, compared to the RM3 variants, it is clear from the results that a dynamic relevance modeling approach that is driven
by query-change (such as SRM) is a better choice for the session
search task. Moving from an ad-hoc relevance modelling approach
(i.e., one that only focuses on the last query in the session) to a
session-context sensitive approach provides significant boost in
performance; with at least +14%, +7.0%, +9.8% and +9.5% improvement in nDCG@10, nDCG, nERR@10 and MRR, respectively,
for both test benchmarks.

REFERENCES
[1] Ben Carterette, Paul Clough, Mark Hall, Evangelos Kanoulas, and Mark Sanderson. Evaluating retrieval over sessions: The trec session track 2011-2014. In
Proceedings of SIGIR’2016.
[2] Steve Cronen-Townsend and W. Bruce Croft. Quantifying query ambiguity. In
Proceedings of HLT’2002.
[3] Dongyi Guan and Hui Yang. Query aggregation in session search. In Proceedings
of DUBMOD’2014.
[4] Jiepu Jiang and Chaoqun Ni. What affects word changes in query reformulation
during a task-based search session? In Proceedings of CHIIR’2016.
[5] Victor Lavrenko and W. Bruce Croft. Relevance based language models. In
Proceedings of SIGIR’2001.
[6] Jiyun Luo, Xuchu Dong, and Hui Yang. Session search by direct policy learning.
In Proceedings of ICTIR’2015.
[7] Jiyun Luo, Sicong Zhang, Xuchu Dong, and Hui Yang. Designing states, actions,
and rewards for using pomdp in session search. In Proceedings of ECIR’2005,
pages 526–537. Springer, 2015.
[8] Xuehua Shen, Bin Tan, and ChengXiang Zhai. Context-sensitive information
retrieval using implicit feedback. In Proceedings of SIGIR’2005.
[9] Marc Sloan, Hui Yang, and Jun Wang. A term-based methodology for query
reformulation understanding. Inf. Retr., 18(2):145–165, April 2015.
[10] Bin Tan, Xuehua Shen, and ChengXiang Zhai. Mining long-term search history
to improve search accuracy. In Proceedings of KDD’2006.
[11] Christophe Van Gysel, Evangelos Kanoulas, and Maarten de Rijke. Lexical query
modeling in session search. In Proceedings of ICTIR’2016.
[12] Hui Yang, Dongyi Guan, and Sicong Zhang. The query change model: Modeling
session search as a markov decision process. ACM Trans. Inf. Syst., 33(4):20:1–
20:33, May 2015.
[13] Chengxiang Zhai and John Lafferty. A study of smoothing methods for language
models applied to information retrieval. ACM Trans. Inf. Syst., 22(2), April 2004.

5 http://trec.nist.gov/data/session/12/session
6λ

eval main.py
∈ {0.1, 0.2, . . . , 0.9}, γ ∈ {0.1, 0.2, . . . , 0.9}, m ∈ {5, 10, . . . , 100}

868

