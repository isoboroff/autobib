Session 6B: Conversations and Question Answering

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Learning to Rank Question Answer Pairs with Holographic Dual
LSTM Architecture
Yi Tay

Minh C. Phan

Nanyang Technological University
ytay017@e.ntu.edu.sg

Nanyang Technological University
phan0005@e.ntu.edu.sg

Luu Anh Tuan

Siu Cheung Hui

Institute for Infocomm Research
at.luu@i2r.a-star.edu.sg

Nanyang Technological University
asschui@ntu.edu.sg

ABSTRACT
We describe a new deep learning architecture for learning to rank
question answer pairs. Our approach extends the long short-term
memory (LSTM) network with holographic composition to model
the relationship between question and answer representations. As
opposed to the neural tensor layer that has been adopted recently,
the holographic composition provides the benefits of scalable and
rich representational learning approach without incurring huge
parameter costs. Overall, we present Holographic Dual LSTM (HDLSTM), a unified architecture for both deep sentence modeling and
semantic matching. Essentially, our model is trained end-to-end
whereby the parameters of the LSTM are optimized in a way that
best explains the correlation between question and answer representations. In addition, our proposed deep learning architecture
requires no extensive feature engineering. Via extensive experiments, we show that HD-LSTM outperforms many other neural
architectures on two popular benchmark QA datasets. Empirical
studies confirm the effectiveness of holographic composition over
the neural tensor layer.

KEYWORDS
Deep Learning, Long Short-Term Memory, Learning to Rank, Question Answering

1

INTRODUCTION

Learning to rank techniques are central to many web-based question answering (QA) systems such as factoid-based QA or communitybased QA (CQA). In these applications, questions are matched
against an extensive database to find the most relevant answer.
Essentially, this is highly related to many search and information
retrieval tasks such as traditional document retrieval and text matching. However, a key difference is that questions and answers are
often much shorter compared to full-fledged documents whereby
the problem of lexical chasm [1, 8, 32] becomes more prevalent.
As such, this makes the already difficult task of designing features
for questions and answers even harder. Furthermore, traditional
Permission to make digital or hard copies of all or part of this work for personal or classroom use
is granted without fee provided that copies are not made or distributed for profit or commercial
advantage and that copies bear this notice and the full citation on the first page. Copyrights for
components of this work owned by others than ACM must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00
DOI: http://dx.doi.org/10.1145/3077136.3080790

695

approaches often involve extensive handcrafted features and domain expertise which can be laborious and expensive. In addition,
constructing features from textual clues [24, 27, 33, 36] such as lexical and syntactic features is difficult and provide limited benefits.
Overall, the challenges of learn-to-rank QA systems are two-fold.
First, feature representations of questions and answers have to be
learned or designed. Second, a similarity function has to be defined
to match questions to answers.
Recently, deep learning architectures have been an extremely
popular choice for learning distributed representations of words,
sentences or documents [11, 14]. Generally, this is known as representational learning whereby low dimensional vectors are learned
for words or documents via neural networks such as the convolutional neural networks (CNN), recurrent neural network (RNN)
or standard feed-forward multi-layer perceptron (MLP). This has
widespread applications in the field of NLP and IR such as semantic
text matching [20, 25], relation detection [10], language modeling
[14] and question answering [18, 20]. Essentially, the attractiveness
of these models stem from the fact that features are learned in deep
learning architectures in an end-to-end fashion and often require
little or no human involvement. Furthermore, the performance of
these models are often spectacular.
Additionally and recently, it has also been fashionable to model
the relationship between vectors via tensor layers [18, 25, 30]. A
recent work, the convolutional neural tensor network (CNTN) [18]
demonstrates impressive results on community-based question
answering. In their work, a convolutional neural network is used
to learn representations for questions and answers while a tensor
layer is used to model the relationship between the representations
using an additional tensor parameter. This is powerful because the
tensor layer models multiple views of dyadic interactions between
question and answer pairs which enables rich representational
learning. Overall, the CNTN is a unified architecture that learns
representations and performs matching in an end-to-end fashion.
However, the use of a tensor layer may not be implication free.
Firstly, adding a tensor layer severely increases the number of
parameters which naturally and inevitably increases the risk of
overfitting. Secondly, this significantly increases computational
and memory cost of the overall network. Thirdly, the inclusion of a
tensor layer also indirectly restricts the expressiveness of the QA
representations since increasing the parameters of the QA representations would easily incur memory and computational costs of
quadratic scale at the tensor layer.

Session 6B: Conversations and Question Answering

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

In lieu of the above mentioned weaknesses, we propose an alternative to the tensor layer. For the first time, we adopt holographic
composition to model the relationship between question and answer embeddings. Our approach is largely based on holographic
models of associative memory and employs circular correlation to
learn the relationship between QA pairs. The prime contributions
of our paper can be summarized as follows:

representations. The tensor layer can be seen as a compositional
technique to learn the relationship between two vectors and was
adapted from the neural tensor network (NTN) by Socher et al.
[22, 23]. The NTN was originally incepted in the field of NLP for
semantic parsing and used as a compositional operator in recursive
neural tensor networks (RNTN) [23] and also relational learning
on knowledge bases [22]. It has also recently seen adoption for
modeling document novelty in [30]. The tensor layer models multiple dyadic interactions between two vectors via an additional
tensor parameter. This suggests rich representational learning that
is useful for matching text pairs.
Additionally, recurrent neural networks such as the long shortterm memory (LSTM) networks are also widely popular for learning
sentence representations and has seen wide adoption in a variety of
NLP tasks. Without an exception, LSTM networks are also widely
adopted in QA [12, 25, 26]. The usage of grid-wise similarity matrices within neural architectures are also recently very fashionable
and have seen wide adoption1 in QA tasks [4, 15, 25] to model the
interactions between QA pairs. For example, in the MV-LSTM [25],
all positional hidden states from both LSTMs are being matched
grid-wise using a variety of similarity scoring functions followed
by a max-pooling layer. On the other hand, the works of [31] are
concerned with learning grid-wise attentions. On a side note, it is
good to note that, grid-wise matching, though highly competitive,
naturally incurs a prohibitive computational cost of quadratic scale.
As seen in many recent works, the tensor layer is highly popular
to model relationship between two vectors [18, 25]. However, a
tensor layer adds a significant amount of parameters to the network
causing implications in terms of runtime, memory, risk of overfitting as well as an inevitable restriction of flexibility in designing
representations for questions and answers. Specifically, increasing
the dimensionality of the LSTM or CNN output by x would incur
a parameter cost of x 2 in the tensor layer which can be non-ideal
especially in terms of scalability. As an alternative to the tensor
layer, our novel deep learning architecture adopts the circular correlation of vectors to model the interactions between question and
answer representations. The circular correlation of vectors, along
with circular convolution, are typically used in Holography to store
and retrieve information [3, 17] and are also known as correlationconvolution (holographic-like) memories. Due to its connections
with holographic models of associative memories [17], we refer
to our model as Holographic Dual LSTM. It is good to note that a
similar but fundamentally different work [16] also used holography
inspired operations within recurrent neural networks. However, our
work is the first work to incorporate holographic representational
learning for QA embeddings.
In addition, holographic composition [17] can also be interpreted
as compressed tensor product which also enables rich representational learning without severely increasing the number of parameters of the network. In this case, the parameters of the network
are learned in a way that best explains the correlation between
questions and answers. In the same domain where the neural tensor network was incepted, holographic embeddings of knowledge
graphs [13], demonstrates the effectiveness of holographic composition in the task of relational learning on knowledge bases. As

• For the first time, we adopt holographic composition for
modeling the interaction between representations of QA
pairs. Unlike the tensor layer, our compositional approach
is essentially parameterless, memory efficient and scalable.
Furthermore, our approach also enables rich representational learning by employing circular correlation.
• As a whole, we present a novel deep learning architecture,
HD-LSTM (Holographic Dual LSTM) for learning to rank
QA pairs. Our model is a unified architecture similar to
[18]. However, instead of the CNN, we use multi-layered
long short-term memory neural networks to learn representations for questions and answers. Similar to other deep
learning models, our approach does not require extensive
feature engineering or domain knowledge.
• We provide extensive experimental evidence of the effectiveness of our model on both factoid question answering
and community-based question answering. Our proposed
approach outperforms many other neural architectures on
TREC QA task and on the Yahoo CQA dataset.

2

RELATED WORK

Our work is concerned with ranking question and answer pairs to
select the most suitable answer for each question. Across the rich
history of IR research, techniques for doing so have been primarily
focused on lexical and syntactic feature-dependent approaches.
These techniques include the Tree Edit Distance (TED) model [5],
Support Vector Machines (SVMs) with tree kernels [21] and linear
chain Conditional Random Fields (CRFs) [33] with features from the
TED model. However, apart from relying heavily on handcrafted
features such as cumbersome parse trees, these approaches have
limited performance and have been shown to be outclassed by
modern deep learning approaches such as convolutional neural
networks [20, 35].
The key intuition behind deep learning architectures is to learn
low-dimensional representations of words, documents or sentences
which can be used as input features. For example, Yu et al. [35] employed a convolutional neural network for feature learning of QA
pairs and subsequently applied logistic regression for prediction.
Despite its simplicity, the performance of Yu et al. has already surpassed all traditional approaches [5, 21, 27–29]. Another attractive
quality of deep learning architectures is that features can be learned
in an end-to-end fashion. Severyn et al. [20] demonstrated a unified
architecture that trains a convolutional neural network together
with a multi-layer perceptron. In short, features are learned while
the parameters of the network are optimized for the task at hand.
In the architectures of Severyn et al. [20] and Yu et al. [35], representations of questions and answers are learned separately and
concatenated for prediction at the end. Qiu et al. [18] introduced a
tensor layer to model the relationship between question and answer

1 Notably, our proposed holographic composition can also be used for grid-wise matching.

696

Session 6B: Conversations and Question Answering

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

a whole, we propose a novel deep learning architecture based on
long short-term memory neural networks while using holographic
composition to model the interactions between QA embeddings,
this enables rich representational learning with improved flexibility
and scalability. The outcome is a highly performant end-to-end
deep learning architecture for learning to rank for QA applications.

3.3

Neural Tensor Network

The Neural Tensor Network [22, 23] is a parameterized composition
technique that learns the relationships between two vectors. The
scoring function between two vectors are defined as follows:
st (~
q , a~ ) = uT f (~
q T M [1:r ]a~ + V [~
q, a~] + b)

3

where f is a non-linear function such as tanh applied element wise.
M [1:r ] ∈ Rn×n×r is a tensor (3d matrix). For each slice of the tensor
M, each bilinear tensor product q~ T Mr a~ returns a scalar value to
form a r dimensional vector. The other parameters are the standard
form of a neural network. We can clearly see that the NTN enables
rich representational learning of embedding pairs by using a large
number of parameters.

In this section, we introduce the background for the remainder of
the paper. Namely, we formally give the problem definition and introduce fundamental deep learning models required to understand
the remainder of the paper.

3.1

Problem Statement and Approach

The task of supervised learning to rank can be typically regarded
as a binary classification problem. Given a set of questions qi ∈ Q,
the task is to rank a list of candidate answers ai ∈ A. Specifically, we try to learn a function f (q, a) that outputs a relevancy
score f (q, a) ∈ [0, 1] for each question answer pair. This score is
then used to rank a list of possible candidates. Typically, there
are three different ways for supervised text ranking, namely, pairwise, pointwise and listwise. Pairwise considers maximizing the
margin between positive and negative question-answer pairs with
an objective function such as the hinge loss. Pointwise considers
each pair, positive or negative, individually. On the other hand,
listwise considers a question and all candidate answers as a training
instance. Naturally, pairwise and listwise are much harder to train,
implement and take a longer time due to having to process more
instances. Therefore, in this work, we mainly consider a pointwise
approach when designing our deep learning model.

3.2

(1)

PRELIMINARIES

4

OUR DEEP LEARNING MODEL

In this section, we introduce Holographic Dual LSTM for representational learning and ranking of short text pairs. In our model, we
use a pair of multi-layered LSTMs denoted Q-LSTM and A-LSTM.
First, the LSTMs learn sentence representations of question and
answer pairs and subsequently holographic composition is employed to model the similarity between the outputs of Q-LSTM and
A-LSTM. Finally, we pass the network through a fully connected
hidden layer and perform binary classification. This is all done in
an end-to-end fashion. Figure 1 shows the overall architecture.

4.1

Learning QA Representations

Our model accepts two sequences of indices (one for question and
the other for answer) and a one-hot encoded ground truth for training. These sequence of indices are first passed through a look-up
layer. At this layer, each index is converted into a low-dimensional
vector representation. The parameters of this layer are W ∈ R |V |×n
where V is the size of the vocabulary and n is the dimensionality
of the word embeddings. Even though these word embeddings can
be learned from the training process of our model, i.e., end-to-end,
we do not do so since learning word embeddings often require an
extremely large corpus like Wikipedia. Therefore, we initialize W
with pretrained SkipGram [11] embeddings which is aligned with
the works of [20, 35]. Next, these embeddings from question and
answer sequences are fed into Q-LSTM and A-LSTM respectively.
Subsequently, the last hidden output from Q-LSTM and A-LSTM
are taken to be the final representation for question and answer
respectively.

Long short-term Memory (LSTM)

First, we introduce the Long Short-Term Memory (LSTM) [6]. LSTMs
are a type of recurrent neural network that are capable of learning
long term dependencies across sequences. Given an input sentence
S = (xo , x 1 ..., x n ), the LSTM returns a sentence embedding ht for
position t with the following equations:
i t = σ (Wi x t + Ui ht −1 + bi )
ft = σ (Wf x t + Uf ht −1 + bf )
c t = ft c t −1 + i t tanh(Wc x t + Uc ht −1 + bc )
ot = σ (Wo x t + Uo ht −1 + bo )
ht = ot tanh(c t )

4.2

where x t and ht are the input vectors at time t. W∗ , b∗ , U∗ are the
parameters of the LSTM network and ∗ = {o, i, f , u, c}. σ is the
sigmoid function and c t is the cell state. For the sake of brevity, we
omit the technical details of LSTM which can be found in many
related works. The output of this layer is a sequence of hidden
vectors H ∈ RL×d where L is the maximum sequence length and d
is the dimensional size of LSTM. It is also possible to stack layers
of LSTMs on top of one another which form multi-layered LSTMs
which we will adopt in our approach.

Holographic Matching of QA pairs

The QA embeddings learned from LSTM are then passed into what
we call the holographic layer. In this section, we introduce our novel
compositional deep learning model for modeling the relationship
between q and a. We denote q◦a as a compositional operator applied
to vectors q and a. We employ the circular correlation of vectors to
learn relationships between question and answer embeddings.
q ◦a = q ?a

697

(2)

Session 6B: Conversations and Question Answering

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 1: Holographic Dual LSTM Deep Learning Model for Ranking of QA Pairs
where ? : Rd × Rd → Rd denotes the circular correlation operator2 .
[q ? a]k =

d−1
X

qi a (k+i ) mod d

(3)

i=0

Circular correlation can be computed as follows:
q ? a = F −1 (F (q)

F (a))

(4)

where F (.) and F −1 (.) are the fast Fourier transform (FFT) and
inverse fast Fourier transform. F (q) denotes the complex conjugate
of F (q). is element-wise (or Hadamard) product. Additionally,
circular correlation can be viewed as a compressed tensor product
[13]. In the tensor product [q ⊗ a]i j = qi a j a separate element is
used to store each pairwise multiplication or interaction between q
and a. In circular correlation, each element of the composed vector
is a sum of multiplicative interactions over a fixed summation
pattern between q and a. Figure 2 describes this process where the
circular arrows depict the summation process in which vector ~c is
the result of composing q~ and a~ with circular correlation.
One key advantage of this composition method is that there
are no increase in parameters. The fact that the composed vector
remains at the same length of its constituent vectors is an extremely
attractive quality of our proposed model. In the case where question
and answer representations are of different dimensions, we can
simply zero-pad the vectors to make them the same length. As
circular correlation uses summation patterns, it is still possible to
compose them without much implications. However, for this paper
we consider that q~ and a~ to have the same dimensions.

4.3

Figure 2: Circular Correlation as Compressed Tensor Product, Circular Arrows denote Summation Process, Adapted
from Plate (1995) [17].
where wh and bh are parameters of the hidden layer and σ is a
non-linear activation function like tanh. Traditionally, most models
[7, 20] use the composition operator of concatenation, denoted ⊕
to combine the vectors of questions and answers. ⊕ : Rd1 × Rd2 →
Rd1 +d2 simply appends one vector after another to form a vector of
their lengths combined. Obviously, concatenation does not consider
the relationship between latent features of QA embeddings. Thus,
the relationship has to be learned from the parameters of the deep
learning model, i.e., the subsequent hidden layer. In summary, the
fully connected dense layer that maps [q ? a] to hout forms the
holographic hidden layer of our network.
Incorporating Additional Features Following the works of [2, 20],
it is also possible (though optional) to incorporate additional features. First, we include an additional similarity measure in our
model between QA embeddings. Namely, this similarity is known
as the bilinear similarity which can be defined as:

Holographic Hidden Layer

Subsequently, a fully connected hidden layer follows our compositional operator which forms the holographic layer.
hout = σ (Wh . [q ? a] + bh )

sim(q, a) = q~ T M~
a

(5)

(6)

where M ∈
is a similarity matrix between vectors q ∈ Rn
and a ∈ Rn . The bilinear similarity is a parameterized approach
Rn×n

2 For Holographic Composition, we use zero-indexed vectors for notational convenience.

698

Session 6B: Conversations and Question Answering

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Network
#Parameters
d/h/k
On TREC
NTN
d 2k + 2dk + 2k 640 / 0 / 5
2.1M
Ours
2dh + 4h
640 / 64 / 0
41.2K
Table 2: Memory Complexity Comparison between Tensor
Layer and Holographic Layer

where M is an additional parameter of the network. The output
of sim(q, a) is a scalar value that is concatenated with [q ? a]. We
experimented with concatenation at hout but empirically found it to
perform worse. It is also possible to include other manual features.
For example, in [20], word overlap features X f eat were included
before the hidden layers at the join layer. The rationale for doing
so is as follows: First, it is difficult to encode features like word
overlap into deep learning architectures [20, 35]. Secondly, word
overlap features are relatively easy and trivial to implement and
incorporate. As such, we are able to do the same with our model.
Thus, when using external features, the inputs to the holographic
hidden layer becomes a vector [[q ? a], sim(q, a), X f eat ].

4.4

Network
Complexity
NTN
O(d 2k + 2dk + 2k )
Ours
O(2dh + 4h + d log d )
Table 3: Complexity Comparison between Tensor Layer and
Holographic Layer

Softmax Layer

The output from the holographic hidden layer is then passed into
a fully connected softmax layer which introduces another two
parameters Wf and bf .
p = so f tmax (Wf . hout + bf )

q ◦a is used to map the composed vector into a scalar value. Clearly,
the cost of a simple tensor layer is of quadratic scale which makes
it difficult to scale with large d. Concatenation, on the other hand,
doubles the length of composed vectors and increases the memory
cost by a factor of two. Finally, we see that the parameter cost of
circular correlation that we employ is only d. As such, there can be
significantly less parameters in the subsequent layers. Finally, the
computational complexity of circular correlation is also relatively
low at d log d. Next, we compare the complexity of our network
and the NTN. To enable direct comparison, we exclude any additional features at the holographic hidden layer of our network and
include the subsequent softmax layer. Finally, the overall network
and similarity between q and a can be modeled as follows:

(7)

where θ k is the weight vector of the kth class and x is the final
vector representation of question and answer after passing through
all the layers in the network.

4.5

Training and Optimization

Finally, we describe the optimization and training procedure of our
network. Our network minimizes the cross-entropy loss function
as follows:
L=−

N
X

[yi log ai + (1 − yi ) log(1 − ai )] + λkθ k 22

(8)

sh (~
q, a~ ) = so f tmax (WfT f (WhT [~
q ? a~] + bh ) + bf )

i=1

where a is the output of the softmax layer. θ contains all the parameters of the network and λkθ k 22 is the L2 regularization. The
parameters of the network can be updated by Stochastic Gradient
Descent (SGD). In our network, we mainly employ the Adam [9]
optimizer.

5

DISCUSSION

In this section, we discuss and highlight some of the interesting
and advantageous properties of our proposed approach.

5.1

(9)

where Wh ∈
is parameters at the holographic hidden layer
following the composition operation, bh is the scalar bias at the
hidden layer, Wf ∈ Rr ×2 converts the output at the hidden layer
to a 2-class classification problem and q, a ∈ Rd where d is the
dimension size of the LSTM. f (.) is a non-linear activation function.
Note that since we consider use a softmax layer at the output, our
final output s (q, a) is a vector of 2 dimensions. Similarly, in the
traditional tensor layer described in Equation (1), we are able to
simply adapt the vector u to become a weight matrix of u ∈ Rk×2
where k is the number of slices of tensors.
In Table 2 and Table 3, we compare the differences between the
tensor layer and our holographic layer with respect to the number
of parameters and computational complexity respectively. Note
that d is the dimensionality of QA embeddings, h is the size of
the hidden layer and k is the number of tensor slices. To facilitate
easier comparison, we do not include complexity from learning QA
representations, computing bias and activation functions but only
matrix and vector operations. From Table 2, it is clear and evident
that our approach does not require as much parameters as the
NTN. We also report the optimal dimensions of the QA embeddings
and hidden layer size on TREC QA. We see that our model only
requires 41.2k parameters3 as opposed to 2.1M parameters with
the NTN. As such, we see that when optimal parameters required
for sentence modeling is high, the cost on the subsequent matching
Rd×r

Complexity Analysis

To better understand the merits of our proposed approach, we
study the computational and memory complexity of our model
with respect to the alternatives like the tensor layer.
Operator
#Parameters Complexity
Tensor Product ⊗
d2
O(d 2 )
Concatenation ⊕
2d
O(d )
Circular Correlation ?
d
O(d log d )
Table 1: Complexity Comparison between Compositional
Operators

First, Table 1 shows the parameter cost and complexity for the
three different compositional operators. This assumes a simple example where q, a ∈ Rd and a vector of the same dimensionality as

3 We exclude word embedding and LSTM parameters in this comparison

699

Session 6B: Conversations and Question Answering

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

layer becomes impractical. The problem of quadratic scale is also
reflected in computational complexity. Thus, from a theoretical
point of view, the holographic composition can be seen as a memory
efficient and faster alternative to the neural tensor network layer.

5.2

• Our composition does not increase the dimensionality of
its constituent vectors, i.e, the composition of q~ ? a~ preserves its dimensionality. On the other hand, concatenation
doubles the parameter cost at the subsequent hidden layers. Furthermore, the relationship between question and
answer embeddings have to be learned by the hidden layer.
• The association of two vectors, namely question and answer vectors are modeled end-to-end in the network. Via
back-propagation, the network learns parameters that best
explains this correlation via gradient descent.
Here it is good to note that the original holographic reduced representations [17] used convolution to encode and correlation to
decode. However, this can be done vice versa as well [13].

Associative Holographic Memories

Holographic models of associative memories employ a series of
convolutions and correlations to store and retrieve item pairs. This
is sometimes referred to as convolution-correlation (holographiclike) memories [17]. At this point, it is apt to introduce circular
convolution: ∗ : Rd × Rd → Rd which is closely related to circular
correlation.
[q ∗ a]k =

d−1
X

qi a (k−i ) mod d

(10)

6

In holographic associative memory models, association of vector
pairs can be encoded via correlation and then decoded with circular
convolution. The relationship between correlation and convolution
is as follows:
q ? a = q̃ ∗ a
(11)

6.1

Experiment Setup

In this section, we introduce the dataset, evaluation procedure,
metrics and compared baselines used in this experiment.

where q̃ is the approximate inverse of q such that qi = q (−i mod d ) .
Typically, the encoding-decoding4 process is done via Hebbian
learning in associative memory models. However, in our case,
our model is holographic in the sense that correlation-convolution
memories are learned implicitly via back-propagation. For example,
let h be the input to the hidden layer hout , the gradients at a can
be represented as:
X ∂E
∂E
=
q
∂ai
∂h j (k −j mod d )

EMPIRICAL EVALUATION ON TREC QA

We evaluate our model on the TREC QA task of answer sentence
selection on factoid based QA.

i=0

6.1.1 Dataset. In this task, we use the benchmark dataset provided by Wang et al. [29]. This dataset was collected from TREC
QA tracks 8-13. In this task of factoid QA, questions are generally
factual based questions such as ”What is the monetary value of the
Nobel Peace Prize in 1989?””. In this dataset, we are provided with
two training sets, namely, TRAIN and TRAIN-ALL. TRAIN consists
of QA pairs that have been manually judged and annotated. TRAINALL is a automatically judged dataset of QA pairs and contains
a larger number of QA pairs. TRAIN-ALL, being a larger dataset,
also contains more noise. Nevertheless, both datasets enable the
comparison of all models with respect to availability and volume of
training samples. Additionally, we are also provided with a development set for parameter tuning. The results of both training sets,
development set and testing set are reported in Table 4. Finally, it is
good to note that the maximum number of tokens for questions and
answers are 11 and 38 respectively and the length of the vocabulary
|V | = 16468.

(12)

k

The gradient at ai according to Equation (12) [16] is equivalent
to correlating h with the approximate inverse of q. Recall that
correlating with the inverse is equivalent to circular convolution.
As such, this establishes the relation of our model to holographic
memories. Since our main point is to illustrate these connections,
we omit the entire back-propagation derivation due to the lack of
space. Finally, we describe and summarize the overall advantages of
employing a holographic layer in our deep architecture for learning
to rank question answer pairs.

Data
TRAIN-ALL
TRAIN
DEV
TEST

• Unlike circular convolution, circular correlation is noncommutative, i.e., q ? a , a ? q. This is useful as most
applications of text matching are non-symmetric. For example, questions to answers or queries to documents are
not symmetric in nature. As such, we utilize correlation as
the encoding operation and allow the network to decode
via circular convolution while learning parameters.
• The first index of the circular correlation composed vector
[q ? a]0 is the dot product of q and a. This is extremely
helpful since question answer matching requires a measure
of similarity.
• The computational complexity of FFT is O(d loд d ) which
makes it an efficient composition.

# Questions
1229
94
82
100

# QA pairs
53417
4718
1148
1517

% Correct
12
7.4
9.3
18.7

Table 4: Dataset Statistics for TREC QA Dataset
6.1.2 Evaluation Procedure and Metrics. Following the experimental procedure in [20], we report the results of all models in two
settings. In the first setting, we measure the representational learning ability of all deep learning models without the aid of external
features. Conversely, in the second setting, we include an additional
feature vector X f eat ∈ R4 containing the count of word overlaps
(ordinary and idf weighted) between question-answer pairs by
considering inclusion and dis-inclusion of stop-words. Finally, the
official evaluation metrics of MAP (Mean Average Precision) and
MRR (Mean Reciprocal Rank) are used as our evaluation metrics.

4 Ideally, that the euclidean norm of q and a should be ≈ 1. However, our preliminary experiments

showed that adding a normalization layer did not improve the performance.

700

Session 6B: Conversations and Question Answering

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

P |Q |
1
MRR is defined as |q1 | q=1 r ank
(q) where rank(q) is the rank of the
PQ
first correct answer. MAP is defined as Q1 q=1 AvдP (q). The MAP
is the average precision across all queries qi ∈ Q. For evaluation,
we use the official trec eval script.

• HD-LSTM∗ (Ours) Holographic Dual LSTMS is the model
architecture introduced in this paper. In our HD-LSTM
model, the QA representations are merged with Holographic Composition.
6.1.4 Implementation Details and Hyperparameters. We implemented all deep learning architectures ourselves with the exception
of Yu et al. [35] which we directly report the results. To facilitate
fair comparison, we implement the exact architecture of the CNN
model from Severyn et al. [20] ourselves using the same evaluation
procedure and optimizer. All hyperparameters were tuned on the
development set using extensive grid search. We trained all models using the Adam [9] optimizer with an initial learning rate of
10−5 for LSTM models and 10−2 for CNN models5 and minimized
the same cross entropy loss in a point-wise fashion. We applied
gradient clipping of 1.0 of the norm for all LSTM models.
With the exception of the single-layered LSTM and MV-LSTM, all
LSTM-based models use a single-direction and multi-layered setting.
The input sequences are all padded with zero vectors to the max
length for questions and answers separately. The dimensionality
of the LSTM models are tuned in multiples of 128 in the range
of [128, 640] for TRAIN and amongst {512, 1024} for TRAIN-ALL
in lieu of the larger dataset. Here it is good to note that the final
feature vector of the model in Severyn et al is ≈ 1000. The number
of LSTM layers are tuned from a range of 2 − 4 and batch size is
fixed to 256 for all LSTM based models. The hidden layer size for all
LSTM models are amongst {32, 64, 128, 256, 512}. For regularization
and preventing over-fitting, we apply a dropout of d = 0.5 and set
the regularization hyperparameter λ = 0.00001. For MV-LSTM,
we followed the configuration setting as stated in [25]. We used
the pretrained word embeddings [20] of 50 dimensions trained on
Wikipedia and AQUAINT corpus. The word embedding layer is set
to non-trainable in lieu of the small dataset. We trained all models
for a maximum of 30 epochs with early stopping, i.e., if the MAP
score does not increase after 5 epochs. We take MAP scores on
the development set at every epoch and save the parameters of
the network for the top three models on the development set. We
report the best test score from the saved models. All experiments
were conducted on a Linux machine with a single Nvidia GTX1070
GPU (8GB RAM).

6.1.3 Algorithms Compared. Aside from comparison with all
published state-of-the-art methods, we also evaluate our model
against other deep learning architectures. Since the deep learning
models compared in [20] are based on Convolutional Neural Networks, we additionally compare our model with MV-LSTM [25]
and a simple LSTM baseline in our experimental evaluation. The
following lists the major and popular deep learning based models
for direct comparison with our model. Model names with ∗ indicate
that we implemented the model ourselves.
• CNN + Logistic Regression (Yu et al.) This is the model
introduced in [35]. Representations of questions and answers are learned by a convolutional neural network. Subsequently, logistic regression over the learned features is
used to score QA pairs. We report two settings, namely
Unigram and Bigram which are also reported in their work.
• CNN∗ We implemented a CNN model following the architecture and hyperparameters of [20]. This model includes
a bilinear similarity feature while concatenating two CNN
encoded sentence representations. Unlike Yu et al.’s model,
this work ranks QA pairs using an end-to-end architecture.
• CNTN∗ We implemented a neural tensor network layer to
performing matching of question and answer representations encoded by convolutional neural networks. This is
similar to [18] but adopts the CNN architecture and hyperparameters of Severyn et al. [20]. For the tensor layer, we
use k = 5 where k is the number of slices of the tensor.
• LSTM∗ We consider both single layer and multi-layered
LSTMs as our baselines. These baseline models do not
specially model the relationships between questions and
answers. Instead, a concatenation operation is used to
combine the QA embeddings in which the relationships
between the two vectors are modeled by the hidden layer.
• MV-LSTM∗ (Wan et al.) This model, introduced in [25],
considers matching of multiple positional embeddings and
subsequently applying max-pooling of top-k interactions.
For scalability reasons, we only consider the bilinear similarity setting for this model. We consider this sufficient for
three reasons. First, it is reported in [25] that the performance benefits of tensor over bilinear is minimal. Second,
it is extremely expensive computationally even when considering a bilinear similarity let alone the tensor similarity.
Third, the comparison with this model mainly aims to investigate the effectiveness of multiple positional embeddings.
• NTN-LSTM∗ We consider a Neural Tensor Network +
LSTM architecture instead of the CNTN to enable fairer
comparison with our LSTM based model. In this model,
we replace the holographic layer in HD-LSTM with a NTN
layer which forms the major comparison in this paper. For
the tensor layer, similar to the CNTN, we use k = 5 where
k is the number of slices of the tensor.

6.2

Experimental Results

This section shows the experimental results on the TREC QA answer
sentence selection task. Table 5 shows the result of all deep learning
architectures in four different configurations, i.e., different training
sets (TRAIN vs TRAIN-ALL) and different feature settings (with
and without additional features). Overall, we see that HD-LSTM
outperforms all other deep learning models. The relative ranking
of each deep learning architecture is in concert with our intuitions.
Using a tensor layer for matching improves performance over their
base models which is aligned with the results of [18]. However, we
see that HD-LSTM outperforms NTN-LSTM by a significant margin
across all datasets and settings despite being more efficient. This
shows the effectiveness of holographic composition for rich representational learning of QA pairs despite having less parameters.
Additionally, the average increase over the baseline multi-layered
5 This learning rate works best for CNN models with Adam

701

Session 6B: Conversations and Question Answering

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Setting 1 (raw)
Setting 2 (with extra features)
All
TRAIN
TRAIN-ALL
TRAIN
TRAIN-ALL
Average
Model
MAP
MRR
MAP
MRR
MAP
MRR
MAP
MRR
MAP
MRR
CNN + LR (unigram) 0.5387 0.6284 0.5470 0.6329 0.6889 0.7727 0.6934 0.7677 0.6170 0.6982
CNN + LR (bigram)
0.5476 0.6437 0.5693 0.6613 0.7058 0.7846 0.7113 0.7846 0.6335 0.7186
LSTM (1 layer)
0.5731 0.6056 0.6204 0.6685 0.6406 0.7494 0.6782 0.7604 0.6280 0.6960
LSTM
0.6093 0.6821 0.5975 0.6533 0.7007 0.7777 0.7350 0.8064 0.6606 0.7299
CNN
0.5994 0.6584 0.6691 0.6880 0.7000 0.7469 0.7216 0.7899 0.6725 0.7208
CNTN
0.6154 0.6701 0.6580 0.6978 0.7045 0.7562 0.7278 0.7831 0.6764 0.7268
MV-LSTM
0.6307 0.6675 0.6488 0.6824 0.7327 0.7940 0.7077 0.7821 0.6800 0.7315
NTN-LSTM
0.6274 0.6831 0.6340 0.6772 0.7225 0.7904 0.7364 0.8009 0.6800 0.7379
HD-LSTM
0.6404 0.7123 0.6744 0.7511 0.7520 0.8146 0.7499 0.8153 0.7042 0.7733
Table 5: Experimental Results of all Deep Learning Architectures on TREC QA Dataset. Best result is in boldface.

LSTM are 4% and 5% in terms of MAP and MRR respectively which
can be considered significant. We also note that there is quite significant improvement with using multi-layered LSTMs over a single
layered LSTM. The performance of MV-LSTM is competitively similar to NTN-LSTM in this task. However, it is good to note that
MV-LSTM takes ≈ 30s per epoch at the bilinear setting as opposed
to our model’s ≈ 0.1s epoch with the same LSTM configurations
and settings and on the same machine and GPU. On the other hand,
we see that the baseline LSTM models perform worse than CNN
based models whereby a single layer LSTM performs poorly and
does almost as poor as CNN with logistic regression from Yu et al.
[35]. However, the NTN-LSTM and MV-LSTM perform better than
the CNTN. It is good to note that our CNN model implementation
achieves slightly worst results as compared to [20] because model
parameters are saved at the batch level in their work while we
evaluate at an epoch level instead. Nevertheless, the performance
is quite similar.
Finally, Table 6 shows the results of all published models including non deep learning systems. Evidently, deep learning has
significantly outperformed traditional methods in this task. It is also
good to note that HD-LSTM outperforms all models (both deep
learning and non-deep learning) even with the smaller TRAIN
dataset. We find this result remarkable.

7

EMPIRICAL EVALUATION ON
COMMUNITY-BASED QA

In this experiment, we consider the task of community-based question answering (CQA). We use the Yahoo QA Dataset6 for this
purpose. The objectives of this experiment are two-fold. First, we
provide more experimental evidence of the QA ranking capabilities of our model. Second, we test all models on the Yahoo QA
dataset which can be considered as a large web-scale dataset with a
diverse range of topics which additionally includes informal social
language.

7.1

Experimental Setup

We describe the dataset used, evaluation metrics and implementation details
# QA Pairs # Correct
TRAIN
253440
50688
DEV
31680
6336
TEST
31680
6336
Table 7: Dataset Statistics of Yahoo QA Dataset.
7.1.1 Dataset. The dataset we use is the Yahoo QA Dataset
containing 142,627 questions and answers. We select QA pairs
containing questions and best answers of length 5-50 tokens after
filtering away non-alphanumeric characters. As such, we obtain
63, 360 QA pairs in the end. The total vocabulary size |V | of this
dataset is 116,900. We construct negative samples for each question
by sampling 4 samples from the top 1000 hits obtained via Lucene7
search. The overall statistics of the constructed dataset is shown
in Table 7. In general, we can consider Yahoo to be a much larger
dataset over TREC QA. Furthermore, in CQA, the questions and
answers are generally of longer length.

Model
MAP
MRR
Wang et al. (2007) [29]
0.6029 0.6852
Heilman and Smith (2010) [5]
0.6091 0.6917
Wang and Manning (2010) [28]
0.5951 0.6951
Yao (2013) [33]
0.6307 0.7477
Severyn & Moschitti (2013) [21] 0.6781 0.7358
Yih et al. (2013) [34]
0.7092 0.7700
Yu et al. (2014) [35]
0.7113 0.7846
Severyn et al. (2015) [20]
0.7459 0.8078
HD-LSTM TRAIN
0.7520 0.8146
HD-LSTM TRAIN-ALL
0.7499 0.8153
Table 6: Performance Comparison of all Published Models
on TREC QA Dataset.

7.1.2 Baselines and Implementation Details. For this experiment,
our comparison against competitors are similar to the first experiment. Specifically, we compare our model with LSTM (baseline),
MV-LSTM (Bilinear) and NTN-LSTM for LSTM-based deep learning models along with CNN and CNTN. In addition, we include
the popular Okapi BM25 benchmark [19] as an indicator of the
6 http://webscope.sandbox.yahoo.com/catalog.php?datatype=l&did=10
7 http://lucene.apache.org/core/

702

Session 6B: Conversations and Question Answering

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

difficulty of the test set. Note that our experimental results would
be naturally different from [25] due to different train/test/dev splits
and variations in the negative sampling process. The implementation details for LSTM based deep learning models are the same as
Section 6.1.4. However, due to scalability reasons and the requirement of processing significantly much more QA pairs, we limit the
dimensions of the LSTM and hidden layer to be 50. The number of
layers of the LSTM is also set to 1. For all models, we only consider
a single direction LSTM. The other hyperparameters, including the
choice of pretrained word embeddings, dropout and regularization
are the same unless stated otherwise.

Due to the lack of space, we only report the hyperparameter tuning
process of the TREC QA task specifically with respect to the MAP
metric.

8.1

7.1.3 Evaluation Metrics. For this experiment we use the same
metrics as [25], namely the Precision@1 and MRR. P@1 can be
PN
defined as N1 i=1
δ (r (A+ ) = 1) where δ is the indicator function
+
and A is the ground truth. For the sake of brevity, we do not restate
MRR as it is already defined in Section 6.1.2. Note that we only
consider the ranking of the ground truth amongst all the negative
samples for a given question.

7.2

Figure 3: Effect of QA Embedding Size (LSTM Dimensions).
Figure 3 shows the influence of the size of the QA embedding
on MAP performance on TREC TRAIN dataset. We investigate the
NTN-LSTM at three different k levels (the number of tensor slices).
We see that NTN-LSTM outperforms LSTM and HD-LSTM when
the dimensionality of QA embeddings are small. This is because the
introduction of extra parameters of quadratic scale at the tensor
layer helps the NTN-LSTM fit the data. However, when increasing
the dimensions of the sentence embedding, HD-LSTM starts to
steadily outperform the NTN-LSTM. Furthermore, we note that at
higher LSTM dimensions, i.e., 640, there is a steep decline in the
performance of the NTN-LSTM probably due to overfitting. Overall,
the performance of the baseline LSTM cannot be compared to both
the HD-LSTM and NTN-LSTM. Evidently, the method used to model
the relationship between the embedding of text pairs is crucial and
has implications on the entire network. We see that the holographic
composition allows more representational freedom in the LSTM
by allowing it to have larger dimensions of text representations
without possible implications.

Experimental Results

Table 8 shows the results of the experiments on Yahoo QA Dataset.
We show that HD-LSTM achieves state-of-the-art performance on
the Yahoo QA Dataset. First, we notice that the performance of
Okapi BM25 model is only marginal compared to random guessing.
This signifies that the testing set is indeed a difficult one.
Model
P@1
MRR
Random Guess 0.2000 0.4570
Okapi BM-25
0.2250 0.4927
CNN
0.4125 0.6323
CNTN
0.4654 0.6687
LSTM
0.4875 0.6829
NTN-LSTM
0.5448 0.7309
HD-LSTM
0.5569 0.7347
Table 8: Experimental Results on Yahoo QA Dataset.
Unfortunately, we were not able to obtain any results with MVLSTM due to computational restrictions. Specifically, each training
instance involves 5000 matching computations to be made. Hence,
each epoch takes easily ≈ 3 hours even with GPUs. Hence, from
the perspective of practical applications, we can safely eliminate
the MV-LSTM as an alternative. Once again, we see the trend that
tensor layer improves results over their base models similar to the
earlier evaluation on the TREC QA task. However, unlike the TREC
datasets, the NTN-LSTM performs significantly better than the
baseline LSTM probably due to the larger dataset. On the other
hand, we also observe that the LSTM performs better compared to
CNN on this dataset similar to the results reported in [25]. Finally,
our HD-LSTM performs the best and outperforms the NTN-LSTM
despite having less parameters and being more efficient as discussed
in our complexity analysis section earlier.

8

Effect of Embedding Dimension

8.2

Effect of Hidden Layer Size

Figure 4: Effect of the Size of Hidden Layer on MAP.
The number of nodes at the hidden layer is an important hyperparameter in our experiments due to its close proximity and direct
interaction with the composition between question and answer
embeddings. Note that the hidden layer size is directly related to
the number of parameters of the model. In this section, we aim

ANALYSIS OF HYPERPARAMETERS

In this section, we discuss some important observations in our
experiments. such as hidden layer size on our model. In particular,
we investigate the HD-LSTM, NTN-LSTM and the baseline LSTM.

703

Session 6B: Conversations and Question Answering

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

to study the influence of the size of the hidden layer with respect
to the LSTM and HD-LSTM. Note that we are unable to directly
compare with the NTN-LSTM as the tensor M in the NTN layer
acts like a hidden layer. Figure 4 shows the effect of the number
of hidden nodes. Evidently, we see that a smaller hidden layer
size benefits the HD-LSTM. On the other hand, the performance of
LSTM is only decent above a certain threshold of hidden layer size.
This is in concert with our understandings of the interactions of
the parameters with the composition layer. Our model requires less
parameters to model the relationship between text pairs because
the correlation between question and answer embeddings is modeled via holographic composition. We see that our model achieves
good results even with a smaller hidden layer, i.e., 64. Contrarily,
LSTM requires more parameters to model the relationship between
the text embeddings. We see that HD-LSTM with a small hidden
layer produces the best results.

9

[14] Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying
Song, and Rabab K. Ward. 2016. Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval. IEEE/ACM Trans. Audio,
Speech & Language Processing 24, 4 (2016), 694–707. DOI:http://dx.doi.org/10.1109/TASLP.
2016.2520371
[15] Ankur P. Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. 2016. A Decomposable
Attention Model for Natural Language Inference. In Proceedings of the 2016 Conference on
Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November
1-4, 2016. 2249–2255.
[16] Tony Plate. 1992. Holographic Recurrent Networks. In Advances in Neural Information Processing Systems 5, [NIPS Conference, Denver, Colorado, USA, November 30 - December 3, 1992].
34–41.
[17] Tony A. Plate. 1995. Holographic reduced representations. IEEE Trans. Neural Networks 6, 3
(1995), 623–641. DOI:http://dx.doi.org/10.1109/72.377968
[18] Xipeng Qiu and Xuanjing Huang. 2015. Convolutional Neural Tensor Network Architecture
for Community-Based Question Answering. In Proceedings of the Twenty-Fourth International
Joint Conference on Artificial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015.
1305–1311.
[19] Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike
Gatford. 1994. Okapi at TREC-3. In Proceedings of The Third Text REtrieval Conference, TREC
1994, Gaithersburg, Maryland, USA, November 2-4, 1994. 109–126.
[20] Aliaksei Severyn and Alessandro Moschitti. 2015. Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, Santiago, Chile, August 9-13, 2015.
373–382. DOI:http://dx.doi.org/10.1145/2766462.2767738
[21] Aliaksei Severyn, Alessandro Moschitti, Manos Tsagkias, Richard Berendsen, and Maarten
de Rijke. 2014. A syntax-aware re-ranker for microblog retrieval. In The 37th International
ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’14, Gold
Coast , QLD, Australia - July 06 - 11, 2014. 1067–1070. DOI:http://dx.doi.org/10.1145/2600428.
2609511
[22] Richard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng. 2013. Reasoning With Neural Tensor Networks for Knowledge Base Completion. In Advances in Neural
Information Processing Systems 26: 27th Annual Conference on Neural Information Processing
Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United
States. 926–934.
[23] Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y
Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over
a sentiment treebank. Citeseer.
[24] Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2011. Learning to Rank Answers to Non-Factoid Questions from Web Collections. Computational Linguistics 37, 2 (2011),
351–383. DOI:http://dx.doi.org/10.1162/COLI a 00051
[25] Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu, Liang Pang, and Xueqi Cheng. 2016. A
Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations.
In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016,
Phoenix, Arizona, USA. 2835–2841.
[26] Di Wang and Eric Nyberg. 2015. A Long Short-Term Memory Model for Answer Sentence
Selection in Question Answering. In Proceedings of the 53rd Annual Meeting of the Association
for Computational Linguistics and the 7th International Joint Conference on Natural Language
Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015,
Beijing, China, Volume 2: Short Papers. 707–712.
[27] Kai Wang, Zhaoyan Ming, and Tat-Seng Chua. 2009. A syntactic tree matching approach to
finding similar questions in community-based qa services. In Proceedings of the 32nd Annual
International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2009, Boston, MA, USA, July 19-23, 2009. 187–194. DOI:http://dx.doi.org/10.1145/1571941.
1571975
[28] Mengqiu Wang and Christopher D. Manning. 2010. Probabilistic Tree-Edit Models with Structured Latent Variables for Textual Entailment and Question Answering. In COLING 2010, 23rd
International Conference on Computational Linguistics, Proceedings of the Conference, 23-27 August 2010, Beijing, China. 1164–1172.
[29] Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy Model?
A Quasi-Synchronous Grammar for QA. In EMNLP-CoNLL 2007, Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, June 28-30, 2007, Prague, Czech Republic. 22–32.
[30] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2016. Modeling Document
Novelty with Neural Tensor Network for Search Result Diversification. In Proceedings of the
39th International ACM SIGIR conference on Research and Development in Information Retrieval,
SIGIR 2016, Pisa, Italy, July 17-21, 2016. 395–404. DOI:http://dx.doi.org/10.1145/2911451.
2911498
[31] Caiming Xiong, Victor Zhong, and Richard Socher. 2016. Dynamic Coattention Networks For
Question Answering. CoRR abs/1611.01604 (2016).
[32] Xiaobing Xue, Jiwoon Jeon, and W. Bruce Croft. 2008. Retrieval models for question and
answer archives. In Proceedings of the 31st Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval, SIGIR 2008, Singapore, July 20-24, 2008.
475–482. DOI:http://dx.doi.org/10.1145/1390334.1390416
[33] Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch, and Peter Clark. 2013. Answer
Extraction as Sequence Tagging with Tree Edit Distance. In Human Language Technologies:
Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, June 9-14, 2013, Westin Peachtree Plaza Hotel, Atlanta, Georgia, USA. 858–867.
[34] Wen-tau Yih, Ming-Wei Chang, Christopher Meek, and Andrzej Pastusiak. 2013. Question
Answering Using Enhanced Lexical Semantic Models. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013, 4-9 August 2013, Sofia, Bulgaria,
Volume 1: Long Papers. 1744–1753.
[35] Lei Yu, Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman. 2014. Deep Learning for
Answer Sentence Selection. CoRR abs/1412.1632 (2014).
[36] Guangyou Zhou, Li Cai, Jun Zhao, and Kang Liu. 2011. Phrase-Based Translation Model for
Question Retrieval in Community Question Answer Archives. In The 49th Annual Meeting of
the Association for Computational Linguistics: Human Language Technologies, Proceedings of
the Conference, 19-24 June, 2011, Portland, Oregon, USA. 653–662.

CONCLUSION

We proposed a novel deep learning architecture based on holographic associative memories for learning to rank QA pairs. The
circular correlation of vectors has attractive qualities such as memory efficiency and rich representational learning. Additionally, we
overcome the problem of scaling QA representations while keeping
the compositional parameters low which is prevalent in models
that adopt a tensor layer. We also outperform many variants of
deep learning architectures including the NTN-LSTM and CNTN
in the task of learning to rank for question answering applications.

REFERENCES
[1] Adam L. Berger, Rich Caruana, David Cohn, Dayne Freitag, and Vibhu O. Mittal. 2000. Bridging the lexical chasm: statistical approaches to answer-finding. In SIGIR. 192–199. DOI:
http://dx.doi.org/10.1145/345508.345576
[2] Antoine Bordes, Jason Weston, and Nicolas Usunier. 2014. Open Question Answering with
Weakly Supervised Embedding Models. In Machine Learning and Knowledge Discovery in
Databases - European Conference, ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, Part I. 165–180. DOI:http://dx.doi.org/10.1007/978-3-662-44848-9 11
[3] D. Gabor. 1969. Associative Holographic Memories. IBM J. Res. Dev. 13, 2 (March 1969), 156–
159. DOI:http://dx.doi.org/10.1147/rd.132.0156
[4] Hua He, Kevin Gimpel, and Jimmy J Lin. Multi-Perspective Sentence Similarity Modeling
with Convolutional Neural Networks.
[5] Michael Heilman and Noah A. Smith. 2010. Tree Edit Models for Recognizing Textual Entailments, Paraphrases, and Answers to Questions. In Human Language Technologies: Conference
of the North American Chapter of the Association of Computational Linguistics, Proceedings,
June 2-4, 2010, Los Angeles, California, USA. 1011–1019.
[6] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735–1780.
[7] Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional Neural Network
Architectures for Matching Natural Language Sentences. In Advances in Neural Information
Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada. 2042–2050.
[8] Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005. Finding similar questions in large question and answer archives. In Proceedings of the 2005 ACM CIKM International Conference on
Information and Knowledge Management, Bremen, Germany, October 31 - November 5, 2005.
84–90. DOI:http://dx.doi.org/10.1145/1099554.1099572
[9] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. CoRR
abs/1412.6980 (2014).
[10] Anh Tuan Luu, Yi Tay, Siu Cheung Hui, and See-Kiong Ng. 2016. Learning Term Embeddings
for Taxonomic Relation Identification Using Dynamic Weighting Neural Network. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016,
Austin, Texas, USA, November 1-4, 2016. 403–413.
[11] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In Advances in
Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada,
United States. 3111–3119.
[12] Jonas Mueller and Aditya Thyagarajan. 2016. Siamese Recurrent Architectures for Learning
Sentence Similarity. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,
February 12-17, 2016, Phoenix, Arizona, USA. 2786–2792.
[13] Maximilian Nickel, Lorenzo Rosasco, and Tomaso A. Poggio. 2016. Holographic Embeddings
of Knowledge Graphs. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence,
February 12-17, 2016, Phoenix, Arizona, USA. 1955–1961.

704

