Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

But Is It Statistically Significant?
Statistical Significance in IR Research, 1995–2014
Ben Carterette

University of Delaware
carteret@udel.edu
venue
SIGIR
ECIR
CIKM

stats
1,159 short, 1,254 long
346 short, 413 long
1,015 short, 1,605 long;
780 IR, 755 KM, 383 DB, 702
not labeled
total 1995–2014
5,792 2,520 short, 3,272 long
232,720 unique terms
Table 1: IR research paper corpora used in this work. Each
paper is labeled “short” or “long”. Some CIKM papers are
additionally labeled with the track they were submitted to
(IR, KM, or DB).

ABSTRACT
We analyze 5,792 IR conference papers published over 20 years
to investigate how researchers have used and are using statistical
significance testing in their experiments.

CCS CONCEPTS
•Information systems →Evaluation of retrieval results; Presentation of retrieval results;

1

INTRODUCTION

Experimentation has long been an important aspect of research in
information retrieval, going all the way back to the 1963 tests of
indexing devices by Cleverdon and Mills [3]. It became clear early
on that two different retrieval systems could differ in performance
by only a few points of precision (or average precision, or, later,
nDCG), and that in order to say whether a difference was likely
to be “real” or not, we would have to turn to statistical hypothesis
testing [11, 12]. Early work published on this problem largely
took a theoretical perspective on which statistical tests were most
appropriate to apply in IR and how to interpret their results [4, 9, 14];
more recent work has investigated from an empirical perspective
in the context of community-wide competitions like those run at
TREC, CLEF, and NTCIR [1, 10, 15].
In this paper we look back over 20 years of research to investigate
the adoption of statistical significance testing by the IR community.
We look at which tests are most widely used, how reporting of
test results has changed over time, how the use of tests differs
by conference and by paper type, why papers sometimes do not
report test results, and distributions of p-values in papers. We also
present a novel method for simulating the distribution of p-values
we should expect; that our method works well suggests there is
still much important work to be done in search.

2

years
1995–2014
2005–2014
2005–2014

papers
2,413
759
2,620

this: the sign test (also called the binomial test), Wilcoxon’s signedrank test, and Student’s t-test [10]. More recently, researchers have
recommended distribution-free tests like Fisher’s exact test (which,
when implemented using random sampling, is sometimes called
the randomization or permutation test) [10] and tests based on
bootstrap sampling [6, 10]. ANOVA (analysis of variance) has been
used for this purpose [15] as well, and it is also commonly used in
user studies.
Other tests that have been used in IR include the χ 2 test, McNemar’s test, and various proportion tests, all of which can be applied
to 2 × 2 contingency tables and are thus useful for classification
tasks; distribution tests such as Kolmogorov-Smirnov and Anderson’s test of normality; correlation tests; and multiple comparisons
adjustments like the Bonferroni correction or Tukey’s HSD. These
are all rare compared to the six mentioned above, however.

3

SIGNIFICANCE REPORTING IN IR

Similar to Sakai in his investigation of power in the IR literature [8],
we obtained all research papers (full papers, short papers, and
poster papers) for all SIGIR conferences from 1995–2014, all ECIR
conferences from 2005–2014, and all CIKM conferences from 20052014. This is a total of 5,792 papers; some additional information
about them is presented in Table 1.
We indexed the papers using indri1 with the Krovetz stemmer
and no stopword list2 . We queried the index using terms and
phrases related to six tests commonly used in IR as well as more
general phrases referring to statistical significance; the queries and
number of papers matching them are shown in Table 2. Fisher’s
exact test seems to have no standard name, which made it more
difficult to search for. In addition, the bootstrap procedure is sometimes used for purposes other than testing, which means some of

SIGNIFICANCE TESTING IN IR

The most common scenario for significance testing in IR is comparing the effectiveness of two automatic retrieval systems over a sample of topics or queries. Three different tests have been suggested for
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.
978-1-4503-5022-8/17/08. . . $15.00
DOI: 10.1145/3077136.3080738

1 http://www.lemurproject.org/indri/
2 We

also verified that indri’s PDF parser was able to successfully parse papers. The
papers it had trouble parsing constitute less than 1% of all papers.

1125

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

0.8
0.6
0.4
0.2

percent reporting significance

SIGIR long papers
ECIR long papers
CIKM long papers

0.0

0.2

0.4

0.6

0.8

SIGIR
ECIR
CIKM

0.0

percent reporting significance

1.0

indri query
matches unique
#ow1(t test)
843
732
#syn(wilcoxon wilcoxons)
372
301
#combine(bootstrap #ow2(bootstrap test))
144
102
anova
132
78
#ow1(sign test)
108
61
#combine(#ow1(exact test) #ow1(randomization test)
58
36
#ow1(permutation test) #ow2(fisher test))
#combine(#ow1(statistical significance) #ow1(statistically significant))
1323
Table 2: Indri queries to find papers that used common statistical significance tests in an index of 5,791 papers published at
SIGIR, ECIR, and CIKM. The bottom row gives the number of papers that matched a general query for statistical significance.
Note that some papers matched one of the individual test queries but not this general query, so it is not equal to the sum of
the number of matches. The last column gives the number of papers that matched that query and none of the others (apart
from the last one).

1.0

test
Student’s t-test
Wilcoxon signed-rank
bootstrap
ANOVA
sign test
Fisher’s exact test

1995

2000

2005

2010

1995

2000

2005

2010

year

1.0

year

0.8
0.6
0.4
0.0

the papers matching that query do not actually use the bootstrap
test. The final line of the table shows the number of papers matching two phrases that often indicate significance. While one of these
phrases appears in most papers that mention a test, they do not
appear in all of them.
Based on Table 2, it seems that most researchers in IR are using
the t-test, with Wilcoxon’s signed-rank test a distant second. But
the biggest takeaway from this table is that, unless we have missed
some major indicator of significance3 , it seems that fewer than 25%
of published papers have reported significance testing.
However, Figure 1 shows that the proportion of papers reporting
significance has trended upward over time, driven primarily by a
steady increase from 2001–2010 leading to around 50% of papers
reporting significance at recent conferences. SIGIR and ECIR are
roughly similar, but CIKM is far below the two of them. This is
most likely due to different standards in CIKM’s three tracks; we
explore this below.
Figure 2 shows the proportion of papers reporting significance
broken out by paper length (short/poster versus long/full). As
expected, short papers tend to report significance less often than
full papers. For SIGIR, about twice as many long papers report
significance as short papers. For ECIR and CIKM, about 50% more
long papers report significance than short papers.

SIGIR short papers
ECIR short papers
CIKM short papers

0.2

percent reporting significance

Figure 1: Proportion of all papers published at SIGIR, ECIR,
and CIKM reporting significance.

1995

2000

2005

2010

year

Figure 2: Proportion of full (top) and short (bottom) papers
reporting significance at SIGIR, ECIR, and CIKM. Note that
SIGIR did not start explicitly calling for short papers until
1999.

CIKM has traditionally had papers submitted to one of three research tracks: IR, databases (DB), or knowledge discovery/management
(KM). Thus our CIKM corpus provides an opportunity to compare
IR to these fields. For all but three years (2005, 2006, and 2011) we
were able to label nearly every paper by track; the proportion of
papers from each track reporting significance is shown in Figure 3.
We note that IR has many more papers reporting significance than
either DB or KM. This is likely not a perfectly fair comparison between fields, as those fields may use different tests than the ones
we use in IR (also, CIKM is rated higher as an IR conference than
either a DB or KM conference4 ), but since it includes vague phrases

3 Other

phrases, such as “significant difference”, are common but often used without
any accompanying statement of test used. Authors use the word “significant” to mean
“large” or “substantial” rather than “statistically significant”.

4 Based

html

1126

on rankings at http://webdocs.cs.ualberta.ca/∼zaiane/htmldocs/ConfRanking.

0.8

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

actual p−values in IR papers
CIKM IR track
CIKM KM track
CIKM DB track

100

0.6

50
0

0.2

density

0.4

simulated p−values

0.0

percent reporting significance

1.0

Short Research Paper

2006

2008

2010

2012

100
50
0

2014

TREC 2012 Web track p−values

year

100

Figure 3: Proportion of CIKM papers reporting significance
by track. Breaks in the lines reflect years for which track
information was not readily available (CIKM 2005, 2006, and
2011).

50
0
0.000

0.075

0.100

Figure 4: Distributions of p-values reported in published IR
papers (top); generated by simulation (middle); and from
paired t-tests between all TREC 2012 Web track runs (bottom).

Why not test significance?

We investigated a sample of papers that did not match any of our
queries in Table 2 to determine whether significance could have
been tested, and if it couldn’t have, why. We sampled around 30
papers uniformly at random from five 4-year spans (1995–1998,
1999–2002, 2003–2006, 2007–2010, and 2011–2014). Based on this
sample, we found that most papers that did not report significance
actually could have (80% of our 150 papers); that is, experiments
were performed and systems compared, but no significance testing
was reported.
The next most common reasons for not reporting significance
were that the paper was entirely theoretical or that the paper was
more of a proposal or proof of concept (this was most common in
short papers). However these two only made up 14% of papers in
our sample, and were more common in the earlier timespans—it has
actually become more common for papers not reporting test results
to be experimental even as experimental papers are more likely
to include test results. The remaining 5% were position papers,
qualitative studies, or difficult to assess.
Over time, the proportion of papers sampled that could have
tested significance rose (from 73% to 95%), while the proportion of
theoretical papers and positions papers fell. As conferences have
required more and more empirical papers, there is less reason to
not test significance. However, these differences are not statistically
significant themselves.

3.2

0.050

p−value

like “statistically significant” that appear in most papers reporting
test results, it is suggestive.

3.1

0.025

some are from other tests and some are from fitted linear models
and other statistical procedures. The distribution is skewed towards
zero, with a little over 50% of p-values less than 0.05.
It is worth asking what the distribution “should” look like, i.e.
what the expected distribution of p-values is in publications in a
field in which there are still discoveries to be made. For this, we will
use the idea of effect size [2, 7, 13]. Effect size refers to the degree
of difference between two treatments (retrieval algorithms) in a
population, independent of the number of samples in a particular
experiment. It is used to estimate the probability that an effect will
be found significant given a certain sample size.
Cohen’s d is a measure of effect size specific to the t-test. In
the behavioral sciences, d < 0.1 is considered a small effect, scientifically not very interesting; 0.1 ≤ d < 0.3 a medium effect; and
0.3 ≤ d < 0.5 a large effect. Small effects occur when the difference
in means is small compared to the variance; they can be significant
with a very large sample (n in the thousands, for instance) even if
they are not scientifically interesting. Large effects occur when the
difference in means is large compared to the variance; large effects
will usually be significant even in small samples (n ≈ 10). Medium
effects typically require on the order of a few dozen samples to be
found significant; test collections in IR consisting of 50–100 topics
are ideal for medium effects.
Cohen’s d is computed as the mean of the difference in effectiveness between two systems divided by the standard deviation
in effectiveness differences. For example, if evaluating by average
precision (AP), we would compute d as:

p-values

Another aspect of significance testing is the p-value produced by
the test. Most papers do not report exact p-values, instead reporting
the threshold at which significance was tested (e.g. p < 0.05) and
whether a result was significant or not. But among papers we
found that did report exact p-values, we transcribed them into a
spreadsheet for analysis.
We found 30 papers that reported a total of 466 exact p-values.
The top histogram in Figure 4 shows the distribution of these 466
p-values. Note that this includes p-values from all tests and experimental settings; most are the results of tests we listed above, but

d=

MAP 1 − MAP 2
std.dev.(AP 1 − AP2 )

It is closely related to the t-statistic used in Student’s t-test:
MAP 1 − MAP 2
√
std.dev.(AP 1 − AP 2 )/ n
√
=d n

t=

where n is the sample size.

1127

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Let us assume the following:

that about 60% of comparisons were significant while 40% were
not. Based on our scanning of papers for this analysis, we would
argue that the larger problem is that most papers do not have
sufficient analysis of negative results that can lead to generalizable
conclusions.

(1) published papers on average present results of 10 t-tests;
(2) a paper must have at least one p-value < 0.05 in order to
be published;
(3) the “true” effect sizes being studied are medium-large, i.e.
Cohen’s d ≈ 0.3.

4

Using these assumptions, we can simulate the p-values that
should appear in published papers: we simply sample 10 t statistics
√
from a t distribution centered at 0.3 n (for some value of n; we
used n = 50 since that is the size of a typical TREC test collection),
then look up those values in a reference t distribution centered at
zero to get 10 p-values. If at least one of these is less than 0.05 we
keep all 10, otherwise we discard the sample.
The middle histogram in Figure 4 shows the result—p-values are
skewed towards zero about as much as they are in actual published
papers, though we find more p-values just above 0 and fewer between 0.025 and 0.05. Differences between the two are most likely
due to the fact that the histogram of actual values conflates many
different tests and experimental settings and designs, while the simulated values assumes a t-test applied to a standard within-subjects
design with 50 topics.
The similarity between the two distributions is suggestive that
there are many medium-large effects being reported in IR, and
hence that there may be many more to be found. This can only
be good for the field: it means there are potentially many more
important results waiting to be discovered.
As another validation, we compare to the p-value distribution in
TREC 2012 Web track results; this is shown in the bottom histogram
in Figure 4. This distribution is similar to the other two as well.
We find nothing in our investigation that would cause us to reject
the hypothesis that published p-values are different than expected
under some reasonable assumptions5 .

3.3

CONCLUSIONS AND FUTURE WORK

We have analyzed 5,792 papers published in IR conferences over
20 years and shown that reporting of statistical significance has
improved over time, but is still lacking. Nevertheless, based on
comparisons across CIKM research tracks, it seems to be substantially better in IR than in other fields. In addition, we have argued
that p-values in IR papers are distributed more or less as expected,
suggesting that researchers are probably “honest” in their computation of tests. This is interesting, as a similar analysis of research in
the biomedical sciences suggested that papers were reporting more
p-values just below 0.05 than should be expected by any reasonable
model [5]. Overall, it seems that reporting of results in IR is rather
good, even if more analysis could be done.
We intend to expand our investigation to include IR journals; it
may be the case that conference publications lagged behind journal publications in reporting significance in the 90’s. In addition,
we would like to compare to other “allied” fields such as NLP and
machine learning in addition to databases and knowledge discovery/data mining.
Acknowledgements This work was supported in part by the National Science Foundation (NSF) under grant number IIS-1350799.
Any opinions, findings and conclusions or recommendations expressed in this material are the authors’ and do not necessarily
reflect those of the sponsor.

REFERENCES
[1] Leonid Boytsov, Anna Belova, and Peter Westfall. 2013. Deciding on an Adjustment for Multiplicity in IR Experiments. In Proc. SIGIR.
[2] Ben Carterette and Mark D. Smucker. 2007. Hypothesis Testing with Incomplete
Relevance Judgments. In Proceedings of CIKM. 643–652.
[3] Cyril W. Cleverdon and J. Mills. 1963. The Testing of Index Language Devices.
Morgan Kaufmann Publishers, 98–110.
[4] David A. Hull. 1993. Using Statistical Testing in the Evaluation of Retrieval
Experiments. In Proceedings of SIGIR. 329–338.
[5] E. J. Masicampo and Daniel R. Lalande. 2012. A peculiar prevalence of p-values
just below .05. The Quarterly Journal of Experimental Psychology 65, 11 (2012).
[6] Tetsuya Sakai. 2006. Evaluating Evaluation Metrics Based on the Bootstrap. In
Proceedings of SIGIR. 525–532.
[7] Tetsuya Sakai. 2014. Designing Test Collections for Comparing Many Systems.
In Proc. CIKM.
[8] Tetsuya Sakai. 2016. Statistical Significance, Power, and Sample Sizes: A Systematic Review of SIGIR and TOIS, 2006–2015. In Proc. SIGIR.
[9] Jacques Savoy. 1997. Statistical Inference in Retrieval Effectiveness Evaluation.
Information Processing and Management 33, 4 (1997), 495–512.
[10] Mark Smucker, James Allan, and Ben Carterette. 2007. A Comparison of Statistical
Significance Tests for Information Retrieval Evaluation. In Proceedings of CIKM.
623–632.
[11] Jean Tague. 1981. The Pragmatics of Information Retrieval Evaluation. Buttersworth, 59–102.
[12] C. J. van Rijsbergen. 1979. Information Retrieval. Butterworths, London, UK.
[13] William Webber, Alistair Moffat, and Justin Zobel. 2008. Statistical Power in
Retrieval Experimentation. In Proceedings of the nth ACM International Conference
on Information and Knowledge Management.
[14] W. J. Wilbur. 1994. Non-parametric significance tests of retrieval performance
comparisons. Journal of Information Sciences 20, 4 (1994), 270–284.
[15] Justin Zobel. 1998. How Reliable are the Results of Large-Scale Information
Retrieval Experiments?. In Proceedings of SIGIR. 307–314.

Negative results

Many fields of science have been criticized for not publishing “negative” results, giving a biased record of what is known and in particular leading to the so-called “publication bias” problem in which
small effects can be published as significant just because one group
found a significant effect by chance, while five others failed to: only
one group gets a publication, while the overall weight of evidence,
if available, would suggest that the effect is not real.
While it is somewhat unclear what is meant by a negative result,
we posit that most IR papers, while written as a description of a
positive result, contain negative results as well. This is because most
papers report on comparisons between methods that had negative
effect or were not statistically significant. The p-values we analyzed
above provide one piece of evidence: if a little over 50% were less
than 0.05, then just under 50% were greater than 0.05, and therefore
wouldn’t qualify as significant by usual standards.
We selected some papers at random to expand this analysis to
those that do not report exact p-values, but just an indicator of
significance (an asterisk, for example). In these papers we found
5 We note that the distributions are statistically significantly different by the
Kolmogorov-Smirnov distribution test. But that can easily be explained by the mismatch between our assumptions and the actuality of taking p -values from experiments
with many different sample sizes, types of tests, experimental conditions, etc.

1128

