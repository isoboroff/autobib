Tutorial

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

From Design to Analysis: Conducting Controlled
Laboratory Experiments with Users
Diane Kelly

Anita Crescenzi

School of Information Sciences
University of Tennessee
Knoxville, TN, USA
dianek@utk.edu

School of Information and Library Science
University of North Carolina at Chapel Hill
Chapel Hill, NC USA
amcc@email.unc.edu
They help us to: (1) examine search behavior (e.g., How does
task complexity impact search behavior?) and the relevance
judgment process (e.g., How does document order impact
relevance assessments?); (2) evaluate system and interface
designs (e.g., Is my new interface any good? Is my query
expansion technique any good?); and (3) test and develop theory
(e.g., To what extent does Information Foraging Theory explain
people’s stopping behaviors in search?).
The controlled laboratory experiment is especially
importantfor examining causal relationships and evaluating
theory. Shadish, Cook and Campbell elucidate criteria for
establishing causal relationships in experiments: an
experimenter must observe the outcome after a manipulation of
the (intended) cause, observe covariation between cause and
effect, and reduce the plausible alternative explanations for the
variation through experimental design and analysis [18].
Experiments also allow for interaction with participants so that
specialized data collection instruments can be used, contextual
aspects can be measured, and additional feedback can be
gathered. Finally, controlled laboratory experiments give
researchers the ability to determine a priori which constructs
they want to study and how they will measure these constructs.
The emphasis is not only on predicting behavior, but also
explaining it.
The inclusion of humans introduces many potential threats to
the internal validity of experiments; fortunately, many of these
threats can be addressed through experimental design
techniques that have been pioneered by researchers from the
behavioral sciences. IR researchers often lack formal training in
the behavioral sciences and have a difficult time understanding
and incorporating this perspective into their IIR experiments.
Educating researchers and students about how to conduct IIR
experiments from a behavioral science perspective is critical if
we want more valid and reliable evaluations of IIR systems and
more basic knowledge about how people interact with IR
systems. In this tutorial, we emphasize the behavioral science
aspects of experimental design. We elaborate our motivations
below.

ABSTRACT
This full-day tutorial provides general instruction about the
design of controlled laboratory experiments that are conducted
in order to better understand human information interaction and
retrieval. Different data collection methods and procedures are
described, with an emphasis on self-report measures and scales.
This tutorial also introduces the use of statistical power analysis
for sample size estimation and introduces and demonstrate two
data analysis procedures, Multilevel Modeling and Structural
Equation Modeling, that allow for examination of the whole set
of variables present in interactive information retrieval (IIR)
experiments, along with their various effect sizes. The goals of
the tutorial are to increase participants’ (1) understanding of the
uses of controlled laboratory experiments with human
participants; (2) understanding of the technical vocabulary and
procedures associated with such experiments and (3) confidence
in conducting and evaluating IIR experiments. Ultimately, we
hope our tutorial will increase research capacity and research
quality in IR by providing instruction about best practices to
those contemplating interactive IR experiments.

CCS CONCEPTS
• Information systems → Information retrieval; Users and
interactive retrieval, Evaluation of retrieval results

KEYWORDS
information retrieval experiments with users; study design; data
collection; quantitative data analysis methods; self-report data

1 BACKGROUND AND MOTIVATION
The controlled laboratory experiment is one of the most
important methods used in interactive information retrieval (IIR)
to study users. Controlled laboratory experiments are useful in a
number of different situations.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or
distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned
by others than the author(s) must be honored. To copy otherwise, or republish, to
post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from Permissions@acm.org.
SIGIR’17, August 07-July 11, 2017, Shinjuku, Tokyo, Japan
©ACM ISBN 978-1-4503-5022-8/17/08…$15.00
http://dx.doi.org/10.1145/3077136.3082063

User Experience Measures and Scales
One of the most crucial types of data in IIR experiments are
user experience measures. These measures are typically gathered
through self-report questionnaires and scales. These types of
measures can be used as dependent variables (e.g., engagement,

1411

Tutorial

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

reviewed forty years of IIR experiments and found the most
common statistical tools used to analyze data were t-tests and
ANOVAs, with little use of modeling techniques that allow for
the investigation of larger numbers of variables in a single
analysis. The convergence on these types of statistical methods
was likely a function of the standard IIR experimental design
that emerged from the 1970s. Both Kelly and Sugimoto [14] and
Toms and O’Brien [19] call for the use of more sophisticated
statistical modeling techniques to better understand IIR.
In our recent work, we demonstrated the usefulness of
multilevel modeling (MLM) as a technique for analyzing data
from IIR experiments [5]. MLM is similar to many other
modeling techniques, such as regression, where there are several
predictor variables and one outcome (or dependent) variable
[17]. MLM allows for analysis of data to multiple levels. In an IIR
context, repeated-measures (e.g., task) would be modeled at
Level 1 and the group variable (e.g., individual) would be
modeled at Level 2. A multilevel model provides multiple
advantages over a repeated-measures ANOVA: (a) the ability to
explicitly model the effect of variables at the task-level (e.g., task
sequence or topic) as well as variables at the individual-level
(e.g., demographics), (b) coefficient estimates indicate the
magnitude, direction and significance of effects of each variable
while partialing-out the effects of other covariates in the model,
and (c) effect size estimates of variance attributable to different
variables. A final advantage of MLM is it allows for easier
analysis of between- and within-subject variables. While one can
run mixed factor ANOVAs, anyone with experience setting-up
these types of analyses appreciates the challenges in doing so,
many of which are specific to IIR studies – for example, there are
many things that almost always vary (e.g., task) even if
researchers are not interested in including such variance in their
experiments.

usability), as well as independent and moderating variables (e.g.,
pre-task difficulty, past knowledge, level of search experience).
These types of data are central to studies of IIR as users are the
only ones who can tell us about their attitudes, feelings and
experiences. Self-report measures offer a critical perspective to
understanding IIR, and when properly constructed, can provide
both a valid and reliable signal.
Researchers often group all self-report measures together and
dismiss them because they are subjective and somehow biased,
or create questions and response sets that introduce
measurement error. It is important to distinguish between
previously validated measures, which have undergone extensive
psychometric testing, and sets of ad-hoc questions researchers
ask that have not been tested. In general, there is a lack of
understanding of psychometric theory in our field and of
techniques for good question/item design, which have been
developed to reduce measurement error [6, 7, 20, 21].
Introducing some of these practices to our field can potentially
increase the quality and reusability of this important class of
measures.

Sample Sizes for Experiments
Another motivation for this tutorial is a general lack of
understanding in our community about how to determine an
appropriate sample size [12]. At present, there are no published
guidelines about how sample sizes are determined in IIR studies.
Instead, researchers often assume more is better and evaluate the
goodness of sample sizes using criteria that were not developed
in the context of controlled laboratory studies. It can be
challenging to infer acceptable sample sizes by scanning the
literature because reported sample sizes vary and researchers
rarely justify their numbers. Furthermore, researchers often
confuse sample sizes needed to generalize results with sample
sizes needed to show causal relationships. These are two
different issues.
Identifying an appropriate sample size is important for a
number of reasons, not the least of which is to avoid making
inappropriate conclusions about one’s research ideas. It is
common for researchers to claim a study without significant
findings was “underpowered,” meaning there were not enough
participants to detect significance. Underpowered studies are
associated with larger risks for Type II errors. Thus, an
underpowered study primary affects the researcher and the state
of research in a field since it potentially inhibits discovery.
Identifying a sample size that gives the researcher the greatest
potential for finding differences if they are present without
overspending is an important issue.

2 AUDIENCE AND OBJECTIVES
The audience for this tutorial includes students and
researchers who are interested in conducting laboratory
experiments with human participants.
We will assume
participants have had little formal experience learning about
experimental design in the context of laboratory user studies, but
have a basic understanding of some terminology such as
variables and hypotheses. With respect to the data analysis
section, we will assume participants have a basic understanding
of descriptive and inferential statistics, and hypotheses testing.

Provide general instruction about the design of
controlled laboratory IIR experiments with human
participants.

Increase participants’ understanding of the technical
vocabulary and procedures associated with IIR
experiments.

Describe different data collection methods and procedures,
with an emphasis on self-report measures and scales.

Introduce the use of statistical power analysis for sample
size estimation in experiments with users.

Data Analysis Techniques
A final motivation for this tutorial is that in the past,
researchers have faced challenges analyzing all aspects of an
experimental in a single model, including (1) experimental
variables, (2) potentially ancillary variables like task number and
task order, (3) demographics variables, (4) process variables like
search behaviors and (5) outcome variables like user experience
measures and performance measures. Kelly and Sugimoto [14]

1412

Tutorial





SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

using experimental design to control for unwanted effects,
random assignment to conditions and other techniques for
maximizing internal validity will be emphasized.
In the next section of the tutorial, we will introduce various
components of IIR experiments [13]. We will start by discussing
search tasks, as these are a required part of a laboratory user
study [3, 4, 15, 22]. We will discuss the advantages and
disadvantages of using open or closed collections. We will also
introduce a number of data collection techniques [11, 13]. Special
attention will be given to self-report measures. Psychometric
theory will be briefly described to help participants better
distinguish between measures that have demonstrated validity
and reliability, and measures that are comprised of ad-hoc sets of
questions that may or may not be valid [6, 21]. Several relevant
scales will be presented and participants will be provided with
some guidelines for constructing their own items. We will cover
methods for reliability assessment for data reduction purposes
(i.e., collapsing multiple variables into a composite variable).
This step is especially critical for data analysis so as not to
increase the risk of Type I errors. It also simplifies analysis and
presentation issues.
In the final part of the morning session, we will discuss one
of the most important components of IIR experiments: research
participants. We will introduce statistical power analysis for
sample size estimation, which helps researchers consider sample
size and the risks associated with Type I and Type II errors [1,
16]. In doing so, we will explain effect size and introduce one
tool for performing power analysis, G*Power, which is freely
available online [9]. We will also distinguish between sample
size and representativeness, which people often conflate, and
explain why it is not just a numbers game.
In the afternoon session, we walk through the process of
analyzing an experimental study using the dataset from one of
our own studies to provide a concrete example [5]. The study
used a mixed experimental design with between- and withinsubjects factors in which participants completed multiple search
tasks.
In the first part of the afternoon, we will introduce the
dataset and its variables, their operational definitions, how they
were measured and their ranges. The dependent variables in the
dataset are self-reported responses to post-task questionnaire
and multiple questions were asked for each construct of interest.
The independent variables include those experimentally
manipulated as well as covariates that may also influence the
dependent variable (demographics such as age and education
level; task variables such as order and topic; pre-task
questionnaire responses such as expected difficulty and topic
interest; search interactions such as task time and number of
queries).
Next, we will show, interpret and compare the results from a
series of statistical tests selected to illustrate how to analyze
repeated measurements (e.g., multiple tasks completed by each
participant) with experimental variables and covariates at the
task level (e.g., topic) and at the individual level (e.g.,
participant’s prior search experience). The statistical tests will
focus on multilevel regression models [17] as well as repeated-

Introduce and demonstrate two data analysis procedures,
Multilevel Modeling [17] and Structural Equation
Modeling [2], that allow for examination of the whole set
of variables present in IIR experiments, including their
various effect sizes.
Increase participants’ confidence in understanding and
conducting IIR experiments.

3 RELEVANCE
The laboratory experiment with users is one of the primary
methods used to understand search behavior and evaluate
systems and interfaces. It is also one of the best ways to test and
develop theories and hypotheses. During the past few years,
there has been a huge growth in the number of researchers who
are interested in studying and understanding users. This can be
seen in the increases in IIR-focused papers and workshops at
SIGIR, as well as the creation of the new SIGIR-sponsored
Conference on Human Information Interaction and Retrieval
(CHIIR). We believe our tutorial will increase research capacity
and research quality in this area by providing instruction about
best practices to those contemplating their own IIR experiments.
We also believe the data analysis section of the tutorial will
highlight new data analysis tools to those already familiar with
IIR experiments.

4 TOPICS AND ORGANIZATION
The basic approach of this tutorial is to use the morning
session to present issues relating to experimental design using
published studies to illustrate experimental design decisions and
their impact. In the afternoon, we will use a dataset from a
published study [5], the design of which will be discussed in the
morning session, to illustrate different approaches to
experimental data analysis.
The tutorial will start by putting the IIR experiment in
context. Specifically, we will discuss its history and legacy as one
of the most essential methods for understanding IR interactions
and evaluating systems [13]. We will then compare and contrast
this approach with other common approaches, and identify and
describe research scenarios for which the laboratory experiment
is a good choice and those for which it is not [8]. Next, we will
introduce some factors that make laboratory experiments with
users challenging, including individual differences in relevance
judgments, the iterative nature of searching, and search tasks [3,
13].
Various kinds of experimental designs, including within- and
between- subjects designs and factorial designs will be presented
[10, 13]. The relationship between experimental design and
one’s research questions and hypotheses will be discussed, with
examples drawn from actual studies. Different types of
hypothesis will also be briefly reviewed including null,
alternative, directional and non-directional since they are
connected to data analysis. The concepts of Type I and Type II
errors will also be reviewed since they provide the foundation
for some of the material we will present later, in particular,
sample sizing and statistical power [1, 9, 16]. The importance of

1413

Tutorial

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

REFERENCES

measures analysis of variance (ANOVA) and covariance
(ANCOVA).
We will also examine multiple methods for analyzing a
construct of interest measured by multiple questions: using
multiple tests, one for each question; creating a composite
variable;and statistical tests for analyzing a dependent variable
with multiple measures. Statistical tests covered include
repeated-measures multivariate analysis of variance (RMMANOVA) and covariance (RM-MANCOVA) as well as a brief
introduction to structural equation modeling focusing on
confirmatory factor analysis and structural equations with latent
variables [2]. We will finish with best practices for reporting
experimental study results.

[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]

5 PRESENTER BIOGRAPHIES
Diane Kelly is Professor and Director of the School of
Information Sciences at the University of Tennessee. Her
research and teaching interests are in interactive information
search and retrieval, information search behavior, and research
methods. Kelly is the recipient of the 2014 ASIST Research Award
and the 2013 British Computer Society’s IRSG Karen Spärck Jones
Award. She is the recipient of two teaching awards: the 2009
ASIST/Thomson Reuters Outstanding Information Science Teacher
Award and the 2007 UNC SILS Outstanding Teacher of the Year
Award. She is the current ACM SIGIR chair and was co-chair of
the inaugural ACM SIGIR Conference on Human Information
Interaction and Retrieval (CHIIR). She is an associate editor for
Transactions on Information Systems, and serves on the editorial
boards of Information Processing & Management, Information
Retrieval Journal and Foundations and Trends in Information
Retrieval.
Anita Crescenzi is a PhD student at the School of
Information and Library Science at the University of North
Carolina at Chapel Hill. Her research interests are in interactive
information retrieval, information behavior, decision-making
and research methods. Crescenzi also has a special interest in
statistical analysis methods including multilevel modeling and
structural equation modeling. Crescenzi has over eight years of
experience in applied user research, usability evaluation and user
experience design in the health insurance and medical library
domains. She has also taught pre-algebra and social studies to
middle school students as well as conducted technical and
software training classes for adults. She has reviewed papers for
the SIGIR conference and CHIIR, and was a member of the 2016
CHIIR local organizing committee.

[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]

1414

Bausell, R. B. & Li, Y. F. (2002). Power analysis for experimental research: A
practical guide for the biological, medical and social sciences. New York:
Cambridge Press.
Bollen, K. (1989). Structural equation modeling with latent variables. New
York: Wiley & Sons.
Borlund, P. (2003). The IIR evaluation model: A framework for the evaluation
of interactive information retrieval systems. Information Research, 8(3), paper
152.
Byström, K. & Hansen, P. (2005). Conceptual framework for tasks in
information studies. JASIST, 56(10), 1050–1061.
Crescenzi, A., Kelly, D. & Azzopardi, L. (2016). Impacts of time constraints and
system delays on user experience. To appear in Proc. of CHIIR 2016, 141-150.
DeVillis, R. F. (2012). Scale development: Theory and applications. Thousand
Oaks, CA: Sage Publications.
Dillman, D. A., Smyth, J. D., & Christian, L. M. (2009). Internet, Mail and
rd
Mixed-Mode Surveys (3 Edition). John Wiley & Sons, Inc.: Hoboken, NJ.
Dumais, S., Jeffries, R., Russell, D. M., Tang, D. & Teevan, J. (2014).
Understanding user behavior through log data and analysis. In J.S. Olson &
W. Kellogg (Eds.), Human Computer Interaction Ways of Knowing. NY:
Springer.
Faul, F., Erdfelder, E., Lang, A.-G., & Buchner, A. (2007). G*Power 3: A flexible
statistical power analysis program for the social, behavioral, and biomedical
sciences. Behavior Research Methods, 39, 175-191.
Field, A. & Hole, G. (2003). How to Design and Report Experiments. Thousand
Oaks, CA: Sage Publications.
Hornbæk, K. (2013). Some whys and hows of experiments in human–
computer interaction. Foundations and Trends in Human-Computer Interaction,
5(4), 299-373.
Kelly, D. (2015). Statistical power analysis for sample size estimation in
information retrieval experiments with users (tutorial). Proceedings of ECIR
2015, 822-825.
Kelly, D. (2009). Methods for evaluating interactive information retrieval
systems with users. Foundations and Trends in Information Retrieval, 3(1-2).
Kelly, D. & Sugimoto, C. R. (2013). A systematic review of interactive
information retrieval evaluation studies, 1967 -2006. JASIST, 64(4), 745-770.
Li, Y. & Belkin, N. J. (2008). A faceted approach to conceptualizing tasks in
information seeking. IP&M, 44, 1822-1837.
Murphy, K. R. (2009). Statistical power analysis: A simple and general model
for traditional and modern hypothesis tests (3rd edition). New York:
Routledge.
Rabe-Hesketh, S. & Skrondal, A. (2012). Multilevel and longitudinal modeling
using Stata. Stata Press, College Station, TX, 3rd edition.
Shadish, W., Cook, T. & Campbell, D. (2002). Experimental and QuasiExperimental Designs for Generalized Causal Inference. Wadsworth Cenage
Learning.
Toms, E. & O’Brien, H. (2009). The ISSS Measurement Dilemma. IEEE
Computer, 42(3), 48.
Tourangeau, R., Rips, L. J., & Rasinski, K. (2000). The Psychology of Survey
Response. Cambridge University Press.
Spector, P. E. (1992). Summated rating scale construction: An introduction.
Thousand Oaks, CA: Sage Publications.
Wildemuth, B. W., Freund, L. & Toms, E. G. (2014). Untangling search task
complexity and difficulty in the context of interactive information retrieval
studies. Journal of Documentation, 70(6), 1118-1140.

