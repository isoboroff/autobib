Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Detecting Positive Medical History Mentions
Bing Bai∗

Pierre-Francois Laquerre†

Richard Jackson‡

Robert Stewart§
made by domain experts and statistical learning models trained on
labeled data. However, both methods tend to be difficult to apply
to datasets that are different from the training data.
One of the most important types of information is the medical history. Effectively extracting information such as previous
diagnoses and treatment would greatly reduce a doctors’ time in
reading EMRs, and much work have been devoted to the extraction
of disorder mentions from doctors’ notes [12, 16].
In this paper, we focus on identifying patients’ disorders from
unstructured text. We not only extracted the disorder mentions, but
also made distinctions between instances applied to patients and
others. In EMR, a disorder can be mentioned for various reasons:
1) a disorder the patient has; 2) a disorder that the patient does not
have; 3) a disorder affecting someone other than the patient (e.g.
a family member); 4) other mentions of disorders in the EMR (e.g.
as a potential side effect). Integrating this distinction in a disorder
extraction system would represent an important step forward in
refining the extracted disorder information, and help doctors or
researchers more effectively characterize a patient’s previous health
experiences.
Our system included a simple rule-based module and a machine
learning-based module. We used string kernels with support vector
machines (SVM) [19] on raw note text which required little human
intervention (e.g. setting up gazetteers for disorders of interest). In
a study on a large mental healthcare EMR database, we obtained
good accuracy scores for most of the disorders we tested, and the
application performed well on unseen datasets.

ABSTRACT
In medical practice, knowing the medical history of a patient is
crucial for diagnosis and treatment suggestion. However, such
information is often recorded in unstructured notes from doctors,
potentially mixed with the medical history of family members and
mentions of disorders for other reasons (e.g. as potential sideeffects). In this work we designed a scheme to automatically extract
the medical history of patients from a large healthcare database.
More specifically, we first extracted medical conditions mentioned
using a rule-based system and a medical gazetteer, then we classified whether such mentions reflected the patient’s history or not.
We designed our method to be simple and with little human intervention. Our results are very encouraging, supporting the potential
for efficient and effective deployment in clinical practice.

KEYWORDS
electrical health record, medical history, string kernels, skip-gram

1

INTRODUCTION

Electronic Medical Records (EMRs) are becoming the standard
repository for healthcare information. Large volumes of invaluable
information in EMRs can potentially help medical scientists, doctors and patients improve the quality of care. However, these large
volumes can also make it very hard to extract information of interest, and manual record screening is tedious, tiring and error-prone.
There is therefore a pressing need for better analytical systems.
A major challenge in EMR mining is that a significant amount of
data is in unstructured natural text. Analysis of such data heavily depends on the unsolved problem of natural language understanding.
Compounding this, medical text frequently contains incomplete
sentences, jargon, idiosyncratic measurements and abbreviations
that make it hard to use general purpose natural language process
(NLP) modules such as sentence splitter and syntactic and semantic
parsing. There is also development of specialist end-to-end systems
for EMR text analysis (e.g. [17]), usually with a combination of rules
∗ bbai@nec-labs.com,

2

RELATED WORK

There are a number of conferences or workshops focusing on
EMR analysis. For example, CLEF eHealth [12], SemEval clinical tasks[16] hold annual events, trying to integrate research to
solve certain problems in EMR mining. Common topics like medical
name entity recognition have been extensively studied. Medical
record data has historically been difficult to access because of privacy issues. This is changing since the release of large de-identified
datasets like MIMIC1 and i2b2 2 . The availability of large datasets
will certainly encourage more research and development on EMR
mining.
Besides the specialized EMR topics, there is also a long history
of end-to-end systems for EMR analytics. Information extraction
systems such as cTAKES [17], MedLee [8], MetaMap [1] can speed
up notes reviewing by extract important information from unformatted notes. These systems usually take advantage of the medical
terminology database UMLS [4]. There are also more ambitious
goals for higher level machine intelligence. For instance, IBM has
devoted significant resources in developing the medical version of

NEC Labs America, INC.
Google INC. This work was conducted when in NEC

† pierre.francois@gmail.com,

Labs America, INC.
‡ richard.r.jackson@kcl.ac.uk, King’s College London.
§ robert.stewart@kcl.ac.uk, King’s College London.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.
978-1-4503-5022-8/17/08. . . $15.00
DOI: http://dx.doi.org/10.1145/3077136.3080717

1 https://mimic.physionet.org/

2 https://www.i2b2.org/NLP/DataSets/Main.php

1049

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

question answering system Watson [7], which claimed fame in the
2013 Jeopardy contest.

choices of n = 2 (bigram), or n = 3 (trigram) give best results in
document classification.
String Kernels. We explore string kernels that are widely used in
sequence analysis [15]. String kernels usually implement heuristics
on how two sequences should be similar. Sparse Spatial String
Kernels (SSSK) [13] got us the best results we have (to be shown
in the experiment section). In such kernels, the kernel function for
two sequences X and Y can be defined as:

3 METHOD
3.1 Problem Setting
We define in this paper that a “disorder mention” (or simply “mention”) is an instance of a disorder name occurred in a medical record.
We are trying to identify medical disorder mentions whose subject
is the patient. As we indicated in the introduction, there are quite
different reasons a disorder is mentioned in notes. In this work,
we only consider two cases: 1) Positive: The patient had or is having the disorder, 2) Negative: all other cases. This is a simple but
effective measure to identify the disorder mention of interest.
Our method has two steps. In the first step, we use off-the-shelf
information extraction engines GATE [5] with customized gazeteers to mark possible mentions (for example, keywords “asthma”,
“asthmatic”, and other descriptive phrases can be used in the gazeteer of the disorder asthma). In the second step, we classify whether
the marked mentions are positive. In this paper, we assume the first
step works well, and focus on building machine learning models
for the second step.

3.2

K (t,k,d ) (X ,Y ) =

X
a i ∈Σk ,0≤d i <d

C X (a 1 ,d 1 , ...,at −1 ,dt −1 ,at )·
CY (a 1 ,d 1 , ...,at −1 ,dt −1 ,at )

(1)

where ai are k-grams, separated by di < d words in the sequence,
and C X and CY are counts of such units in X and Y respectively.
To give an example, suppose t = 2, k = 1, and d = 2, we have
two sequence X = “ABC” and Y = “ADC”. We can see the count
C X (“A”, 1, “C”) = 1 and CY (“A”, 1, “C”) = 1, thus K (1,1,2) (X ,Y ) =
1 ∗ 1 = 1.
We also explore a new variation of SSSK, in which the distance
requirements are relaxed as follows:
(t,k,d )

Kr

Classification

(X ,Y ) =

X
a i ∈Σk ,0≤d i ,d i0 <d

3.2.1 SVM with Kernel Functions. We do not discuss Support
Vector Machines (SVM) in details, due to page limit and abundance
of literature (e.g. [18]). In short, SVM is a classification method
that proved very flexible and powerful by customizing kernel functions K (x i ,x j ) for data samples x i and x j . A kernel function can
be viewed as the similarity between two data samples. The classification of a new data sample is based on its similarity (kernel
function value) with learned landmark data samples (support vectors). Knowledge about a specific problem can be integrated into
a kernel function to allow SVM to find rather sophisticated class
boundaries. Being able to leverage kernel functions is an important reason that SVM is one of the most popular machine learning
methods.

C X (a 1 ,d 1 , ...,at −1 ,dt −1 ,at )·
CY (a 1 ,d 10 , ...,at −1 ,dt0−1 ,at )

(2)
In other words, in Eq.1, K (1,1,2) (“ABC”, “AC”) = 0, but in its relaxed
(1,1,2)
version Eq.2, Kr
(“ABC”, “AC”) = 1. Intuitively, this adaptation
will enable the model to match phrases like “her mother had …”,
and “her mother earlier had …”.
Interestingly, when using SVM, the above SSSK kernel method
is equivalent to “skip-grams” [10] features with a linear kernel,
when k = 1. Similar to n-grams, a “skip-gram” is a sequence of
n words. The difference from n-gram is that skip-grams allow
intervals between words: a 2-skip-gram for the sequence “The cat
sat on the mat” can be “sat mat”, when 2-gram can only allow
consecutive words like “sat on” or “the mat”.

4 EXPERIMENTS
4.1 Dataset and experiment setup

3.2.2 Feature Extraction. We then build features for each mention. Most information needed to classify the mention is typically
around the mention. In our experiments, we found that taking
a fixed-length window worked better than taking a sentence or
a paragraph. The window includes 20 words before and after the
mention, unless it reaches the beginning or the end of the document.
Formally, we define the window to be

We used a medical record database for patients with mental disorders. All records were de-identified, with the name entities replaced
by special tokens like “YYYYY” or “ZZZZZ”. We extracted 500 to
1000 mentions on each of the following disorders: asthma, arthritis,
stroke, diabetes, and myocardial infarction, and manually labeled
them with either “Positive” or “Negative”.
As shown in Table 1, the distribution of labels varies between
different disorders. For asthma, most mentions are positive. On
the other hand, most mentions of strokes are about family history
rather than about the patient. Overall the dataset slightly favors
positive cases (56% being positive). However, Myocardial Infarction
is dominated by negative cases. We looked at these cases, and
found that one of the acronyms of Myocardial Infarction is “MI”
was also used as an acronym of “Mental Illness”, which caused many
wrong instances be extracted by the GATE module, and in turn be
labeled as “negative” by human labelers. This problem needs to be
addressed in future work.

W = [wb ,wb+1 ,wb+2 , ...,wm , ...,w e−2 ,w e−1 ,w e ],
where w i is the i-th word in the document, b = max(m − 20, 0), and
e = min(m + 20, N ), where m is the index of the mention and N is
the length of the document.
Bag of Words (BOW). The most straightforward feature scheme
is “bag-of-word” (BOW): each word in window W is a separate
feature. The information on the order of words is discarded, thus
making it a “bag”. A simple yet powerful variation of BOW is the
n-gram model. That is: we take each sequence of n words in W to
be a single feature. There have been studies [2] that show common

1050

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 1: Statistics on experimental datasets

Asthma
Arthritis
Diabetes
Stroke
Myocardial Infarction
All

Total
964
745
981
1090
575
4355

Positive
769
631
509
441
120
2470

Table 3: Testing accuracy. Trained on 4 other disorders, and
tested on the listed disorder.

Negative
195
114
472
649
455
1885

Asthma
Arthritis
Diabetes
Stroke
Myocardial Infarction
Average

Table 2: Testing accuracy. Trained on all disorders (80% of
each disorder merged), and tested on the rest 20% of each
disorder

Asthma
Arthritis
Diabetes
Stroke
Myocardial Infarction
Average

1-gram
0.878
0.858
0.829
0.834
0.771
0.834

2-gram
0.887
0.873
0.824
0.864
0.830
0.860

3-gram
0.895
0.874
0.851
0.870
0.824
0.863

Number of Train Disorders
1
2
3
4

SSSK
0.901
0.877
0.845
0.869
0.821
0.863

3-gram
0.801
0.820
0.739
0.790
0.708
0.772

SSSK
0.807
0.816
0.745
0.782
0.708
0.773

1-gram
0.597
0.683
0.724
0.743

2-gram
0.603
0.705
0.750
0.770

3-gram
0.605
0.707
0.752
0.772

SSSK
0.603
0.701
0.748
0.773

reasonably well, with the SSSK achieving the best score, although
the difference is minor.
In Table 4, we show that, as we add more disorders to the training
set, the performance on unseen disorders goes up.

4.4

Analysis of learned models

Besides the quantitative results, here we also analyze the models and
try to interpret the results. We list most positive and most negative
n-gram/skip-gram features in Table 5. For each set of features,
on the left are the features with largest learned weights, and on
the right the smallest. “DISO” is the masked disorder mention, as
indicated in 4.1. Note in the column SSSK, there could be up to d
words between the presented words (see Eq. 2).
We can get quite intuitive features even using 1-gram features.
For example, words “mother”, “father” are strong indicator that this
might be the condition of family members. Also, “died” is most
likely not about the patient, since these are the ongoing record of
the patient.
If we look at the 3-gram features, we can see many useful features
that were missing in the 1-gram case. “her/his DISO” are likely to be
positive because the mention of family history would probably not
going to details that using words of “his/her”, also it is clear that the
mention is not a negation of the patient having that condition. “or
DISO” is among top negative features because it is often in a family
history list: “… nobody in family had diabetes, or DISO …”. The word
“history” are seen here in both positive feature “medical history”,
and negative feature “family history”, which were not caught in the
1-gram features sets. Also note that there is no actual 3-grams in
the top 20 3-gram features since the list 1-gram or 2-gram features
turned out to be more significant for the model.
The SSSK features looks similar to the 3-gram results. However
we can still notice features that were missing in the latter. For
example: “medical history DISO” can be in slightly different forms
in records: “medical history : DISO”, or “medical history: asthma,
DISO”. Since 3-gram features require consecutive words, they will

Training/testing on the same disorders

We first study the performance on the same disorders that were
present in the training set. We split cases of each disorder into 80%
training and 20% testing.
The corresponding accuracies are in Table 2. Note the average
are micro average, meaning we first calculate the score of each
disorder, then take average of those scores.
We can see that in this case, the SSSK ties the performance of
3-gram.

4.3

2-gram
0.806
0.847
0.705
0.783
0.634
0.770

Table 4: Testing accuracy. Trained on different number of
disorders, and tested on the rest.

To extract features for mention classification, we take a surrounding window according to section 3.2.2. Then we build the
n-gram features or skip-gram features for equivalent SSSK from
this window. The mention in the middle is replaced with a constant
token “DISO”, forcing the learned model focus on the context rather
than the name of the disorder, thus making it more generalizable
to untrained disorders. For n-gram models, higher n-grams is a
superset of lower n-grams. For examples, 3-gram features include
all 2-grams and 1-grams. For SSSK/skip-gram features, we set t = 3,
k = 1, and d = 2.
All our results are tested on liblinear [6], with model hyperparameters selected using grid search, with different model types and
cost coefficients selected using grid search,

4.2

1-gram
0.771
0.846
0.715
0.757
0.593
0.743

Training/testing on different disorders

We now turn to the more important problem of generalization to
disorders that were not in the training set. After all, it is impractical
to collect training data for each medical disorder. So in practice, we
will certainly want mentions of new disorders to be classified.
In Table 3, we listed the results of disorder-wise cross validation.
That is, we train on 4 disorders, and test the remaining disorder. The
first column are the disorders tested. We can see the model works

1051

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 5: The most positive and most negative features learned with different feature schemes.
1-gram
Positive Negative
medical
mother
inhalers
died
able
or
since
father
prior
feels
type
epilepsy
although
who
controlled
family
allergies
believes
control
dad
history
no
physical
denied
previous
brother
left
stroke
which
maternal
sleep
possibly
she
staff
harm
check
diagnosed
insulin
childhood
risk

3-gram
Positive
Negative
her DISO
died
his DISO
mother
since
or
inhalers
father
able
who
medical history
or DISO
although
a DISO
medical
: integer
prior
DISO unit
prior to
brother
diagnosed
DISO or
type
believes
left
her mother
DISO control
feels
controlled
that this
allergies
staff
history
DISO /
control
stroke
),
family history of
previous
denied

miss the matching of these type of flexible forms of features. Thus,
although in this dataset the SSSK models didn’t manage to get significantly better results than 3-grams, we still think it has potential
to outperform n-grams in other datasets.

4.5

Negative
a DISO
mother
died
or
a DISO
father
: integer
or DISO
, or
DISO or
who
her mother
brother
DISO or
feels
stroke
staff
a DISO .
believes
/

[5] H. Cunningham, V. Tablan, A. Roberts, and K. Bontcheva. 2013. Getting More
Out of Biomedical Documents with GATE’s Full Lifecycle Open Source Text
Analytics. PLoS Comput Biol 9, 2 (2013).
[6] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008. LIBLINEAR: A Library for Large Linear Classification. Journal of Machine
Learning Research 9 (2008).
[7] David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek,
Aditya A. Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John Prager,
Nico Schlaefer, and Chris Welty. 2010. Building Watson: An Overview of the
DeepQA Project. AI Magazine (2010).
[8] C. Friedman. 2000. A broad-coverage natural language processing system. Proceedings of the AMIA symposium (2000).
[9] Stephan Gindl. 2006. Negation Detection in Automated Medical Applications.
Technical Report. Vienna University of Technology.
[10] David Guthrie, Ben Allison, Wei Liu, Louise Guthrie, and Yorick Wilks. 2006. A
CLoser Look at Skipgram Modelling. In LREC.
[11] Rie Johnson and Tong Zhang. 2015. Semi-supervised Convolutional Neural
Networks for Text Categorization via Region Embedding. In NIPS.
[12] Liadh Kelly, Lorraine Goeuriot, Hanna Suominen, Tobias Schreck, Gondy Leroy,
Danielle L. Mowery, Sumithra Velupillai, Wendy W. Chapman, and David Martinez. 2014. Overview of the ShARe/CLEF eHealth Evaluation Lab 2014. Lecture
Notes in Computer Science 8685 (2014).
[13] Pavel P. Kuksa and Vladimir Pavlovic. 2010. Spatial Representation for Efficient
Sequence Classification. In International Conference on Pattern Recognition (ICPR).
[14] John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional
Random Fields: Probabilistic Models for Segmenting and Labeling Sequence
Data. In ICML.
[15] Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cristianini, and Chris
Watkins. 2002. Text Classification using String Kernels. Journal of Machine
Learning Research 2 (2002).
[16] Sameer Pradhan, Noemie Elhadad, Wendy Chapman, Suresh Manandhar, and
Guergana Savova. 2014. SemEval-2014 Task 7: Analysis of Clinical Text. In 8th
International Workshop on Semantic Evaluation (SemEval 2014).
[17] Guergana K Savova, James J Masanz, Philip V Ogren, Jiaping Zheng, Sunghwan
Sohn, Karin C Kipper-Schuler, , and Christopher G Chute1. 2010. Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture,
component evaluation and applications. J Am Med Inform Assoc 17, 5 (2010).
[18] Bernhard Scholkopf and Alexander J. Smola. 2001. Learning with Kernels. MIT
Press.
[19] Vladimir Vapnik. 1998. Statistical Learning Theory.

Additional experiments

We also tried 1) “RBF kernels”, “polynomial kernels” as alternative
to the linear kernels and SSSK, 2) domain adaptation as in [3], 3)
Conditional Random Field (CRF) [14] (sequence labeling “positive
mention” or otherwise), 4) deep learning methods as in [2, 11],
5) extra features with negation detection NegEx [9], and 6) EHR
analysis toolkit cTAKES [17]. We did not include the results for
their unsatisfying results. We think these methods, though with
good potentials, are not necessarily as powerful for a small training
set as the proposed methods.

5

SSSK
Positive
her DISO
his DISO
since
inhalers
medical history
medical
although
able
her DISO .
),
since DISO
. medical
. medical history
medical history DISO
control
controlled
following DISO
prior
type
diagnosed

CONCLUSION

We presented a system that extracts positive disorder mentions
from unstructured text. The method we used is simple, effective,
interpretable and has good capability to generalize. We believe it
can be a useful component in a EMR analysis system.

REFERENCES
[1] Alan R Aronson and Franois-Michel Lang. 2010. An overview of MetaMap:
historical perspectiveand recent advances. JAMIA 17 (2010), 229–236.
[2] Dmitriy Bespalov, Bing Bai, Yanjun Qi, and Ali Shokoufandeh. 2011. Sentiment
Classification Based on Supervised Latent n-gram Analysis. In CIKM.
[3] John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, Bollywood,
Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification. In
ACL.
[4] Olivier Bodenreider. 2004. The Unified Medical Language System (UMLS): integrating biomedical terminology. Nucleic Acids Research 32 (2004).

1052

