SIRIP 3: Research at Large-scale Search Engines

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Find Shoes Like These
Hideyuki Maeda
Yahoo Japan Corporation
hidmaeda@yahoo-corp.jp

ABSTRACT

the relevance to real users’ search intents that adopts image indexing based on Euclidean image embedding.

We present an Euclidean embedding image representation, which
serves to rank auction item images through wide range of semantic similarity spectrum, in the order of the relevance to the given
query image much more effective than the baseline method in terms
of a graded relevance measure. Our method uses three stream deep
convolutional siamese networks to learn a distance metric and we
leverage search query logs of an auction item search of the largest
auction service in Japan. Unlike previous approaches, we define
the inter-image relevance on the basis of user queries in the logs
used to search each auction item, which enables us to acquire the
image representation preserving the features concerning user intents in real e-commerce world.

2 TRIPLET LOSS
At the training time, three stream deep convolutional siamese networks input triplet of 256 × 256 fixed size RGB images of auction
items, and minimize the triplet loss defined as follows:
L=

N [
∑

p

∥ f (x ia ) − f (x i )∥22 − ∥ f (x ia ) − f (x in )∥22 + α

i

]
+

(1)

The embedding is represented by f (x ) ∈ IRd (d ∈ Z+ ), where x is
an image and d is the dimension of the representation. Here x ia is
p
the anchor image, x i is the positive image and x in is the negative
image of the i th triplet. α denotes the margin. This loss function
trains d-dimensional representations to ensure that the positive is
closer to the anchor than the negative.
Triplets are prepared by using search query logs of Yahoo! JAPAN
Auction service. All images are annotated by the query in response
to which the auction item containing the image is clicked more
than five times; if there are more than one queries, we adopt the
longest query on the basis of the number of tokens, e.g. keywords
separated by a whitespace. Let V be the vocabulary of query tokens and x, an image, the bag of tokens of thus selected annotation
query, w (x ) ⊆ 2V is used as the descriptor of the image.
Unlike classification tasks where the goal is to learn a class separation hyper-surface, image representations for ranked retrieval
are preferably able to discriminate every images throughout the semantic space of the target domain, e.g. “red Ferragamo pumps” are
closer to “blue Ferragamo pumps” rather than simple “blue pumps”.
Although image metadata is widely used for the preparation of
image examples, we adopted queries used to search auction items
where the focus of user interests in item search is well represented.
We sample a triplet based on partial matching of bags of tokens: assume an image x ia as anchor of the i t h triplet, we sample
p
two images where the positive image x i shares more common tokens with the anchor than the negative image x in so that an image
with the descriptor, {“shoes”, “brand A”, “red”} is closer to that with
{“shoes”, “brand A”, “blue”} rather than {“shoes”, “blue”}. Depending upon whether considering partial matching as positive or not,
we examine two types of P/N triplet definition in the hope that
the partial matching enables the ranker a smooth ranking through
wider range of semantic similarity spectrum.

1 INTRODUCTION
Due to the wide usage of smart devices with a high definition camera, Content Based Image retrieval (CBIR), or search by image either from a SNS site or taken by the camera of user’s device becomes useful means to specify their information needs in the image
search. When you find a favorite celebrity wearing a pretty sexy
jumpsuit on Instagram, you might want to buy something similar
for yourself or your partner, by seeking an item on an e-commerce
or auction site. Despite the recent breakthrough in various image processing tasks by deep learning approaches, CBIR is still a
challenging task due to the difficulties in identifying users’ search
intents from the image. In the above example, the system would
not understand, from your query image, that you are interested
neither in the celebrity’s scandal story nor in her hair style but in
her clothes.
While a general purpose CBIR faces serious difficulties in detecting image regions relevant to search user intent, shopping/auction
item search systems are able to identify user’s intents by applying search log based methods frequently used by query suggestion
techniques in web search. We propose a search by image system
for a large scale auction service, that directly learns a mapping
from auction item images to a low-dimensional Euclidean embedding by a triplet loss function, where the L 2 distance function represents dissimilarity between auction item images. Because of the
difficulty in preparing a large number of triplets, there is no previous work addressed such issues while we leverage the bags of
tokens of search queries in the logs as annotations of images, representing real search user intents. To the best of our knowledge,
this is the first large scale evaluation of search by image based on

3 TALK OUTLINE

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the owner/author(s).
SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 Copyright held by the owner/author(s). 978-1-4503-5022-8/17/08.
DOI: 10.1145/3077136.3096466

In our talk, we present our triplet loss based CBIR, experimented
on our auction search service, along with the applications of such
triplet loss based embedding techniques to knowledge graph completion tasks and open domain QA tasks.

1363

