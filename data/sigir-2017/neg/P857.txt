Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Centered kNN Graph for Semi-Supervised Learning
Ikumi Suzuki

Kazuo Hara

Yamagata University
Yonezawa, Yamagata, Japan
suzuki.ikumi@gmail.com

National Institute of Genetics
Mishima, Shizuoka, Japan
kazuo.hara@gmail.com

ABSTRACT

For SSL to be efficient and robust to noise, sparsity of the graph
is required [2]. A sparse graph commonly used for graph-based
SSL is the k-nearest neighbor (kNN) graph. However, the kNN
graph likely contains hub nodes (i.e., nodes connected by many
other nodes) and they affect label propagation, which is performed
subsequently on the graph. Recently, Ozaki et al. [4] reported that
the mutual kNN graph reduces the number of hub nodes, and de
Sousa et al. [1] stated that the mutual kNN graph, at present, is the
best choice for graph construction.
While the mutual kNN graph is considered state-of-the-art at
present, we point out that the mutual kNN graph, which is usually a
very sparse graph, suffers from over sparsification problem. Namely,
although the number of edges connecting nodes that have different
labels favorably decreases in the mutual kNN graph, the number
of edges connecting nodes that have the same labels also reduces.
In addition, over sparsification can produce a disconnected graph,
which hinders label propagation on the graph.
In this paper, we present a new graph construction method,
which not only reduces hub nodes but also solves the over sparsification problem. The new method, which constructs a graph called
centered kNN graph, does not remove edges from the kNN graph in
order to reduce hub nodes as in the case of a mutual kNN graph.
Instead, before constructing the kNN graph, the new method revises similarities between samples so that hubs are not produced, by
using a technique known as centering. In the section of experiments,
we demonstrate that, when using the centered kNN graph, the label propagation works well, and the accuracy of label prediction
improves, compared with the mutual kNN graph.

Graph construction is an important process in graph-based semisupervised learning. Presently, the mutual kNN graph is the most
preferred as it reduces hub nodes which can be a cause of failure
during the process of label propagation. However, the mutual kNN
graph, which is usually very sparse, suffers from over sparsification
problem. That is, although the number of edges connecting nodes
that have different labels decreases in the mutual kNN graph, the
number of edges connecting nodes that have the same labels also
reduces. In addition, over sparsification can produce a disconnected
graph, which is not desirable for label propagation. So we present
a new graph construction method, the centered kNN graph, which
not only reduces hub nodes but also avoids the over sparsification
problem.

CCS CONCEPTS
• Information systems → Clustering and classification;

KEYWORDS
graph-based SSL, mutual kNN graph, over sparsification problem

1

INTRODUCTION

Semi-supervised learning (SSL) is a class of supervised learning
techniques that can make use of unlabeled data for training when
a small amount of labeled data is available. SSL is known to work
well even without label information if the unlabeled data can be
used for finding data boundaries in a feature space, or if cluster or
manifold structures can be recovered from the dataset.
One of the mainstream techniques for SSL follows a graph-based
approach, wherein the samples are represented as nodes in a graph.
The graph is constructed such that the manifold of the data is
captured, and the information of labeled samples is propagated to
target samples along the structure of the manifold, i.e., along the
edges of the constructed graph, to predict their labels.
Graph-based SSL methods consist of two parts. (1) Constructing
a graph without using label information, and (2) Propagating label
information on the graph. In this paper, we focus on the first part.
We present a new method to construct a graph that is appropriate
for label propagation.

2

NOTATION OF GRAPH-BASED SSL

Assume that we are given a set of d-dimensional feature vectors
n with their labels Y = {y ∈ S }n , where n is
X = {x i ∈ Rd }i=1
i
i=1
the number of samples and S is a set of possible labels. Among
l
the n samples, we can only access to the labels {yi ∈ S }i=1
⊂Y
of l samples (i.e., labeled data). In addition, a similarity or kernel
function (including cosine similarity and Gaussian kernels) f :
Rd × Rd 7→ R is defined for a pair of samples, and a dense matrix W
of size n × n is given, where the its ijth-component Wi j = f (x i , x j ).
More formally, we assume f (x i , x j ) = ⟨φ(x i ), φ(x j )⟩, i.e., the
inner-product on a Hilbert space, where φ(x) is a mapping function
of a sample x ∈ X to the Hilbert space. Thus, Wi j = ⟨φ(x i ), φ(x j )⟩.
n
The goal is to predict the labels {yi ∈ S }i=l
⊂ Y of n − l
+1
samples (i.e., unlabeled data) which we cannot observe, using X and
l . To achieve this goal, the graph-based SSL constructs a
{yi ∈ S }i=1
weighted graph G from X, where the n samples correspond to the
nodes in the graph. A construction of graph G is equivalent to a
selection of an adjacency matrix A that determines the weight of
the edges in the graph G.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan.
© 2017 Association for Computing Machinery.
ACM ISBN 978-1-4503-4887-4/17/08. . . $15.00
https://doi.org/10.1145/3077136.3080662

857

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

0

0

φ( )
φ( )

φ( )

1

3

2

0

origin
origin

φ( )

φ( )

φ( )

φ( )

φ( )

origin

0

4

0

5

0
4

1

1 1

1

0

2

2

1

2

0 1 2 3 4

0 1 2 3 4

(a) An example dataset on a Hilbert space (an
inner-product space)

1

1

(b) Hubness occurs when the origin is away
from the dataset center.

(c) Hubness disappears after we move the
origin to the center.

Figure 1: An intuitive explanation of why the centering reduces hubs. (a)(b) When the origin is located on the left side of the
dataset, most of the 1NN relations shown in red arrows tend to direct from left to right. Hence, some samples on the rightmost
side become hubs with a large number of incoming arrows. (c) In contrast, when the origin is in the dataset center, the red
arrows do not point in a certain direction.
Here, it seems that setting A = W is the simplest choice. However,
because similarity scores in W for dissimilar sample pairs are often
unreliable, using all the components of a dense matrix W leads to
low accuracy in predicting labels. Thus, it is desirable to construct
a sparse graph from W [2, 4], and the most common one is the kNN
graph.
In the following, we review existing sparse graphs used for graphbased SSL, and then, we will present the centered kNN graph, which
we consider appropriate for label propagation.

matrix V is symmetrized, which is carried out by
U = min(V , V T ).

Note that any node in the mutual kNN graph has a degree (i.e.,
the number of edges that the node has) less than or equal to k,
implying that hub nodes do not exist. Ozaki et al. [4] attributed the
improvement in prediction accuracy when using the mutual kNN
graph to this point.
On the other hand, we notice that the procedure to construct the
mutual kNN graph involves removing edges from the standard kNN
graph. In other words, the mutual kNN graph is constructed by
sparsifying the standard kNN graph. Considering that the adjacency
matrix A of the standard kNN graph is obtained by sparsifying the
similarity matrix W , the mutual kNN graph can be considered as a
doubly-sparsified graph.
Although the mutual kNN graph is rendered free from hub nodes
through double-sparsification, we point out that such strong sparsification not only removes harmful edges connecting nodes that have
different labels, but also removes desirable edges connecting nodes
that have same labels. Furthermore, over sparsification sometimes
produces a disconnected graph, which affects label propagation.
Therefore, in the next section, we present a method to construct a
graph that contains few hub nodes but is not very sparse.

3 EXISTING GRAPHS FOR SSL
3.1 The kNN Graph
The process to construct the kNN graph involves three steps. First,
for a given similarity matrix W , the k most similar samples for each
of n samples are determined, and a sparse binary matrix V of size
n × n is generated. Formally, Vi j = 1 if the ith sample is one of the
k most similar samples of the jth sample according to the similarity
scores in W , and Vi j = 0 otherwise. Next, by using a max operation,
the matrix V is transformed into a symmetry matrix U , which is
given by
U = max(V , V T ).

(1)

Finally, a weighted adjacency matrix A for the kNN graph is obtained for the ijth component as

4

Ai j = Ui j Wi j .

THE CENTERED KNN GRAPH

Let I be the identity matrix of size n × n and 1 be an n-dimensional
all-ones vector. A centered similarity matrix is computed from W ,
in the form
1
1
W Cent = (I − 11T )W (I − 11T ).
(3)
n
n
Using the procedure described in Section 3.1, we generate a standard
kNN graph from W Cent instead of W . The resulting graph is called
the centered kNN graph.
The purpose of the centering operation in Equation (3) is to reduce hubs. We provide an intuitive explanation by using a small

Note that, if the dimension d of the feature space is high, the kNN
graph constructed from W tends to contain hub nodes that have
an unexpectedly high degree (i.e., nodes connected by many other
nodes), and the existence of hub nodes affects label propagation on
the graph [4].

3.2

(2)

The Mutual kNN Graph

The procedure to construct the mutual kNN graph is similar to
that of the standard kNN graph. The difference lies in the way the

858

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

kNN

MkNN

kNN

kNN cent

MkNN

kNN cent

kNN

0.5
0.4

0.4

20

40

60

0.35
0

20

k

0.3
0.28

40

60

0

20

40

60

k

k

(a) Mini Newsgroups

kNNcent

0.32

0.26
0

MkNN

0.34

Accuracy

0.6

Accuracy

Accuracy

0.45

(b) Reuters Transcribed

(c) WSD

Figure 2: Accuracy of label prediction using the standard (kNN), mutual (MkNN), and centered (kNN cent) kNN graphs for
varying size of k.
dataset D = {x 1 , . . . , x 8 } consisting of eight samples. Figure 1(a)
shows the dataset mapped onto a Hilbert space as {φ(x 1 ), . . . , φ(x 8 )}.
To construct the kNN graph, we find the k most similar samples
for each sample x ∈ D. Similarity is measured by the inner-product
on the Hilbert space.
For the standard kNN graph, when k = 1, the sample most similar
to x i , denoted by 1NN(x i ), is determined as

5

We compared the centered kNN graph with the state-of-the-art
mutual kNN graph, as well as the standard kNN graph as a baseline.
We evaluated the accuracy of label prediction in two multi-class
classification tasks: document classification and word sense disambiguation (WSD).
Datasets and SSL Settings: For the document classification task,
we used the Reuters Transcribed and Mini Newsgroups datasets.2
The number of documents (i.e., n) is 201 and 2000, and the number
of classes (i.e., |S |) is 10 and 20, respectively.
For the WSD task, we used the data for the Senseval-3 ELS task,
which is a collection of 57 datasets for different polysemous words
[3]. Averaged over 57 datasets, 207.1 occurrences (n = 207.1) of a
target word are annotated from 6.4 gold standard senses (|S | = 6.4)
of the target word. For each dataset, we created a tf-idf weighted
vector for each occurrence of the target word from its surrounding
context, and we computed W using cosine similarity.
After constructing the standard, mutual, and centered kNN graphs
using the similarity matrix W , we ran the Local and Global Consistency [6], one of the state-of-the-art label propagation algorithms,
with a smoothing parameter µ = 0.9, following [4].
To simulate SSL in which only a small amount of labeled data
is available, we selected l samples from the original dataset and
used these as labeled data, where l was 5, 10, 15, or 20% of the total
number of samples.3 The remaining n − l samples were used as
unlabeled data, whose labels form the prediction target. For each
dataset, we randomly selected l samples 10 times, and reported the
results obtained by taking the average over all trials.

1NN(x i ) = argmax ⟨φ(x i ), φ(x)⟩ = argmax ⟨ui , φ(x)⟩,
x ∈D\{x i }

x ∈D\{x i }

where ui is the unit vector of φ(x i ). That is, the sample most similar
to x i is the sample x ∈ D \ {x i } such that the mapped vector φ(x)
has the largest component of the vector ui . Thus, in Figure 1(a),
1NN(x 1 ) = x 7 .
Figure 1(b) shows a directed graph where each sample x i is
connected to 1NN(x i ) by a red arrow. Note that if we transform it
to an undirected graph (i.e., ignore the direction of the arrows), we
obtain the standard kNN graph.
We note that there is a common direction in the arrows in Figure 1(b): most of the arrows tend to direct from the origin to the
data center, i.e., from left to right in this case. Consequently, a few
samples with a large number of incoming arrows, i.e., hub samples,
emerge on the side right to the origin. In Figure 1(b), the number
attached to a sample denotes the number of incoming arrows to the
sample (i.e. the reverse kNN count). Moreover, their histogram is
shown. The histogram is not symmetric (it is skewed to the right),
and this indicates the occurrence of hubness [5].
In contrast, the centered kNN graph is obtained using Equation (3), equivalently by moving the origin to the data center c =
1 Í ′
φ(x ′ ), as in Figure 1(c). We notice that the red arrows
|D | x ∈D
in Figure 1(c) do not have a common direction (i.e., we obtain more
scattered k nearest neighbors) and the histogram becomes symmetric. This implies that hubness is reduced, at least for single-cluster
data.1
Notably, in contrast with the mutual kNN graph, the centered
kNN graph is not very sparse. This is because the max operation in
Equation (1) is used, rather than the min operation in Equation (2).
Consequently, the sparsity level of the centered kNN graph is the
same as that of the standard kNN graph.

Results: The label prediction accuracy is compared for the Mini
Newsgroups, Reuters Transcribed, and WSD datasets, in Figure 2(a),
2(b) and 2(c), respectively. Because the WSD data consists of 57
datasets, the averaged accuracy over all datasets is displayed in
Figure 2(c). These figures show that in most cases, the centered
kNN graph achieves better accuracy than the other graphs.

2

https://archive.ics.uci.edu/ml
Because the same trend was observed, only the figures when l = 0.10 × n are
displayed in Figure 2.
3

1

EXPERIMENTS

For multi-cluster data, we may consider more than one origins for different clusters.

859

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Skewness

15
10
5

5

10

20

30

10

0

-10
-0.1

0
1

graphs in terms of two properties: hubness, and the F1 score regarding correct edges.4
First, we evaluate the emergence of hub nodes, which affects label
propagation and hence deteriorates the performance, by measuring
the skewness of the node degree (i.e., the number of edges incident
to a node) distribution. A large skewness indicates strong hubness
[5].
For the WSD data, the skewness is shown in Figure 3(a). We
have computed the skewness for each of the 57 datasets in the WSD
data, and have presented the distribution of the skewness in the
form of box plots. Figure 3(a) shows that the centered kNN graph
successfully reduces the hub nodes. However, as for the mutual
kNN graph, skewness tends to increase compared to the standard
kNN graph when k is small (i.e., k ≤ 10). This is because, owing
to the over sparsification, the mutual kNN graph often contains
small disconnected components including isolated nodes, and the
hubness appears as a result of creating a connected graph using the
maximum spanning tree algorithm [4].
Next, we discuss the reason behind the occasional drop in accuracy of the mutual kNN graph, as in the case of Mini Newsgroups
in Figure 2(a). For each of the kNN graphs, we counted the number
of correct edges connecting samples bearing the same labels. We
then calculated its ratio to the number of edges in the kNN graph
(i.e., precision), and also calculated its ratio to the number of correct edges in the complete graph where every pair of samples is
connected (i.e., recall). Figure 4 shows the precision, recall, and F1
scores for the correct edges in the standard, mutual, and centered
kNN graphs. The figure indicates that although precision is improved in the mutual kNN graph, recall is considerably deteriorated
owing to the over sparsification of the graph. As a result, the F1
score for the correct edges could become very small, and hence, it
can be considered that the mutual kNN graph makes the accuracy
worse for the Mini Newsgroups.
In contrast, the centered kNN graph reduces hub nodes without
changing F1 score for correct edges significantly, which can be
considered a desirable property for successful label propagation.

mutual kNN - standard kNN
centered kNN - standard kNN

kNN
MkNN
kNNcent

40

-0.05

0

0.05

F1 score

k

(a) Hubness

(b) Hubness and F1

Figure 3: (a) Hubness measured by the skewness of node degree distribution in the standard, mutual, and centered kNN
graphs computed for WSD datasets. (b) A scatter plot of 57
WSD datasets for comparison between the mutual and centered kNN graphs (i.e., difference of skewness and F1 score
against the standard kNN graph, when k = 5).

0.5
0.4
0.3

0.25
0.2

Recall

0.6

precision

0.15

kNN
kNNcent
MkNN

0.1

F1

0.7

0.15
0.1

0.05

0.05
20

40

0

60

20

k

40

0

60

20

k

40

60

k

0.5

0.4

0.4

0.3

0.3
0.2

0.25
0.2
0.15

F1

Recall

Precision

(a) Mini Newsgroups

0.2

0.1
0.1

0.1
40

60

kNN
kNNcent
MkNN

0.05

0
20

0
20

k

40

60

20

k

40

60

k

(b) Reuters Transcribed
kNN
kNNcent
MkNN

Recall

0.5
0.45

0.3

0.3

0.2

0.2

ACKNOWLEDGMENTS
We would like to thank anonymous reviewers for their helpful
comments. This work was supported by JSPS KAKENHI Grant
Numbers JP16K00066 and JP16H02821.

F1

0.55

Precision

Skewness of degree dist

20

0.1

0.1

0

0

0.4

REFERENCES

0.35
20

40

60

k

20

40

k

60

20

40

[1] Celso Andre Rodrigues de Sousa, Solange Oliveira Rezende, and Gustavo EnriqueAlmeidaPradoAlves Batista. 2013. Influence of Graph Construction on
Semi-supervised Learning. In ECML/PKDD. 160–175.
[2] Tony Jebara, Jun Wang, and Shih-Fu Chang. 2009. Graph Construction and
B-matching for Semi-supervised Learning. In ICML. 441–448.
[3] Rada Mihalcea, Timothy Chklovski, and Adam Kilgarriff. 2004. The Senseval-3
English lexical sample task. In Senseval-3. 25–28.
[4] Kohei Ozaki, Masashi Shimbo, Mamoru Komachi, and Yuji Matsumoto. 2011.
Using the Mutual k-Nearest Neighbor Graphs for Semi-supervised Classification
on Natural Language Data. In CoNLL. 154–162.
[5] Miloš Radovanović, Alexandros Nanopoulos, and Mirjana Ivanović. 2010. On
the existence of obstinate results in vector space models. In SIGIR. 186–193.
[6] Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Schölkopf. 2004. Learning with local and global consistency. In NIPS.
595–602.

60

k

(c) WSD
Figure 4: Precision, Recall and F1 score for the correct edges
in the standard (kNN), mutual (MkNN), and centered (kNNcent) kNN graphs.

6

DISCUSSION

We now discuss why the centered kNN graph performed better
in these experiments, by examining the quality of the constructed

4

Due to lack of space, we have not shown the skewness for the document classification
tasks, in which both the centered and mutual k NN graphs have reduced hubness.

860

