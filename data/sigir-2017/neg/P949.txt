Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

A Comparative Live Evaluation of Multileaving Methods on a
Commercial cQA Search
Tomohiro Manabe

Akiomi Nishida

Makoto P. Kato

Yahoo Japan Corporation
Chiyoda-ku, Tokyo, Japan 102-8282
tomanabe@yahoo-corp.jp

Yahoo Japan Corporation
Chiyoda-ku, Tokyo, Japan 102-8282
anishida@yahoo-corp.jp

Graduate School of Informatics,
Kyoto University
Sakyo-ku, Kyoto, Japan 606-8501
kato@dl.kuis.kyoto-u.ac.jp

Takehiro Yamamoto

Sumio Fujita

Graduate School of Informatics,
Kyoto University
Sakyo-ku, Kyoto, Japan 606-8501
tyamamot@dl.kuis.kyoto-u.ac.jp

Yahoo Japan Corporation
Chiyoda-ku, Tokyo, Japan 102-8282
sufujita@yahoo-corp.jp

ABSTRACT

a crucial shortcoming: as different rankings are examined by different users with different search intents, A/B testing requires a large
number of page views to reach a conclusion on the effectiveness
of subjects. Alternative ways include interleaving or multileaving.
Interleaving methods combine two search result rankings [5, 6],
whereas multileaving methods combine more than two rankings
[9]. The quality of each input ranking is measured by user clicks
observed on the combined ranking. Since different rankings are
compared by the same user in interleaving or multileaving, we can
evaluate multiple rankings more efficiently than A/B testing [1].
Currently, there are four well-known interleaving methods, three
of which have multileaving counterparts: balanced interleaving
(BI) [4], team draft interleaving/multileaving (TDI/TDM) [6, 9], optimized interleaving/multileaving (OI/OM) [5, 9], and probabilistic
interleaving/multileaving (PI/PM) [2, 8]. They are promising as they
can evaluate multiple rankings at a time. However, they have been
studied mainly on the basis of simulation where user clicks were
artificially generated by simple models. Thus, the evaluation accuracy and efficiency of multileaving methods with real users need to
be understood in order to apply them to production environments
in which more complicated models are expected.
This paper describes the first attempt to examine their performance on one of the world’s largest commercial cQA search services. We introduce practical implementations of TDM and OM
and answer the following research questions in a real service:

We present one of the world’s first attempts to examine the feasibility of multileaving evaluation of document rankings on a large
scale commercial community Question Answering (cQA) service.
As a natural enhancement of interleaving evaluation, multileaving
merges more than two input rankings into one and measures the
search user satisfaction of each input ranking on the basis of user
clicks on the multileaved ranking. We evaluated the adequateness
of two major multileaving methods, team draft multileaving (TDM)
and optimized multileaving (OM), proposing their practical implementation for live services. Our experimental results demonstrated
that multileaving methods could precisely evaluate the effectiveness of five rankings with different quality by using clicks from
real users. Moreover, we concluded that OM is more efficient than
TDM by observing that most of the evaluation results with OM
converged after showing multileaved rankings around 40,000 times
and an in-depth analysis of their characteristics.

CCS CONCEPTS
•Information systems →Evaluation of retrieval results; Retrieval effectiveness; Information retrieval;

KEYWORDS
QA search and retrieval; Interleaving and multileaving; Evaluation
of document ranking

1

RQ 1. How precisely can we evaluate the effectiveness of rankings
by using multileaving methods in a live test of a commercial service?

INTRODUCTION

Evaluating search result rankings on the basis of implicit feedback
from a large number of real users is a fascinating way to improve the
effectiveness of search services. For this purpose, A/B testing has
been widely conducted in search services. However, A/B testing has

RQ 2. How fast do the results converge? How stable are they?
RQ 3. Which multileaving method is more effective and efficient?
Our experimental results demonstrated that multileaving methods could precisely evaluate the effectiveness of five rankings by
using clicks from real users. Moreover, we concluded that OM is
more efficient than TDM, with an observation that most of the
evaluation results with OM converged after showing the combined
rankings around 40,000 times and an in-depth analysis of their characteristics. Note that we did not compare PM in our experiments
due to its property: PM can generate a combined ranking that has

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00
DOI: http://dx.doi.org/10.1145/3077136.3080687

949

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Algorithm 1: Team Draft Multileaving (TDM)
Require : Input rankings I, number of output rankings m,
and number of items in each output ranking l
1 O ← {} and for k = 1, . . . , m, j = 1, . . . , n do Tk, j ← {};
2 for k = 1, . . . , m do
3
for i = 1, . . . , l do
4
Select j randomly s.t. |Tk, j | is minimized;
5
r = 1;
6
while I j,r ∈ O k do r ← r + 1;
7
if r ≤ |I j | then
8
O k,i = I j,r and Tk, j ← Tk, j ∪ {I j,r };
9
end
10
end
11
O ← O ∪ {O k };
12 end
13 return O;

Algorithm 2: Optimized Multileaving (OM)
Require : Input rankings I, number of output rankings m,
and number of items in each output ranking l
1 O ← {};
2 for k = 1, . . . , m do
3
for i = 1, . . . , l do
4
Select j randomly;
5
r = 1;
6
while I j,r ∈ O k do r ← r + 1;
7
if r ≤ |I j | then
8
O k,i = I j,r ;
9
end
10
end
11
O ← O ∪ {O k };
12 end
13 return O;

significantly worse quality than any of the input rankings, which
is not a preferable feature for a live test of an operating service.
The remainder of the paper is organized as follows. Section 2
explains the details of TDM and OM. Section 3 describes the settings
of our live test, and presents the experimental results. Finally,
Section 4 concludes the paper by summarizing our contributions.

to O k the item at the highest rank that is not in O k yet (Lines 5–9).
The appended item is also added to the team (Tk, j ) of I j (Line 8).
Note that I j,n+1 here is defined as null for convenience.
The credit of TDM is given to an input ranking corresponding
to a team that holds a clicked item and is defined as: δ (O k,i , I j ) = 1
if O k,i ∈ Tk, j ; otherwise, 0. The presentation probability pk is the
same for all the output rankings, i.e. pk = 1/m.

2

MULTILEAVING METHODS

A multileaving method takes a set of rankings I = {I 1 , I 2 , . . . , In }
and returns a set of combined rankings O = {O 1 , O 2 , . . . , Om },
where each combined ranking O k consists of l items. The i-th items
of an input ranking I j and an output ranking O k are denoted by I j,i
and O k,i , respectively. When a user issues a query, we return an
output ranking O k with probability pk and observe user clicks on
O k . If O k,i is clicked by the user, we give a credit δ (O k,i , I j ) to each
input ranking I j . Each multileaving method consists of a way to
construct O from I, probability pk for each output ranking O k , and
a credit function δ . The original multileaving methods decide which
input ranking is better for each input ranking pair every time it
presents an output ranking, whereas we opt to accumulate the credits through all the presentations and to measure the effectiveness
of each input ranking on the basis of the sum of the credits, mainly
because this approach must provide more informative evaluation
results. We next explain the details of TDM and OM.

2.1

2.2

Optimized Multileaving

OM [9] is a multileaving method that generates output rankings
by Algorithm 2, which is almost the same as Algorithm 1 except
for Lines 1, 4, and 8. Furthermore, OM computes the presentation
probability pk that maximizes the sensitivity of the output rankings
while ensuring no bias. The sensitivity of an output ranking is
the power to discriminate effectiveness differences between input
rankings when the output ranking is being presented to users.
Intuitively, the sensitivity is high if random clicks on an output
ranking give a similar amount of credits to each input ranking. High
sensitivity is desirable as it leads to fast convergence of evaluation
results. Bias of output rankings measures the difference between
the expected credits of input rankings for random clicks. If the bias
is high, a certain input ranking can be considered better than the
others even if only random clicks are given. Thus, multileaving
methods should reduce the bias as much as possible.
The sensitivity can be maximized by minimizing insensitivity,
defined as the variance of credits given by an output ranking O k
through rank-dependent random clicks [9]:
Í

2
Í
l
σk2 = nj=1
(1)
i=1 f (i)δ (O k,i , I j ) − µ k ,

Team Draft Multileaving

TDM [9] outputs a combined ranking by randomly selecting an
input ranking, choosing an item from the input ranking that has not
been added to the output ranking yet, and appending the item to
the output ranking. An input ranking obtains a certain credit if an
item attributed to the input ranking is clicked by a user. Although
the original TDM generates an output ranking every time a query is
issued, we generate m output rankings offline and randomly choose
one of them online to reduce the query latency.
As shown in Algorithm 1, TDM generates a new output ranking
O k of length l by randomly choosing an input ranking I j that has
appended the least number of items into O k (Line 4) and appending

where f (i) is the probability with which a user clicks on the ith item. We follow the original work [9] and use f (i) = 1/i
and δ (O k,i , I j ) = 1/i if O k,i ∈ I j ; otherwise, 1/(|I j | + 1). The
mean credit µ k of the output ranking O k is computed as: µ k =
Í Í
(1/n) nj=1 li=1 f (i)δ (O k,i , I j ). Since each output ranking O k is
presented to users with probability pk , OM should minimize the
Í
expected insensitivity: E[σk2 ] = m
p σ 2.
k =1 k k

950

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 1: Comparison of nDCG@10 scores of input rankings.
Statistical significance (p < 0.05) by t-test are marked by ∗.

The bias of output rankings O is defined as the difference between
expected credits of input rankings. The expected credit of an input
ranking I j given rank-independent random clicks on top-1, top-2,
· · · , and top-r items is defined as follows:
Í
Ír
E[д(I j , r )] = m
p
δ (O k,i , I j ).
(2)
k =1 k i=1

Method
1. pop
2. rel
3. qa
4. cat
5. irrel

If the expected credits of input rankings are different, evaluation
results obtained by multileaving is biased. Thus, the original version
of OM imposes a constraint that the expected credits of all the input
rankings must be the same, i.e. (∀r , ∃c r , ∀j) E[д(I j , r )] = c r .
According to the paper by Schuth et al. [9] and their publicly
available implementation1 , their version of OM tries first to satisfy
the constraint for letting the bias be zero, and then to maximize
the sensitivity given that the bias constraint is satisfied. However,
we found that the bias constraint cannot be satisfied for more than
90% of the cases in our experiment, i.e. we could not find any
solution that satisfied the bias constraint. Hence, we propose using
a more practical implementation of OM that minimizes a linear
combination of the sum of biases and the insensitivity as follows:
Í
Í
minpk α lr =1 λr + m
p σ2
(3)
k=1 k k
Ím
subject to k =1 pk = 1, 0 ≤ pk ≤ 1 (k = 1, . . . , m), and

2. rel
3. qa
4. cat
5. irrel

Difference from
2. rel
3. qa

4. cat

-0.004
-0.029∗
-0.036∗
-0.063∗

-0.025∗
-0.032∗
-0.060∗

-0.028∗

-0.007
-0.035∗

(a) Team Draft (TDM)
1. pop 2. rel 3. qa 4. cat
1185 ∗
1187 ∗ 2
1697 ∗ 512 ∗ 510 ∗
1649 ∗ 464 ∗ 462 ∗ −48

(b) Optimized (OM)
1. pop 2. rel 3. qa 4. cat
977∗
1360∗ 382∗
1854∗ 876∗ 494∗
1767∗ 789∗ 407∗ −87

answer (qa), (4) BM25 score of category labels of the question (cat),
and (5) ascending order of the topical relevance (irrel). Note that
tied documents are ranked in descending order of their timestamps.
As the prior work did, we needed another evaluation methodology for measuring the accuracy of evaluation results obtained by the
multileaving methods. We evaluated the rankings by nDCG@10 [3],
Í10 дi
where DCG@10 = i=1
(2 − 1)/log(1 + i), and the grade of the
i-th document is given by дi ∈ {0, 1, 2, 3}. In our experiments, we
estimated дi on the basis of a clickthrough log of the same service,
rather than relevance judgments from assessors, as we aimed to
evaluate rankings on the basis of real user preferences. We used
a simple position based click model to estimate the grade of each
question [7]. Note that we estimate the grade on the basis of a
clickthrough log between Nov. 2014 and Nov. 2016, much longer
than the live test period. Table 1 shows the evaluation results of
the five input rankings. We used ∗ to mark the pairs for which the
differences were found statistically significant (p < 0.05) by paired
t-test. The results suggest that users prefer questions that have
more answers and are more topically relevant to the queries.
Finally, we generated multileaved rankings from the five input
rankings for each query using both TDM and OM methods in the
same ratio each, and examined them in the live test, where we set
100 to m. Therefore, the number of examined rankings for each
query accounts for 200 in total. The hyper-parameter α was set to 1
because we had no knowledge about a reasonable value. We carried
out the live test from Nov. 25 2016 to Jan. 29 2017. We presented
multileaved rankings for a particular portion of search requests and
received 210,389 page views of multileaved search results.

EXPERIMENTS

To answer our research questions, we conducted a live test of Yahoo!
Chiebukuro, a community-based commercial QA search service in
Japan, which claimed 714 million page views in October 2014.

3.1

1. pop

Table 2: Difference between the sum of credits. Statistical
significance (p < 0.05) by t-test are marked by ∗.

∀r , ∀j, j 0, −λr ≤ E[д(I j , r )] − E[д(I j 0 , r )] ≤ λr ,
where α is a hyper-parameter that controls the balance between
the bias and insensitivity, and λr is the maximum difference of the
expected credits in any input rankings pairs. If λr is close to zero,
the expected credits of input rankings are close, and accordingly, the
bias becomes small. Also, our implementation is publicly available2 .
In summary, we developed an evaluation method on the basis
of accumulated credits, offline sampling of output rankings for
TDM, and a solvable optimization problem for OM for enabling
multileaving methods to be practically used.

3

nDCG
@10
0.605
0.601
0.576
0.569
0.541

Evaluation Methodologies

Similar to a web search, the cQA search service receives keyword
queries and returns ranked questions in decreasing order of their
query relevance scores. From its search query logs, we pooled
queries that were issued five or more times on two or more days
in Oct. 2016, from which we randomly chose 1,062 queries for the
live test. As for questions to be ranked, we selected ten or more
questions for each query from the current production search results,
which account for 19,637 questions in total. We set 10 to l, which
is the length of output rankings, since after this position in the
ranking, the click probability of items steeply decreases, which
would make the evaluation procedure inefficient.
We adopted five methods to rank the questions for each query:
(1) Descending order of the number of answer postings (pop), (2) Descending order of human-annotated topical relevance between the
query and question (rel), (3) BM25 score of the question and its best

3.2

Results and Discussions

Table 2 shows the difference between the sum of credits obtained by
the methods of each column and row. “∗” denotes the statistically
significant differences between credits given to two input rankings,
found by paired t-test (p < 0.05). Both methods correctly evaluated
the effectiveness of all the pairs, except for cat and irrel. Statistically
significant differences were indicated for nine pairs with OM, but
only for eight pairs with TDM. Note that it may not be a failure of

1 https://github.com/djoerd/mirex
2 https://github.com/mpkato/interleaving

951

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Team Draft Multileaving (TDM)

Team Draft Multileaving (TDM)

Optimized Multileaving (OM)

6

1.0

5

0.8
Insensitivity

p-value

4

0.6

0.4

3

2

1

0.2
0

0.0
−1
−0.2

Optimized Multileaving (OM)
1.0

pop-rel
pop-qa
pop-cat
pop-irrel
rel-qa
rel-cat
rel-irrel
qa-cat
qa-irrel
cat-irrel

p-value

0.8

0.6

0.4

20

30

40

50

0.4

0.6

0.8

1.0

1.2

Bias

−0.2

0.0

0.2

0.4

0.6

0.8

1.0

1.2

Bias

of expected credits are smaller with OM than TDM thanks to OM
directly minimizing both the bias and the insensitivity.

4

CONCLUSION

We compared two promising multileaving methods, team draft multileaving (TDM) and optimized multileaving (OM), on a large scale
commercial community Question Answering (cQA) search service.
We introduced some practical implementations of TDM and OM to
apply them to production environments. We compared the methods
for answering our three research questions (RQs). As an answer
to RQ1, our experimental results demonstrated that both multileaving methods could precisely evaluate the effectiveness of five
rankings of different quality by using clicks from real users. Finally,
as answers to RQ2 and RQ3, we conclude that OM performs more
efficiently than TDM on the basis of the observation that most OM
results converged after having presented the combined rankings
around 40,000 times and the analyses of bias and sensitivity.

0.0
10

0.2

Figure 2: Scatter plot of bias-insensitivity tradeoff. Horizontal axis shows bias, and vertical axis shows insensitivity,
where each dot corresponds to a query.

0.2

0

0.0

60

Days passed

Figure 1: Time series of p-values. Each line corresponds to a
pair of two input ranking methods. Horizontal axis shows
number of days passed from beginning of live test.

multileaving to indicate statistically significant differences which
are not in Table 1 since their numbers of samples (paired nDCG@10
scores or credits) are different. Figure 1 shows the time series of
p-values obtained by each multileaving method for all the pairs of
input rankings. As shown in the figure, OM indicated statistically
significant differences (p < 0.05) of eight out of ten pairs after one
week from the beginning (bottom). The evaluation result of OM
between rel and qa converged (p < 0.05) on the 13th day, or with
around 40,000 page views. In contrast, TDM indicated differences
of only six out of ten pairs (top) after a week and showed unstable
evaluation results for rel-qa and cat-irrel throughout the live test.
In a simulation study [9], Schuth et al. observed that OM converged
slightly faster than TDM, whereas TDM achieved more accurate
evaluation results than OM. Unlike their simulation, our live test
showed that the evaluation results of OM converged much faster
than TDM while achieving the same evaluation results as TDM.
We try to explain these results on the basis of their bias and
insensitivity. Figure 2 shows a scatter plot of actual bias and insensitivity of TDM and OM. Each dot represents a query and the
values are normalized considering the distributions of their credits.
Since the scale of the credits is different for the two methods, we
Í
used (1/l) lr =1 (1 − minj E[д(I j , r )]/maxj 0 E[д(I j 0 , r )]) as the bias.
We divided the insensitivity σk2 by the square of the average credit
µ k2 , since the insensitivity is proportional to it. Comparing two
scatter plots, we can see that both the bias and the insensitivity

ACKNOWLEDGMENTS
This work was supported by JSPS KAKENHI Grant Numbers 26700009
and 16K16156.

REFERENCES
[1] Olivier Chapelle, Thorsten Joachims, Filip Radlinski, and Yisong Yue. 2012. Largescale Validation and Analysis of Interleaved Search Evaluation. ACM TOIS 30, 1
(2012), 6.
[2] Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. 2011. A Probabilistic
Method for Inferring Preferences from Clicks. In CIKM. 249–258.
[3] Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated Gain-based Evaluation
of IR Techniques. ACM TOIS 20, 4 (2002), 422–446.
[4] Thorsten Joachims. 2002. Unbiased Evaluation of Retrieval Quality Using Clickthrough Data. Technical Report. SIGIR MF/IR.
[5] Filip Radlinski and Nick Craswell. 2013. Optimized Interleaving for Online
Retrieval Evaluation. In WSDM. 245–254.
[6] Filip Radlinski, Madhu Kurup, and Thorsten Joachims. 2008. How Does Clickthrough Data Reflect Retrieval Quality?. In CIKM. 43–52.
[7] Matthew Richardson, Ewa Dominowska, and Robert Ragno. 2007. Predicting
Clicks: Estimating the Click-through Rate for New Ads. In WWW. 521–530.
[8] Anne Schuth, Robert-Jan Bruintjes, Fritjof Buüttner, Joost van Doorn, Carla
Groenland, Harrie Oosterhuis, Cong-Nguyen Tran, Bas Veeling, Jos van der Velde,
Roger Wechsler, David Woudenberg, and Maarten de Rijke. 2015. Probabilistic
Multileave for Online Retrieval Evaluation. In SIGIR. 955–958.
[9] Anne Schuth, Floor Sietsma, Shimon Whiteson, Damien Lefortier, and Maarten
de Rijke. 2014. Multileaved Comparisons for Fast Online Evaluation. In CIKM.
71–80.

952

