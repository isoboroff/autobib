Doctoral Consortium

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Relevance Judgments: Preferences, Scores and Ties
Ziying Yang

University of Melbourne
ziyingy@student.unimelb.edu.au
one in the pair. Workers will also be asked to give the absolute
assessment (relevant or irrelevant) and a numeric relevance score
(any positive number) for both documents in the pair.
For each document, we denote its global preference frequency as
the rate that it is preferred, and the relevance frequency as how many
times that this document is considered as relevant to the given topic,
in a total of K ×X ×Y assessments. The two scores can be combined
by some mechanism to estimate where the document fits in the
final computed list. We denote the local preference frequency of a
document as the proportion that this document is preferred over
the comparator document in Y assessments. The local preference
frequencies and the numeric relevance scores indicate relevance
distances between paired documents and help adjust the ranks of
documents having close global preference frequencies.
To verify that this model is suitable and useful for gathering
document relevance data for IR, the collected judgments will be
compared with three relevance scales: NIST binary [11]; Sormunen
judgments; and Magnitude Estimation [6]. The comparisons will
be in terms of document relevance, system ordering and cost. We
will use ANOVA [4] to analyze relevance judgment variations with
dependent variables topic, assessor, document and judgment technique, in order to determine the relative weight of the factors that
affect relevance assessment.
Further, we will discover how the collected judgments can be
mapped to normalized numeric relevance scores for documents in
the computed list. The distribution of normalized relevance scores
will provide additional knowledge of user perceptions of relevance.
The gain profiles found by collected judgments and Magnitude
Estimation will be compared. The finding will suggest how gain
functions of metrics can be refined.

CCS CONCEPTS
•Information systems → Relevance assessment;

KEYWORDS
Pairwise Preference, Relevance Assessment, Crowdsourcing
Conventionally, relevance judgments were assessed using ordinal
relevance scales such as binary and Sormunen categories [9]. Such
judgments record how much overlap there is between the document and the topic. However they have been argued as unreliable
and not objective [3, 5, 10] because: (1) documents are usually assessed by limited numbers of experts, with different viewpoints
of relevance because of individual factors such as gender, age and
background [1]; (2) the distinctions of relevance levels expected by
users disparate types may be diverse [7]; (3) assessors’ examining
criteria drift in varying degrees as more documents are judged [8];
(4) many judgment ties are generated using ordinal scales. In order
to have a better understanding of users’ perceptions of relevance
and collect data with high fidelity, we propose to use the Pairwise
Preference technique [2] to collect relevance judgments from a
crowdsourcing platform. With the collected judgments, a computed rank list containing all judged documents for each topic will
be generated with the goal of having fewer relevance ties.
We select 15 topics from the TREC-8 [11] ad hoc collection and
the top-10 documents returned by the contributing systems as the
data set. For each topic, documents are randomly and equally
partitioned into different groups a total of X times. For each group,
a list of document pairs will be generated in which each document
is compared with K other documents from the same group. Pair
lists will be randomly assigned to workers on CrowdFlower as units
to obtain Y distinct assessments for each list.
We will employ forced choice testing and embedded quality
control processes to reduce the assessment noise. Before starting
the real task in work mode, workers need to answer test questions
and achieve the minimum accuracy requirement of quiz mode. Test
questions contain pseudo documents which are hand-crafted short
summaries relative to the topic. In work mode, each unit contains
a gold standard pair, containing two documents whose relevance
levels are known to be highly relevant and not relevant respectively.
The assessment of a unit will be treated as valid only if the gold
standard pair is correctly ordered.
For each document pair, we will collect a preference judgment
that records which document is considered as the more relevant

REFERENCES
[1] P. Bailey, N. Craswell, I. Soboroff, P. Thomas, A. P. de Vries, and E. Yilmaz.
Relevance assessment: Are judges exchangeable and does it matter. In Proc.
SIGIR, pages 667–674, 2008.
[2] B. Carterette, P. N. Bennett, D. M. Chickering, and S. T. Dumais. Here or there:
Preference judgments for relevance. In Proc. ECIR, pages 16–27, 2008.
[3] R. A. Fairthorne. Implications of test procedures. J. Inf. Retr. in. Act., pages
109–113, 1963.
[4] R. A. Fisher. Statistical Methods for Research Workers, pages 66–70. Springer New
York, 1992.
[5] R. V. Katter. The influence of scale form on relevance judgments. Inf. Str. & Retri.,
4(1):1–11, 1968.
[6] E. Maddalena, S. Mizzaro, F. Scholer, and A. Turpin. On crowdsourcing relevance
magnitudes for information retrieval evaluation. ACM Trans. Inf. Sys., 35(3):
19:1–19:32, 2017.
[7] F. Scholer and A. Turpin. Metric and relevance mismatch in retrieval evaluation.
In Proc. Asia Info. Retri. Soc. Conf., pages 50–62, 2009.
[8] F. Scholer, A. Turpin, and M. Sanderson. Quantifying test collection quality based
on the consistency of relevance judgements. In Proc. SIGIR, pages 1063–1072,
2011.
[9] E. Sormunen. Liberal relevance criteria of TREC: Counting on negligible documents? In Proc. SIGIR, pages 324–330, 2002.
[10] A. Turpin and F. Scholer. Modelling disagreement between judges for information
retrieval system evaluation. In Proc. ADCS, page 51, 2009.
[11] E. M. Voorhees and D. Harman. Overview of the eighth text retrieval conference
(TREC-8). In Proc. TREC, 1999.

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
SIGIR ’17, August 7-11, 2017, Shinjuku, Tokyo, Japan
© 2017 Copyright held by the owner/author(s). 978-1-4503-5022-8/17/08
DOI: http://dx.doi.org/10.1145/3077136.3084154

1373

