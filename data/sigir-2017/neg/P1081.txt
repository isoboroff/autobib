Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Non-negative Matrix Factorization Meets Word Embedding
Melissa Ailem

LIPADE - Paris Descartes University
melissa.ailem@parisdescartes.fr

Aghiles Salah

LIPADE - Paris Descartes University
aghiles.salah@parisdescartes.fr

stream of works has focused on establishing theoretical connections between NMF and k-means clustering [3, 5]. All these efforts
highlighted the potential of NMF for text document clustering. Despite its success for handling text data [10], NMF, like other models
in this context, still exhibits some shortcomings, namely it uses
the “bag-of-words” or vector space representation and, therefore,
ignores the sequential order in which words occur in documents.
This is an important issue, because it may result in a significant
loss of information, especially the semantic relationships between
words. Hence, words with a common meaning—synonyms—or
more generally words that are part of a common context are not
guaranteed to be mapped in the same direction in the latent space.
In order to overcome the above issue, we propose a novel NMF
model that explicitly accounts for the sequential order in which
words occur in documents so as to capture the semantic relationships between words. The research question lies in how to do so
appropriately. In this paper we propose a solution which draws inspiration from the recent success of neural word embedding models,
such as word2vec [11], which have proven effective in learning continuous representations of words—embeddings—that successfully
capture meaningful syntactic and semantic regularities between
words [12]. More precisely, we rely on the distributional hypothesis
[6], which states that words in similar contexts have similar meanings. Given a corpus of unannotated text documents, each of which
represented as a sequence of words, the contexts for word w j are
the words w j 0 surrounding it in an L-sized window in any document
of the corpus. Other definitions of contexts are possible [8]. Thus,
following the distributional hypothesis, we assume that frequently
co-occurring words in a L-word window in any article are likely to
have a common meaning. Based on the above assumptions, we propose a novel NMF model, Semantic-NMF, which jointly decomposes
the document-word and word-context co-occurrence matrices with
shared word factors. The decomposition of the word-context matrix
has been shown to be equivalent to a state-of-the-art neural word
embedding method, namely the skip-gram model [8].
For inference, we propose a scalable alternating optimization
procedure based on a set of multiplicative update rules, which are
analogous to those of the original NMF model [7, 14]. Empirical
results on several real-world datasets provide strong support for
the effectiveness of the proposed model in terms of both clustering
and embedding.
Notation. Matrices are denoted with boldface uppercase letters and vectors with boldface lowercase letters. The Frobenius
norm is denoted by k.k and the Hadamard multiplication by .
The document-word matrix is represented by a matrix X = (x i j ) ∈
t h row represents the weighted term frequency vector
Rn×d
+ , its i
of document i ∈ I, i.e., xi = (x i1 , . . . , x id )> where > denotes the
transpose. The word and context vocabularies are identical and
denoted by J . Without loss of generality, we define the context
for word j ∈ J to be the words surrounding it in a L-word window

ABSTRACT
Document clustering is central in modern information retrieval
applications. Among existing models, non-negative-matrix factorization (NMF) approaches have proven effective for this task.
However, NMF approaches, like other models in this context, exhibit
a major drawback, namely they use the bag-of-word representation and, thus, do not account for the sequential order in which
words occur in documents. This is an important issue since it may
result in a significant loss of semantics. In this paper, we aim to address the above issue and propose a new model which successfully
integrates a word embedding model, word2vec, into an NMF framework so as to leverage the semantic relationships between words.
Empirical results, on several real-world datasets, demonstrate the
benefits of our model in terms of text document clustering as well
as document/word embedding.

CCS CONCEPTS
•Computing methodologies → Unsupervised learning; Cluster analysis; Non-negative matrix factorization;

KEYWORDS
Non-negative Matrix Factorization, Word Embedding, Text Document Clustering.

1

Mohamed Nadif

LIPADE - Paris Descartes University
mohamed.nadif@parisdescartes.fr

INTRODUCTION

Text data clustering and embedding are of great interest in modern
text mining and information retrieval (IR) applications, for several
practical reasons such as: automatic summarization and organization of documents, efficient browsing and navigation of huge text
corpora, speed up search engines, etc. Among existing approaches,
Non-negative Matrix Factorization (NMF) [7, 14] has proven very
effective for both text document clustering and embedding. In this
context, NMF models decompose the document-word matrix into
two non-negative document and word factor matrices containing,
respectively, the low dimensional representations—embeddings—of
documents and words [14]. Even though clustering is not the primary objective of NMF, it turns out that, the document factor matrix
encodes a latent structure of the original data that is well suited to
cluster text documents. Since the works of [7, 14], there has been
a lot of work to develop new variants of NMF in the direction of
clustering, most of which focused on text data [1, 5, 10, 15]. Another
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.
978-1-4503-5022-8/17/08. . . $15.00
DOI: http://dx.doi.org/10.1145/3077136.3080727

1081

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

function, to be minimized, is given by
1
1
F (Z, W, S) = ||X − ZW> || 2 + ||M − WSW> || 2 ,
2
2
|
{z
} |
{z
}

in any document of the corpus. The word-context matrix is represented by C = (c j j 0 ) ∈ Rd×d
+ , where row j ∈ J corresponds to
word w j , column j ∈ J denotes context word w j , and each entry
c j j 0 denotes the number of times word pair (w j , w j 0 ) occurred in a
L-word window in any document of the corpus.

2

NMF

where S ∈
is a symmetric extra factor which plays a key role
in the approximation of M. Indeed, it has been found that the above
form of symmetric NMF provides better approximations than its
basic variant where S is the identity matrix [9]. Note that, both
NMF and word embedding infer low dimensional representations of
words. In the former, word factors capture how words are used in
documents, while in the latter word embeddings capture the word
co-occurrence information. By sharing W between NMF and word
embedding in (3), Semantic-NMF seeks to leverage both the above
information, simultaneously. Note that, objective function (3) can
be interpreted as regularizing NMF’s objective function with word
embedding. To infer the factor matrices we rewrite (3) as follows

1
F (Z, W, S) = Tr XX> − 2XWZ> + ZW> WZ>
2

1
+ Tr MM> − 2MWSW> + WSW> WSW> . (4)
2
In the following, we derive a set of multiplicative update rules in
order to minimize F under the constrains of positivity of Z, W and
S. Let α ∈ Rn×k , β ∈ Rd×k and γ ∈ Rk ×k be the Lagrange multipliers for the above-mentioned constraints, the Lagrange function
L(Z, W, S, α , β, γ ) = L is given by

NMF on a document-term matrix X is to find the document and
n×д
d×д
word factor matrices Z = (zik ) ∈ R+ and W = (w jk ) ∈ R+ ,
T
respectively, such that X ≈ ZW . The inference of the factor matrices is usually done by optimizing a cost function that quantifies
the quality of the approximation. The most widely used one is the
square of the Frobenius norm: 12 kX − ZWT k 2 . In the context of
clustering, the document factor matrix Z is usually treated as a
soft cluster membership matrix, where zik denotes the degree to
which document i belongs to cluster k. A partition of the set of
documents can then be obtained by assigning each document to
the most likely cluster. Notice that, in order to make the solution to
the minimization of the NMF’s cost function unique, Z is usually
normalized so as to have unit-length column vectors.
Neural word embedding models seek continuous representations
of words, in such a way that words with a common meaning have
roughly similar embeddings in the latent space. The skip-gram
model [11] tries to achieve this objective by maximizing the dotproduct between the vectors of frequently occurring word-context
pairs. Recently, Levy and Goldberg [8] showed that the skip-gram
model with negative sampling (SGNS) [11] is implicitly factorizing a word-context matrix, whose cells are the pointwise mutual
information (PMI) shifted by log(N ), where N is the number of
negative samples in SGNS. The PMI is widely used to quantify the
association between pairs of words. Formally, the PMI between
word w j and its context word w j 0 is given as follows
p(w j , w j 0 )
.
p(w j )p(w j 0 )

L = F (Z, W, S) + Tr (α Z> ) + Tr (βW> ) + Tr (γ S> ).
The derivative of L with respect to Z, W and S are
∂L
= −XW + ZW> W + α
(5a)
∂Z

∂L
= −X> Z − 2MWS + W Z> Z + 2SW> WS + β
(5b)
∂W
∂L
= −W> MW + W> WSW> W + γ .
(5c)
∂S
Making use of the Kuhn-Tucker conditions α Z = 0, β W = 0
and γ S = 0, we obtain the following stationary equations:

(1)

Given the word-context matrix C defined above, we can estimate
c 0 ×c . .
the PMI empirically as follows PMI(w j , w j 0 ) = log c jj j. ×c 0 where
.j
Í
Í
Í
c .. = j j 0 c j j 0 , c j . = j 0 c j j 0 and c .j 0 = j c j j 0 . Since it is intractable
to work directly with the shifted PMI matrix, which is dense and
high-dimensional, Levy and Goldberg [8] proposed to approximate
it with the sparse Shifted Positive PMI matrix (SPPMI), M = (m j j 0 ) ∈
Rd×d
+ , defined as follows
m j j 0 = max{PMI(w j , w j 0 ) − log(N ), 0}.

−(XW)
>

−(X Z + 2MWS)
>

Z + (ZW> W)

Z=0

>

>

W + W Z Z + 2SW WS

−(W MW)

>

>

S + (W WSW W)

(6a)


W=0

S = 0.

(6b)
(6c)

Based on the above equations we derive the following update rules
XW
Z←Z
(7a)
ZW> W
X> Z + 2MWS
(7b)
W←W
W(2SW> WS + Z> Z)
W> MW
S←S
.
(7c)
>
W WSW> W
These update rules are analogous to those of NMF [7]. The difference
is in how we update the word factors. In equation (7b), the update
of W depends on two sources of data (i) the document-word matrix
and (ii) the SPPMI co-occurence matrix.
Equation (7a) is similar to that of NMF [7], and equation (7c)
is similar to that of three factor NMF [4]. Therefore, based on
Theorem 1 in [7] and Theorem 7 in [4] the objective function of

(2)

Levy and Goldberg [8] proposed, thus, to obtain the word embeddings by performing Singular Value Decomposition (SVD) on M.
Nevertheless, they pointed out the importance to investigate other
factorization techniques. Given that the above SPPMI matrix is
positive and symmetric, we perform a symmetric NMF on M.

3

word embedding

Rk+×k

RELATED BACKGROUND

PMI(w j , w j 0 ) = log

(3)

JOINT NMF AND WORD EMBEDDING

In order to explicitly leverage the semantic relationships among
words, we propose to jointly decompose the document-word and
word-context co-occurrence matrices with a shared word factor matrix. This gives rise to our model, Semantic-NMF, whose objective

1082

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Semantic-NMF is non-increasing under these update rules, for a
fixed W. Fixing Z and S, it is complicated to demonstrate that (7b)
monotonically decreases (3), the full proof has not been done, yet.
Nevertheless, the update rule (7b) is correct because at convergence
it satisfies the KKT fixed point condition in equation (6b). Furthermore, we illustrate empirically, in the experimental section,
that the objective function (3) is non-increasing under the above
three update rules, and the iterations converge to a locally optimal
solution.
Computational Complexity. Below, we shall analyse the computational complexity of semantic-NMF.

SeNMF to NMF. We also consider the orthogonal (ONMF) [15] variant,
due to its strong relation to clustering.
Settings. We perform 50 runs for all methods, using 50 different
starting points obtained from the spherical k-means algorithm [2].
We set д to the true number of clusters and N to 2 which is a good
trade-off between keeping much information and increasing the
sparsity of the PPMI matrix [8].
Evaluation metrics. We retain two widely used measures to assess the quality of clustering: the Normalized Mutual Information
(NMI) and the Adjusted Rand Index (ARI).
Results and discussion. Figure depicts the values of F (Z, W, S)
as a function of the number of iterations. The successive iterations
of update rules (7a), (7b) and (7a) monotonically decrease SeNMF’s
objective function and converge to a locally optimal solution. In
table 2, we report the average performances of the different models
in terms of all metrics, over all datasets. Between brackets, we
report the result corresponding to the trial with the highest/lowest
criterion. From this table, the proposed model SeNMF markedly
outperforms the other competing methods in terms of all metrics.
Without the word embedding component, SeNMF reduces to the
original NMF model. Therefore, we can attribute the improvement
achieved by SeNMF over NMF to the word embedding due to the
factorization of the SPPMI matrix.

Proposition 1. Let nz X and nz M denote respectively the number of
non-zero entries in X and M, and let t be the number of iterations.
The computational complexity of semantic-NMF is O(t · д · (nz X +
nz M ) + t · д2 · (n + d)).
Proof. The computational bottleneck of Semantic-NMF is with the
multiplicative update formulas (7a), (7b) and (7c). Equations (7a)
and (7c) are similar to those of NMF and symmetric 3-factor NMF,
and their respective complexities are O(nz X · д + (n + d) · д2 ) and
O(nz M · д + d · д2 ). The number of operations in (7b), including
multiplications, additions and divisions, is д(2nz X + 2nz M + d +
д(8d + 2n + 4д + 2)). The complexity of (7b) is thereby given in
O(д · (nz X + nz M ) + (n + d) · д2 ). Therefore, the total computational
complexity of semantic-NMF is O(t ·д ·(nz X +nz M )+t ·д2 ·(n+d)).

4.18e+06

1.09e+07
1.96e+07
1.09e+07

Proposition 1 shows that the complexity of Semantic-NMF scales
linearly with the number of non-zero entries in the documentword and SPPMI matrix. In practice X and M are very sparse, i.e.,
nz X  n × d and nz M  d × d. Furthermore, since multiplicative
update rules (7a), (7a) and (7c) involve only basic matrix operations,
it is possible to take benefits from distributed computing. Thereby,
semantic-NMF can easily scale to large datasets.

4

4.16e+06

nz X (%)

Balance

CLASSIC4

7095

5896

4

0.59

0.323

2.41

REUTERS

6387

16921

4

0.25

0.080

0.47

NG20

18846

26214

20

0.59

0.628

1.80

F(Z ,W ,S)

1.90e+07

1.88e+07
4.08e+06
0

20

40
60
I t e r a t io n s

80

100

(a) CLASSIC4

1.08e+07
0

20

40
60
I t e r a t io n s

80

100

0

(b) REUTERS

20

40
60
I t e r a t io n s

80

100

(c) NG20

Figure 1: Convergence curves.
Table 2: NMI and ARI results over different datasets. Increase indicates the
difference between the performances of SeNMF and NMF.

REUTERS

NG20

CLASSIC4

Data

Characteristics
#Clusters

1.92e+07

1.08e+07

Table 1: Description of Datasets
#Words

1.08e+07

4.10e+06

EXPERIMENTAL STUDY

#Documents

4.12e+06

1.94e+07

1.09e+07

1.08e+07

Our primary purpose is to study the effects of neural word embedding on NMF. To this end, we benchmark the proposed model,
Semantic-NMF (SeNMF), against comparable NMF models.
Datasets. We use three popular datasets, described in Table 1,
namely CLASSIC41 , REUTERS2 the four largest classes of the
Reuters corpus and 20-newsgroups NG202 . We use the TF-IDF
data representation and normalize each document to have unit L 2
norm. For each dataset we build the word-context matrix M based
on the original corpus, using a 10-word window.

Datasets

4.14e+06

F(Z ,W ,S)

F(Z ,W ,S)

1.09e+07

per. SKmeans
NMI 0.60±0.02
(0.59)
ARI 0.47±0.01
(0.46)
NMI 0.48±0.02
(0.52)
ARI 0.30±0.03
(0.33)
NMI 0.40±0.05
(0.38)
ARI 0.21±0.10
(0.18)

NMF
ONMF
SeNMF
increase (%)
0.53±0.006 0.50±0.005 0.74±0.004
(0.53)
(0.50)
(0.74)
21%
0.45±0.003 0.42±0.004 0.69±0.02
(0.45)
(0.42)
(0.73)
28%
0.42±0.02
0.41±0.01 0.52±0.02
(0.44)
(0.42)
(0.53)
9%
0.23±0.02
0.20±0.01 0.33±0.01
(0.26)
(0.20)
(0.34)
8%
0.35±0.0004 0.36±0.004 0.61±0.02
(0.35)
(0.36)
(0.62)
27%
0.13±0.001 0.15±0.02 0.57±0.02
(0.13)
(0.15)
(0.58)
45%

nz M (%)

word/document embeddings. Figures 2a, 2b and 2c show the
distribution of the cosine similarities between pairs of top words3
of the same class computed using the word vectors inferred by
NMF and SeNMF. Because the cosine similarity is likely to be high
between low dimensional vectors (e.g. д = 4), we vary д from the
real number of clusters to 300 for each dataset. The top words of
each class have more similar embeddings under SeNMF than NMF.
This confirms that SeNMF does a better job than NMF in capturing

Competing methods. SeNMF corresponds to the original (NMF)
[14] with an extra word embedding term. Hence, we can study the
effects of word embedding on NMF most effectively by comparing
1 http://www.dataminingresearch.com/

2 http://www.cad.zju.edu.cn/home/dengcai/Data/TextData.html

3 Based on the document-word matrix, we select the top thirty words of each true class.

1083

Short Research Paper

Se N M F

1.2

NM F

Se N M F

0.8

0.8

0.8

0.8

0.4

0.6

0.4

Cosine sim ila rit y

1.0

Cosine sim ila rit y

1.0

0.6

1.2

NM F

1.0

Cosine sim ila rit y

Cosine sim ila rit y

1.2

NM F

0.6

0.4

Se N M F
NM F

0.6

0.4

0.8

0.6

0.4

0.2

0.2

0.2

0.2

0.2

0.0

0.0

0.0

0.0

0.0

4

4

100 100

200 200

300 300

4

4

100 100

g

200 200
g

(a) CLASSIC4

300 300

(b) REUTERS

20

20

100 100

200 200
g

300 300

4

4

100 100

200 200

300 300

g

(c) NG20

1.2

Se N M F
NM F

1.0

(d) CLASSIC4

Se N M F
NM F

1.0

Cosine sim ila rit y

1.2

Se N M F
1.0

Cosine sim ila rit y

1.2

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

0.8

0.6

0.4

0.2

0.0
4

4

100 100

200 200
g

300 300

(e) REUTERS

20

20

100 100

200 200
g

Figure 2: (a), (b), (c): Distribution of cosine similarities between the top 30 words characterizing each document class, computed using the words’ embeddings obtained by NMF and SeNMF.
(d), (e), (f): Distribution of cosine similarities between pairs of documents belonging to the same class, computed using the documents’ embeddings obtained by NMF and SeNMF.
Table 3: Top 20 words inferred on CLASSIC4 dataset. corpus.

5

SeNMF
treatment
liver
patient
rat
blood
tissu
cell
serum
anim
metabol
diseas
renal
plasma
lung
tumor
lesion
protein
infect
acid
hormon
PMI : 3.62

NMF
matrix
integr
squar
determin
fit
invers
function random
gener
normal
algorithm exponenti
linear
evalu
number complex
order
permut
polynomi gamma
PMI : 2.43

commun
user
system
inform
search
public
research
retriev
index
librari
scientif
docum
univers
book
studi
servic
scienc
catalog
literatur journal
PMI : 1.83

CONCLUSION

Inspired by the recent success of neural word embedding models,
we propose to jointly perform NMF on the document-word and
SPPMI word-context matrices, with shared word factors. This gives
rise to a new NMF model which explicitly follows the distributional hypothesis. Empirical results on real-world datasets show
that the proposed model, Semantic-NMF, does a better job than
original NMF in capturing semantics. More interestingly, in doing
so, Semantic-NMF implicitly brings the embeddings of documents
which are about the same topic closer to each other, which results
in document factors that are even better for clustering, as illustrated
in our experiments.
Our findings open up good opportunities for future research.
For instance, a possible improvement is to consider regularization
schemes already applied to the original NMF. On the other hand, the
idea of Semantic-NMF could be extended to other models including
the different variants of NMF. Another interesting line of future
work is to investigate SPPMI matrices estimated using huge external
corpora such as WIKIPEDIA and GOOGLE.
Acknowledgment. This work has been funded by AAP Sorbonne Paris Cité.

distanc
veloc
languag
execut
depart
industri
angl aerodynam program
compil research medic
vehicl
wing
scheme
bit
institut
social
lift
propel
arithmet symbol univers
servic
motion
nose
code
memori school librarian
jet
vortic
implement processor scienc profession
plane
superson algorithm fortran
educ
societi
radiu
drag
ibm
store
public scientist
axi
diamet
algol
string
librari
academ
pitch
chord
input
sequenc
book
nation
PMI : 2.40
PMI : 2.35
PMI : 2.01
solut
transfer languag system
method
pressur comput process
equat
heat
techniqu discuss
flow
layer
program
gener
number
bodi
problem
data
distribut
shock
machin
time
surfac
plate
method
design
boundari
wing
oper
compil
effect
laminar automat
paper
theori
mach
code
structur
PMI : 1.75
PMI : 1.56

semantics, by making the representations of words which are about
the same topic (class) closer to each other in the latent space.
Similarly, figures 2d, 2e and 2f show the distribution of the cosine
similarities between pairs of documents belonging to the same class.
We observe that documents from the same class (topic) tend to have
more similar embeddings under SeNMF than NMF. This provides
empirical evidence that accounting for the semantic relationships
among words yields document factors that encode the clustering
structure even better.
Cluster interpretability. Herein, we compare SeNMF with NMF in
terms of cluster interpretability. To human subjects, interpretability
is closely related to coherence [13], i.e., how much the top words
of each cluster are “associated” with each other. For each cluster
k, we select its top 30 words based on the kth column of W. We
use the PMI, which is highly correlated with human judgments
[13], to measure the degree of association between top word pairs
of clusters obtained with each method. Following Newman et al.
[13], we use the whole English WIKIPEDIA corpus, that consists
of approximately 4 millions of documents and 2 billions of words.
Hence, p(w j ) is the probability that word w j occurs in WIKIPEDIA,
and p(w j , w j 0 ) is the probability that words w j and w j 0 co-occur in
a 5-word window in any WIKIPEDIA document. Table 3 illustrates
the top words inferred by SeNMF and NMF on the ClASSIC4 dataset;
for each cluster we average the PMIfis among its top words. The
average PMI’s of each method over all datasets, presented in the
format (dataset, SeNMF, NMF), are as follows: (CLASSIC4, 2.59, 1.89),
(REUTERS, 1.7, 1.21), (NG20, 2.64, 1.49). From these results, it is
clear that SeNMF does a better job than NMF in capturing semantics
and inferring more interpretable clusters.

REFERENCES
[1] Deng Cai, Xiaofei He, Jiawei Han, and Thomas S Huang. 2011. Graph regularized
nonnegative matrix factorization for data representation. IEEE TPAMI 33, 8
(2011), 1548–1560.
[2] Inderjit S. Dhillon and Dharmendra S. Modha. 2001. Concept Decompositions for
Large Sparse Text Data Using Clustering. Mach. Learn. 42, 1-2 (2001), 143–175.
[3] Chris Ding, Xiaofeng He, and Horst D Simon. 2005. On the equivalence of
nonnegative matrix factorization and spectral clustering. In SIAM SDM. 606–
610.
[4] Chris Ding, Tao Li, Wei Peng, and Haesun Park. 2006. Orthogonal nonnegative
matrix t-factorizations for clustering. In SIGKDD. ACM, 126–135.
[5] Chris HQ Ding, Tao Li, and Michael I Jordan. 2010. Convex and semi-nonnegative
matrix factorizations. IEEE TPAMI 32, 1 (2010), 45–55.
[6] Zellig S Harris. 1954. Distributional structure. Word 10, 2-3 (1954), 146–162.
[7] Daniel D Lee and H Sebastian Seung. 2001. Algorithms for non-negative matrix
factorization. In NIPS. 556–562.
[8] Omer Levy and Yoav Goldberg. 2014. Neural word embedding as implicit matrix
factorization. In NIPS. 2177–2185.
[9] Tao Li and Chris Ding. 2006. The relationships among various nonnegative
matrix factorization methods for clustering. In ICDM’06. IEEE, 362–371.
[10] Tao Li and Chris HQ Ding. 2013. Nonnegative Matrix Factorizations for Clustering: A Survey. (2013).
[11] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality. In
NIPS. 3111–3119.
[12] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic Regularities
in Continuous Space Word Representations.. In Hlt-naacl, Vol. 13. 746–751.
[13] David Newman, Sarvnaz Karimi, and Lawrence Cavedon. 2009. External evaluation of topic models. In in Australasian Doc. Comp. Symp.
[14] Wei Xu, Xin Liu, and Yihong Gong. 2003. Document clustering based on nonnegative matrix factorization. In SIGIR. 267–273.
[15] Jiho Yoo and Seungjin Choi. 2008. Orthogonal nonnegative matrix factorization:
Multiplicative updates on Stiefel manifolds. In IDEAL. 140–147.

1084

300 300

(f) NG20

