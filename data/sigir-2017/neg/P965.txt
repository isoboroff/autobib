Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Translation of Natural Language Query Into Keyword Query
Using a RNN Encoder-Decoder
Hyun-Je Song

Naver Search
Naver Corporation
Seongnam 13561, Korea
hyunje.song@navercorp.com

A-Yeong Kim

Seong-Bae Park

School of Comp. Sci. & Eng.
Kyungpook National University
Daegu 41566, Korea
aykim@sejong.knu.ac.kr

School of Comp. Sci. & Eng.
Kyungpook National University
Daegu 41566, Korea
sbpark@sejong.knu.ac.kr

ABSTRACT
The number of natural language queries submitted to search engines is increasing as search environments get diversified. However,
legacy search engines are still optimized for short keyword queries.
Thus, the use of natural language queries at legacy search engines
degrades the retrieval performance of the engines. This paper proposes a novel method to translate a natural language query into a
keyword query relevant to the natural language query for retrieving
better search results without change of the engines. The proposed
method formulates the translation as a generation task. That is,
the method generates a keyword query from a natural language
query by preserving the semantics of the natural language query. A
recurrent neural network encoder-decoder architecture is adopted
as a generator of keyword queries from natural language queries.
In addition, an attention mechanism is also used to cope with long
natural language queries.

(a)

Figure 1: Examples on different search results according to
query forms. (a) is the result of ‘Obama age’, while (b) is that
of “I would be happy if you can tell me how old Obama is.”
natural language queries, however, suffer from so-called “lexical
chasm” [18] between keyword queries and natural language queries,
since each query type has its own style and vocabulary. Therefore,
the direct use of natural language queries at traditional web search
engines degrades the retrieval performance of the engines. Figure
1b shows such an example which is the search result of a natural
language query “I would be happy if you can tell me how old Obama
is.” Although this query is semantically similar to the previous
query at Figure 1a, its result is much different from Figure 1a and
is not satisfactory at all. Some studies adopted natural language
processing (NLP) techniques such as dependency parsing or relation
extraction to solve this problem [5], but these NLP techniques
are specialized in question manipulation and are not adequate for
keyword queries.
This paper proposes a simple method to translate a natural language query into its corresponding keyword query for retrieving
better search results. The proposed method formulates the query
translation as a generation task. That is, a keyword query is generated from a natural language query by preserving the semantics
of the natural language query as much as possible. For this, this
paper adopts a recurrent neural network (RNN) encoder-decoder
[3, 19] which is widely used in several NLP tasks such as machine
translation [19] and text summarization [4]. These NLP tasks are
similar to ours in that they need to preserve the semantics between
their input and output, and the output is generated from the input.
In the proposed RNN encoder-decoder architecture, an encoder
encodes a natural language query into a vector representation, and a
decoder generates a keyword query relevant to the natural language
query from the vector representation. Note that the length of
natural language queries is in general quite long. Thus, an attention
mechanism [2] is adopted in the encoder-decoder architecture to

CCS CONCEPTS
•Information systems →Query reformulation; •Computing
methodologies →Neural networks;

KEYWORDS
Query reformulation; RNN encoder-decoder; Translation natural
language query into keyword query; Neural machine translation

1

(b)

INTRODUCTION

Keyword query is a common input form to most traditional web
search engines. When a query composed of a single or a few keywords is submitted to a search engine, the engine provides its best
result relevant to the query. With the help of the huge number of
studies on search engines and factual information derived from
knowledge graphs [21] or community QAs [13], most search engines provide their users with high quality results. For instance,
Figure 1a shows the search result of a query ‘Obama age’ by Google.
The result is accurate as well as concise.
As virtual assistants such as Google Allo or Microsoft Cortana
get available, natural language queries become widely used. The
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00
DOI: http://dx.doi.org/10.1145/3077136.3080691

965

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

cope with the long natural language queries, since it can focus on
some specific parts of an input sequence. As a result, the proposed
method comes to generate a relevant keyword query, even if an
input natural language query is quite long.
The advantages of the proposed method are twofold. One is
that the proposed method requires just a set of pairs of a natural language query and its corresponding keyword query, since
encoder-decoder architectures are trainable only with input-output
pairs. Therefore, it does not require any other resources dependent
on a search engine like click-through graphs. The other is that
legacy web search engines can be used to retrieve search results
of natural language queries, since the proposed method outputs a
keyword query adequate to the engines.

2

conditional probability p(y|x) from the probabilistic perspective, a
recurrent neural network (RNN) encoder-decoder is used to model
the conditional probability. The RNN encoder-decoder is a neural
network that is designed to model directly the conditional probability of translating an input sequence x to an output sequence y, and
consists of two RNNs. The first RNN (encoder) encodes the input
sequence to its continuous representation. It reads x sequentially,
and then encodes it into a sequence of hidden states h represented
as h = (h 1 , h 2 , ..., hn ), where ht ∈ Rdh is a hidden state at time t
and dh is the number of dimensions. Following the study of Graves
and Schmidhuber [7], each hidden state hi in the encoder is represented again by two RNNs: a forward RNN and a backward RNN.
The forward RNN reads x in forward direction, while the backward
RNN reads it in reverse direction. Then, the hidden state ht at time
t is actually a concatenation of the hidden states from the forward
→
− ←
−
→
−
←
−
and the backward RNNs. That is, ht = [ht ; ht ], where ht and ht are
→
− −−−→ −−−→
←
− ←−−− ←−−−
defined as ht = fenc (ht −1 , e x (x t )) and ht = fenc (ht +1 , e x (x t )), re−−−→
←−−−
spectively. Here, fenc and fenc are non-linear activation functions,
and e x (x t ) is an embedding vector of x t .
The second RNN (decoder) defines the probability of a keyword
query y by decomposing the probability into ordered conditional
probabilities.

RELATED WORK

Query reformulation for search engines has been an important task
of information retrieval to provide search engine users with better
search results. Thus, there have been a number of approaches to
query reformulation. The approaches include query expansion by
pseudo-relevance feedback [22] and by knowledge base [16], and
query rewriting with correlation information derived from query
logs [11] or click-through graphs [1]. The major problem of these
approaches is that they require some resources dependent on a
specific search engine.
Several previous studies adopted machine translation techniques
as an extension of query rewriting [6, 18]. These studies regard a
user query as a sentence in a source language of machine translation
and a retrieved web document as a sentence in a target language,
respectively. Then, they translate a user query into an extended
query suitable for retrieving web documents. Recently, He et al.
[8] adopted a recurrent neural network encoder-decoder that is
the state-of-the-art architecture in machine translation. However,
since these studies focused on query expansion, they just tried
to preserve the whole semantics of a user query while the query
contains many words that cannot be keywords.
There have been also several studies that adopt the translation of
natural languages into keywords for sub-query identification [12].
In order to identify sub-queries, Huston and Croft [10] removed a
few unimportant words, and Wang and Zhai [20] substituted the
words in a query with their similar words. Park and Croft [17] and
Maxwell and Croft [15] calculated the importance of the words in
a query, and then chose relevant words from the query according
to the importance. All these studies formulated translation as a
selection or a substitution of words. Thus, it is difficult to produce
concise expressions relevant to the original query by the methods.
On the other hand, the proposed method considers translation as a
generation of a query. As a result, the translated keyword query
by the proposed method contain some expressions unseen at the
original query, but relevant to the query.

3

p(y) =

m
Y

p(yi |y <i , x),

i=1

where y <i = {y1 , ..., yi−1 }. That is, the decoder generates a keyword query by predicting a word at a time. In predicting a word, the
decoder uses its hidden state and the previously generated words.
Thus, the conditional probability p(yi |y <i , x) is modeled as
p(yi |y <i , x) = д(ey (ỹt −1 ), zt ),

(1)

where д is a non-linear function that outputs the probability of yi ,
ỹt −1 is the word generated at time t − 1, and ey (yt ) is an embedding
0
vector of yt . In this equation, zt ∈ Rdh is a hidden state of the
decoder at time t which is updated by
zt = fdec (zt −1 , ey (ỹt −1 )).

(2)

Here, fdec is again a non-linear activation function. The initial
hidden state of the decoder, z 0 , is initialized based on the last hidden
state of the encoder. That is, z 0 = hn .
The semantics between input and output sequences is, in general,
preserved well in the RNN encode-decoder [3]. However, in query
translation, the semantics of long natural language queries is not
kept completely in the output keyword queries. This is because
a long natural language query contains many words which have
nothing to do with or are weakly related with the theme of the query.
If both relevant and weakly-relevant words are equally considered
in translation, the encoded representation can be biased negatively
by the weakly-relevant words. As a result, the decoder generates
inaccurate keywords due to the biased representation. Therefore,
the translation should not consider only the whole semantics of
natural language queries, but also focus more on some relevant
words of the queries.
An attention mechanism [2] is adopted to our encoder-decoder
architecture as a solution to the problem. Since the decoder with the
attention mechanism can focus selectively on some specific words

TRANSLATION OF NATURAL LANGUAGE
QUERY INTO KEYWORD QUERY

When a natural language query x = {x 1 , ...x n } is given by a user
where x i is the i-th word of the query, the goal of this paper is to
translate x into a keyword query y = {y1 , ..., ym } such that y has the
same meaning as x. Since this translation can be explained with the

966

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

of x, it predicts a word at a time reflecting the specific words. In
order to reflect the mechanism into our encoder-decoder, Equation
(1) should be rewritten as

Table 1: Simple statistics on dataset for training RNN
encoder-decoder

p(yi |y <i , x) = д(ey (ỹt −1 ), zt , ct ),
where ct is a context vector that captures relevant source-side information to help predicting yi . This context vector also influences
the update of hidden states in the decoder. Thus, Equation (2) is
rewritten as
zt = fdec (zt −1 , ey (ỹt −1 ), ct ).

Information

Value

# of natural language and keyword query pairs
Average # of tokens in a natural language query
Average # of tokens in a keyword query
# of unique vocabularies in natural language queries
# of unique vocabularies in keyword queries

828,826
7.94
2.57
151,657
108,838

To obtain ct at time t, the relevance score of each hidden representation of the encoder should be first calculated which is defined
as
βt,i = fscor e (hi , zt −1 , ey (ỹt −1 ))

Table 2: Simple statistics on evaluation dataset

for all i = 1, ..., n. fscor e is a neural network that measures how
relevant the i-th hidden representation in the natural language
query is in deciding the t-th keyword query. The local attention of
Luong et al. [14] among various approaches is used to implement
this neural network, since it focuses only on a small subset of the
words. After normalizing each relevance score by

8
33.38
7.89

Average # of tokens
Ratio of queries with
nonexistent tokens

the context vector ct is computed as a weighted sum of the hidden
representations with their normalized scores. That is,
ct =

Value

# of users
Average # of queries per a user
Average # of tokens in a query

Table 3: Statistics on translated queries

exp(βt,i )
,
α t,i = Pn
exp(βt,k )
k=1

n
X

Information

Nouns

RNN

Proposed Method

3.40

2.56

2.45

0%

7.5%

9.0%

α t,i ht .

i=1

average number of tokens in natural language queries is 7.94 which
is around three times larger than that of keyword queries.
Another set of user queries is needed to evaluate the search
results by the proposed method. We collect these queries from eight
users who are independent from our work. The only restriction to
preparing the set is that the queries should be written in a natural
language. As a result, the set includes various questions, asks, and
recommendations. Table 2 summarizes this evaluation set.
The proposed method is compared with three baselines. One is
to use original natural language queries without any transformation into keywords (NL), and another is to use only nouns within
a natural language query (Nouns). The third is to use the RNN
encoder-decoder without an attention mechanism (RNN). Long
Short Term Memory (LSTM) [9] is used as all non-linear activation
−−−→ ←−−−
functions ( fenc , fenc , and fdec ).

The attention-based RNN encoder-decoder is trained by maximiz)N
(
ing the conditional log-likelihood over a dataset D = (x (j ) , y (j ) )
j=1
defined as
N m


1 XX
(j ) (j )
L(θ ) =
log p yi = yi |y <i , x (j ) ,
N j=1 i=1
(j )

where yi is the i-th ground-truth keyword of the j-th training
instance. The parameters θ are optimized by the backpropagation
with stochastic gradient descent.

4 EXPERIMENTS
4.1 Experimental Settings
The search result by each query type is used as a criterion to evaluate the effectiveness of the proposed query translation. After a
user submits natural language queries, the relevance of all results
returned by search engines is judged manually. Google and Naver1
are adopted as search engines that return the search results from
queries.
Since there is no dataset publicly available to train the encoderdecoder for our task, we prepare the dataset D first. As the first
step for preparing D, keyword queries are sampled from the Naver
query log, and then the top-1 title in a CQA site2 per a keyword
query is regarded as a natural language query of the keyword query.
Table 1 shows the simple statistics of this dataset. Note that the

4.2

Experimental Results

Before comparing the proposed method with baselines, we show
the statistics on translated queries by each method in Table 3. The
average number of tokens in a natural language query is around
eight, but all translation methods reduce this number to around
three. One thing to note here is that the methods based on the RNN
encoder-decoder generate the tokens which do not exist in the
natural language queries. For instance, a natural language query
“computer is really slow” is translated to ‘computer lag’ by the RNN
encoder-decoder, where the token ‘lag’ is nonexistent in the natural
language query. This implies that such methods are able to generate
new tokens that represent the theme of a natural language query.

1 http://m.naver.com

2 http://kin.naver.com

967

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

5

CONCLUSION

This paper has proposed a method to translate a natural language
query into a keyword query. The proposed method regards the
translation as a generation and uses a RNN encoder-decoder architecture as a generation model. The proposed method also adopts
an attention mechanism to the RNN encoder-decoder architecture
in order to concentrate on some words related with the theme of
the natural language query. As a result, the proposed method does
not reflect only the whole semantics of the natural language query,
but also focuses more on the words relevant to the theme. The experiments have shown that the proposed model returns the search
results with high NDCG score in legacy search engines.

ACKNOWLEDGEMENT

Figure 2: NDCG@5 scores over two search engines

This work was partly supported by IITP grant funded by the Korea
government (2013-0-00109, WiseKB: Big data based self-evolving
knowledge base and reasoning platform).

Table 4: Performances of pair comparisons

Google
Naver

NL - Nouns
0.66 ± 0.38
0.41 ± 0.37

NL - RNN
0.49 ± 0.59
0.52 ± 0.46

NL - Proposed Method
1.08 ± 0.57
1.04 ± 0.46

REFERENCES
[1] I. Antonellis, H. G. Molina, and C. C. Chang. 2008. Simrank++: Query Rewriting
Through Link Analysis of the Click Graph. Proceedings of VLDB 1, 1 (2008),
408–421.
[2] D. Bahdanau, K. Cho, and Y. Bengio. 2015. Neural Machine Translation by Jointly
Learning to Align and Translate. In Proceedings of ICLR.
[3] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
and Y. Bengio. 2014. Learning Phrase Representations using RNN Encoder–
Decoder for Statistical Machine Translation. In Proceedings of EMNLP. 1724–
1734.
[4] S. Chopra, M. Auli, and A. M. Rush. 2016. Abstractive Sentence Summarization
with Attentive Recurrent Neural Networks. In Proceedings of NAACL. 93–98.
[5] D. Ferrucci, E. Brown, J. Chu-Carroll, J. Fan, D. Gondek, A. A Kalyanpur, A. Lally,
J W. Murdock, E. Nyberg, and J. Prager. 2010. Building Watson: An overview of
the DeepQA project. AI magazine 31, 3 (2010), 59–79.
[6] J. Gao and J.-Y. Nie. 2012. Towards Concept-Based Translation Models Using
Search Logs for Query Expansion. In Proceedings of CIKM. 1:1–1:10.
[7] A. Graves and J. Schmidhuber. 2005. Framewise phoneme classification with
bidirectional LSTM and other neural network architectures. Neural Networks 18,
5 (2005), 602–610.
[8] Y. He, J. Tang, H. Ouyang, C. Kang, D. Yin, and Y. Chang. 2016. Learning to
Rewrite Queries. In Proceedings of CIKM. 1443–1452.
[9] S. Hochreiter and J. Schmidhuber. 1997. Long short-term memory. Neural
computation 9, 8 (1997), 1735–1780.
[10] S. Huston and W. B. Croft. 2010. Evaluating Verbose Query Processing Techniques.
In Proceedings of SIGIR. 291–298.
[11] R. Jones, B. Rey, O. Madani, and W. Greiner. 2006. Generating query substitutions.
In Proceedings of WWW. 387–396.
[12] G. Kumaran and V. R. Carvalho. 2009. Reducing Long Queries Using Query
Quality Predictors. In Proceedings of SIGIR. 564–571.
[13] B. Li and I. King. 2010. Routing Questions to Appropriate Answerers in Community Question Answering Services. In Proceedings of CIKM. 1585–1588.
[14] T. Luong, H. Pham, and C. D. Manning. 2015. Effective Approaches to Attentionbased Neural Machine Translation. In Proceedings of EMNLP. 1412–1421.
[15] K. T. Maxwell and W. B. Croft. 2013. Compact Query Term Selection Using
Topically Related Text. In Proceedings of SIGIR. 583–592.
[16] A. Otegi, X. Arregi, O. Ansa, and E. Agirre. 2015. Using knowledge-based
relatedness for information retrieval. Knowledge and Information Systems 44, 3
(2015), 689–718.
[17] J. H. Park and W. B. Croft. 2010. Query Term Ranking Based on Dependency
Parsing of Verbose Queries. In Proceedings of SIGIR. 829–830.
[18] S. Riezler and Y. Liu. 2010. Query Rewriting Using Monolingual Statistical
Machine Translation. Computational Linguistics 36, 3 (2010), 569–582.
[19] I. Sutskever, O. Vinyals, and Q. V Le. 2014. Sequence to sequence learning with
neural networks. In Proceedings of NIPS. 3104–3112.
[20] X. Wang and C. Zhai. 2008. Mining Term Association Patterns from Search Logs
for Effective Query Reformulation. In Proceedings of CIKM. 479–488.
[21] G. Weikum and M. Theobald. 2010. From Information to Knowledge: Harvesting
Entities and Relationships from Web Sources. In Proceedings of SIGMOD/PODS.
65–76.
[22] J. Xu and W. B. Croft. 1996. Query Expansion Using Local and Global Document
Analysis. In Proceedings of SIGIR. 4–11.

First of all, we evaluate the baselines and the proposed method
by measuring their retrieval results. For this evaluation, all methods (Nouns, RNN, and the proposed method) transform the natural
language queries made by eight users into keyword queries. Then,
the search results by Google and Naver for the four query types including NL query are anonymized, and then evaluated by the users.
The normalized discount cumulative gain at ranks 5 (NDCG@5) is
used as an evaluation metric. The scale of NDCG is set from 0 to 3
by the relevance to query intent. Figure 2 shows average NDCG@5
scores over the two search engines. The performances of query
translation methods are better than natural language query itself in
both search engines, which indicates that the translated keyword
queries are useful for legacy search engines because the engines are
optimized for keyword queries. The proposed method achieves 0.80
score in Google and 0.72 score in Naver, which are up to 0.13 higher
than those of the noun-only query. In addition, its performance is
also higher than that of the RNN encoder-decoder without attention. This implies that keyword queries by the proposed method
focus more on the theme of natural language queries through the
attention mechanism.
We evaluate also the performance of the proposed method by
comparing the search result of NL and results of keyword queries
translated by each translation method. As an evaluation metric,
7-point Likert scale from -3 to 3 is used, where negative values
indicate the result by a translated query is worse than that of NL
query and positive values imply better result over NL query. The
comparison result is given in Table 4. All the values in this table
are positive, which implies that the search results of transformed
keyword queries are more satisfactory to users than that of natural
language queries. Especially, the proposed method achieves the
best performance. Thus, the keywords translated by the proposed
method are the best among the models compared. These two experiments prove that the proposed method is effective for translating
a natural language query into a keyword query.

968

