Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

X-Dart: Blending Dropout and Pruning for Efficient
Learning To Rank
Claudio Lucchese

Franco Maria Nardini

ISTI-CNR, Pisa, Italy
c.lucchese@isti.cnr.it

ISTI-CNR, Pisa, Italy
f.nardini@isti.cnr.it

Raffaele Perego

Salvatore Trani

ISTI-CNR, Pisa, Italy
r.perego@isti.cnr.it

ISTI-CNR, Pisa, Italy
s.trani@isti.cnr.it
However, MART suffers from over-specialization [8]. It builds
the model iteratively by adding a tree at a time trying to minimize
the cost function adopted. The trees added at later iterations tend
however to impact the prediction of only a few instances and to
give a negligible contribution to the final score of the remaining
instances. This has two important negative effects: i) it negatively
affects the performance of the model on unseen data, and ii) it
makes the learned model over-sensitive to the contributions of a
few initial trees. To address these two limitations, recently Rashmi
and Gilad-Bachrach proposed Dart [8], a new algorithm that borrows the concept of dropout from neural networks [10] and blends
it in the MART iterative algorithm. In neural networks, dropout is
used to mute a random fraction of the neural connections during
the learning process. As a consequence, nodes at higher layers of
the network must relay on fewer connections to deliver the information needed for the prediction. This method has contributed
significantly to the success of deep neural networks for many tasks.
Dropout is adapted to ensemble of trees in a novel way, by muting
complete trees as opposed to muting neurons. The experimental
results obtained on publicly available datasets prove that Dart
remarkably outperforms MART both in classification and regression tasks. Moreover, Rashmi and Gilad-Bachrach show that Dart
overcomes the over-specialization issue to a considerable extent.
On the other hand, driven by efficiency reasons [3, 7], recently Lucchese et al. proposed CLEaVER [6], a post-learning optimization
framework for MART models. The goal is in this case improving the
efficiency of the learned model at document scoring time without
affecting ranking quality. CLEaVER works in two steps: i) it first
removes a subset of the trees from the MART ensemble and ii) it
then fine-tunes the weights of the remaining trees according to the
given quality measure.
Inspired by these two orthogonal works, in this paper we propose X-Dart, a novel LtR algorithm that improves over Dart by
borrowing from CLEaVER the tree pruning strategy, eventually
providing more robust and compact ranking models. As Dart,
X-Dart mutes from the ensemble built so far some of the trees
(dropouts) before learning an additional tree. Differently from Dart
but like CLEaVER, it drops permanently these trees when their contribution to the accuracy is considered negligible. We investigate
three different pruning strategies aimed at maximizing pruning
opportunities and limiting overfitting. The experiments conducted
on two publicly available datasets show that X-Dart achieves a
statically equivalent effectiveness with respect to the reference
Dart and λ-Mart models by employing up to 40% and 75% less

ABSTRACT
In this paper we propose X-Dart, a new Learning to Rank algorithm
focusing on the training of robust and compact ranking models.
Motivated from the observation that the last trees of MART models
impact the prediction of only a few instances of the training set,
we borrow from the Dart algorithm the dropout strategy consisting in temporarily dropping some of the trees from the ensemble
while new weak learners are trained. However, differently from
this algorithm we drop permanently these trees on the basis of
smart choices driven by accuracy measured on the validation set.
Experiments conducted on publicly available datasets shows that
X-Dart outperforms Dart in training models providing the same
effectiveness by employing up to 40% less trees.

KEYWORDS
Multiple Additive Regression Trees, Dropout, Pruning.
ACM Reference format:
Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego,
and Salvatore Trani. 2017. X-Dart: Blending Dropout and Pruning for
Efficient Learning To Rank. In Proceedings of SIGIR ’17, August 07-11, 2017,
Shinjuku, Tokyo, Japan, , 5 pages.
DOI: http://dx.doi.org/10.1145/3077136.3080725

1

Salvatore Orlando

Ca’ Foscari Univ. of Venice, Italy
orlando@unive.it

INTRODUCTION

Learning-to-Rank (LtR) techniques leverage machine learning algorithms and large training datasets to build high-quality ranking models. A training dataset consists in a collection of querydocument pairs where each document is annotated with a relevance
label. These labels induce a partial ordering over the assessed documents, thus defining an ideal ranking which the LtR algorithm aims
at approximating. LtR boosting algorithms building forests of regression trees, e.g., Multiple Additive Regression Trees (MART) [4],
are considered nowadays the state-of-the-art solutions for addressing complex ranking problems [1, 5]. Their success is also witnessed
by the Kaggle 2015 competitions, where the majority of the winning
solutions exploited MART models, and by the KDD Cup 2015 where
MART-based algorithms were used by all the top-10 teams [2].
Publication rights licensed to ACM. ACM acknowledges that this contribution was
authored or co-authored by an employee, contractor or affiliate of a national government. As such, the Government retains a nonexclusive, royalty-free right to publish
or reproduce this article, or to allow others to do so, for Government purposes only.
SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.
978-1-4503-5022-8/17/08. . . $15.00
DOI: http://dx.doi.org/10.1145/3077136.3080725

1077

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Algorithm 1 The X-Dart Algorithm.
1:

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

models which are more efficient when applied. Second, the smaller
models are less prone to overfitting, and therefore X-Dart can
potentially achieve better quality figures than the original Dart.
The pruning step introduced by X-Dart is indeed driven by the
dropout selection strategy δ . The strategy δ determines the size k of
the dropout set D. When k is large, several trees are disregarded
before applying the learning algorithm A. A large perturbation
forces A to explore novel regions of the search space, potentially
moving away from local optima. On the other hand, a large k may
introduce too noise and drive the algorithm away from the optimal
solution. The size k has also a relevant impact on the Dart-based
normalization. A large k reduces the impact of the newly learned
tree T (line 11). The original Dart algorithm uses a value of k
proportional to the current ensemble size |E |. Increasing the value
of k during the Dart’s iteration has the same effect of a decaying
learning rate. During the first iterations a large learning rate is
used so as to speed-up the convergence, while a smaller learning
rate is used later on to achieve a fine-grained optimization.
The proposed dropout selection strategies are aimed at exploiting the above trade-offs among convergence speed, accuracy and
overfitting, by properly tuning k over the training iterations. We
investigate three different strategies.
Ratio (X-Dart R ). This strategy mimics the behavior of Dart.
At each iteration the dropout set size k is set equal to a fraction r
of the current ensemble size |E |. This provides a good convergence
speed and high resilience to overfitting as the model grows.
Fixed (X-Dart F ). The dropout set size k is constant along the
algorithms iterations. This variant stems from the observation
that large models are often required to achieve high quality, and
therefore a proportional dropout set size k may introduce too much
noise into the learning process. At the same time, a large k reduces
tree removal opportunities as it may be difficult find a tree able to
replace several. On the other hand, a small k allows to increase tree
removal chances, producing compact but highly effective forests.
This is apparent when k = 1, where a new tree is intended to simply
replace the dropout one (when the pruning condition is verified),
thus improving effectiveness without increasing the model size.
Adaptive (X-Dart A ). In order to benefit from the advantages
of the above two strategies we also propose an adaptive variant.
We aim at exploiting a small k when a promising search direction
is detected. This speeds-up convergence as it reduces the scaling
factor of the Dart-based normalization for the new tree. When
the algorithm falls into a local optimum, we aim at exploiting a
large k. To this end, the dropout set size k, initially set to k = 1, is
increased at each iteration by a constant value 0.5. Whenever the
loss of the new model after the introduction of T , either with or
without permanent removal of the dropout set, improves over the
smallest loss observed so far, this is interpreted as the discovery of a
promising search direction and the value of k is reset to k = 1 for the
subsequent iteration. Note that reducing k improves tree removal
chances, especially with large models. To avoid introducing large
noise the value of k is upper-bounded to a user-defined parameter.

function X-Dart(N , δ, L, A, σ )
N : ensemble size, δ : dropout strategy, L : loss function
A : learning algorithm (λ-Mart), σ : shrinkage
E←∅
. the current ensemble model
while |E | < N do
k ← DropOut-Size(E, δ )
. use dropout strategy
D ← DropOut-Set(E, k)
. random sample
E ←E\D
. discard dropout set
T ← σ · A.Train-Tree(E, L)
. train a new tree
if L(E ∪ T ) < LBest then
. pruning condition
E ← E ∪T
. permanent removal of D
else
E ← E ∪ {1/(k + σ ) · T }
. Dart normalization
E ← E ∪ {k/(k + σ ) · Td | Td ∈ D}
return E

trees, respectively. To the best of our knowledge, this is the first
work investigating pruning strategies in Dart-based algorithms.

2

METHODOLOGY

X-Dart improves Dart by embedding actual tree removal strategies that allow more compact ranking models to be learned. We
use the pseudo-code of X-Dart shown in Algorithm 1 to discuss
its differences with respect to Dart.
X-Dart is an iterative algorithm where at each iteration a given
base tree-learning algorithm A is used to train a new tree T which is
used to possibly grow the current forest E until the desired model
size N is achieved. At each iteration, the dropout size k is selected
first (line 4). In the original Dart algorithm, this size is set as a
fraction of the current ensemble size |E |. The proposed X-Dart
can exploit different selection strategies δ as discussed later on. A
dropout set D is then selected by picking uniformly at random k
trees from the current model E. This set of trees is discarded from
the current model E (line 6) when building a new tree T (line 7).
This perturbation strategy allows to reduce the risk of overfitting.
Note that a learning rate σ (or shrinkage) is applied to the tree
T produced by A. The Dart algorithm adds back the dropout
set D together with the new tree T to the pruned model E after
a normalization step (lines 11-12). As T is build without taking
into consideration D, adding back the dropout set would affect
negatively (overshoot [8]) the overall model prediction. To avoid
this effect, the leaves predictions of T are rescaled by a factor of
σ /(k +σ ), and for the trees in the dropout set by a factor of k/(k +σ ).
The resulting model E is further refined in the subsequent iteration.
The proposed X-Dart algorithm introduces a pruning step that
drops permanently the dropout set D from the model. After removing from E the dropout set and learning the new tree T , its benefit
is evaluated by applying the loss function L to the model E ∪ T . If
the loss L(E ∪ T ) is smaller than the smallest loss LBest observed
in any previous iteration, then we conclude that the quality of T
is larger than the joint quality of the dropout set D (line 8). In
this case, X-Dart behaves differently from Dart as it permanently
removes D and, consequently, it does not normalize the tree T
which is added to the pruned model E. The rationale of introducing
a pruning step is twofold. First, X-Dart aims at building compact

3

RESULTS AND DISCUSSION

We evaluate the effectiveness of X-Dart on two publicly available
datasets: MSLR-WEB30K-F1 (Fold 1) and Istella-S [6].

1078

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 1: Comparison of X-Dart against Dart and λ-Mart. NDCG@10 is reported at incremental steps of 100 trees. Values in
bold highlight the models that are statistically equivalent or better than the reference Dart model employing 500 trees.
Strategy

Dropout

Dart
λ-Mart
λ-Mart Adapt.

1.5%
–
–
1.0%
1.5%
2.0%
3.0%
1
2
3
4
UpperBound 5
UpperBound 10
UpperBound 1.5%

X-Dart
Ratio
X-Dart
Fixed
X-Dart
Adaptive

100
.4659
.4540
.4520
.4627
.4646
.4694
.4746
.4715
.4740
.4738
.4725
.4729
.4729
.4709

MSLR-WEB30K-F1
Model Size
200
300
400
500
.4737 .4775 .4795 .4807
.4694 .4745 .4752 .4766
–
–
–
–
.4713 .4749 .4766 .4784
.4729 .4758 .4785 .4788
.4757 .4778 .4797 .4812
.4785 .4792 .4808 .4809
.4722
–
–
–
.4770 .4784 .4776 .4777
.4775 .4783 .4799 .4793
.4776 .4788 .4800 .4804
.4777 .4794 .4805 .4802
.4765 .4770 .4789 .4793
.4737 .4768 .4770 .4766

The MSLR-WEB30K-F11 dataset is composed of 31,351 queries.
Each of the 3,771,125 query-document pairs is represented by
means of 136 features. The Istella-S2 dataset is composed of 33,018
queries and each of the 3,408,630 query-document pairs is represented by means of 220 features. The query-document pairs in both
the datasets are labeled with relevance judgments ranging from 0
(irrelevant) to 4 (perfectly relevant). Each dataset is split in three
sets: train (60%), validation (20%), and test (20%).
Training and validation data were used to train two reference
algorithms: i) λ-Mart, a list-wise algorithm that is capable of using
NDCG in its loss function, resulting in a predictor of the ranking [11] and ii) Dart [8] a novel algorithm employing dropout as
an effective regularization mechanism. The two algorithms were
fine-tuned by sweeping their parameters to maximize NDCG@10.
For the training of the λ-Mart models, the maximum number of
leaves was tested in the set {5, 10, 25, 50}, while the learning rate in
{0.05, 0.1, 0.5, 1.0}. To avoid overfitting, we implemented an early
stop condition, allowing the algorithm to train up to 1,500 trees
unless there is no improvement in NDCG@10 on the validation set
during the last 100 iterations. The Dart algorithm was trained up
to 500 trees by varying the dropout rate in {1%, 1.5%, 2%, 3%} and
the learning rate in {0.05, 0.1, 0.5, 1.0}.
The best performance of λ-Mart have been obtained by using a
learning rate equal to 0.05 and 50 leaves. The final sizes of the forests
are 1,199 and 1,500 on the MSLR-WEB30K-F1 and Istella-S datasets,
respectively. The former result is due to the aforementioned early
stop condition reached by the algorithm. Regarding Dart, the best
performing model was produced by using a learning rate of 1.0 and
a dropout rate of 1.5%.

1,199
N. A.

.4791
N. A.
N. A.
N. A.
N. A.
N. A.
N. A.
N. A.
N. A.
N. A.
N. A.
N. A.
N. A.

100
.7329
.6988
.7255
.7274
.7364
.7344
.7377
.7414
.7403
.7374
.7330
.7427
.7427
.7428

200
.7434
.7239
.7278
.7408
.7451
.7442
.7431
.7465
.7476
.7455
.7439
.7480
.7478
.7484

Istella-S
Model Size
300
400
.7477 .7494
.7343 .7397
.7303 .7314
.7467 .7499
.7488 .7517
.7477 .7490
.7449
–
.7465 .7471
.7504 .7517
.7482 .7508
.7491 .7511
.7517 .7532
.7514
–
.7507 .7518

500
.7516
.7433
.7320
.7513
.7526
–
–
.7482
.7529
.7532
.7535
.7526
–
.7525

1,500
N. A.

.7530
.7353
N. A.
N. A.
N. A.
N. A.
N. A.
N. A.
N. A.
N. A.
N. A.
N. A.
N. A.

λ-Mart model while on the latter dataset Dart performs similarly
to the full λ-Mart model of size 1,500. Dart is thus able to achieve
same or higher performance of λ-Mart by using up to 66% less
trees. The superiority of Dart in learning smaller models is thus
confirmed in both the datasets employed.
We claim that Dart is able to achieve significant improvements
by exploiting i) dropout as a regularization strategy and ii) an adaptive learning rate. We further investigate which one of the two
aspects is most responsible for the gain we observe with respect to
λ-Mart. To this end, we extend the λ-Mart algorithm by introducing an adaptive learning rate strategy, namely λ-Mart Adapt., that
simulates the behaviour of Dart without performing dropout during each training iteration. λ-Mart Adapt. thus adopts σ /(k + σ )
as learning rate for the i-th iteration, where k = r · i.
Table 1 reports the performance of the aforementioned algorithm when adopting the best performing learning rate of Dart,
i.e., σ = 1.0. λ-Mart Adapt. does not outperform the Dart algorithm in terms of NDCG@10. Moreover, on the MSLR-WEB30K-F1
dataset, the algorithm reached the early stop condition during the
first 100 iterations thus confirming that a large learning rate, even
when employed with adaptive learning rate strategies, does not
allow to mitigate the over-specialization phenomenon in the MART
algorithm. It is worth remarking that Dart and λ-Mart Adapt.
share almost the same initial training behavior since Dart cannot
drop any tree as long as r · i < 1. From this point on, and differently
from λ-Mart Adapt., Dart is able to go beyond this local optimum
while λ-Mart Adapt. soon reaches the early stop condition. This
analysis reveals that the performance of Dart are mostly due to the
adoption of dropout as a regularization strategy since the adaptive
learning rate alone is not sufficient for handling over-specialization.

Performance of Dart. We first investigate the effectiveness of
Dart compared to that of λ-Mart (Table 1). Interestingly Dart
(500 trees) is able to achieve a NDCG@10 of 0.4807 on MSLRWEB30K-F1 and of 0.7516 on Istella-S (see Table 1), thus outperforming the original λ-Mart models of similar size. On the former
dataset the performance of Dart is higher than that of the full

Performance of X-Dart. We analyse the performance of XDart3 by reporting the effectiveness of each of the three dropout
strategies introduced in Section 2, namely X-Dart R , X-Dart F ,
X-Dart A . We experiment X-Dart R by varying the dropout rate in
the set {1.0%, 1.5%, 2.0%, 3.0%} while X-Dart F employs a constant

1 http://research.microsoft.com/en-us/projects/mslr/

2 http://blog.istella.it/istella-learning-to-rank-dataset/

3 The

1079

source code of X-Dart is available at: http://quickrank.isti.cnr.it.

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Dart (dashed horizontal lines) at incremental steps of 100 trees on
the Istella-S dataset. We report the X-Dart R strategy employing a
dropout rate of 1.5%, X-Dart F with a fixed k = 2, and X-Dart A
using an upper bound of 5. It is worth noting that all X-Dart
strategies outperform the Dart model of equivalent size and, more
importantly, they allow to always save at least 100 trees without
affecting the quality of the resulting model. Moreover, the effectiveness of X-Dart employing 400 trees is always higher or equal
than the one provided by the reference Dart model, with X-Dart A
showing statistically significant improvement.
The proposed experimental evaluation allows for a deeper understanding of the dynamics behind the Dart algorithm and, more
importantly, confirms the validity of the proposed X-Dart strategies as a way to blend pruning in the training of Dart ensembles.

4

CONCLUSION AND FUTURE WORK

We proposed X-Dart, a new LtR algorithm that improves Dart
by embedding tree removal strategies within the learning process.
X-Dart mutes some of the trees from the ensemble built so far
(dropouts) before learning an additional tree but, differently from
Dart, it drops permanently these trees when their contribution
to the accuracy is considered negligible. By doing so, X-Dart
is able to reduce overfitting affecting the learned model as the
strategies proposed for permanently deleting the trees allow for a
better exploration of the search space. Experiments conducted on
publicly available datasets confirm the validity of the proposals by
showing that X-Dart achieves statistically equivalent effectiveness
(measured in terms of NDCG@10) with respect to the reference
Dart and λ-Mart models by employing up to 40% and 75% less
trees, respectively. As future work we intend to investigate in depth
the behavior of X-Dart when training forests of larger size and to
assess the performance of X-Dart on other LtR datasets.
Acknowledgments. This work was supported by EC H2020 INFRAIA1-2014-2015 SoBigData: Social Mining & Big Data Ecosystem (654024).

Figure 1: Effectiveness of X-Dart strategies and Dart in
terms of NDCG@10 on Istella-S dataset.
dropout k in {1, 2, 3, 4}. We test X-Dart A by using three different upper bounds for k. Two of them are fixed: 5 and 10, while
the third one is expressed as a percentage of the ensemble size,
i.e., 1.5%. For each X-Dart strategy reported in Table 1, values
in bold highlight performance statistically greater than or equal
to that of the reference Dart model employing 500 trees. We do
the analysis by employing the randomization test [9] with 10, 000
permutations and p-value ≤ 0.05 to assess if the differences in terms
of NDCG@10 between the reference and the pruned models are
statistically significant or not.
The best performance of X-Dart R on the MSLR-WEB30K-F1
dataset is observed when employing a dropout rate equal to 2.0%
and 3.0%. Indeed, both the models of 500 trees achieve an higher
value of NDCG@10 w.r.t. the reference Dart despite the difference
is not statistically significant. More interestingly, X-Dart R allows
to achieve a statistically equivalent performance w.r.t. Dart by
employing almost half of the trees needed by Dart, i.e., 300 trees
against 500 used by Dart, with a saving of up to 40% in terms
of efficiency. A similar behavior can be observed on the Istella-S
dataset. Here, the best dropout rate is 1.5%, where X-Dart R is able
to achieve a statistically equivalent performance w.r.t. Dart by
employing 100 trees less.
The X-Dart F strategy shows good performance on the Istella-S
dataset by employing k ∈ {2, 3, 4} with a NDCG@10 higher than
the reference Dart model. By analyzing the statistical equivalence,
we observe that X-Dart F allows to prune up to 200 and 100 trees
on Istella-S and MSLR-WEB30K-F1 datasets, respectively.
X-Dart A is globally the best performing strategy as it allows to
save up to 200 and 1,100 trees w.r.t. Dart and λ-Mart respectively
on both the datasets when employing an upper bound of 5. This
is a consequence of how X-Dart A works as it inherits the advantages of the two previous dropout strategies. We also remark that
the same upperbound resulted the best on both datasets. We can
conclude that the dynamic strategy X-Dart A can effectively adapt
on different datasets.
To highlight the effectiveness of X-Dart, Figure 1 reports the
performance of the three X-Dart strategies (solid lines) against

REFERENCES
[1] G. Capannini, C. Lucchese, F. M. Nardini, S. Orlando, R. Perego, and N. Tonellotto.
2016. Quality versus efficiency in document scoring with learning-to-rank
models. Information Processing & Management 52, 6 (2016), 1161 – 1177.
[2] T. Chen and C. Guestrin. 2016. XGBoost: A Scalable Tree Boosting System. In
Proc. ACM SIGKDD. ACM, 785–794.
[3] D. Dato, C. Lucchese, F. M. Nardini, S. Orlando, R. Perego, N. Tonellotto, and
R. Venturini. 2016. Fast Ranking with Additive Ensembles of Oblivious and
Non-Oblivious Regression Trees. ACM Trans. Inf. Syst. 35, 2, Article 15 (2016),
15:1–15:31 pages.
[4] J. H. Friedman. 2000. Greedy Function Approximation: A Gradient Boosting
Machine. Annals of Statistics 29 (2000), 1189–1232.
[5] A. Gulin, I. Kuralenok, and D. Pavlov. 2011. Winning The Transfer Learning
Track of Yahoo!’s Learning To Rank Challenge with YetiRank.. In Yahoo! Learning
to Rank Challenge. 63–76.
[6] C. Lucchese, F. M. Nardini, S. Orlando, R. Perego, F. Silvestri, and S. Trani. PostLearning Optimization of Tree Ensembles for Efficient Ranking. In Proc. ACM
SIGIR ’16’. ACM, 949–952.
[7] C. Lucchese, F. M. Nardini, S. Orlando, R. Perego, N. Tonellotto, and R. Venturini. 2015. QuickScorer: A Fast Algorithm to Rank Documents with Additive
Ensembles of Regression Trees. In Proc. ACM SIGIR. 73–82.
[8] K.V. Rashmi and R. Gilad-Bachrach. 2015. Dart: Dropouts meet multiple additive
regression trees. Journal of Machine Learning Research 38 (2015).
[9] M. D. Smucker, J. Allan, and B. Carterette. 2007. A Comparison of Statistical
Significance Tests for Information Retrieval Evaluation. In Proc. CIKM. ACM.
[10] N. Srivastava, G. E Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov.
2014. Dropout: a simple way to prevent neural networks from overfitting. Journal
of Machine Learning Research 15, 1 (2014), 1929–1958.
[11] Q. Wu, C.J.C. Burges, K.M. Svore, and J. Gao. 2010. Adapting boosting for
information retrieval measures. Information Retrieval (2010).

1080

