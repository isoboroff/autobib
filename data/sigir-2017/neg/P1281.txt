Demonstration Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

RankEval: An Evaluation and Analysis Framework for
Learning-to-Rank Solutions
Claudio Lucchese

ISTI-CNR
Via G. Moruzzi, 1
Pisa, Italy
claudio.lucchese@isti.cnr.it

Cristina Ioana Muntean
ISTI-CNR
Via G. Moruzzi, 1
Pisa, Italy
cristina.muntean@isti.cnr.it

Raffaele Perego

ISTI-CNR
Via G. Moruzzi, 1
Pisa, Italy
francomaria.nardini@isti.cnr.it

Salvatore Trani

ISTI-CNR
Via G. Moruzzi, 1
Pisa, Italy
raffaele.perego@isti.cnr.it

ISTI-CNR
Via G. Moruzzi, 1
Pisa, Italy
salvatore.trani@isti.cnr.it
training dataset consists in a collection of query-document pairs
where each example is annotated with a relevance label. A LtR
algorithm aims at learning a document scoring function that approximates the ideal ranking induced by the training relevance
labels.
Today, models exploiting forests of regression trees, e.g., Gradient Boosted Regression Trees (GBRTs) [4], are considered as
the most effective LtR approaches [5]. Their success is also witnessed by the results of the Kaggle 2015 competitions, where GBRTs
were adopted by the majority of the winning solutions, and by the
KDD Cup 2015 where all of the top-10 teams used GBRT-based
solutions [2]. Indeed, the success of GBRTs fostered the development of several open-source implementations. Among them we
cite LightGBM [6], XGBoost [2], QuickRank [1], RankLib [3], and
scikit-learn [8]. However, all these libraries offer very limited
help for the tuning and evaluation of the trained models. However,
their implementation of most common IR evaluation metrics differ significantly. For example, when considering NDCG, a query
with no relevant results is assigned a score equal to -1 in LightGBM,
equal to 0 in QuickRank and RankLib, and equal to 0.5 in the Yahoo
Learning to Rank Challenge. Today is thus not trivial to evaluate
and compare models learnt with different tools since a common
and open-source framework for comprehensively evaluating LtR
solutions is still missing.
RankEval tries to fill this gap by proposing a comprehensive
framework for tuning, evaluation and comparison of GBRT LtR
models. Like trec eval [7] in the TREC framework, it aims at becoming a new de facto standard for evaluation among IR researchers
and practitioners. While the popular LtR libraries mentioned above
focus on the training phase, and provide very little support to the
analysis of the produced model, RankEval provides a consistent
evaluation framework, offering the possibility of comparing different models trained with different libraries, and, even more, helping
users in understanding the model and tuning its effectiveness.
RankEval supports an in-depth analysis of a given LtR model.
Regarding effectiveness analysis, RankEval allows to compare different models under different IR metrics (MAP, DCG, NDCG, etc.),
to identify low-performing queries in the test set, to measure quality
on varying sizes of the model, and to evaluate the contribution of
each feature and each tree to the model accuracy. RankEval allows

ABSTRACT
In this demo paper we propose RankEval, an open-source tool for
the analysis and evaluation of Learning-to-Rank (LtR) models based
on ensembles of regression trees. Gradient Boosted Regression
Trees (GBRT) is a flexible statistical learning technique for classification and regression at the state of the art for training effective LtR
solutions. Indeed, the success of GBRT fostered the development of
several open-source LtR libraries targeting efficiency of the learning phase and effectiveness of the resulting models. However, these
libraries offer only very limited help for the tuning and evaluation
of the trained models. In addition, the implementations provided for
even the most traditional IR evaluation metrics differ from library
to library, thus making the objective evaluation and comparison
between trained models a difficult task. RankEval addresses these
issues by providing a common ground for LtR libraries that offers
useful and interoperable tools for a comprehensive comparison and
in-depth analysis of ranking models.

KEYWORDS
Learning to Rank, evaluation, analysis.
ACM Reference format:
Claudio Lucchese, Cristina Ioana Muntean, Franco Maria Nardini, Raffaele
Perego, and Salvatore Trani. 2017. RankEval: An Evaluation and Analysis
Framework for Learning-to-Rank Solutions. In Proceedings of SIGIR ’17,
August 07-11, 2017, Shinjuku, Tokyo, Japan, , 4 pages.
DOI: http://dx.doi.org/10.1145/3077136.3084140

1

Franco Maria Nardini

INTRODUCTION

In this demo paper we propose RankEval, an extensible tool for the
analysis and evaluation of LtR ensemble-based models. LtR techniques leverage efficient machine learning algorithms and large
amounts of training data to build high-quality ranking models. A
Publication rights licensed to ACM. ACM acknowledges that this contribution was
authored or co-authored by an employee, contractor or affiliate of a national government. As such, the Government retains a nonexclusive, royalty-free right to publish
or reproduce this article, or to allow others to do so, for Government purposes only.
SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.
978-1-4503-5022-8/17/08. . . $15.00
DOI: http://dx.doi.org/10.1145/3077136.3084140

1281

Demonstration Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

to conduct a structural analysis reporting statistics about shape,
depth and balancing of trees in the forest. Given multiple models,
it supports statistical validation with standard randomisation tests.
Finally, it allows to translate a model into the different formats
adopted by the most popular tools.
RankEval is implemented as a lightweight Python library. It
can be used within a Jupyter notebook1 in which case the user
can take advantage of inline plots and interactive visualisation
features. The proposed functionalities may take advantage of a
parallel processing infrastructure when available.
We believe that RankEval can bring significant value to the
scientific community as well as to industrial engineers. On the one
hand, RankEval allows a fair comparison and analysis among stateof-the-art LtR methods. On the other hand, “this high value, low
cost” tool can speed up the process of model tuning by supporting
the feature analysis and engineering phase. Ultimately, RankEval is
designed with the goal of speeding up the process of investigating
strengths and weaknesses of given LtR models, and to support the
user in achieving an optimal model fine-tuning.

2

Figure 1: Code snippet measuring model performance.
RankEval is the first tool focusing on providing a common framework for evaluation and analysis of machine-learned tree-based
models. It provides a common ground between all pre-existing
tools and offers services which support the interpretation of differently generated models in a unified environment, allowing an easy,
comprehensive comparison and in-depth analysis.

RELATED WORK

Several tools propose implementations of LtR algorithms and offer
some basic evaluation facilities. In the following we discuss existing
projects and what they offer in terms of evaluation support.
A very popular tool is scikit-learn [8], a general machine
learning library in Python which implements several algorithms
from simple regressors to random forests classifiers, gradient boosting and so on. For each of these algorithms the library implements
evaluation metrics (accuracy, average precision, F1, precision, recall,
mean squared/absolute error) for quantifying the quality of predictions, but does not go into the depth of analysing the generated
models. Moreover, the framework does not provide dedicated LtR
functionalities, both in terms of algorithms or metrics. Despite this
lack of support, scikit-learn is successfully used for adressing
ranking problems with regression-based algorithms.
RankLib [3] is a Java LtR library currently implementing eight
popular algorithms, as well as several retrieval evaluation metrics.
Regarding model evaluation, RankLib provides basic support by
computing win/loss statistics and relative p-value.
QuickRank [1] is a C++ library offering a variety of LtR algorithms. Differently from other tools, it introduces post-learning
optimisations pipelined with the LtR algorithms.
XGBoost [2] provides “scalable, portable and distributed gradient boosting (GBM, GBRT, GBDT)” for different tasks: regression,
classification, ranking. It supports multiple languages (C++, Java,
Python R and Julia) and it is extensively used by machine learning
practitioners: it recently resulted to be the most popular GBRT
solution adopted by Kaggle contributors.
LightGBM [6] is a gradient boosting framework developed by
Microsoft. It is designed to be distributed and efficient and it was
shown to outperform existing boosting framework on both efficiency and accuracy, with significant lower memory consumption.
All the tools mentioned above are mostly oriented to the efficiency of the training phase rather than to the thorough analysis of
learned models and their effectiveness. To the best of our knowledge

3

RANKEVAL

In the following, we summarise RankEval functionalities by grouping them along five dimensions, namely: effectiveness analysis, feature analysis, structural analysis, topological analysis, interoperability among GBRT libraries. These functionalities can be applied to
several models at the same time, so to have a direct comparison of
the analysis performed. Below, we provide a detailed explanation
along with concrete examples of use.

3.1

Effectiveness Analysis

This class of functionalities focuses on assessing the performance
of the models in terms of accuracy.
Model performance. RankEval provides a general and unified
framework for measuring model performance in terms of one or
more of the following metrics: DCG, NDCG, MAP, ERR, MRR,
pfound, RBP, precision, recall, Spearman’s Rho, Kendall’s tau. This
functionality is important because different libraries available today
do this evaluation by implementing different versions of the same
metric. Consider for instance list-wise metrics, the way ties or
queries without relevant results are dealt with can yield inconsistent
results. RankEval provides a detailed documentation of the metrics
implemented and the possibility for the user to easily control and
customise each of them with preferred settings. Figure 1 reports a
snippet of code showing how easy, user-friendly and customisable
is the evaluation process offered by RankEval.
Tree-wise performance. RankEval provides the possibility to
evaluate different effectiveness metrics incrementally by varying
the number of trees in the model, i.e., top-k trees. Moreover, it
allows to measure the contribution of each single tree to the global
model accuracy independently of their order in the forest (see
Fig. 2). The first functionality can be fruitfully employed to find
the “optimal” model size, e.g., by evaluating the overfitting effect
on a validation set. By evaluating the impact of each tree to the

1 http://jupyter.org/

1282

Demonstration Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 2: Tree-Wise model comparison (left) and average
contribution of each tree to the global score (right).
Figure 4: Rank confusion matrix.
total global accuracy, the second functionality allows to reduce the
size of the forest by identifying subsets of trees to prune, as their
contribution to the model is limited.

3.2

Query-wise performance. RankEval allows to compute the cumulative distribution of a given performance metric, e.g., fraction
of queries with a NDCG score smaller than any given threshold τ ,
over a specific set of queries (see Figure 3(left)).
Query class performance. RankEval complements the feature
above with a comprehensive analysis of the effectiveness of a given
model by providing a breakdown of the performance over query
class. Whenever query class information is given, e.g., navigational, informational, transactional, number of terms composing
the query, etc., RankEval provides a complete analysis of the model
effectiveness over such classes. This analysis is extremely important especially in a production environment, as it allows to better
calibrate the ranking infrastructure w.r.t a specific context.
Document graded-relevance performance. For each document
relevance class RankEval allows the evaluation of the cumulative
predicted score distribution. As an example, in Figure 3 (right), for
each relevance label available in the data, RankEval renders one
curve detailing, for each possible document score, the fraction of
documents with a smaller predicted score. The bigger the distance
amongst curves the larger the model’s discriminative power.
Rank confusion matrix. RankEval allows for a novel rank-oriented
confusion matrix by reporting for any given relevance label li , the
number of documents with a predicted score smaller than documents with label l j (see Figure 4). When li > l j , it corresponds
to the number of mis-ranked document pairs. This can be considered as a breakdown over the relevance labels of the ranking
effectiveness of the model.

Feature analysis

The feature analysis conducted by RankEval is aimed at investigating how features are used in a given model and their impact on
accuracy. Similar functionalities are also present in scikit-learn
for tasks different from ranking, and feature importance is evaluated by a few LtR software. RankEval contributes in this specific
topic by enabling the analysis on all the available ranking metrics,
instead of doing it on the root mean squared error (RMSE) only.
Feature importance. RankEval allows an evaluation of the contribution of each feature to the global error reduction. This analysis
can be computed on the whole forest or incrementally, by taking
into account each tree of the forest. Moreover, this error reduction
breakdown can be provided in combination with specific metrics of
interest, e.g., (NDCG, MAP, etc.). By doing so, RankEval enables
the possibility to identify a subset of high-quality features, and to
validate their contribution across different metrics and datasets.
Eventually, RankEval provides several user-friendly and customizable ways of reporting this information to allow an easy interpretation of results.
Feature use statistics. Additional insights on the use of features
can be provided by reporting statistics about how and where features appears in the forest. RankEval allows for a quick assessment
of several indicators like, e.g., fraction of nodes of the forest using a
given feature, their average depth in the ensemble, the distribution
of thresholds associated with the feature, etc. These indicators have
an important role in engineering a specific LtR model by allowing
the researcher/IR practitioner to better understand the role and the
use of each feature in the forest. As an example, when this analysis
shows that a feature is used frequently and always in the top nodes
of the trees is it more likely to impact significantly on the score of
the ranked documents.

3.3

Statistical Analysis

Statistical Significance. Statistical significance tests are of paramount importance in IR, helping to assess when and how a given
solution provides statistically better performance w.r.t a competitor.
RankEval allows the user-friendly computation and visualization
of statistical significance tests on two given LtR models. Several
tests are implemented, including the Randomisation test as recommended in [9].
Bias vs. variance decomposition. To further investigate strengths
and weaknesses of a specific LtR algorithm, RankEval provides bias

Figure 3: Query-wise performance (left) and Document
graded-relevance performance analyses (right).

1283

Demonstration Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

vs. variance analysis. To this end, RankEval automatically produces
several bootstrap samples (without replacement) of the training
data on which the algorithm under analysis is tested. The set of
learnt models are used to analyse the bias vs. variance and error
decomposition of the algorithm as a function of a specific target
metric given by the user.

3.4

Topological analysis

This analysis focuses on the topological characteristics of a given
LtR model. The statistics provided by RankEval include min/max/avg
depth of trees in the model, most frequent shapes of the trees,
i.e., balanced trees or chain-like structures. This is another important feature of RankEval allowing to identify overfitting behaviours, usually associated with deep chains, and to support the
researcher/IR practitioner in fine-tuning the hyper-parameters of
the LtR algorithm such as number of trees, maximum depth, maximum number of leaves, etc.

3.5

Interoperability among GBRT libraries

The aim of RankEval is to provide a unified, user-friendly and
comprehensive framework for in-depth analysis of LtR models. To
enable this, RankEval must be able to analyse models trained with
different libraries and saved in different formats. RankEval can
thus read and write ranking models in any of the formats adopted
by the five most popular GBRT implementations, namely XGBoost,
LightGBM, scikit-learn, QuickRank, and RankLib. Additionally,
it provides format conversion facilities that allow users to build a
model with one tool and apply it through a different software.

3.6

How to use RankEval

RankEval is an open-source project2 written in Python to ensure
flexibility, extensibility, and efficiency. The core of the library exploits numpy, scipy and scikit-learn.
There are two ways in which RankEval can be used: i) as a
Python module (offering also a command line interface) and ii) as a
Python Jupyter notebook. The functionalities presented above are
implemented in submodules. The results of each test are presented
in the form of report tables and plots, which can be either saved
into files or observed inline, if running the module in a Python
Jupyter notebook.

4

REFERENCES
[1] Gabriele Capannini, Domenico Dato, Claudio Lucchese, Monica Mori,
Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, and Nicola Tonellotto.
2015. QuickRank: a C++ Suite of Learning to Rank Algorithms. In 6th Italian
Information Retrieval Workshop.
[2] Tianqi Chen and Carlos Guestrin. 2016. XGBoost: A Scalable Tree Boosting
System. In Proceedings of the 22Nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (KDD ’16). ACM, New York, NY, USA,
785–794.
[3] Van Dang. 2011. RankLib. Online. (2011). http://www.cs.umass.edu/∼
[4] Jerome H. Friedman. 2000. Greedy Function Approximation: A Gradient Boosting
Machine. Annals of Statistics 29 (2000), 1189–1232.
[5] Andrey Gulin, Igor Kuralenok, and Dmitry Pavlov. 2011. Winning The Transfer
Learning Track of Yahoo!’s Learning To Rank Challenge with YetiRank.. In Yahoo!
Learning to Rank Challenge. 63–76.
[6] Microsoft. 2016. LightGBM. Online. (2016). https://github.com/Microsoft/
LightGBM
[7] NIST. 2017. trec eval. Online. (2017). http://trec.nist.gov/trec eval/
[8] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.
Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine
Learning in Python. Journal of Machine Learning Research 12 (2011), 2825–2830.
[9] Mark D. Smucker, James Allan, and Ben Carterette. 2007. A Comparison of
Statistical Significance Tests for Information Retrieval Evaluation. In Proceedings
of the Sixteenth ACM Conference on Conference on Information and Knowledge
Management (CIKM ’07). ACM, New York, NY, USA, 623–632.

RANKEVAL DEMO AT SIGIR 2017

We propose RankEval as a demonstration at ACM SIGIR 2017, consisting of a light and flexible solution for LtR model evaluation.
As we argue in this section, it is intended as a product itself more
than a simple evaluation module for scientific/research purposes.
The target audience for such a product is varied. It can be used
by research scientists in the LtR field, but also software engineers
and product managers who look for the best practical solution for
analysing and solving their machine-learned ranking problems.
To illustrate RankEval’s applicability, suppose we have the following three scenarios: i) an engineer is looking for the best hyperparameter combination for training a target model, ii) a research
scientist wants to obtain a fair evaluation between his model and
a model derived with one competitor solution, and iii) a product
manager wants to experiment the contribution and effectiveness of
2 Interested

new features to decide whether to add them or not in the production
search environment.
In i) RankEval helps the engineer to evaluate the model by using
the topological analysis offering insights about the number of leaves,
tree depth, and the tree-wise performance of the forest, which plots
performance improvements by increasing the number of trees. In
addition, for efficiency reasons he may decide to use only 50% of the
trees as the statistical analysis submodule confirms that a smaller
forest achieves statistically similar results in terms of accuracy w.r.t.
the original one.
In ii) RankEval allows to import and process models through
the interoperability among GBRT software offering the possibility
to unify the models to a single, comparable format by using the
conversion facilites. Not only can the researcher evaluate model
performance individually, but also compare them by plotting the
performance metrics of the two models in the same view and by
exploiting the statistical analysis.
In iii) RankEval enables the manager to understand the contribution of the features to a model. Using the feature analysis submodule,
she is able to compute feature importance and usage statistics in
several ways. By loading distinct models, she can observe if the
features have the same impact and how they may differ.
The demonstration of RankEval at ACM SIGIR 2017 will allow
the three target users above to directly interact with its features.
We will provide two PCs with a ready-to-use RankEval installation
and we will invite people to experience and test RankEval on their
own ranking models, so as to understand the potential benefits of
the analysis that RankEval provides.
Participants will also have the possibility to test RankEval on a
repository of several ready-to-use ranking models built on publicly
available LtR datasets by means of five different libraries: LightGBM,
XGBoost, QuickRank, RankLib, and scikit-learn. This will allow
them to directly compare and assess the different results such libraries produce.
Acknowledgments. This work was supported by the EC H2020
INFRAIA-1-2014-2015 SoBigData (654024).

users can find it at: http://rankeval.isti.cnr.it.

1284

