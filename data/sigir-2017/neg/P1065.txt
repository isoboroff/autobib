Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Gaussian Embeddings for Collaborative Filtering
Ludovic Dos Santos

Benjamin Piwowarski

Patrick Gallinari

Sorbonne Universities, UPMC Univ
Paris 06, CNRS, LIP6 UMR 7606
ludovic.dossantos@lip6.fr

Sorbonne Universities, UPMC Univ
Paris 06, CNRS, LIP6 UMR 7606
benjamin.piwowarski@lip6.fr

Sorbonne Universities, UPMC Univ
Paris 06, CNRS, LIP6 UMR 7606
patrick.gallinari@lip6.fr

ABSTRACT

sources of information [12] as well as their good performance, both
in efficiency and effectiveness [6]. The most successful approaches
are based on learning-to-rank approaches such as [5, 10].
Despite their successes, one limitation of those models is their
lack of handling of uncertainty. Even when they are based on a
probabilistic model, e.g. [5, 7], they only suppose a Gaussian prior
over the user and item embeddings but still learn a deterministic
representation. The prior is thus only used as a regularization term
on user and item representations.
Uncertain representation can have various causes, either related
to the lack of information (new users or items, or users that did not
rate any movie of a given genre), or to contradictions between user
or item ratings. To illustrate the latter, let us take a user that likes
only a part of Kung Fu movies, but with no clear pattern, i.e. no
subgenre. With usual approaches, such a user will have a component
set to zero for the direction of the space corresponding to Kung Fu.
With our proposed approach, we would still have a zero mean, but
with a high variance. Our hypothesis is that, because of these two
factors, namely cold start and conflicting information, training will
result in learned representations with different confidences, and
that this uncertainty is important for recommending.
Moreover, using a density rather than a fixed point has an important potential for developing new approaches to recommendation.
First, instead of computing a score for each item, the model can compute a probability distribution, as well as the covariance between
two different item scores. This can be interesting when trying to
diversify result lists, since this information can be leveraged e.g.
by Portfolio approaches [9]: The model could propose diversified
lists with different degrees of risk. Secondly, such models are interesting because they can serve as a basis for integrating different
sources of external information, and thus serve as a better bridge
between content-based approaches and collaborative filtering ones.
Thirdly, time could be modeled by supposing that the variance of
the representation increases with time if no new information (i.e.
user ratings) are provided.
Our model use Gaussian embeddings which have been recently
and successfully used for learning word [8], knowledge graph [4]
or nodes in a graph [2] embeddings. More precisely, each user or
item representation corresponds to a Gaussian distribution where
the mean and the variance are learned. Said otherwise, instead
of a fixed point in space, a density represents each item or user.
The variance term is a measure of uncertainty associated to the
user/item representation.
In the following, we first describe the model, and then show
experimentally that it performs very well compared to state-of-theart approaches on three different datasets. Our contributions are
(1) the use of a new type of representations for items and users in
collaborative filtering; (2) extensive experimental work that shows

Most collaborative filtering systems, such as matrix factorization,
use vector representations for items and users. Those representations are deterministic, and do not allow modeling the uncertainty
of the learned representation, which can be useful when a user has
a small number of rated items (cold start), or when there is conflicting information about the behavior of a user or the ratings of an
item. In this paper, we leverage recent works in learning Gaussian
embeddings for the recommendation task. We show that this model
performs well on three representative collections (Yahoo, Yelp and
MovieLens) and analyze learned representations.

CCS CONCEPTS
• Information systems → Recommender systems;

KEYWORDS
recommender system, gaussian embeddings, probabilistic representation
ACM Reference format:
Ludovic Dos Santos, Benjamin Piwowarski, and Patrick Gallinari. 2017.
Gaussian Embeddings for Collaborative Filtering. In Proceedings of SIGIR’17,
August 7–11, 2017, Shinjuku, Tokyo, Japan, , 4 pages.
DOI: http://dx.doi.org/10.1145/3077136.3080722

1

INTRODUCTION

Recommender systems help users find items they might like in
large repositories, thus alleviating the information overload problem. Methods for recommending text items can be broadly classified
into collaborative filtering (CF), content-based, and hybrid methods
[6]. Among recommender systems, collaborative filtering systems
are the most successful and widely used because they leverage past
ratings from users and items, while content-based approaches typically build users (or items) profiles from items (or users) predefined
representations. Among collaborative recommender systems we
are interested in those that represent users and items as vectors in a
common latent recommendation space. Matrix factorization derivatives are a typical example of such models. The main assumption is
that the inner product between a user and an item representation is
correlated with the rating the user would give to the item. The main
interest of these models is their potential for integrating different
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR’17, August 7–11, 2017, Shinjuku, Tokyo, Japan
© 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00
DOI: http://dx.doi.org/10.1145/3077136.3080722

1065

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

(1) Item representations are independent given the model parameters. This implies that
 the difference Xj − Xi follows
a normal distribution N µ j − µ i , Σi + Σ j
(2) We approximate the inner product distribution by a Gaussian using moment matching (mean and variance), using
the results from [1] who defined the moments of the inner product of two Gaussian random variables; with our
notations, the two first moments of Zui j are defined as:

the good performance of the model; (3) qualitative analysis to show
how the variance component is used in the model.

2

MODEL

Our model is learned through a pairwise learning-to-rank criteria
that was first proposed by Rendle et al. [5] for collaborative filtering
(Bayesian Personalized Ranking, BPR). Formally, they optimize the
maximum a posteriori of the training dataset D, i.e. p (Θ|D) ∝
p (D |Θ) p (Θ) where D represents the set of all ordered triples
(u, i, j) – the user u ∈ U prefers item i ∈ I to item j ∈ I, and Θ,
the model parameters. The factor p (Θ) corresponds to the prior on
the item and user representations (a Gaussian prior in [5]). Then,
using the standard hypothesis of the independence of samples given
the model parameters, we have
Y
p (D |Θ) =
p (i >u j |Θ)
(1)

f
g
E Zui j = µu> (µ j − µ i )
(3)
f
g



>
Var Zui j = 2µu> Σi + Σ j µu + µ j − µ i Σu (µ j − µ i )
 

+ tr Σu Σ j + Σi
X



2
2
=
σik + σ jk 2µuk
+ σuk + σuk µ jk − µ ik
(4)
k

(u,i, j ) ∈ D

The above assumptions allow us to write:
Z bi −b j

f
g
f
g
p (i >u j |Θ) ≈
N x; E Zui j , Var Zui j dx

In [5], the probability that user u prefers item i to item j, noted
by i >u j, is given by the sigmoid function of the difference of the
inner products, that is:


p (i >u j |Θ) = σ x i · xu + bi − x j · xu − b j
(2)

which is the Normal cumulative distribution function. This function
can be easily differentiated with respect to the µ •k and σ•k (• is a
user or an item) since it can be rewritten using the error function
(ERF), which is trivially derivable.
From Equations (5) and (4) and ignoring the biases, we can see
that the difference between the inner products µu · µ i and µu · µ j is
controlled by the variance of the user and the item (especially for
the components of the means that have a high magnitude) – the
larger, the closer to 0.5 the probability. This is the main difference
with other matrix factorization-based approaches.
Finally, we have to define the prior distribution over the parameters Θ, i.e. the means and variances. As we consider only a diagonal
covariance matrix, we can consider an independent prior for each
mean and variance, i.e. for any µ •k and Σ•kk . In this case, using a
normal-gamma prior is a natural choice since it is the conjugate
distribution of a normal. More specifically, we suppose that


µ •k , Σ−1
(6)
•kk ∼ NormalGamma (ν, λ, α, β )

where bi (resp. b j ) is the bias for item i (resp. j).
The Gaussian Embeddings Ranking Model. In this work, while we
start with Eq. (1), the different variables x • (• is either a user u or an
item j) are random variables, denoted X• , and not elements of a vector space. We can hence estimate directly probability p (i >u j |Θ)
of the inner product Xu · Xi being higher than Xu · Xi .
We start by supposing that user and item representations follow
a normal distribution: for any user u and item i,
Xi ∼ N (µ i , Σi ) and Xu ∼ N (µu , Σu )
We suppose that Σ• = diag(σ•1 , . . . , σ•N ) is a diagonal covariance matrix to limit the complexity of the model (N is the dimension
of the latent space): in practice, we have 2N parameters for each
user, and 2N + 1 for each item (+1 because of the bias). The variance
is associated with each element of the canonical basis of the vector
space. In practice, we hypothesize that this helps the model to learn
meaningful dimensions, since it forces the use of the canonical
latent space basis.
Since Xi and Xu are random variables, their inner product is also
a random variable, and we do not need to use the sigmoid function
of Eq. (2) as for BPR to compute the probability that user u prefers
i to j. We can instead directly use the random variables defined
above, which leads to:

with ν = 0, λ = 1, α = 2 and β = 2 – these parameters do not
change much the solution, and were chosen so that user and item
representations are not too much constrained. In the absence of
data, this would set, for each component, the mean to 0 and the
variance to 1, which corresponds to the mode of the normal-gamma
distribution.
Loss function. Plugging in Eq. (6) and (1) gives
X
L = log p (Θ) +
log p (i >u j |Θ)
(u,i, j ) ∈D S



p (i >u j |Θ) = p Xi · Xu + bi > Xj · Xu + b j |Θ




= p Xu · Xj − Xi < bi − b j Θ
|
{z
}

2 

X   1
X
2β + λµ •i

+
 − α log (Σ•i ) −
=
−c
bi2

2
2Σ


•i
•,i 
i

X
+
log p (i >u j |Θ)

Zui j

(u,i, j ) ∈ D

where bi and b j are the item biases1 . To compute the above equation,
we use the following two simplifications:
1 In

(5)

−∞

+

where • is a user or an item, = means equal up to a constant, and
p (i >u j |Θ) is given by Eqs. (5), (3) and (4), and where we suppose
a normal prior for the biases (to avoid overfitting). The parameters

this work, we did not consider them to be random variables, but they could be

1066

Short Research Paper

Users
10
20
50
10
Yahoo!
3,386 1,645 286 1,000
MovieLens 743 643 448 1,336
Yelp
13,775 8,828 3,388 44,877
Dataset

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Items
20
50
10
1,000 995 158,868
1,330 1,307 94,491
39,355 27,878 980,528

Ratings
20
50
99,915 32,759
91,053 80,643
790,859 466,522

Train Size →
10
20
50
Model ↓ N@1 N@5 N@10 N@1 N@5 N@10 N@1 N@5 N@10

Table 1: Datasets statistics by number of training examples
(10, 20 or 50 ratings by user).

Yahoo!
MP
BPRMF
SM
CR
GER

53.0
52.8
50.9
53.5
53.5

59.1
59.0
56.7
60.3
60.3

67.3
67.2
65.4
68.2
68.3

52.5
52.2
49.7
57.8
53.8

58.3
58.3
55.6
61.7
60.7

66.4
66.4
64.2
68.9
68.2

53.6
52.2
49.9
56.0
54.3

57.8
57.7
54.1
60.0
59.6

64.0
63.5
60.3
65.6
65.3

Θ = Σ• , µ • •∈U∪I are optimized with stochastic gradient update,
picking a random triple at each step, and updating the correspond
means and variances (for user u, and items i and j).

MovieLens
MP
BPRMF
SM
CR
GER

66.0
66.1
55.9
69.0
70.3

64.7
64.6
57.5
67.3
67.7

65.8
65.7
60.3
68.6
70.0

68.4
66.3
58.3
69.7
72.0

65.3
64.3
59.6
68.5
69.5

66.3
65.8
61.6
69.5
71.1

69.1
66.9
58.6
71.4
72.5

67.4
65.0
60.4
69.4
71.3

67.5
66.2
62.5
70.6
71.5

Yelp
MP
BPRMF
SM
CR
GER

40.7
40.8
37.3
47.1
55.2

41.5
41.3
38.3
46.9
52.2

46.9
46.8
44.4
51.1
56.2

39.5
39.6
35.8
46.5
57.4

39.9
39.8
36.9
46.6
53.5

44.7
44.6
41.9
50.4
56.4

37.4
37.3
33.4
46.2
58.2

37.6
37.2
34.1
45.8
53.7

41.4
40.9
38.0
48.6
55.3



Ordering items. We could have used pairwise comparison from
our model since the relation i >u j ⇐⇒ p (i >u j |Θ) > 0.5
defines a total order over items, but it does not make any difference
in the ranking if p (i >u j |Θ) equals 0.51 or 0.99, and this difference
could be due to the sole difference between variances. To avoid this
problem, we order items by their probability of having a positive
score, i.e. sui = p(Xu · Xi + bi > 0) 2 . This ordering preserves the
variance information in contrast to the pairwise comparison – a
score with a high variance will be associated with a score sui close
to 0.5.

3

Table 2: Collaborative Ranking results. nDCG values (×100)
at different truncation levels are shown within the main
columns, which are split based on the amount of training
ratings.

EXPERIMENTS

We experimented with three different datasets extracted respectively from the Yahoo! Music ratings for User Selected and Randomly
Selected songs, version 1.0 3 , the MovieLens 100k dataset 4 and the
Yelp Dataset Challenge 5 . The Yahoo! dataset contains ratings for
songs. The original rating data includes 15,400 users, and 1,000
songs for approximately 350,000 ratings. The MovieLens dataset
contains ratings for movies. The original rating data includes 943
users, and 1,682 movies for approximately 100,000 ratings. The Yelp
dataset contains ratings for businesses. The original rating data
includes 1,029,432 users, and 144,072 businesses for approximately
4.1 million ratings.
We experimented with the following models, which are all based
on a learning-to-rank criterium (except for MP): (MP) Most popular
ranking where items are ranked based on how often they have been
seen in the past – while simple, this baseline is always competitive. There is no hyper-parameter to optimize for this procedure;
(SM) Soft margin (hinge) ranking loss, using stochastic gradient
descent [11] and the implementation from [3] optimizing all possible hyper-parameters; (BPRMF) Bayesian Personalized Ranking
[5] on which our model cost function is based, using the implementation from [3] and optimizing all possible hyper-parameters;
(CofiRank) A state-of-the art [10] ranking algorithm for recommender systems that optimizes an upper bound of nDCG6 . We
chose the hyper-parameters based on [10]; GER Our model named
Gaussian Embeddings Ranking. The number of dimensions was set
among 5, 10, 20, 50, 100, 200, 500 and 1000.

We followed mostly the experimental procedure of [10]. More
precisely, the dataset has been preprocessed as follows. For each
dataset, and for each user, a fixed amount (10, 20, or 50) of items
are kept to train the model, 10 for the validation set and the rest for
testing. Users with fewer than 30, 40, 70 ratings were removed to
ensure that we at least 10 ratings were available for testing. We also
removed items that were not rated by at least 5 users. The statistics
of the training datasets are shown in Table 1. The validation set is
used to select the best hyper-parameters for GER7 , SM and BPRMF.
Finally, for evaluation, we re-rank the judged items using the evaluated model, and use a metric for ranked lists of items, namely
nDCG at ranks 1, 5 and 10. The reported results are the average of
5 different experiments, with 5 different train/validation/test splits.
Results are reported in Table 2. There are roughly three groups of
models: (1) Soft Margin (SM) that performed the worst; (2) BPRMF
and most popular (MP); (3) GER and CofiRank (CR). Most popular
(MP) performs well because of the chosen experimental evaluation
where only items seen by the user are evaluated and ranked.
Our model (GER) outperforms the others on the MovieLens and
Yelp dataset. It is usually above CofiRank (nDCG difference between
0.01 and 0.08), with the exception of the Yahoo dataset for train sizes
20 and 50 (nDCG difference between 0.001 and 0.02). Overall, GER is
performing very well on three datasets with different characteristics
in terms of rating behavior and number of ratings.

2 We

use the same moment matching approximation to compute the inner product
distribution.
3 This dataset is available through the Yahoo! Webscope data sharing program http:
//webscope.sandbox.yahoo.com/.
4 Available at http://grouplens.org/datasets/movielens/100k/.
5 See https://www.yelp.com/dataset_challenge for more information.
6 https://github.com/markusweimer/cofirank

7 We

observed that a dimension of 50 generally gives good results independently of
the dataset, while c (bias regularization) ≈ 10−5 give good results on MovieLens and
Yelp, but needs to be higher (≈ 10−3 ) on Yahoo to achieve good results.

1067

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 1: User (left) and item (right) learned first component
representation plots for T50 and a representation dimension
of 50.

4

ANALYSIS

We now turn to the analysis of results, based on the study of the
learned representations for the Yahoo dataset. We used the hyperparameters that were selected on the validation set, and looked
at the set of mean-variance couples, i.e. the (µ •k , Σ•k ) for k ∈
{1, . . . , N } and • ∈ U ∪ I.
In Figure 1, we plotted the different couples (µ •k , Σ•k ), as well
as the histogram of mean and variance. We can see that the model
exploits the different ranges of mean (-0.3 to 0.3) and, more importantly, variance (0.4 to 1.2) around the priors. This was satisfying
since it was not obvious that the variance component would be
used by the model, i.e. that it would deviate from its prior.
To have a deeper insight on the use of the variance, in Figure 2,
we plotted violin (density) plots of the variance of users and items,
depending on the latent space dimension (5 to 1000), and on the
training size (10 to 50). We can observe that when the amount of
training data increases, the median of the σ•k decreases while the
variance of the σ•k increases. This corresponds to our hypotheses:
the former shows that more evidence reduces the uncertainty on
the representation, while the latter shows that more training data
induces more conflicting ratings and hence an increase in some of
the variances – this is confirmed by the case of item representations
that receive more ratings. Finally, we can observe that one the
dimension increases too much, most of the density of the σ•k is
centered around the prior – which could imply that most of the
dimensions were simply not used.

5

Figure 2: Violin (density) plots for the variance of users (left)
and items (right) for latent space dimensions T10 (top) and
T50 (bottom).
and more uncertain if no new information is provided); and (3), the
diversification of proposed rankings (e.g. Portfolio theory can be
used to produce rankings with various risks).
Acknowledgments. This work has been partially supported by:
FUI PULSAR (BPI France, Région Ile de France) and LOCUST (ANR15-CE23-0027-01).

REFERENCES
[1] Gerald G Brown and Herbert C Rutemiller. 1977. Means and Variances of Stochastic Vector Products with Applications to Random Linear Models. Management
Science 24, 2 (October 1977).
[2] Ludovic Dos Santos, Benjamin Piwowarski, and Patrick Gallinari. 2016. Multilabel
Classification on Heterogeneous Graphs with Gaussian Embeddings. In ECML.
DOI:http://dx.doi.org/10.1007/978-3-319-46227-1_38
[3] Zeno Gantner, Steffen Rendle, Christoph Freudenthaler, and Lars SchmidtThieme. 2011. MyMediaLite: A Free Recommender System Library. In RecSys.
[4] Shizhu He, Kang Liu, Guoliang Ji, and Jun Zhao. 2015. Learning to Represent
Knowledge Graphs with Gaussian Embedding. DOI:http://dx.doi.org/10.1145/
2806416.2806502
[5] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars SchmidtThieme. 2009. BPR: Bayesian personalized ranking from implicit feedback. In
Uncertainty in Artificial Intelligence. AUAI Press, 452–461.
[6] Francesco Ricci, Lior Rokach, Bracha Shapira, and Kantor Paul B. 2011.
Recommender Systems Handbook. Vol. 532. DOI:http://dx.doi.org/10.1007/
978-0-387-85820-3
[7] R Salakhutdinov and A Mnih. 2007. Probabilistic Matrix Factorization. In Proceedings of Advances in Neural Information Processing Systems, Vol. 20. 1257–1264.
[8] Luke Vilnis and Andrew McCallum. 2014. Word Representations via Gaussian
Embedding. In ICLR.
[9] Jun Wang and Jianhan Zhu. 2009. Portfolio theory of information retrieval. In
SIGIR. ACM Press. DOI:http://dx.doi.org/10.1145/1571941.1571963
[10] Markus Weimer, Alexandros Karatzoglou, Quoc Viet Le, and Alex Smola. 2007.
CofiRank Maximum Margin Matrix Factorization for Collaborative Ranking. In
Advances in Neural Information Processing Systems. 1–3.
[11] Markus Weimer, Alexandros Karatzoglou, and Alex Smola. 2008. Improving
maximum margin matrix factorization. Machine Learning 72, 3 (2008), 263–276.
[12] Fuzheng Zhang, Nicholas Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma. 2016.
Collaborative Knowledge Base Embedding for Recommender Systems. In KDD.
DOI:http://dx.doi.org/10.1145/2939672.2939673

CONCLUSION

In this paper, we proposed a collaborative recommender system
where user and items correspond to distributions in a latent space,
rather than to a deterministic point for other representation-based
models, allowing to cope with the inherent uncertainty due to
lack of information, or to conflicting information. We show on
three datasets that our model outperforms state-of-the-art works.
We analyzed qualitatively the results showing that the variance
distribution was correlated with space dimension and train size.
Moreover, this type of model has various advantages that we
have not exploited yet: (1) the integration of different sources of
information, exploiting the uncertainty of each source; (2) the use
of the variance for handling time (the representation becomes more

1068

