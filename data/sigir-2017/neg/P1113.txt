Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

ENCORE: External Neural Constraints Regularized Distant
Supervision for Relation Extraction
Siliang Tang, Jinjian Zhang, Ning Zhang, Fei Wu, Jun Xiao, Yueting Zhuang∗
College of Computer Science, Zhejiang University
{siliang,jinjianzhang,aning,wufei,junx,yzhuang}@zju.edu.cn

ABSTRACT
Distant Supervision is a widely used approach for training relation
extraction models. It generates noisy training samples by heuristically labeling a corpus using an existing knowledge base. Previous
noise reduction methods for distant supervision fail to utilize information such as data credibility and sample confidence. In this paper,
we proposed a novel neural framework, named ENCORE (External
Neural COnstraints REgularized distant supervision), which allows
an integration of other information for standard DS through regularizations under multiple external neural networks. In ENCORE,
a teacher-student co-training mechanism is used to iterative distilling information from external neural networks to an existing
relation extraction model. The experiment results demonstrated
that without increasing any data or reshaping its original structure,
ENCORE enhanced a CNN based relation extraction model for over
12%. The enhanced model also outperforms the state-of-the-art
relation extraction method on the same dataset.

Figure 1: An example of distant supervision labeled samples
for “spouse” relation

that contain a pair of entity mentions with known relation can be
labeled as positive samples of that relation, while sentences that
contain only one wrong entity mention with correct entity type
are labeled as negative samples. As shown in Figure 1, sentences
with entity mention “Barack Obama” and another person but not
“Michelle Obama”, e.g., “President Barack Obama and Secretary
Hilary Clinton discuss their friendship”, will be labeled as negative
examples. Many studies [1, 3, 13] have proved that DS can improve
the performance of relation extraction, and therefore DS becomes
a de facto standard data preprocessing step before model training.
However in many occasions, such seemingly plausible assumption
is too strong to yield a fair training set, and further misleading
the true target of our learning methods[13]. This is because noisy
and trustless DS labeled samples often occupy an overwhelming
proportion in training data, therefore a well-trained supervised
model that relies on DS samples often fails to perform normally in
a real scenario. Regularization [14, 15] is generally used to assist
model in learning method.
In order to reduce the noise from enormous amount of DS labeled samples, a filter, which is trained on human labeled ground
truth data only, is often applied to generate relatively small but
more consistent filtered samples[1, 16]. Therefore the training set
for relation extraction is usually composed of samples from three
different sources, i.e., ground truth samples, the DS labeled samples, and filtered samples with confidence, and each source has its
own credibility. Although the credibility of samples [11] may be
essential to train a better supervised extraction model, all samples
are treated equally during the model training. To the best of our
knowledge, no direct studies can be found on modeling this part of
information. We suspect that it is partly because DS is usually considered as a framework for data preprocessing, which is separated
from modeling. On the other hand, incorporate such information is
non-trivial, especially for neural networks.
In this paper we proposed a novel distant supervision framework,
named ENCORE (External Neural COnstraints REgularized distant

CCS CONCEPTS
• Information systems → Information extraction; • Computing methodologies → Neural networks;

KEYWORDS
Distant Supervision, Relation Extraction, Neural Network

1

INTRODUCTION

Relation extraction plays an important role in transforming unstructured data to structured information, which is a cornerstone for
many applications such as open IE, knowledge base population, and
QA system[3, 10]. The task of relation extraction is to detect and
classify relations between entities in the given sentences. However,
collecting labeled training data for learning based extraction methods is usually quite expensive[13, 17], especially for training deep
models, which requires huge amount of labeled training samples.
To alleviate this problem, Distant Supervision (DS) [9], an idea
of heuristically labeling a corpus using an existing knowledge base
is proposed. DS follows an assumption that unlabeled sentences
∗ Corresponding

author: Jun Xiao

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 Association for Computing Machinery.
ACM ISBN 978-1-4503-5022-8/17/08. . . $15.00
https://doi.org/10.1145/3077136.3080735

1113

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

First, we denote our training samples as (X , Y ), where X are input
sentences and Y are their corresponding one-hot encoded relation
labels. For samples from three different sources, we denote ground
truth samples as (X д , Y д ), which are manually annotated by human
experts; DS labeled samples, which are denoted as (X d , Y d ), are
generated by ordinary distant supervision method; Filtered samples
with confidence level are denoted as (X c , Y c ). They are generated
as follows: first, training a binary classifier f ′ on (X д , Y д ), then
repeatedly sample data x i ∈ X d , and used the classifier f ′ to predict
its labels f ′ (x i ) = yi′ . If the predicted label yi′ , yi , where yi ∈ Y d ,
the corresponding DS sample x i is removed from X d .
This yields three non-overlapping data sources. Their total number of samples as well as sample credibilities follow a descending order of (X d , Y d ), (X c , Y c ) and (X д , Y д ), that is, n(X d , Y d ) ≤
n(X c , Y c ) ≤ n(X д , Y д ), and c(X d , Y d ) ≤ c(X c , Y c ) ≤ c(X д , Y д ),
where n(D) is the total number of samples of set D, and c(D) is the
average credibilities of set D.

supervision), which allows an integration of different information
such as data credibility, sample confidence through multiple external neural networks. These external neural networks behave like
teachers to transform extra information of different views into a
student model (e.g., the original relation extraction model), and the
transferred information is treated as constraints to further regularize the behaviors of the student model. ENCORE is a general
framework that can be applied to arbitrary differentiable student
models without reshaping their original designs or structures. It is
also flexible enough that allows multiple teachers to impose their
influences on a single student.
The Figure 2 demonstrated how ENCORE is applied to improve
a basic relation extraction model fb . As shown in Figure 2, instead
of mixing training samples from different sources, ENCORE uses
external neural networks, i.e., confidence network fc and ground
truth network fд to attend machine evaluated and human annotated
samples respectively. Sample credibility of a particular data source is
modeled as constraints to regularize the posterior distribution of the
corresponding external neural network[4], which yields different
posterior distributions, i.e., pc and pд . Together with the original
posterior distribution pb from basic model, ENCORE may receive
two different posterior evaluations (one from basic model, another
from external neural network) for each sample. At each iteration,
ENCORE utilizes Multi Instance Learning (MIL)[2] to select the
best sample by fuse these posterior distributions, and then return
loss to the student network. The experiment results proved that
ENCORE framework can significantly enhance the performance of
the original relation extraction model for over 12%, and the overall
performance of the enhance model outperforms the state-of-the-art
relation extraction method on TAC dataset.

2

2.2

Basic Relation Extraction Model

ENCORE can be viewed as a model level extension of traditional
distance supervision. It can enhance arbitrary differentiable relation
extraction model (typically deep neural network), which we called
basic model. Some recent studies [6, 16] have reported promising
advances in applying deep learning methods such as long-short term memory or convolutional neural network on relation extraction.
Therefore, in this paper, we deploy the end to end Text-CNN [6]
architecture as our basic model.
Different from the original ImageNet CNN architecture[7], there
are only one convolution-pooling layer and less feature maps in
Text-CNN. During training, Text-CNN learns sentence representations by back-propagation from a softmax output layer. Its posterior
probability P fb can be written as follows:

METHODOLOGY

In this section, we will present technical details of ENCORE for
relation extraction by enumerating its essential components.

P fb (Yfb |X ; θ ) = ÍK

e fb (X ;θ )

k =1

e fb (X ;θ )k

(1)

, where X = {X d , X c , X д }, K is the total number of relations, fb is
the basic model, and θ is its parameters.

2.3

Figure 2: An overview of ENCORE for relation extraction

2.1

External Neural Networks

In ENCORE, we use external neural networks to produce regularized posterior distributions for samples from different data sources,
and then these regularized posterior samples are learned by basic
model through an iterative distillation process, which is described
in several literatures[4, 5]. During this process, external neural networks behave like teachers, the basic model is their student, and
the student learns by imitating the teachers’ behaviors. The teacher
networks usually share the same structure and even the same parameters of the student, therefore in each iteration the teachers and
the student can coordinate each other, and reach a final agreement
when ENCORE reaches its convergence. At that point, the information carried by external neural networks is completely transferred
into the basic model, and therefore yields a superior model that
beyond its past.
More specifically, on relation extraction, we design two external
neural networks to help our basic model, one is called ground

Data Generation

In this subsection, we will explain the generation of a relation
extraction training set by standard distance supervision approach
with some useful notations.

1114

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

, where η is penalty coefficient. Similar to the ground truth network, this optimization problem can also be solved by Lagrangian
dual method, the optimal posterior distribution P fc (Y |X )∗ for basic
network can be written as follows:
P fb (Yfc |X c ; θ )exp{η(λ − ϕ(X c , Y c ))}
b
(5)
P fc (Yfc |X c ; θ, η, ζ )∗ =
c
Z
Í
, where Z = Y c P fb (Yfc |X c ; θ )exp{η(λ − ϕ(X c , Y c ))}.

truth network, which is used to emphasize the influence of ground
truth samples; another is called confidence network, which aims to
integrate information of confidence level of filtered samples into
basic model. We will discuss their implementations in the following
subsections.

2.4

Ground Truth Network

b

Since the ground truth samples have the best data credibility, we
designed a ground truth network fд to emphasize their influences
on basic model.
We first define our regularization on posterior distribution of
fд . Our constrains for fд is that the posterior evaluations X д on
д
fд must match the ground truth label Y д , i.e., 1(Yf = Y д ) = 1,
д

2.6

д

where 1 is the indicator function. Therefore, E P fд [1(Yf = Y д )] is
д

expected to be 1, when most ground truth samples are correctly
predicted.
In order to transfer this knowledge to basic model, we minimize
the KL divergence between the posterior distribution of fд (i.e.,
д
д
P fд (Yf |X д ; θ )) and fb (i.e., P fb (Yf |X д ; θ )). Therefore, the objective
д

b

function of our ground truth network can be written as follows:
д

д

b

д

min

KL(P fb (Yf |X д ; θ )||P fд (Yf |X д ; θ ))

s.t .

E P fд [1(Yf = Y д )] = 1

д

(2)

д

This objective function aims to fit the ground truth samples more
appropriately, and minimize the posterior difference between the
two networks. Obviously, this optimization problem is convex and
can be solved by Lagrangian dual method. Finally, the optimal
posterior distribution P fд (Y |X )∗ for basic network can be written
as follows:
д
P fд (Yf |X д ; θ )∗
д

, where Z =

2.5

=

д

д

b

д

P fb (Yf |X д ; θ )exp{1(Yf = Y д ) − 1}
Z

д
д
д
Y д P fb (Yfb |X ; θ )exp{ 1(Yfд

Í

P(y j |B j ) = max(P(yi |b ij )), 1 ≤ i ≤ M

(3)

Confidence Network

j=1

Filtering generated samples to reduce samples with wrong labels
is proved to be quite useful in distant supervision[1]. However,
although mismatched samples are removed, the classifier may still
make errors on samples with lower confidence. This is one of the
reasons that DS does not use per-trained classifier to go through
all generated samples. The confidence network fc aims to integrate
the missing confidence information into the basic model.
In fc , we define a new regularization on its posterior distribution
P fc (Yfc |X c ) to bias filtered samples with high confidence. Formally,
c
we define ϕ(X c , Y c ) as the value of “negative confidence of correct
label”, and require ϕ(X c , Y c ) has expectation at least λ. Since the
number of filtered samples is much higher than ground truth samples, and the hyper-parameter λ is difficult to tune, we relax this
regularization into a soft expectation ζ . Therefore the objective
function of fc can be written as follows:
KL(P fb (Yfc |X c ; θ )||P fc (Yfc |X c ; θ )) + ηζ

s.t .

E P fc [ϕ(X c , Y c )] − λ ≥ 1 − ζ

b

c

(6)

Finally, according to Eq (1), (3) and (5), we optimize three objective functions jointly, and define our final objective function for
ENCORE as follows:
Q
Q
Õ
Õ
J (θ ) =
loд P fb (y j |B j ; θ ) +
loд P fд (y j |B j ; θ )

= Y д ) − 1}.

min

Posterior Selector

After above mentioned regularizations, ENCORE may receive two
different posterior evaluations for each sample. One is the original posterior probability from the basic model, and another one
is the regularized posterior probability from one of the external
neural networks. In order to fuse these posterior evaluations and
further reduce their noises. We employ multi instance learning[2]
as ENCORE’s posterior selector.
MIL is an effective learning method that can help supervised
model to deal with noisy label problems. Instead of receiving a
set of training samples, which are individually labeled, the learner
receives a set of labeled “bags”, each containing many instances. A
bag is positively labeled if at least one instance in it is positive, and
is negatively labeled if all instances in it are negative. In ENCORE,
we assume that each “bag” contains several posterior evaluations
from basic model and external neural networks, and in a positive
“bag”, at least one sample is trustable. Only trustable samples will
be selected by our selector to update the basic model.
Formally, Suppose a mini-batch B is composed of N bags B =
{B 1 , B 2 , ...B N } and the j-th bag has M instances B j = {b j1 , b j2 , ...b jM }.
The confidence of bag is determined by the instance with highest
posterior probability.

+

Q
Õ
j=1

j=1

(7)

loд P fc (y j |B j ; θ, η, ζ )

, where Q is the number of instances in one bag. Note that this
objective function is updated in bag level instead of instance level.

3 EXPERIMENT
3.1 Dataset
We use the TAC Cold Start 2015 dataset to evaluate the efficiency of the ENCORE framework. The Cold Start 2015 dataset includes a Wikipedia snapshot (May 2014), a New York Times corpus
(LDC2015E45) and human labeled ground truth sentences (LDC2015E48) for relation extraction[1]. It defines 41 relations such as
per:spouse, per:tile, and org:alternate_names. The New York Times
corpus includes 4,877,463 documents and each document contains
around 15 sentences in average. The ground truth data contains
29,781 sentences.

(4)

1115

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

We use DS algorithm to label 354,365 sentences, and then filtered
90,213 sentences by a CNN which was trained on ground truth only.
Since the dataset of DS is extremely unbalanced with more negative
samples than positive samples for most relations, we samples 15%
negative samples randomly. Some statistics of generated dataset
are shown in Table 1.

application of relation extraction. We demonstrated its efficiency in
transferring information through multiple external neural networks
into an existing neural network. The experiment results suggest
that the design of ENCORE can be very helpful on retraining a deep
model. Although in this paper, ENCORE is designed particularly for
solving problems in DS, it could be easily adopted to enhance neural
networks in other applications like images [8, 12] by incorporating
useful side information as constraints.

Source
Ground truth Filtered DS labeled
# of samples
29,781
90,213
354,356
Table 1: Statistics of different sources on TAC 2015. Ground
truth only accounts for 8% of all data and other data are generated by distant supervision.

3.2

ACKNOWLEDGMENTS
This work was supported in part by NSFC (No. U1611461, 61402401,
61572431), 973 program (No. 2015CB352302), Chinese Knowledge
Center of Engineering Science and Technology (CKCEST), Qianjiang Talents Program of Zhejiang Province 2015, Zhejiang Provincial Natural Science Foundation of China (No. LZ17F020001), Key
program of Zhejiang Province (2015C01027).

Experiments and Results

Since there is no development dataset, we tuned models using
ten-fold cross validation. In order to compare with the work by
[1, 16], we use the same word vectors proposed by Google (300dimensional) to initialize the embedding layer and the vectors are
fine-tuned duringqtraining.
q we initialized parameters with uniform

REFERENCES
[1] Heike Adel, Benjamin Roth, and Hinrich Schütze. 2016. Comparing convolutional neural networks to traditional models for slot filling. arXiv preprint
arXiv:1603.05157 (2016).
[2] Boris Babenko. 2008. Multiple instance learning: algorithms and applications.
View Article PubMed/NCBI Google Scholar (2008).
[3] Vittorio Castelli, Hema Raghavan, Radu Florian, Ding Jung Han, Xiaoqiang Luo,
and Salim Roukos. 2012. Distilling and exploring nuggets from a corpus. In
International ACM SIGIR Conference on Research and Development in Information
Retrieval. 1006–1006.
[4] Kuzman Ganchev, Jennifer Gillenwater, Ben Taskar, et al. 2010. Posterior regularization for structured latent variable models. Journal of Machine Learning
Research 11, Jul (2010), 2001–2049.
[5] Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric Xing. 2016.
Harnessing deep neural networks with logic rules. arXiv preprint arXiv:1603.06318
(2016).
[6] Yoon Kim. 2014. Convolutional neural networks for sentence classification. arXiv
preprint arXiv:1408.5882 (2014).
[7] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural networks. In Advances in neural information
processing systems. 1097–1105.
[8] Changzhi Luo, Bingbing Ni, Shuicheng Yan, and Meng Wang. 2015. Image
Classification by Selective Regularized Subspace Learning. IEEE Transactions on
Multimedia 18, 1 (2015), 40–50.
[9] Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision
for relation extraction without labeled data. (2009).
[10] Liqiang Nie, Meng Wang, Yue Gao, Zheng Jun Zha, and Tat Seng Chua. 2013. Beyond Text QA: Multimedia Answer Generation by Harvesting Web Information.
IEEE Transactions on Multimedia 15, 2 (2013), 426–441.
[11] Liqiang Nie, Meng Wang, Zhengjun Zha, Guangda Li, and Tat Seng Chua. 2011.
Multimedia answering:enriching text QA with media information. In Proceeding of the International ACM SIGIR Conference on Research and Development in
Information Retrieval, SIGIR 2011, Beijing, China, July. 695–704.
[12] Liqiang Nie, Shuicheng Yan, Meng Wang, Richang Hong, and Tat Seng Chua.
2012. Harvesting visual concepts for image search with complex queries. In ACM
International Conference on Multimedia. 59–68.
[13] Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations
and their mentions without labeled text. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases. Springer, 148–163.
[14] Meng Wang, Weijie Fu, Shijie Hao, Hengchang Liu, and Xindong Wu. 2017.
Learning on Big Graph: Label Inference and Regularization with Anchor Hierarchy. IEEE Transactions on Knowledge and Data Engineering PP, 99 (2017),
1–1.
[15] Meng Wang, Weijie Fu, Shijie Hao, Dacheng Tao, and Xindong Wu. 2016. Scalable
Semi-Supervised Learning by Efficient Anchor Graph Regularization. IEEE
Transactions on Knowledge and Data Engineering 28, 7 (2016), 1864–1877.
[16] Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. 2015. Distant Supervision for
Relation Extraction via Piecewise Convolutional Neural Networks.. In EMNLP.
1753–1762.
[17] Hanwang Zhang, Fumin Shen, Wei Liu, Xiangnan He, Huanbo Luan, and TatSeng Chua. 2016. Discrete collaborative filtering. In Proc. of SIGIR, Vol. 16.

samples from [− d6 , + d6 ] where we set dimension d = 300. The
number of filters in convolution layer is 3 and the window size of
max-pooling layer is 2. We use a fixed dropout rate 0.5 at CNN to
mitigate overfitting. The λ is set as 0.7 and η is set as 0.1. Our model
was optimized by AdaDelta with learning rate of 1.0 and decay rate
ρ of 0.05. For MIL we set the bag size 5 with batch size 50.
We designed several experiments to evaluate the effectiveness
of ENCORE, and compared it with following methods:
P-CNN [16] is the state-of-the-art relation extraction model on
TAC 2015 dataset. It combines CNN with piece-wise polling layer
and uses multi-instance learning to reduce the noise of distant
supervision.
Text-CNN (basic model) can be regarded as ENCORE without
any neural constrains, which is described in subsection 2.2. It is the
basic model that we are going to enhance. All of the models share
the same data sources.
Methods
Precision Recall
P-CNN[16]
Text-CNN (basic model)
49.9
48.3
ENCORE enhanced Text-CNN
54.7
55.9
Table 2: Performance comparisons

F1-score
52
49.1
55.3

The experiments results are presented Table 2. As shown in Table
2, without increasing any data or reshaping the original structure of
Text-CNN, ENCORE significantly enhance the performance of the
Text-CNN model for over 12%, and the overall performance of the
enhance model outperforms the state-of-the-art relation extraction
method P-CNN, which proves the effectiveness of our framework.

4

CONCLUSION

In this paper, we proposed ENCORE, a general neural framework,
which extends the traditional distant supervision approach in an

1116

