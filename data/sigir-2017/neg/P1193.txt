Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Social Media Advertisement Outreach: Learning the Role of
Aesthetics
Avikalp Srivastava

IIT Kharagpur
avikalp22@iitkgp.ac.in

Madhav Datt

Harvard University
madhav datt@college.harvard.edu

Shubham Mangla

IIT Kharagpur
jaikrishna.ch@iitkgp.ac.in

Priyadarshi Patnaik

IIT Kharagpur
shubhammangla@iitkgp.ac.in

IIT Kharagpur
bapi@hss.iitkgp.ernet.in
select a set of hand-crafted describable image aesthetic features and
train our system to learn the relative significance and influence of
each of these features on engagement. To ensure that the results of
our research are actionable by graphic designers, we restrict our
work to human understandable/describable features and do not use
generic or learned deep features such as the ones used in [6].
We also go on to show that low level features that work for
digital photography (not accounting for biases) do not work for
social media ad success. We use the model described in [1] for aesthetics of digital photographs as a baseline. Our method performs
significantly better (71.8% vs 57.5%) than the baseline on classifying
our dataset of 8000 promotional images on Twitter into successful
and unsuccessful images.
In the end, we describe the basic function and design of an
automated interactive system based on the results obtained from
our model. The system takes promotional images from corporations
as input and provides human understandable/describable aesthetic
attributes of the image that may be tuned (for example, increasing
spatial smoothness of hue property by 14%) by designers to obtain
the most significant increase in engagement on Twitter.
Through this paper, our key contribution is developing a method
to deal with the bias related challenges associated with analyzing effects of aesthetic features on outreach of social media advertisement
images. This elimination of bias to give comparable image scores
based only on aesthetic attributes, across different images and pages
on social media, opens possibilities for research in computational
aesthetics around the social media advertising industry.

ABSTRACT
Corporations spend millions of dollars on developing creative imagebased promotional content to advertise to their user-base on platforms like Twitter. Our paper is an initial study, where we propose
a novel method to evaluate and improve outreach of promotional
images from corporations on Twitter, based purely on their describable aesthetic attributes. Existing works in aesthetic based image
analysis exclusively focus on the attributes of digital photographs,
and are not applicable to advertisements due to the influences of
inherent content and context based biases on outreach.
Our paper identifies broad categories of biases affecting such
images, describes a method for normalizing outreach scores to
eliminate effects of those biases, which enables us to subsequently
examine the effects of certain handcrafted describable aesthetic features on image outreach. Optimizing on the features resulting from
this research is a simple method for corporations to complement
their existing marketing strategy to gain significant improvement
in user engagement on social media for promotional images.

1

Jaikrishna Chaparala

INTRODUCTION

In an effort to reach out to their user base, corporations spend
millions of dollars developing creative image-based promotional
content for social media platforms such as Twitter. The ability of
corporations to engage a large portion of their target audience
has very direct monetary consequences for them. Because of their
focus on sales and brand promotion, these images come with certain
inherent content and context based biases beyond just aesthetic
attributes that influence overall outreach.
Most advertisement outreach research and development is based
on data from advertisement quality surveys. Conducting such surveys is an extremely resource intensive task. There has also been
significant work in aesthetic image analysis [1, 2, 6, 7] for predicting user ratings of digital photographs. However, these studies
cannot be applied to promotional images on social media, because,
social media user engagement of an image, unlike user ratings, are
influenced by multiple factors beyond just image aesthetics.
In our paper, we develop an engagement score for images on
Twitter, identify such broad ”biases” or factors, propose an automated method to identify their presence in images and learn a
transformation on scores to eliminate the effects of such biases. We

2

DATA COLLECTION

We build a data set of 8,000 image based promotional tweets by
scraping Twitter profiles of 80 different corporations. These corporations are particularly active on Twitter and have between 36,000
(@Toblerone) to 13 million (@PlayStation) followers, and 3,000
(@tictac) to 753,000 (@united) tweets. We select these corporations from across 20 broad categories such as retail, fast food, automobiles etc. to account for the diversity in promotional image
representation. We scrape such image based promotional tweets
along with their likes count, retweets count, date and time, page
followers, page tweets and tweet text from the Twitter API for each
corporation page, in proportion to their total number of tweets.
For each such image i ∈ page p, we define our engagement evaluation score ϵ i , as the sum of image likes and image retweets. Due
to inherent industry differences, and the variances across pages in
total followers, we normalize our scores to ensure comparability
between scores from different pages. We get a Pearson Correlation Coefficient of 0.46 and Spearman Rank Correlation of 0.63,
suggesting no linear or monotonic correlation between number of

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR’17, August 7–11, 2017, Shinjuku, Tokyo, Japan
© 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00
DOI: http://dx.doi.org/10.1145/3077136.3080759

1193

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 1: From left to right: No bias present, holiday season, animal presence, human presence, discounts, product launch

followers and average engagement scores of a page. This is also
supported by the sampled distribution in Figure 2.
Therefore, in order to account for the difference in image based
tweet engagement between pages of different sizes, industries etc.
we use the mean and variance of engagement scores (ϵ i ) of images
from the page p for normalization. Thus, we define normalized
i : For image i ∈ p,
engagement ϵ N
i
ϵN
=

Figure 2: Distribution of Variances and Medians of Engagement Scores vs Number of Followers of Twitter Pages

ϵ i − µp
σp

where µp and σp are mean and standard deviation of image scores
from page p.

3

BIAS DRIVEN ENGAGEMENT

Datasets used by previous aesthetic image researches (Photo.Net
[1], DPChallenge [2], AVA [7]) only involve digital photographs
rated by users purely based on their aesthetic appeal. Image based
promotions on Twitter contain certain content and context based
biases that significantly influence engagement scores ϵ N , for example, advertisements involving a cute cat will, on average, have much
greater success and outreach, and consequently higher ϵ N scores
compared to aesthetically better images free from any significant
biases. This makes analyzing the effects of aesthetic features on
engagement an extremely challenging task. To ensure that our
scores represent effects of, and are strongly correlated with visual
appeal and aesthetic factors, we detect biases affecting each image
and remove their influences on the score.
For each Twitter page within our 8000 image dataset, we detect outliers in terms of engagement scores using normalized local
outlier factor scores and manually identify 8 broad categories of
the most significant biases (listed in Table 1). In this paper, we
account for and eliminate the effects of 4 biases - Animal Presence
(cats, dogs etc.), Human Presence (babies, celebrities etc.), Special
Days (Black Friday, Christmas etc.), and Discounts. Handling the
remaining biases is beyond the scope of this paper and can give
direction to future research in this area.

occurring animals) scraped from the web. We manually identify all
the 4 above mentioned biases from a sample of 1000 images from
our 8000 image dataset, to assess quality of our automated bias
identification, and obtained 75.5% accuracy for human presence
detection and 69.6% accuracy for animal presence detection.
To extract text from the image, we use the Tesseract OCR Engine. In this paper, we define tweet text as the OCR extracted text
along with the text associated with each image from its tweet. To
account for the surge in Twitter engagement in periods leading
up to major holidays such as Christmas, Black Friday, Halloween
etc. we define date ranges around each such holiday (for example
7 days before and after Christmas). We manually build a list of 20
words commonly associated with holidays (such as Thanksgiving,
Hanukkah etc.) We augment this list of words by finding the 20
most linguistically and semantically similar words using GloVe [8],
which are then manually validated and filtered, and classify all
tweets which occur within a holiday date range containing any of
these words as Holiday biased.
To identify tweets affected by biases caused by discounts/offers,
we repeat the same process as described above using a different set
of common initial words such as free, discount, sale, offer
etc. with GloVe. We also identify tweets that urge users to retweet
to get offers or win as part of some promotion. On our manually
labeled 1000 image test set, we obtain 88.3% accuracy on holidaythemed image identification and 84.4% accuracy in identification of
discounts and offers. While some images contain multiple biases, we
restrict our paper to images with at most one bias which constitute
a majority of our dataset (923 of our sampled 1000 images).

Table 1: Significant biases affecting Twitter engagement
Discounts/Give-aways
Hashtags/Celebrity Mentions
Special Days/Holiday Season
Social/Motivational Message

3.1

Human Presence
Animal Presence
Product Launches
Brand Popularity

Bias Identification

3.2

We use the Viola-Jones face detector to give a binary classification
detecting the presence of faces (as a proxy for human presence)
in promotional images. To detect presence of animals, we train a
spatial pyramid matching based SVM classifier described by Lazebnik et al. [5] on 5000 images of cats and dogs (most frequently

Bias Removal

We define set of unbiased images U and mutually exclusive sets of
identified biased images Bj ∈ B, where B = {Images with human
presence, Images with animal presence, Holiday-themed images,
Images with discounts}.

1194

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 3: Left: Distribution of image scores with holiday
bias. Right: Distribution comparison for unbiased, humanpresence and animal-presence biased images. Both estimated via Gaussian curves

Figure 4: 1st row: Images with highest feature values: Average hue for DoF, localized light exposure, average intensity
of largest segment. 2nd row: Feature visualizations based on
Region Adjacency Graphs

X ∈ IR(d+1)×n given by:
1

y1j
 2
y
 1j
 .
 .
 .
 d
y1j


We define the discrete probability distribution of scores of imi , ∀i ∈ U as P, and that of image scores ϵ j , ∀j ∈ B , for
ages ϵ N
j
N
Bj ∈ B, as Q. To eliminate the effects of the bias, we apply the
τ

transformation Q −
→ Q̄, such that the distribution Q̄ can be used
as an approximation of P, while maintaining relative ranking order
of image scores of biased images as in their original distribution
Q. Since, we transform Q to eliminate the effects of bias, Q̄ should
i
be distributed similar to P, and transformed image scores ϵ N
T for
i ∈ Q̄ are used as if the original images had not been affected by
the bias. To this effect, we use the Kullback-Leibler divergence
D K L (P|| Q̄) as our objective function for minimization, to capture
the loss incurred while using Q̄ to approximate P.
Let Yj be the set of scores for images in Bj , thus the set of transformed scores Ȳ J is obtained as follows:

‘ , y ‘ , ..., y ‘ ], where y ‘ represent the intermediate
where O = [y1j
nj
ij
2j
approximate values of the transformed score ȳi j . We minimize the
above described divergence or loss function, J (W ) = D KL (P||O) to
learn the values for the matrix W . With the learned W , we apply
the transformation τ ∗ (= τW ) to image scores in Y J to eliminate
effects of bias described in Bj . We repeat this process for each set
of biased images in B.

4

τ ∈D

i ∈Yj

AESTHETIC FEATURE LEARNING

Computational assessment of aesthetic image quality is a well tackled problem. Having removed the content and context based biases
associated with the scores received by the images in our dataset,
we obtain image features strongly related with aesthetic appeal and
visual attractiveness of the image to get the feature vector xi for
each image i. We now formulate our feature learning problem with
i , i.e. represents
training data set {xi , yi }i ∈[1, N ] , where yi = ϵ N
T
the transformed score of image i after bias elimination. A function
f : X → Y is learned for providing feedback and suggestions for
improving user engagement through image feature tuning.

τ ∗ = arд min (D K L (P|| Q̄τ ))

P(i) log

1 
ynj 
2 
ynj

.. 
. 
d 
ynj


O = WTX

where the probability distribution of the set Y¯j is defined as Q̄τ .
Thus, we learn the transformation τ ∗ ∈ D, where D is the space
of all transformation functions, such that

Õ

...
...
...
..
.
...

where n = |Yj |. At each step, the intermediate output transformation is given by


Y¯j = y¯i j | y¯i j = τ (yi j )

where, D KL (P||Q) =

1
y2j
2
y2j
..
.
d
y2j

P(i)
Q(i)
p

In this paper, we explore the space of polynomial functions Dd ∈ D,
where the parameter d defines the degree of polynomial for the
p
input features yi j , ∀yi j ∈ Yj . Therefore for the space Dd , τ as
parameterized by W = [W0 ,W1 , ..Wd ] on the input yi j ∈ Yj is
defined as follows:

4.1

Feature Selection And Extraction

We select describable/human-understandable image attributes based
on handcrafted features used in previous works, augmenting the
list with an additional set of features deemed important to capture
the multi-object majority nature identified in advertisement images
vis-a-vis photographic images. We implement the 56 features defined by Datta et al. [1] in addition to non-overlapping features
from Ke et al.[4], compositional attributes from Dhar et al.[3], along
with added features based on Region Adjacency Graphs (RAG) such
as threshold and recursive normalized cut information. Thus, we
obtain a total of 74 describable aesthetic features for each image.

τW (yi j ) = W0 + W1yi j + W2yi2j + .. + Wd yidj
We add another constraint that W ∈ (IR+ )(d+1) , so that during transformation, the relative ranking order of biased images is maintained
and arbitrary transformations that disregard original distribution’s
ranking information and overfit are disallowed. For the bias associated with Yj in consideration, the input to the system is the matrix

1195

Short Research Paper

4.2

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Experimental Evaluation
Figure 5: On input (a) to system with k=2, t=4% suggestions:
Increase features, light exposure by 24%, spatial smoothness
of 2nd level of saturation by 16%; (b) final image with suggestions incorporated

We use a standard support vector regressor with RBF kernel to
learn the function f : X → Y where X denotes the 74 feature based
vector set and Y denotes the set of normalized and bias-removed
engagement scores. This provides a quantitative evaluation of the
relation between the feature vector values to the predicted engagement score, necessary for providing feedback on feature tuning
for the query image to maximize the outreach through increased
aesthetic and visual appeal, given that all other components for
that image remain same.
We also show that aesthetic features that work for digital photographs do not necessarily work for promotions/advertisement
based images. From our dataset {xi , yi }i ∈[1, N ] , we sample 20%
points to form our test set T. To model our data for the classification task, we specify thresholds to partition images with scores in
the lowest and highest quartile as ”unsuccessful” and ”successful”
respectively. We first run a SVM classifier model to learn weights
for the 56 features used in [1] on the Photo.net dataset and use this
trained model to classify on our test set T. While the accuracy of
trained model on test set from Photo.net dataset is 69.12% (close to
value reported in [1]), on T this model’s accuracy reduces to 57.5%.
Our 74 feature model trained on Twitter advertisement training
dataset {xi , yi }i ∈[1, N ] − T performs with 71.8% accuracy on T, and
on inspection we find that a good proportion of misclassified images
contained biases we didn’t handle in this study, and thus also good
motivation for future work. The reduced accuracy achieved when
using aesthetic features learned from non-advertisement dataset
strongly suggests that it is necessary to capture image features
linked with success of advertisement related images differently
from those of purely aesthetically motivated digital photographs.

number of features where changes are suggested to at most k, where
each feature is not changed by more than a value s. For finding the
optimal tuning combination among these k features, a distance of s
on either side of the original feature value is traversed in steps of
size t. The tuning combination that achieves the highest predicted
engagement score from the trained support vector regressor is
chosen as the suggestion output, as demonstrated in Figure 5.

5

Our paper proposes a novel method to evaluate and improve outreach of promotional images from corporations on Twitter by identifying inherent biases and transforming scores to eliminate their
effect on engagement in order to discover attributes that contribute
most to advertisement outreach. Our model gives an aestheticfeature based representation with corresponding outreach scores,
enabling vector space model based retrieval strategies. It also opens
new possibilities for research and applications of computational
aesthetic analysis of images in the social media advertisement industry. Exploring and tackling the biases excluded by this study, using
generic or deep learned features, computational improvements on
the feedback system etc. promise exciting scope for future work.

Table 2: The 5 highest significance attributes identified using
linear kernel SVM for Photo.Net dataset vs. Twitter Advertisement Dataset
Photo.Net
Familiarity measure
Brightness measure
Avg. hue in wavelet
transformation
3rd largest patch size

4.3

CONCLUSION AND FUTURE WORK

Twitter Ads
Low DoF hue component
Largest segment
avg. intensity

REFERENCES

Low DoF saturation

[1] Ritendra Datta, Dhiraj Joshi, Jia Li, and James Z Wang. 2006. Studying aesthetics
in photographic images using a computational approach. In European Conference
on Computer Vision. Springer, 288–301.
[2] Ritendra Datta, Jia Li, and James Z Wang. 2008. Algorithmic inferencing of
aesthetics and emotion in natural images: An exposition. In Image Processing,
2008. ICIP 2008. 15th IEEE International Conference on. IEEE, 105–108.
[3] Sagnik Dhar, Vicente Ordonez, and Tamara L Berg. 2011. High level describable
attributes for predicting aesthetics and interestingness. In Computer Vision and
Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEE, 1657–1664.
[4] Yan Ke, Xiaoou Tang, and Feng Jing. 2006. The design of high-level features for
photo quality assessment. In Computer Vision and Pattern Recognition, 2006 IEEE
Computer Society Conference on, Vol. 1. IEEE, 419–426.
[5] Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. 2006. Beyond bags of
features: Spatial pyramid matching for recognizing natural scene categories. In
Computer vision and pattern recognition, 2006 IEEE computer society conference
on, Vol. 2. IEEE, 2169–2178.
[6] Luca Marchesotti, Florent Perronnin, Diane Larlus, and Gabriela Csurka. 2011.
Assessing the aesthetic quality of photographs using generic image descriptors.
In Computer Vision (ICCV), 2011 IEEE International Conference on. IEEE, 1784–
1791.
[7] Naila Murray, Luca Marchesotti, and Florent Perronnin. 2012. AVA: A large-scale
database for aesthetic visual analysis. In Computer Vision and Pattern Recognition
(CVPR), 2012 IEEE Conference on. IEEE, 2408–2415.
[8] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe:
Global Vectors for Word Representation. In Empirical Methods in Natural
Language Processing (EMNLP). 1532–1543. http://www.aclweb.org/anthology/
D14-1162

RAG segment count

Applications and Feedback System

Our paper describes the basic functioning and design of a system,
based on our trained SVM regressor, to identify the aesthetic feature
tunings that can be applied to image based promotions, in conjugation with other marketing strategies, to maximize user engagement.
Given an input promotional advertisement image i, we apply our
system for feature extraction to obtain the feature vector xi . Our
system ideally seeks to find the nearest-neighbor feature vectors for
xi that lead to maximum increase in predicted engagement. That is,
the system outputs a set of features and percentage changes for each,
from our initially chosen human-understandable feature space, to
maximize predicted engagement scores for the image. However,
from a practical perspective, a graphics designer would be more
interested in a tuning that provides changes to small number of
features, rather than suggesting small changes to a large number
of features, which can be inconvenient. A user may restrict the

1196

