Short Resource Papers

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

A Collection for Detecting Triggers of Sentiment Spikes
Anastasia Giachanou, Ida Mele, and Fabio Crestani
Faculty of Informatics
Università della Svizzera Italiana (USI)
Lugano, Switzerland
{anastasia.giachanou,ida.mele,fabio.crestani}@usi.ch

on many social media platforms and data, including reviews, forum
discussions, blogs and microblogs with Twitter being one of the
most popular platforms for analyzing sentiment [4]. Due to the
increasing interest, there have also been numerous attempts for
building test collections for sentiment analysis [10, 11].
However, typical sentiment analysis does not consider the dynamic nature of the opinions expressed towards an entity. Since
public opinion changes over time, it is very critical to capture its
evolution and detect changes in sentiment and sentiment spikes,
i.e., specific points in time with an unusually large number of documents expressing a specific sentiment. Detecting changes in sentiment gives the opportunity to the interested parties to take quick
reactions, whereas understanding the reasons that likely triggered
a sentiment spike provides valuable information for governments
and companies to be proactive and improve their strategies. For
example, suppose that the negative sentiment towards a politician
increases during an electoral campaign, then the respective Press
Office can extract the causes for such increase and change their
strategies accordingly.
In this paper we present, to the best of our knowledge, the first
publicly available collection for identifying and ranking triggers of
sentiment spikes in Twitter. The collection is annotated towards
three different entities: Michelle Obama, Angela Merkel, and Angelina Jolie. To collect the judgments we used CrowdFlower1 that
is a popular crowdsourcing platform. For each set of tweets representing a topic about an entity, we asked CrowdFlower’s workers
to decide the sentiment polarity (positive, neutral, or negative) and
the sentiment strength. The collection has already been used in our
research work for detecting sentiment spikes and for detecting and
ranking the likely triggers of the identified spikes [6]. The collection
can be useful for further research on sentiment analysis, detecting
and ranking triggers of sentiment spikes and sentiment prediction.
The collection and the ground truth are publicly available2 .

ABSTRACT
The advent of social media has given the opportunity to users to
publicly express and share their opinion about any topic. Public
opinion is very important for the interested entities that can leverage such information in the process of making decisions. In addition,
identifying sentiment changes and the likely causes that have triggered them allows interested parties to adjust their strategies and
attract more positive sentiment. With the aim to facilitate research
on this problem, we describe a collection of tweets that can be used
for detecting and ranking the likely triggers of sentiment spikes
towards different entities. To build the collection, we first group
tweets by topic which are then manually annotated according to
sentiment polarity and strength. We believe that this collection
can be useful for further research on detecting sentiment change
triggers, sentiment analysis and sentiment prediction.

CCS CONCEPTS
• Information systems → Test collections; Sentiment analysis; Temporal data;

KEYWORDS
Sentiment spikes, test collections, social media
ACM Reference format:
Anastasia Giachanou, Ida Mele, and Fabio Crestani. 2017. A Collection for
Detecting Triggers of Sentiment Spikes. In Proceedings of SIGIR ’17, Shinjuku,
Tokyo, Japan, August 07-11, 2017, 4 pages.
http://dx.doi.org/10.1145/3077136.3080715

1

INTRODUCTION

The rapid growth of social media platforms has given users the
capability to publish their thoughts and opinions about different
entities (e.g., people, enterprises, brands) in a very simple way.
The so-called User Generated Content (UGC) is a good source of
users’ opinion and can be very useful for applications that require
understanding the public opinion about a concept. In an attempt to
understand users’ opinions, researchers have focused on sentiment
analysis and tried to understand if a given text expresses positive,
neutral, or negative sentiment. Sentiment analysis has been studied

2

RELATED WORK

Analyzing how public opinion towards an entity evolves over time
and detecting sentiment changes and the possible triggers are problems that have attracted much research interest. For example, Bollen
et al. [3] used a psychometric instrument to analyze different moods
detected in tweets and found that the mood level in tweets was
correlated with cultural, political and other world global events.
Another interesting study is that of Balog et al. [1] who extracted
unusually common words from LiveJournal posts to find the causes
of mood changes. More recently, Montero et al. [7] used empirical
heuristics to identify emotion spikes and keyphrases to extract the

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 Association for Computing Machinery.
ACM ISBN 978-1-4503-5022-8/17/08. . . $15.00
http://dx.doi.org/10.1145/3077136.3080715

1 https://www.crowdflower.com/

2 http://www.inf.usi.ch/phd/giachanou/resources.html

1249

Short Resource Papers

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 1: Statistics of the different sentiment spikes

causes of the identified spikes, whereas Tan et al. [12] analyzed
sentiment variations and extracted the possible causes of them by
proposing a methodology based on topic modeling.
Due to the increasing interest on analyzing users’ opinions some
researchers created collections that can be used for a number of
different problems. For example, there have been attempts to build
collections for sentiment analysis [10, 11] or event detection [9]. In
addition, SemEval evaluation campaign has initiated tasks related
to sentiment analysis since 2013 [8]. However, none of these collections can be used for sentiment change and sentiment spikes’
trigger detection because most of them span over a short period
of time. To capture the dynamic nature of sentiments expressed
on social media, a collection that spans over a long period of time
is needed. Also, the collection needs to contain a large number of
opinionated documents towards different entities. In an attempt to
fill this gap, we present the first publicly available dataset that can
be used for analyzing the evolution of opinions expressed towards
different entities and more importantly for sentiment spikes’ trigger detection. The collection contains tweets about three different
entities (Michelle Obama, Angela Merkel, and Angelina Jolie) and
spans over nine months.

3

Michelle
Obama

Angela
Merkel

Angelina
Jolie

Date of spike

03 May 2015
30 May 2015
13 July 2015
10 Aug. 2015
06 Nov. 2015
21 May 2015
01 July 2015
26 July 2015
15 Aug. 2015
25 Nov. 2015
17 Apr. 2015
02 June 2015
20 July 2015
24 Sep. 2015
29 Sep. 2015

07 May 2015
31 May 2015
16 July 2015
12 Aug. 2015
08 Nov. 2015
23 May 2015
03 July 2015
29 July 2015
16 Aug. 2015
29 Nov. 2015
19 Apr. 2015
04 June 2015
21 July 2015
26 Sep. 2015
30 Sep. 2015

Number
of tweets
20,839
7,496
12,521
4,890
29,214
10,264
31,454
8,627
2,507
28,578
9,357
18,360
7,099
12,220
9,654

To identify the topics discussed around the date that the sentiment spike occurred, we applied the Latent Dirichlet Allocation
(LDA) topic model [2] on the tweets posted between the date that
the number of tweets started increasing and the actual date of the
sentiment spike. We treated each tweet as a document and we extracted 10 topics for each spike. Before applying LDA, we removed
all the occurrences of terms that were referring to the entity (e.g.,
for the entity Michelle Obama we removed all the occurrences of the
terms “Michelle” and “Obama” as well as their variations). For the
analysis, we applied Gibbs sampling for the LDA model parameter
estimation and inference as proposed in [14]. We set the number of
iterations to 2000.
The next step was to identify the sentiment spikes. For this problem, we used an outlier detection approach adopted from the field
of time series which was first applied on the problem of detecting
sentiment spikes by Giachanou and Crestani [5]. Table 1 summarizes some of the most important statistics about the sentiment
spikes of our collection.

BUILDING THE COLLECTION

In order to create a collection that can be used for detecting and
ranking the triggers of sentiment spikes, we need to analyze the
sentiment evolution towards the entity of interest. We build the
collection based on the following pipeline: collect the tweets towards three different entities, annotate tweets by sentiment polarity,
identify sentiment spikes, identify candidate topics that triggered
each sentiment spike and produce a ranking of the candidate topics
based on their contribution to the sentiment change.

3.1

Start date

Data Collection

The task of extracting and ranking sentiment spikes’ triggers requires a dataset that spans over several months. There are other
available collections of tweets but most of them are over a short
period of time. Due to Twitter’s restrictions, it was not possible
to extend the available datasets by additional months, since given
an entity you can collect the tweets that are published no more
than two weeks earlier. Hence, we used the Twitter API to collect
our data. Our collection spans from the 10th of April to the 31st of
December 2015. We focused on three entities that are well known
personalities: Michelle Obama (1,076,690 tweets), Angela Merkel
(1,369,306 tweets), and Angelina Jolie (1,264,828 tweets).
To measure the sentiment of a tweet we used SentiStrength [13]
that has been shown to be effective in different social media platforms, and it does not need any training. Although SentiStrength
can also assign a sentiment score to each tweet, we only considered
the three following classes: positive, neutral, and negative. For the
entities Michelle Obama and Angela Merkel we focused on negative sentiment polarity, whereas for Angelina Jolie we focused on
positive sentiment polarity. The reason for this decision is that
users tend to be critical with people or topics related to politics
whereas they tend to post tweets that express positive opinions
about celebrities.

3.2

Design of the CrowdFlower Experiment

As already mentioned, our final aim was to get a ranking of the
extracted topics based on the sentiment polarity and strength of
the tweets that belong to each topic. First, we extracted the topics
discussed before and on the specific date of the sentiment spike
using LDA. Each topic was related to a list of keywords and a set
of tweets. One problem that we encountered was that it was not
possible to show to the annotators the whole set of tweets since
most of the sets contained few thousand tweets. Therefore, we
decided to get a sample of the tweets that belong to each set. This
number ranged from 3 - 6 % depending on the popularity of the
topics. The same percentage was used for all the topics that belong
to the same spike. This is important since we wanted annotators to
have an estimate of the popularity of each topic.
Another challenge was how to select each sample of tweets. One
possible solution would be to rank them chronologically and then
use systematic sampling, that would be adding a tweet into the
sample using a constant step (e.g., select one tweet every 100 tweets).

1250

Short Resource Papers

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

However, still there would be a risk of creating a sample that was
not representative. Therefore, we ranked the tweets based on their
similarity with the topic. To do this, we used the representative
keywords of the topic generated by LDA and we ranked its tweets
based on the number of their common words. Also, we tried to
exclude as many retweets as possible since repeated content could
be annoying for the annotators. However, in some cases showing
retweets was inevitable. In particular, for those topics in which we
had only a small number of distinct tweets and the majority of the
tweets were simple retweets.
For our evaluation we needed a ranking of the extracted topics,
however, it would be very tricky and difficult for the annotators to
go through the lists of tweets, each one representing a topic, and
give back a ranking of these topics. Therefore, we asked the annotators to rate only one topic at a time. Given the list of keywords of
the topic and the sample of the tweets, we asked the following question: “What is the polarity and the strength of the sentiment/emotions
expressed in this set of tweets?" and one annotator had to give a
rating for all the topics that we extracted from the period before
and related to a sentiment spike. Note that the order we showed
the topics was completely random and different for each annotator.

3.3

the spike on the 16th of August, 70% percent of his ratings were
equal to -3, 20% percent of his ratings were equal to -2, and 10% to
-1. Trying to explain this weird behavior, we looked at the demographic data and realized that the annotator was Greek and that
most of the topics that he/she rated with -3 were about Greece (e.g.,
topics about Greek crisis, greek referendum, greek debt). Even if
this annotator probably was not a spammer, we considered that
his/her replies as biased for the specific topics and we removed
his/her contributions.
After we removed the biased and the low-quality contributions
we ended up with the following annotations per entity; for Michelle
Obama we had 30 different annotators that submitted 470 evaluations for the 50 sets of tweets with an average of 15.66 sets per
annotator; for Angela Merkel we had 22 different annotators and
420 total annotations with an average of 19.1 sets per annotator;
for Angelina Jolie we had 16 different annotators and 500 total
annotations with an average of 31.25 sets per annotator.

4

ANALYSIS AND DISCUSSION

As previously mentioned, for each of the entities we focused on five
different sentiment spikes. To understand the causes of a sentiment
spike, we looked through the tweets that were published not only
on the specific day but also a bit before. Since we considered 5
different spikes we had 5 different time windows per entity, and we
extracted the topics discussed in all of them using LDA.

Annotators

Collecting human judgments with crowdsourcing has the risk of
low-quality submissions. The most popular technique for checking
the quality of submissions is having test questions which are used
in the quiz page (e.g., the first page displayed to the annotator) to
train the annotators, and which are also randomly displayed in each
page for checking the performance of each annotator. In our case,
each annotator had to evaluate all the topics that were extracted
from a specific spike (i.e., all the ten topics extracted from a specific
date must be displayed on the same page) and therefore we had to
annotate at least 8 test questions (out of 10) which could be used in
the quiz page. Due to this design restrictions of CrowdFlower and
the design of our experiment, it was not possible to have enough
test questions during the training and the execution of the task.
Instead of having test questions, we measured the accuracy of
the submissions afterwards and removed the annotators with lowquality or biased submissions. To do so, we followed a specific
process. In particular, for each topic, we first tried to understand the
trend in the rating, that is the majority class, and then to identify any
ratings that deviated from the trend. For example, if one annotator
rated a topic as expressing a positive sentiment (ranking +1, +2, or
+3) whereas the large majority of the annotators rated this topic
as expressing a negative sentiment (ranking -1, -2, or -3) then this
rating was labeled as deviated. If the annotator had at least two
deviated ratings on a specific date, then his/her contributions on
the specific date were removed. Here, we want to notice that there
were cases where one annotator submitted ratings for more than
one spikes. If the annotator had two or more deviated ratings for
only one spike, we removed only the specific contributions and not
all his/her contributions.
One interesting case with many deviated ratings was one annotator of the entity Angela Merkel. The specific annotator used a
high percentage of the score -3 whereas the rest of the annotators
were more conservative for most of those topics. For example, for

Table 2: Sample of extracted topics from different sentiment
spikes

07 May 2015
31 May 2015
16 July 2015
12 Aug. 2015
08 Nov. 2015
23 May 2015
03 July 2015
29 July 2015
16 Aug. 2015
29 Nov. 2015
19 Apr. 2015
04 June 2015
21 July 2015
26 Sep. 2015
30 Sep. 2015

Michelle Obama
ruined lunch cookies college signing
grieving tonight bidens beau death
mayor gorilla resign face racist
stand years feminist attacks miss
kids drag fam roast mom
Merkel
scandal political crisis spy germany
debt unsustainable greek wikileaks phone
girl palestinian cry caused abolish
bigger challenge migrants crisis european
syria military french downing aircraft
Angelina Jolie
life structure cheekbones appreciation amazing
birthday happy beautiful women inspirational
blood imagine veins donate celebrities
awards academy flaws bones kardashians
pitt brad smith movie amazing

Table 2 shows one of the topics that was extracted from the
five different sentiment spikes of each entity. We could observe
that LDA managed to group terms that were about the same topic
together. Some of those topics are related to important news (e.g.,
the topic detected for Angela Merkel on the 3rd of July that is
about German chancellor admitting in a 2011 phone call that Greek
debt is unsustainable) whereas other are less important events (e.g.,
the topic on the 4th of June that is about wishing happy birthday
to Angelina Jolie). This is due to the informal style of Twitter in

1251

Short Resource Papers

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

which users frequently retweet a message that does not refer to an
important event but the users may find it interesting or funny.
In total we collected 1,390 relevance assessments. Figure 1 shows
the average inter-annotator agreement for each sentiment spike
and each entity. We considered two different settings to calculate
the inter-annotator agreement. In the first setting (Setting_1) we
considered all the possible ratings (-3, -2, -1, 0, 1, 2, 3) such that two
annotators agree if and only if they have given the exact same rating. In the second setting (Setting_2) we considered three different
classes (positive, neutral, negative). In this case we considered that
two annotators agree if both of them have given a positive (1, 2, 3),
a neutral (0) or a negative (-1, -2, -3) rating. As can be observed, the
percentage of agreement increased when we considered only three
classes. In addition, we observe that there is higher agreement for
the entity Angelina Jolie in most of the spikes compared to Michelle
Obama and Angela Merkel. We believe that one reason is that positive sentiment is easier to understand compared to negative and
therefore is more likely the annotators to give similar ratings.
1

Positive

80%
70%
60%
50%
40%
30%
20%
10%

Michelle Obama

Merkel

30-Sep

26-Sep

21-Jul

4-Jun

19-Apr

29-Nov

16-Aug

29-Jul

3-Jul

23-May

8-Nov

12-Aug

16-Jul

31-May

7-May

0%

Jolie

Figure 2: Distribution of relevance assessments in reference
to the sentiment polarity class per spike and entity

5

CONCLUSIONS

In this paper, we presented a new collection for extracting and
ranking triggers of sentiment spikes. The collection contains tweets
grouped by topics and labeled based on their sentiment polarity and
strength. The collection spans over 9 months and contains tweets
about three entities: Michelle Obama, Angela Merkel, and Angelina
Jolie. The ground truth was collected using crowdsourcing. The
collection can be used for further research on sentiment analysis,
temporal analysis of sentiment and sentiment prediction.

Setting_2

0.8

Negative

90%

Setting_1

0.9

Neutral

100%

0.7
0.6
0.5
0.4
0.3
0.2
0.1

ACKNOWLEDGEMENT
Michelle Obama

30-Sep

21-Jul

26-Sep

4-Jun

19-Apr

29-Nov

29-Jul
Merkel

16-Aug

3-Jul

23-May

8-Nov

16-Jul

12-Aug

31-May

7-May

0

This research was partially funded by the Swiss National Science
Foundation (SNSF) under the project OpiTrack.

Jolie

REFERENCES

Figure 1: Average inter-annotator agreement per sentiment
spike and entity

[1] K. Balog, G. Mishne, and M. de Rijke. Why are they excited?: Identifying and
explaining spikes in blog mood levels. In EACL ’06, pages 207–210, 2006.
[2] D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022, 2003.
[3] J. Bollen, A. Pepe, and H. Mao. Modeling Public Mood and Emotion : Twitter
Sentiment and Socio-Economic Phenomena. In ICWSM’11, pages 450–453, 2011.
[4] A. Giachanou and F. Crestani. Like it or not: A survey of twitter sentiment
analysis methods. ACM Computing Surveys (CSUR), 49(2):28, 2016.
[5] A. Giachanou and F. Crestani. Tracking sentiment by time series analysis. In
SIGIR ’16, pages 1037–1040, 2016.
[6] A. Giachanou, I. Mele, and F. Crestani. Explaining sentiment spikes in twitter. In
CIKM ’16, pages 2263–2268. ACM, 2016.
[7] C. S. Montero, H. Haddad, M. Mozgovoy, and C. B. Ali. Detecting the likely causes
behind the emotion spikes of influential twitter users. In CICLing ’16, 2016.
[8] P. Nakov, Z. Kozareva, A. Ritter, S. Rosenthal, V. Stoyanov, and T. Wilson. Semeval2013 task 2: Sentiment analysis in twitter. In SemEval ’13, pages 312–320, 2013.
[9] S. Petrović, M. Osborne, and V. Lavrenko. The edinburgh twitter corpus. In
NAACL HLT 2010 Workshop, pages 25–26, 2010.
[10] H. Saif, M. Fernández, Y. He, and H. Alani. Evaluation Datasets for Twitter
Sentiment Analysis A survey and a new dataset, the STS-Gold. In ESSEM ’13:
Approaches and Perspectives from AI Workshop, 2013.
[11] M. Speriosu, N. Sudan, S. Upadhyay, and J. Baldridge. Twitter polarity classification with label propagation over lexical links and the follower graph. In EMNLP
’11: Unsupervised Learning in NLP Workshop, pages 53–63, 2011.
[12] S. Tan, Y. Li, H. Sun, Z. Guan, X. Yan, J. Bu, C. Chen, and X. He. Interpreting the
public sentiment variations on twitter. IEEE Transactions on Knowledge and Data
Engineering, 26(5):1158–1170, 2014.
[13] M. Thelwall, K. Buckley, G. Paltoglou, D. Cai, and A. Kappas. Sentiment strength
detection in short informal text. Journal of the American Society for Information
Science and Technology, 61(12):2544–2558, 2010.
[14] L. Yao, D. Mimno, and A. McCallum. Efficient methods for topic model inference
on streaming document collections. In KDD ’09, pages 937–946, 2009.

Figure 2 shows the overall distribution of the relevance assessments for all the extracted topics per sentiment spike and entity.
We observe that the majority of the assessments given for the topics
about Michelle Obama and Angela Merkel were rated as negative
whereas the majority of the assessments given about Angelina Jolie
were rated as positive. One likely reason for this is that users tend
to express positive opinion when they post about celebrities usually
showing their admiration whereas they tend to be more critical on
persons or topics related to politics.
Finally, after additional analysis, we found that some topics have
a high standard deviation. For example, the topic with the highest
standard deviation is Topic 1 of Michelle Obama on May 31, 2015
for which the collected ratings have a standard deviation of 1.856,
whereas the average standard deviation on the specific spike was
1.347. This topic contains tweets about the death of Beau Biden (i.e.,
“Michelle and I are grieving tonight. Beau Biden was a friend of ours." ).
This topic is very subjective and some annotators considered that
if one is grieving for having lost a person, this can be seen as a
positive sentiment towards the person who passed away, whereas
others considered it as expressing negative sentiment.

1252

