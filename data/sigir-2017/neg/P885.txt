Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

A Poisson Regression Method for Top-N Recommendation
Jiajin Huang, Jian Wang

Ning Zhong

Beijing University of Technology
Beijing, China 100124
{hjj,wj1989}@emails.bjut.edu.cn

Maebashi Institute of Technology
Maebashi, Japan 371-0816
zhong@maebashi-it.ac.jp

ABSTRACT

is suitable for modeling rating scores on items which users
explicitly tell us.
However, users would be not willing to explicitly tell us
their preferences. Fortunately, under the assumption that a
user is more interested in the item with her higher visiting
frequency than ones with lower frequency, we can represent
a user’s preference on diﬀerent items according to how frequently the user visits [11]. In this case, the Poisson factor
model (PoiFM) [3, 10, 11] is proposed to model user preferences by assuming the observed user-item frequency matrix
follows a Poisson distribution with the parameter generated
by the inner product of two low-dimensional factor matrices:
user latent matrix and item latent matrix. By extending the
PoiFM, Chaney et al. [2] developed a social poisson factorization method for recommendations.
Moreover, the item-item correlation plays an important
role in recommender systems [7, 9]. For example, the widely
used item-based collaborative ﬁltering assumes that a user
may like items similar to what he has bought. Koren et
al. [8] used item-item correlations as user-speciﬁc weights
in the matrix factorization (MF) method. Christakopoulou
et al. [5] added a local item-item correlation into the MF
method. However, the existing PoiFM ignores that a user’s
decision on the target item may be aﬀected by what the user
visited. We also need to consider correlations among items
to improve the recommendation quality of PoiFM.
In this paper, we aim to study how to integrate the itemitem correlation within the framework based on the Poisson regression model for recommendations. Following the
PoiFM model, we also model a user preference by the inner product of the user latent factor and item latent factor.
The item-item correlation is modeled by a correlation matrix.
Then by summing up the user preference and the item-item
correlation, we can deﬁne the parameters of the Poisson likelihood function corresponding a target user on a target item.
Under the assumption that the correlation matrix is sparse,
we estimate the correlation matrix by using the alternating
direction method of multipliers (ADMM) [1] which can deal
with the constraints for the model parameters. At last, we
apply the proposed model to top-N recommendation tasks.

Top-N recommendation tasks aim to solve the information
overload problem for users in the information age. As a user’s decision may be aﬀected by correlations among items, we
incorporate such correlations with the user and item latent factors to propose a Poisson-regression-based method for
top-N recommendation tasks. By placing priori knowledge
and using a sparse structure assumption, this method learns
the latent factors and the structure of the item-item correlation matrix through the alternating direction method of
multipliers (ADMM). The preliminary experimental results
on two real-world datasets show the improved performance
of our approach.

CCS CONCEPTS
•Information systems →Collaborative filtering;

KEYWORDS
Recommender systems; poisson regression; item-item correlations
ACM Reference format:
Jiajin Huang, Jian Wang and Ning Zhong. 2017. A Poisson Regression Method for Top-N Recommendation. In Proceedings of
SIGIR ’17, August 7–11, 2017, Shinjuki, Tokyo, Japan, 4 pages.
DOI: http://dx.doi.org/10.1145/3077136.3080670

1

INTRODUCTION

In recent years, recommender systems are used to help users
tackle the information overload problem by providing them
with what they may like. Many methods are proposed to
rank items according to the relevance between the target user and item in descending order [6]. And then, top items
in the list are recommended to users. The probabilistic matrix factorization (PMF) [12] method assumes the responses
of users on items follow a Gaussian distribution, and then
ranks items according to the predicted response. The research result shows that the Gaussian distribution in PMF
Permission to make digital or hard copies of all or part of this work
for personal or classroom use is granted without fee provided that
copies are not made or distributed for profit or commercial advantage
and that copies bear this notice and the full citation on the first page.
Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee. Request permissions
from permissions@acm.org.
SIGIR ’17, August 7–11, 2017, Shinjuki, Tokyo, Japan
© 2017 ACM. ISBN 978-1-4503-5022-8/17/08. . . $15.00.
DOI: http://dx.doi.org/10.1145/3077136.3080670

2

MODEL

We deﬁne the user set U = {u1 , u2 , · · · , uM } and the item set
V = {v1 , v2 , · · · , vN }. The response (rating, like, or implicit
frequency) of a user ui on an item vj is yij . A user ui ∈ U
has a response on an item vj ∈ V, written as ui Y vj . The
relation Y between U and V is a subset of pairs (ui , vj ) in
the Cartesian product U × V. By projecting Y on a given

885

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

where ΘT is Lagrange multipliers, tr(·) denotes the trace of
a square matrix, ρ is a penalty parameter and || · ||F denotes
the Frobenius norm of a matrix.
To minimize Lρ (U, V, W, H, Θ) with respect to U , V , and
W , we use the gradient method with the gradients computed
as
N
∑
∂Lρ
=
(µij − yij )Vj + λ1 Ui
∂Ui
j=1

user ui , we have a user-oriented relation
Yui = {vj |(ui , vj ) ∈ Y }.

(1)

We use a Poisson distribution to model the likelihood function for yij as
yij

∼

P(µij )

ln µij

=

Ui VjT +

∑

ωsj lnyis

(2)

s∈Yui

∂Lρ
∂Vj

where Yui is the set of items which user ui visited, P denotes
the Poisson distribution with its probability density function
formulated as p(x) = µx exp{−µ}/x!.
According to Eq. (2), lnµij consists of two parts. The
ﬁrst part is the linear function of a user latent vector Ui
and an item latent vector Vj , which is identical to the basic
PoiMF model. The second part is the contribution of items
that a user visited. In the second part, the inﬂuence factor
is described by the matrix W whose (s, j)th element is ωsj .
The inﬂuence between vs and vj can be parameterized by
ωsj and ωjs . Speciﬁcally, ωsj could be described as the nonnegative possibility of a user liking item vj after the user
visited vs . By deﬁning ln0 = 0, we can have
ln µij = Ui VjT +

N
∑

ωsj ln yis

∂Lρ
∂ωsj

H≥0

(6)
1
θsj )
ρ

(7)

Algorithm 1 User-oriented ADMM algorithm (U-ADMM)
01. Initialize U (0) , V (0) , W (0) , and Θ(0) ;
02. for t = 1 to itermax do
03. U (t) =arg minU Lρ (U, V (t−1) , W (t−1) , H (t−1) , Θ(t−1) );
04. V (t) =arg minV Lρ (U (t−1) , V, W (t−1) , H (t−1) , Θ(t−1) );
03. W (t) =arg minW Lρ (U (t−1) , V (t−1) , W, H (t−1) , Θ(t−1) );
04. H (t) =arg minH Lρ (U (t−1) , V (t−1) , W (t) , H, Θ(t−1) );
05. Θ(t) =Θ(t−1) + ρ(W (t) − H (t) );
06. end
The proposed model predicts the frequency of a user ui
visiting an item vj as µij . We make recommendations based
on the predicted values. The larger µij means that ui will
more likely choose vj .

(4)

3

Following the study of [13], the augmented Lagrangian is
deﬁned as

EXPERIMENTS

In this section, we ﬁrst compare the performance of our model with PMF and PoiFM models. And then, we analyze the
impact of the important parameters in our model U-ADMM.

M ∑
N
∑
(µij − yij ln µij )
i=1 j=1

3.1

M
N
∑
λ1 ∑
||Ui ||22 +
||Vj ||22 )
+ (
2 i=1
j=1
∑
+λ2
ωsj + tr(ΘT (H − W ))

Datasets and Metrics

Experimental data are from Gowalla website and Brightkite
website [4]. The Gowalla dataset includes a total of 6442890
check-in records generated by 196591 users from February
2009 to October 2010; And the Brightkite dataset includes
a total of 4747200 check-in records generated by 51406 users
over the period of April 2008 - October 2010. We remove

s,j

ρ
+ ||H − W ||2F
2

s.t.

ρ
||H − W ||2F
2

Based on the above notations, the U-ADMM algorithm
with the learning rate γ is described in Algorithm 1 by extending the ADMM [1] .

(3)

N
∑
λ1 ∑
ωsj
+
||Vj ||22 + λ2
2 j
s,j

=

tr(ΘT (H − W )) +

hsj = max(0, ωsj −

M ∑
N
M
∑
λ1 ∑
(µij − yij ln µij ) +
||Ui ||22
2
i=1 j=1
i

Lρ (U, V, W, H, Θ)

i=1

which has an analytical solution as

Then we can formulate the objective function of the MAP
solution as

H = W, H ≥ 0

i=1

min
H

1
Ui , Vj ∼ N (0,
I)
λ1
1
ωsj ∼ L(0,
)
λ2

s.t.

=

M
∑
(µij − yij ) ln yis + λ2 − θsj

For H, we can have

We add a normal prior on Ui and Vj to avoid over-ﬁtting.
And we also place a Laplace prior on non-negative W to
make it be sparse. So we have

U,V,W

M
∑
(µij − yij )Ui + λ1 Vj

+ρ(ωsj − hsj )

s=1

min

=

(5)

886

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

users and items that occur less than twenty-ﬁve times, and
extract 159335 check-in records generated in Los Angeles
by 1471 users from the Gowalla dataset. At the same time,
we remove users and items that occur less than ten times,
and extract 257249 check-in records by 1850 users from the
Brightkite dataset. Statistics of datasets are given in Table
1.

20
U-ADMM
PoiFM
PMF

12

8

U-ADMM
PoiFM
PMF

16

recall(%)

precision(%)

16

12
8
4

4
0

Table 1: Dataset Statistics

5

10

15

20

25

5

10

Top

#users
#items
#check in
#density

Gowalla

Brightkite

1471
1539
159335
0.9596

1850
1666
257249
0.9877

10

20
6
4

15
10
5

0

u∈U

U-ADMM
PoiFM
PMF

25

2

×R
and F 1 = 2×P
, where R(u) is the set of the recommendP +R
ed items for user u, T (u) is the set of items which user u
preferred in the test set, and | · | is the cardinality of a set.

0
5

10

15

20

25

5

10

Top

15

20

25

Top

Figure 2: Performance comparison between UADMM and other methods on the Brightkite
dataset

Comparison

In this section, we show the preliminary results of U-ADMM
model compared with some baseline methods including PMF
and PoiFM. In the training datasets, all methods are trained
to a relatively optimal state through learning respective parameters. On the Gowalla dataset, we set the number of latent factors K=30 for all methods. For PMF, learning rate
γ=0.0012, λ1 =0.01. For PoiFM, γ=0.003, and the requested
prior Gamma distribution parameter au =5, bu =0.06, av =3,
bv =0.1. For our model U-ADMM, γ=0.01, λ1 =0.7, λ2 =0.07,
ρ=0.005. Figure 1 shows the results about the three models with precision and recall on the Gowalla dataset. Then,
we turn our attention to the Brightkite dataset. For PMF,
K=90, γ=0.0002, λ1 =0.007. For PoiFM, K=70, γ=0.0005,
au =3, bu =0.008, av =1, bv =0.01. For U-ADMM, K=10,
γ=0.003, λ1 =1.2, λ2 =0.007, ρ=0.0001. The performance
on the Bright-kite dataset is shown as Fig. 2. And all the
models are evaluated with diﬀerent number of top items in
the recommended list.
As shown in Fig. 1 and 2, with the not very good precision
and recall values of PMF, we can observe that it is not easy
to predict how many times exactly a user visited an item.
However, we can obtain a better result by assuming that
the frequency of a user visiting an item follows a Poisson
distribution, and the proposed U-ADMM can improve the
performance by considering the correlations among items.

3.3

25

30
U-ADMM
PoiFM
PMF

recall(%)

precision(%)

We randomly divide each dataset into ﬁve parts, and select four parts as the training set and measure the prediction
performance on the rest testing set. We evaluate the eﬀectiveness of the∑proposed system by respectively
using the
∑
|R(u)∩T (u)|
|R(u)∩T (u)|
u∈U
∑
∑
precision P = u∈U
,
recall
R
=
,
|R(u)|
|T (u)|

of latent factors K; 2) the learning rate γ, 3) the λ1 , λ2 and
ρ that control regularization.
To be fair, we investigate the impact of those parameters
at a relatively optimal state. By varying one of the parameters and ﬁxing the other, we investigate the impact of those
parameters. First, we vary the latent factors K= {5, 10, 30,
50, 70, 90} and evaluate the predicted result with F 1 which
considers both precision and recall. As shown in Fig. 3, UADMM performs best when K = 10 on the both datasets.
11.85

9.5
9.4

F1(%)

11.80

F1(%)

3.2

20

Figure 1: Performance comparison between UADMM and other methods on the Gowalla dataset

8

u∈U

15

Top

11.75

11.70

9.3
9.2
9.1

11.65

9.0
5

10

30

50

K

70

90

5

10

30

50

70

90

K

Figure 3: Impact of K in U-ADMM on the Gowalla
(a) and Brightkite (b) dataset

Impact of Parameters

Next, we investigate the sensitivity of γ, λ1 , λ2 , and ρ.
We also vary one of them and ﬁx the other, and measure
the result by F 1. As shown in Fig. 4 and 5, U-ADMM

Here we show the impact of the ﬁve important parameters in
U-ADMM for top 10 recommendation tasks: 1) the number

887

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

4

12.5

12.0

12.0

11.8

11.5

11.6

F1(%)

F1(%)

model performs better with the parameter values mentioned
in Section 3.2. From Fig. 4 and 5, we can also observe that
smaller values of the learning rate γ and penalty parameter ρ
tend to give better performances on the both datasets. The
best ranges of prior parameter λ1 and λ2 are diﬀerent on
diﬀerent datasets.

11.0

ACKNOWLEDGMENTS
11.4

We thank Zhaoqiang Li for making a double check of the
experimental results. The work is supported by the National Natural Science Foundation of China under Grant
No.: 61420106005, 61562009.

11.2

10.5

10.0
5

10

15

20

25

30

11.0
(10 -3 )
0.0

0.2

0.4

γ

0.6

0.8

1.0

λ1
12.0

12.0

REFERENCES
[1] S. Boyd, N.Parikh, E. Chu, B. Peleat, and J. Eckstein. 2011. Distributed optimization and statistical learning via the alternating
direction method of multipliers. Foundations and Trends in
Mache Learning 3, 1 (2011), 1–122.
[2] A.J.B. Chaney, D.M. Blei, and Eliassi-Rad. 2015. A probabilistic
model for using social networks in personalized item recommendation. In Proceedings of the 2015 ACM Conference on Recommender Systems (RecSys 2015). 43–50.
[3] Y. Chen, D. Pavlov, M. Kapralov, and J.F. Canny. 2009. Factor
modeling for advertisement targeting. In Proceedings of the 2009
International Conference on Advances in Neural Information
Processing Systems (NIPS 2009). 324–332.
[4] E. Cho, S.A. Myers, and J. Leskovec. 2011. Friendship and mobility: user movement in location-based social networks. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2011). 1082–1090.
[5] E. Christakopoulou and G. Karypis. 2016. Local item-item models for top-N recommendation. In Proceedings of the 10th ACM
Conference on Recommender Systems (Recsys 2016). 67–74.
[6] W. Cohen, R.E. Schapire, and Y. Singer. 1999. Learning to order
things. Artificial Intelligence Research 10 (1999), 243–270.
[7] L. Guo, J. Ma, Z.M. Chen, and H. Zhong. 2015. Learning to recommend with social contextual information from implicit feedback. Soft Computing 19, 5 (2015), 1351–1362.
[8] Y. Koren. 2010. Factor in the neighbors: Scalable and accurate
collaborative filtering. ACM Transactions on Knowledge Discovery from Data 4, 1 (2010), 1:1–1:24.
[9] S. Li, A. Karatzoglou, and C. Gentile. 2016. Collaborative filtering bandits. In Proceedings of the 39th International ACM
SIGIR conference on Research and Development in Information Retrieval (SIGIR 2016). 539–548.
[10] B. Liu, H. Xiong, S.Papadimitriou, Y.J. Fu, and Z.J. Yao. 2015.
A general geographical probabilistic factor model for point of interest recommendation. IEEE Transactions on Knowledge and
Data Engineering 27, 5 (2015), 1167–1179.
[11] H. Ma, C. Li, I. King, and M.R. Lyu. 2011. Probabilistic factor
models for web site recommendation. In Proceeding of the 34th
International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2011). 265–274.
[12] R. Salakhutdinov and A. Mnih. 2008. Probabilistic matrix factorization. In Proceedings of the 2008 International Conference
on Advances in Neural Information Processing Systems (NIPS
2008). 1257–1264.
[13] Y. Zhang, W.K. Cheung, and J.M. Liu. 2015.
A unified
framework for epidemic prediction based on poisson regression.
IEEE Transactions on Knowledge and Data Engineering 27, 11
(2015), 2878–2892.

11.9

F1(%)

F1(%)

11.9
11.8

11.8
11.7

11.7
0.00

0.02

0.04

0.06

0.08

11.6
0.00

0.10

0.02

0.04

λ2

0.06

0.08

0.10

ρ

10

9.6

8

9.5

6

9.4

F1(%)

F1(%)

Figure 4: Impact of Parameter γ, λ1 , λ2 and ρ in
U-ADMM on the Gowalla dataset

4

9.3

9.2

2

9.1

0
0.03

0.3

3

30

(10 -2 )

300

0.8

1.0

γ

1.2

1.4

1.6

0.1

1

λ1

9.55

9.6

9.50

9.4

F1(%)

F1(%)

9.45
9.40

9.2

9.0

9.35
8.8

9.30
9.25

8.6
0

2

4

6

λ2

CONCLUSIONS

This paper proposes the Poisson regression model U-ADMM
for top-N recommendation tasks. The proposed U-ADMM
adds correlations among items to revise user preference about
a target item by extending the ADMM. The results for topN recommendation tasks show that introducing correlations
among items to revise user preference can eﬀectively improve
recommendation quality in the poisson factor model. In our
future work, we will add the correlations among users to our
model to yield further improvements.

8

10

(10 -3 )

0.0001

0.001

0.01

ρ

Figure 5: Impact of Parameter γ, λ1 , λ2 and ρ in
U-ADMM on the Brightkite dataset

888

