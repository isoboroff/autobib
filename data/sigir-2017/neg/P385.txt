Session 3C: Document Representation and Content Analysis 2

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

On the Power Laws of Language: Word Frequency Distributions
Flavio Chierichetti∗

Ravi Kumar

Bo Pang

Sapienza University
Rome, Italy
flavio@di.uniroma1.it

Google
Mountain View, CA, USA
ravi.k53@gmail.com

Google
Mountain View, CA, USA
bopang42@gmail.com

ABSTRACT

and refine Zipf’s law [10, 15, 19, 20, 28, 29, 31, 34, 40, 41]. Additionally, Zipf’s law has been considered in the context of document
retrieval by IR systems [1–3, 8, 9].
In large-scale empirical studies, however, the rank-vs-frequency
distributions do not appear as straight lines on a log-log plot. Instead, they exhibit a bend that makes the curve look concave; we
call the rank value at which the bend occurs as the knee. Interestingly, the bend is consistent with Zipf’s original plot: the maximum
rank in his plots is close to 103 , whereas the knee is usually observed at a rank that is an order of magnitude higher. It is likely
that the lack of computing power and automated tools made it infeasible for Zipf to move to a rank significantly larger than 103 . This
concavity-in-the-tail phenomenon has been noted empirically [23].

About eight decades ago, Zipf postulated that the word frequency
distribution of languages is a power law, i.e., it is a straight line on a
log-log plot. Over the years, this phenomenon has been documented
and studied extensively. For many corpora, however, the empirical
distribution barely resembles a power law: when plotted on a loglog scale, the distribution is concave and appears to be composed
of two differently sloped straight lines joined by a smooth curve. A
simple generative model is proposed to capture this phenomenon.
The word frequency distributions produced by this model are shown
to match the observations both analytically and empirically.

1

INTRODUCTION

In this work we focus on the concavity phenomenon of the word
frequency distribution. We postulate that the concavity arises from
a seamless fusion of two power laws around the knee; this fusion
is the byproduct of a natural corpus generative model that we
introduce. To validate our model, we examine a variety of corpora,
ranging from novels to news articles, and fit the functional form
that comes out of our process to their word frequency distributions.
The fit is surprisingly accurate, at the head, the tail, and the knee
of the distributions.

The distribution of word frequencies is a fundamental phenotype
of a language. Word frequency distributions have been studied
by statisticians and linguists since the statistics of word usage
yield valuable insights into the language, its construction, and its
evolution. These distributions have been long-studied outside of
statistics and linguistics as well. In information retrieval, word frequency distributions (and sometimes the ranks of word frequency)
are directly used by many algorithms for many tasks, e.g., weighting the significance of documents and query terms [2, 36], text
classification [6, 26], topic distillation [7, 13, 38], latent semantic
analysis [24, 25], and so on. The word frequency distribution plays
a central role in determining the size of inverted indices [14, 30],
the compression ratio of natural texts [11, 12].

Informally stated, our model works in two stages. In the first
stage, a vocabulary for the corpus is generated by choosing the
words from a power law distribution on the language. In the second
stage, the corpus is generated by sampling the vocabulary words
according to the same or another power law distribution. We show
that this two-stage process gives rise to a distribution that is made
up of two fused power laws. We validate this model by showing
that the distortion between the distributions produced by our model
and the empirical distributions is quite small. We also argue that a
double Pareto distribution, which is a natural candidate to explain
two fused power laws, would not be able to produce such a small
distortion. The use of a two-stage process is convenient for modeling corpora obtained from different topics (e.g., sports, politics),
where the first stage selects the topic vocabulary. Latent factor
models [24, 25] also use a two-level process for text generation;
however, each word in the text is determined by a mixture of topics
rather than a single topic.

In his pioneering work, Zipf postulated that the frequency of
any word in the language is inversely proportional to a power of
its rank [44, 45]. On a log-log plot, with the x-axis representing the
rank, and the y-axis representing the frequency, the distribution
would thus appear as a straight line with a negative slope. Subsequent studies have confirmed similar phenomena on different
corpora and genre. Even though the actual parameters can depend
on the corpus, the power-law phenomenon itself was shown to be
pervasive and robust. There have been many attempts to explain
∗ Supported

in part by the ERC Starting Grant DMAP 680153, by a Google Focused
Research Award, and by the SIR Grant RBSI14Q743.

We then turn our attention to the distribution of k-grams, which
has also been studied [17, 21]. It has been observed for some English
and Chinese corpora that the distribution becomes flatter as k
increases [22, 23]. However, to the best of our knowledge, no work
has tried to explain this phenomenon. We prove analytically that
the k-gram distributions become flatter as k increases, under the
simple assumption that the head of the word frequency distribution
is a power law.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.
978-1-4503-5022-8/17/08. . . $15.00
DOI: http://dx.doi.org/10.1145/3077136.3080821

385

Session 3C: Document Representation and Content Analysis 2

2

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

RELATED WORK

these works attempted to explain the concavity in the tail of the
word frequency distribution. Mitzenmacher [32] provides a good
survey on the topic of power laws.

Power laws, also known as Pareto distributions or Zipf’s laws, have
been observed in a broad range of settings, i.e., city populations,
sizes of earthquakes, number of citations received by papers, sales
of books, number of hits on webpages, etc. [33]. According to
Mitzenmacher [32], “The first known attribution of the power law
distribution of word frequencies appears to be due to Estoup [18],
although generally the idea (and its elucidation) are attributed to
Zipf [44–46].”

3

EMPIRICAL ANALYSIS

We first study if the concavity in word frequency distribution is
pervasive and robust, i.e., does it exist over a broad range of datasets
and does it exist even when we restrict the data to a specific genre
or topic? A plausible hypothesis for observing the concavity could
be that for a collection of text restricted to a given genre or a
given topic, the distribution would be straighter; and mixing such
distributions leads to the concavity. To test this hypothesis, we
constructed different datasets that can be split by different criteria.
What we observe is that the concavity exists for each sub-sample.

The power law, as stated by Zipf [46] (y = Kx −α ), appears as a
straight line on a log-log plot. Mandelbrot [29] extended this form
to y = K (B + x ) −α to obtain a more accurate fit for high-frequency
words. On the other hand, Simon [41] developed a stochastic process based on the work of Yule [43]. Sichel [40] studied empirical fit
of word frequencies with compound Poisson distribution. Baayen
[5] compared different statistical models proposed in previous work
for word frequency distribution. These early papers used smallscale datasets (by today’s standards) and therefore do not necessarily provide a good fit to the tail of large datasets. Ha et al. [23]
noted that on large-scale datasets, the word frequency distribution
clearly has a concavity when plotted on the log-log scale. That is,
the curve bends away from the single straight line predicted by
Zipf’s law. This phenomenon was more pronounced when they
looked at the distribution of Chinese characters. Ha et al. [23] did
not attempt to explain the form of the curves. The double Pareto
distribution, which approximates the concavity with two straight
lines, has also been considered for approximating word frequency
distributions [14]; the fit achieved by our model is significantly
more accurate than the one achievable by double Pareto distributions (see Section 5.3). Baayen [4] studied similarity relations
between words and word frequency distribution. He also noted
that function words straighten out the head of the distribution and
complex words straighten the tail. Samuelsson [37] related Zipf’s
law to Turing’s local re-estimation formula and van Leijenhorst
and van der Weide [42] related Zipf’s law to Heap’s law.

3.1

Datasets

We conducted our empirical studies over the following four datasets.
The first is Gutenberg, which is a mixed-genre, multi-topic, multilingual, and multi-author corpus of electronic books that are in the
public domain from the Gutenberg project. We use the average of
the birth and death years of the author as the approximation for the
publication year of the book. We took the subset of 16,797 books
that were written in English and has a publication year between
the 17th century and the 20th century, and grouped them into four
disjoint time periods (by century). The vocabulary sizes range from
100K words to over 800K words, and corpus sizes from 10 million
to over 500 million tokens. In addition, we can also sample this
dataset by authors.
The second dataset is News, which is a large-scale collection of
news articles on two topics, namely, sports and politics. The third
dataset is ANC (American National Corpus), which is a collection
of American English, with written texts of different genres and
transcripts of spoken data produced post 1990. The fourth dataset
is Europarl, which is a multilingual collection of European Parliament proceedings [27]. It includes semantically equivalent content
in 21 European languages. The size of the text in each language
ranges from 10 million to 50 million tokens. This allows us to examine the word frequency distribution in multiple languages without
having to worry whether differences were due to differences in
topics or genres.

There has been some work on studying the distribution of kgrams as well. Ha et al. [22, 23] studied k-grams in English and
Chinese and observed that the distribution became flatter as k
increased. Character k-grams and k-tuple distributions were also
studied in [17] and [21]. None of these works attempted to explain
these phenomena; in our work, we analytically establish the form
of the k-gram distributions and show these become flatter as k
increases.

All datasets went through the same preprocessing, where all
punctuation marks were removed, and all remaining tokens lowercased. Table 1 shows the main statistics of each of these four
datasets.

Various generative models have been proposed for producing
power laws. Zipf [46] hypothesized that the power law is the
result of the “principle of least effort”; this was re-examined later
by Mandelbrot [29], Ferrer i Cancho and Sole [20], and Ferrer i
Cancho et al. [19], who developed arguments for deriving power
law distributions based on information-theoretic considerations.
In another line of research, people have argued that preferential
attachment can lead to power laws. The general argument can
be traced back to Yule [43], and a generalization was proposed
by Simon [41]. Perc [34] proposes a preferential attachment process
for the evolution of a language, and uses it to derive the Zipf’s law.
Power laws can also be obtained through the “monkeys typing
randomly” (or “not-so-randomly”) processes [15, 28, 31]. None of

vocabulary size

corpus size

dataset
(# types)
(# tokens)
Gutenberg 19th
400,876
185M
News (politics)
256,758
31M
ANC (written)
115,806
8.6M
Europarl
87,554
56M
Table 1: Details of the four datasets

386

Session 3C: Document Representation and Content Analysis 2

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 3: Word frequency distribution in 21 European languages. English is shown in blue solid line, other languages
shown in dotted lines

Figure 1: Word frequency distribution in the Gutenberg
datasets: books in different time period (centuries), as well
as a random subset of authors, and the subset of authors
whose names begin with “J”.

Figure 2 shows word frequency distributions for two different
topics (politics vs sports) in News. We observe a similar concave
shape for both of them as AuthorJ. Figure 3 shows that the concave
shape exists in a broad range of (21 European) languages. Furthermore, one could have hypothesized that smaller vocabulary leads to
a straighter line (given previous studies with smaller datasets that
focused on the straight-line Zipf distribution); but note the curve
for English exhibits a more concave shape than that for AuthorJ,
even though it is a smaller corpus and arguable over a more focused
range of topics with less variations in styles.

3.3 k-gram frequency distribution
Figure 7 plots the empirical distribution of k-grams for k = 1 up
to 5. Given the space constraints, we include only the plot for the
Europarl data (where the unigram frequency distribution exhibits
the highest degree of concavity). As observed in [22, 23] for some
English and Chinese corpora, the lines get flatter as k grows.

4
Figure 2: Word frequency distribution in news articles: politics vs sports

3.2

MODEL

We define a simple and natural stochastic process for generating a
corpus in a language. The process takes place in two stages. In the
first stage, a founding text for the topic is written by choosing words
from the language; the set of distinct words used in the founding
text will form the vocabulary of the topic. In the second stage, the
corpus itself is generated using the words in the vocabulary.
Let the parameters α, β ∈ (0, 1), γ > 0, and a positive integer n,
(α )
be given. Here, α is the exponent of the power law P |U | defined
over the universe of words U ; n will be length of the founding text;
β determines the position of knee (which will be located around
n β ). The parameter γ is not necessary, but lends more flexibility to
our model as we
see
 will
 below.
1−α β
We set N = n 1−α = |U |, i.e., N is the number of words in the

Word frequency distribution

First, we plot the empirical observations of word frequency distributions in different datasets on a log-log scale. We observe a clear
concave shape over a broad range of corpora (Figures 1–3). Figure
1 shows word frequency distributions for different time periods in
the Gutenberg dataset. As we can see, while the time periods (and
vocabulary sizes) differ greatly, all curves closely resemble each
other. In fact, the curve for AuthorJ (authors whose names begin
with “J”) largely follows the same shape. In subsequent plots, we
include the AuthorJ curve as a reference point.

language. The distribution on the language will be the power law

387

Session 3C: Document Representation and Content Analysis 2

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

corpus
α
β
γ
n
Gutenberg 19th 0.618 0.795 1.034 400,876
News (politics) 0.569 0.725 0.898 256,758
ANC (written)
0.595 0.866 0.996 115,806
Europarl
0.691 0.800 0.986
87,554
Table 2: Fitted parameters for various corpora.

Also, let [N ] = {1, 2, . . . , N }. For α > 0, if we let U = [N ], we
have that the truncated power law distribution on U is given by
(α )
P N (i) = i −α /ζ N (α ), for i ∈ U .
(α )

We first obtain some bounds on ζ N (α ) and P N (i).
1−α

Lemma 5.1. For each 0 < α < 1, it holds that ζ N (α ) = N1−α ±



(α )
O (1) and P N (i) = 1 ± O N α −1 i −α N1−α
1−α .

5.2

(α )

P N . As we will see later, this choice of N guarantees that the knee
will be positioned close to the rank n β .
(i) In the first stage, we choose n words independently from U
(α )
according to the power law P N to produce the founding text of
the topic. The vocabulary V of the topic will be the set of words
that appeared at least once in the founding text. As we will see
below, with high probability we will have |V | ≈ n.
(γ )
(ii) In the second stage, we use a second power law P N over

Word frequency distribution

In this section we analyze the word frequency distribution produced
by the generative model. We proceed to study the probability R(i)
that the ith word, 1 ≤ i ≤ N , appears in the vocabulary V . Since V
was constructed using n independent samples, we have
(α )

R(i) = 1 − (1 − P N (i)) n .

 1−α β  1−α β
· n 1−α , and by Lemma 5.1, we have
By N = 1 + O n− 1−α

(α )

(α )

the language, possibly but not necessarily different from P N . A
corpus for the topic is generated by choosing words independently
(γ )
from P N restricted to the vocabulary (i.e., support) V .
The use of a two-stage process is convenient to model corpora
belonging to different topics (e.g., sports, politics): the first stage
effectively determines the vocabulary of the topic. The assumption
(α )
of choosing words from V according to the original power law P N
has been made before; see, for example [40].
In our model, the parameter γ gives additional flexibility, since
one is not forced to use the same power law exponent α to choose
words from the vocabulary. If we insist on model parsimony, the γ
parameter can be removed by choosing γ as a function of α.
Figure 5 shows the best fit of our model (formalized in the next
Section) to four empirical corpora. Note that the fit from our model
traces the empirical distribution quite accurately (we discuss this
in Section 5.4). We include the parameters for each fit in Table 2.

5

P N (i) =



− O i −α n 2α β −2 .

That is,
1−α
.
i α n 1−α β
Since 0 < α, β < 1 the multiplier is no worse than 1 − o(1).
Our analysis begins by showing that R(i) — that is, the probability
that the ith term of the language appears in the vocabulary — can
be expressed by a simple exponential term.
!
nα β
Lemma 5.2. R(i) = (1 ± o(1)) 1 − e −(1−α ) i α .



(α )
P N (i) = 1 − O n α β −1

(α )

Lemma 5.2 can be shown by approximating P N (i) as in Lemma 5.1,
since the error term in Lemma 5.1 is small enough to prove the
statement of Lemma 5.2.
We then use the new expression of R(i) to compute the expected
number of terms with rank at most k that appears in the vocabulary.
Specifically, let Uk ⊆ U be the set of words that have rank at most
(α )
k in P N . We focus on the number of words in Uk that make it to
the vocabulary V . I.e., we focus on the random variable |V ∩ Uk |.
P
Observe that E[|V ∩ Uk |] = ki=1 R(i). Lemma 5.3 shows that this
expectation is very well approximated by the function D (k ) (we
use this notation as a shorthand for D α,n β (k )):

THEORETICAL ANALYSIS

We prove in this section that, while having a very small number
of parameters, our model is able to generate curves that match
the empirically observed curves. We then study the distribution of
k-grams and prove that the distribution gets flatter as a function of
k, which matches the empirical observation as well.
All the proofs missing from this section can be found in the
Appendix.

5.1

1−α
i α n 1−α β

α

(1 − α ) 1/α β * 1
nβ
D (k ) = k −
n Γ − , (1 − α ) * + + .
α
, α
,k - Lemma 5.3. E[|V ∩ Uk |] = (1 ± o(1))D (k ).

Preliminaries

We begin with some basic notation. Let
R ∞ the upper incomplete
Gamma function be given by Γ(a, b) = b x a−1e −x dx; let Γ(a) =
Γ(a, 0). The function Γ(a, b) is well-defined for every real a if b > 0
and is well-defined for every a > 0 if b = 0. For each integer k ≥ 1,
we have Γ(k ) = (k − 1)!.
P∞ −α
Let ζ (α ) = i=1
i be the value of the Riemann Zeta function
PN −α
at α > 1. For α > 0, and an integer N ≥ 1, let ζ N (α ) = i=1
i .
Suppose that α > 1, and let P α (i) = i −α /ζ (α ) for each integer
i ≥ 1. Then, P (α ) is the power law distribution with exponent α,
defined on the universe U = {1, 2, . . .} = N.

The above lemma can be shown by integrating the expression of
R(i) that we obtained in Lemma 5.2, and by controlling the error
term.
The next step of the proof is showing that D (k ) behaves like a
simple polynomial in the ranges k < o(n β ) and k > ω (n β ), i.e., for
all k far enough from the knee. This will be key for proving that
the head and the tail of the final distribution will be power laws.
 
 
Lemma 5.4. If k < o n β , then D (k ) = (1±o(1))k. If k > ω n β ,
then D (k ) = (1 ± o(1))n α β k 1−α .

388

Session 3C: Document Representation and Content Analysis 2

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

The proof of Lemma 5.4 is relatively simple. We just have to use
the approximations
 of the R(i)’s given by Lemma 5.2, i.e., R(i)
 =

1 − o(1) if i < o n β , and R(i) = (1 ± o(1))n α β 1−α
if
i
>
ω
nβ .
iα
Then, the linearity of expectation, and Lemma 5.2, directly entail
the claim.
Negative dependence can be used to prove the next Lemma,
which simply states that with high probability for each k ∈ [N ],
the random variable |V ∩ Uk | will be quite close to its expectation
E[|V ∩ Uk |]; by Lemma 5.3, that random variable will then be very
close to the function D (k ) itself.
Lemma 5.5. With probability 1 − o(1), we will have that for each
k ∈ [N ],
|V ∩ Uk | = (1 ± o(1)) · D (k ).
Lemma 5.5 allows us to get an expression for the frequency curve
of the corpus. For k ≥ 1, the frequency curve can be expressed
parametrically as
x (k ) = D (k ),

and

y(k ) = W · k −γ ,

Figure 4: The result of an execution of the stochastic process

where W is the normalization factor. In other words, the abscissa
x (y) that one hasto associate to a given ordinate y is equal to
  1/γ
.
x (y) = D W
y

1−α β

9 , n = 106 and N = n 1−α = 108 . In this
with α = 12 , β = 32 , γ = 10
execution, the vocabulary V ended up with a cardinality of
|V | = 984328, i.e., so many distinct words were randomly selected in the first stage of the process. The curve represents
the probability distribution of the vocabulary. The head of
the curve follows a power law with exponent −γ = −0.9
and the tail of the curve follows a power law with exponent
γ
− 1−α = −1.8. Observe that the two power laws cross at an
abscissa value close to n β = 104 .

Finally, we can state our main result about the word frequency
distribution of the generative model. It follows as a corollary from
Lemma 5.4 and Lemma 5.5.
Theorem 5.6. With probability 1 − o(1), we will have:
(i) |V | = (1 ± o(1))n, and
(ii) for each rank 1 ≤ k ≤ |V |, the probability associated to the word
of rank k in V will be proportional to
 
(1 ± o(1)) · k −γ , if k = o n β ,

− γ
 
(1 ± o(1)) · n kα β 1−α , if k = ω n β .

We now show how this distortion can be computed, and obtain
its limiting values at α = 0, 1. First, for a given α, let k α be the
β
β
minimum integer suchj that
k D (k α ) ≥ n . Observe that k α ≥ n .
β
The probability of the n th word in our model’s dictionary will

Theorem 5.6 states the main properties of the word frequency
distribution: the model produces a vocabulary of size close to n,
the head of the vocabulary frequency curve follows a power law
with exponent −γ , and the tail follows a power law with exponent
−γ /(1−α ). Moreover, our parametric definition of the curve gives a
precise description of how the two power laws merge in one another.
This is important for us since we want to precisely fit the curve to the
datasets we have. Figure 4 shows the word frequency distribution
produced by our model. Observe that our model produces two
power laws that are joined around the knee at n β , as expected.

5.3

−γ

then be (1 ± o(1)) · W · k α for some normalizing factor W .
Consider the double Pareto curve having the same head and tail
β
power laws of our curve, and
j kthe same knee n . The value of this
β
double Pareto curve at the n th word will be (1 ± o(1)) ·W · n −βγ .
j k
Therefore, the distortion of the two curves at the n β th word (i.e.,
at the knee) for a given α, as n tends to infinity, is at least (d α )γ ,
where
kα
d α = lim inf
.
n→∞ n β
e Moreover, we
We show in Lemma 5.7 that limα →0+ d α = e−1
will show in Lemma 5.8 that, as α converges to 1, d α diverges at
least as fast as c 1/(1−α ) for some constant c > 1.
We state the two Lemmas, and briefly comment on how they
can be proved.

Relation with double Pareto

One might wonder why we did not use a simple double Pareto curve
(as in [14]) to model the distributions. The main reason is that the
ratio of the probability at the rank i ' n β of the double Pareto curve
(with the correct power laws, and the correct knee) and of our curve
at the same i is quite large. We will show
in

 this section that (i)
e γ = (1.5819 . . .)γ for
the multiplicative distortion is at least e−1
α → 0, and (ii) the distortion diverges exponentially to ∞ as α
approaches 1. Moreover, we numerically obtain that at α = 0.6
(close to empirical numbers; see Table 2), the distortion becomes
(3.0270 . . .)γ . Later in Section 5.4, we empirically analyze the distortion in two of our corpora.

Lemma 5.7. For 0 < α < 12 , we have
 e

kα =
± O (α ln 1/α ) · n β .
e −1
A heuristic proof of the above statement would argue that, if
α = 0, then all the terms are equally likely to be chosen, i.e., they
have probability N −1 , with N = n

389

1−α β
1−α

= n. In other words, the

Session 3C: Document Representation and Content Analysis 2

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

vocabulary is constructed by throwing n balls (the words in the
founding text), into N = n bins (the words of the language). By
the Chernoff bound, any given set of t  1 bins will be hit by
approximately t balls with high probability. Moreover, by classic
e · n β balls that hit the first e · n β
balls-in-bins arguments, the e−1
e−1
bins will be distributed across approximately n β distinct bins with
high probability (essentially because of the Poissonian approximae · nβ .
tion of the binomial distribution). Hence, k α ≈ e−1
The above reasoning can be made formal, so that it can be applied
to small α > 0.
Finally, we show that in the opposite regime, α = 1 − ϵ, d α
diverges to infinity at an exponential rate.
Lemma 5.8. There exists a constant c > 1 such that, for all 12 <
α < 1, we have
1

k α ≥ c 1−α · n β .
The above statement can be proven by partitioning the set of
1
words of index up to c 1−α · n β into buckets in such a way that
words in a given bucket have roughly the same probability of being
selected in the vocabulary. The bucketing makes it easy to compute
the expected number of words per bucket that end up in the vocabulary. Finally, adding up these expected numbers gives the above
lower bound.

5.4

Fitting

The parameters of the fitting were obtained by minimizing the
Kullback-Leibler divergence D KL (P ||E) of our model’s frequency
given E, we
distribution P from the empirical distribution E. I.e.,

P 
P (i )
searched for the P that minimizes D KL (P ||E) = i P (i) · ln E (i ) .

More precisely, if the empirical distribution E was over n distinct
words then, given a triple of parameters (α, β, γ ), we computed a
candidate distribution P = Pn,α, β,γ by letting, for each i = 1, . . . , n,
−γ
P (i) be proportional to ki with ki equal to the solution of i =
D α,n β (ki ).
We used a brute-force approach to guess the optimal fitting
parameters α, β, γ . The results are reported in Table 2. The empirical
curves, and their fittings, are shown in Figure 5. To show how much
better our curve’s fits are (with respect to the double Pareto fits),
we plot in Figure 6 the ratios between the probabilities given by our
curve and the actual distribution, and those given by double Pareto
and the actual distribution, for the Gutenberg and News corpora.

6 K-GRAM FREQUENCY DISTRIBUTION
Let P (α ) be the power law distribution with exponent α > 1 over
an infinite language U = N. Given an integer k ≥ 1, let P (α,k ) be the
probability distribution on k-tuples hu 1 , . . . , uk i, where u 1 , . . . , uk ∈
U are chosen independently from P (α ) . We now show analytically
that (i) the distribution of P (α,k ) will be close to the original power
law P (α ) and (ii) the curves corresponding to k-grams will become
flatter as k increases, when plotted on a log-log scale; this phenomenon can be observed empirically in Figure 7.
Figure 5: The empirical and the (fitted) theoretical curves of
four corpora.

Theorem 6.1. If we sort the k-tuples decreasingly by their probabilities in P (α,k ) , then the probability of the r th k-tuple will be equal

390

Session 3C: Document Representation and Content Analysis 2

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 7: k-grams frequency distribution for the Europarl
dataset (English).

Figure 6: A log-log plot of the ratios between our fitted curve
(resp., the Double Pareto curve) and the empirical curve, at
word ranks 500 to 50000 (that is, around the knee). The vertical dashed line represents the position of the Double Pareto
knee. Our curve is a very good multiplicative approximation of the empirical curve (the ratios induced by our curve
are quite close to 1), and generally a much better approximation than Double Pareto; the maximum ratios, or distortions
(see Section 5.3), incurred by Double Pareto happen around
the knees: the ratios, there, are close to 2.8 in the Gutenberg
corpus, and to 2.3 in the News corpus.
Figure 8: Computed k-gram distribution, α = 1.5.
to

α

(1 ± or (1)) · ζ (α ) −k · Γ(k ) −α · *
,

lnk −1 r +
.
r -

which follows from [35, 39]. Inverting this, we obtain
n = (1 + or (1))Γ(k )

Proof. Our goal is to compute the position (or rank) r i 1, ...,i k
of the product P (α ) (i 1 ) · · · P (α ) (i k ) in the ordered multiset
(
)
P (α ) (j 1 ) · · · P (α ) (jk ) | j 1 , . . . , jk ∈ Z+ .

.

The proof is concluded by recalling that the probability of the tuple
hi 1 , . . . , i k i is
P (α ) (i 1 ) · · · P (α ) (i k ) = ζ (α ) −k n −α .

In other words, we aim to compute the number of tuples hj 1 , . . . , jk i
such that P (α ) (i 1 ) · · · P (α ) (i k ) < P (α ) (j 1 ) · · · P (α ) (jk ). Rewriting
this condition using the fact p(i) ∝ i −α and letting n = i 1 · · · i k , we
get

r = rn =
hj 1 , . . . , jk i | j 1 · · · jk < n
=

r
lnk −1 r



Observe that our Theorem gives sharp bounds on the probability
of the r th k-gram, as r diverges. Figure 8 shows the k-gram distribution computed with a synthetic power law distribution with
α = 1.5; Figure 9 shows the curves predicted by Theorem 6.1. We
can see that the empirical curves agree asymptotically with the
theoretical estimates.

n lnk −1 n
+ O (n lnk −2 n),
Γ(k )

391

Session 3C: Document Representation and Content Analysis 2

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

[13] Soumen Chakrabarti, Mukul Joshi, and Vivek Tawde. 2001. Enhanced topic
distillation using text, markup tags, and hyperlinks. In SIGIR. 208–216.
[14] Flavio Chierichetti, Ravi Kumar, and Prabhakar Raghavan. 2009. Compressed
web indexes. In WWW. 451–460.
[15] Brian Conrad and Michael Mitzenmacher. 2004. Power laws for monkeys typing
randomly: The case of unequal probabilities. IEEE Transactions on Information
Theory 50, 7 (2004), 1403–1414.
[16] Devdatt Dubhashi and Alessandro Panconesi. 2009. Concentration of Measure for
the Analysis of Randomized Algorithms. Cambridge University Press.
[17] Leo Egghe. 2004. The distribution of N -grams. Scientometrics 47, 2 (2004),
237–252.
[18] Jean-Baptiste Estoup. 1916. Gammes Sténographiques. Institut Stenographique
de France.
[19] Ramon Ferrer i Cancho, Oliver Riordan, and Béla Bollobás. 2005. The consequences of Zipf’s law for syntax and symbolic reference. Proceedings of the Royal
Society B: Biological Sciences 272, 1562 (2005), 561–565.
[20] Ramon Ferrer i Cancho and Ricard V. Sole. 2003. Least effort and the origins of
scaling in human language. PNAS 100, 3 (2003), 788–791.
[21] Xiaocong Gan, Dahui Wang, and Zhangang Han. 2009. N -tuple Zipf analysis and
modeling for language, computer program and DNA. Technical Report 0908.0500v1.
arXiv.
[22] Le Q Ha, P Hanna, D W Stewart, and F J Smith. 2006. Reduced n -gram models
for English and Chinese corpora. In COLING-ACL. 309–315.
[23] Le Quan Ha, E. I. Sicilia-Garcia, Ji Ming, and F. J. Smith. 2002. Extension of Zipf’s
law to words and phrases. In COLING. 1–6.
[24] Thomas Hofmann. 1999. Probabilistic Latent Semantic Analysis. In UAI. 289–296.
[25] Thomas Hofmann. 1999. Probabilistic Latent Semantic Indexing. In SIGIR. 50–57.
[26] Thorsten Joachims. 2001. A statistical learning model of text classification for
support vector machines. In SIGIR. 128–136.
[27] Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. MT summit. (2005).
[28] Andreas Krause and Andreas Zollmann. 2002. Not so randomly typing monkeys–
Rank-frequency behavior of natural and artificial languages. Algorithms for
Information Networks–Project Report. (2002).
[29] Benoit Mandelbrot. 1953. An informational theory of the statistical structure of
language. In Communication Theory, W. Jackson (Ed.). Butterworths, London,
486–502.
[30] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to Information Retrieval. Cambridge University Press, New York.
[31] George A Miller. 1957. Some effects of intermittent silence. The American Journal
of Psychology 70, 2 (1957), 311–314.
[32] Michael Mitzenmacher. 2004. A brief history of generative models for power law
and lognormal distributions. Internet Mathematics 1, 2 (2004), 226–251.
[33] Mark EJ Newman. 2005. Power laws, Pareto distributions and Zipf’s law. Contemporary Physics 46, 5 (2005), 323–351.
[34] Matja Perc. 2012. Evolution of the most common English words and phrases over
the centuries. Journal of The Royal Society Interface 9, 77 (2012), 3323–33238.
[35] Adolf Piltz. 1881. Über das Gesetz, nach welchem die mittlere Darstellbarkeit der
natürlichen Zahlen als Produkte einer gegebenen Anzahl Faktoren mit der Grösse
der Zahlen wächst. Ph.D. Dissertation. University of Berlin.
[36] Gerard Salton and Christopher Buckley. 1988. Term-weighting approaches in
automatic text retrieval. Information Processing & Management 24, 5 (1988),
513–523.
[37] Christer Samuelsson. 1996. Relating Turing’s Formula and Zipf’s Law. In Proceedings of the 4th Workshop on Very Large Corpora. 70–78.
[38] Issei Sato and Hiroshi Nakagawa. 2010. Topic Models with power-law using
Pitman–Yor process. In KDD. 673–682.
[39] Atle Selberg. 1954. Note on a paper by L. G. Sathe. J. Indian Math. Soc., N. Ser. 18
(1954), 83–87.
[40] Herbert S Sichel. 1975. On a distribution law for word frequencies. J. Amer.
Statist. Assoc. 70, 351a (1975), 542–547.
[41] Herbert A Simon. 1955. On a class of skew distribution functions. Biometrika 42,
3/4 (1955), 425–440.
[42] D.C. van Leijenhorst and Th.P. van der Weide. 2005. A formal derivation of
Heaps’ law. Information Sciences 170 (2005), 263–272.
[43] G Udny Yule. 1925. A mathematical theory of evolution, based on the conclusions
of Dr. JC Willis, FRS. Philosophical Transactions of the Royal Society of London.
Series B, Containing Papers of a Biological Character 213 (1925), 21–87.
[44] George K. Zipf. 1932. Selective Studies and the Principle of Relative Frequency in
Language. Harvard University Press.
[45] George K. Zipf. 1935. The Psycho-Biology of Language: An Introduction to Dynamic
Philology. Houghton Mifflin Company.
[46] George K. Zipf. 1949. Human Behavior and the Principle of Least Effort. AddisonWesley Press.

Figure 9: k-grams distribution using Theorem 6.1, α = 1.5.

7

CONCLUSIONS

In this paper we took a closer look at the word frequency distribution. We observed a knee, leading to a concavity, in the empirical
distributions of many different kinds of corpora, and proposed a
natural text generation model to explain the knee and the concavity.
We then analytically showed that our model produces distributions
nearly identical to the empirically observed ones. We also analyzed
the k-gram distribution that one obtains by picking words independently from a power law distribution. We proved that the k-gram
distribution becomes flatter as k increases; this phenomenon had
only been empirically observed in the literature but never analyzed.
Our generative model opens up many interesting questions: can
the distributions it produces be used in applications such as text
compression, translation, and information retrieval?

REFERENCES
[1] IJsbrand Jan Aalbersberg. 1991. Posting compression in dynamic retrieval environments. In SIGIR. 72–81.
[2] IJsbrand Jan Aalbersberg. 1994. A document retrieval model based on term
frequency ranks. In SIGIR. 163–172.
[3] Leif Azzopardi. 2009. Query Side Evaluation: An empirical analysis of effectiveness and effort. In SIGIR. 556–563.
[4] Harald Baayen. 1991. A stochastic process for word frequency distributions. In
ACL. 271–278.
[5] Harald Baayen. 1992. Statistical models for word frequency distributions: A
linguistic evaluation. Computers and the Humanities 26, 5 (1992), 347–363.
[6] L. Douglas Baker and Andrew Kachites McCallum. 1998. Distributional clustering
of words for text classification. In SIGIR. 96–103.
[7] Krishna Bharat and Monika R. Henzinger. 1998. Improved algorithms for topic
distillation in a hyperlinked environment. In SIGIR. 104–111.
[8] David C Blair. 1990. Language and Representation in Information Retrieval. Elsevier
Science Publishers.
[9] David C. Blair. 2002. The challenge of commercial document retrieval, Part I:
Major issues, and a framework based on search exhaustivity, determinacy of representation and document collection size. Information Processing & Management
38, 2 (2002), 273–291.
[10] Andrew D Booth. 1967. A “Law” of occurrences for words of low frequency.
Information and Control 10, 4 (1967), 386–393.
[11] Nieves R. Brisaboa, Antonio Fariña, Susana Ladra, and Gonzalo Navarro. 2008.
Reorganizing compressed text. In SIGIR. 139–146.
[12] Nieves R. Brisaboa, Antonio Fariña, Gonzalo Navarro, and José R. Paramá. 2007.
Lightweight Natural Language Text Compression. Information Retrieval 10, 1
(2007), 1–33.

392

Session 3C: Document Representation and Content Analysis 2

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

x’s, it holds e −x ≤ 1 − x2 — equivalently, 1 − e −x ≥ x2 . Therefore,

APPENDIX
Proof of Lemma 5.1

1 − e −(1−α )n
we have

First we upper bound ζ N (α ) with one (the first term of its sum)
plus the area under x −α in the interval [1, N ],
Z N
N 1−α − α
ζ N (α ) ≤ 1 +
x −α dx =
.
1−α
1
Analogously, a lower bound is given by the area under (x
in the interval [0, N ],
Z N
N 1−α − 1
ζ N (α ) ≥
(x + 1) −α dx ≥
.
1−α
0

Proof of Lemma 5.2

For each integer k ≥ 1 and for each non-decreasing and nonnegative function f (x ) admitting a finite integral in [0, k + 1], we
have
Z k
Z k +1
k
X
FL =
f (x )dx ≤
f (i) ≤
f (x )dx = FU .
0

k

(α )

If i < o(n β ), then observe that, by R(i) = 1 − (1 − P N (i)) n and



(α )
P N (i) = 1 − O n α β −1 i α 1−α
, we have
n 1−α β
(1)

FL =

(α )

R(i) = 1 − e −(1−α )n

x =ϵ

(2)

Now, consider the first o(n β ) terms of the LHS sum in (2). The
exponent of e in each of them is ω (1), and therefore each of them
has value 1 − o(1). It follows that the LHS of (2) has value ω (1).
Since 0 ≤ ξ ≤ 1, we have
!
k
X
nα β
(1 ± o(1))
1 − e −(1−α ) i α = D (k ).



≤ 1, we have

α β i −α

=

i=1

α β −α
nα β
= *1 ± O * α n α β −1 ++ e −(1−α )n i .
i
,
,
-α β i −α

−ξ =

α

It follows that

Moreover, since 0 ≤ e −(1−α )n

nα β
iα

(1 − α ) 1/α * 1
nβ
nβ
Γ − , (1 − α ) · * + + ,
α
α
,
,k - since by definition limx →∞ Γ(a, x ) = 0. Thus, we have
!
k
X
nα β
1 − e −(1−α ) i α = D (k ) − ξ .

the error term is o(1) for each i in our range.

2



 αβ

(α )
(α )
(α )
(2) 2n P N (i) = Θ nP N (i) P N (i) = O ni α n α β −1 ,


(α )
since P N (i) = O n α β −1 for each i ≥ 1.
1 − R(i) = e

e −(1−α )

1
 
k
α β /α
α β 
 (1 − α )n
1
(1
−
α
)n
+
lim 
Γ *− ,
α
xα
ϵ →0+ 
, α
-



Recall that 1 − R(i) = (1 − P N (i)) n . Then, we bound the following
quantities, using (5.2):
 αβ

(α )
(1) nP N (i) = (1 − α )n α β i −α + O ni α n α β −1 . Observe that

n α β −1

k
X

nα β
iα

i=1

2
e −ab ≥ (1 − a)b ≥ e −ab−2a b .

nα β
iα

0

where c is a constant. By choosing q = (1−α )n α β , we get e −(1−α )
f (i). Now,

The claim is thus proved.
Next assume that i is a positive integer such that i > ω (n2β −1/α ).
Observe that, by α, β < 1, this case includes all the i’s that are not
part of the previous case.
For 0 < a < 21 , and b > 0, it holds that



1

P
and hence ki=1 f (i) = F L + ξ for some ξ ∈ [0, 1].
Observe that for all q > 0 and α ∈ (0, 1), the function f (x ) =
−α
e −qx satisfies the above conditions. We also have
Z
 1

1 1
f (x )dx = q /α Γ − , qx −α + c,
α
α

The right-hand expression in our claim simplifies to:


(1 ± o(1)) 1 − e −ω (1) = 1 ± o(1).

−(1−α )n α β i −α ±O

i=1

Suppose that 0 ≤ f (x ) ≤ 1. Then,
Z k +1
Z 1
FU − F L =
f (x )dx −
f (x )dx ≤ 1,



R(i) = 1 − o(1).



Proof of Lemma 5.3

+ 1) −α

(α )

1−α
1−α
(α )
≤ P N (i) ≤ i −α 1−α
.
N 1−α − α
N
−1

≥ 12 (1 − α )n α β i −α . Therefore, even for i ≥ n β ,



α β −α
R(i) = (1 ± o(1)) 1 − e −(1−α )n i
.

If α < 1, using the expression for P N (i), we get
i −α

α β i −α

nα β
± O * α nα β −1 +
, i
-

i=1

The claim then follows from Lemma 5.2 and the linearity of expectation. 

−(1−α )n α β i −α

Observe that, if i ≤ n β, then we have
1−e
≥ 1−



αβ
e −1+α = Θ(1), while O ni α n α β −1 ≤ O n α β −1 . That is, R(i) =


α β −α
(1 ± o(1)) 1 − e −(1−α )n i
.

Proof of Lemma 5.4
The first part is implied directly by
p(1) and
 Lemma 5.3. Hence, let
 
k = ω n β . Let us define д =
n β k to be the ceiling of the
 
geometric mean of n β and k. Observe that ω n β < д < o(k ).

On the other hand, if i ≥ n β , then if we let x be the exponent
of the exponential term, we have 0 ≤ x ≤ 1. For this range of

393

=

Session 3C: Document Representation and Content Analysis 2

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Since R(i) ≤ 1, we have
k
X

Proof of Lemma 5.7

R(i) ≤ E[|V ∩ Uk |] ≤ д +

i=д+1

k
X

Suppose that i = x · n β . Then,

R(i).


−α 
R(i) = (1 ± o(1)) 1 − e −(1−α )x
.

i=д+1



1
Observe that, if x ≥ α, then x −α ≤ α −α = e α ln α = 1 + O α ln α1 .
Moreover, if x ≤ e then, x −α ≥ e −α = 1 − O (α ).
−α we thus obtain that x −α = 1 ±
By themonotonicity of x
1
O α ln α for all x ∈ [α, e].

By Lemma 5.2, we have that R(i) = (1 ± o(1)) · n α β 1−α
i α whenever
 
i > д, since д > ω n β . Then, we can write:
k
X

R(i) = (1 ± o(1))(1 − α )nα β

i=д+1

k
X

i −α

For all αn β ≤ i ≤ en β , we then have

i=д+1



R(i) = (1 ± O (α ln 1/α )) · 1 − e −1 .
P α
For i < αn β , we have R(i) ≤ 1. Therefore, for ki=1
R(i) to be at
β
least n , we need
kα
1
≥
− O (α ln 1/α ).
1 − e −1
nβ
Moreover, for the inequality to hold, it suffices to have
kα
1
≤
+ O (α ln 1/α ) . 
1 − e −1
nβ



= (1 ± o(1))(1 − α )nα β ζk (α ) − ζд (α )
= (1 ± o(1))nα β k 1−α ,
where
  from Lemma 5.1.
 The value of the sum
 the last step follows
is Θ nα β k 1−α = ω n β , since k > ω n β . Therefore,
E[|V ∩ Uk |] = (1 ± o(1)) · nα β k 1−α .
Lemma 5.3 completes the proof.



Proof of Lemma 5.5
Observe that, by Lemma 5.3, it is sufficient to prove that, with
probability 1 −o(1), it will happen that, for each k ∈ [N ], |V ∩Uk | =
(1 ± o(1))E [|V ∩ Uk |].
1
Let us define X = n β log− α n. We will use two arguments for
proving the claim: one that holds if k < o(X ), and one that holds if


β
k > ω X 1−ϵ , for any constant 0 < ϵ < 4 .
  β α 
(α )
First, consider k < o(X ). Recall that we have P N (i) = Θ n1 ni
.


log n
(α )
Therefore, for i < o(X ), we have P N (i) > ω n . For the same
i’s, therefore, we have

log n
R(i) ≥ 1 − 1 − ω
n

Proof of Lemma 5.8
P
Let us define ϵ = 1 − α. Recall that E[|V ∩ Uk |] = ki=1 R(i). Let
1 = t 0 < t 1 < . . . < tr = k be integers and, for 0 ≤ j ≤ r − 1, let p j
be any real number such that p j ≥ R(t j ). Then, by the monotonicity
of the R(i)’s, we have p j ≥ R(i) for 1 ≤ i ≤ t j . Therefore,
E[|V ∩ Uk |] =

≤

tj
r X
X
j=1 i=t j−1
r 
X

R(i) ≤

tj
r X
X

R(t j−1 )

j=1 i=t j−1


t j · p j−1 .

j=1

! !n
≥ 1−n

−ω (1)

.

 j−2

We set t 0 = 1 and, for j ≥ 1, let t j = 2 α · n β . We let r be
unspecified for now. Also, let p0 = 1, and, for j ≥ 1,


α β −α
2−j
p j = 1 − e −(1−α )n t j = 1 − e −ϵ 2 ≤ O ϵ2−j .

By the union bound, the probability that at least one of the terms
of rank i < o(X ) in U does not end up in V is n−ω (1) . The claim is
then proved for each k < o(X ).
Now consider k > ω X 1−ϵ . Let Yi, j be the indicator random
variable of the event “the jth term of the founding text happened
to be the ith term of the language”. Then, for each j, the variables
Y1, j , Y2, j , . . . , YN , j are negatively associated (see Chapter 3 of [16]).
Moreover, by closure under product, the variables Yi, j , for each
i ∈ [N ], j ∈ [n] are as a whole negatively associated. Finally,
since max is a monotone non-decreasing function the variables
Yi = maxj=1, ...,n Yi, j , i ∈ [N ], are also negatively associated —
the Chernoff bound can then be applied to their sum, that is, to
Pk
i=1 Yi = |V ∩ Uk |. Thus, for each 0 < δ < 1,

1 − 1 ≤ O (ϵ ). For j = 1, we have
We have that α1 − 1 = 1−ϵ
 β
t j p j−1 = t 1 ≤ n2 . Moreover, for j ≥ 2,


1
t j · p j−1 ≤ O ϵn β 2j ( α −1) = ϵn β 2O (jϵ ) .

As j ≤ O (1/ϵ ), the latter is at most O (ϵn β ). In fact, there exists a
constant b > 0 such that if we let r = db/ϵ e, we have
 β  X
r

 3
n
E[|V ∩ Uk |] ≤   +
O ϵn β < · n β .
4
 2  j=2
Lemma 5.3 shows that D (k ) = (1 ± o(1))E[|V ∩ Uk |]. With our
choice of r , k equals
 r −2

1
k = tr = 2 α · n β ≥ c /ϵ · n β ,

Pr [ V ∩ Uk − E [ V ∩ Uk ] ≥ δE [ V ∩ Uk ]]
δ2

≤2e − 3 E[|V ∩Uk |] .




Since k > ω X 1−ϵ , we have E[|V ∩ Uk |] > ω n β −2ϵ . If we
1
choose δ = n− 2 β +2ϵ , we get:

for some constant
 c > 1. 
Therefore, D c 1/ϵ · n β ≤ (1 ± o(1)) 34 n β . By the latter, and by
the monotonicity in k of E[|V ∩ Uk |], we have that k α ≥ c 1/ϵ · n β .


Pr [ V ∩ Uk = (1 ± 2δ ) · E [ V ∩ Uk ]] ≤ e −Ω(n ) .


By the union bound, the claim is then proved for each k > ω X 1−ϵ .
.
2ϵ

394

