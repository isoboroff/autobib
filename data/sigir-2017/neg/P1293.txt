Demonstration Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Teaching the Information Retrieval Process Using a Web-Based
Environment and Game Mechanics
Thomas Wilhelm-Stein

Chemnitz University of Technology
09107, Chemnitz, Germany
thomas.wilhelm-stein@cs.
tu-chemnitz.de

Stefan Kahl

Chemnitz University of Technology
09107, Chemnitz, Germany
stefan.kahl@cs.tu-chemnitz.de

case. She also knows which components work well if combined
and which will not. An unacquainted learner needs to acquire this
knowledge through lots of trials and errors.
The use of existing information retrieval systems as a starting
point enables learners to quickly set up their systems and conduct
experiments. For this approach, an easy access to these systems
and good documentation are essential. Special courses at universities allow students to get in touch with these systems and also
provide first insights into the underlying components and their
interdependencies.
We present a web-based application, which allows learners to
conduct experiments without the need to set up a retrieval system
beforehand and without the need to deal with the specifics of an
evaluation.

ABSTRACT
Predicting the performance of individual components of information retrieval systems, in particular the complex interactions between those components, is still challenging. Therefore, professionals are needed for the implementation and configuration of
retrieval systems and retrieval components. Our web-based application, called Xtrieval Web Lab, enables newcomers and learners to
gain practical knowledge about the information retrieval process.
They can arrange a multitude of components of retrieval systems
and evaluate them with real world data without utilizing a programming language. Game mechanics guide the learners in their
discovery process and motivate them.

CCS CONCEPTS
•Information systems →Search engine architectures and scalability; Retrieval models and ranking; Evaluation of retrieval results;
•Applied computing →Interactive learning environments;

KEYWORDS
evaluation; information retrieval; software-based teaching environment; game mechanics

1

Maximilian Eibl

Chemnitz University of Technology
09107, Chemnitz, Germany
maximilian.eibl@cs.tu-chemnitz.de

MOTIVATION

Information retrieval systems permeate a multitude of aspects of
our digital life. They are used as an entry point to the Internet or
help us organize emails and documents. Beyond that, they are also
important in other highly specialized contexts, like patent research
or preservation of cultural heritage.
All these use cases require information retrieval systems which
are adapted to those tasks and specialized in their behavior. There
is an eminent need for trained professionals, who can perform this
kind of adaptations in an efficient way.
An information retrieval system can be represented as a pipeline
of diverse components for indexing and searching. Such components can be stemmers, retrieval algorithms, or blind relevance
feedback. Based on her practical knowledge, an experienced researcher can decide which components fit well given a special use

2

EXISTING SYSTEMS

Although some systems, like SMART [20] or Terrier [18], have
emerged from research projects at universities, a self-evident use
for teaching is rarely the case or undocumented. In contrast, the
popular and open-sourced framework Apache Lucene is often used
in practically oriented courses [19]. Lemur, which has its origins in
research as well, and its successor Galago are also used for teaching
[2, 11].
Courses based on open-source libraries are primarily beneficial
for students of computer sciences, as they are more likely to have
some knowledge in a programming language and its basic concepts. Without prior knowledge in programming, the information
retrieval concepts tend to lose priority and students are forced to
learn programming languages rather than the actual concepts and
techniques of information retrieval. [8, 16, 17]
As a result, the technical aspects of information retrieval systems
are inadvertently at the center of those courses.

2.1

Query-based Systems

Soekia by Jurjević, Stöcklin and Hartmann [12] has been used in
various classes from secondary school to college. Tests showed
that Soekia improves the understanding of how search engines
work. It presents the information retrieval process on a small scale.
Therefore, the learning environment is suitable only for small data
collections and the variety of possible settings are limited. Additionally, tools for the evaluation of retrieval results are missing entirely.
The search queries can only be entered individually and manually.
The results are presented in the form of plain HTML pages.
INSTRUCT (Interactive System for Teaching Retrieval Using
Computational Techniques) was presented by Hendry, Willett and
Wood [9, 10] as early as 1986. Its primary goal was to acquaint

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.
978-1-4503-5022-8/17/08. . . $15.00
DOI: 10.1145/3077136.3084143

1293

Demonstration Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

SulaIR presented by Fernández-Lune, Huete, Rodrı́guez-Cano
and Rodrı́guez [6] is a Java desktop application, which much like IR
Toolbox, represents the basic steps of the information retrieval process. Visualizations support the understanding of each step of the
retrieval process. According to Fernández-Lune et al. [6] the system
was tested in an information retrieval course at the University of
Holguı́n in Cuba and the results have been positive. However, the
nature of the test and the actual results are not available. Although
the project is labeled open-source, there is no source code publicly
available and the project does seem to be discontinued.
The web-application VIRLab by Fang, Wu, Yang and Zhai [5] is
specialized on experiments with IR models. It allows learners to
create their own IR models and to compare them with each other.
The IR models are created in a C-based language, which allows
the access to document and term statistics. The learners can test
their retrieval models with their own queries or predefined test
collections consisting of queries and relevance assessments. For the
comparison with other models, metrics and charts are available on
experiment and query level. The focus on retrieval models reduces
the complexity of such a system by a great margin. Thus, it is
easier for learners to get started with the system. In addition, all
experiments are performed server-side. This greatly reduces the
learners’ effort to set up their own IR environment. For each test
collection, a ranking of the ten best retrieval models is available.
López-Garcia and Cacheda [15] presented IR-Components, which
is a Java-based framework for developing information retrieval applications. IR-Components contains basic components for each
step of the IR process, which can be used for simple experiments.
Learners are encouraged to develop their own components in order
to improve the results of the base system. To conduct their first
experiments, learners need to know Java as well as the important
interfaces and classes of IR-Components. The user interface for
search and distributed execution environment are already included.
IR-BASE by Calado, Cardoso-Cachopo and Oliveira [1] has been
built to support research and teaching. Much like IR-Components,
it provides a framework for the implementation of new information
retrieval components. A guideline and a documentation ensure
compatibility between those components. At first, students create
a pool of basic components, which serve as the baseline for more
sophisticated components. With the next step, students extend
the functionality of the whole system by creating more components. Unfortunately, there is no built-in option to evaluate new
components and measure their performance. Therefore, implementation errors might remain undetected and can compromise the
performance of the entire system.

Figure 1: Overview of existing teaching software for IR and
their capabilities

library and information science students with new developments
regarding query formulation in the field of information retrieval
systems. The application employed a character-based, menu-driven
user interface. The users could choose available features and options from various menus, which were often described in detail. In
addition to the Boolean retrieval, which was very common in this
era, queries could be formulated in natural language.
IR Game by Sormunen et al. [22], also known as Query Performance Analyser (QPA), is a web application to evaluate the impact
of different formulations of search queries, especially in a multilingual context. The IR Game was used for both: research and teaching
[21]. The in-depth analysis of the queries and the promotion of an
exploratory approach supports the learners in developing conceptual and mental models of the search engine. An important aspect
of all tests are instructional exercises that help students handling
the IR Game, as the system itself is not self-explanatory.
The IR Game and INSTRUCT focus on the construction of highquality search queries. IR components or algorithms are underrepresented. Although a few characteristics of the index can be
altered, the students do not have direct control over the underlying
IR mechanics.

2.2

Component-based Systems

IR-Toolbox by Efthimiadis and Freier [4] is a web-based learning
tool with the goal of visualizing the IR process to students. Through
practical experimentation without the use of a programming language, students can develop a richer conceptual model and thus
a better understanding of the IR process and the underlying algorithms. Using the IR Toolbox, students are enabled to configure
specific, predetermined components of the IR process. However,
the components can not be arranged freely by learners, resulting
in a very restricted scope. The user interface is very minimalistic,
which limits the comprehensibility for new users. However, technically savvy students are able to look up specific implementation
details. [3]

3

OUR SYSTEM

The Xtrieval Web Lab1 is a web-application, which builds upon
our Xtrieval framework [14]. It is designed to simplify the experimentation and evaluation process in a way that it can be adapted
and executed without prior knowledge in programming languages.
Through the web interface, experiments can be composed and
parametrized using pre-fabricated components. A live preview
of the processed data assists the students during the process of
fine-tuning the components. After an experiment is composed by
1 https://medien.informatik.tu-chemnitz.de/weblab/

1294

Demonstration Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 2: Screenshot of the user interface with assignment,
components pipeline, and preview

Figure 3: Screenshot of the user interface displaying the results as chart and table

the users, the configuration of this very experiment is submitted
to the server which handles the main processing load. Finally, the
results are returned to the web-interface. The students can explore
these results and gain insight using various visualizations. Furthermore, they can continue to change the parameters of the composed
retrieval workflow to influence the results and observe the effects
of individual changes to the components.
While designing our retrieval web-application we focused on
the following design goals:
• Experiments are compiled of components,
• Components can be parametrized,
• A preview, based on a sampled document of the actual
collection, visualizes how components manipulate data,
• Experiments are processed on a web-server, and
• Detailed results are displayed to allow the user to explore
and optimize.
In order to maximize flexibility for components and parameters
and the responsiveness of the entire system, experiments need to
be run individually for every user in (near) real-time. The students
should not be restricted by limitations of pre-designed workflows.
For instance, stop words can be specified by the students without
relying on any predefined list, components can be deactivated or
re-ordered and individual parameter sets may result in hundreds of
different result lists. Additionally, experiments need to be processed
very fast. Users of web applications are not willing to wait as long
as researchers would in their labs; short feedback loops are vital.
We optimized our system regarding execution speed. In the IAPR
TC-12 Benchmark [7], which was used for ImageCLEF from 2006 to
2008, our system is capable of rendering a preview after an average
of only 400 milliseconds. A complete run of an entire experiment,
including the creation of an index, the search and the calculation
of evaluation metrics, can be performed within 5 to 20 seconds.
This performance varies depending on the size of the collection,
the number of components used and the hardware configuration of
the web-server. Overall, our web-application is one of the fastest
retrieval systems suitable for consumer hardware.
Although the back-end of our system is implemented in Java due
to the use of Apache Lucene and Terrier, the actual components
are written in JavaScript which enables students to design their

own components without the restrictions of a static desktop UI.
Components run in a sandbox which allows efficient access to the
collections and the search engine via the Oracle Nashorn JavsScript
Engine2 . Parallel IO-threads process collections efficiently and
utilize threading capabilities of Lucene to a great extend. This
results in a major speed-up compared to other retrieval systems.
We use Game mechanics to achieve two main objectives: First,
by using a level system in which a user can ascend after reaching
certain milestones, the initial complexity of the system at start is
kept low and intricacy increases according to the user’s experience.
The complexity of the retrieval system crafted by the students
increases until all components have been unlocked, only skilled
users will be presented all available options. Secondly, students
should be motivated to use and understand as many components
as possible even beyond a potential increase in knowledge. In our
conducted studies regarding the motivation for extended learning of
retrieval components and processes, students who were presented
with achievements and badges for their learning progress showed
great desire to reach even higher scores and better results despite
the lack of further assignments. This proves that gamification of
the teaching process for retrieval systems can be very effective to
increase the students ability to understand the underlying concepts.
Besides, by using assignments, learners are familiarized with
the user interface and are guided in learning about the available
parameters and their influence on the retrieval results. A mode for
unrestricted experimentation allows the use of all available components and serves as playground for free exploration. Achievements
do not only inform about certain milestones, but also grant access
to further components and motivate continued experimentation.
We incorporated leaderboards so users can compare their results
with each other and are able to deduct which strategies might be
best for the completion of certain tasks.
We designed the Xtrieval Web Lab with non-computer science
students in mind as it does not require any knowledge about programming languages. It is aimed at listeners of lectures on the topic
of information retrieval and benefits the learning experience when
used in exercises and seminars.
2 http://www.oracle.com/technetwork/articles/java/jf14-nashorn-2126515.html

1295

Demonstration Paper

4

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

SUMMARY

[8] David G. Hendry. 2007. History Places: A Case Study for Relational Database
and Information Retrieval System Design. J. Educ. Resour. Comput. 7, 1, Article 3
(March 2007). DOI:http://dx.doi.org/10.1145/1227846.1227849
[9] Ian G Hendry, Peter Willett, and Frances E. Wood. 1986. INSTRUCT: a teaching
package for experimental methods in information retrieval. Part I. The users
view. Program 20, 3 (1986), 245–263. DOI:http://dx.doi.org/10.1108/eb046940
arXiv:http://dx.doi.org/10.1108/eb046940
[10] Ian G Hendry, Peter Willett, and Frances E. Wood. 1986. INSTRUCT: a teaching
package for experimental methods in information retrieval. Part II. Computational aspects. Program 20, 4 (1986), 382–393. DOI:http://dx.doi.org/10.1108/
eb046949 arXiv:http://dx.doi.org/10.1108/eb046949
[11] Seikyung Jung and Joseph Lawrance. 2011. Web Information Retrieval and
Filtering Course to Undergraduates Using Open Source Programming. ACM
Inroads 2, 3 (Aug. 2011), 47–50. DOI:http://dx.doi.org/10.1145/2003616.2003634
[12] Diana Jurjević, Nando Stcklin, and Werner Hartmann. 2009. Informationskompetenz: ein Thema für den Informatikunterricht. Lecture Notes in Informatics
(2009).
[13] Jens Kürsten. 2012. A Generic Approach to Component-Level Evaluation in
Information Retrieval. (2012).
[14] Jens Kürsten, Thomas Wilhelm, and Maximilian Eibl. 2008. Extensible retrieval
and evaluation framework: Xtrieval. In LWA 2008: Lernen - Wissen - Adaption,
Wuttrzburg, October 2008, Workshop Proceedings.
[15] Rafael López-Garcı́a and Fidel Cacheda. 2011. A Technical Approach to Information
Retrieval Pedagogy. Springer Berlin Heidelberg, Berlin, Heidelberg, 89–105. DOI:
http://dx.doi.org/10.1007/978-3-642-22511-6 7
[16] Frank McCown. 2010. Teaching Web Information Retrieval to Undergraduates. In
Proceedings of the 41st ACM Technical Symposium on Computer Science Education
(SIGCSE ’10). ACM, New York, NY, USA, 87–91. DOI:http://dx.doi.org/10.1145/
1734263.1734294
[17] Masnizah Mohd. 2011. Development of Search Engines using Lucene: An Experience. Procedia - Social and Behavioral Sciences 18 (2011), 282 – 286. DOI:
http://dx.doi.org/10.1016/j.sbspro.2011.05.040
[18] Iadh Ounis, Gianni Amati, Vassilis Plachouras, Ben He, Craig Macdonald, and
Christina. 2006. Terrier: A High Performance and Scalable Information Retrieval
Platform. In In Proceedings of ACM SIGIR’06 Workshop on Open Source Information
Retrieval (OSIR 2006).
[19] Ian Ruthven, David Elsweiler, and Emma Nicol. 2008. Designing for users:
an holistic approach to teaching information retrieval. In Second International
Workshop on Teaching and Learning of Information Retrieval (TLIR 2008).
[20] Gerard Salton. 1991. The Smart Document Retrieval Project. In Proceedings of the
14th Annual International ACM SIGIR Conference on Research and Development
in Information Retrieval (SIGIR ’91). ACM, New York, NY, USA, 356–358. DOI:
http://dx.doi.org/10.1145/122860.122897
[21] Eero Sormunen, Sakari Hokkanen, Petteri Kangaslampi, Petri Pyy, and Bemmu
Sepponen. 2002. Query Performance Analyser -: A Web-based Tool for IR
Research and Instruction. In Proceedings of the 25th Annual International ACM
SIGIR Conference on Research and Development in Information Retrieval (SIGIR
’02). ACM, New York, NY, USA, 450–450. DOI:http://dx.doi.org/10.1145/564376.
564491
[22] Eero Sormunen, Juha Laaksonen, Heikki Keskustalo, Jaana Keklinen, Harri
Laitinen, Ari Pirkola, and Kalervo Jrvelin. 1998. IR Game : a tool for rapid query
analysis in cross-language IR experiments. In Singapore: Kent Ridge Digital Labs.
22–32.

The Xtrieval Web Lab is a learning environment that enables learners to perform real IR experiments on the basis of individually
configurable components. Relying on the Xtrieval Framework,
which has already proven its performance and flexibility in several
evaluation campaigns, it benefits from established software components. Although both systems use similar tools, the Xtrieval Web
Lab represents a more qualitative approach. It was designed for
manual experimentation, which is supervised and controlled by
a tutor. The Xtrieval Framework, on the other hand, represents a
more quantitative approach since it was developed primarily for
the execution of unsupervised experiments with many different
configurations [13].
You can access the Xtrieval Web Lab for free by visiting the following web-site: https://medien.informatik.tu-chemnitz.de/weblab/

REFERENCES
[1] Pável Calado, Ana Cardoso-Cachopo, and Arlindo L. Oliveira. 2007. IR-BASE: An
Integrated Framework for the Research and Teaching of Information Retrieval
Technologies. In Proceedings of the First International Conference on Teaching and
Learning of Information Retrieval (TLIR’07). BCS Learning & Development Ltd.,
Swindon, UK, 2–2. http://dl.acm.org/citation.cfm?id=2228236.2228238
[2] W Bruce Croft, Donald Metzler, and Trevor Strohmann. 2010. Search engines.
Pearson Education.
[3] Efthimis N. Efthimiadis, Jamie Callan, and Ray R. Larson. 2007. Approaches to
teaching & learning information retrieval. Proceedings of the American Society
for Information Science and Technology 44, 1 (2007), 1–3. DOI:http://dx.doi.org/
10.1002/meet.1450440136
[4] Efthimis N. Efthimiadis and Nathan G. Freier. 2007. IR-Toolbox: An Experiential
Learning Tool for Teaching IR. In Proceedings of the 30th Annual International
ACM SIGIR Conference on Research and Development in Information Retrieval
(SIGIR ’07). ACM, New York, NY, USA, 914–914. DOI:http://dx.doi.org/10.1145/
1277741.1277982
[5] Hui Fang, Hao Wu, Peilin Yang, and ChengXiang Zhai. 2014. VIRLab: A Webbased Virtual Lab for Learning and Studying Information Retrieval Models. In
Proceedings of the 37th International ACM SIGIR Conference on Research &#38;
Development in Information Retrieval (SIGIR ’14). ACM, New York, NY, USA,
1249–1250. DOI:http://dx.doi.org/10.1145/2600428.2611178
[6] JM Fernández-Luna, JF Huete, JC Rodrı́guez-Cano, and MC Rodrı́guez. 2012.
Teaching and learning information retrieval based on a visual and interactive
tool: Sulair. In EDULEARN12 Proceedings. IATED, 6634–6642.
[7] Michael Grubinger, Paul Clough, Henning Müller, and Thomas Deselaers. The
IAPR TC-12 Benchmark: A New Evaluation Resource for Visual Information
Systems. In OntoImage 2006 Workshop on Language Resources for Content-based
Image Retrieval during LREC 2006 Final Programme.

1296

