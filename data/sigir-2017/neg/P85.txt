Session 1C: Document Representation and Content Analysis 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

ICE: Item Concept Embedding via Textual Information
Chuan-Ju Wang

Research Center for Information
Technology Innovation, Academia Sinica
cjwang@citi.sinica.edu.tw

Ting-Hsiang Wang

Research Center for Information
Technology Innovation, Academia Sinica
thwang@citi.sinica.edu.tw

Ming-Feng Tsai

Bo-Sin Chang

Department of Computer Science,
National Chengchi University
mftsai@nccu.edu.tw

Department of Computer Science,
National Chengchi University
105753001@nccu.edu.tw

needs from a collection of information resources are to be obtained,
such as item recommendation and entity retrieval. In traditional
methods, each item is usually assumed to be independent of other
items and is represented using a “one-hot” or “bag-of-objects” encoding format, for instance by representing a movie in terms of
its genre or a song in terms of its listeners; this typically leads to
difficulty with data sparsity, resulting in relatedness among items
being simply ignored.
In recent years, distributed representations have been proposed
to address the data sparsity problem in several fields, such as language modeling [1, 6–9] and information networks [2, 4, 10, 11,
14, 15]. The purpose of these approaches is to keep similar items
(e.g., words, documents, or nodes) close to each other by embedding items as low-dimensional vectors. In [8], a skip-gram word
embedding model is proposed to embed the target word to predict
the embedding of each individual context word in a local window.
Moreover, in [6] paragraph vectors are proposed to embed arbitrary pieces of text, e.g., sentences and documents. The fundamental
idea behind these embedding approaches comes from the distributional hypothesis: “You shall know a word by the company it
keeps” (Firth, J. R. 1957:11). In this quotation, Firth draws attention
to the context-dependent nature of meaning with his notion of
‘context of situation.’ DeepWalk [10] further generalizes context
dependency from sequences of words to graphs, and uses local
information obtained from truncated random walks to learn latent
representations of nodes in an information network. In addition,
LINE [14] proposes an efficient edge-sampling method with carefully designed objective functions to learn latent representations
in a large-scale information network. In comparison to classical
approaches such as nearest neighbors that also utilize the distributional similarity of context, these embedding approaches have
been proved to be effective and efficient, and scale to millions of
nodes on a single machine.
Following the fruitful progress of these embedding approaches
for information network modeling, several advanced embedding
studies for more complex graphs or for specific applications have
recently been proposed [3, 5, 13, 16, 17]. For example, HPE (heterogeneous preference embedding) [3] uses heterogeneous graphs
(including users, tracks, albums, and artists) and encodes user preferences into low-dimensional vector spaces for music recommendations. Also following progress in LINE, which is mainly designed
for homogeneous networks, the authors extend their previous work
of unsupervised information network embedding and propose a

ABSTRACT
This paper proposes an item concept embedding (ICE) framework
to model item concepts via textual information. Specifically, in
the proposed framework there are two stages: graph construction
and embedding learning. In the first stage, we propose a generalized network construction method to build a network involving
heterogeneous nodes and a mixture of both homogeneous and heterogeneous relations. The second stage leverages the concept of
neighborhood proximity to learn the embeddings of both items and
words. With the proposed carefully designed ICE networks, the
resulting embedding facilitates both homogeneous and heterogeneous retrieval, including item-to-item and word-to-item retrieval.
Moreover, as a distributed embedding approach, the proposed ICE
approach not only generates related retrieval results but also delivers more diverse results than traditional keyword-matching-based
approaches. As our experiments on two real-world datasets show,
ICE encodes useful textual information and thus outperforms traditional methods in various item classification and retrieval tasks.

KEYWORDS
conceptual retrieval; concept embedding; textual information; information network
ACM Reference format:
Chuan-Ju Wang, Ting-Hsiang Wang, Hsiu-Wei Yang, Bo-Sin Chang, and MingFeng Tsai. 2017. ICE: Item Concept Embedding via Textual Information.
In Proceedings of SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan, ,
10 pages.
DOI: http://dx.doi.org/10.1145/3077136.3080807

1

Hsiu-Wei Yang

Research Center for Information
Technology Innovation, Academia Sinica
leoyang@citi.sinica.edu.tw

INTRODUCTION

Learning meaningful and valuable representations of items is important for various applications where items that relate to information
∗ The

second author (Ting-Hsiang Wang) and the third author (Hsiu-Wei Yang) contributed equally.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.
978-1-4503-5022-8/17/08. . . $15.00
DOI: http://dx.doi.org/10.1145/3077136.3080807

85

Session 1C: Document Representation and Content Analysis 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

semi-supervised predictive text embedding approach via heterogeneous text networks [13]. In these approaches, the learned vectors
are used to make meaningful similarity comparisons only between
homogeneous nodes and are thus appropriate for tasks involving
only one type of entity, such as document classification and itemto-item retrieval. Despite that, in practice, various scenarios call
for similarity comparisons between heterogeneous entities, e.g.,
using words to discover related songs for music recommendation.
However, due to the essence of the complex graph structures designed in previous studies, comparison of heterogeneous nodes
(e.g., heterogeneous search) is a difficult and complicated problem.
In this paper, we propose an item concept embedding (ICE) approach to model concepts of items via their associated textual information. We incorporate textual information because text is a
coherent and readable set of symbols that transmits information,
in which the learned low-dimension representations can easily be
used to meet an information need in certain scenarios. In particular, there are two stages in our proposed approach: graph construction and embedding learning. In the first stage, a generalized
network construction method is proposed by which we assemble a
graph involving heterogeneous nodes (i.e., items and words) and a
mixture of both homogeneous and heterogeneous relations. Here
we leverage conceptual similarity between words and integrate
these relations into the graph to better capture concepts behind
both items and words. In the second stage, we leverage information about each node’s neighbors to learn the embeddings of texts
and items. The resulting embeddings are effective for both homogeneous and heterogeneous retrieval, including item-to-item
and word-to-item retrieval. Moreover, as a distributed embedding
approach, the proposed ICE approach provides not only related retrieval results but also delivers more diverse results than traditional
keyword-matching-based approaches.
We conduct extensive experiments on two real-world datasets,
including one movie dataset and one commercial music dataset.
Experimental results show that the ICE network encodes useful
textual information, outperforming traditional methods in various item classification and retrieval tasks. Most importantly, the
proposed ICE approach is effectual for the tasks involving heterogeneous objects, such as word-to-item and item-to-word search. In
summary, we make three major contributions in this paper:

graph construction, and embedding model learning. The results of
empirical experiments are provided and discussed in Section 3.3.
Section 4 concludes the paper.

2

METHODOLOGY

In Section 2.1, we define and formulate the item concept embedding (ICE) problem. We then describe in Section 2.2 how to
construct the ICE network, and in Section 2.3 we show how to
learn the ICE by leveraging neighborhood proximity, in the process
briefly describing both model description and model optimization.

2.1

Problem Definition and Formulation

In this section we first define the ICE problem, and formulate it
as a heterogeneous information network with entities and texts.
The proposed ICE network is composed of two types of networks:
entity-text networks and text-text networks. Below we define and
describe in detail the two networks together with ICE networks.
Definition 2.1. (Entity-Text Network) An entity-text network,
denoted as G et = (V ∪ T , Eet ), is a directed bipartite graph with a
has-a relation between items and words; V is a set of items, T is a
set of words, and each edge r ∈ Eet is associated with a positive
weight reflecting the strength of the relation between an item and
a word.
In an entity-text network, nodes and relations are both heterogeneous [12]. For such a network, entities can be different types
of items, and the text can be any descriptive textual information
associated with the item, depending on the real-world application.
For example, for music recommendation, songs and the words in
their lyrics form a typical entity-text bipartite network, where the
weights of the edges can be either binary or any positive real number (e.g., the tf-idf value of a word in the lyrics). Note that in this
paper we consider only positive network weights.
Definition 2.2. (Text-Text Network) A text-text network, denoted as G t t = (T , Et t ), is a bidirected graph with a conceptually
similar relation between two words in T , where T is a set of words,
and each edge r ∈ Et t is associated with a positive weight reflecting
the strength of the relation between two words.

(1) We propose a novel item concept embedding (ICE) approach to model concepts of items via their associated
textual information.
(2) We propose a generalized network construction method
to assemble a graph involving heterogeneous nodes and
a mixture of both homogeneous and heterogeneous relations. The resulting ICE improves on the limitations of
previous network embedding approaches and enhances
the effectiveness of heterogeneous retrieval.
(3) We conduct extensive experiments on two real-world datasets.
Experimental results attest the effectiveness of the proposed ICE approach. We also publish the IMDB dataset,
which includes the textual descriptions of 36,586 movies.
The rest of this paper is organized as follows. In Section 2, we
present the proposed ICE methodology in detail, including the formal definitions of the ICE problem, its related information network,

By leveraging NLP resources (e.g., the pre-trained word2vec
model [8] and ConceptNet1 ), a homogeneous text-text network can
be built that illustrates the conceptual similarity between any two
words in the network. For example, with a set of word2vec word
vectors for words in a set T (denoted as Q = (~
qt1 , q~t2 , . . . , q~t |T | ),
where q~ti denotes the low-dimensional vector of word ti ), one way
to build such a network is to connect two words ti and t j if their
cosine similarity is greater than a threshold h (i.e., cos(~
qti , q~t j ) > h);
the weight of the connecting edge can be set to either one or the
cosine distance. Note that in the text-text network, each node has
a self-referencing bidirectional link (i.e., a self-loop), and the nodes
and relations are all homogeneous [12].
In order to better model the item concept via textual information
and facilitate heterogeneous retrieval, we further present the ICE
network, a novel expanded entity-text-text network, the definition
of which is below.
1 http://conceptnet5.media.mit.edu

86

Session 1C: Document Representation and Content Analysis 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Definition 2.3. (ICE Network) The ICE network is an entitytext-text network denoted as G ice = (V ∪ T , E et ∪ Eet ∪ Et t ),
where E et denotes the expanded has-a relations between items and
words.

1

2

W1

1

W2

2

W3
3

Figure 1(a) is an example of the proposed ICE network, in which
there exist heterogeneous nodes (items and words) and a mixture of
both homogeneous (conceptually similar, purple lines) and heterogeneous (has-a, pink lines) relations. In the network, the expanded
has-a relations E et (the pink dashed lines in the figure) can be designed differently depending on the application. A general method
of establishing such relations is described in Section 2.2.
Obtaining representative embeddings for such complex information networks is a vital but difficult problem; a solution for this
would be of great use in a variety of retrieval and recommendation
applications. In this paper, we aim to leverage the similarity of
neighbor network structures of both items and words to describe
each item and word via the corresponding low-dimensional vectors
learned by our carefully designed ICE networks.

W4

4

W5

1

2

W1

3

Eet : has-a relation
Eet : has-a relation (expanded)
Ett : conceptually similar relation

(a)

W1

3

W2
4

W3

W1

W4

W2

W5

W3
W4
W5

W2
4

W3

W1

W4

W2

W5

W3
W4

(b)

W5

(c)

Figure 1: The basic idea behind the ICE network

• MG t t = (ti j ) is an adjacency matrix of size |T | × |T | such
that each element ti j is the weight of an edge for adjacent
nodes and zero for nonadjacent nodes (see Figure 2(b)).

Definition 2.4. (Neighborhood Proximity) Given a graph G =
(V, E), the neighborhood proximity of a pair of nodes (u, v) in
the network is defined as the similarity between their neighborhood network structures. Mathematically, given the neighborhood
structure of node b, denoted as Nb = (ub1 , ub2 , . . . , ub | V | ),2 the
neighborhood proximity between nodes u and v is decided by the
similarity between the two vectors Nu and Nv . If there are no
shared neighbors between u and v, the neighborhood proximity
between them is 0.

Note that since the conceptual similarity described by MG t t is a
homogeneous relation, rows and columns should not be permuted
independently, but simultaneously, whereas MG e t relates to heterogeneous has-a relations, whose rows and columns can be permuted
independently.
Establishing the expanded relations E et between each item i ∈ V
and each word t ∈ T is a way to better capture the concepts of items
and words in the proposed ICE network. In this paper, we present
a unified approach to construct these relations and construct the
ICE network on the basis of the resulting relations. Assume for
simplicity and demonstration purposes that both the entity-text
bipartite network G et and the text-text bidirected network G t t are
(0, 1)-graphs; i.e., each element in MG e t or MG t t , which represents
the relation between nodes i and j, is one when there is a directed
edge from node i to node j, and zero when there is no edge. Note
that the diagonal elements of MG t t are all set to one to represent
the self-referencing links in the network G t t . The inner product of
the two matrices MG e t and MG t t is defined as

In Figure 1, panels (b) and (c) illustrate the basic idea behind the
ICE network, which can be seen as an alternative view of the original ICE network shown in panel (a). As shown in the figures, the
neighbors of both items and words are of the same type (i.e., words),
which means that both items and words are embedded via textual
information. In this case, by quantifying neighborhood proximity,
the resultant low-dimensional vectors of both items and words not
only embed similar concepts illustrated with textual information
but also facilitate homogeneous or heterogeneous retrieval. As an
example, consider the second item (the music note with subscript 2)
and the third word (w3 ) in Figure 1. These two heterogeneous
nodes share the same neighborhood structure, i.e., (w1 , w2 , w3 ), as
illustrated in panel (c) of the figure; the learned representations of
these two nodes are therefore close to each other.

2.2

The Intuition of an ICE Network
— A Bipartite Graph

ICE Network

A = (ai j ) = MG e t · MG t t ∈ R | V |× | T | .

(1)

The intuition behind this operation is described as follows. Each
element ai j ∈ A is the inner product of two vectors:
• ~ei = (ei1 , ei2 , . . . , ei | T | ) | ∈ R | T | for item i, and
• ~t j = (t 1j , t 2j , . . . , t | T |j ) | ∈ R | T | for word j.

ICE Network Construction

In this section, we describe a generalized network construction
method for the proposed ICE network. Given an entity-text bipartite network G et = (V ∪ T , Eet ) and a text-text bidirected
network G t t = (T , Et t ) (see Definitions 2.1 and 2.2), we represent
the two networks and their relations as matrices MG e t and MG t t
respectively:

There are two cases in which ai j > 0:
• There is a directed edge from item i to word j;
• There is no directed edge from item i to word j, but there
is at least one directed edge from item i to another word
k and word j is bi-directionally connected to word k (the
two words k and j are conceptually similar); i.e., ∃k such
that k , j, eik = 1, t jk = tk j = 1.

• MG e t = (ei j ) is a biadjacency matrix of size |V | × |T | such
that each element ei j is the weight of an edge for adjacent
nodes and zero for nonadjacent nodes (see Figure 2(a)).

Given the two matrices MG e t and MG t t associated with the two
initial graphs G et and G t t , respectively, we define the transformation function f (·) to carry out the aforementioned operations
together with a matrix augmenting operation to construct the ICE

2 The neighborhood structure of b can be represented by the first-order proximity of b
to all the other nodes [14].

87

Session 1C: Document Representation and Content Analysis 1

Entity-Text Network
(a)

W1 W2 W3 W4 W5

I1
I2
I3
I4

1
1
0
0

0
1
0
1

1
0
1
0

0
0
1
1

1
0
0
0

MGet
1

W1

2

W2

3

W3
W4

4

Text-Text Network
(b)
W1
W2
W3
W4
W5

W1 W2 W3 W4 W5

1
0
1
0
0

0
1
1
1
0

1
1
1
0
0

0
1
0
1
1

0
0
0
1
1

MGtt

W1 W2 W3 W4W5
I1
I2
I3
I4

W3

2
3

W4

Eet : has-a relation
Eet : has-a relation (expanded)
Ett : conceptually similar relation

2
1
1
0

1
1
2
2

2
2
1
1

1
1
1
2

1
0
1
1

A = MGet · MGtt

1

W2

W5

W5

in an ICE network, both types of nodes (i.e., items and words) and
relations (i.e., has-a and conceptually similar relations) are heterogeneous, the neighbors of each node are the same type: words; this
reflects the careful design behind the ICE network, as introduced
in Section 2.2.
Given an ICE network G ice = (V ∪ T , E et ∪ Eet ∪ Et t ), constructed via Equation (2), we introduce two vectors v~c and v~c0 , where
v~c is the vector representation of node c when it plays the role of
itself, whereas v~c0 denotes node c’s representation when it is taken
as a specific context of other nodes. For each has-a relation between
an item ni ∈ V and a word nw ∈ T , the conditional probability of
word nw generated by item ni is modeled as
 0|

exp v~nw · v~ni
 0|
.
P (nw | ni ) = P
(4)
~ns · v~ni
s ∈ T exp v

ICE Network
(c)

Gice : ICE Network

W1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

4

W1
W2
W3

W1 W2 W3 W4 W5
I1
I2
Ã = I
3
I4
W1
W2

MGtt = W3
W4
W5

1
1
1
0
1
0
1
0
0

1
1
1
1
0
1
1
1
0

1
1
1
1
1
1
1
0
0

1
1
1
1
0
1
0
1
1

MGice = f(MGet , MGtt ) =

W4

1
0
1
1
0
0
0
1
1

Ã
MGtt

W5

Figure 2: ICE network construction
network (the expanded entity-text-text network defined in Definition 2.3) as follows:
"
#
Ã
MG ic e = f (MG e t , MG t t ) =
∈ R ( | V |+ | T |)× | T | , (2)
MG t t

Similarly, for each conceptually similar relation between two words,
nw ∈ T and nw̌ ∈ T , we have the conditional probability of word
w̌ given word w as
 0|

exp v~nw̌ · v~nw
.
 0|
P (nw̌ | nw ) = P
(5)
~ns · v~nw
s ∈ T exp v

in which the matrix Ã can be derived from matrix A in Equation (1).
When building a (0, 1)-ICE-network, we have


Ã = (ãi j ) = 1 {ai j >0} .
(3)

To model neighborhood structures via the item-word and wordword relations while preserving neighborhood proximity, for each
node nk , the conditional probabilities P ( · | nk ) described by a set
of low-dimensional representations should approximate the empirical probabilities P̂ ( · | nk ). Here we follow the setting in [14] in
setting the empirical probability for P̂ (n ` | nk ) equal to x k ` /d (nk ),
in which x k ` denotes the weight of the edge between nodes nk
and n ` , and d (nk ) is the out-degree of node nk ; that is, d (nk ) =
P
nд ∈N B (n k ) x kд , where N B(nk ) is the set of the direct successors
of node nk . We proceed to minimize the following two objective
functions O E and OT to learn the embeddings of items and words,
respectively:
X


OE =
d (ni ) D KL P̂ ( · | ni ) , P ( · | ni ) ,
(6)

Although matrix Ã can be designed in different ways (e.g., Ã = A
for weighted graphs), it is straightforward to apply our unified
approach to different transformations. Finally, given the resulting matrix MG ic e (see Equation (2)), we obtain the ICE network,
G ice , treated as the input of the embedding learning algorithm
(introduced in Section 2.3).
Figure 2 illustrates the ICE network construction method with
an example (a (0,1)-graph), in which the matrix representations for
the directed bipartite entity-text network G et and the bidirected
text-text network G t t are shown in panels (a) and (b), respectively.
As shown in panel (c), we obtain the expanded has-a relations by
multiplying the two matrices MG e t and MG t t . For instance, there is
no edge between the first item (the music note with subscript 1) and
the second word (w2 ) in the original entity-text network; however,
since the first item is linked to the third word (w3 ) and w2 and w3
are conceptually similar according to G t t in panel (b), we have an
expanded link between the first item and the second word (the pink
dashed line) in panel (c).

2.3

n i ∈V

and
OT =

X
nw ∈ T



d (nw ) D KL P̂ ( · | nw ) , P ( · | nw ) ,

(7)

in which the Kullback-Leibler divergence D KL (·, ·) is adopted to
measure the difference between the two distributions. By omitting
some constants and adding O E and OT in Equations (6) and (7), the
objective function becomes

Learning Embedding via Similarity of
Neighborhood Network Structures

In a previous study [14], the LINE model was mainly proposed to
learn the embeddings for homogeneous information networks, that
is, graphs with the same type of vertices. In this paper we adopt
neighborhood proximity, an essential idea from LINE as well as from
various word embedding models, to manage the ICE networks,
which are composed of heterogeneous nodes and a mixture of both
homogeneous and heterogeneous relations. Neighborhood proximity presumes that nodes with similar neighborhood structures are
similar and thus, when represented in a low-dimensional vector
space, should be positioned close to each other.
To capture this proximity, each node in the graph plays two roles:
the node itself and a specific “context” of other nodes. Although

O ice = −

X
*.
x i ` log P (n ` | ni )
.
, (ni ,n ` ) ∈Ẽe t
X
+
xw ` log P (n ` | nw ) +/ ,
(nw ,n ` ) ∈E t t
-

(8)

where Ẽet = Eet ∪ E et . Equation (8) is minimized with stochastic gradient descent using edge sampling [14] and negative sampling [8], resulting in two sets of optimized representations {~
vc }c ∈V∪T
and {~
vc0 }c ∈V∪T .

88

Session 1C: Document Representation and Content Analysis 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

dataset. The classification model is trained via the LIBSVM toolkit,6
and its input feature vectors are generated from 3 different types
of document representation methods, which leverage the textual
information of the movie descriptions:
(1) BOW: This is the traditional bag-of-words method for generating the word features for text classification. In specific,
we use top tf-idf keywords of each movie description to
represent a movie and train the classification models;
(2) BPT: This is the baseline embedding method for learning
the representation of heterogeneous entities using a bipartite structure. In particular, we also use the top tf-idf
keywords and movies to construct a simple entity-text bipartite network, as defined in Definition 2.1, and we use
network embedding techniques to learn the representation
of each movie for training the classification models;
(3) ICE: This is the proposed item concept embedding (ICE)
method for learning the representations of heterogeneous
entities. In specific, we also use the top tf-idf keywords and
movies to construct the ICE network. However, different
from the simple bipartite structure, the ICE network contains several careful designs, as described in Section 2.2, to
better learn the representation of each movie for training
the classification models.
For the experiments of this task, 5-fold cross validation is applied
to verify the classification performance in terms of three measures:
exact match ratio, micro-average and macro-average F-measures.

Table 1: Statistics of the two real-world datasets
# movies/songs
Average text length
Average # unique words
Vocabulary size
# single genres
# multi-label genres

3

IMDB

KKBOX

36,586
65.0
47.8
66,924
28
915

33,106
215.24
81.37
101,395
-

EXPERIMENTS

We evaluate the performance of our proposed ICE approach on two
real-world datasets. Section 3.1 first provides the details of the two
datasets and the data processing conducted for the experiments.
We then describe the experimental settings in Section 3.2, including
the compared baselines and state-of-the-art methods, and the statistics of the constructed ICE information networks. Experimental
results on various classification and retrieval tasks are provided in
Section 3.3, where some interesting cases are also discussed.

3.1

Datasets and Data Preprocessing

In the experiments, two real-world datasets are used to assess the
performance of the proposed ICE method, including one movie
dataset (denoted as IMDB) and one music dataset (denoted as
KKBOX, which is partially provided by a music-streaming company). Table 1 lists the statistics of the two data collections.
Below, we describe how we collect and process the datasets. The
movie list of the IMDB dataset is that of the MovieLens 10/2016
Full datasets,3 and we crawl the descriptions and genres of each
movie from the IMDB website via the OMDB API.4 Then, we filter
out those movies without any descriptions, resulting in total 36,586
movies out of the 40,110 ones of the original MovieLens dataset;
in average, the description of each movie contains around 65.0
words and 47.8 unique words. Moreover, there are 28 different
single genres (e.g., horror and action) and 915 multi-label genres
(e.g., horror-action-thriller) in total. For the KKBOX dataset, the
music list is obtained from a commercial music-streaming company
consisting of 33,106 Chinese songs, and we crawl the lyrics of
each song for the following experiments. To process lyrics in the
dataset, we use the Jieba Chinese segmentation toolkit5 to conduct
the word segmentation. After the procedure, there are in average
215.24 words and 81.37 unique words in each song, and there are
in total 101,395 unique vocabularies, as shown in Table 1.

3.2

3.2.2 Settings for the Retrieval Tasks. For the retrieval tasks,
there are two kinds of retrievals: homogeneous (i.e., word-to-word
or item-to-item) and heterogeneous (i.e., item-to-word or word-toitem). In addition to the traditional keyword-based retrieval, we
also conduct several methods based on embedding techniques for
a comprehensive comparison. Below, we describe the settings of
each retrieval method for the retrieval tasks:
(1) RAND: This is the baseline method which selects items or
words randomly for each search task;
(2) KBR: This is the traditional keyword-based retrieval method
for selecting items based on the given query. Note that the
traditional keyword-based retrieval method can only be
used for the word-to-item search task;
(3) AVGEMB: This is the method which represents entities
using averaged word embeddings. In specific, we select top
tf-idf words of each movie description or song lyric, and
then average their word embeddings to represent the movie
or song. In our experiments, the word embeddings used
to represent entities in the IMDB dataset and the KKBOX
dataset are trained on the Google News dataset7 and the
Chinese Wikipedia archive8 using word2vec, respectively.
The dimension of the word embedding vectors is set to 300;
(4) BPT: This is the baseline embedding method for learning
the representation of heterogeneous entities using a bipartite structure. In particular, we also use the top tf-idf
keywords and all items to construct a bipartite network,
as defined in Definition 2.1, and apply network embedding

Experimental Setup

In our experiments, two different types of tasks are conducted to
evaluate the performance of the proposed ICE approach, including
classification task and retrieval task, the purpose of which is to
verify the quality of the representations learned by ICE. Below, we
describe the settings for each different task, respectively.
3.2.1 Settings for the Classification Task. For the classification
task, we conduct a multi-label genre classification on the IMDB
3 https://grouplens.org/datasets/movielens/

6 https://www.csie.ntu.edu.tw/∼cjlin/libsvm/
7 https://code.google.com/archive/p/word2vec/

4 https://www.omdbapi.com

5 https://github.com/fxsjy/jieba

8 https://dumps.wikimedia.org/zhwiki/latest/zhwiki-latest-pages-articles.xml.bz2

89

Session 1C: Document Representation and Content Analysis 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 2: Statistics of IMDB information networks
BPT

ICE (exp-3)

ICE (exp-5)

|W |

10

20

10

20

10

20

|V |
|T |
|E e t |
|E t t |
|E e t |
d (·)

36,586
62,066
358,160
7.3

36,586
65,036
622,004
12.2

36,586
84,990
358,160
281,368
942,633
26.0

36,586
87,222
622,004
289,800
1,696,638
42.1

36,586
113,820
358,160
444,194
1,569,753
31.5

36,586
116,630
622,004
457,566
2,823,767
51.0

Horror
Action
Sci-Fi
Thriller
Western
Short

(a) BPT with |W | = 20

Horror
Action
Sci-Fi
Thriller
Western
Short

(b) ICE (exp-5) with |W | = 20

Figure 3: Visualization of the movies embeddings
techniques to learn the representations of both items (i.e.,
movies or songs) and words;
(5) ICE: This is the proposed item concept embedding (ICE)
method for learning the representation of heterogeneous
entities. In specific, we also use the top tf-idf keywords
and items to construct the ICE network. However, unlike
the simple bipartite structure, the ICE network contains
several careful designs, as described in Section 2.2, to better
learn the representations of both items and words.

3.3

Quantitative Results

3.3.1 Performance of the Classification Task. Table 4 lists the
results of the three compared methods (i.e., BOW, BPT, and ICE) on
the IMDB movie genre prediction, in which there are two sets of
experiments, including |W | = 10 and |W | = 20. Note that, for this
task, the testing items are all the same type (i.e., homogeneous).
As shown in the table, increasing the number of words for representing a movie (i.e., |W | = 10 → |W | = 20) clearly improves the
classification performance of the three methods in terms of the
three measures. Moreover, the two embedding-based methods (i.e.,
BPT and ICE) considerably outperform the BOW method, which
suggests that the learned low-dimensional representations are indeed effective for such a classification task, as demonstrated in [13].
Furthermore, as observed in the table, the proposed ICE method
yields comparable results with the BPT method; the result is anticipated due to the network constructions of both embedding-based
methods are appropriate for the homogeneous classification task.
Figure 3 visualizes the learned representations of the movies
from 6 different genres based on the BPT and ICE methods, in
which 100 randomly picked movies for each genre are used for the
visualization.9 As shown in both figures, the movies of the same
genre generally aggregate into one group, especially for western
and action movies; the groups of thriller and horror movies overlap
substantially with each other, which means that the usage of words
in those movie descriptions are highly similar.

Note that, during the retrieval task, the above three embedding
methods (i.e., AVGEMB, BPT, and ICE) will select the entities whose
embeddings are the most similar to that of the user query in terms
of the cosine similarity.
3.2.3 Information Networks. In the experiments, there are two
types of information networks: the BPT and ICE networks. Table 2
and Table 3 tabulate the statistics of the two information networks.
In these two tables, W denotes the set of the top tf-idf words of
an item (e.g., movie descriptions or song lyrics) that is used to
represent the item and to construct the information network; the
“exp-n” in the parentheses denotes the number of expanded n words
for each word. In the experiments, the expanded words for each
given word are selected according to the cosine similarity of their
pre-trained word embeddings, which are trained on the Google
News dataset and the Chinese Wikipedia archive for the IMDB
dataset and the KKBOX dataset using word2vec, respectively. For
the IMDB dataset, the number of the top tf-idf words is set to 10 or
20 for each movie to construct the network, whereas for the KKBOX
dataset, since songs are usually lyrically repetitive, the number is
set to 1, 3, 5, 8, or 10 for each song to allow exploration for the
optimal setting. Note that for the BPT networks, since there is only
one type of relationship (has-a, described by Eet in the tables), the
average degrees of nodes, d (·), in such networks are smaller than
those of the ICE networks.

3.3.2 Performance of the Word-to-Movie Retrieval Task. We first
conduct a heterogeneous retrieval task, i.e., word-to-movie retrieval
task, on the IMDB dataset. The goal of this task is to use keywords
to retrieve the movies with similar genre. In specific, we select the
top 20 tf-idf words from all the movie descriptions in each genre as
the query words, and then use the queries to retrieve the similar
genre movies. For instance, the word “killer” is the top 1 tf-idf word
from all thriller movie descriptions; therefore, we use “killer” as a
query in an attempt to retrieve thriller movies.
Table 5 tabulates the retrieval results of the five compared methods in terms of precision@50 and precision@100. For the AVGEMB
method, in addition to using top 20 tf-idf words of each movie
description (i.e., |W | = 20), we also conduct an experiment of

3.2.4 Evaluation Metrics and Parameter Settings. In our experiments, the performance of multi-class classification is measured
by three metrics: exact match ratio, micro-average F1, and macroaverage F1, whereas that of retrieval is measured by precision and
diversity. For the classification task, all the parameters are set to the
LIBSVM’s default values. For the network embedding approaches
(BPT and ICE), the number of negative samples is set as 5, and the
dimensionality of item and word vectors is set to 256 and 300 for
the experiments on the IMDB and KKBOX datasets, respectively.

9 Each

movie representation is a 256-dimensional real-valued embedding vector and
transformed into two-dimensional space using t -distributed stochastic neighbor embedding (t -SNE), a technique for dimensionality reduction that is particularly well
suited for the visualization of high-dimensional data.

90

Session 1C: Document Representation and Content Analysis 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 3: Statistics of KKBOX information networks
BPT

ICE (exp-3)

|W |

1

3

5

8

10

1

3

5

8

10

|V |
|T |
|E e t |
|E t t |
|E e t |
d (·)

33,106
15,483
33,106
1.4

33,074
25,806
99,222
3.4

32,999
29,357
164,995
5.3

32,603
31,292
260,824
8.2

31,527
31,315
315,270
10.0

33,106
32,372
33,106
120,368
99,318
7.7

33,074
42,225
99,222
184,133
296,455
15.4

32,999
44,964
164,995
204,942
491,875
22.1

32,603
46,269
260,824
215,861
775,502
31.8

31,527
46,204
315,270
215,780
935,767
37.7

Table 4: Movie genre classification task
|W | = 10
Exact match ratio
Micro-average F-measure
Macro-average F-measure

Other movies
Godzilla movies
Japan (word)
Godzilla (word)

(a) BPT with |W | = 20

|W | = 20

BOW

BPT

ICE (exp-3)

ICE (exp-5)

TF-IDF

BPT

ICE (exp-3)

ICE (exp-5)

0.136
0.365
0.087

0.160
0.401
0.166

0.156
0.408
0.170

0.157
0.410
0.170

0.162
0.415
0.156

0.182
0.464
0.229

0.182
0.462
0.223

0.181
0.463
0.222

3.3.3 Performance of the Word-to-Song Retrieval Task. We then
conduct another heterogeneous retrieval task — word-to-song retrieval — on the KKBOX music dataset. In the experiments, three
types of contextual keywords: mood, location, and time related
keywords, are used as our queries; we select the top 5 tf-idf words
for each category from our crawled song lyrics. Since we do not
have a relevancy ground truth for this retrieval task, we consider a
retrieved song as a relevant result on the following two conditions:
(1) the lyrics of the retrieved song contains the query word, or (2)
the song lyric contains at least one of the top k concept-similar
words of the query. Note that each query case has its own set of
relevant songs in our experiments.
Figures 5 and 6 report the results of the proposed ICE together
with the four comparing models (i.e., RAND, KBR, AVGEMB, and
BPT) with respect to the size of words used to describe a song (|W |).
Overall, both ICE and AVGEMB improve their performance as |W |
increases; however, BPT hardly retrieve any songs containing the
query word, which confirms again that the embeddings learned
on the simple bipartite network is not appropriate for a heterogeneous retrieval. Note that in Figure 5, we do not compare the
performance of these models with KBR because its performance is
the upper bound of this task. For the case that songs with any of the
top 3 concept-similar words are considered relevant, the proposed
ICE approach generally outperforms the other four baseline methods, as shown in Figure 6. The experiments in these two figures
suggest that the word embeddings learned by ICE are capable of
retrieving the songs containing the given query and/or the query’s
concept-similar words, and thus the ICE method is effective for
heterogeneous retrieval on the basis that the items and the query
words are embedded by the textual information.
Table 6 tabulates the precision of each query word, in which there
are five query words for each of the contexts. With Figures 5 and 6,
we can observe some interesting phenomenon in the experiments.
As shown in the table, for most of the queries, our ICE approach
obtains much better precision than both AVGEMB and BPT; that
is, in average, ICE achieves 0.533 and 0.201 for precision@100 in
the keyword and concept-similar word retrieval tasks, surpassing

Other movies
Godzilla movies
Japan (word)
Godzilla (word)

(b) ICE (exp-5) with |W | = 20

Figure 4: Visualization of the Representations of the
Godzilla-related Movies and Two Related Keywords

AVGEMB (all) of averaging all word embeddings learned by word2vec,
which can be considered another competitive baseline for this task.
As shown in the table, in terms of average precision@50 and precision@100, the proposed ICE method can outperform all the compared baselines. Although, for some genres, the performance of ICE
is only comparable with KBR and AVGEMB, for both action and
short movies, the proposed ICE approach outperforms all of the
baselines with a considerable amount. Note that for this heterogeneous word-to-movie retrieval task, BPT yields poor performance
compared to our ICE approach; the result is due to the straightforward bipartite network is appropriate for the tasks involving only
one type of entity, such as document classification and item-to-item
retrieval, and fails for such a heterogeneous retrieval task.
Figure 4 visualizes an example to explain the phenomenon. In
the figure, the red triangles represent Godzilla-related movies; the
two selected words: Japan and Godzilla, which are highly related to
the Godzilla-related movies, are denoted as cross and plus symbols,
respectively. As shown in the figure, these two words are distant
from all Godzilla-related movies with BPT, whereas the proposed
ICE demonstrates an opposite outcome — the two words are close
to the highly correlated movies, which again shows the superior
ability of our ICE approach for the heterogeneous retrieval task.

91

Session 1C: Document Representation and Content Analysis 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 5: Word-to-movie retrieval task
|W | = 20

Horror
(3754/36586)

Thriller
(4636/36586)

Western
(751/36586)

Action
(5029/36586)

Short
(1094/36586)

Sci-Fi
(2004/36586)

Average

0.000
0.062
0.092
0.089
0.032
0.142

0.120
0.373
0.392
0.401
0.086
0.392

0.070
0.288
0.290
0.285
0.080
0.305

0.000
0.057
0.074
0.074
0.034
0.109

0.060
0.307
0.372
0.382
0.086
0.362

0.058
0.258
0.273
0.270
0.082
0.278

P@50
RAND
KBR
AVGEMB
AVGEMB (all)
BPT
ICE (exp-5)

0.080
0.324
0.322
0.324
0.096
0.354

0.080
0.230
0.212
0.225
0.104
0.204

0.060
0.321
0.316
0.304
0.010
0.294

0.080
0.418
0.406
0.366
0.154
0.444
P@100

RAND
KBR
AVGEMB
AVGEMB (all)
BPT
ICE (exp-5)

0.050
0.327
0.324
0.314
0.088
0.321

0.100
0.224
0.215
0.208
0.116
0.193

Word-to-Song Retrieval Task (Keyword)

0.9

0.110
0.395
0.385
0.376
0.156
0.421

Word-to-Song Retrieval Task (Keyword)

0.7

0.8

0.030
0.236
0.266
0.269
0.012
0.264

Word-to-Song Retrieval Task (Keyword)

0.6

0.6

0.5

0.7
0.5

0.5
0.4

Precision@100

0.4
Precision@50

Precision@10

0.6
0.4

0.3

0.3

0.3

0.2
0.2

0.2

0.0

0.1

0.1

0.1

1

3

5
|W |

8

0.0

10

1

3

(a)

5
|W |

8

0.0

10

1

3

(b)

5
|W |

8

10

RAND
AVGEMB
BPT
ICE (exp-3)

(c)

Figure 5: Word-to-song retrieval task (relevant songs: keyword included songs)
Word-to-Song Retrieval Task (Concept-similar Word)

0.25

Word-to-Song Retrieval Task (Concept-similar Word)

0.25

0.20

0.20

0.15

0.15

Word-to-Song Retrieval Task (Concept-similar Word)

0.30

0.25

0.10

Precision@100

Precision@50

Precision@10

0.20

0.10

0.15

0.10

0.05

0.00

0.05

1

3

5
|W |

(a)

8

10

0.00

0.05

1

3

5
|W |

8

10

0.00

1

3

(b)

5
|W |

8

10

RAND
KBR
AVGEMB
BPT
ICE (exp-3)

(c)

Figure 6: Word-to-song retrieval task (relevant songs: concept-similar words included songs)
those of the AVGEMB (0.239 and 0.186) and the BPT (0.017 and
0.037). Moreover, we can observe that it is relatively difficult for
the location-based queries to retrieve songs containing the conceptsimilar words, which is due to the fact that few location-based
songs exist in our datasets; for example, the query “房間 (room)”
has 3 expanded words, “浴室 (bathroom)”, “地下室 (basement)”,
and “臥室 (bedroom)”, but only 28 songs contain any of these three
concept-similar words in the KKBOX dataset.

Finally, we evaluate the proposed ICE in terms of precision and
diversity using ConceptNet10 (see Table 7). Given a keyword, we
use ConceptNet, a human-labeled semantic knowledge graph, to
obtain the keyword’s concept-similar words.11 On the basis of these
obtained concept-similar words, we consider the songs that contain
at least one concept-similar word relevant in our experiments; note
10 http://conceptnet5.media.mit.edu

11 In this experiments, we only selected 5 queries, whose corresponding concept-similar
words are greater than 10 and less than 40, for consideration. Note that we also remove
the one-character word in the set of concept-similar words.

92

Session 1C: Document Representation and Content Analysis 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 6: Performance comparison on the 15 keywords
|W | = 10

Keyword

Concept-similar word

BPT

AVGEMB

ICE (exp-3)

# concept-similar songs

BPT

AVGEMB

ICE (exp-3)

Mood

失落 (lost)
心痛 (heartache)
想念 (pining)
深愛 (affectionate)
難過 (sad)

516
824
1,729
380
1678

0.000
0.050
0.050
0.000
0.040

0.160
0.080
0.250
0.090
0.200

0.470
0.250
0.700
0.550
0.530

403
4,075
1,176
442
1,781

0.030
0.170
0.080
0.020
0.080

0.120
0.500
0.180
0.110
0.320

0.050
0.610
0.060
0.250
0.070

Location

P@100

# keyword songs

回家 (home)
房間 (room)
海邊 (seaside)
火車 (train)
花園 (garden)

934
610
264
151
139

0.040
0.000
0.000
0.010
0.000

0.310
0.420
0.230
0.330
0.160

0.900
0.510
0.360
0.510
0.390

1,190
28
91
20
2

0.020
0.000
0.000
0.000
0.000

0.340
0.010
0.070
0.040
0.000

0.160
0.060
0.080
0.020
0.000

Time

P@100
Query

夕陽 (dusk)
日出 (sunrise)
日落 (sunset)
月亮 (moon)
黑夜 (dark night)

387
240
226
598
1,189

0.010
0.000
0.030
0.000
0.030

0.180
0.290
0.380
0.360
0.140

0.360
0.430
0.590
0.930
0.510

307
390
407
1,608
279

0.020
0.060
0.010
0.030
0.030

0.100
0.380
0.270
0.320
0.030

0.070
0.690
0.530
0.350
0.010

Total/Avg. P@100

9,865

0.017

0.239

0.533

12,199

0.037

0.186

0.201

Table 7: Performance evaluated by ConceptNet
|W | = 10

P@10

Query

Diversity@10

Diversity@100

# words in ConceptNet

KBR

ICE (exp-3)

KBR

ICE (exp-3)

KBR

ICE (exp-3)

KBR

ICE (exp-3)

11
39
17
33
17

0.00
0.60
0.40
0.30
0.50

0.20
0.10
1.00
0.10
1.00

0.00
0.00
0.00
0.00
0.00

0.00
0.00
0.70
0.00
0.00

0.25
0.36
0.30
0.34
0.50

0.08
0.16
0.24
0.08
0.57

0.00
0.00
0.00
0.00
0.00

0.75
0.69
0.75
0.50
0.68

23.4

0.36

0.48

0.00

0.14

0.35

0.23

0.00

0.67

夕陽 (dusk)
房間 (room)
日出 (sunrise)
花園 (garden)
黑夜 (dark night)
Average

Table 8: An example for movie-to-word retrieval

that, in this task, the songs containing the query word only are not
considered as relevant songs. Then, we use the following measure
to assess the diversity of the retrieved songs:
Diversity@n =

|R ∩ S k |
,
|R|

Query movie: Toy Story, 1995
(Animation, Adventure, Comedy)
BPT

(9)

manias
entraineuse
taddeo
anuelo
portico
bep
meanness
zanchi
sarti
raffin

where R denotes the set of relevant songs and Sk denotes the set
of songs containing the given keyword k, from the retrieved n
songs. Observing from the table, the proposed ICE outperforms
the KBR method for the top 10 retrieval results, although the KBR
method is naturally easy to achieve high precision in this task;
that is because all songs retrieved by this method contain the given
query word. Although the proposed ICE method cannot outperform
the KBR method in terms of precision@100, our method provides
more diverse retrieved results than the KBR approach does; that
is, our ICE approach can retrieve the songs relevant in terms of
similar concepts without resorting to keyword-matching, which is
intractable for the classic KBR approach.

3.4

P@100

ICE (exp-5)
andy
gave
give
sid
tabbed
robertson
Named
stuffed animals
toys
Toys

both homogeneous and heterogeneous entities. First, Table 8 gives
an example for a movie-to-word retrieval task, in which the top
10 cosine-similar words in the given movie, Toy Story (1995), are
listed. As demonstrated in the table, while using the embeddings
learned on BPT produces words unrelated to the given movie, our
method provides much more reasonable results; for example, Andy
is a major character and Sid is the main antagonist in the Toy Story.

Case Study

This section provides several interesting case studies to demonstrate
the capability of the proposed ICE in managing tasks which involve

93

Session 1C: Document Representation and Content Analysis 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 9: An example for movie-to-movie retrieval
Movie query: The Avengers, 2012 (Action, Adventure, Sci-Fi)
BPT

ICE (exp-5)

Justice League: War, 2014 (Animation, Action, Adventure)
Lego DC Comics Super Heroes, 2015 (Animation, Action, Adventure)
Falling Skies, 2011-2015 (Action, Adventure, Drama)
Marvel Super Hero Adventures: Frost Fight!, 2015 (Animation)
Doctor Mordrid, 1992 (Action, Fantasy, Horror)

Justice League: War, 2014 (Animation, Action, Adventure)
A Cosmic Christmas, 1977 (Animation, Short, Sci-Fi)
Howard the Duck, 1986 (Action, Adventure, Comedy)
Hellboy II: The Golden Army, 2008 (Action, Adventure, Fantasy)
The War in Space, 1977 (Action, Adventure, Sci-Fi)

Table 10: An example for word-to-movie retrieval
Word query: alien
BPT

ICE (exp-5)

The Blue Lagoon, 1949 (Adventure, Drama, Romance)
Turner & Hooch, 1989 (Comedy, Crime, Drama)
Only the Young, 2012 (Documentary, Comedy, Romance)
Brute Force, 1947 (Crime, Drama, Film-Noir)
Home, 2015 (Animation, Adventure, Comedy)

Coneheads, 1993 (Comedy, Sci-Fi)
Without Warning, 1980 (Sci-Fi, Horror)
They Came from Beyond Space, 1967 (Adventure, Sci-Fi)
Battle of the Stars, 1978 (Sci-Fi)
Howard the Duck, 1986 (Action, Adventure, Comedy)

Table 9 tabulates the top 5 retrieved movies given the movie, The
Avengers (2012), as the query, which again demonstrates the fact
that both BPT and ICE are effective for homogeneous retrieval.
Finally, we use the word alien to search for the relevant movies
(see Table 10). Clearly, the BPT approach fails on this task because
of retrieving irrelevant movies in terms of genre, whereas 4, if not
all, of the top 5 movies retrieved by the proposed ICE are highly
correlated Sci-Fi movies.

4

(2003), 1137–1155.
[2] Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2015. GraRep: Learning graph
representations with global structural information. In Proceedings of the 24th
ACM International on Conference on Information and Knowledge Management.
891–900.
[3] Chih-Ming Chen, Ming-Feng Tsai, Yu-Ching Lin, and Yi-Hsuan Yang. 2016.
Query-based music recommendations via preference embedding. In Proceedings
of the 10th ACM Conference on Recommender Systems. 79–82.
[4] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for
networks. In Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. 855–864.
[5] H. Gui, J. Liu, F. Tao, M. Jiang, B. Norick, and J. Han. 2016. Large-Scale embedding
learning in heterogeneous event data. In Proceedings of IEEE 16th International
Conference on Data Mining. 907–912.
[6] Quoc V Le and Tomas Mikolov. 2014. Distributed representations of sentences
and documents. In Proceedings of the 31st International Conference on Machine
Learning. 1188–1196.
[7] Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernockỳ, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Proceedings of
Interspeech. 1045–1048.
[8] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality. In
Advances in Neural Information Processing Systems 26. 3111–3119.
[9] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove:
Global vectors for word representation. In Proceedings of Conference on Empirical
Methods in Natural Language Processing, Vol. 14. 1532–1543.
[10] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. 701–710.
[11] Bryan Perozzi, Vivek Kulkarni, and Steven Skiena. 2016. Walklets: Multiscale graph embeddings for interpretable network classification. arXiv preprint
arXiv:1605.02115 (2016).
[12] Gunther Schmidt. 2011. Relational Mathematics. Vol. 132. Cambridge University
Press.
[13] Jian Tang, Meng Qu, and Qiaozhu Mei. 2015. PTE: Predictive text embedding
through large-scale heterogeneous text networks. In Proceedings of the 21th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.
1165–1174.
[14] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei.
2015. Line: Large-scale information network embedding. In Proceedings of the
24th International Conference on World Wide Web. 1067–1077.
[15] Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural deep network embedding. In Proceedings of the 22Nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining. 1225–1234.
[16] Suhang Wang, Jiliang Tang, Charu Aggarwal, and Huan Liu. 2016. Linked document embedding for classification. In Proceedings of the 25th ACM International
on Conference on Information and Knowledge Management. 115–124.
[17] Min Xie, Hongzhi Yin, Fanjiang Xu, Hao Wang, and Xiaofang Zhou. 2016. Graphbased metric embedding for next poi recommendation. In Proceedings of International Conference on Web Information Systems Engineering. 207–222.

CONCLUSION

This paper presents a novel item concept embedding approach
called the “ICE,” which aims to model item concepts via textual
information. In this approach, we provide a generalized network
construction method to build a network involving heterogeneous
nodes and a mixture of both homogeneous and heterogeneous relations. We then leverage the concept of neighborhood proximity to
learn the embeddings of both items and words. With the proposed
carefully designed ICE networks, the resulting embedding facilitates
both homogeneous and heterogeneous retrieval, including item-toitem and word-to-item search tasks. Experiments on two real-world
datasets show that ICE encodes useful textual information and thus
outperforms traditional methods in various item classification and
retrieval tasks. In the future, we plan to investigate how to integrate the proposed ICE method into the recommender systems to
advance contextual recommendation tasks.

5

ACKNOWLEDGEMENTS

The authors thank Chih-Ming Chen for his helpful comments and
thank KKBOX Inc. for providing the music dataset for investigation.
This research was partially supported by the Ministry of Science
and Technology in Taiwan under the grants MOST 105-2221-E-001035,104-2221-E-004-010, and 104-2420-H-004-033-MY3.

REFERENCES
[1] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003.
A neural probabilistic language model. Journal of Machine Learning Research 3

94

