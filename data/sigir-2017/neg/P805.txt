Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Combining Top-N Recommenders with Metasearch Algorithms
Daniel Valcarce

Javier Parapar

Álvaro Barreiro

Information Retrieval Lab
Department of Computer Science
University of A Coruña
daniel.valcarce@udc.es

Information Retrieval Lab
Department of Computer Science
University of A Coruña
javierparapar@udc.es

Information Retrieval Lab
Department of Computer Science
University of A Coruña
barreiro@udc.es

ABSTRACT

However, even with these constraints, several algorithms can meet
our requirements even if we constrain our search to collaborative
filtering approaches. In these circumstances, selecting one of them
may be a tough decision.
A possible solution for this problem could be as simple as using
multiple recommendation algorithms instead of relying only on one
method. However, this alternative poses its own challenge—how
should the algorithms be combined? We can distinguish two main
approaches: ensemble learning and unsupervised heuristic methods. The former strategies employ machine learning techniques to
combine algorithms obtaining a higher predictive capability while
the latter use a preconceived strategy. On the one hand, learning
approaches are more sophisticated and can potentially yield better
results at the expense of requiring training data and parameter
optimisation. On the other hand, heuristic methods, if used wisely,
can be quite cost-effective thanks to their simplicity.
Following the line of research of applying techniques from Information Retrieval to the Recommender Systems field [16, 18, 23],
we argue that the task of combining recommendation algorithms
is analogous to metasearch [2, 22]. Metasearch engines merge the
results of multiple search systems presenting a unique result list.
Meanwhile, in this paper, we aim to fuse the output of multiple
recommendation systems. In general, this problem of combining
ranked lists of elements is called rank aggregation. Rank aggregation is particularly useful for the top-N recommendation where we
care about the positions of the items in the ranking and not the
values of the predicted scores [8].

Given the diversity of recommendation algorithms, choosing one
technique is becoming increasingly difficult. In this paper, we explore methods for combining multiple recommendation approaches.
We studied rank aggregation methods that have been proposed for
the metasearch task (i.e., fusing the outputs of different search engines) but have never been applied to merge top-N recommender
systems. These methods require no training data nor parameter tuning. We analysed two families of methods: voting-based and scorebased approaches. These rank aggregation techniques yield significant improvements over state-of-the-art top-N recommenders.
In particular, score-based methods yielded good results; however,
some voting techniques were also competitive without using score
information, which may be unavailable in some recommendation
scenarios. The studied methods not only improve the state of the
art of recommendation algorithms but they are also simple and
efficient.

CCS CONCEPTS
• Information systems → Rank aggregation; Recommender
systems;

KEYWORDS
Recommender systems, metasearch, Borda count, Condorcet
ACM Reference format:
Daniel Valcarce, Javier Parapar, and Álvaro Barreiro. 2017. Combining TopN Recommenders with Metasearch Algorithms. In Proceedings of SIGIR ’17,
Shinjuku, Tokyo, Japan, August 07-11, 2017, 4 pages.
http://dx.doi.org/10.1145/3077136.3080647

1

1.1

Ensemble methods

Ensemble methods combine the outputs of multiple algorithms.
These methods have been applied to recommendation [4, 11] and
metasearch [22]; however, these learning algorithms usually need
the numerical scores given to the items in the ranked lists, not only
their ordinal positions. There exist numerous families of ensemble methods. We can distinguish three main strategies: bagging,
boosting and stacking/blending.
Both bagging and boosting are usually employed for building
single-model or homogeneous ensembles: they train the same algorithmic model on different data. Although they are not very
popular in the field of Recommender Systems, they have been applied with success for collaborative filtering [4]. On the one side,
bagging (also known as bootstrap aggregating) consists in training
models for different subsets taken from the training set. These subsets are bootstrap samples, i.e., random samples taken uniformly
with replacement. The prediction is computed taking the average
of the models for each sample. On the other hand, incrementally
boosting methods build an ensemble training weak learners making
emphasis on those examples in which the previous model failed.

INTRODUCTION AND BACKGROUND

Such is the fertility of the field of Recommender Systems that new
developments of recommendation algorithms emerge every month.
Each method usually has its particular advantages over the rest, and
there is no clear answer to which is the best one. We should prefer
one algorithm over others depending on multiple factors such as
the objective function or the available computational resources.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery.
ACM ISBN ACM ISBN 978-1-4503-5022-8/17/08. . . $15.00
http://dx.doi.org/10.1145/3077136.3080647

805

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 1: Algorithms for combination of scores. We refer to
the score that system k gave to item i by scorek (i) and ni
indicates the number of systems that have a score for i.

Finally, stacking or blending approaches learn a combiner model
for the underlying algorithms. The blend is based on a combiner
model whose inputs are the outputs of the underlying learning
algorithms. Blending has shown to be very successful in rating
prediction recommendation; for example, the winner solution of
the Netflix Prize contest was a blend of more than a hundred recommender systems [11]. Also, blending methods such as linear
regression have been thoroughly studied for metasearch [22].

1.2

CombANZ
CombSum
CombMNZ

Í
score(i) = ni−1 k scorek (i)
Í
score(i) = k scorek (i)
Í
score(i) = ni k scorek (i)

two variants of Condorcet voting for the task of rank aggregation
in collaborative filtering. This task can be assimilated to a voting
procedure. We can identify the ranked output of a recommender
system as a voting ballot while the candidates of the election are the
items susceptible of being recommended. Following this approach,
we applied Borda Count and Condorcet voting systems.
2.1.1 Borda Count. This method devised by Borda [5] assigns a
decreasing score to the candidates according to their position in the
voting ballots. In our case, the last item of a recommender list gets
zero points; the next one gets one point, etc. We rank the items by
the sum of the points awarded by each recommendation algorithm.
Aslam and Montague [2] exploited this strategy for metasearch
obtaining good results.
2.1.2 Condorcet. The Condorcet voting method chooses as a
winner (called Condorcet winner) the candidate who would win (or
at least tie) against every other candidate in a one-to-one election
[9]. A candidate A wins against another candidate B if A is placed
over B in more voting ballots than B over A. Since we need a ranked
list of elements, we can compute the next element in the ranking
removing the previous Condorcet winner. Iterating through this
process is equivalent to the graph-based approach proposed by
Montague and Aslam for metasearch [14].
One problem of Condorcet voting is that it may lead to many
ties (called voting paradoxes [9]). In [14], the authors broke ties
using quicksort which is an unstable sorting algorithm. We call
this randomised approach simply Condorcet. However, alternatives
to address these ties have been proposed. In this paper, we used
Copeland’s method for computing the Condorcet winner [7]: in
the case of a tie, sort the tied elements by the number of victories
minus the number of defeats against the other candidates. Even
with this approach, ties may happen, but more rarely. In these cases,
we break ties at random.

METASEARCH ALGORITHMS FOR
RECOMMENDATION

Metasearch is a well-studied task in Information Retrieval [2, 10,
14, 22]. In this paper, we study two families of methods: voting
algorithms and techniques based on the combination of scores.
The voting procedures are based on the positions in the ranking;
therefore, scores are ignored. Additionally, they have theoretical
properties based on Social Choice Theory. However, score-based
techniques may yield better results since the use of scores constitutes extra information. Next, we describe the studied techniques.
To foster reproducibility, we also provide the source code of our
implementation1 .

2.1

Formula

Heuristic Methods

In contrast, unsupervised heuristic methods do not require training
a model. Therefore, they avoid the need for training data and the
parameter optimisation process which can be critical depending on
the ensemble algorithm. Additionally, some heuristic methods rely
solely on the ordinal positions of the elements in the ranking. This
fact would be an advantage if we do not have access to the scores.
In particular, in this paper, we want to explore some heuristic
methods that have been used for metasearch with good results
[2, 10, 14, 22]. Although some of them have been used for the group
recommendation task [3, 6], to the best of our knowledge, these
metasearch algorithms have never been applied to combine the
output of different systems in the top-N recommendation task. In
the next section, we describe in detail these metasearch methods
and their application to aggregate multiple recommender systems
producing a unified ranking for the top-N recommendation task.

2

Algorithm

Voting systems

The study of the combination of individual opinions to reach an
agreement—a collective choice—receives the name of Social Choice
Theory. Arrow coined this term in 1951 and founded the basis of
the modern theory [1]; however, its origins date from the end of the
18th century, just a few years before the French Revolution. JeanCharles de Borda and the Marquis de Condorcet were the pioneers
who studied different voting systems for choosing among several
candidates [5, 9]. They established two of the principal families of
preferential or ranked voting systems we have nowadays.
Social Choice Theory principles have guided the development
of group recommendation techniques [3, 6]; however, we do not
aim to fuse the recommendations of different users; instead, we
seek to merge the suggestions offered to the same user by various
algorithms. In this paper, we explore the Borda Count method and

2.2

Combination of scores

In the Information Retrieval field, a traditional family of metasearch
techniques are the score combination methods suggested by Fox
and Shaw [10]. These simple yet effective techniques combine the
scores given to the items by different recommender systems. In this
paper, we analyse CombANZ, CombSum and CombMNZ algorithms
[10] (their formulas are gathered in Table 1).
2.2.1 Score Normalisation. Score-based methods depend heavily
on the absolute values of the scores. This dependency is not desirable because each recommender system may produce its output in
a different range. For example, one algorithm can be a probabilistic

1 https://github.com/dvalcarce/metarecsys

806

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

technique whose outputs range from 0 to 1 (representing an estimated probability of relevance) and another algorithm may be a
sparse linear method whose output is unbounded. For dealing with
this issue, we must apply a score normalisation technique. In particular, we explored some alternatives already used for metasearch
[13]:

0.45
0.44

nDCG@10

0.43

Standard Sets the minimum to 0 and the maximum to 1.
Sum Sets the minimum to 0 and the sum to 1.
ZMUV Sets the mean to 0 and the variance to 1.
ZMUV+1 Sets the mean to 1 and the variance to 1.
ZMUV+2 Sets the mean to 2 and the variance to 1.

combSum
combMNZ
combANZ
Copeland
Condorcet
Borda Count

0.41
0.40
0.39

Since we assign a score of 0 to non-recommended items, we
explored shifting the mean of ZMUV to avoid giving a relatively
high score to non-suggested items.

3

0.42

0.38

1

2

3

4

5

6

7

8

9

10

#systems

EXPERIMENTS

Figure 1: Values of nDCG@10 on the MovieLens dataset for
the best combination of 1 to 10 recommendation algorithms
using different metasearch techniques. Score-based methods uses standard normalisation.

We focused on collaborative filtering approaches because they are
quite popular; however, these rank aggregation algorithms can be
applied to any ranked list of items independently of the recommendation techniques. We fine-tuned the following algorithms on the
MovieLens 100k2 and Yahoo! Webscope R3 Music3 collections: Hitting Time (HT) [24], SVD++ [12], UIR-Item [23], CHI2-NMLE [20],
RM2-L-PD [16, 19], BPRMF [17], NNCosNgbr [8], LM-DP-WSR [21],
PureSVD [8] and SLIM [15]. We used the training and test splits
provided by the datasets. The training splits are used to train each
method on each dataset and the test splits are used for evaluating
the performance of the recommenders and the aggregations.
We combined the rankings of the top-100 recommendations
generated by these algorithms using the techniques from the previous section. For the score-based methods, we found that the standard normalisation works best on the MovieLens dataset and the
ZMUV+1 normalisation on the Yahoo! collection (bear in mind that
we are dealing with two datasets from different domains).
Figures 1 and 2 show the nDCG@10 (normalised cumulative
discounted gain) values of the best combination of algorithms from
1 (the single best algorithm) to 10 (fusing all the approaches) on
the MovieLens and Yahoo! datasets, respectively. The optimal combination of recommender systems on MovieLens is composed by
SLIM, PureSVD, BPRMF and RM2; on the Yahoo! dataset, the best
fusion is built by SLIM, BPRMF, CHI2, HT and SVD++ methods. On
both datasets, the performance of the fusion improves with more
algorithms until the optimal point. Then, including more systems
only adds noise to the aggregation worsening the quality of the
results.
In general, score-based methods outperformed voting techniques.
CombSum was the best approach for combining scores in every
scenario. This outcome is consistent with the results of metasearch
in Information Retrieval [10]. Among the voting techniques, Borda
is the worst technique while the behaviour of the simple Condorcet
is very volatile (remember that this method broke ties at random).
However, using Copeland’s rule, we obtained nearly the same values
of nDCG@10 as with the score-based methods. Condorcet is also
reported to yield better results than Borda for metasearch [14].

0.031
0.030

nDCG@10

0.029
0.028

combSum
combMNZ
combANZ
Copeland
Condorcet
Borda Count

0.027
0.026
0.025

1

2

3

4

5

6

7

8

9

10

#systems

Figure 2: Values of nDCG@10 on the Yahoo! dataset for the
best combination of 1 to 10 recommendation algorithms using different metasearch techniques. Score-based methods
uses standard normalisation.

Table 2 shows the best nDCG@10 values of each recommender
alone on the MovieLens and Yahoo! datasets. Additionally, we
present the value of the best combination on each dataset. A remarkable result is that the best aggregation outperforms all the methods
alone, however, it is not only composed by the best algorithms
regarding nDCG. This fact paves the way for future investigation
to determine which algorithms should be part of the fusion. Clearly,
precision measures such as nDCG are not enough, and we should
analyse other aspects such as the complementarity of the algorithms
among themselves.
To highlight the potential of rank aggregating methods, we conducted an additional experiment. We built the best aggregations
of recommendation algorithms that do not include SLIM (which

2 http://grouplens.org/datasets/movielens
3 http://research.yahoo.com/Academic_Relations

807

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 2: Best nDCG@10 values for Movielens 100k and Yahoo! collections for each recommender algorithm and for
the best aggregation. Statistically significant improvements
(Wilcoxon two-sided test p < 0.01) w.r.t. HT, SVD++, UIR,
CHI2, RM2, BPRMF, NNCosNgbr, WSR, PureSVD and SLIM
are superscripted with a, b, c, d, e, f , д, h, i and j, respectively.
Algorithm

MovieLens

Yahoo!

HT
SVD++
UIR-Item
CHI2-NMLE
RM2-L-PD
BPRMF
NNCosNgbr
LM-DP-WSR
PureSVD
SLIM

0.0123
0.1182a
0.2180ab
0.3659abc
0.3784abcd
0.3869abcde
0.3889abcde
0.4017abcde f д
0.4152abcde f дh
0.4221abcde f дhi

0.0164
0.0142
0.0174b
0.0270abci
0.0272abci
0.0278abci
0.0274abci
0.0277abci
0.0233abc
0.0301abcdef i

Best aggregation

0.4436abcde f дhi

0.0310abcde f дhi j

( “Centro Singular de Investigación de Galicia” accreditation 20162019 ED431G/01). The first author also acknowledges the support
of the “Ministerio de Educación, Cultura y Deporte” of the Government of Spain (grant FPU014/01724).

REFERENCES
[1] Kenneth J. Arrow. 1963. Social Choice and Individual Values (2nd ed.). Yale
University Press.
[2] Javed A. Aslam and Mark Montague. 2001. Models for Metasearch. In SIGIR
’01. ACM, New York, NY, USA, 276–284. DOI:http://dx.doi.org/10.1145/383952.
384007
[3] Linas Baltrunas, Tadas Makcinskas, and Francesco Ricci. 2010. Group Recommendations with Rank Aggregation and Collaborative Filtering. In RecSys ’10. ACM,
New York, NY, USA, 119–126. DOI:http://dx.doi.org/10.1145/1864708.1864733
[4] Ariel Bar, Lior Rokach, Guy Shani, Bracha Shapira, and Alon Schclar. 2011. Improving Simple Collaborative Filtering Models Using Ensemble Methods. In MCS
2013, Vol. 7872. Springer, 1–12. DOI:http://dx.doi.org/10.1007/978-3-642-38067-9
[5] Jean C. Borda. 1781. Mémoire sur les Élections au Scrutin. Paris. 657–664 pages.
[6] Iván Cantador and Pablo Castells. 2012. Group Recommender Systems: New
Perspectives in the Social Web. In Recommender Systems for the Social Web.
Vol. 32. Springer, 139–157. DOI:http://dx.doi.org/10.1007/978-3-642-25694-3
[7] Arthur Herbert Copeland. 1951. A reasonable social welfare function. University
of Michigan.
[8] Paolo Cremonesi, Yehuda Koren, and Roberto Turrin. 2010. Performance of
Recommender Algorithms on Top-N Recommendation Tasks. In RecSys ’10. ACM,
New York, NY, USA, 39–46. DOI:http://dx.doi.org/10.1145/1864708.1864721
[9] Marquis de Condorcet. 1785. Essai sur l’application de l’analyse à la probabilité
des décisions rendus à la pluralité des voix. Imprimerie Royale, Paris.
[10] Edward A. Fox and Joseph A. Shaw. 1995. Combination of Multiple Searches. In
TREC-2. 243–252.
[11] Michael Jahrer, Andreas Töscher, and Robert Legenstein. 2010. Combining
Predictions for Accurate Recommender Systems. In KDD ’10. ACM, New York,
NY, USA, 693–702. DOI:http://dx.doi.org/10.1145/1835804.1835893
[12] Yehuda Koren. 2008. Factorization Meets the Neighborhood: a Multifaceted
Collaborative Filtering Model. In KDD ’08. ACM, New York, NY, USA, 426–434.
DOI:http://dx.doi.org/10.1145/1401890.1401944
[13] Mark Montague and Javed A. Aslam. 2001. Relevance Score Normalization
for Metasearch. In CIKM ’01. ACM, New York, NY, USA, 427–433. DOI:http:
//dx.doi.org/10.1145/502585.502657
[14] Mark Montague and Javed A Aslam. 2002. Condorcet Fusion for Improved
Retrieval. In CIKM ’02. ACM, New York, NY, USA, 538–548. DOI:http://dx.doi.
org/10.1145/584792.584881
[15] Xia Ning and George Karypis. 2011. SLIM: Sparse Linear Methods for Top-N
Recommender Systems. In ICDM ’11. IEEE Computer Society, 497–506. DOI:
http://dx.doi.org/10.1109/ICDM.2011.134
[16] Javier Parapar, Alejandro Bellogín, Pablo Castells, and Álvaro Barreiro. 2013.
Relevance-Based Language Modelling for Recommender Systems. Information
Processing & Management 49, 4 (2013), 966–980. DOI:http://dx.doi.org/10.1016/j.
ipm.2013.03.001
[17] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.
2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. In UAI ’09.
AUAI Press, 452–461. http://dl.acm.org/citation.cfm?id=1795114.1795167
[18] Daniel Valcarce. 2015. Exploring Statistical Language Models for Recommender
Systems. In RecSys ’15. ACM, New York, NY, USA, 375–378. DOI:http://dx.doi.
org/10.1145/2792838.2796547
[19] Daniel Valcarce, Javier Parapar, and Álvaro Barreiro. 2015. A Study of Priors
for Relevance-Based Language Modelling of Recommender Systems. In RecSys
’15. ACM, New York, NY, USA, 237–240. DOI:http://dx.doi.org/10.1145/2792838.
2799677
[20] Daniel Valcarce, Javier Parapar, and Álvaro Barreiro. 2016. Efficient PseudoRelevance Feedback Methods for Collaborative Filtering Recommendation. In
ECIR ’16. Springer, 602–613. DOI:http://dx.doi.org/10.1007/978-3-319-30671-1_
44
[21] Daniel Valcarce, Javier Parapar, and Álvaro Barreiro. 2016. Language Models
for Collaborative Filtering Neighbourhoods. In ECIR ’16. Springer, 614–625. DOI:
http://dx.doi.org/10.1007/978-3-319-30671-1_45
[22] Christopher C. Vogt. 1999. Adaptive Combination of Evidence for Information
Retrieval. PhD thesis, University of California.
[23] Jun Wang, Arjen P. de Vries, and Marcel J. T. Reinders. 2006. A User-Item
Relevance Model for Log-Based Collaborative Filtering. In ECIR ’06. Vol. 3936.
Springer, London, UK, 37–48. DOI:http://dx.doi.org/10.1007/11735106_5
[24] Hongzhi Yin, Bin Cui, Jing Li, Junjie Yao, and Chen Chen. 2012. Challenging
the Long Tail Recommendation. Proc. VLDB Endow. 5, 9 (2012), 896–907. DOI:
http://dx.doi.org/10.14778/2311906.2311916

is the best algorithm alone) on each dataset. This aggregation outperformed SLIM on the MovieLens dataset with a nDCG@10 value
of 0.4366. On the Yahoo! collection, there exists a fusion that is
statistically equivalent to SLIM (nDCG@10 value of 0.0299). On
both collections, the meta-recommender notably surpassed the participating methods. This result is very promising: we can create
new powerful recommenders without the need of using the best
algorithms.

4

CONCLUSIONS

We have shown the effectiveness of different unsupervised metasearch algorithms applied to recommendation techniques. We studied voting techniques (Borda and Condorcet variants) and score
combination methods. No examined algorithm requires training
data to fuse the rankings of the recommenders systems. Furthermore, voting techniques also ignore scores. We found that CombSum method was the best; however, if we lack scores, Copeland’s
rule is a nearly effective method. Our methods produced recommendations that outperformed state-of-the-art recommenders. As
future work, it would be interesting to study how to select which
recommendation algorithms should we merge instead of analysing
all the possible combinations. The idea behind combining recommender systems is they may suggest similar relevant items but
different non-relevant items. Therefore, we think that not only
precision but also other aspects such as diversity and novelty are
crucial for choosing the best possible combination.

ACKNOWLEDGMENTS
This work has received financial support from the i) “Ministerio
de Economía y Competitividad” of the Government of Spain and
the ERDF (project TIN2015-64282-R), ii) Xunta de Galicia – “Consellería de Cultura, Educación e Ordenación Universitaria” (project
GPC ED431B 2016/035), and iii) Xunta de Galicia – “Consellería
de Cultura, Educación e Ordenación Universitaria” and the ERDF

808

