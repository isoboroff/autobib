Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Personalized Response Generation via Domain adaptation
Min Yang

Zhou Zhao

Wei Zhao

Xiaojun Chen

Tencent AI Lab
min.yang1129@gmail.com

Zhejiang University
zhaozhou@zju.edu.cn

Tencent
weizhao@tencent.com

Shenzhen University
xjchen@szu.edu.cn

Jia Zhu

Lianqiang Zhou

South China Normal University
Tencent AI Lab
jzhu@m.scnu.edu.cn
tomcatzhou@tencent.com

ABSTRACT

CCS CONCEPTS
• Natural language processing → Response generation;

KEYWORDS
Response generation, Domain adaptation, Reinforcement learning

INTRODUCTION

Conversational system (also called dialogue system) has become
increasingly important in a large variety of applications, such as
e-commerce, technical support services, entertaining chatbots, and
information retrieval dialogue systems. Generally, we have two
different kinds of conversational systems: goal-oriented and nongoal-oriented (open domain) conversational system. In this paper,
we focus on the non-goal-oriented conversational system. Instead
of multiple rounds of conversation, we only consider one round of
conversation, in which each round is formed by two short texts,
with the former being an input (referred to as post) and the latter a
response. Hopefully, we expect our work of one round conversation
to help understand the intricate mechanism behind the natural
language conversation.
∗ Zigang

IIE, Chinese Academy of Sciences
caozigang@iie.ac.cn

Inspired by recent success of recurrent neural network (RNN) in
statistical machine translation, most non-goal-oriented conversational systems employ sequence-to-sequence (seq2seq) framework
to generate responses [6, 7]. In general, the seq2seq model firstly
uses an encoder to summarize the post as a vector representation,
and then it feeds this representation into a decoder to generate the
response. Despite the remarkable progress of existing conversational systems, generating personalized responses still remains a
challenge.
Some recent studies [2, 13–15] show that the user-specific information (e.g., identity, age, gender, personal information) is valuable,
since it is directly related to the content and the style of user’s
responses, which may further impact the chatting process and the
user experience. Arguably, personalization plays an important role
in improving conversational system and making it closer to the
user’s actual information requirement. However, it is difficult to
train a personalized conversational system because the data collected from each individual is often insufficient. In particular, if a
user has only a few conversations, the generated responses often
have fluency or even grammatical problems. One way to solve this
problem is to consider a large collection of general training data as
a source domain and the personalized data as the target domain,
and perform transfer learning from the source domain to the target
domain. Namely, we can learn common conversation knowledge
from the source domain and then adapts this knowledge to the
target user.
To address the above challenge, in this paper, we propose a novel
personalized response generation model via domain adaptation
(PRG-DM). Firstly, we pre-train the response generation model
with an attention LSTM encoder decoder architecture on a large
scale general training data (without user-specific information). Secondly, we fine-tune the model on a small size of personalized data
(with user-specific information) to generate personalized responses
with a dual learning mechanism. Moreover, we propose three new
rewards to characterize good conversations that are personalized, informative and grammatical. We employ the policy gradient method
to generate highly rewarded responses.
The most related work is [14], which uses one single seq2seq
model on both the source domain and the target domain. Our approach differs from [14] in several aspects. First, we generate personalized responses with a dual learning mechanism, which extract
promising reward signals for reinforcement learning. Second, in
order to generate personalized responses, we formulate the userspecific information as a vector representation, and then we feed it
directly into the LSTM decoder as an additional input. Third, we

In this paper, we propose a novel personalized response generation
model via domain adaptation (PRG-DM). First, we learn the human
responding style from large general data (without user-specific
information). Second, we fine tune the model on a small size of
personalized data to generate personalized responses with a dual
learning mechanism. Moreover, we propose three new rewards to
characterize good conversations that are personalized, informative
and grammatical. We employ the policy gradient method to generate highly rewarded responses. Experimental results show that
our model can generate better personalized responses for different
users.

1

Zigang Cao∗

Cao is the corresponding author.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00
DOI: http://dx.doi.org/10.1145/3077136.3080706

1021

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

propose three new rewards rewards (i.e., reconstruction reward
and language model reward) to characterize good conversations.

2

For long sequences, the last state of the LSTM encoder may
not reflect important information seen at the beginning of the
sequence. Thus, we adopt the bidirectional LSTM, which runs two
chains: one forward through the input and another backward, i.e.
reversing the tokens in the input sentence. In step t, we summarize
the information in the forward and backward LSTM hidden states
→
− ←
−
by taking the concatenation of the two LSTMs ht = [ht , ht ].

RELATED WORK

Inspired by recent success of RNNs in statistical machine translation,
there are a number of studies attempting to extend the neural
language model to the field of dialogue modeling. [7] and [6] made
use of sequence-to-sequence (seq2seq) model to learn response in
short conversations.
There has been increasing interest in applying deep reinforcement learning to dialogue generation [3, 5]. For example, [3] proposed a model which learned a policy by optimizing the long-term
reward from ongoing dialogue simulations using policy gradient
methods [8], rather than the MLE objective defined in standard
seq2seq models. On the other hand, dual learning [9] has been
proposed to improve the neural machine translation.
Recently, several studies have been proposed to capture personal
characteristics and handle the information consistency of a user
[2, 5, 11, 12, 14]. [2] integrated the speaker embedding and word
embedding into a sequence to sequence learning framework. [14]
extended the traditional encoder decoder approach to personalized
response generation by introducing a domain adaptation scheme,
namely initialization-then-adaptation. [5] proposed a transfer learning framework based on POMDP to learn a personalized dialogue
system.

3

3.1.2 LSTM Decoder. Following [1], we use the attention signals
to determine which part of the hidden representation h should be
emphasized during the generation process. The LSTM decoder
is essentially a LSTM language model except conditional on the
context input c = {c 1 , . . . , cTy }. The generation probability of the
i-th word is calculated by
P (yi |y1 , . . . , yi−1 , x) = д(yi−1 , si , c i )

where д is a LSTM decoder, and st is the hidden state of LSTM
decoder at time t, computed by
si = f (yi−1 , si−1 , c i )

(3)

The context vector c i is then computed as a weighted sum of
these annotations ht :
Tx
X
ci =
α i,t ht
(4)
t =1

The weight α i,t of each annotation ht is computed by
exp(ei,t )
α i,t = PT
; ei,t = σ (si−1 , ht )
x
exp(ei,k )
k=1

PERSONALIZED RESPONSE GENERATION

(5)

where σ is a feed-forward neural network, which maps a vector
to a real valued score. This attention mechanism α i,t models the
alignment between the input sequence at position t and the output
at position i.
We use the post-response pairs and response-post pairs in D s
to pre-train the post agent (denote its parameter as θ A ) and the
response agent (denote its parameter as θ B ) respectively, though
the attention LSTM encoder decoder model. The response agent is
trained using the same model as that of the post agent, with posts
and responses interchanged.

In this section, we elaborate our personalized response generation
system.
We use D s to denote the collection of general post-response
pairs data (source domain data) that do not contain user specific
information, and use D t to denote the collection of personalized
post-response pairs data (target domain data) that contain user
specific information. We use p and r to represent the post and the
response, respectively.
Suppose we have two agents (post agent A and response agent B)
that can generate responses based on posts and generate posts based
on responses, respectively. These two agents are pre-trained on the
post-response pairs and response-post pairs in general dataset D s .
Our goal is to fine-tune the post agent and the response agent on the
personalized dataset D t . Specifically, we expect to learn common
conversation knowledge from the source domain and then adapt
this knowledge to the target user.

3.1

(2)

3.2

Model adaptation

After obtaining the post agent and the response agent that learn
common conversation knowledge from general dataset D s , in this
section, we aim to exploit user-specific information in personalized
dataset D t to improve the performance of our personalized conversational system. Inspired by the success of dual learning in [9],
we leverage the duality of the post agent (primal) and the response
agent (dual). The primal and dual tasks can form a closed loop, and
generate informative feedback signals to train the conversational
system, even with a small number of personalized data.
Starting form a post p in D t , we first generate a middle response
srmid with the post agent, and then further generate the post back
with the response agent. By evaluating this two-hop generation
results, we will get a sense about the quality of the post agent and
the response agent, and be able to improve them accordingly. This
process can be iterated for many rounds until both agents converge,
via policy gradient. We denote the post agent and the response agent
as P (srmid |p; θ A ) and P (p|srmid ; θ B ), respectively.

Model Pre-training

LSTM encoder decoder model is originally described in [7], which
first uses an encoder to summarizes the input as a vector representation, then it feeds this representation into a decoder to generate
the output.
3.1.1 LSTM Encoder. The LSTM encoder converts the input
sequence x = (w 1 , . . . , wTx ) into a sequence of hidden states h =
{h 1 , . . . , hTx }:
ht = f (x t , ht −1 )
(1)
where ht ∈ Rn is a hidden state of LSTM encoder at time t, and we
use LSTM as f .

1022

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

3.2.1 Personalized LSTM decoder. To generate personalized responses, in model adaptation step, we explicitly calculate the user
vector u for each user based on the user-specific information (e.g.,
identity, age, gender, personal information), and directly feed it into
the LSTM decoder as an additional input. We describe the process
of building user vector representation below.

natural the output response is in the language. The language model
reward for the response r is:
(LM )

rA

= LM (srm id )

(6)

We train the language model with LSTM on Wikipedia data. Then
the language model is fixed and the log likelihood of a received
message is used to reward the output utterance.
Finally, we simply adopt a linear combination of the reconstruction reward r (r ec ) and language model reward r (LM ) as the total
reward for post agent A:

User Vector Representation . In response generation, user-specific
information (e.g., age, gender, job, education) play an important role
in generating personalized responses [10]. We represent each user
as a vector representation which encodes user-specific information.
In this project, we mainly employ the following user attributes:
User identity, Age (we divide age into five evenly spaced classes),
Gender (female, male or None), Education, Location. We convert
these user specific information into a user vector using a one-hot
representation, denoted as u. Since the one-hot representation of the
user profile suffers from data sparsity, we further transform the user
profile vector to a user embedding u e which is low-dimensional and
dense by using the matrix-vector product: u e = W e u. The matrix
W e ∈ Rde ×|u | is a parameter to be learned, and the size of each
column de is a hyperparameter. The user profile vector is directly
incorporated into the decoder as an additional input.

r A = γ 1r (r ec ) + γ 2r (LM )

(7)

where γ 1 + γ 2 = 1, and we set γ 1 = 0.5 and γ 2 = 0.5.
3.2.7 Updating model parameters. We can explore the stateaction space and learn the policies P (srmid |p; θ A ) and P (p|srmid ; θ B )
that leads to the optimal expected reward E[r ]. According to the policy gradient theorem [8], we compute the gradient of the expected
reward E[r ] with respect to parameters θ A and θ B :
∇θ A E[r ] = E[r A ∇θ A log P (srmid |p; θ A )]

(8)

∇θ B E[r ] = E[λ 1 ∇θ B log P (p|srmid ; θ B )]

(9)

3.2.2 Dual learning with policy gradient . In this section, we
propose a policy gradient [8] reinforcement learning algorithm
to optimize long-term rewards of the post agent and the response
agent. We first introduce the state, action, policy and reward of the
reinforcement learning architecture below.

The reader can refer to [8] for details of policy gradient theorem.
We update the parameters using stochastic gradient descent.

3.2.3 State. In time t, a state s is denoted by the input sequence
of the post agent or the response agent.

In the experiments, we first pre-train the attention LSTM encoder
decoder using a large scale general training data (without user
specific information), and then fine-tunes the model on the small
size of personalized training data (with user specific information).
The detailed properties of the general dataset and the personalized
dataset are described as follow.
Sina Weibo conversation data without user information (Sinasource): We use the Sina Weibo dataset introduced by [6] as our
general training data. On Sina Weibo 1 , a user can post short messages (referred to as posts in this paper) visible to the public or a
group of users following her/him, and other users make comments
on the published posts ( referred to as responses). This dataset is
a collection of 4,435,959 post-response pairs (with 219,905 posts)
crawled from Sina Weibo.
Sina Weibo conversation data with user information (Sinatarget): We write a crawler to grab tweets from Sina Weibo as our
personalized dataset. Following the strategy used in [6], we first
crawl a large number of post-response pairs, and then clean the raw
data. Finally, we obtain 51,921 post-response pairs (with 6028 posts).
We randomly choose 1000 post-response pairs (with 178 posts)
as our test set, and treat the remaining as training set. For each
involved user, we crawled his/her profile information such as user
id, province ID user located, city ID user located, user description,
gender.
For both dataset, we use Stanford Chinese word segmenter 2 to
split the posts and responses into sequences of words. To reduce
data sparsity, we construct two separate vocabularies for posts and

4 EXPERIMENTS
4.1 Datasets description

3.2.4 Policy. We employ the stochastic policy gradient method
to approximate a stochastic policy directly using an independent
function approximator with its own parameters. In this paper, the
policies for the post agent and the response agent are P (srmid |p; θ A )
and P (p|srmid ; θ B ), whose inputs are the representations of the
states, whose outputs are action selection probabilities.
3.2.5 Action. An action a in timestep k is the sequence to generate (srmid or p) by the policy (P (srmid |p; θ A ) or P (p|srmid ; θ B )).
3.2.6 Reward. The policy gradient algorithm is a type of reinforcement learning method, which relies upon optimizing parametrized
policy with respect to the expected return (long-term cumulative
reward) by gradient descent. We assume that our model receives a
reward r at each iteration. We discuss the major factors that contribute to the reward for post agent A below, and denote the total
reward as r A . Similarly, we can get the reward r B for response agent
B in the dual learning procedure.
Reconstruction reward. For dual learning, given the generated
middle response srmid by P (srmid |p; θ A ), we use the log probability
of post p recovered from srmid by P (p|srmid ; θ B ) as the reward of
the reconstruction. Mathematically, we define the reconstruction
reward as r A (r ec ) = log P (p|srmid ; θ B ).
Language model reward. To generate grammatical responses, we
have an immediate language model reward r (LM ) , indicating how

1 http://www.weibo.com
2 http://nlp.stanford.edu/software/segmenter.shtml

1023

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

ID
Post

responses by using 20,000 most frequent words on each side. All
the remaining words are replaced by a special token “UNK”.

4.2

Resp 1

Baseline Methods

In the experiments, we evaluate and compare our model with several baseline methods: Seq2Seq model [7], Neural Responding Machine (NRM) [6], Speaker-Addressee model (SA) [2], Deep Reinforcement Learning (RL) [3], Personalized Response Model (PRM)
[14].

Resp 2

4.3

Resp 5

Resp 3
Resp 4

Implementation details

In this paper, the word embeddings are initialized using 300-dimensional
word2vec [4] trained on Chinese Wikipedia Dumps, and these word
embeddings are fine-tuned during model training. We initialized
the recurrent parameter matrices as orthogonal matrices, and all
other parameters from a Gaussian random distribution with mean
zero and standard deviation 0.01. We set the encoder and decoder
hidden state space sizes as 300. We conduct mini-batch training
with Adadelta optimization method.

4.4

4.4.1 Quantitative evaluation. Following the same evaluation
as in [2], we firstly compare our model with baseline methods in
terms of BLEU score, perplexity and human evaluation.
Seq2Seq
1.03%
40.1
0.892

NRM
1.12%
38.5
0.932

SA
1.15%
37.7
1.096

questions in Table 2. We randomly select 5 users from the test
dataset, and integrate their user vector representations into our
personalized LSTM decoder. The model tends to generate specific
responses for different people in response to the factual questions.

RL
1.23%
35.3
1.152

PRM
1.19%
36.1
1.158

CONCLUSION AND FUTURE WORK

In this paper, we proposed a novel personalized response generation model via domain adaptation. We first pre-trained the response
generation model with an attention LSTM encoder decoder architecture on a large scale general training data, and then fine-tuned
the model on a small size of personalized data with a dual learning
mechanism. In the future, we plan to devote our effort to adapt our
model to task-oriented dialogue systems.

In this section, we compare our model with baseline methods from
quantitative and qualitative perspectives.

Evaluation
BLEU
Perplexity
Human

Conversation 2
我要戒烟,你信不信?
I will quit smoking,do you believe?
等着瞧
Wait and see
好的
It’s good
怎么会?
How could be?
真的,你确定?
Really, are you sure?
哈哈,要有信心
Haha, have confidence

Table 2: Some responses generated for five different users to
two real case questions.

5

Experiment results

Conversation 1
我的人生可能是假的
My life may be fake
我睡了
I am sleeping
我晕了
I feel faint
太深奥了
It is too abstruse
互相关注
let’s follow each other
累并快乐
Tired and very happy

PRG-DM
1.32%
34.2
1.192

REFERENCES
[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine
translation by jointly learning to align and translate. Preprint arXiv:1409.0473
(2014).
[2] Jiwei Li, Michel Galley, Chris Brockett, Georgios P Spithourakis, Jianfeng Gao,
and Bill Dolan. 2016. A persona-based neural conversation model. Preprint
arXiv:1603.06155 (2016).
[3] Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. 2016. Deep reinforcement learning for dialogue generation. Preprint
arXiv:1606.01541 (2016).
[4] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality. In
NIPS. 3111–3119.
[5] Kaixiang Mo, Shuangyin Li, Yu Zhang, Jiajun Li, and Qiang Yang. 2016. Personalizing a Dialogue System with Transfer Learning. Preprint arXiv:1610.02891
(2016).
[6] Lifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neural Responding Machine
for Short-Text Conversation. Computer Science (2015).
[7] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning
with neural networks. In NIPS. 3104–3112.
[8] Ronald J Williams. 1992. Simple statistical gradient-following algorithms for
connectionist reinforcement learning. Machine learning 8, 3-4 (1992), 229–256.
[9] Yingce Xia, Di He, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, and Wei-Ying
Ma. 2016. Dual Learning for Machine Translation. Preprint arXiv:1611.00179
(2016).
[10] Min Yang, Jincheng Mei, Fei Xu, Wenting Tu, and Ziyu Lu. 2016. Discovering
Author Interest Evolution in Topic Modeling. In SIGIR. 801–804.
[11] Min Yang, Wenting Tu, Wenpeng Yin, and Ziyu Lu. 2015. Deep Markov Neural
Network for Sequential Data Classification. In ACL, Vol. 2. 32–37.
[12] Min Yang, Fei Xu, and Kam-Pui Chow. 2016. Interest Profiling for Security
Monitoring and Forensic Investigation. In ACISP. 457–464.
[13] Min Yang, Dingju Zhu, Yong Tang, and Jingxuan Wang. 2017. Authorship
Attribution with Topic Drift Model. AAAI (2017).
[14] Weinan Zhang, Ting Liu, Yifa Wang, and Qingfu Zhu. 2017. Neural Personalized
Response Generation as Domain Adaptation. Preprint arXiv:1701.02073 (2017).
[15] Zhou Zhao, Lijun Zhang, Xiaofei He, and Wilfred Ng. 2015. Expert finding for
question answering via graph regularized matrix completion. TKDE (2015).

Table 1: Quantitative evaluation results.
BLEU scores has been shown to correlate well with human judgment on the response generation task. Table 1 (second line) shows
the BLEU scores. Our PRG-DM model achieves BLEU value of 1.32%
on the test data, which is 0.09% higher than the best BLEU value of
baseline methods(i.e., RL).
Perplexity is a measurement of how well a probability model
predicts a sample text. The lower the perplexity, the better the
model. We calculate the average perplexity (log-likelihood) of the
responses. We summarize the perplexity results in Table 1 (third
line). PRG-DM substantially outperforms other models.
We also adopt human annotation to compare the performance of
different models. We generate the response for each post in the test
data. Three researchers who work on natural language processing
are invited to do human evaluation. Three levels are assigned to a
response with scores from 0 to 2, where 2 denotes suitable response,
1 denotes neutral response, and 0 denotes unsuitable response. The
experimental results based on human annotation are summarized
in Table 1 (fourth line). PRG-DM outperforms other methods. For
example, the average score of PRG-DM is 0.034 higher than the best
results of baselines (i.e., PRM) on the test data.
4.4.2 Qualitative evaluation. To evaluate the proposed model
qualitatively, we show the generated responses for two real case

1024

