Demonstration Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Smart Media Generation System for Broadcasting Contents
Jeong-Woo Son

Smart Media Research Group, ETRI
218 Gajeong-ro, Yuseong-gu
Daejeon, Korea
jwson@etri.re.kr

Wonjoo Park

Smart Media Research Group, ETRI
218 Gajeong-ro, Yuseong-gu
Daejeon, Korea
wjpark@etri.re.kr

Jinwoo Kim

Sun-Joong Kim

Korean Broadcasting System
13 Yeouigongwon-ro,
Yeongdeungpo-gu
Seoul, Korea
starseeker@kbs.co.kr

Smart Media Research Group, ETRI
218 Gajeong-ro, Yuseong-gu
Daejeon, Korea
kimsj@etri.re.kr
reveal implicit connections. Such efforts can be explained with the
fact that connectivities among contents are essential information
for services in content retrieval and recommendation. Basically,
different kinds of connections provide different services.
This paper proposes a novel system to generate various connections among broadcasting contents. Broadcasting contents have
been the typical way to obtain information and to enjoy entertatinment for several decades. Thus, a number of broadcasting contents
are already produced. These broadcasting contents contain much
valuable information on diverse topics, and in some case, they contain unique information. As a result, most broadcasting stations and
content providers have demanded a system to analyze broadcasting
content and to establish various connectivities among contents.
The proposed system divides the broadcasting content into smaller
units ‘scene’, when it is given a broadcasting content with its closed
captions. A broadcasting content can be regarded as a mass of
semantics or stories. Thus, for constructing accurate connections,
scenes are determined to disentangle a part of semantics or stories
from the entire content. The proposed system adopts a new scene
boundary detection method using a multi-view spectral clustering.
In the proposed system, scenes are connected with automatically
generated information like ‘keyword’, ‘topic’, and ‘story’. Topics
are estimated with a non-parametric topic model for short text and
keywords are generated from topics. On the other hand, a story is
constructed with words and their relations observed in the closed
captions of a scene, Connections among scenes are determined with
shared keywords and similarities between topics. In case of ‘story’,
two scenes can be connected if they have consistent stories. The
consistency between stories is computed with a graph kernel [9].
Connections have different gradualities with respect to their
accuracy and coverage. For instance, a keyword can make more
connections than other information, however it has lower accuracy
than those from a topic and a story. With different gradualities
in connections, the proposed system can provide information that
meets the needs from diverse services. A ‘keyword’ based content
retrieval is a simple and reasonable example to represent connections, while ‘topic’ based content retrieval and ‘story’ based content
re-organizing services also can be realized with our system. In this
paper, two services are introduced as simple use-cases of our system: ‘keyword based VoD (Video on Demand) clip’ and ‘topic based

ABSTRACT
Broadcasting contents are the most plausiable resources for services
with video contents. Even though we already have huge amount
of produced broadcasting contents, there rarely exists a system
to analyze and generate information on broadcasting contents to
support content retrieval and recommendation services. This paper
proposes a new system for this purpose. In the proposed system,
a broadcasting content is segmented into semantic units, scenes,
based on its multiple characteristics. The proposed system analyzes
scenes and generates their keywords, topics, and stories. Connections among scenes are automatically establishing based on shared
keywords, similar topics, and consistency in stories. To support
operaters, the proposed system offers two tools: Scene Studio and
SceneViz. We prepare several Open APIs in the proposed system
to provide information and connections for service providers. The
feasibility of the proposed system is shown with numerical evaluations on the qualities of generated information. We also introduce
two video clip services implemented with our system.

CCS CONCEPTS
•Information systems → Multimedia information systems;
Multimedia content creation;

KEYWORDS
Broadcasting content; Smart media; Content mining; Scene segmentation; Topic model

1

Sang-Yun Lee

Smart Media Research Group, ETRI
218 Gajeong-ro, Yuseong-gu
Daejeon, Korea
syllee@etri.re.kr

INTRODUCTION

Contents like web pages, images, and videos are connected with
each other based on some proxy information such as keywords,
hyper-link, objects, and so on. Even though authors did not give any
explicit connection, it was usually assumed implicit connections
among contents and various techniques have been proposed to
ACM acknowledges that this contribution was authored or co-authored by an employee,
contractor or affiliate of a national government. As such, the Government retains a
nonexclusive, royalty-free right to publish or reproduce this article, or to allow others
to do so, for Government purposes only.
SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00
DOI: http://dx.doi.org/10.1145/3077136.3084139

1297

Demonstration Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

clip recommendation’ services. The functionality in both services
can explain the advantage of the proposed system efficiently.

2

3.1

RELATED WORK

The proposed system is related with both scene boundary detection and topic model technically. Scene boundary detection is a
well-known problem in content analysis and it has been studied
with diverse view points. Recently, clustering methods are applied
to tie similar shots in scenes. Baraldi et. al [1] proposed a deep
siamese network to detect scene boundaries in broadcasting contents. In this work, audio and text features in shots are embedded
in a low-dimensional space by using a deep siamese network and
then, a spectral clustering is applied to estimate scene boundaries
with the embedded shots. According to [3], shots should be represented with multiple aspects like visual, audio, and text. Thus,
to adopt information in shots, a scene boundary detection method
needs to an ability to handle multi-view data. Son et al. proposed a
modified spectral clustering to handle complementary information
in multi-view data [10]. They designed a brainstorming process to
encourage an explicit information sharing among different views
rather than combining multiple views to minimize a disagreement.
The proposed system adopts the spectral clustering with the brainstorming process to handle multiple aspects in shots and to detect
scene boundaries more accurately.
A topic model is one of the popular techniques for modeling
text data and the proposed system adopts one of well-known topic
models, hierarchical dirichlet process (HDP) [11]. When HDP is
applied into the script text, we must face with a problem caused
by short lengths of text. To resolve this problem, text in scenes is
expanded with topical dependencies between scenes. There exist
several studies for this purpose. Du et al. [4] proposed a method
of topic modeling that exploits document relative similarities as
a regularizer. They maximized the model log-likelihood with the
constraint where the distance among similar documents should be
much smaller with certain margin than the distance among dissimilar documents. In [6], hyper-linked texts on the web are modeled
with some additional random variables. Instead of assuming direct
dependency between topic distributions of web pages, they introduced the latent link variable generates a link and the latent link
variable is dependent on the topic distibutions of other web pages.
Existing methods enhanced the topic quality based on document
similarities in corpus. However, since text in scenes are too short to
derive strong dependencies with ordinary methods, we determined
dependencies among scenes by using topical similarities directly.

3

Scene Segmentation

An operator starts the analyzing process by giving a broadcasting
content, its closed caption, and the size of scenes. SMGS extracts
shots from the broadcasting content. Shots are determined with
similarities between adjacent frames and variations of similarities
on local timeline [8]. For each shot, twenty three features are extracted to represent a shot. HS (Hue and Saturation) histogram,
gray intencity, HoG (Histogram of Gradients), and average motion
vectors are extracted as visual features. As audio features, SMGS
uses eighteen features such as Bark/Mel/ERB bands, spectral peaks,
MFCC (Mel-Frequency Cepstral Coefficients), GFCC (Gammatone
Feature Cepstrum Coefficients), pitch salience, and so on. When
shots are extracted, closed captions can be assigned to shots by using tagged timeline positions. Thus, the text feature is constructed
with the words extracted from closed captions and their frequencies. In case of shots without any closed caption, SMGS estimates
frequencies by using an exponential smoothing [5].
Scenes are determined by using a multi-view spectral clustering.
To handle multiple features more effectively, SMGS adopts the
spectral clustering with the brainstorming process proposed by
[10]. In SMGS, similarities are computed by using a gaussian kernel
with local variances [12]. Views are constucted by tying similar
kernel matrixes in seven groups. When seven kernel matrixes
are ratified, the brainstorming process and spectral clustering are
performed to generate scenes based on information in seven views.

3.2

Establishing Connections among Scenes

SMGS represents a scene with its own meta-data composed of selfcontained information (keywords, topics, and story), and two kinds
of connections (topic based and story based connections). A scene
may contain several sentences from the closed caption, and thus,
it can be regarded as a document to train a topic model. Thus,
after a set of scenes are constructed, thirty to fourty documents are
forthcoming. SMGS trains a topic model for the given broadcasting
content to generate both topics and keywords. However, unlike
general documents in a text domain, the closed caption composed
of several sentences is too short to train a model.
To overcome this problem, we designed a modified version of
Hierarchical Dirichlet Process (HDP) [11]. In our HDP, documents
are expanded with other documents with similar topics. The topic
model is re-trained with expanded documents. This process is
iteratively performed to obtain rich topics. Through training the
topic model, scenes are naturally tagged with some topics by using
timelines in closed captions. Keywords for scenes are generated
from their topics. Connections among scenes are established with
shared keywords and average cosine similarities between topics.
SMGS represents a story of a scene with a set of graphs. Thus,
n where n is the
a scene s can be represented with G s = {G si }i=0
number of shots in s. When all words in shots are extracted, binary relations between words are determined if two words are
appeared together in a sentence. Then, the weights in a graph
are adjusted with the trainsition probabilities from one node to
other nodes based on random walks between them. In the weight
estimation, all relations belong to s are used to reflect implicit relations among words. A shot graph is constructed with words
appeared in the shot and weights between the words. G s is used

SMART MEDIA GENERATION SYSTEM

The broadcasting contents analyzed by the proposed system can
represent itself with its keywords, topics, and story, further the
meta-data contains relations from the broadcasting content to the
others. To distinguish these analyzed content from ordinary one,
we named it as ‘Smart Media’ content and the proposed system as
‘Smart Media Generation System (SMGS)’. In this section, we introduce SMGS with its own analyzing techniques and functionalities
to handle smart media contents.

1298

Demonstration Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 1: Performances on scene boundary detection

ARI (%)

SV
62.3

KS
59.7

KP
50.2

CSC
78.9

SMGS
85.7

Table 2: Perplexities of experimental methods

perplexity

GPU
4.3392

SMGS
3.1232

constructed with connections among smart media contents. SMGS
gives SceneViz for the operator to support information verification.
SceneViz is a web based graph visualization tool for smart media.
Figure 2 shows the screen shot of SceneViz. SceneViz has mainly
two components: graph visualization (the left side) and information
enumeration (the right side). In the graph visualization part, nodes
denote programs (red circle), episodes (green circle), and scenes
(blue circle), while edges show their weighted relations. The information enumeration shows detail information for the selected node
such as topics (red box), keywords (orange box), story (blue box),
and its clip (green box). The operator can check all the connections
among scenes by selecting an item in the information enumeration. This figure shows the situation when the operator select one
keyword (green line) and one topic (orange and purple lines) We
translated the selected topic and keyword manually.

Figure 1: The screenshot of Scene studio

4

Figure 2: The screenshot of sceneViz

EVALUATIONS AND APPLICATIONS

SMGS is verified qualitatively and quantitatively. For the qualitative
verification, Qualities of detected scene boundaries and topics are
measured with Korean TV-series and documentaries. Two use-cases
are introduced for quantitative evaluation.

to make direct connections with other scenes. When two scenes
s and t are given, the strength of the connections are determined
with both (K(G s , G t ) and K(G t , G s )), where K(G s , G t ) is defined
Í
n−m+i , G i ). Here, Z is a normalization factor, while
as Z · m
t
i=0 k(G s
k() is a random walk graph kernel to compare two graphs [9].

3.3

LDA
3.4855

4.1

Evaluations

The performance on the scene boundary detection is evaluated
with two Korean TV programs with ten episodes (‘Descendants
of the Sun’ and ‘Mysteries of the Human Body’). For all contents,
two human annotators generate scene boundaries independently
and the goldstandards are constructed with commonly annotated
one. The average number of scenes is 16.7 with 216 seconds. We
compared four ordinary methods: single view clustering, kernel
summation, kernel production, CSC (Co-training approach for Spectral Clustering) [7], and the proposed system. The performances
are measured with ARI (Adjusted Rand Index). Table 1 shows the
experimental results. As shown in this table, SMGS achieves the
best performance, 85.7% of ARI. In many cases, only one or two
shots are wrongly assigned to surrounding scenes.
The qualities of generated topics are determined with the first
episode of the Korean TV series ‘Heard it through the grapevine’. it
contains 55 scenes with 24 average words. We used first 45 scenes to
train a topic model, while remained scenes are applied to measure
the performances, perplexity. SMGS is compared with LDA (Latent
Dirichlet Allocation) [2] and GPU (Generalized Pólya urn Model)
[4]. Table 2 shows the experimental result. Parameters in LDA
and GPU are determined as in [2] and [4]. As shown in this table,
SMGS shows 3.1232 of perplexity and it is better than others. This

Scene Studio and SceneViz

Both the scene segmentation and meta-data generation are performed systematically with Scene studio. Scene studio is a tool
to support an operator to convert ordinary broadcasting contents
to smart media contents. Figure 1 shows the screenshot of Scene
studio. As shown in this figure, Scene studio notices the processing
status with several icons (in green box): input, shot extraction,
scene segmentation (including story generation), topic and keyword annotation, and meta-data generation. An operator needs to
put the broadcasting content, the closed caption, and the size of
scenes. Automatic information generation in SMGS takes about
five minutes without encoding video clips for scenes. The metadata repository is designed to restore program-, episode-, scene-,
and shot-level information. SMGS supplies the generated metadata by using thirteen Open APIs like selectSceneListWithKeyword,
getSceneMeta, and so on.
Scene studio is designed by focusing on performing sequential
processes in SMGS. Thus, it does not have any funtionality to visualize the generated information effectively. An operator needs
to check the information in generated meta-data and the topology

1299

Demonstration Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Figure 4: Screenshots of topic based clip recommendation

5

Figure 3: The screenshot of the keyword based VoD clip

This paper introduces SMGS to analyze broadcasting contents.
SMGS divides a broadcasting contents into smaller semantic units
and connects them with various information like topics, keywords,
and story. SMGS adopts novel scene segmentation and topic generation techniques. In this paper, the qualities of these techniques
are evaluated with real Korean TV series and documentaries. The
feasibility of SMGS is verified with two use cases: the keyword
based VoD Clip and the topic based Clip recommendation. For all
evaluations, SMGS is proved its effectiveness to mine information
in broadcasting contents and to provide the information for service
providers.

result implies the effectiveness of SMGS to construct topics for
broadcasting contents.

4.2

Keyword based VoD Clip

Keywords and connections established with shared keywords are
the general information to utilize for a clip service. To show the feasibility of SMGS, a VoD Clip service is implemented with generated
keywords. Figure 3 shows the screen shot of the VoD Clip service.
In the VoD Clip service, each episode is composed of several scenes.
A user can watch a scene clip with its keywords. When the user selects a keyword, the system can suggest scenes which the keyword
belongs to. This is a typical example of a clip service with keyword
based content retrieval. By using Open APIs of SMGS, this kind of
services can be easily developed.

4.3

CONCLUSIONS

ACKNOWLEDGMENTS
This work was supported by ICT R&D program of MSIP/IITP. [20150-00219 , Development of smart broadcast service platform based
on semantic cluster to build an open-media ecosystem]

REFERENCES
[1] Lorenzo Baraldi, Costantino Grana, and Rita Cucchiara. 2015. A deep siamese
network for scene detection in broadcast videos. In Proc. of the 23rd ACM MM.
1199–1202.
[2] David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. JMLR 3, Jan (2003), 993–1022.
[3] Manfred Del Fabro and Laszlo Böszörmenyi. 2013. State-of-the-art and future
challenges in video scene detection: a survey. Multimedia systems 19, 5 (2013),
427–454.
[4] Jianguang Du, Jing Jiang, Dandan Song, and Lejian Liao. 2015. Topic modeling
with document relative similarities. In Proc. of the 24th AAAI. 3469–3475.
[5] Everette S Gardner. 1985. Exponential smoothing: The state of the art. Journal
of forecasting 4, 1 (1985), 1–28.
[6] Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2008. Latent topic models for
hypertext. (2008), 230–239.
[7] A. Kumar and H. Daumé III. 2011. A Co-training Approach for Multiview Spectral
Clustering. In Proc. of ICML. 393–400.
[8] Xiaoxiao Luo, Qing Xu, Mateu Sbert, and Klaus Schoeffmann. 2014. F-divergences
driven video key frame extraction. In Proc. of ICME 2014. 1–6.
[9] Alexander J Smola and Risi Kondor. 2003. Kernels and regularization on graphs.
In Learning theory and kernel machines. Springer, 144–158.
[10] Jeong-Woo Son, Junkey Jeon, Alex Lee, and Sun-Joong Kim. 2017. Spectral
clustering with brainstorming process for multi-view data. In Proc. of the 31st
AAAI.
[11] Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. 2004. Sharing
Clusters among Related Groups: Hierarchical Dirichlet Processes.. In Advances
in NIPS 17. 1385–1392.
[12] L. Zelnik-Manor and P. Perona. 2004. Self-Tuning Spectral Clustering. In Advances
in NIPS 17. 1601–1608.

Topic based Clip Recommendation

Korean Broadcasting System launches a β-test of a topic based clip
recommendation by adopting SMGS’ meta-data. Figure 4 shows
the screen of the service. In this service, all episodes of Korean TV
documentary ‘Mysteries of the Human Body’ is served as scene
clips to audiences by using a second screen device. The information
from SMGS is used to suggest related scenes to audiences based on
topical relevances. For example, an audience watches a scene about
diabetes and eating habits, the service can recommend other scenes
with diabetes and eating habits. To support this service, SMGS
supplies information like topics belong to a scene, similar topics in
other scenes, and rankings of scenes based on topic similarities via
Open APIs. We performes a survey about satisfaction of the fifteen
β-testers. The questionnaire contains thirteen questions on usability
of the service. Among them, four questions are directly related
with the performance of SMGS such as scene boundary quality,
accuracies of tagged topics, and satisfactions on recommended
scenes. We achieved about 76.36% of satisfaction on average.

1300

