Session 1C: Document Representation and Content Analysis 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Stacking Bagged and Boosted Forests
for Effective Automated Classification∗
Raphael Campos, Sérgio Canuto, Thiago Salles, Clebson C. A. de Sá and Marcos André Gonçalves
Federal University of Minas Gerais
Computer Science Department
Av. Antônio Carlos 6627 - ICEx
Belo Horizonte, Brazil
{rcampos,sergiodaniel,tsalles,clebsonc,mgoncalv}@dcc.ufmg.br

ABSTRACT

KEYWORDS

Random Forest (RF) is one of the most successful strategies for automated classification tasks. Motivated by the RF success, recently
proposed RF-based classification approaches leverage the central RF
idea of aggregating a large number of low-correlated trees, which
are inherently parallelizable and provide exceptional generalization
capabilities. In this context, this work brings several new contributions to this line of research. First, we propose a new RF-based
strategy (BERT) that applies the boosting technique in bags of extremely randomized trees. Second, we empirically demonstrate
that this new strategy, as well as the recently proposed BROOF and
LazyNN RF classifiers do complement each other, motivating us
to stack them to produce an even more effective classifier. Up to
our knowledge, this is the first strategy to effectively combine the
three main ensemble strategies: stacking, bagging (the cornerstone
of RFs) and boosting. Finally, we exploit the efficient and unbiased
stacking strategy based on out-of-bag (OOB) samples to considerably speedup the very costly training process of the stacking
procedure. Our experiments in several datasets covering two highdimensional and noisy domains of topic and sentiment classification
provide strong evidence in favor of the benefits of our RF-based
solutions. We show that BERT is among the top performers in the
vast majority of analyzed cases, while retaining the unique benefits
of RF classifiers (explainability, parallelization, easiness of parameterization). We also show that stacking only the recently proposed
RF-based classifiers and BERT using our OOB-based strategy is not
only significantly faster than recently proposed stacking strategies
(up to six times) but also much more effective, with gains up to 21%
and 17% on MacroF1 and MicroF1 , respectively, over the best base
method, and of 5% and 6% over a stacking of traditional methods,
performing no worse than a complete stacking of methods at a
much lower computational effort.

Classification; Ensemble; Bagging; Boosting; Stacking

1

INTRODUCTION

Since the advent of the Web, the amount of data available has grown
unprecedentedly. Thus, organizing and extracting useful information from this enormous amount of data has become an important
(if not vital) task for industry and society. By using machine learning techniques to automatically associate documents with classes,
Automatic Text Classification (ATC) provides means to organize
information, allowing better comprehension and interpretation of
the data [1]. Many important applications, such as topic categorization, sentiment analysis, spam filtering, language identification,
recommender systems, among others, can be effectively and efficiently solved by automatic textual classifiers. Despite the wide
applicability of ATC, text classification brings its own challenges,
such as high dimensionality and the presence of noise [14]. Properly handling these issues is of great importance to guarantee high
classification effectiveness. This is the central topic of this work.
Several machine learning techniques aimed at tackling the challenging ATC problem have been proposed. In particular, ensembles
of classifiers have been shown to excel in this situation [4, 6, 23],
enjoying high effectiveness in this domain. In particular, Random
Forest (RF) is one of the most successful classifier ensembles in a
wide variety of classification tasks [8]. Despite being a classifier
with great generalization power, it has been shown that RF models
may suffer from overfitting issues [24], having their effectiveness
degraded in the presence of many irrelevant or noisy attributes—a
characteristic of textual classification tasks. Recently, some RFbased classifiers were shown to achieve state-of-the-art results
in text classification by exploiting distinct strategies to mitigate
the overfitting issue faced by RF, namely, a lazy RF version called
LazyNN RF [22], and a boosted RF strategy named BROOF [23].
Both methods learn classification models focusing on specific subregions of the input space, hoping to filter out irrelevant attributes
and data—the primary factors that contribute to RF’s overfitting.
In this work, we advance the state-of-the-art in text classification by proposing a novel derivation of the RF classifier. More
specifically, we propose a new boosted version of the RF classifier, based on some ideas explored by the BROOF classifier: the
so-called Boosted Extremely Randomized Trees (BERT) classifier.
While BROOF is able to mitigate the overfitting issue faced by the RF
classifier when applied to high-dimensional noisy data, by avoiding
the generation of overly complex trees, it offers limited capability

∗ This

work was partially funded by projects InWeb (grant MCT/CNPq 573871/20086)
and MASWeb (grant FAPEMIG/PRONEX APQ-01400-14), and by the authors’ individual grants from CNPq, FAPEMIG, Capes and Google Inc.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00
DOI: http://dx.doi.org/10.1145/3077136.3080815

105

Session 1C: Document Representation and Content Analysis 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

of bias reduction (through the so-called selective out-of-bag based
weight update strategy). Thus, bias may still pose as an important
factor to contribute to the error rate. To tackle this potential issue,
we here propose to introduce another source of randomization in
the boosted strategy proposed in [23] in order to better control the
learner’s bias, by applying the BROOF-like strategies into the Extremely Randomized Trees (Extra-Trees) classifier [10]. This novel
strategy has the following motivations: (i) we expect to avoid overly
complex models (and thus mitigate overfitting) through the application of the BROOF-like strategies; (ii) to provide more control over
the learner’s bias through the additional randomization offered by
the Extra-Trees classifier; and, finally, (iii) to exploit the fact that
the Extremely Randomized was shown to be more robust to noise
than the RF classifier. As we shall see, our proposed classifier outperforms the analyzed state-of-the-art classifiers, including SVM,
kNN, Naı̈ve Bayes, BROOF, and LazyNN RF, often by large margins.
This is the first main contribution of this work.
Moreover, motivated by the fact that distinct learning methods
may complement each other, uncovering specific structures that
underlie the input/output relationship of the data at hand, in this
work we also propose to exploit the complementary characteristics
of the recently proposed RF-based approaches and ours, by stacking
them in order to learn an even more effective meta-classifier. As we
shall see later, their level of disagreement is high, which motivates
our idea. Up to our knowledge, this is the first attempt to combine
the three main ensemble strategies: bagging, boosting and stacking.
According to our experiments, the meta-classifier that exploits only
RF-based techniques is able to achieve gains of up to 21% and 17%
in MicroF1 and MacroF1 , respectively, over the best base method,
and of 5% and 6% over a stacking of traditional method, being
the top performer in all analyzed cases. This is the second main
contribution of this work.
Finally, when stacking classifiers, one usually relies on k fold
cross-validation procedures to estimate the a posteriori class probabilities for each example, to serve as input for the meta-classifier.
Based on these predicted a posteriori class distribution estimates,
the meta-classifier induces a relationship between these predictions
and the true class. However, such estimation strategy may be very
costly and sometimes ineffective, since it depends on learning k
different models to estimate the probability distributions that serve
as input for the stacking procedure. In order to cope with this
problem, we here propose to exploit the efficient and unbiased outof-bag (OOB) error estimate, an out-of-the-box estimate naturally
produced by the bootstrap procedure used in each RF-based learner.
Thus, we avoid additional computation efforts to learn a stacked
classifier. According to our findings, this strategy considerably
speeds up the training process of stacking while preserving its high
effectiveness. This is our third main contribution.
In summary, the main contributions of this work are: (i) the
proposal of a novel RF-based classifier, named BERT, that is able
to outperform state-of-the-art classifiers; (ii) the proposal of a new
stacking classifier that exploits the complementary characteristics
of BROOF, LazyNN RF and BERT that is able to outperform all analyzed classification algorithms, including a stacking of traditional
methods, often by large margins; (iii) the proposal of a new estimation strategy based on the exploitation of OOB for generating the
input for the stacked meta-classifier that substantially reduces the

computational effort/runtime of the stacking strategy while retaining its predictive power; and (iv) an extensive experimentation with
15 datasets in two domains – topic categorization and sentiment
analysis; – against several baselines including traditional classifiers
(to compare with BERT), several stacking combinations (to compare
with the stacking of Forests) and several state-of-the-art stackers
(to compare with our OOB-based approach).
Roadmap. Section 2 reviews some relevant literature related
to this work. In Section 3 we detail our proposed BERT classifier,
providing its motivations, learning strategy details, and extensive
experimental evaluation. Section 4 details our proposed method
to efficiently stack RF-based methods as well as its experimental
evaluation and analysis. Finally, in Section 5 we conclude the paper,
pointing out some possible directions for further investigation.

2

RELATED WORK

Ensembles of classifiers are an extensively studied machine learning
technique [21]. Among several learning methods used in practice,
ensembles are one of the most effective in several applications and
can be divided into two groups: ensembles of homogeneous and heterogeneous classifiers. The former relies on learning many versions
of the same classification technique, each one built by somehow disturbing the training set. Then, usually one averages the predictions
in order to come up with final decisions with higher generalization
capabilities. The latter combines a set of distinct learning methods
trained with the same training set and then producing the final
decisions according to the predictions made by these classifiers.
Regarding the text classification realm, in [7], the authors use
what they call Moderated Asymmetric Naı̈ve Bayes (MANB) as a
base learner in their homogeneous ensemble configuration. Several
homogeneous ensemble methods are contrasted, such as k-fold
partitioning, bagging and boosting, as well as a heterogeneous ensemble method which combines SVM and NB learning algorithms.
In [23], the authors combine two well-known homogeneous ensemble techniques, bagging and boosting, by exploiting Random
Forests (RF) as “weak learners” in the boosting framework. The combination is achieved by means of “smoothly” updating the weights
of only the out-of-bag instances. This combination mitigates the
overfitting issue faced by RF models in textual classification tasks
and leverages its generalization power. Recently, in [19] the authors empirically evaluate the effectiveness of ensemble learning
methods on textual documents represented by keywords. They
apply different keyword extraction strategies on the dataset. The
authors evaluate five different homogeneous ensemble methods
that use four different base classifiers. In [2, 4, 20], ensembles of
heterogeneous classifiers are proposed, by combining classical text
classification methods (e.g., SVM, kNN, NB and Rocchio). In all
analyzed cases, significant improvements were observed when compared to the traditional classifiers. Furthermore, a combination of
several distinct polarity classifiers for Twitter sentiment analysis is
proposed in [13]. Unlike these works, we here aim at combining
homogeneous and heterogeneous classifiers by stacking different
RF-based methods in an original manner.
There are two common techniques to combine predictions of
distinct classifiers, namely, fixed combining methods and trainable combining methods [18]. An advantage of applying fixed

106

Session 1C: Document Representation and Content Analysis 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

in the new scaled space. Recently, Nguyen et al. [18] proposed a
combining classifier based on the variational inference which is
based on the assumption that instances belonging to a given class of
the meta-level training set are drawn from a multivariate Gaussian
distribution. Thus, M distributions are estimated by using Bayesian
variational inference on the instances belonging to each class. The
predicted class for a new unseen observation is given by selecting
the label associated with maximum posterior probabilities computed by the M multivariate Gaussian models. All aforementioned
methods are used as baselines in our stacking experiments.
Moreover, all those methods rely on the meta-level training
sets obtained by costly procedures, such as cross-validation while
combining bagging-based methods. In contrast, we here propose to
take advantage of the out-of-bag (OOB) samples, naturally available
by the bagging procedure, to yield an unbiased meta-level training
set. This allows our solution to stack bagging-based methods with
any other learning method without additional computational cost
while using any combining methodology. In fact, some works
attempt to improve the bagging procedure by utilizing stacking
to combine the base classifiers [25, 26]. For instance, in [25] the
authors propose a variant of bagging, where stacking rather than
uniform majority voting is used to achieve the combination. The
meta-level training set is obtained by the predictions of the each
base learner over the original training set, which can lead the metaclassifier to overfit. In contrast to [25], in [26] the authors propose
a linear combination in which the coefficients are estimated by
the OOB error, thus generating a less biased combination. Those
methods differ from ours since here we construct the meta-level
data for bagging-based methods in a distinct manner, while also
treating them as black-boxes when combining them with any other
learning method.

methods for ensemble systems is that there are no need to train a
meta-classifier since they do not take into consideration the label
information in the meta-level training set when combining the
learners. Thus, they are less time-consuming than their counterparts. Many fixed combining methods can be found in literature,
such as Sum, Product, Vote, and Average.
On the other hand, trainable combining methods work on metalevel data, in order to learn the prediction model. Although exploiting metal-level data to extract knowledge usually enhances
classification effectiveness, it comes at the price of additional computational effort [18].
Perhaps, the most relevant studies about trainable combining
methods are based on Stacking (a.k.a. Blending), which was originally conceived as “Stacked Generalization” by Wolpert [27]. In
this case, a meta-level training set Dmet a is generated by applying
a cross-validation procedure, in which the original training set
Dt r ain is divided into T equally sized disjoint sets. Each base-level
classifier is learned by considering Dt r ain \ Ft as training set and
reserving Ft for testing. Formally, ∀i = 1, ..., K : ∀t = 1, ...,T :
Cit = Li (Dt r ain \ Ft ). Subsequently, each learned classifier Cit produces a estimation for the posterior probability pit (cm |x j ) that an
observation x j belongs to a class cm ; ∀x j ∈ Ft : Cit (x j ) = pit (x j ) =
(pit (c 1 |x j ), pit (c 2 |x j ), ..., pit (c M |x j )). The obtained meta-level training set Dmet a is thus:
 p1 (y1 |x 1 )
 p (y |x )
1 1 2


.

.


.
 p (y |x )
 1 1 N

...
...
..

.
...

p1 (y M |x 1 )
p1 (y M |x 2 )
.
.
.
p1 (y M |x N )

...
...
..

.
...

p K (y1 |x 1 )
p K (y1 |x 2 )
.
.
.
p K (y1 |x N )

...
...
..

.
...

p K (y M |x 1 )
p K (y M |x 2 )
.
.
.
p K (y M |x N )











Finally, a combining classifier is trained on the meta-level training
set and used to produce the final prediction.
Several methods have been designed to exploit label information
in the meta-level training set. In one strategy, the predictions of
classifiers are grouped according to the given classes and then a
template associated with each label is built. Three methods using
that strategy are Multiple Response Linear Regression (MLR) [25],
Decision Template (DT) [15] and the recently proposed VIG [18].
MLR assumes that each classifier weights differently each class. In
this case, the combining algorithm is based on the M linear combinations of the posterior class probabilities and the associated class
weights. The predicted class is then decided by selecting the maximum value among these combinations. The Decision Template
strategy [15] groups the meta-level training set according to the
classes of each instance. Then, Decision Templates are built by
averaging the meta-level instances observed in each class (forming
centroids). Subsequently, the predicted class is decided by selecting the class label of the Decision Template that is more similar
to the meta-data unlabeled observation. To this end, the authors
propose eleven similarity functions based on fuzzy logic. Due to
its simple computation, this method has low computational cost
in both training and testing. In [17] a combination of Stacking,
Correspondence Analysis (CA) and K-Nearest Neighbor (KNN) is
proposed, in the form of a single learning algorithm called SCANN.
The goal of such algorithm is to find the underlying relationship
between the learning observations and the predictions of the base
classifiers, by applying CA to an indicator matrix formed by the
learned meta-level instances and their corresponding true labels.
Then, a kNN procedure is employed to classify the unseen data

3

THE BERT CLASSIFIER

In this section, we detail BERT, a boosted version of the RF classifier
that aims at taking the advantages of BROOF while avoiding its
potential bias tendency. As we detail in the following, BERT introduces an additional source of randomization to reduce bias, while,
at the same time, exploiting the advantages of the boosting strategy,
in order to achieve superior generalization. We start our discussion
with a brief overview of the BROOF strategy. Then, we describe
the key building block for the BERT strategy, namely, Extremely
Randomized Trees. Finally, we describe BERT.

3.1

BROOF

Proposed in [23], the BROOF classifier combines boosting and bagging by exploiting RF as “weak learners” in a boosting framework.
Boosting is a sequential meta-algorithm which trains several “weak
learners” (that is, classifiers capable of yielding predictions better
than random guessing) in order to generate a more precise model.
For each iteration i of the boosting algorithm, let ∆i be a probability
distribution over the training set of size M. When i = 0, ∆0 (j) = M1 ,
M . At iteration i > 0, for all examples in the training set x , if x
∀j |j=1
j
j
is misclassified, its weight is incremented so that, in the succeeding
iteration, an updated distribution ∆i+1 is considered, which emphasizes the misclassified instances (e.g, the hardest to classify ones).
In contrast to AdaBoost [9], the update rule in BROOF uses the

107

Session 1C: Document Representation and Content Analysis 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

out-of-bag (OOB) samples, promptly generated by the RF classifier
at training time, as an unbiased error estimate to drive the boosting
iterations and “smoothly” reweights just the OOB instances.
In [23] the authors show that restricting the influence of training data to the hard-to-classify sub-regions of the input space, by
means of a smooth combination of random forests and boosting,
can mitigate the overfitting issue observed when learning decision
trees composing the ensemble classifier, thus leveraging classification effectiveness on new unseen data. Also, the selective weight
update strategy slows down boosting’s tendency to focus on just
a few hard-to-classify samples, thus offering some bias reduction
capabilities. These ideias were even extended and generalized to
realm of learning-to-rank (L2R) tasks [5].
However, although BROOF was shown to provide competitive
results in text classification and L2R tasks, this can be mainly attributed to variance reduction, since its limited bias reduction capability
may not be enough to fully mitigate its tendency to overly emphasize hard-to-classify examples, mainly in noisy environments. This
has to do with the underlying boosting strategy adopted by BROOF
and may compromise classification effectiveness. We here propose
to tackle the potentially high bias faced by BROOF by means of an
additional source of randomization, through the use of the so-called
extremely randomized trees, described in the following.

classifiers in all tested datasets, outperforming the original BROOF
in several cases
BERT combines boosting and Extra-Trees, by exploiting the
following BROOF-like strategies: (i) to use the out-of-bag (OOB)
error estimate as a less biased error estimation to drive the boosting
algorithm; and (ii) to only update the weights of the out-of-bag
instances during the boosting iterations. In order to smoothly
combine these ideas to the Extremely Randomized Trees framework,
we propose to introduce the bagging procedure into its training
procedure, in order to allow proper OOB error estimation.
We argue that exploring these strategies through this novel
classification framework brings two benefits: (i) it enables us to
minimize variance (mitigating the overfitting problem faced by the
trees composing the ensemble) and (ii) provide means to minimize
bias, through the additional randomization source, leveraging the
framework ability to avoid being stuck on a few hard-to-classify
examples. We summarize the proposed method in Algorithm 1.
Algorithm 1 BERT Pseudo Code
1:
2:
3:
4:

3.2

Extremely Randomized Trees

Extremely Randomized Trees (a.k.a., Extra-Trees) [10] is an ensemble of trees similar to the RF. Unlike RFs, Extra-Trees do not
bootstrap the training data when learning its composing trees. Instead, it uses the entire training set for doing so, relying on another
more aggressive source of randomization to learn decorrelated trees.
More specifically, while RF builds its trees by using bootstrapped
samples from the training set, defining the decision nodes in order to
optimize some information theoretic statistic (such as Information
Gain, Chi Squared or Gini Index), the Extra-Trees classifier defines
the decision nodes in a purely random fashion, thus guaranteeing
reduced tree correlation in the ensemble.
The Extra-Trees classifier has some benefits when compared to
RF classifiers. Besides yielding better classification effectiveness, it
has been shown that Extra-Trees is more robust to noise than RFs. In
this work, we take advantage of BROOF-like techniques and ExtraTrees, by proposing what we call BERT, a smooth combination of
both, as detailed next.

3.3

N }, M, n
function Train(Dt r ain = {(X i , yi )|i=1
t r ees )
1
L ← ∅ ; w 1 ← |D
t r ain |
for each m ∈ {1, .., M } do
XT , (x, y, ŷ)oob ) ← ExtraTrees(D
(hm
t r ain , n t r ees , wm )
i
w i I [y,ŷ]
i ∈O
Í m i
i ∈O w m
1−OO B ewr r
OO B ewr r
i ϵ αm I [y,ŷ]
wm

5:

OOBw
er r ←

6:

αm ← log(

7:

i
wm+1

←

Í

, where O = (x, y, ŷ)oob
i

)

Z

, where Z is a normalizing con-

stant
8:
9:
10:
11:

3.4

XT , α )}
L ← L ∪ {(hm
m
end for
return L
end function

Experimental Evaluation

We now report and discuss the conducted experimental evaluation
regarding the proposed BERT classifier considering a set of datasets
on topic categorization and sentiment analysis. We first detail the
explored datasets and the experimental setup. Then, we discuss the
obtained experimental results, comparing our proposal to state-ofthe-art classifiers.
3.4.1 Experimental Setup and Parameterization. In order to evaluate the BERT classifier considering the textual classification domain, we consider five real-world topic categorization data sets as
well as ten sentiment analysis ones, related to computer science
articles (ACM), news (REUT), web pages (4UNI), medicine (MEDLINE), items reviews (Amazon), posts on social networks (Twitter,
Debate), user comments (Youtube) and snippets of opinion news
(NYT). Due to space restrictions, a detailed description of each
dataset can be found in an online appendix1 . In all cases, we performed a traditional preprocessing task that consists of removing
stopwords (using the standard SMART list) and applying a simple
feature selection procedure, removing terms with low “document

Boosted Extremely Randomized Trees

Recall that boosting algorithms tend to overly emphasize hard-toclassify examples, mainly when applied to noisy data. Therefore,
the undesired bias towards these hard-to-classify examples is minimized by BROOF by updating only the probabilities ∆i related
to out-of-bag samples as proposed in [23], hoping to decrease the
misclassification rate when the “weak learner” is focused on hardto-classify regions. Hence, by using a “weak learner” more robust
to noisy data than Random Forests such as Extra-Trees [10], it is
expected to achieve better performance when focused on hard-toclassify regions, thus, producing a model with higher generalization
power. In fact, as we shall see, BERT is among the top performer

1 homepages.dcc.ufmg.br/∼rcampos/papers/sigir2017/appendix.pdf

108

Session 1C: Document Representation and Content Analysis 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

frequency (DF)”2 . Regarding term weighting, we tested TF, TF-IDF
and L2 normalization schemes, choosing the best strategy for each
classification approach. Particularly, we use TF for all classifiers
based on RF and Naive Bayes, and TF-IDF with L2 normalization
for both SVM and kNN. The explored classifiers were compared
using two standard information retrieval measures: micro averaged
F 1 (MicroF 1 ) and macro averaged F 1 (MacroF 1 ). While the MicroF1
measures the classification effectiveness overall decisions (i.e., the
pooled contingency tables of all classes), the MacroF1 measures the
classification effectiveness for each individual class and averages
them. To compare the average results of our 5-fold cross-validation
experiments, we assess their statistical significance by applying a
paired two-tailed t-test with 95% confidence and Bonferroni correction to account for multiple comparisons. This test assures that the
best results, marked in bold, are statistically superior to others (up
to the chosen confidence level).
We evaluate ten distinct learning algorithms. We use the scikitlearn implementation3 of linear SVM, k-Nearest Neighbors (kNN),
Multinomial Naı̈ve Bayes (NB), Random Forests (RF) and Extremely
Randomized Trees (Extra-Trees). We use our own implementation
of the RF-based methods, namely, BROOF, the lazy version of the
RF classifier (LAZY) and the lazy version of extra-trees (LXT), since
there is no freely available implementation for these classifiers.
The free parameters of these classifiers include the cost C (for
SVM), neighborhood size k (for KNN and LAZY) and the number of features considered in the split of a node on the RF-based
approaches. These free parameters were set using 5-fold crossvalidation within the training set. For the RF-based approaches,
each tree is grown without pruning, as suggested in [12], and since
the results obtained with 200, 300 and 500 trees are statistically tied
(with 95% confidence), we adopted 200 trees due to the lower cost.
Concerning the BROOF classifier, we use 8 weak learners, setting
the maximum number of iterations to 200, as suggested in [23]. We
use the same parameters for the proposed BERT method.
We would like to point out that some of the results obtained in
some datasets may differ from the ones reported in other works
for the same datasets. Such discrepancies may be due to several
factors such as differences in dataset preparation4 , the use of different splits of the datasets (e.g., some datasets have “default splits”
such as REUT and 20NG5 ). We would like to stress that we ran all
alternatives under the same conditions in all datasets, using the
best traditional feature weighting scheme, using standardized and
well-accepted cross-validation procedures that optimize parameters
for each of alternatives, and applying the proper statistical tools
for the analysis of the results. Our datasets are available (for result
replication and testing of new configurations) under request.

different classification paradigms. While BERT and other RF-based
approaches are based on extracting specific association rules that
relate different features, SVM measures the complexity of hypotheses based on the margin with which they separate the data, which
is independent of the number of features. This characteristic makes
the SVM as one of the best-known classification strategies to exploit
discriminative evidence from high dimensional and sparse textual
data. Even considering the specificities of each feature, BERT is
capable of combining small pieces of evidence to build a general
model with the same generalization power of SVM in scenarios
which SVM traditionally works the best.
Despite SVM and BERT obtain statistically tied results in most
datasets, BERT shows significantly superior results to SVM in the
SPAM dataset. For this particular dataset, the BERT capability of
identifying specific non-trivial relationships between individual
words to the spam category grants superior effectiveness for BERT.
On the other hand, the same SVM mechanism that captures good
general patterns limits such classifier in finding specific pieces of
evidence that relate individual features in more complex ways.
BERT is also superior to other RF-based approaches in most
datasets, which provides evidence towards the benefits of the proposed strategy in mitigating the RF overfitting problem. Specifically,
BERT presented significant improvements over RF on all datasets,
with gains up to 7% and 30% on MicroF1 and MacroF1 , respectively.
Other strategies designed to overcome the RF limitations also
presented inferior results to BERT. The LAZY method is always
inferior to BERT in terms of MacroF1 , indicating that LAZY has
a hard time on discriminating minor classes. We argue that by
trying to find discriminative patterns based on the neighborhood
of documents, LAZY can bias its model towards the larger classes,
since their documents are most likely to appear in the neighborhood
of an arbitrary test document.
Another strategy aimed at improving RFs is the BROOF classifier.
Our experimental results show that the proposed combination of
boosting with extremly randomized trees can achieve better results
than combining boosting with the original RFs. Although very
competitive to BERT, BROOF is no match to our proposal in both
the 20NG and ACM datasets.
Now we turn our attention to the classification results of the sentiment analysis task, presented in Table 2. The experimental results
show that, overall, BERT outperforms or ties with the best classification method for each dataset. Specifically, BERT and BROOF
achieved the best (and statistically tied) results in almost all datasets,
which provides additional evidence towards the benefits of reducing
the overfitting issues of RF-based methods using boosting. In the
sentiment analysis context, both methods take advantage of being
able to identify the presence of individual words that are highly
correlated to a positive or negative sentiment. Their generalization
capabilities of identifying noisy or irrelevant information provide
improvements over the original RF, with 14% and 5% on MacroF1
and MicroF1 , respectively.
In the sentiment analysis scenario, SVM is not always among the
best approaches, since it does not have the ability to discriminate the
fine-grained discriminative evidence present in individual words
and non-linear relationships between words. Moreover, there is
no clear winner between SVM and the simple NB classifier. In this
scenario, NB has the advantage of learning how each word relates to

3.4.2 Results and Discussions. We now turn our attention to the
obtained results regarding the described classifiers. We start by
considering the effectiveness of each classifier in the topic categorization task (see Table 1). In this case, BERT presents statistically
tied results with SVM on most datasets despite their fundamentally
removed all terms that occur in less than six documents (i.e., DF < 6).
in http://scikit-learn.org/
instance, some works do exploit complex feature weighting schemes or feature
selection mechanisms that do favor some algorithms in detriment to others.
5 We believe that running experiments only in the default splits is not the best experimental procedure as it does not allow a proper statistical treatment of the results.
2 We

3 Available

4 For

109

Session 1C: Document Representation and Content Analysis 1

BERT
SVM
BROOF
XGBOOST
LAZY
NB
KNN
Extra-Trees
LXT
RF

microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1

20NG
89.45 ± 0.46
89.13 ± 0.58
90.06 ± 0.43
89.93 ± 0.43
87.96 ± 0.24
87.44 ± 0.28
81.88 ± 0.65
81.38 ± 0.68
87.96 ± 0.37
87.39 ± 0.37
88.99 ± 0.54
88.68 ± 0.55
87.53 ± 0.69
87.22 ± 0.66
87.03 ± 0.41
86.65 ± 0.56
88.39 ± 0.51
88.05 ± 0.44
83.64 ± 0.29
83.08 ± 0.35

4UNI
84.61 ± 0.98
73.61 ± 1.85
83.48 ± 1.08
73.39 ± 2.17
84.41 ± 1.07
73.23 ± 1.10
85.52 ± 0.88
74.44 ± 1.28
82.34 ± 0.61
68.33 ± 1.6
62.63 ± 1.7
51.38 ± 3.19
75.63 ± 0.94
60.34 ± 1.36
82.87 ± 1.00
68.54 ± 2.60
81.24 ± 0.71
66.89 ± 1.23
81.52 ± 1
65.44 ± 1.91

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

ACM
74.8 ± 0.59
62.1 ± 0.99
75.4 ± 0.66
63.84 ± 0.55
73.35 ± 0.79
60.76 ± 0.8
73.78 ± 0.51
62.79 ± 1.38
74.02 ± 0.79
59.46 ± 1.35
73.54 ± 0.71
58.03 ± 0.85
70.99 ± 0.96
55.85 ± 0.97
73.08 ± 0.55
58.71 ± 0.89
69.63 ± 0.91
57.33 ± 1.48
71.05 ± 0.31
56.56 ± 0.45

REUT90
67.33 ± 0.72
29.24 ± 1.40
68.19 ± 1.15
31.95 ± 2.59
66.79 ± 0.97
28.48 ± 2.17
65.21 ± 0.84
29.06 ± 2.46
66.3 ± 1.07
26.61 ± 2.12
65.32 ± 1.13
27.86 ± 0.79
68.07 ± 1.07
29.93 ± 2.48
64.87 ± 0.81
26.18 ± 2.55
65.92 ± 0.82
26.71 ± 2.53
63.92 ± 0.81
24.36 ± 1.98

SPAM
96.11 ± 0.52
95.93 ± 0.55
92.55 ± 0.8
92.12 ± 0.87
96.09 ± 0.84
95.9 ± 0.88
95.08 ± 0.52
94.84 ± 0.54
92.91 ± 0.68
92.54 ± 0.71
79.27 ± 0.81
78.18 ± 0.83
83.31 ± 0.98
82.91 ± 0.93
95.74 ± 0.55
95.52 ± 0.58
92.42 ± 0.78
92.05 ± 0.82
95.46 ± 0.74
95.22 ± 0.79

MED
83.68 ± 0.32
74.25 ± 0.37
86.19 ± 0.05
78.46 ± 0.42
83.05 ± 0.05
73.25 ± 0.42
84.19 ± 0.05
76.81 ± 0.21
84.88 ± 0.08
72.90 ± 0.08
82.92 ± 0.14
63.8 ± 0.43
82.16 ± 0.08
68.00 ± 0.34
82.49 ± 0.07
71.15 ± 0.31
83.84 ± 0.11
71.02 ± 0.23
81.61 ± 0.05
70.41 ± 0.36

Table 1: Topic categorization - Obtained results for base classifiers.

BERT
BROOF
NB
XT
SVM
RF
XGBOOST
LXT
LAZY
KNN

microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1

AMAZON
75.76
74.16
74.52
73.02
74.68
73.32
73.88
72.1
74.24
72.84
73.8
71.76
71.49
69.98
72.22
69.24
72.3
69
69.86
67.64

BBC
86.04
53.78
86.17
55.04
86.84
46.48
86.58
52.35
86.43
50.34
86.84
52.4
86.16
52.39
87.11
49.36
87.37
50.19
86.44
46.36

DEBATE
79.38
77.24
79.23
77.07
76
73.7
79.33
76.93
78.58
76.2
78.53
75.79
75.89
73.01
78.07
74.98
76.35
72.89
74.03
72.25

DIGG
75.7
63.03
76.34
65.56
76.09
64.38
76.85
60.84
75.58
60.96
75.31
57.04
74.29
57.46
74.93
56.91
76.21
58.9
74.8
52.77

MYSPACE
86.46
67.35
86.57
67.93
85.5
60.39
85.64
65.24
86.23
67.94
85.74
60.7
85.38
63.57
85.26
55.64
85.14
55.55
85.49
56.07

NYT
68.14
67.01
68.01
66.85
67.1
66.06
67.43
66.03
66.34
65.48
67.43
65.41
61.06
60.08
64.41
61.5
64.64
61.61
60.61
54.92

TWEETS
88.3
85.49
88.11
85.36
86.82
84.4
86.46
83
86.87
84.1
84.63
79.54
84.24
80.11
82.36
75.35
80.86
73.53
77.34
68.14

TWITTER
76.15
74.71
75.01
73.73
74.88
73.89
75.19
73.26
74.53
73.15
73.83
71.21
72.47
69.01
71.91
68.51
70.86
67.49
67.02
64.08

YELP
94.26
94.26
93.58
93.58
90.44
90.44
91.74
91.73
92.94
92.94
90.76
90.75
90.84
90.83
90.82
90.82
90.18
90.17
74.5
73.83

YOUTUBE
79.85
76.47
79.69
76.18
83.43
80.3
79.65
76.22
82.24
77.99
79.89
76.29
74.74
68.13
77.22
67.56
76.48
66.4
75.74
67.27

Table 2: Sentiment analysis - Obtained results for base classifiers.
a positive or negative sentiment, and they combine these pieces of
evidence to determine the sentiment of a text document. However,
unlike RF-based methods, there is no mechanism to identify noise
or irrelevant words, which may affect its effectiveness results.
The methods KNN, LAZY, and LXT are usually associated with
the worse results for the sentiment analysis task. Instead of focusing on individual words, these methods exploit the distribution of
training examples among neighbors of a test example. This analysis
of the neighborhood can be a potential source of noise since similar
text documents can be associated with different sentiment labels.
Overall, according to the reported experimental results, the BERT
classifier is the only approach consistently among the best results
in both topic categorization and sentiment analysis. These results
provide empirical evidence towards the benefits of improving the
generalization power of random forests by using elaborated strategies to reduce overfitting.
Finally, for both sentiment analysis and topic classification, BERT
is one of the most effective classification methods that combines
discriminative pieces of evidence derived from exploring the complex sub-regions of the input space. Other classification approaches
provide completely different strategies to exploit discriminative
patterns, which motivates us to stack these different classification

approaches to combine the potentially complementary information
captured by each of them (see Section 4).
3.4.3 Effects of Extra-Trees as weak-learner. An important aspect
to be further analyzed is the influence of the additional randomization enjoyed by BERT through the use of Extra-Trees (instead
of traditional RFs, as done in BROOF). To this end, we analyze
the effect of gradually increasing the number of Extra-Trees in the
BROOF process over MicroF1 and MacroF1 . Here, the number of
iterations and trees were fixed to 200 and 8, respectively. Figure 2
shows the obtained results, where the x-axis represents the proportion of Extra-Trees composing the ensemble (0% means the absence
of Extra-Trees, which degenerates to BROOF, and 100% represents
the BERT classifier, composed entirely by Extra-Trees). There is a
clear tendency of growth in MicroF1 and MacroF1 as we increase
the proportion of Extra-Trees composing the ensemble. Thus, the
additional randomization procedure employed in BERT plays an
important role in improving the BROOF algorithm.

4

STACKING RF-BASED LEARNERS

Based upon the results reported in Section 3.4, one aspect should
be clear by now: the analyzed RF-based classifiers do excel in both
the explored text classification tasks. One question that naturally

110

Session 1C: Document Representation and Content Analysis 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Dis i,norm
BROOF LAZY SVM
j

NB

KNN

Dis i,norm
BROOF LAZY SVM
j

NB

KNN

Dis i,norm
BROOF LAZY SVM
j

NB

KNN

BERT
BROOF
LAZY
SVM
NB

0.32
0.32
0.32
0.27
-

0.33
0.33
0.19
0.30
0.33

BERT
BROOF
LAZY
SVM
NB

0.32
0.35
0.32
0.29
-

0.25
0.29
0.23
0.25
0.29

BERT
BROOF
LAZY
SVM
NB

0.39
0.41
0.43
0.37
-

0.35
0.42
0.38
0.36
0.39

0.12
-

0.29
0.29
-

0.30
0.32
0.31
-

(a) 4UNI

0.07
-

0.18
0.20
-

0.23
0.23
0.25
-

0.15
-

(b) ACM

0.23
0.26
-

0.28
0.24
0.29
-

(c) 20NG

Table 3: Normalized Degree of Disagreement
This is a symmetric statistic that captures to what extent two classifiers disagree in terms of prediction. Classifiers with low disagreement degree tend to have similar behavior in terms of correctly or
incorrectly classifying unseen examples and thus have low complementarity. In order to offer a more appropriate measure of
disagreement degree between classifiers, however, one should also
take into account their prediction capabilities. That way we guarantee a proper comparison between their behavior. For this purpose,
we propose a normalized degree of disagreement metric. Recall
that, in order to have minimal degree of disagreement, we must
have D01 ⊂ D10 or D10 ⊂ D01 . Also, we have that hi accuracy can
be expressed as Ri = Í n01 +nn11 , with R j expressed analogously.
a,b ∈0, 1

a,b

It is straightforward to show that Disi,min
j = R i + R j − 2 min(R i , R j ).
Similarly, we maximize the degree of disagreement when n 00 and
n 11 tend to 0. In that case, with some algebraic manipulation, one
can show that Disi,max
j = 2 − R i − R j . With such derivations in place,
the normalized degree of disagreement metric is defined as

(a) 20NG

Disi,norm
=
j

Disi, j − Disi,min
j
min
Disi,max
j − Disi, j

.

(2)

The Degree of Disagreement values computed for the exemplified
cases can be found on Table 3. This gives us some evidence that the
explored learning methods, such as BROOF, LazyNN RF and BERT
do have some complementary information that can potentially be
explored in order to come up with more effective learners. This
is the main motivation to what we propose here: a novel strategy
to stack RF based classifiers that, besides producing highly effective meta-learners, it also enjoys a significantly reduced runtime,
guaranteeing its applicability on large classification problems.
We now introduce an efficient way of stacking bagging-based
classifiers. Recall that bagging-based methods, such as Random
Forests, typically yield models with higher generalization capabilities by controlling variance with the bootstrap procedure [3]. We
rely on the out-of-bag samples produced by the bootstrap technique
performed by these classifiers in order to estimate the a posteriori
probability distributions for the training samples, thus producing
the meta attributes to be fed to the stacked classifier. Since this
information is promptly generated at training time by bagged classifiers, the meta learner can thus be built with negligible additional
computational effort.
In details, recall that the bootstrap procedure (random sampling
with replacement) generates samples Dboot comprising of approximately 1 − e −1 ≈ 63% of the original training set Dtrain , with the
remaining 36% samples being the so-called out-of-bag samples [12].
We here propose to use Doob = Dtrain \ Dboot to estimate the class
probability distribution at a point x ∈ Doob to be used as meta
attributes to train a stacked classifier. This comes at a very low cost,
since the meta attributes can be efficiently computed during the

(b) ACM

Figure 2: Effect of Extra-Trees as weak-learner.
arises is: can we explore these methods somehow in order to learn an
even more effective classifier? This is what we pursue in this section.
In order to combine RF-based classifiers, these classifiers should
exhibit some degree of complementarity. In fact, each RF based
classifier explore different learning strategies to come up with more
effective predictions. However, it is still important to assess whether
these distinct strategies do produce complementary information
that could be explored to leverage classification effectiveness. To
this end, we quantify such complementarity degree by means of the
Degree of Disagreement [16] observed for a pair of classifiers. Let
hi and h j be two classifiers, applied to examples from a validation
set Dvalid (e.g., a fold). Also, let D00 be the examples misclassified
by both hi and h j (n 00 = |D00 |), D01 be the examples correctly classified just by hi ((n 01 = |D01 |)) and D10 be the examples correctly
classified just by h j ((n 10 = |D10 |)). Finally, let D11 be the examples
correctly classified by both learners ((n 11 = |D11 |)). The Degree of
Disagreement Disi, j between hi and h j is given by:
n 01 + n 10
Disi, j =
(1)
n 00 + n 01 + n 10 + n 11

111

Session 1C: Document Representation and Content Analysis 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

training stage of bagged learners, without the needs to perform
costly estimation strategies, such as cross-validation. Let M be the
M be the bootstrap samples
number of bootstrap iterations, Diboot |i=1
M the classifiers trained with the corresponding bootstrap
and hi |i=1
samples. We compute the probability distribution estimates p oob (x)
for each instance x ∈ Dtrain as follows:
p oob (x) =

M
1 Õ hj
j
p (x)I [x ∈ Doob ],
M j=1

information for RF-based methods, which is the expected since they
follow completely different classification paradigms.
Considering the sentiment analysis datasets, one can observe
that most of the results regarding stacking different classifiers are
statistically tied, as shown in Table 5. Moreover, BERT is not substantially improved when stacked with other classifiers. In fact,
BERT ties with BROOF + LAZY + BERT + LXT in all datasets. Also,
BERT also ties with Allst ack in all datasets but one, YOUTUBE
(in fact, due to the presence of the NB classifier in the Allst ack
ensemble). These results show that, despite the potential benefits of
stacking various methods, the proposed BERT classifier remains as
one of the top performers when detecting the sentiment of textual
documents, being a strong candidate for consideration.

(3)

where I denotes the indicator function, p h j (x) = (p h j (c 1 |x),
p h j (c 2 |x), · · · , p h j (c K |x)), {c 1 , ...c K } is the set of possible classes
and p h j (c k |x) denotes the probability of x to belong to class c k ,
according to learner h j .
The estimated a posteriori probability p oob (x) can be naturally
used in the traditional stacking framework as meta-features, along
side the meta-features obtained by traditional means, in order to
train any meta-learner as discussed in the Section 2. By doing so,
one can stack bagged-based models efficiently either with bagged
and non-bagged ones, thus, making feasible the applicability of
such stacking systems in real world problems.

4.1

4.1.2 Effectiveness vs. diversity tradeoff. In the following, we
perform an analysis of the Pareto’s frontier of the generated ensembles. With this, we want to confirm that the RF-based methods are
important to generate ensembles of classifiers with high generalization power and complementarity/diversity.
Figure 3 exhibits a scatter plot for several datasets, in which each
point represents a stacking ensemble out of the possible ensembles
generated by the combination of the nine base classifiers (number
of classifiers composing the ensemble varies from 2 to 9). The x-axis
represents the diversity metric Double-Fault, which was chosen because it is the pairwise metric that best represents the relationship
between accuracy and diversity of an ensemble system[16]. The
metric was originally used by [11] to form a pairwise diversity
matrix for a classifier pool and subsequently to select classifiers
that are least related. The metric estimates the likelihood of both
classifiers incorrectly classifying the same sample (the smaller the
value of the metric, the higher the diversity/complementarity). The
y-axis is the effectiveness metric MacroF 1 . For those points, we
calculate the Pareto’s frontier which maximizes the effectiveness
metric and minimizes the Double-Fault metric (maximizes diversity), which are desirable characteristics of any ensemble method
(highly diverse and effective). The ensembles in the frontier showed
in Figures 3 (a), (b), and (c) are the following:

Experimental Evaluation

We now report our experimental evaluation of the proposed stacking model. To this end, we consider all the previously explored
datasets regarding topic categorization and sentiment analysis (see
Section 3.4.1). We contrast the proposed RF-based stacked classifier
against traditional stacking of classical state-of-the-art methods in
text categorization (e.g. SVM, kNN, Naı̈ve Bayes). Then, we analyze
our proposal in terms of runtime.
4.1.1 Results and Discussion. As we detail in the following,
stacking RF-based classifiers brings substantial improvements in
classification effectiveness over traditional methods and their combinations in both analyzed text classification tasks. The proposed
RF-based stacking is even able to produce as good results as the
combination of all evaluated classifiers in most analyzed datasets,
at a much lower runtime, guaranteeing its applicability in text
classification tasks.
The results regarding the topic categorization task can be found
in Table 4. First, note that the combination of RF-based approaches
BERT + BROOF + LXT + LAZY achieves better results over the
two best base classifiers (BERT and SVM) in most datasets, with
substantial gains of up to 17% and 21% in MacroF1 and MicroF1 ,
respectively. In fact, stacking RF-based approaches clearly produces
superior results when compared to the top-notch SVM and BERT
classifiers. The combination BERT + BROOF + LXT + LAZY is also
superior to the combination of classifiers SV M +N B +KN N in 4UNI
and SPAM. These results provide evidence to our claim that stacking
RF-based methods for text classification is a strong alternative to the
stacking of traditional text classification approaches, since besides
effective they are highly parallelizable, easily parameterized and
efficiently combined by our OOB stacking proposal.
Furthermore, stacking RF-based methods also performs as well
as the combination of all classification approaches (i.e., Allst ack )
in all but 20NG and MED datasets. This indicates that traditional
classification methods may provide supplementary discriminative

• (LXT , N B), (LAZ Y, N B), (SV M, K N N ), (LXT , SV M, N B),
(LAZ Y, SV M, N B), (LXT , SV M, N B, K N N ),
(LXT , SV M, N B, K N N , XT ), (LAZ Y, BERT , LXT , SV M, N B, K N N ),
(LAZ Y, LXT , SV M, N B, K N N , F A)
• (BERT , SV M ), (BERT , LXT , SV M ),
(BROO F, BERT , SV M, K N N ), (BERT , SV M, N B)
• (SV M, N B), (LAZ Y, SV M, N B), (BERT , SV M, N B),
(LAZ Y, BERT , SV M, N B), (LAZ Y, BERT , LXT , SV M, N B),
(LAZ Y, BERT , SV M, N B, K N N ), (BROO F, LAZ Y, SV M, N B, K N N ),
(BERT , LXT , SV M, N B, AEA), (BROO F, BERT , LXT , SV M, N B, K N N )

One can notice that as we go from left to right on the Pareto’s
frontier both, effectiveness and diversity6 decreases. This implies
that not necessarily a highly diverse combination will result in more
accurate classifiers, but that there is a tradeoff between effectiveness
and diversity of the base classifiers of the ensemble. By analyzing
the Pareto’s frontier, one can clearly notice that the RF extensions
play an important role in providing a balance between diversity
and generalization power in the most effective ensembles. It is also
possible to note that the loss in diversity brought by the insertion
of more similar methods into the ensemble (e.g., RF-based ones) is
compensated by the high generalization power of those methods.
These results along with the effectiveness analysis corroborate
our hypothesis: by using RF-based algorithms in multi-classifier
systems, it is possible to achieve higher effectiveness.
6 In this Section,

112

we will use the terms diversity and complementarity interchangeably.

Session 1C: Document Representation and Content Analysis 1

All s t ac k
BROOF+LAZY+BERT+LXT
SVM+NB+KNN
SVM+BERT
BERT
SVM

microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1

20NG
92.21 ± 0.44
92.01 ± 0.42
90.63 ± 0.57
90.4 ± 0.57
90.65 ± 0.45
90.42 ± 0.44
90.85 ± 0.50
90.68 ± 0.50
89.45 ± 0.46
89.13 ± 0.58
90.06 ± 0.43
89.93 ± 0.43

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

4UNI
86.85 ± 1.17
79.43 ± 2.23
86.79 ± 0.86
79.63 ± 1.91
84.95 ± 1.15
75.86 ± 1.48
86.65 ± 1.05
80.13 ± 2.49
84.61 ± 0.98
73.61 ± 1.85
83.48 ± 1.08
73.39 ± 2.17

ACM
79.02 ± 0.72
66.25 ± 1.01
77.83 ± 0.80
63.42 ± 0.92
77.78 ± 0.73
65.08 ± 1.71
77.25 ± 0.60
66.29 ± 0.97
74.8 ± 0.59
62.1 ± 0.99
75.4 ± 0.66
63.84 ± 0.55

REUT90
80.76 ± 1.24
39.28 ± 1.14
80 ± 1.60
38.66 ± 2.85
78.53 ± 1.09
37.10 ± 1.41
78.43 ± 1.33
36.8 ± 2.45
67.33 ± 0.72
29.24 ± 1.40
68.19 ± 1.15
31.95 ± 2.59

SPAM
96.06 ± 0.78
95.87 ± 0.82
96.13 ± 0.86
95.94 ± 0.90
93.67 ± 0.34
93.37 ± 0.34
95.91 ± 0.79
95.72 ± 0.82
96.11 ± 0.52
95.93 ± 0.55
92.55 ± 0.8
92.12 ± 0.87

MED
88.76 ± 0.11
81.62 ± 0.34
87.10 ± 0.07
79.36 ± 0.59
87.96 ± 0.05
80.33 ± 0.57
87.65 ± 0.03
80.55 ± 0.52
83.68 ± 0.32
74.25 ± 0.37
86.19 ± 0.05
78.46 ± 0.42

Table 4: Topic categorization - Obtained results for stacking and top-performer base classifiers.

All s t ac k
BROOF+LAZY+BERT+LXT
SVM+NB+KNN
SVM+BERT
BERT
NB

microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1

AMAZON
75.6
74.44
75.6
74.44
75.37
74.14
75.96
74.85
75.76
74.16
74.68
73.32

BBC
86.84
51.56
86.84
50.03
86.7
46.44
86.7
47.34
86.04
53.78
86.84
46.48

DEBATE
80.14
77.86
79.89
77.41
78.83
76.71
79.99
77.88
79.38
77.24
76
73.7

DIGG
76.47
65.8
75.96
62.71
77.5
64.84
76.46
63.05
75.7
63.03
76.09
64.38

MYSPACE
86.1
68.27
86.94
68.88
86.09
65.98
86.11
67.14
86.46
67.35
85.5
60.39

NYT
68.1
67.35
67.79
66.87
67.41
66.53
67.73
66.92
68.14
67.01
67.1
66.06

TWEETS
89.16
87.08
89.01
86.63
88.35
86.07
88.25
85.81
88.3
85.49
86.82
84.4

TWITTER
75.71
74.83
76.23
75.26
75.32
74.24
75.89
74.79
76.15
74.71
74.88
73.89

YELP
94.82
94.82
94.22
94.22
93.84
93.84
94.64
94.64
94.26
94.26
90.44
90.44

YOUTUBE
83.93
81.11
81.5
77.63
84.13
81.05
81.42
77.85
79.85
76.47
83.43
80.3

Table 5: Sentiment analysis - Obtained results for stacking and top-performer base classifiers

(a) 20NG

(b) 4UNI

(c) ACM

Figure 3: MacroF 1 vs. Double-Fault - each point represents a stacking out of all possible combination of the 9 base classifiers.
The line connecting the highlighted points is the Pareto’s frontier.
4.1.3 Computational Time and Effectiveness to Stack Baggingbased Methods. Table 6 shows the average time to combine all
nine base learning algorithms using ours and the baseline stacking
approaches, which generate the meta-level data by means of 5fold cross-validation. We also contrast our proposed method with
a fixed combination method (simple voting), which is the fastest
possible combing strategy since it does not require training. As can
be seen in Table 6, the stacking using our strategy based on Out-ofBag (OOB) samples combined with RF as meta-level learner shows
significant speedups in relation to all other stacking strategies,
without degrading its predictive performance. Specifically, in the
MEDLINE dataset, the stacking approaches using cross-validation
were not able to handle the dataset in a suitable time. Moreover, one
can notice that our approach is competitive regarding execution
time when compared to a simple voting algorithm while excelling
in effectiveness.

OOB-RF
RF
MLR
DT
SCANN
VIG
Voting

20NG
7244
20479
21455
20448
21651
20620
2921

4UNI
2413
7358
7334
7327
7330
7499
1046

ACM
6762
17194
17170
17163
17166
17335
2451

REUT90
13163
29256
29131
27124
29007
30296
4017

MEDLINE
458013
375297

Table 6: Avg. time in seconds to combine (training + testing
time) all 9 base learning algorithms with different stacking
strategies. In the cases that a method was not able to handle
a dataset, we marked the corresponding table cell with ‘-’.
Table 7 shows the effectiveness of different combination strategies, such as MLR, DT, SCANN and the recently proposed VIG
in the largest datasets7 . The results of these strategies are never
superior to the use of RF as a meta-level combiner. Despite not
being designed as a meta-level combiner, RF is capable of achieving
7 This

113

excludes the small SPAM dataset.

Session 1C: Document Representation and Content Analysis 1

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

the best effectiveness results due to its capabilities of automatically
identifying discriminative patterns on the relationship among classifier results. Moreover, the proposed usage of OOB can provide
the same discriminative evidence as the output of the RF-based
obtained by means of cross-validation, which eliminates the need
for costly procedures to stack RF-based methods.

OOB-RF
RF
MLR
DT
VOTING
SCANN
VIG

microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1
microF1
macroF1

20NG
92.24
92.04
92.21
92.01
91.36
91.14
91.44
91.23
91.55
91.29
90.87
90.55
90.13
89.65

4UNI
86.87
79.35
86.85
79.43
85.34
76.62
83.82
76.11
84.27
72.2
85.31
73.88
82.14
74.33

ACM
78.99
65.75
79.02
66.25
77.99
68.26
77.53
65.07
77.7
64.31
76.84
63.08
75.11
65.49

REUT90
80.25
40.8
80.76
39.28
76.33
35.64
68.19
32.43
66.71
28.89
71.14
29.25
19.4
3.17

[2] Yaxin Bi, David Bell, Hui Wang, Gongde Guo, and Jiwen Guan. 2007. COMBINING MULTIPLE CLASSIFIERS USING DEMPSTER’S RULE FOR TEXT CATEGORIZATION. Appl. Artif. Intell. 21, 3 (March 2007), 211–239.
[3] Leo Breiman. 1996. Bagging Predictors. Mach. Learn. 24, 2 (Aug. 1996), 123–140.
[4] A. Danesh, B. Moshiri, and O. Fatemi. 2007. Improve text classification accuracy
based on classifier fusion methods. In Information Fusion, 2007 10th International
Conference on. IEEE, 1–6.
[5] Clebson C. A. de Sá, Marcos André Gonçalves, Daniel Xavier de Sousa, and
Thiago Salles. 2016. Generalized BROOF-L2R: A General Framework for Learning
to Rank Based on Boosting and Random Forests. In Proceedings of the 39th
International ACM SIGIR conference on Research and Development in Information
Retrieval, SIGIR 2016, Pisa, Italy, July 17-21, 2016. 95–104.
[6] Yan-Shi Dong and Ke-Song Han. 2004. A comparison of several ensemble methods
for text categorization. In Services Computing, 2004. (SCC 2004). Proceedings. 2004
IEEE International Conference on. IEEE, 419–422.
[7] Yan-Shi Dong and Ke-Song Han. 2004. A Comparison of Several Ensemble
Methods for Text Categorization. In IEEE International Conference on Services
Computing. IEEE.
[8] Manuel Fernández-Delgado, Eva Cernadas, Senén Barro, and Dinani Amorim.
2014. Do We Need Hundreds of Classifiers to Solve Real World Classification
Problems? J. Mach. Learn. Res. 15, 1 (Jan. 2014), 3133–3181.
[9] Yoav Freund and Robert E Schapire. 1997. A Decision-Theoretic Generalization
of On-Line Learning and an Application to Boosting. J. Comput. Syst. Sci. 55, 1
(Aug. 1997), 119–139.
[10] Pierre Geurts, Damien Ernst, and Louis Wehenkel. 2006. Extremely randomized
trees. Machine Learning 63, 1 (2006), 3–42.
[11] Giorgio Giancinto and Fabio Roli. 2001. Design of Effective Neural Network
Ensembles for Image Classification Purposes. IMAGE VISION AND COMPUTING
JOURNAL 19 (2001), 699–707.
[12] T. Hastie, R. Tibshirani, and J. H. Friedman. 2009. The Elements of Statistical
Learning. Springer.
[13] Monisha Kanakaraj and Ram Mohana Reddy Guddeti. 2015. Performance analysis
of Ensemble methods on Twitter sentiment analysis using NLP techniques. In
IEEE International Conference on Semantic Computing (ICSC). IEEE.
[14] Aurangzeb Khan, Baharum Baharudin, Lam Hong Lee, Khairullah Khan, and
Universiti Teknologi Petronas Tronoh. 2010. A Review of Machine Learning Algorithms for Text-Documents Classification. In Journal of Advances In Information
Technology. Academy Publisher.
[15] Ludmila I. Kuncheva, James C. Bezdek, and Robert P. W. Duin. 2001. Decision
templates for multiple classifier fusion: an experimental comparison. Pattern
Recognition 34 (2001), 299–314.
[16] Ludmila I. Kuncheva and Christopher J. Whitaker. 2003. Measures of Diversity in
Classifier Ensembles and Their Relationship with the Ensemble Accuracy. Mach.
Learn. 51, 2 (May 2003), 181–207.
[17] Christopher J. Merz. 1999. Using Correspondence Analysis to Combine Classifiers.
Mach. Learn. 36, 1-2 (July 1999), 33–58.
[18] Tien Thanh Nguyen, Thi Thu Thuy Nguyen, Xuan Cuong Pham, and Alan WeeChung Liew. 2016. A Novel Combining Classifier Method Based on Variational
Inference. Pattern Recogn. 49, C (Jan. 2016), 198–212.
[19] Aytuğ Onan, Serdar Korukoğlu, and Hasan Bulut. 2016. Ensemble of Keyword
Extraction Methods and Classifiers in Text Classification. Expert Syst. Appl. 57,
C (Sept. 2016), 232–247.
[20] Gabriel Pui, Cheong Fung, Jeffrey Xu Yu, Haixun Wang, David W. Cheung, and
Huan Liu. 2006. A Balanced Ensemble Approach to Weighting Classifiers for
Text Classification. In ICDM ’06. Sixth International Conference on Data Mining.
IEEE.
[21] Lior Rokach. 2009. Taxonomy for characterizing ensemble methods in classification tasks: A review and annotated bibliography. In Computational Statistics
Data Analysis, In Press, Corrected Proof.
[22] Thiago Salles, Marcos Gonçalves, and Leonardo Rocha. 2017. PhD Dissertation:
Random Forest based Classifiers for Classification Tasks with Noisy Data. (2017).
Federal University of Minas Gerais.
[23] Thiago Salles, Marcos Gonçalves, Victor Rodrigues, and Leonardo Rocha. 2015.
BROOF: Exploiting Out-of-Bag Errors, Boosting and Random Forests for Effective
Automated Classification. In Proc. of the 38th International ACM SIGIR Conference
on Inf. Retrieval. ACM, 353–362.
[24] M. R. Segal. 2004. Machine Learning Benchmarks and Random Forest Regression.
Technical Report. University of California.
[25] Kai Ming Ting and Ian H. Witten. 1997. Stacking Bagged and Dagged Models.
In Proceedings of the Fourteenth International Conference on Machine Learning
(ICML ’97). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 367–375.
[26] David Wolpert and William G. Macready. 1996. Combining Stacking With Bagging
To Improve A Learning Algorithm. Technical Report.
[27] David H. Wolpert. 1992. Stacked Generalization. Neural Networks 5 (1992),
241–259.

MED
88.76
81.62
86.91
77.41
-

Table 7: Obtained results for different stacking strategies. In
the cases that a method was not able to handle a dataset, we
marked the corresponding table cell with ‘-’

5

CONCLUSIONS AND FUTURE WORK

In this work, we propose a boosted version of the extremely randomized trees classifier, named BERT, in order to leverage the
learner’s capability to minimize bias while maintaining high predictive power by properly reducing variance. As our experimental
analysis reveal, our proposal enjoys top-notch classification effectiveness, being among the top performers in the vast majority of
cases covering two challenging text classification tasks, namely,
topic categorization and sentiment analysis. We also propose to
stack the explored RF-based classifiers in order to exploit the complementarities observed among those classifiers. Unlike traditional
stacking, that makes use of cross-validation procedures to learn the
meta-features to be fed to the stacking procedure, we here rely on
the out-of-bag samples obtained through bootstrapping the training
set when learning the forests. More specifically, the out-of-bag samples are used to estimate the a posteriori class distributions used
by the stacking procedure to learn the underlying input/output
relationships. We show that such novel stacking approach is not
only able to provide state-of-the-art classification effectiveness, but
also at a significantly lower runtime.
Clearly, there is still room for improvements. For example, we
plan to investigate if non-bagging strategies, such as the traditional kNN, Naı̈ve Bayes and SVM classifier, could benefit from
the bootstrap procedure in order to come up with a uniform stacking strategy that generalizes to any other classifier. Also, in the
same vein of exploring the out-of-bag samples to estimate the a
posteriori class distributions in our stacking approach, we could explore the out-of-bag error estimates in order to select the candidate
features for decision nodes in order to improve both effectiveness
and efficiency of the RF based classifiers, such as BERT, BROOF,
LazyNN RF, LXT and the traditional RF.

REFERENCES
[1] Ricardo A. Baeza-Yates and Berthier A. Ribeiro-Neto. 1999. Modern Information
Retrieval. ACM Press / Addison-Wesley.

114

