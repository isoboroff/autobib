Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Weighted Domain Translation for Online News Comments
Emotion Tagging
Ying Zhang, Li Yu, Xue Zhao, Xiaojie Yuan∗

Lei Xu

College of Computer and Control Engineering,
Nankai University, China
{zhangying,yuli,zhaoxue,yuanxj}@dbis.nankai.edu.cn

School of Distance Education,
Nankai University, China
lxu@nankai.edu.cn

ABSTRACT

of great value for not only helping understand the perspectives
and preferences of individual users, but also facilitating online
publishers to provide more personalized services.
Emotion tagging for online news comments can be formulated
as a sentiment classification problem, which focuses on detecting
the polarity (e.g., positive or negative) or emotion categories (e.g.,
happy, sad, angry, etc.) from user-generated contents including user
reviews of products or services, posts on blogs or social networks,
and comments in forums or online news services.
Machine learning algorithms have been widely used for online news comments emotion tagging. Recently, neural networks,
such as CNNs, RNNs, LSTM, have become very popular in sentiment classification tasks for learning robust feature representations
automatically[7]. The performance of supervised learning models
and neural networks largely relies on the availability of a relative large amount of manually labeled comments. Online news has
many domains such as society, entertainment and sports. In many
practical cases, we may have very limited labeled data in the target
domain, but have abundant data in other domains. If we directly
apply the classifiers trained from one domain to another, it usually
leads to poor classification performance. The reason is that comments in different news domains talk about different sets of topics
in different styles, which results in different word distributions.
This has motivated much research on cross-domain sentiment classification which transfers the knowledge from the source domain
to the target domain[5]. Structural Correspondence Learning (SCL)
has become one of the most promising techniques for transfer learning, using alternating structural optimizations[1]. Zhang at al.[8]
proposed a transfer learning approach using joint probabilities of
text features and emotion categories to model the domain difference. These methods can model the difference between domains to
some extent, but cannot describe the relationship between domains
accurately.
This paper proposes a novel transfer learning approach for crossdomain emotion tagging based on BLSTM with “domain translation” module to overcome difference in two domains. Inspired by
the word embedding transformation in machine translation[4], we
make this “domain translation” module by building a weighted
linear relationship between domains. Figure 1 gives a simple visualization of “domain translation”, which shows embedding of words
in society domain(source) and entertainment domain(target). There
exist similar geometric arrangements between two domains, with
which we can build a linear relationship across domain. Moreover,
LSTM neural networks are particularly designed to overcome the
gradient vanishing problem of traditional RNNs and then be able
to remember information for a longer period of time. BLSTM contains two opposite LSTM layers, considering both left and right

This paper studies an emotion classification problem, which aims to
classify online news comments to one of fine-grained emotion categories, e.g. happy, sad, and angry, etc. Neural networks have been
widely used and achieved great success in sentiment classification.
However, there must be sufficient labeled comments available for
training neural networks, which usually requires labor-intensive
and time-consuming manual labeling. One of the most effective
solutions is to apply transfer learning, which uses abundant labeled
comments from a source news domain to help the classification for
another target domain with limited amount of labeled data. Still,
the comments from different domains can have very different word
distributions, which makes it difficult to transfer knowledge from
one domain to another. In this paper, we accomplish cross-domain
emotion tagging based on an advanced neural network BLSTM
(bidirectional long short-term memory) with “domain translation”,
which can overcome the difference between domains. A weighted
linear transformation is utilized to “translate” knowledge from
source to target domain. An extensive set of experimental results
on four datasets from popular online news services demonstrates
the effectiveness of our proposed models.

KEYWORDS
Emotion Tagging; Transfer Learning; Neural Networks
ACM Reference format:
Ying Zhang, Li Yu, Xue Zhao, Xiaojie Yuan and Lei Xu. 2017. Weighted
Domain Translation for Online News Comments Emotion Tagging. In Proceedings of SIGIR ’17, Shinjuku, Tokyo, Japan, August 07-11, 2017, 4 pages.
https://doi.org/http://dx.doi.org/10.1145/3077136.3080653

1

INTRODUCTION

With the development of online news services, online news has
become an important type of media that attracts billions of users
to read, respond, and actively interact with each other by making
comments. Users often express subjective emotions such as sadness,
surprise and anger in comments. Capturing this information is
∗ Corresponding

Author

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 Association for Computing Machinery.
ACM ISBN 978-1-4503-5022-8/17/08. . . $15.00
https://doi.org/http://dx.doi.org/10.1145/3077136.3080653

821

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan

society

country
children

star

society

Ying Zhang, Li Yu, Xue Zhao, Xiaojie Yuan and Lei Xu

country

2.3

children

In this section, we utilize a linear transformation[4] to exploit relationship between domains. Given vocabulary Vs , Vt in source
domain and target domain, Vs∩t = Vt ∩ Vs is the overlapped
words set between domains. Next, we are given their associated
vector representations {(x si , x ti )ni=1 }, where x si is the word embedding in source domain, x ti is the word embedding in target domain
and n is the size of word pairs. It is our goal to find a linear transformation f (x si ) = w · x si + b, which we call “domain translation”,
such that f (x si ) approximates x ti .
Figure 1 gives a simple visualization of example to illustrate the
key point. We show embedding of words in society domain(source)
and entertainment domain(target). These vectors are projected
down into two dimensions using PCA, and then manually rotated
to accentuate their similarity. It can be seen that these concepts
have similar geometric arrangements, and a linear relationship, can
be established between domains.
Moreover, word “star” is more likely to appear in entertainment
domain(target) as an unique feature. When building a linear relationship from source to target domain, we would prefer word
“star” to have a more similar geometric arrangement and weight
more on “star”. Likewise, word “country” is more likely to appear
in society domain(source) rather than target domain, thus can be
less valuable in target domian. We weight less on “country” and
pay less attention to its geometric arrangement between domains.
As we expected, “star” have more similar geometric arrangements
than word “country” in Figure 1. As the document frequency represents the importance of a word in one domain, we define the words’
weight βi = d fti /d fsi when building a linear relationship. d fti is
the document frequency of word x ti in Dt and d fsi is the document
frequency of word x si in Ds . Both d fti and d fsi are normalized to
the scale from range 0 to 1.
This linear relationship f can be learned by the following transformation loss:
n
X
min
βi || f (x si ) − x ti || 2
(1)

star
fashion

Society Domain(Source)

fashion

Entertainment Domain(Target)

Figure 1: Embedding of words in Society Domain(source)
and Entertainment Domain(target).
context to make decisions[6]. Therefore, with “domain translation”,
BLSTM can utilize more context information and extract features
from other domains more properly.
An extensive set of experiments is conducted with four datasets
collected from popular online news services. Our proposed models
significantly outperform baselines in the task of comments emotion
prediction. In particular, our weighted linear transferring method
is very flexible and can work with many other basic supervised
learning models and neural networks.

2 METHODOLOGY
2.1 Problem Definition
In our problem setting, two domains Ds and Dt are specified,
where Ds is the source domain and Dt is the target domain. The
emotion category sets E = {el }(l = 1, . . . , K ) are defined on Ds and
Dt respectively, where they share the same K emotion categories.
The i th comment is described as a feature vector c i and its label
is defined as yi = {yil }(l = 1, . . . , K ). If comment c i is tagged with
P
emotion el , yil equals to 1, otherwise 0. Therefore, lK=1 yil = 1.
We have a set of labeled data from the source domain denoted as
Ds = {(c si , ysi ) i=1
}, where |Ds | is the size of source domain data.
|D |
s

Similarly, we have Dt = {(c ti , yti ) i=1
} and |Dt | is the size of
| Dt |
labeled target domain data. In general, 0 < |Dt | ≪ |Ds |, as there
are abundant labeled data in the source domain but limited labeled
data in the target domain. Our task is to learn an emotion classifier
to predict the emotion tags of unlabeled comments in the target
domain Dt based on the labeled data of both Ds and Dt .

2.2

Exploiting relationship between domains

i=1

The weight βi can be a significance of domain translation, solving
the problem of different word distributions.
Once the relationship is built, word, such as “children”, which
only exists in source domain, can also be transferred into another
domain.
In particular, a pruning strategy to remove word pairs can be applied to the linear transformation. Those words whose normalized
d f is close to 1 are more likely to be some stop words or words
which convey little social emotions. Also, words which appear less
than 5 times in the whole dataset may not be trained sufficiently.

Word Embedding

Word embedding is to convert words to fixed-length real-value
vectors, at the same time, to map the words with similar context
to the closer vectors. As for the first layer of neural network, there
are three strategies to input data into it:
• random All vectors are randomly initialized and then modified during training.
• static All vectors are pre-trained from word2vec 1 and kept
static. Only other parameters of the model are learned.
• non-static All vectors are pre-trained from word2vec and
fine-tuned by each batch.

2.4

Neural Network

The model architecture is shown in Figure 2. Firstly, word embedding is used to represent each word in both domains. In order to
transfer knowledge across domains, a weighted linear transformation is applied to the instances from source domain. Specifically, for
each comment c si , f (c si ) is used to transfer knowledge. This linear
transformation has the same parameters(w, b) as that in transformation loss mentioned in section 2.3. Then data from both domains

In our paper, we explore the performance of all embedding strategies in our emotion tagging task.
1 https://code.google.com/p/word2vec/

822

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Weighted Domain Translation for Online News Comments Emotion Tagging

Science and Technology comments are merged together, referred
to as Reddit Sci&Tech. Both datasets are the same as in [8].
Neutral comments are excluded since we focus on emotion classification instead of subjectivity detection. The statistics of annotated
datasets are as shown in Table 1.
Table 1: The statistics of labeled comments on four datasets.

cti
Touched
Sympathetic
Angry

Target

Amused
Sad

f(csi)

Surprised

Sina Society
Emotion
Number
Touched
899
Sympathetic
612
Angry
1743
Amused
409
656
Sad
Surprised
195
Poli&WorldNews
Emotion
Number
Sympathetic
300
Angry
619
Disgust
687
699
Surprised
628
Happy
Sad
178

Source

Embedding Layer

Transformation Layer

BLSTM Layer

Softmax Layer

Figure 2: The Architecture of the Proposed Transferred
BLSTM Neural Network.
is fed into BLSTM layer which is applied right after the transformation layer. BLSTM contains two LSTM layers running in the
opposite directions. It takes into account both left and right context
in making decisions, which makes it capable of modeling complete
sequential information of comments in a long distance. The outputs of both directions are concatenated together. At last, we add a
softmax layer as the output to generate conditional probability for
each emotion label.
We introduce the cross-entropy loss between predicted emotion
probability P (el | f (c si )), P (el |c ti ) and ground truth ysl i , ytl i . The final loss is the summary of the cross-entropy loss and the transformation loss:
|X
Ds | X
|X
Dt | X
K
K
min(−
ysl i log(P (el | f (c si ))) − λ 1
ytl i log(P (el |c ti ))
i=1 l =1
n
X

3.2

i=1 l =1

βi || f (x si ) − x ti || 2 )

+ λ2

SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan

(2)

i=1

Here, λ 1 and λ 2 are two trade-off parameters.

3 EXPERIMENTAL EVALUATION
3.1 Datasets
Two groups of datasets(same as [8]) are used to conduct the experiments and each group contains two datasets from different domains.
More specifically, in four rounds of experiments, one dataset is chosen as the target domain and the other in the group acts as the
source domain.
The first two datasets come from two online news websites
as Sina News and QQ News, which are among the largest news
services in China. In particular, top 20 comments of most-viewed
news articles are collected within six months of 2011 from the
Society channel of Sina News and the Entertainment channel of
QQ News. These two datasets are referred to as Sina Society and
QQ Entertainment respectively. Both datasets have eight emotions
but we only select six emotions that exist in both domains.
The second two datasets come from the famous social news
portal Reddit in English. Most popular comments from four domains(Politics, WorldNews, Science, and Technology) are collected
within three months of 2011. Moreover, Politics and WorldNews
comments are merged together, referred to as Reddit Poli&WorldNews.

823

QQ Entertainment
Emotion
Number
Touched
139
Sympathetic
643
Angry
1641
Amused
564
Sad
358
Surprised
86
Sci&Tech
Emotion
Number
Sympathetic
125
Angry
556
Disgust
704
Surprised
461
Happy
1000
Sad
287

Baselines

Experiments in this section investigate the effectiveness of our
proposed approach and baseline methods:
Transfer Learning Methods
• BLSTM-T The approach proposed in Section 2,
i.e. BLSTM + “domain translation”.
• CNN-T A similar approach with BLSTM-T,
i.e. CNN + “domain translation”.
• CDET_J A transfer learning approach using joint probabilities of text features[8].
• SCL A multi-task learning method implemented with logistic regression and pivot features.[2].
Non-Transfer Learning Methods
• BLSTM The BLSTM neural network[6].
• CNN The Convolutional Neural Network for sentence
classification[3].
• ETLR A multinomial logistic regression model with L2
regularization, same as[8].

3.3

Experiment

For all the neural networks, we use dropout rate of 0.5, mini-batch
size of 32, embedding size of 128. Result is reported of averaged
20 independent runs. Word embedding is initialized by word2vec
and fine-tuned during training. Parameters in f are initialized by
formula in section2.3 and fine-tuned during training. The performance of all methods is evaluated on the two groups of datasets
and each group has two ways choosing either dataset as the target.
We randomly selected 1/64, 1/32, 1/16, 1/8 and 1/4 of labeled data
in target domain and all labeled data in the source domain as the
training data. The remaining data in the target domain are used for
testing.

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan
Sina Society -> QQ Entertainment

QQ Entertainment -> Sina Society

Reddit Sci&Tech -> Reddit Poli&WorldNews

0.52

0.44

0.50

0.42

0.48

0.40

0.50

0.48

BLSTM-T
CNN-T
BLSTM
CNN

0.46
1/64

1/32

1/16
Train Ratio

CDET_J
SCL
ETLR

1/8

1/4

Accurary

Accurary

Accurary

0.52

0.46
0.44

BLSTM-T
CNN-T
BLSTM
CNN

0.42
0.40

1/64

1/32

1/16
Train Ratio

CDET_J
SCL
ETLR

BLSTM-T
CNN-T
BLSTM
CNN

BLSTM-T
CNN-T
BLSTM
CNN

0.40
0.38

0.38
0.36
0.34

CDET_J
SCL
ETLR

0.36
0.34
0.32

0.32
0.30

0.30
1/8

Reddit Poli&WorldNews -> Reddit Sci&Tech

CDET_J
SCL
ETLR

Accurary

0.54

Ying Zhang, Li Yu, Xue Zhao, Xiaojie Yuan and Lei Xu

1/4

1/64

1/32

1/16
Train Ratio

1/8

1/4

1/64

1/32

1/16
Train Ratio

1/8

1/4

Figure 3: Accuracy on four datasets under different size of target domain data.

3.5

Figure 3 shows the accuracy on both two groups of datasets with
different ratios of training data in the target domain. The performance of all method improves with more training data in target
domain. Under most settings, transfer learning methods perform
better than non-transfer learning methods. This fact clearly demonstrates the advantage of transferring knowledge from source to
target domain. We can also observe that BLSTM-T and CNN-T beat
CDET_J and SCL, which proves the advantage of “domain translation” methods we proposed. Moreover, BLSTM-T performs better
than CNN-T, as recurrent neural networks take into account of the
emotion of the whole sentence rather than local emotion. Transferring knowledge can bring more gain along with the decreasing of
training data in target domain(1/4->1/64). The “domain translation”
brings more advantage to the classification accuracy in the case of
limited data in the target domain than abundant data in the target
domain. In particular, when the training data in target domain is
sufficient, neural networks like BLSTM can be better than those
transfer learning methods, such as CDET_J.
Significance tests using t-test are conducted for the accuracy
results. On most cases, our method outperforms others with 0.95
confidence level under all training ratios.

3.4

In section 2.3, we introduce a novel weighted linear transformation
method. This set of experiment is conducted to explore the effect
of weight. As a comparison to weighted linear transformation, averaged weight strategy sets the weight βi to 1. Table 3 shows the
accuracy of BLSTM-T on datasets (Sina Society -> QQ Entertainment) between two strategies under different size of target domain
data. Weighted linear transformation outperforms averaged weight
method and more training data in target domain can help improve
the classification ability.
Table 3: Weighted vs. Averaged under different size of target
domain data
strategy
W eiдhted
Averaдed

4

This set of experiment is conducted to explore the effect of three
strategies of word embedding initialization mentioned in section2.2.
Table 2 shows the accuracy of BLSTM-T on datasets (Sina Society ->
QQ Entertainment) between three strategies of word embedding under different size of target domain data. BLSTM-T with all randomly
initialized vectors does not perform well. This result is identical
with expectation that random initial achieve the worst, just as we
expected. Linear transformation parameters are initialized at the beginning of algorithm and bad word embeddings lead to inaccurate
relationship between source and target domain. As a conclusion,
our method heavily relies on the effect of initial word embedding.
Also, static strategy performs slightly worse than non-static, as
fine-tuning allows them to learn more meaningful representations.

1/32
0.5034
0.4980
0.4872

1/16
0.5211
0.5226
0.5097

1/8
0.5315
0.5239
0.5089

1/32
0.5034
0.4924

1/16
0.5211
0.5136

1/8
0.5315
0.5238

1/4
0.5344
0.5321

CONCLUSIONS

5

ACKNOWLEDGEMENTS

This work is partially supported by National Natural Science Foundation of China under Grant No. 61402243, National 863 Program of
China under Grant No. 2015AA015401. This work is also partially
supported by Tianjin Municipal Science and Technology Commission under Grant No. 16JCQNJC00500.

REFERENCES
[1] R.K. Ando and T. Zhang. 2005. A framework for learning predictive structures
from multiple tasks and unlabeled data. JMLR (2005).
[2] J. Blitzer, M. Dredze, and F. Pereira. 2007. Biographies, bollywood, boom-boxes
and blenders: Domain adaptation for sentiment classification. In ACL.
[3] Yoon Kim. 2014. Convolutional neural networks for sentence classification. arXiv
preprint arXiv:1408.5882 (2014).
[4] Tomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013. Exploiting similarities
among languages for machine translation. arXiv preprint arXiv:1309.4168 (2013).
[5] S.J. Pan and Q. Yang. 2010. A survey on transfer learning. TKDE (2010).
[6] Mike Schuster and Kuldip K Paliwal. 1997. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing 45, 11 (1997), 2673–2681.
[7] Duyu Tang, Bing Qin, and Ting Liu. 2015. Learning Semantic Representations
of Users and Products for Document Level Sentiment Classification.. In ACL.
1014–1023.
[8] Ying Zhang, Ning Zhang, Luo Si, Yanshan Lu, Qifan Wang, and Xiaojie Yuan.
2014. Cross-domain and cross-category emotion tagging for comments of online
news. In Proceedings of the 37th international ACM SIGIR. ACM, 627–636.

Table 2: Static vs. Non-static vs. Rand under different size of
target domain data.
1/64
0.4975
0.4896
0.4732

1/64
0.4975
0.4852

We present domain translation for online news comments emotion
tagging, which incorporates a novel weighted linear tranformation
to borrow the knowledge from one domain to benefit another. Our
experimental results demonstrate the effectiveness of the proposed
approach. For possible future research, we plan to explore the case
when source and target domain share different emotion categories.

Static vs. Non-static vs. Rand

strategy
Non − static
Static
Rand

Weighted vs. Averaged

1/4
0.5344
0.5296
0.5086

824

