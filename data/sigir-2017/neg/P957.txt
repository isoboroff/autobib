Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

AutoSVD++: An Efficient Hybrid Collaborative Filtering Model
via Contractive Auto-encoders
Shuai Zhang

Lina Yao

Xiwei Xu

University of New South Wales
Sydney, NSW 2052, Australia
shuai.zhang@student.unsw.edu.au

University of New South Wales
Sydney, NSW 2052, Australia
lina.yao@unsw.edu.au

Data61, CSIRO
Sydney, NSW 2015, Australia
Xiwei.Xu@data61.csiro.au

ABSTRACT

However, most of these approaches are either relying handcrafted advanced feature engineering, or unable to capture the nontriviality and non-linearity hidden in interactions between content
information and user-item matrix very well. Recent advances in
deep learning have demonstrated its state-of-the-art performance in
revolutionizing recommender systems [4], it has demonstrated the
capability of learning more complex abstractions as effective and
compact representations in the higher layers, and capture the complex relationships within data. Plenty of research works have been
explored on introducing deep learning into recommender systems
to boost the performance [2, 9, 10, 12]. For example, Salakhutdinov et al. [9] applies the restricted Boltzmann Machines (RBM)
to model dyadic relationships of collaborative filtering models. Li
et al. [6] designs a model that combines marginalized denoising
stacked auto-encoders with probabilistic matrix factorization.
Although these methods integrate both deep learning and CF
techniques, most of them do not thoroughly make use of side information (e.g., implicit feedback), which has been proved to be
effective in real-world recommender system [3, 7]. In this paper,
we propose a hybrid CF model to overcome such aforementioned
shortcoming, AutoSVD++, based on contractive auto-encoder paradigm in conjunction with SVD++ to enhance recommendation
performance. Compared with previous work in this direction, our
contributions of this paper are summarized as follows:

Collaborative filtering (CF) has been successfully used to provide
users with personalized products and services. However, dealing
with the increasing sparseness of user-item matrix still remains a
challenge. To tackle such issue, hybrid CF such as combining with
content based filtering and leveraging side information of users
and items has been extensively studied to enhance performance.
However, most of these approaches depend on hand-crafted feature
engineering, which is usually noise-prone and biased by different
feature extraction and selection schemes. In this paper, we propose
a new hybrid model by generalizing contractive auto-encoder paradigm into matrix factorization framework with good scalability and
computational efficiency, which jointly models content information
as representations of effectiveness and compactness, and leverage
implicit user feedback to make accurate recommendations. Extensive experiments conducted over three large-scale real datasets
indicate the proposed approach outperforms the compared methods
for item recommendation.

CCS CONCEPTS
•Information systems →Recommender systems;

KEYWORDS
collaborative filtering; deep learning; contractive auto-encoders

1

• Our model naturally leverages CF and auto-encoder framework in a tightly coupled manner with high scalability. The
proposed efficient AutoSVD++ algorithm can significantly
improve the computation efficiency by grouping users that
shares the same implicit feedback together;
• By integrating the Contractive Auto-encoder, our model
can catch the non-trivial and non-linear characteristics
from item content information, and effectively learn the
semantic representations within a low-dimensional embedding space;
• Our model effectively makes use of implicit feedback to
further improve the accuracy. The experiments demonstrate empirically that our model outperforms the compared methods for item recommendation.

INTRODUCTION

With the increasing amounts of online information, recommender
systems have been playing more indispensable role in helping
people overcome information overload, and boosting sales for ecommerce companies. Among different recommendation strategies,
Collaborative Filtering (CF) has been extensively studied due to its
effectiveness and efficiency in the past decades. CF learns user’s
preferences from usage patterns such as user-item historical interactions to make recommendations. However, it still has limitation in
dealing with sparse user-item matrix. Hence, hybrid methods have
been gaining much attention to tackle such problem by combining
content-based and CF-based methods [7].

2
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00
DOI: http://dx.doi.org/10.1145/3077136.3080689

PRELIMINARIES

Before we dive into the details of our models, we firstly discuss the
preliminaries as follows.

2.1

Problem Definition

Given user u = [1, ..., N ] and item i = [1, ..., M], the rating rui ⊂
R ∈ RN ×M is provided by user u to item i indicating user’s preferences on items, where most entries are missing. Let ruˆ i denote the

957

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

predicted value of rui , the set of known ratings is represented as
K = {(u, i)|rui is known}. The goal is to predict the ratings of a set
of items the user might give but has not interacted yet.

2.2

Latent Factor Models

2.2.1 Biased SVD. Biased SVD [5] is a latent factor model, unlike conventional matrix factorization model, it is improved by
introducing user and item bias terms:
ruˆ i = µ + bu + bi + ViT Uu

(1)

where µ is the global average rating, bu indicates the observed
deviations of user u, bi is the bias term for item i, Uu ∈ Rk and
Vi ∈ Rk represent the latent preference of user u and latent property
of item i respectively, k is the dimensionality of latent factor.

Figure 1: Illustration of AutoSVD (remove the implicit feedback) and AutoSVD++.

2.2.2 SVD++. SVD++ [5] is a variant of biased SVD. It extends
the biased SVD model by incorporating implicit information. Generally, implicit feedback such as browsing activity and purchasing
history, can help indicate user’s preference, particular when explicit
feedback is not available. Prediction is done by the following rule:
Õ
1
ruˆ i = µ + bu + bi + ViT (Uu + |N (u)| − 2
yj )
(2)

3.1

Suppose we have a set of items, each item has many properties
or side information, the feature vector of which can be very highdimensional or even redundant. Traditional latent factor model like
SVD is hard to extract non-trivial and non-linear feature representations [12]. Instead, we propose to utilize CAE to extract compact
and effective feature representations:

j ∈N (u)

where y j ∈ Rf is the implicit factor vector. The set N (u) contains
the items for which u provided implicit feedback, N (u) can be
replaced by R(u) which contains all the items rated by user u [7], as
implicit feedback is not always available. The essence here is that
users implicitly tells their preference by giving ratings, regardless of
how they rate items. Incorporating this kind of implicit information
has been proved to enhance accuracy [5]. This model is flexible to be
integrated various kinds of available implicit feedback in practice.

2.3

cae(Ci ) = s f (W · Ci + bh )

ruˆ i = µ + bu + bi + (β · cae(Ci ) + ϵi )T Uu

Contractive Auto-encoders

(6)

Similar to [11], we divide item latent vector Vi into two parts,
one is the feature vector cae(Ci ) extracted from item-based content information, the other part ϵi ∈ Rk (i = 1...n) denotes the
latent item-based offset vector. β is a hyper-parameter to normalize
cae(Ci ) . We can also decompose the user latent vector in a similar
way. However, in many real-world systems, user’s profiles could
be incomplete or unavailable due to privacy concern. Therefore, it
is more sensible to only include items side information.

x ∈D n

where x ∈ Rd x is the input, D n is the
n training set,o L is the recon-

3.2

0

struction error, the parameters θ = W ,W , bh , by , д(f (x)) is the
reconstruction of x, where:

AutoSVD++

While the combination of SVD and contractive auto-encoders is
capable of interpreting effective and non-linear feature representations, it is still unable to produce satisfying recommendations with
sparse user-item matrix. We further propose a hybrid model atop
contractive auto-encoders and SVD++ , which takes the implicit
feedback into consideration for dealing with sparsity problem. In
many practical situations, recommendation systems should be centered on implicit feedback [3]. Same as AutoSVD, we decompose
the item latent vectors into two parts. AutoSVD++ is formulated as
the following equation:
Õ
1
ruˆ i = µ +bu +bi + (β ·cae(Ci ) +ϵi )T (Uu + |N (u)| − 2
y j ) (7)

0

(4)

s f is a nonlinear activation function, sд is the decoder’s activation
function, bh ∈ Rdh and by ∈ Rd x are bias vectors, W ∈ Rdh ×d x
0
and W ∈ Rdh ×d x are weight matrixes, same as [8], we define
0
W = W . The network can be trained by stochastic gradient descent
algorithm.

3

(5)

where Ci ∈ Rdc represents the original feature vector, cae(Ci ) ∈ Rk
denotes the low-dimensional feature representation. In order to
integrate the CAE into our model, the proposed hybrid model is
formulated as follows:

Contractive Auto-encoders (CAE) [8] is an effective unsupervised
learning algorithm for generating useful feature representations.
The learned representations from CAE are robust towards small
perturbations around the training points. It achieves that by using
the Jacobian norm as regularization:
Õ
cae (θ ) =
(L(x, д(f (x))) + λ k Jf (x) kF2 )
(3)

д(f (x)) = sд (W s f (W x + bh ) + by )

AutoSVD

PROPOSED METHODOLOGY

j ∈N (u)

In this section, we introduce our proposed two hybrid models,
namely AutoSVD and AutoSVD++, respectively.

Figure 1 illustrates the structure of AutoSVD and AutoSVD++.

958

Short Research Paper

3.3

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Optimization

Algorithm 1 Efficient training algorithm for AutoSVD++

We learn the parameters by minimizing the regularized squared
error loss on the training set:
Õ
ˆ )2 + λ · fr eд
min
(rui − rui
(8)
b∗,ϵ ∗,U ∗,y∗

procedure Update Parameters
for all user u do
1 Í
p im ← |N (u)| − 2 · j ∈N (u) y j
p old ← p im
for all training samples of user u do
upadate other parameters
p im ← p im + γ 2 (eui · (β · cae(Ci ) + ϵi ) − λ 2 · p im )
end for
for all i in items rated by u do
1
yi ← yi + |N (u)| − 2 · (p im − p old )
end for
end for
end procedure

1:
2:
3:
4:

(u,i)∈K
5:

where fr eд is the regularization terms to prevent overfitting. The
fr eд for AutoSVD++ is as follows:
Õ
2
yj
(9)
fr eд = bu2 + bi2 + kϵi k 2 + kUu k 2 +

6:
7:
8:
9:

j ∈N (u)

10:

The regularization for AutoSVD is identical to AutoSVD++ with
the implicit factor y j removed.
In this paper, we adopt a sequential optimization approach. We
first obtain the high-level feature representations from CAE, and
then integrated them into the AutoSVD and AutoSVD++ model.
An alternative optimization approach, which optimizes CAE and
AutoSVD (AutoSVD++) simultaneously, could also apply [6]. However, the later approach need to recompute all the item content
feature vectors when a new item comes, while in the sequential
situation, item feature representations only need to be computed
once and stored for reuse.
The model parameters are learned by stochastic gradient descent
(SGD). First, SGD computes the prediction error:

11:
12:
13:

Table 1: Datasets Statistics

(10)

bu ← bu + γ 1 (eui − λ 1 · bu )

(11)

bi ← bi + γ 1 (eui − λ 1 · bi )
ϵi ← ϵi + γ 2 (eui · Uu − λ 2 · ϵi )

(12)
(13)

Uu ← Uu + γ 2 (eui · (β · cae(Ci ) + ϵi ) − λ 2 · Uu )

(14)

Update rules for AutoSVD++ are:
Õ

#users

#ratings

density(%)

MovieLens 100k
MovieLens 1M
MovieTweetings

1682
3706
27851

943
6040
48852

100000
1000209
603401

6.30
4.46
0.049

y j ) − λ 2 · ϵi )

Experimental Setup

4.1.1 Dataset Description. We evaluate the performance of our
AutoSVD and AutoSVD++ models on the three public accessible
datasets. MovieLens1 is a movie rating dataset that has been widely
used on evaluating CF algorithms, we use the two stable benchmark
datasets, Movielens-100k and Movielens-1M. MovieTweetings[1]
is also a new movie rating dataset, however, it is collected from
social media, like twitter. It consists of realistic and up-to-date data,
and incorporates ratings from twitter users for the most recent
and popular movies. Unlike the former two datasets, the ratings
scale of MovieTweetings is 1-10, and it is extremely sparse. The
content information for Movielens-100K consists of genres, years,
countries, languages, which are crawled from the IMDB website2 .
For Movielens-1M and Movietweetings, we use genres and years as
the content information. The detailed statistics of the three datasets
are summarized in Table 1.

then modify the parameters by moving in the opposite direction
of the gradient. We loop over all known ratings in K. Update rules
for AutoSVD are as follows:

1

#items

4.1

de f

ˆ
eui = rui − rui

ϵi ← ϵi + γ 2 (eui · (Uu + |N (u)| − 2 ·

dataset

(15)

j ∈N (u)
1

∀j ∈ N (u) : y j ← y j + γ 2 (eui · |N (u)| − 2 · (β · cae(Ci ) + ϵi ) − λ 2 · y j )
(16)
Where γ 1 and γ 2 are the learning rates, λ 1 and λ 2 are the regularisation weights. Update rule for Uu of AutoSVD++ is identical to
equation (14).
Although AutoSVD++ can easily incorporate implicit information, it’s very costly when updating the parameter y. To accelerate
the training process, similar to [13], we devise an efficient training
algorithm, which can significantly decrease the computation time
of AutoSVD++ while preserving good performance. The algorithm
for AutoSVD++ is shown in Algorithm 1.

4

4.1.2 Evaluation Metrics. We employ the widely used Root Mean
Squared Error (RMSE) as the evaluation metric for measuring the
prediction accuracy. It is defined as
v
t
1 Õ
ˆ − rui )2
RMSE =
(rui
(17)
|T |
(u,i)∈T

ˆ denotes
where |T | is the number of ratings in the testing dataset, rui
the predicted ratings for T , and rui is the ground truth.

4.2

Evaluation Results

4.2.1 Overall Comparison. Except three baseline methods including NMF, PMF and BiasedSVD, four very recent models closely
relevant to our work are included in our comparison.

EXPERIMENTS

In this section, extensive experiments are conducted on three realworld datasets to demonstrate the effectiveness of our proposed
models.

1 https://grouplens.org/datasets/movielens
2 http://www.imdb.com

959

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Table 2: Average RMSE for Movielens-100k and Movielens1M from compared models with different training data percentages.

1.5
Biased SVD

1.49

SVD++
AutoSVD

12

AutoSVD++

1.48

11

1.47

log(t[ms])

RMSE

1.45

1.44

Methods

ML-100K
90%
50%

Methods

ML-1M
90%
50%

0.997
0.977
*
0.931
0.936
0.938
0.925
0.926

NMF
PMF
U-AutoRec
RBM-CF
Biased SVD
SVD++
AutoSVD
AutoSVD++

0.915
0.883
0.874
0.854
0.876
0.855
0.864
0.848

7
6

90%

0.927
0.890
0.911
0.901
0.889
0.884
0.877
0.875

50%

Training Size

5
10

20

30

40

50

60

70

80

90

training size (%)

(a)

(b)

Figure 2: (a) Average RMSE Comparson on MovieTweetings
Dataset (the lower the better). (b) Comparison of training
time of one epoch on Movielens-100K (the lower the better).
AutoSVD++, which significantly speeds up the training process. We
conduct a comprehensive set of experiments on three real-world
datasets. The results show that our proposed models perform better
than the compared recent works.
There are several extensions to our model that we are currently
pursuing .
• First, we will leverage the abundant item content information such as textual, visual information and obtain richer
feature representations through stacked Contractive Autoencoders;
• Second, we can further improve the proposed model by
incorporating temporal dynamics and social network information.

• RBM-CF [9], RBM-CF is a generative, probabilistic collaborative filtering model based on restricted Boltzmann
machines.
• NNMF (3HL) [2], this model combines a three-layer feedforward neural network with the traditional matrix factorization.
• mSDA-CF [6] , mSDA-CF is a model that combines PMF
with marginalized denoising stacked auto-encoders.
• U-AutoRec [10], U-AutoRec is novel CF model based on
the autoencoder paradigm. Same as [10], we set the number
of hidden units to 500.

REFERENCES
[1] Simon Dooms, Toon De Pessemier, and Luc Martens. 2013. MovieTweetings: a
Movie Rating Dataset Collected From Twitter. In Workshop on Crowdsourcing
and Human Computation for Recommender Systems, CrowdRec at RecSys 2013.
[2] Gintare Karolina Dziugaite and Daniel M Roy. 2015. Neural network matrix
factorization. arXiv preprint arXiv:1511.06443 (2015).
[3] Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative filtering for implicit feedback datasets. In Data Mining, 2008. ICDM’08. Eighth IEEE International
Conference on. Ieee, 263–272.
[4] Alexandros Karatzoglou, Balázs Hidasi, Domonkos Tikk, Oren Sar-Shalom, Haggai Roitman, and Bracha Shapira. 2016. RecSys’ 16 Workshop on Deep Learning
for Recommender Systems (DLRS). In Proceedings of the 10th ACM Conference on
Recommender Systems. ACM, 415–416.
[5] Yehuda Koren. 2008. Factorization meets the neighborhood: a multifaceted
collaborative filtering model. In Proceedings of the 14th ACM SIGKDD international
conference on Knowledge discovery and data mining. ACM, 426–434.
[6] Sheng Li, Jaya Kawale, and Yun Fu. 2015. Deep collaborative filtering via marginalized denoising auto-encoder. In Proceedings of the 24th ACM International on
Conference on Information and Knowledge Management. ACM, 811–820.
[7] Francesco Ricci, Lior Rokach, Bracha Shapira, and Paul B. Kantor. 2010. Recommender Systems Handbook. Springer-Verlag New York, Inc., NY, USA.
[8] Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio.
2011. Contractive auto-encoders: Explicit invariance during feature extraction.
In Proceedings of the 28th international conference on machine learning (ICML-11).
833–840.
[9] Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey Hinton. 2007. Restricted Boltzmann machines for collaborative filtering. In Proceedings of the 24th international
conference on Machine learning. ACM, 791–798.
[10] Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie. 2015.
Autorec: Autoencoders meet collaborative filtering. In Proceedings of the 24th
International Conference on World Wide Web. ACM, 111–112.
[11] Chong Wang and David M Blei. 2011. Collaborative topic modeling for recommending scientific articles. In Proceedings of the 17th ACM SIGKDD international
conference on Knowledge discovery and data mining. ACM, 448–456.
[12] Hao Wang, Naiyan Wang, and Dit-Yan Yeung. 2015. Collaborative deep learning
for recommender systems. In Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. ACM, 1235–1244.
[13] Diyi Yang, Tianqi Chen, Weinan Zhang, Qiuxia Lu, and Yong Yu. 2012. Local
implicit feedback mining for music recommendation. In Proceedings of the sixth
ACM conference on Recommender systems. ACM, 91–98.

We use the following hyper-parameter configuration for AutoSVD
in this experiment, γ 1 = γ 2 = 0.01, λ 1 = λ 2 = 0.1, β = 0.1 . For
AutoSVD++, we set γ 1 = γ 2 = 0.007, λ 1 = 0.005, λ 2 = 0.015, and
β = 0.1. For all the comprison models, we set the dimension of
latent factors k = 10 if applicable. We execute each experiment for
five times, and take the average RMSE as the result.
According to the evaluation results in Table 2 and Figure 2(a), our
proposed model AutoSVD and AutoSVD++ consistently achieve
better performance than the baseline and compared recent methods. On the ML-100K dataset, AutoSVD performs slightly better
than AutoSVD++, while on the other two datasets, AutoSVD++
outperforms other approaches.
4.2.2 Scalability. Figure 2(b) shows CPU time comparison in log
scale. Compared with traditional SVD++ and Original AutoSVD++,
our efficient training algorithm achieves a significant reduction in
time complexity. Generally, the optimized AutoSVD++ performs
R̄ times better than original AutoSVD++, where R̄ denotes the
average number of items rated by users[13]. Meanwhile, compared
with biased SVD model, the incorporated items Cae(Ci ) and offset
ϵi does not drag down the training efficiency. This result shows
our proposed models are easy to be scaled up over larger datasets
without harming the performance and computational cost.

5

8

1.42

1.4

0.958
0.952
0.907
*
0.911
0.913
0.901
0.904

9

1.43

1.41

NMF
PMF
NNMF(3HL)
mSDA-CF
Biased SVD
SVD++
AutoSVD
AutoSVD++

Biased SVD
SVD++
AutoSVD
Original AutoSVD++
Efficient AutoSVD++

10

1.46

CONCLUSIONS AND FUTURE WORK

In this paper, we present two efficient hybrid CF models, namely
AutoSVD and AutoSVD++. They are able to learn item content
representations through CAE, and AutoSVD++ further incorporates
the implicit feedback. We devise an efficient algorithm for training

960

