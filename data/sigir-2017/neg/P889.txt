Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

A Hierarchical Multimodal Attention-based Neural Network
for Image Captioning
Yong Cheng

Fei Huang

Lian Zhou

School of Computer Science
Shanghai Key Laboratory of Intelligent
Information Processing
Fudan University, China
13110240027@fudan.edu.cn

School of Computer Science
Shanghai Key Laboratory of Intelligent
Information Processing
Fudan University, China
15210240036@fudan.edu.cn

School of Computer Science
Shanghai Key Laboratory of Intelligent
Information Processing
Fudan University, China
16110240019@fudan.edu.cn

Cheng Jin

Yuejie Zhang

Tao Zhang

School of Computer Science
Shanghai Key Laboratory of Intelligent
Information Processing
Fudan University, China
jc@fudan.edu.cn

School of Computer Science
Shanghai Key Laboratory of Intelligent
Information Processing
Fudan University, China
yjzhang@fudan.edu.cn

School of Information Management and
Engineering
Shanghai University of Finance
and Economics, China
taozhang@mail.shufe.edu.cn

generation model must identify what objects are in the picture, the
attributes of these objects, and the relationship between them. A
language model is needed to express the semantic understanding
with meaningful sentences. Thus it’s necessary to construct an
effective unified model to generate optimal captions for images.
Some pioneering approaches try to address image captioning
by using hard-coded visual concepts and sentence templates [3].
However, such methods are highly human-crafted and the
generated sentences are less natural. Recently, inspired by the
neural translation model [4], the combination of Convolutional
Neural Network (CNN) and Recurrent Neural Network (RNN) is
widely applied to image captioning. Kiros et al. proposed
Multimodal Log-Bilinear Models to generate text conditioned on
images [5]. Mao et al. proposed a model called m-RNN to generate
image captions with two sub-networks [6]. Vinyals et al. and
Donahue et al. also used the “CNN+RNN” framework to generate
captions but replaced the normal RNN with LSTM [1, 7]. Very
recently, Xu et al. introduced two attention-based image caption
generators, in which the visual attention was integrated to show
“where” and “what” the attention focused on [8]. You et al.
proposed an algorithm to generate image captions with the
semantic attention on concept proposals [9]. Such models first
encode the image intto a fixed-length vector using CNN, and then
feed the feature vector into the RNN decoder to generate the target
sentences. The fundamental CNN-RNN model is still a “rough” one
without considering much common sense in the expressing
process of human. Usually humans tend to describe an image in a
hierarchical way. Given an image, they may first recognize the
objects and the relationships among them, and then use a natural
sentence to describe the image. Another important aspect is the
human attention, which means the human’s attention dynamically
focuses on different objects or scenes in the image.
Motivated by the above observations, we propose a novel
hierarchical multimodal attention-based neural network, which
can be treated as an “end-to-end” network to generate good
descriptions for images. The whole network contains three subnetworks, that is, a deep CNN to extract the visual features, a deep
RNN to identify the objects in images sequentially with the deep
visual features as the input, and a multimodal attention-based

ABSTRACT
A novel hierarchical multimodal attention-based model is
developed in this paper to generate more accurate and descriptive
captions for images. Our model is an “end-to-end” neural network
which contains three related sub-networks: a deep convolutional
neural network to encode image contents, a recurrent neural
network to identify the objects in images sequentially, and a
multimodal attention-based recurrent neural network to generate
image captions. The main contribution of our work is that the
hierarchical structure and multimodal attention mechanism is both
applied, thus each caption word can be generated with the
multimodal attention on the intermediate semantic objects and the
global visual content. Our experiments on two benchmark datasets
have obtained very positive results.

CCS CONCEPTS
• Information retrieval → Multimedia and multimodal
retrieval

KEYWORDS
Image Captioning, Multimodal Attention, Hierarchical Recurrent
Neural Network, Long-Short Term Memory Model

1 INTRODUCTION
Automatically generating the textual description for an image is a
very challenging task [1]. Although it is natural for a human to
describe an image with a quick glance, it needs more complicated
design for a computer to do the same task [2]. The caption
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Request permissions from Permissions@acm.org.
SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 Association for Computing Machinery.
ACM ISBN 978-1-4503-5022-8/17/08…$15.00
http://dx.doi.org/10.1145/3077136.3080671

889

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

RNN to generate the description based on the extracted visual
features and the identified objects. Every time the proposed model
generates a word, it searches for a set of positions in the generated
object sequence and the whole image to obtain useful multimodal
information as the input for that moment. Thus the most relevant
object information is concentrated, and then the model can predict
a target word based on the contextual multimodal information and
all the previous generated words. Our experiments have obtained
very positive results on two benchmark datasets.

a solution to address these problems by controlling the gates that
allow the network to learn when and how to update the cell’s
memory. In our model, the Vanilla LSTM architecture is integrated
into our network, which contains three gates (i.e., input gate,
forget gate and output gate). The input gate and output gate
control the IO information through the LSTM block, while the
forget gate determines when to forget the previous hidden states.
The gates and memory cell are defined as:
𝑖𝑡 = 𝜎(𝑊𝑖𝑥 𝑥 𝑡 + 𝑊𝑖𝑦 𝑦 𝑡−1 + 𝑏𝑖 )
𝑓 𝑡 = 𝜎(𝑊𝑓𝑥 𝑥 𝑡 + 𝑊𝑓𝑦 𝑦 𝑡−1 + 𝑏𝑓 )
𝑜 𝑡 = 𝜎(𝑊𝑜𝑥 𝑥 𝑡 + 𝑊𝑜𝑦 𝑦 𝑡−1 + 𝑏𝑜 )
𝑧 𝑡 = ℎ(𝑊𝑧𝑥 𝑥 𝑡 + 𝑊𝑧𝑦 𝑦 𝑡−1 + 𝑏𝑧 )
𝑐 𝑡 = 𝑖𝑡 ☉𝑧 𝑡 + 𝑓 𝑡 ☉𝑐 𝑡−1
𝑦 𝑡 = 𝑜 𝑡 ☉ℎ(𝑐 𝑡 )
(5)
where ☉ denotes the point-wise multiplication of two vectors; σ
and h are two point-wise non-linear activation functions, that is,
t
t
sigmoid and tangent; x is the input vector at time t; y is the
output vector; and W and b are two model parameters.
Our hierarchical attention-based model has three inter-related
sub-networks, i.e., a convolutional neural network for representing
the input image, an intermediate recurrent neural network for
recognizing the objects in the image, and a final recurrent neural
network for generating the captions for the image. We apply the
16-layer VGG-Net as the encoding network, and then the raw
image pixels can be transformed into a 4,096-dimensional
activation of fully connected layer before the classifier, which can
be used as the feature vector to represent the input image. Thus
the feature vector can be mapped into the embedding space via a
linear transformation, shown as follows:
𝑣 = 𝑊𝑣 [𝐶𝑁𝑁𝜃𝑐 (𝐼)] + 𝑏𝑣
(6)
where θc is the parameter of CNN; the mapping matrix Wv and the
bias weight bv are used to map the feature vector of I into the
embedding space; and v is the encoded vector to be fed into RNN.
The objective of the intermediate RNN is to generate the
objects appearing in the image. Different from the other object
identification methods using the traditional classification model,
we use the sequential model to generate the objects one by one.
The main reason is because we observe that humans tend to
identify the objects sequentially according to their saliency in the
image, and the most significant object would be noticed first. Thus
we incorporate such an observation into our model and utilize
RNN to imitate the same process. In our work, all the object words
are extracted from the training captions. At time t, the current
t
object word O is generated through a LSTM layer and a softmax
layer. The input of LSTM contains two parts, i.e., the previous
t-1
hidden state y and the word embedding of the previous object
t-1
word O . At time t=0, the encoded vector v and the word
embedding of the start symbol ‘<S>’ will be fed to the network,
shown as follows:
𝑦 0 = 𝑣, 𝑂 0 = ′ < 𝑆 > ′
𝑡
𝑦 = 𝐿𝑆𝑇𝑀( 𝑦 𝑡−1 , 𝑊𝑒𝑜 (𝑂 𝑡−1 ))
𝑝(𝑂 𝑡 ) = 𝑠𝑜𝑓𝑡𝑚𝑎𝑥(𝑦 𝑡 )
(7)
where 𝑊𝑒𝑜 ∈ 𝑅|𝐷𝑜|∗𝑑𝑖𝑚 is the word embedding matrix, Do is the
dictionary of the object word, dim is the dimension of the word
t
vector; and p(O ) is a probability distribution over Do.
As for the final caption generation part in our model, different
from the intermediate RNN, we introduce the multimodal
attention mechanism into the model. It means except the original
input, the contextual object information will be a new input to
LSTM. The intuitive explanation for this is that each time a

2 HIERARCHICAL MULTIMODAL ATTENTIONBASED NETWORK
The ultimate goal of image captioning is to generate good
descriptions for images. Inspired by the “encoder-decoder”
framework in neural machine translation, many existing methods
apply this principle to the caption generation. These models make
use of a convolutional neural network to map the source image
into a fixed-dimension vector, and then a recurrent neural
network is used to decode the fixed vector to the target sentences.
The training goal is to maximize the log-likelihood of the correct
description given the original image, shown as follows:
𝑜𝑏𝑗(𝜃) = 𝑎𝑟𝑔𝑚𝑎𝑥𝜃 ∑(𝐼,𝑆) 𝑙𝑜𝑔𝑃(𝑆|𝐼; 𝜃)
(1)
where I denotes an original image; S is the correct description for I;
and θ is the model parameter. We model the image captioning
process in a hierarchical way. First the important objects in an
image are recognized, and then the description sentence is
generated based on such observed objects and the whole image.
Because the sentence is directly related to the image content, we
believe that the important objects in the image may be described
with some words in the sentence. Thus we transfer the sentence S
to a tuple (O, S), where O is a sequence that consists of all the
object words appearing in S, and O keeps the same order as that in
S. The new objective function of our model is defined as:
𝑜𝑏𝑗(𝜃) = 𝑎𝑟𝑔𝑚𝑎𝑥𝜃 ∑(𝐼,𝑂,𝑆) 𝑙𝑜𝑔𝑃(𝑂, 𝑆|𝐼; 𝜃)
(2)
= 𝑎𝑟𝑔𝑚𝑎𝑥𝜃 ∑(𝐼,𝑂) 𝑙𝑜𝑔𝑃(𝑂|𝐼; 𝜃) + ∑(𝐼,𝑆) 𝑙𝑜𝑔𝑃(𝑆|𝐼, 𝑂; 𝜃)
where the first part corresponds to the intermediate object word
generation, and the second part is the caption generation based on
the observed object information and the global image scene. Thus
the chain rule can be applied to model the object sequence and the
caption sentences, shown as follows:
𝑃(𝑂|𝐼; 𝜃) = ∏𝑡=1..𝑇𝑂 𝑝(𝑂 𝑡 |𝐼, 𝑂1..𝑡−1 ; 𝜃)
(3)
𝑃(𝑆|𝐼, 𝑂; 𝜃) = ∏𝑡=1..𝑇𝑆 𝑝(𝑆 𝑡 |𝐼, 𝑆1..𝑡−1 , 𝑂; 𝜃)
(4)
where To is the length of the object sequence O; and Ts is the
length of the caption sentence S.
In our work, the convolutional neural network is exploited to
represent the image, and two hierarchical LSTM-based recurrent
neural networks are joined to model the object sequence and the
caption sentence separately. In addition, we introduce the
multimodal attention mechanism into the final caption generation,
which means the intermediate LSTM information will be used as
t
the context in the caption generation. Each time when a word S is
generated, the attention will be focused on the most relevant
information in the object sequence O and the global image I.
RNNs with Long Short-Term Memory (LSTM) is an effective
model to model the sequential data, which has been used to solve
many learning problems. The main idea behind the LSTM
architecture is the memory cell which can store its state over time,
and several non-linear gates to control the behavior of the
memory cell. Compared to the traditional RNN which suffers the
problems of gradient vanishing and exploding, LSTM can provide

890

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

follow the split setting in [2], that is, 29,000 images for training,
1,000 for validation and 1,000 for testing. Each image comes with 5
reference sentences. As for MSCOCO, there are 82,783 training
images and 40,504 validation images, and each image with 5
descriptions. Following the same split setting in [2], all the 82,783
images are used for training, 5,000 for validation and 5,000 for
testing. The n-gram BLEU and METEOR metrics are adopted,
which are computed with the coco-caption code [10].
One important improvement of our model is that we adopt the
hierarchical framework within the whole network. To show the
effect of the hierarchical mechanism, we compare the performance
results for different model structures. Here we design three
different patterns according to the content of the intermediate
layer. 1) No_Hie: It means there is no intermediate layer in the
model, and the visual feature extracted by CNN is fed into the final
LSTM directly to generate the captions. 2) NN_Hie: In this model,
we aggregate all the nouns appear in the captions as the
intermediate RNN vocabulary. 3) ATT_Hie: All the most common
semantic attributes which can be any part of speech (noun, verb,
adjective, etc.) are aggregated as the vocabulary in the
intermediate RNN layer. Pattern NN_Hie and Pattern ATT_Hie
use the same hierarchical framework, and the only difference is
the vocabulary content used by the intermediate RNN. The
intermediate vocabulary sizes for three patterns are all set to 256.
The related experimental results are shown in Table 1.

caption word is generated, the attention will focus on the
multimodal information source including some semantic concepts
and the global image scene. At time t, the input of LSTM contains
t-1
three parts, i.e., the previous hidden state h , the word embedding
t-1
t
of the previous generated word S , and the context vector c
which depends on the output information of the intermediate
t
RNN. The definition of c is shown as:
𝑐 𝑡 = 𝛼𝑡,0 ∗ 𝑦 0 + ∑𝑖=1..𝑇𝑜 𝛼𝑡,𝑖 ∗ 𝑦 𝑖
(8)
where the first part means the focus on the global image scene; the
th
latter part means a strong focus on the parts surrounding the i
object; To is the length of the object sequence; and the weight αt,0 is
used to measure the correlation between the current state and the
global image, while αt, i (i >=1) measures the correlation between
th
the current state and the i object, the related definition is shown
as follows:
exp(𝑒𝑡,𝑖 )
𝛼𝑡,𝑖 =
∑𝑘=0..𝑇𝑜 exp(𝑒𝑡,𝑘 )
𝑒𝑡,𝑖 = 𝑐𝑜𝑟𝑟(ℎ𝑡−1 , 𝑦 𝑖 )
0 ≤ 𝑖 ≤ 𝑇𝑜
(9)
where corr( ) is computed through a multi-layer perceptron. Thus
the final generation process is formulated as:
ℎ0 = 𝟎, 𝑆 0 =′ < 𝑆 >′
1
1
𝑐0 =
∗ 𝑦0 +
∑
𝑦𝑖
𝑇𝑂 + 1
𝑇𝑂 + 1 𝑖=1..𝑇𝑜
ℎ𝑡 = 𝐿𝑆𝑇𝑀(ℎ𝑡−1 , 𝑐 𝑡−1 , 𝑊𝑒𝑠 (𝑆 𝑡−1 ))
𝑝(𝑆 𝑡 ) = 𝑠𝑜𝑓𝑡𝑚𝑎𝑥(ℎ𝑡 )
(10)
|𝐷𝑠 |∗𝑑𝑖𝑚
where 𝑊𝑒𝑠 ∈ 𝑅
is the word embedding matrix for all the
words in the captions, Ds is the word dictionary, dim is the word
t
vector dimension; and the output state h can be fed to a softmax
t
layer to produce a probability distribution p(S ) over all the words.
It is noted that all the object words are collected from the caption
words, and they share one common word embedding matrix,
which means Do∈Ds and Weo∈Wes.
In our work, the triple training data (I, O, S) is needed to train
the model, where I and S represent the image and its description,
and O is the object sequence appearing in I. As all the salient
objects have already been included in the caption S, we take an
approximate approach to get O. For each image-sentence pair, we
first do the part-of-speech tagging for each word in the sentence.
All the nouns are selected as the object names, and such nouns
remain the original order in the sentence. For example, given a
sentence “A/DT dog/NN and/CC a/DT cat/NN are/VBP
playing/VBG on/IN the/DT grass/NN”, the object sequence is “dog
cat grass”. The object sequence order is also considered in our
model. Thus the final cost function can be defined as:
𝐶𝑜𝑠𝑡 =
1
1
∑(𝐼,𝑂,𝑆) ( ∑𝑡=1..𝑇𝑜 −𝑙𝑜𝑔𝑝(𝑂 𝑡 ) +
𝑁𝑠

1
𝑇𝑠

Table 1: The experimental results with different patterns.
Dataset
Flickr30k
MSCOCO

B-1
66.3
66.5
65.8
66.6
71.0
69.6

B-2
42.3
45.0
44.1
46.1
51.3
50.1

B-3
27.7
30.6
29.1
32.9
37.2
36.2

B-4
18.3
20.9
20.1
24.6
27.1
26.4

It can be seen from Table 1 that for the hierarchical multimodal
attention-based model on Flickr30k and MSCOCO, the best
performance can be obtained with Pattern NN_Hie, in which all
the nouns in the captions are aggregated as the vocabulary content
in the intermediate layer. In comparison with the baseline pattern
without any hierarchical mechanism, the performance could be
greatly promoted by adding the hierarchical layer in the model,
which confirms the obvious advantage of our hierarchical model.
In addition, we observe that the performance is affected by the
vocabulary content, when comparing two patterns with different
contents, NN_Hie and ATT_Hie, the performance of NN_Hie
appears better. Since the nouns in the captions usually represent
the object names, these comparative results confirm our
assumption that humans tend to firstly recognize the objects in an
image, and then use a natural sentence to describe the image based
on these objects.
To show the relationships among the intermediate recognized
objects with the final image captions, we compare the
performance rising speeds with different recognition accuracy in
the intermediate layer. Here we use the F-1 value to measure the
recognition performance, which balances the Precision and Recall
values. We conduct the experiment on the largest dataset
MSCOCO, and the experimental results are shown in Figure 1. It
can be clearly viewed that the BLEU scores are proportional to the
F-1 value of the intermediate object identification, which means
that the final performance is greatly affected by the intermediate
results. In our model, the average precision on MSCOCO is about
0.48, and the corresponding BLEU scores are 71.0, 51.3, 37.2 and
27.1 for B-1, B-2, B-3, and B-4 respectively.

𝑇𝑜

∑𝑡=1..𝑇𝑠 −𝑙𝑜𝑔𝑝(𝑆 𝑡 )) +λ ∙ ‖𝜃‖2

Structure Pattern
No_Hie
NN_Hie
ATT_Hie
No_Hie
NN_Hie
ATT_Hie

(11)

where Ns denotes the number of sentences; To and Ts are the
lengths of the object sequence and the caption sentence; and θ
denotes all the parameters. The first two part in the cost function
are the average log-likelihood of the given object sequence and the
training sentences, and the last one is a regularization term. Our
training objective is to minimize this cost function, which is
equivalent to maximizing the probability of the training data.

3 EXPERIMENT AND ANALYSIS
Two benchmark datasets are used in our experiments, i.e.,
Flickr30k and MSCOCO. Flickr30k contains 31,783 images. We

891

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

incorporate more complicated visual attention mechanism to
further acquire stronger description ability.
Table 2: The related comparison results.
Dataset

Flickr30k

Figure 1: The experimental results on MSCOCO.
To show the multimodal attention changes during the caption
generation process, we test the attention weight distribution on
the semantic information (semantic_att) and visual image
contents (visual_att) respectively for different positions in the
generated word sequences. We perform the experiments on
Flickr30k and MSCOCO, and the related experimental results are
shown in Figure 2. It can be seen that the rules of the attention
weight changes are similar for both datasets. In the beginning of
the caption generation process, the attention almost totally focuses
on the image contents. As the position moves, the attention
gradually focuses on the semantic information more and becomes
flat at last. The intuitive explanation for this is that the model
tends to explore the global scene information in the image at the
beginning, and then considers the semantic information for more
details.

MSCOCO

Approach
NeuralTalk
Google NIC
LogBilinear
g-LSTM
Hard-Attention
Ours
NeuralTalk
Google NIC
LogBilinear
g-LSTM
Hard-Attention
Ours

B-1
57.3
66.3
60.0
64.6
66.9
66.5
62.5
66.6
70.8
67.0
71.8
71.0

B-2
36.9
42.3
38.0
44.6
43.9
45.0
45.0
46.1
48.9
49.1
50.4
51.3

B-3
24.0
27.7
25.4
30.5
29.6
30.6
32.1
32.9
34.4
35.8
35.7
37.2

B-4
15.7
18.3
17.1
20.6
19.9
20.9
23.0
24.6
24.3
26.4
25.0
27.1

METEOR
16.9
17.9
18.5
19.1
19.5
22.7
23.0
23.3

4 CONCLUSIONS
A novel multimodal attention-based neural network is
implemented to generate good descriptions for images. Different
from the existing “CNN+RNN” framework, we generate the
captions in a hierarchical way, which is more consistent with the
expressing process of human. In our model, the object sequence is
first generated through an intermediate recurrent neural network,
and then the final captions are generated by another recurrent
neural network with the multimodal attention. Our future work
will focus on making our system available online, so that more
Internet users can benefit from our research.

ACKNOWLEDGMENTS
This work was supported by National Natural Science Fund of
China (61572140; 61672165), Shanghai Municipal Science and
Technology Commission (16JC1420401; 16511105402; 16511104704),
Shanghai Municipality Program of Technology Research Leader
(17XD1425000) and The Application of Big Data Computing
Platform in Smart Lingang New City based BIM and GIS
(#ZN2016020103). Yuejie Zhang is the corresponding author.

REFERENCES
[1]

Vinyals O., Toshev A., Bengio S., and Erhan D., “Show and tell: A neural image
caption generator,” CVPR 2015, pp. 3156-3164, 2015.
[2] Karpathy A., and Li F.F., “Deep visual-semantic alignments for generating image
descriptions,”, CVPR 2015, pp. 3128-3137, 2015.
[3] Kulkarni G., Premraj V., and Berg T.L., “Baby talk: Understanding and
generating simple image descriptions,” CVPR 2011, pp. 1601-1608, 2011.
[4] Cho K., Bahdanau D., and Bengio Y., “Learning phrase representations using
RNN encoder-decoder for statistical machine translation,” EMNLP 2014, pp.
1724-1734, 2014.
[5] Kiros R., Salakhutdinov R., and Zemel R., “Multimodal neural language models,”
ICML 2014, pp. 595-603, 2014.
[6] Mao J.H., Xu W., and Yuille A., “Deep captioning with multimodal Recurrent
Neural Networks (m-RNN),” arXiv:1410.1090, 2014.
[7] Donahue J., Hendricks L.A., and Rohrbach M., “Long-term recurrent
convolutional networks for visual recognition and description,” CVPR 2015, pp.
2625-2634, 2015.
[8] Xu K., Ba J.L., and Bengio Y. “Show, attend and tell: Neural image caption
generation with visual attention,” ICML 2015, pp. 2048-2057, 2015.
[9] You Q.Z., Jin H.L., Wang Z.W., Fang C., and Luo J.B., “Image captioning with
semantic attention,” arXiv:1603.03925, 2016.
[10] Chen X.L., Fang H., Lin T.Y., Vedantam R., Gupta S., Dollar P., and Zitnick C.L.,
“Microsoft COCO captions: Data collection and evaluation server,”,
arXiv:1504.00325, 2015.
[11] Jia X., Gavves E., and Tuytelaars T., “Guiding long-short term memory for image
caption generation,” ICCV 2015, 2015.

Figure 2: The attention weights for different positions.
Compared to the common “CNN+RNN” framework for image
captioning in recent years, our approach is a new exploration for
taking full advantage of the hierarchical characteristic and
multimodal attention mechanism in the whole expressing process.
We have performed a comparison between our method and the
other existing popular approaches, that is, LogBilinear [5], Google
NIC [1], NeuralTalk [2], g-LSTM [11], Hard-Attention [8], as shown
in Table 2. For fairness, all the methods reported in Table 2 apply
the “CNN+RNN” framework, and the VGG-Net or GoogLeNet are
used as the image feature decoder. It can be found that on
Flickr30k our approach can achieve the BLEU scores of 66.5, 45.0,
30.6, 20.9 and 19.1 for METEOR, and for MSCOCO the scores are
71.0, 51.3, 37.2, 27.1 and 23.3. In general, our approach performs on
par with the latest state-of-the-art method on both datasets, and
the experimental results confirm the advantage of our hierarchical
modeling structure. Since all the other methods just apply the
common “CNN+RNN” framework, they do not consider the
hierarchical mechanism in their network. Compared with the
single-attention model in [8] we can see that our multimodal
attention mechanism can achieve better performance in most
cases, which confirms the advantage of our model again. In
addition, our model is much simpler and can be easy to

892

