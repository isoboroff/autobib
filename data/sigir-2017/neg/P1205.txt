Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

Timestamping Entities using Contextual Information
Adam Jatowt

Kyoto University
Japan
adam@dl.kuis.kyoto-u.ac.jp

Daisuke Kawai

Kyoto University
Japan
daisuke@gauge.scphys.kyoto-u.ac.jp

Katsumi Tanaka

Kyoto University
Japan
tanaka@dl.kuis.kyoto-u.ac.jp

estimating entity lifetimes. In the experiments, we test our
method on the dataset of person-related articles extracted from
Wikipedia trying to predict the birth and death dates of persons.
Our approach is mainly based on harnessing the link structure
without the need for touching the article content. It bears then
resemblance to the methods for estimating publication dates of
web pages using their link structures [7,8]. This kind of
approach can be advantageous in cases when the articles on
entities have limited content, especially, when they lack
sufficient amount of temporal references within their content. In
the experiments we demonstrate that the context-based
approach is better than the content-based approach which relies
on extracting dates from article contents.

ABSTRACT
Wikipedia is the result of collaborative effort aiming to represent
human knowledge and to make it accessible to the public. Many
Wikipedia articles however lack key metadata information. For
example, relatively large number of people described in
Wikipedia have no information on their birth and death dates.
We propose in this paper to estimate entity’s lifetimes using link
structure in Wikipedia focusing on person entities. Our approach
is based on propagating temporal information over links
between Wikipedia articles.

CCS CONCEPTS
• Information systems → Information Retrieval; Retrieval
Tasks and Goals; Information Extraction

2 METHODOLOGY

Given an entity with unknown lifetime (or with only partial
lifetime information) the task is to predict its lifetime, that is, the
values of its start and end temporal attributes [tb,te], such as the
birth and death in the case of a person.
While it could be possible to extract temporal information
from an article content to estimate person’s lifetime, we are
mainly interested in predicting the entity’s lifetime based on
contextual information without using the article’s content.
Actually, for many entities the content can be quite sparse, it
may lack sufficient number of temporal references, or the
temporal references contained in the content can be outside the
range of the entity’s lifetime1. The key hypothesis behind our
method is as follows:

KEYWORDS
Wikipedia; entity dating; temporal link analysis
ACM Reference format:
A. Jatowt, D. Kawai, and K. Tanaka. 2017. Timestamping Entities using
Contextual Information. In Proceedings of ACM SIGIR conference, Tokyo,
Japan, August 2017 (SIGIR’17), 4 pages.
DOI: 10.1145/3077136.3080762

1 INTRODUCTION

Wikipedia abounds in various types of entities and offers rich
data about them, yet, still relatively many of the articles lack
basic metadata about the discussed entities [6]. As Wikipedia is
used in many knowledge processing and information retrieval
tasks, we should look into effective means of its improvement
and complementation [2]. Automatically extending and
semantifying Wikipedia could then help boosting accuracy of
many information processing systems that rely on its data.
In this work we especially focus on temporal information such
as entity lifetime. The key temporal attributes of entities - their
start and end dates - are used in many applications such as ones
for building entity-centered graphs, studying and detecting
entity-to-entity relationships, or in other knowledge processing
tasks that utilize semantic knowledge derived from Wikipedia
(e.g., [3,4,5]). We develop an approach to automatically

Hypothesis: Entity’s lifetime is similar to the lifetimes of its
related entities.
In other words, the lifetimes of related entities are correlated
with the one of the target entity. Naturally, the strength of this
correlation depends on entity types. Nevertheless, it should be
relatively high for many entities including the most common
entity type in Wikipedia — persons. In this work we focus on
persons as an example and consider the links between personrelated Wikipedia articles as an indication of relevance. Hence,
the variant of the above hypothesis for the case of Wikipedia
articles about people is: A person tends to be linked more
from/with contemporary people (i.e., people alive at its lifetime)
than from/with people from other time, especially, distant time.
The higher probability of links between people living at the same

Permission to make digital or hard copies of part or all of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for profit or commercial advantage and that copies bear this notice
and the full citation on the first page. Copyrights for third-party components of
this work must be honored. For all other uses, contact the owner/author(s).
SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan
© 2017 Association for Computing Machinery.

1

E.g., events or related entities that happened before or after the entity’s lifetime
but which either impacted it or are related to it.

ACM ISBN 978-1-4503-5022-8/17/08…$15.00
http://dx.doi.org/10.1145/3077136.3080762

1205

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

time (a kind of “temporal homophily”) is intuitive and has been
also recently demonstrated in Wikipedia [6].
In our approach we will first estimate the initial probability
distribution of a person’s lifetime (Sec. 2.1). Based on such
computed probability we will then introduce several approaches
to determine the person’s lifetime (Sec. 2.2).

probability distribution for that decade. Otherwise, we assign 0.
bi is then expressed as follows:
( , )≠0
1⁄| |
=
( , )=0
0
We perform the iterative computation as in Eq. 2 separately for
each decade belonging to the time frame of analysis. After all the
scores are computed for all decades, we normalize the scores of
every person to obtain the probability distribution for each
single person over the entire time frame (all decades).

2.1 Probability Propagation
We first create a directed social graph G(V,E) with V being the
set of nodes representing Wikipedia articles on persons and E
being the set of connecting them edges (see Sec. 3.1 for details on
the dataset creation). An edge eij (eij∈E) from a node vi to a node
vj (v∈V) indicates the presence of a hypertext link in vi that
leads to vj.
For predicting the lifetimes, we propagate the temporal
information over the edges. In our approach we propagate the
data separately for each decade2. Intuitively, if a person has links
from many people who lived at a given decade, then it is quite
probable that this decade belongs to the lifetime of the person. Let
S(di,p) be the score of decade di determining the probability of di
as for whether it should be included in the lifetime of a person p.
To compute S(di,p) we apply the random walk theory [8]. In the
standard procedure within the random walk theory, node scores
are computed iteratively as follows:
1
= (1 − ) × +
(1)
| | | |×

2.2 Lifetime Detection
We propose four procedures for determining the entity’s lifetime
boundary points, tb and te, based on the probability distributions
computed by the approach described in the previous section.
Start Point Focused Method (SPFM). This method first finds
the birth date of an entity and then uses it as a reference point
for determining the entity’s end date. The detailed procedure is:
1.
2.

3.

Ri is a vector containing node scores for decade di, M is
aperiodic transition matrix, while α is a decay factor equal to
0.15. However, in our approach we bias the random walk on G
using the static score distribution vector bi as in Eq. 2.
= (1 − )

×

+

l denotes here the average length of the lifetime of persons
with known dates in our dataset. For finding it we apply linear
regression. Function y(db) which predicts the death date given
the birth date is fitted on the data to return the death decade for
a person born at given db (R2 = 0.917):

(2)

To determine nodes to which the random walk should be
biased through assigning non-zero values to the static score
distribution vector bi, we set up the initial probability
distributions for every person in our dataset using the known
birth and death dates. We denote such scores by Sini(di,p). For
example, a person born in decade 1410s who died in 1500s has
the probability distribution containing zero values in all other
decades outside of her lifetime, while for each decade between
1410s and 1500s (inclusive) it has non-zero values.
In case when there is no metadata given for a person (lack of
start and end dates) we use a uniform probability distribution
over the entire timeline. On the other hand, in the case of a
partial information, i.e., only one date given, we set the uniform
distribution from (or after) the date until the beginning (or until
the end) of the entire time frame depending on whether the
known date is a birth or a death date. Finally, for the case of
currently alive persons we assume their death decades to be the
end of the time time frame period (i.e., 2010s(!)). Once the initial
probability distributions are prepared, we compute the static
score distribution vector for a given decade such that 1 is
considered for each person who has non-zero value in his initial
2

For a given entity, take its computed probability
distribution.
Find the earliest decade db when the entity’s probability
distribution is higher than the uniform probability
distribution over the entire time frame. Assign db to be the
start date, tb, of the entity’s lifetime.
Find the earliest de (db ≤ de ≤ db+2l) that has higher score
than the score of the uniform distribution (l is described
below), and assign te = de. Otherwise assign te = db+l

(

) = 1.003

+ 54.61

(3)

Hence, we set l to 6 (i.e., 6 decades).
End Point Focused Method (EPFM). The next method proceeds
in a similar fashion to SPFM approach. However, this time the
procedure starts from the end date and uses it as a reference
point for finding the birth date.
Peak Based Method (PBM). This method considers the peak
point as the key reference point for estimating the start and end
dates of a person’s life. Its steps are as follows:
1.
2.
3.

4.

In this work we assume decade granularity for the ease of computation.

1206

For a given entity take its computed probability
distribution.
Find the peak decade dp such that the entity’s probability
distribution has the highest value for dp.
Find the earliest decade db when the entity’s probability
distribution is still higher than the uniform probability
distribution and dp-l ≤ db ≤ dp. Assign db as the start date, tb,
of entity’s lifetime. If there is no such decade, then tb = dp-l/2
Find the oldest decade de when the entity’s probability
distribution is still higher than the uniform probability
distribution and dp ≤ de ≤ dp+l. Assign de as the end date, te,
of entity’s lifetime. If there is no such decade then te = dp+l/2

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

3.2 Test Set and Metrics

Peak Integral Based Method (PIBM). The weak point of the
above three schemes is their strong reliance on the average
probability per decade as given by the uniform distribution. We
introduce another solution that is based on the shape of the
estimated distribution plot of the entities’ lifetime.
1.
2.
3.

4.

5.

6.

To construct the test set we have randomly selected from each
century 90 persons based on their centrality scores computed per
each century using Eq. 1 with V being the set of persons alive in
a given century. A person was assigned to a given century based
on the containment of it midlife point. Next, based on the
centrality scores we selected persons for each century such that
30 were highly linked, 30 were middle linked and 30 were rarely
linked. This was carried by ordering the nodes in each century
based on their computed centrality scores, dividing into three
equal parts and then randomly choosing 30 people from each
part. This selection scheme allowed us to have equal
representation of persons in each century as well as relatively
uniform overall distribution of nodes based on the strengths of
their connectivity. In total, we obtained the test set comprising
990 persons.
To evaluate the accuracy of our methods we use two measures:
Jaccard Coefficient. This measure is based on computing the
relative overlap of the ground truth and the estimated lifetimes.
We compute the length of the intersection of both the periods
and divide it by the length of their union.
Pearson Correlation. We compare two vectors, each with 102
elements (i.e., total number of decades in our time frame) using
Pearson Correlation Coefficient. The first vector is the ground
truth vector and the other one is the vector of the lifetime
estimated by our method. Both vectors have 1s as values on
decades contained in the person’s lifetime and 0s in other
decades.

For a given entity take its computed probability
distribution.
Find the peak decade dp such that the entity’s probability
distribution has the highest value for dp.
Construct a normal distribution at dp with sigma value
equal to l/2. This is equal to assuming that 68.2% persons
have lifetimes of the length l or less than that. We call such
distribution a reference distribution.
Compute the ratios between the values of the computed
probability distribution of a person and the reference
distribution in each decade. Denote the ratio for decade dj as
j. The decade with high ratio is considered more
important.
Let W1,W2,..,W|T| (|T|=l*2) 3 be the lengths of possible
lifetimes and dp∈T. For each Wi find such pairs of tbi∈T and
tei ∈ T that the integral of the entity’s probability
distribution from tbi to tei has the highest value.
⁄∑
⁄∑
Finally,
=∑
and
=∑
after
rounding-off their values.

3 EXPERIMENTS
3.1 Dataset
We utilize the dataset composed of articles from Wikimedia4. We
used DBpedia ontology datasets (PersonData ontology class) [1]
to extract person articles. We then removed noise and
unimportant content from articles retaining only the core
content using BeautifulSoup library5. Lists and content under
common footers (e.g., ‘See also’, ‘References’, ‘External links’,
‘Notes’) were also removed. We collected hyperlinks using
Yago2 knowledge base [5] and merged any redirect nodes as well
as removed self-links. Finally, we collected information about the
start and end dates of persons from Yago2. The dates were then
converted to the decade-level granularity for simplifying
computation. We set up the time frame from AD1000 to AD2017.
The final dataset contains 478k persons and is summarized in
Tab. 1.

3.3 Results
We show the main results in Tab. 2. Initially, we use the same
direction of links as in the original graph G (this setting is called
Inlink-method). In addition, we experiment with reverse
direction of links in G (called Outlink-method). By this we
attempt to judge whether persons linked from the article of a
target person are more useful for estimating her lifetime than the
persons linking to her article. We also combine the computed
probability distributions for the Inlink-method and Outlinkmethod and run the lifetime detection procedures (described in
Sec. 2.2) on such a merged probability distribution. We call this
last approach as Mixed-method. Looking at Tab. 2 we can see
that the best results are obtained for Mixed-method when
using PIBM strategy of lifetime estimation. Outlink-method
and sometimes Mixed-method methods outperform usually the
Inlink-method. It means that using outlinks as a signal gives
better results than using inlinks, and combining information of
the both types is the best approach. We can also see that among
the lifetime estimation strategies PBM and PIBM tend to be
superior to SPFM and EPFM, while PIBM is the best choice for
both types of links.
Table 2 Results for the proposed approaches.

Table 1 Statistics of the dataset used for experiments. Note
that #nodes with both dates includes also currently alive
people.
#nodes (total)

478,367

#nodes with both dates
#nodes with birth date only
#nodes with death date only
#nodes without both dates
#links

389,514
2,538
5,279
81,036
939,549

Method
Inlink

3

I.e., 12 decades is set as the maximum allowed length of lifetime.
https://dumps.wikimedia.org/enwiki
5 https://pypi.python.org/pypi/beautifulsoup4

Outlink

4

Mixed

1207

SPFM
Jacc.
Pear.
0.351
0.395
0.42
0.48
0
9
0.348

0.403

EPFM
Jacc.
Pear.
0.311
0.357
0.43
0.503
2
0.427

0.50
5

PBM
Jacc.
Pear.
0.203
0.216
0.67
0.75
3
2
0.653

0.738

PIBM
Jacc.
Pear.
0.324
0.367
0.687

0.753

0.69
2

0.77
1

Short Research Paper

SIGIR’17, August 7-11, 2017, Shinjuku, Tokyo, Japan

We compare next these results with the lifetime estimation
based on dates extracted from the article content using a method
called Text-method as shown in Tab. 3. The procedure here is
to count the number of times that years within each particular
decade are mentioned in the content of an article of a person
belonging to the test set. We use Gaussian Kernel Density
estimate approach to smooth the obtained histogram of dates for
each person. Finally, we perform normalization for converting
the scores to the probability distribution and we later apply each
of our 4 strategies of lifetime point detection (see Sec. 2.2).
Table 3 Results when using temporal expressions in
text.
Method
Text

SPFM
Jacc.
Pear.
0.296
0.345

EPFM
Jacc.
Pear.
0.232
0.269

PBM
Jacc.
Pear.
0.555
0.641

Figure 1 Pearson Correlation results for different
centuries with PIBM procedure applied for lifetime
detection.

PIBM
Jacc.
Pear.
0.510
0.611

4 CONCLUSIONS

The results are worse than the ones for the Outlink-method
and Mixed-method, as it can be seen by comparing Tab. 2 and
3, indicating that relying on text only is not enough to predict
the birth and death dates. Future work will investigate the effect
of the text length on the results and the combination of both the
content and context based approaches.

In this paper we have proposed a graph based approach that
relies on propagating temporal information for predicting entity
lifetimes in Wikipedia. We have also introduced 4 different
methods for estimating the boundary points of person lifetimes.
Our approach is based purely on utilizing link structure without
the need for touching the article content, and can be thus useful
for automatically timestamping entities that have sparse content.
In the future, we plan to test our approaches on other entity
types and to extend the framework for finding year granularity
dates.

3.3 Detailed Analysis
Intuitively, the more links connecting an article, the higher
should be the accuracy of the lifetime estimation of the entity
described in that article. In Tab. 4 we display the average results
for the Inlink-method according to the centrality scores of
persons. The results are computed for low, medium and high
values. Due to space limitations we only show the results given
by Pearson Correlation.
Looking at Tab. 4 we observe that for each lifetime extraction
procedure the best results are obtained for the high centrality
persons. This agrees with the intuition that the better
connectivity, the more accurate is lifetime prediction.
Table 4 Pearson Correlation results for different centrality
measures in Inlink-method.
Settings
Low
Medium
High

SPFM
0.269
0.329
0.588

EPFM
0.280
0.346
0.444

PBM
0.135
0.190
0.322

PIBM
0.252
0.309
0.539

Finally, we show in Fig. 1 the score distribution for each
century. We plot the Pearson Correlation scores for Inlink-,
Outlink-, Mixed- and Text-method per each century using
PIBM strategy for date detection. We can notice that Outlinkmethod and Mixed-method outperform the Text-method
baseline in all the centuries except for the current century.
Another observation is that the improvement of Mixed-method
over Outlink-method occurs only for centuries before the 17th
century. Also, in general, the task of dating people in the more
recent centuries is harder as indicted by the decreased
performance of all the methods including the baseline.

ACKNOWLEDGMENTS
This research was supported in part by MEXT Grants-in-Aid for
Scientific Research (#17H01828, #15K12158).

REFERENCES
[1] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, Z. Ives.
DBpedia: A Nucleus for a Web of Open Data. In ISWC’07/ASWC’07,
722–735. 2007.
[2] W. Fei, and D. S. Weld. Autonomously Semantifying Wikipedia. In
CIKM2007, 2007.
[3] E. Gabrilovich and S. Markovitch. Computing Semantic Relatedness
using Wikipedia-based Explicit Semantic Analysis. In IJCAI 2007,
1606–1611, 2007.
[4] E. Gabrilovich, et al. Overcoming the Brittleness Bottleneck Using
Wikipedia: Enhancing Text Categorization with Encyclopedic
Knowledge. In AAAI 2006.
[5] J. Hoffart et al. YAGO2: Exploring and Querying World Knowledge in
Time, Space, Context, and Many Languages. In WWW 2011, 229-232,
2011.
[6] A. Jatowt, D. Kawai and K. Tanaka. Digital History Meets Wikipedia:
Analyzing Historical Persons in Wikipedia, Proceedings of the 16th
ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL 2016), ACM
Press, Newark, USA, pp. 17-26 (2016)
[7] S. Nunes, C. Ribeiro, and G. David. Using Neighbors to Date Web
Documents. In Proc. of WIDM’07 Workshop associated to CIKM2007,
129-136, 2007.
[8] L. Ostroumova Prokhorenkova, et al. Publication Date Prediction
through Reverse Engineering of the Web. In WSDM2016, 123-132.

1208

