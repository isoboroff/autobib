Re-Examining Search Result Snippet Examination Time for
Relevance Estimation
Dmitry Lagun

Eugene Agichtein

Emory University

Emory University

dlagun@emory.edu

eugene@mathcs.emory.edu

ABSTRACT
Previous studies of web search result examination have provided valuable insights in understanding and modelling searcher
behavior. Yet, recent work (e.g., [3]) has been developed
based on the assumption that the time a searcher spends
examining a particular result abstract or snippet, correlates
with result relevance. While this idea is intuitively attractive, to the best of our knowledge it has not been empirically
tested. This poster investigates this hypothesis empirically,
in a controlled setting, using eye tracking equipment to compare search result examination time with result relevance.
Interestingly, while we replicate previous findings showing
examination time to be indicative of whole-page relevance,
we find that viewing time of individual results alone is a
poor indicator of either absolute result relevance or even of
pairwise preferences. Our results should not be taken as
negating the usefulness of modeling searcher examination
behavior, but rather to emphasize that snippet examination
time is not in itself a good indicator of relevance.

Categories and Subject Descriptors
H.4 [Informational storage and retrieval]: evaluation,
search process.

Keywords
Web search snippet examination; web search behavior; search
evaluation.

1.

MOTIVATION AND OVERVIEW

As terabytes of search log data are being generated daily,
correct interpretation of patterns in these data becomes ever
more challenging. Mining of click data has become a fertile
area for many applications, including result relevance estimation, automatic query suggestion, and many others. One
interesting application of using search logs to infer result
examination behavior on Search Result Pages (SERP) was
introduced by He et al. [3]. Another successful application
of examination data is identification of relevant sub-parts of
documents, which can be used for query expansion or term
re-weighting (see Buscher et al. [1] for more details). Yet,
while result examination data is likely to provide rich contextual information for machine learning algorithms for accurate user modeling and interpretation of past clicks, to our
knowledge, the examination behavior on the SERP itself as
a predictor of relevance has not been empirically evaluated
in prior research [3].
This poster reports preliminary results on analyzing whether
Copyright is held by the author/owner(s).
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
ACM 978-1-4503-1472-5/12/08.

1141

viewing time of a search result snippet can be directly used
to estimate its relevance label. Using eye tracking equipment, we confirm that previously suggested measures of
viewing behavior (e.g., the distribution of viewing time across
all result ranks) indeed correlate with overall result quality.
However, we also find that examination time alone is unlikely to be a good indicator of relevance of individual results,
primarily due to severe position bias in the examination behavior. Our results are consistent with and complement the
findings of Guan and Cutrell [2], where users were observed
to spend more time finding relevant results placed in the
bottom of the result list, as opposed to the original result
ordering. Next, we describe the details of eye tracking user
study that we conducted, after what we present our findings
on correlating result viewing time and relevance, both for
the whole set of results and for individual search results.

2.

USER STUDY

Our study used 25 benchmark search tasks selected from
the Web Track of the TREC 2009 competition. The goal
for each task (the description) was provided to the participants. For example, the goal of the query ”toilet” was stated
as: ”Find information on buying, installing, and repairing
toilets”. For each task, the query keywords were submitted
to the Google search engine, and the Search Engine Result
Pages (SERPs), as well as all the result URLs linked from
each SERP, were cached. We did not remove non-organic
results such as ads, embedded image results, news or local
results, etc. - thereby recreating a realistic search experience for the subjects. Paid annotators rated every search
result for relevance, with at least five ratings per document,
on a three-point scale (“Bad”, “Partially Relevant” and “Perfect”). We retained the majority opinion as the final result
relevance label. For the rest of the details about user study
we refer to [5].

3.

RESULTS AND DISCUSSION

While our main goal is to investigate whether snippet examination time can be used as a proxy for result relevance,
we first confirm that our data are in correspondence with
prior work on studying user examination behavior on the
search result page.
Figure 6(a) of [5] reports the fraction of the examination
time of the snippets, broken down by the result rank. Specifically, the values were computed for each subject and query
(that is, the viewing time for each result abstract by a subject was divided by the total viewing time of the corresponding SERP) for an individual query, and then averaged across
all queries. The decaying shape of the viewing time distribution is similar to that reported in prior work (e.g., by
Joachims et al. [4]). The fact that web search users tend

Pooled (all)
Pairwise - All
Pairwise - Above
Pairwise - Adjacent

Viewing time (rel.)
0.157
0.194
0.119
0.007

Viewing time (abs.)
0.107
0.148
0.063
0.003

Table 1: Pearson correlation between individual result examination time and relevance labels.
to spend more time reading top results irrespective to the
result relevance is well known and typically referred as position bias (sometimes called presentation bias). However,
when considering viewing time for individual queries, the effect of the position bias is smaller. In fact, for some of the
queries in our dataset, the viewing time does not peak at
rank one, and actually can be used to estimate the whole
page relevance, as we discuss below.
Viewing time vs. individual result relevance: we now
investigate whether variation in snippet examination time,
as measured per result or by entire distribution, could be
attributed to result relevance. Figure 1 reports the box plot
of relative viewing time, broken down by relevance labels,
where the boxes correspond to 25th and 75th percentiles,
and the median is marked with a red line. As Figure 1
reports, there is a substantial overlap in values of viewing
time for different relevance levels. Statistical tests on differences of result viewing time between Bad and Partially
Relevant groups, and Bad and Perfect groups fail to reject
the null hypothesis, leading us to conclude that snippet examination time alone can not be used to infer individual result relevance. To verify our findings, we pool viewing time
and relevance labels from all queries, and compute Pearson correlation between them. Overall, we find that there
is only a weak correlation (0.157) between snippet viewing
time and relevance. In order to mitigate the effects of position bias, we also compared relative result preferences based
on relevance vs. based on the difference in examination time.
Specifically, we compared the differences of snippet viewing
time per result to the differences in relevance levels, in three
ways: Pairwise-All, pairwise preferences derived based on all
(usually 10) results on the SERP; Pairwise-Above, pairwise
preferences between each result and only the results ranked
above it; Pairwise-Adjacent, pairwise preferences between
adjacent results only. Surprisingly, as reported in Table
1, we find that even pairwise preferences do not exhibit a
stronger correlation with viewing time, except for PairwiseAll, where pairwise preferences have a higher correlation
with viewing time compared to using the absolute document
relevance. Additionally, we re-calculated these correlation
coefficients using absolute viewing time, instead of the relative viewing time. Finally, we repeated our computations
and considered only the results viewed at least once, without finding significantly higher correlation levels. We omit
details on these experiments due to space constraints.
Viewing time distribution vs. whole page relevance:
while viewing time on individual results is a poor predictor of relevance, we also investigated whether we can relate
viewing behavior on the SERP as a whole, with result relevance of the whole result set. To quantify overall page
quality we calculated standard information retrieval metrics, namlely: Mean Average Precision (MAP), Normalized
Discounted Cumulative Gain (NDCG) and Expected Reciprocal Rank (ERR). Intuitively, when the result ranking is
poor, the users are expected to spend more time reading

Figure 1: Viewing time on search
down by relevance label.
MAP NDCG
Views@1
0.452 0.249
Mean Rank 0.162
0.041

results broken
ERR
0.102
-0.042

Table 2: Pearson correlation between viewing time
and whole page relevance.
result abstracts at lower ranks. To validate our intuition
we compute correlation between relative viewing time at
rank one (Views@1) and mean rank of viewing time distribution (analog of distribution mean). Note that the relative
Views@1 is linearly depended on viewing time at lower ranks
(due to normalization constraint), hence when more attention is spent on lower ranks, the value of Views@1 decreases.
Table 2 reports Person correlation between Views@1 and
mean rank and whole page quality represented by one of the
three metrics above. We find that Views@1 has substantial
correlation of 0.452 with the MAP metric, while exhibiting
smaller, but non-negligible correlation with NDCG (0.249)
and to a smaller degree with ERR (0.102). Summary: to
our knowledge, we performed the first empirical examination
of the result examination hypothesis, disproving the intuition
that longer snippet examination times indicate higher relevance of individual results. Surprisingly, this was the case
both for absolute relevance levels, and for pairwise result
preferences, under commonly observed strategies of result
examination (i.e., the “Skip-Above” and “Skip-Next” strategies in reference [4]). However, we confirmed that the distribution of the viewing times on the SERP does correlate
with ranking quality of the SERP as a whole, suggesting a
more promising direction for further research. In our future
work we plan to conduct more thorough analysis on result
examination behavior and associated relevance that would
account for consistent biases in result perception such as
abstract length and judgeability.

4.

REFERENCES

[1] G. Buscher, A. Dengel, R. Biedert, and L. V. Elst. Attentive
documents: Eye tracking as implicit feedback for information
retrieval and beyond. ACM Trans. Interact. Intell. Syst.,
1(2):9:1–9:30, 2012.
[2] Z. Guan and E. Cutrell. An eye tracking study of the effect of
target rank on web search. In Proc. of CHI, pages 417–420, 2007.
[3] Y. He and K. Wang. Inferring search behaviors using partially
observable markov model with duration (pomd). In Proc. of
WSDM, pages 415–424, 2011.
[4] T. Joachims, L. Granka, B. Pan, H. Hembrooke, F. Radlinski,
and G. Gay. Evaluating the accuracy of implicit feedback from
clicks and query reformulations in web search. ACM TOIS,
25(2):7, 2007.
[5] D. Lagun and E. Agichtein. Viewser: enabling large-scale remote
user studies of web search examination and interaction. In
Proceedings of the 34th international ACM SIGIR conference
on Research and development in Information, pages 365–374.
ACM, 2011.

1142

