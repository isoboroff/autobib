Utilizing Inter-Document Similarities in Federated Search
Savva Khalaman
Oren Kurland
savvakh@tx.technion.ac.il kurland@ie.technion.ac.il
Faculty of Industrial Engineering and Management
Technion — Israel Institute of Technology
Haifa 32000, Israel

ABSTRACT
We demonstrate the merits of using inter-document similarities for federated search. Speciﬁcally, we study a resultsmerging method that utilizes information induced from clusters of similar documents created across the lists retrieved
from the collections. The method signiﬁcantly outperforms
state-of-the-art results merging approaches.
Categories and Subject Descriptors: H.3.3 [Information Search
and Retrieval]: Retrieval models
General Terms: Algorithms, Experimentation

the merging (fusion) methods used to assign initial scores to
documents exploited this overlap. The adapted method that
we study integrates retrieval scores assigned by a state-ofthe-art results-merging approach (e.g., CORI [1] and SSL
[7]) with information induced from clusters created from
similar documents across the retrieved lists. Speciﬁcally,
a document can provide relevance-status support to documents in the same list or in other lists that it is similar to.
The resultant retrieval performance is substantially better
than that of using only the initial retrieval scores assigned
by the state-of-the-art merging approach.

Keywords: inter-document similarities, federated search

1.

2. RESULTS-MERGING METHOD

INTRODUCTION

Federated search is the task of retrieving documents from
multiple (possibly non-overlapping) collections in response
to a query [1]. The task is typically composed of three
steps: attaining resource (collection) description, selecting
resources (collections), and merging the results retrieved
from the selected collections [1]. We focus on the resultsmerging step; speciﬁcally, we study the merits of using information induced from inter-document similarities.
While there is much work on utilizing inter-document similarities for the single-corpus retrieval setting, there is little
work along that venue for federated retrieval. For example,
clustering was used to transform a single-collection retrieval
setting into that of multiple collections [8]. Clusters of sampled documents were used for performing query expansion
in federated search [5]; yet, inter-document similarities were
not used for results merging. Furthermore, it was shown
that among the clusters created across the lists retrieved
from diﬀerent collections there are some that contain a high
percentage of relevant documents [3]; still, a results merging
method exploiting these clusters was not proposed
The only work, to the best of our knowledge, that uses
inter-document-similarities for (direct) results merging is based
on scoring a document by its similarity with other documents in the retrieved lists [6]. We show that the method
we study substantially outperforms this approach.
The method we present for merging results in federated
search is adapted from recent work on fusing lists that were
retrieved from the same collection [4]. In contrast to the
non-overlapping collections setting we explore here, the retrieved lists in this work [4] were (partially) overlapping and
Copyright is held by the author/owner(s).
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
ACM 978-1-4503-1472-5/12/08.

1169

Suppose that some resource (collection) selection method
was applied in response to query q [1]. We assume that document lists were retrieved from the selected (non-overlapping)
collections and merged by some previously proposed results[n]
merging algorithm [1, 7]. Let Dinit denote the list of the n
documents most highly ranked by the merging algorithm
that assigns the (initial) score Finit (d; q) to document d.
[n]
Our goal is re-ranking Dinit to improve ranking eﬀec[n]
tiveness. (Documents not in Dinit remain at their original ranks.) To that end, we study the utilization of interdocument similarities by adapting a method proposed in
work on fusing lists retrieved from the same corpus [4]. Let
[n]
Cl be the set of document clusters created from Dinit using
some clustering algorithm; c will denote a cluster. Then,
[n]
the score assigned to document d (∈ Dinit ) by the Clust
method is:
Finit (d; q)
+

[n] Finit (d ; q)
d ∈Dinit
P
X
F (c; q)
di ∈c Sim(di , d)
P
P
P
;
λ


[n]
c ∈Cl F (c ; q)
di ∈c Sim(di , d )
d ∈D
c∈Cl
def

FClust (d; q) = (1 − λ) P

init

def Q
F (c; q) =
di ∈c Finit (di ; q); as all the clusters in Cl contain
the same number of documents (see details below), there is
no cluster-size bias incurred; Sim(·, ·) is the inter-document
similarity measure used to create Cl; λ is a free parameter.
Thus, d is highly ranked if (i) its (normalized) initial score
is high; and, (ii) it is similar to documents in clusters c
that contain documents that were initially highly ranked.
In other words, similar documents provide relevance-status
support to each other via the clusters to which they belong.
Note that if λ = 0, then cluster-based information is not
used and FClust (d; q) is d’s normalized initial score.

CORI
Uni
SSL

CORI
Rel
SSL

CORI
NRel
SSL

CORI
Rep
SSL

CORI
KM
SSL

initial
CRSC
Clust
initial
CRSC
Clust
initial
CRSC
Clust
initial
CRSC
Clust
initial
CRSC
Clust
initial
CRSC
Clust
initial
CRSC
Clust
initial
CRSC
Clust

initial
CRSC
Clust
initial
CRSC
Clust

Queries: 51-100
p@5
p@10 MAP
.460
.420 .060
.524
.474i .062
.536i .498i .064i
.432
.410 .060
.492
.458 .062
.504
.490i .064i
.424
.384 .059
.428
.402 .061
.476
.442i .063ic
.412
.384 .095
.412
.404 .096
.480ic .454ic .099ic
.452
.446 .064
.476
.476 .066
.552ic .516ic .069ic
.464
.448 .065
.488
.462 .067
i
.536
.516ic .070ic
.428
.408 .060
.416
.418 .060
.492c .474ic .064ic
.444
.408 .061
.448
.434 .060
.476
.470i .064ic
Queries: 201-250
p@5
p@10 MAP
.468
.402 .135
.484
.396 .138
.496
.460ic .147i
.480
.412 .152
.512
.412 .150
.532
.478ic .165ic

Queries: 101-150
p@5
p@10 MAP
.396
.380 .054
.376
.372 .052
.480ic .454ic .059ic
.412
.396 .055
.400
.346 .052
.492c .468ic .060ic
.432
.370 .064
.456
.442 .072i
.496
.466i .073i
.440
.376 .095
.452
.436 .103i
.492
.474i .105i
.416
.394 .054
.472
.414 .057
.500i .450 .059ic
.396
.398 .055
.456
.412 .058
i
.504
.452 .060i
.428
.388 .050
.432
.424 .052
.496
.474i .056ic
.452
.408 .052
.396
.418 .052
.508c .484ic .058ic

[n]
d (=d )∈D
init

always outperforms the initial ranking that was induced by
a state-of-the-art results-merging method; often, the improvements are statistically signiﬁcantly. This ﬁnding attests to the merits of integrating the initial results-merging
score with information induced from clusters of similar documents. Further exploration reveals that λ ∈ {0, 1} often
yields optimal performance. This shows that the integration
just mentioned yields performance that is better than that
of using each of the two integrated components alone. We
also see that Clust consistently outperforms CRSC, which
does not utilize the initial results-merging scores, nor uses a
cluster-based approach.
Acknowledgments We thank the reviewers for their comments. This paper is based upon work supported in part by
the Israel Science Foundation under grant no. 557/09. Any
opinions, ﬁndings and conclusions or recommendations expressed here are the authors’ and do not necessarily reﬂect
those of the sponsors.

EVALUATION

We conducted experiments with testbeds that are commonly used in work on federated search [1, 2, 5, 8, 7]: (i)
Uni: Trec123-100col-bysource (Uniform), (ii) KM: Trec4kmeans (K-means), (iii) Rep: Trec123-2ldb-60col (Representative), (iv) Rel: Trec123-AP-WSJ-60col (Relevant), and
(v) NRel: Trec123-FR-DOE-81col (non-relevant). Titles of
the TREC topics 51-150 served for queries for all testbeds
except for KM where the description ﬁelds of TREC topics 201-250 were used. Tokenization, Porter stemming, and
stopword removal were applied using the Lemur toolkit
(www.lemurproject.org), which was used for experiments.
To acquire resource (collection) description, we adopt the
query-based sampling method from [2] which was also used
in [1, 7, 5]. Following common practice [1, 7], the 10 highest ranked collections are selected in the resource-selection
phase using CORI’s resource selection method. As in previous report [7], 1000 documents are retrieved from each selected collection using the INQUERY search engine. Then,
the retrieved lists are merged using either CORI’s merging
method [1] or the single-model SSL merging approach [7].
The initial score, Finit (d; q), assigned to d by these methods is used in our Clust method.
[n]
To cluster Dinit , we use a simple nearest-neighbors-based
[n]
clustering approach [4]. For each d ∈ Dinit we create a cluster that is composed of d and the δ − 1 (δ = 5) documents
[n]

init

Clust method incorporates two free parameters: n (∈ {10, 30,
[n]
50, 100}), the number of documents in Dinit , and λ (∈ {0, 0.1,
. . . , 1}), the interpolation parameter; CRSC only depends
on n. We set the free-parameter values for each method using leave-one-out cross validation performed over the queries
per testbad; MAP@1000 serves as the optimization criterion
in the learning phase. In addition to MAP, we also present
p@5 and p@10 performance numbers. Statistically significant diﬀerences in performance are determined using the
two-tailed paired t-test at a 95% conﬁdence level.

Results and Conclusions. We see in Table 1 that Clust

Table 1: Results. Boldface: the best result per
testbed, evaluation measure, and initial merging
method; ’i’ and ’c’: statistically signiﬁcant diﬀerences with initial and CRSC, respectively.

3.

“
”
[0]
[µ]
[µ]
exp −D(pd (·)||pd (·)) ; px is the Dirichlet-smoothed unigram language model induced from x with the smoothing parameter μ (= 1000); D is the KL divergence; the
term-counts statistics used for smoothing language models
is based on the query-based sampling mentioned above.
The initial ranking induced by the CORI and SSL resultsmerging methods serves for a baseline. Additional reference
comparison that we use is the Cross Rank Similarity Com[n]
parison (CRSC) approach [6] that re-ranks Dinit here by
P

Sim(d ,d)
scoring d with d (=d)∈D [n] P
. Our
Sim(d ,d )

4. REFERENCES
[1] J. Callan. Distributed information retrieval. In W. Croft, editor,
Advances in information retrieval, chapter 5, pages 127–150.
Kluwer Academic Publishers, 2000.
[2] J. Callan and M. Connell. Query-based sampling of text
databases. ACM Transactions on Information Systems,
19(2):97–130, 2001.
[3] F. Crestani and S. Wu. Testing the cluster hypothesis in
distributed information retrieval. Information Processing and
Management, 42(5):1137–1150, 2006.
[4] A. Khudyak Kozorovitsky and O. Kurland. Cluster-based fusion
of retrieved lists. In Proceedings of SIGIR, pages 893–902, 2011.
[5] M. Shokouhi, L. Azzopardi, and P. Thomas. Eﬀective query
expansion for federated search. In Proceedings of SIGIR, pages
427–434, 2009.
[6] X. M. Shou and M. Sanderson. Experiments on data fusion using
headline information. In Proceedings of SIGIR, pages 413–414,
2002.
[7] L. Si and J. Callan. A semisupervised learning method to merge
search engine results. ACM Transactions on Information
Systems, 21(4):457–491, October 2003.
[8] J. Xu and W. B. Croft. Cluster-based language models for
distributed retrieval. In Proceedings of SIGIR, 1999.

def

d in Dinit (d = d) that yield the highest Sim(d, d ) =

1170

