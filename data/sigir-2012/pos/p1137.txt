Rewarding Term Location Information to Enhance
Probabilistic Information Retrieval
Jiashu Zhao1 , Jimmy Xiangji Huang2 , Shicheng Wu2
1

Information Retrieval and Knowledge Management Research Lab
Department of Computer Science & Engineering, 2 School of Information Technology
York University, Toronto, Canada

jessie@cse.yorku.ca, {jhuang, scwu}@yorku.ca
ABSTRACT

1

We investigate the effect of rewarding terms according to
their locations in documents for probabilistic information
retrieval. The intuition behind our approach is that a large
amount of authors would summarize their ideas in some particular parts of documents. In this paper, we focus on the
beginning part of documents. Several shape functions are
defined to simulate the influence of term location information. We propose a Reward Term Retrieval model that combines the reward terms’ information with BM25 to enhance
probabilistic information retrieval performance.

Cosine
Linear
Parabolic

Influence

0.8
0.6
0.4
0.2
0

0

20

40
60
Position in Document

80

100

Figure 1: Influence Shape Functions

Categories and Subject Descriptors
H.3.3 [Information Storage & Retrieval]: Information
Search & Retrieval

structed from clickthrough data. Compared with the above
approaches, our approach does not require any domain-specific
knowledge.
For structured data, different fields could be given different weights. Robertson and Zaragoza [3] proposed an extension of BM25 to weight multiple fields separately, such
as title text, body text, and anchor text. However, each
part of a document is treated equally for unstructured documents. Compared to their approach, our approach rewards
terms smoothly according to their locations in documents,
and doesn’t incorporate any field information.
The remainder of this paper is organized as follows. In
Section 2, we propose several Influence Shape functions and
extend BM25 by rewarding terms according to their locations in documents. Section 3 presents the experimental
results and parameter sensitivity in ISF function. Section 4
concludes the findings in this paper and discusses possible
future directions.

Keywords
BM25-RT, Influence Shape Function

1. INTRODUCTION
Using document semantic information is usually regarded
as an effective approach for improving Information Retrieval
(IR) performance. In this paper, we propose to reward terms
according to their location information in documents.
Human beings do not treat each word equally when reading a document. People pay more attention on certain parts
of the document. This is due to the fact that many authors
would summarize their ideas in some particular locations,
e.g. the beginning and/or the end. It particularly fits articles that have abstract and introduction at the beginning
and conclude the summary in the end. In this paper, we
focus on the situation that the beginning of the documents
should be rewarded during retrieval.
Many researchers have studied using semantic information in some particular domains in IR. For example, Ontology Web Language and Information Retrieval (OWLIR)
system [4] is designed for Semantic Web documents, using
several techniques such as information extraction, annotation and inference. Wang and Li [5] investigated semantically annotated Wikipedia XML corpus to improve retrieval
performance. Zhao and Callan [6] utilized query structure
expansion to enhance structured retrieval in a question answering (QA) application. Gao and Toutanova [1] learned
semantic models from large amounts of query-title pairs con-

2.

OUR APPROACH

We firstly build the Influence Shape Functions (ISF) that
could represent the term influence at the locations closer
to the beginning part, and then integrate the influence into
BM25 by adding an extra part to term frequency. Under
our assumption, when a term occurs closer to the beginning part in the document, it would be more likely to reveal
the meaning of the document. Therefore, an ISF should
have the highest value at the first position and gradually
decreases. We build three functions with different gradients
which satisfy this property.
ISFP arabola (p, D) = (

Copyright is held by the author/owner(s).
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
ACM 978-1-4503-1472-5/12/08.

p
− 1)2 , when p < λ · |D|
λ · |D|

ISFLinear (p, D) = 1 −

1137

p
, when p < λ · |D|
λ · |D|

π·p
), when p < λ · |D|
2 · λ · |D|

where p is the position that a term occurs, |D| is the length
of the document, and λ is a parameter controlling the percentage of the document that should be considered as the
beginning part. It ranges from 0 to 1. When λ = 0, p is
always greater than λ · |D|. Therefore the value of each ISF
is always 0. It becomes a special case that ISFs do not affect
the retrieval process. The above ISFs are normalized to 0
to 1, and their shapes are shown in Figure 1. In this figure,
the document length |D| is 100 and λ is 0.8.
In [3], the field information is integrated into BM25 by using weighted term frequency on each field. Here we enhance
the within-document term frequency of a query term by the
ISFs to reward query terms occurring closer to the beginning of the document. The within-document query term
frequency is linearly combined with the accumulation of a
term’s ISF value.
tf (i)
X
tfRT (i, D) = tf (i, D) +
ISF (pj , D)
(1)

BM25

WT2G
MAP
P@10
0.2694
0.4400

TREC8
MAP
P@10
0.2410
0.4460

MAP
0.2966

Cosine
Linear
Parabola

0.2812*
0.2814*
0.2787

0.2467*
0.2460*
0.2460*

0.2968
0.2874
0.2982*

0.4580
0.4580
0.4560

Blog06
P@10
0.6033
0.6173*
0.6200*
0.6180*

Table 1: MAP and P@10 for BM25-RT with different ISFs (* indicates significant improvements compared to BM25)
0.455
Cosine
Linear
Parabola

0.282
0.28

Cosine
Linear
Parabola

0.45
0.445

MAP

0.278
0.276
0.274
0.272

0.44
0.435
0.43

0.27

0.425
0.268
0.266

0.42
0

0.2

0.4

0.6

0.8

1

0

0.2

(a) MAP

j=1

0.4

0.6

0.8

1

λ

λ

where ISF could adopt any of the above defined shape function. The only parameter that is affected is k1 , which controls the non-linear tf function.
P
RT
D∈Index tfRT (i, D)
(2)
k1 (i) = k1 · P
D∈Index tf (i, D)

(b) P@10

Figure 2: Parameter Sensitivity of λ on WT2G
for BM25-RT with Cosine ISF, Linear ISF, and
Parabola ISF.

The weighting function of BM25 with Rewarded Terms (BM25RT) is as follows.
wRT (qi , D) =

0.4500
0.4440
0.4460

P@10

ISFCosine (p, D) = cos(

(k1RT + 1) ∗ tfRT (k3 + 1) ∗ qtf
N − n + 0.5
∗log
∗
K + tfRT
k3 + qtf
n + 0.5

where N is the number of documents in the collection, n
is the number of documents containing qi , tfRT is withindocument term frequency, qtf is within-query term frequency,
dl is the length of the document, avdl is the average document length, nq is the number of query terms, k3 is tuning
constant defaulted to be 8, K equals to k1RT ∗ ((1 − b) + b ∗
dl/avdl). In this paper, we set b = 0.75 and k1 = 1.2, which
are regarded as default parameter settings in many BM25
applications [2].

BM25-RT becomes BM25 when λ = 0. When λ is increasing, it means BM25-RT takes into more proportion of document into account. We can see that MAP and P@10 of
BM25-RT increase at first when λ increments from 0. It
means the term rewarding technique boosts BM25-RT’s performance. As λ keeps incrementing, both MAP and P@10
decrease after reaching optimal values.

4.

CONCLUSIONS AND FUTURE WORK

In this paper, we propose a BM25-RT model which tries to
reward terms according to their location information during
retrieval. A term occurrence is regarded to be more important when it is closer to the beginning of the document. In
the future, we plan to discuss more possible document distributions, for instance, to reward the endings of documents.

3. EXPERIMENTAL RESULTS

5.

We evaluate the proposed approach on three data sets:
WT2G (topic 401-450), TREC8 (topic 401-450), and Blog06
(topic 851-950). The WT2G collection is a 2G size crawl of
Web documents. The TREC8 contains 528,155 newswire
articles from various sources, such as Financial Times (FT),
the Federal Register (FR) etc., which are usually considered as high-quality text data with little noise. The Blog06
collection includes 100,649 blog feeds collected over an 11
week period from December 2005 to February 2006. For all
test collections used, each term is stemmed using Porter’s
English stemmer, and standard English stopwords are removed. We use the MAP and P@10 as measurements in our
experiments.
Table 1 shows the result of BM25-RT on the above three
data sets using Cosine ISF, Linear ISF, and Parabolic ISF.
We compare BM25-RT with BM25, since BM25-RT doesn’t
incorporate any field or annotation information. Significant
improvement is observed on all the data sets.
Figure 2 shows how the parameter λ in ISF affects the
retrieval performance. We have discussed in Section 2 that

This research is supported in part by the research grant
from the Natural Sciences & Engineering Resarch Council
(NSREC) of Canada and the Early Researcher/Premier’s
Research Excellence Award.

6.

ACKNOWLEDGEMENTS

REFERENCES

[1] J. Gao, K. Toutanova, and W. Yih. Clickthrough-based latent
semantic models for web search. In SIGIR’11, pages 675–684,
New York, NY, USA, 2011. ACM.
[2] S. Robertson, S. Walker, and M. Beaulieu. Okapi at TREC-7:
automatic ad hoc, filtering, vlc and interactive track. In
TREC’99, pages 253–264, 1999.
[3] S. Robertson, H. Zaragoza, and M. Taylor. Simple BM25
extension to multiple weighted fields. In CIKM’04, pages 42–49,
New York, NY, USA, 2004. ACM.
[4] U. Shah, T. Finin, A. Joshi, R. S. Cost, and J. Matfield.
Information retrieval on the semantic web. In CIKM’02, pages
461–468, New York, NY, USA, 2002. ACM.
[5] Q. Wang, Q. Li, S. Wang, and X. Du. Exploiting semantic tags
in xml retrieval. In INEX’09, pages 133–144, Berlin, 2010.
[6] L. Zhao and J. Callan. Effective and efficient structured
retrieval. In CIKM’09, pages 1573–1576, New York, NY, USA,
2009. ACM.

1138

