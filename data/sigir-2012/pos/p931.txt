Rhetorical Relations for Information Retrieval
Christina Lioma

Birger Larsen

Wei Lu

Computer Science
University of Copenhagen
Denmark

Royal School of Library and
Information Science
Copenhagen Denmark

School of Information
Management
Wuhan University China

blar@iva.dk

reedwhu@gmail.com

c.lioma@diku.dk

ABSTRACT
Typically, every part in most coherent text has some plausible reason for its presence, some function that it performs to
the overall semantics of the text. Rhetorical relations, e.g.
contrast, cause, explanation, describe how the parts of
a text are linked to each other. Knowledge about this socalled discourse structure has been applied successfully to
several natural language processing tasks. This work studies the use of rhetorical relations for Information Retrieval
(IR): Is there a correlation between certain rhetorical relations and retrieval performance? Can knowledge about a
document’s rhetorical relations be useful to IR?
We present a language model modiﬁcation that considers
rhetorical relations when estimating the relevance of a document to a query. Empirical evaluation of diﬀerent versions
of our model on TREC settings shows that certain rhetorical
relations can beneﬁt retrieval eﬀectiveness notably (> 10%
in mean average precision over a state-of-the-art baseline).

Figure 1: Rhetorical relations example (from [11]).

language, rhetorical relations may be unstated. The goal of
discourse analysis is therefore to infer rhetorical relations,
and speciﬁcally to identify their span, constraints and function.
There is a large body of research on both descriptive
and predictive models of rhetorical structure and discourse
analysis in natural language text. For instance, annotation
projects have taken signiﬁcant steps towards developing semantic [12, 18] and discourse [5] annotated corpora. Some of
these annotation eﬀorts have already had a computational
impact, making it possible to automatically induce semantic
roles [15] and to automatically identify rhetorical relations
[14], achieving near-human levels of performance on certain
tasks [27]. In addition, applications of discourse analysis
to automatic language processing tasks such as summarisation or classiﬁcation (overviewed in section 2) indicate that
rhetorical relations can enhance the performance of welltrained natural language processing systems.
Motivated by these advances, this work brings perspectives from discourse analysis into Information Retrieval (IR)
with the aim of investigating if and how rhetorical relations
can beneﬁt retrieval eﬀectiveness. Is there a correlation between certain rhetorical relations and retrieval performance?
Can knowledge about a document’s rhetorical relations be
useful to IR? For example, consider the rhetorical relations
of the text shown in Figure 1 (borrowed from [11]). Should
some of the terms in this sentence be given extra weight by
an IR system, according to their rhetorical relations? Can
some rhetorical relations be considered more informative and
hence more useful for IR ranking than others? These questions have been posed before (see discussion in section 2),
however to our knowledge this is the ﬁrst time that a principled integration of rhetorical relations into a probabilistic
IR model improves precision by > 10%.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval
Models; H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing—linguistic processing

Keywords
Rhetorical relations, discourse structure, retrieval model,
probabilistic retrieval

1. INTRODUCTION
According to discourse analysis, every part in most coherent text tends to have some plausible reason for its presence,
some function that it performs to the overall semantics of
the text. Rhetorical relations, e.g. contrast, explanation, condition, are considered critical for text interpretation, because they signal how the parts of a text are linked
to each other to form a coherent whole [23]. Unlike grammatical relations, which are generally explicitly manifest in

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

931

informativeness; instead, the usefulness of rhetorical relations is in providing constraints for navigating through the
text’s structure. These ﬁndings are compatible with the
study of Clarke & Lapata [7] into constraining text compression on the basis of rhetorical relations. For a more indepth look into the impact of individual rhetorical relations
to summarisation see Teufel & Moens [30].
In domain-speciﬁc IR, Yu et al. [34] focus on psychiatric
document retrieval, which aims to assist users to locate documents relevant to their depressive problems. They propose
the use of high-level discourse information extracted from
queries and documents, such as negative life events, depressive symptoms and semantic relations between symptoms, to
improve the precision of retrieval results. Their discourseaware retrieval model achieves higher precision than the vector space and Okapi models.
Closer to our work, Wang et al. [31] extend an IR ranking
model by adding a re-ranking strategy based on document
discourse. Speciﬁcally, their re-ranking formula consists of
the original retrieval status value computed with the BM11
model, which is then multiplied by a function that linearly
combines inverse document frequency and term distance for
each query term within a discourse unit. They focus on one
discourse type only (advantage-disadvantage) which they
identify manually in queries, and show that their approach
improves retrieval performance for these queries. Our work
diﬀers on several points. We use an automatic (not manual) discourse parser to identify rhetorical relations in the
documents to be retrieved (not queries). We consider 15
rhetorical relations (not 1) and we study their impact to retrieval performance using a modiﬁcation of the IR language
model.
Finally, Suwandaratna & Perera [29] also present a reranking approach for Web search that uses discourse structure. They report a heuristic algorithm for reﬁning search
results based on their rhetorical relations. Their implementation and evaluation is partly based on a series of ad-hoc
choices, making it hard to compare with other approaches.
They report a positive user-based evaluation of their system
for ten test cases.

Reasoning about query - document relevance using the
language modeling formalism [9], we present a model that
conditions the probability of relevance between a query and
a document on the rhetorical relations occurring in that document. We present an application of this model to an IR
re-ranking task, where, given a list of documents initially
retrieved for a query, the goal is to improve the ranking
of the documents by reﬁning their estimation of relevance
to the query. Experimental evaluation of diﬀerent versions
of our model on TREC data and standard settings demonstrates that certain rhetorical relations can be beneﬁcial to
retrieval, with notable improvements to retrieval eﬀectiveness (> 10% in mean average precision and other standard
TREC evaluation measures over a state-of-the-art baseline).

2.

RELATED WORK

Discourse analysis and rhetorical structures have been studied in the context of several automatic text processing applications. This has been partly enabled by the availability
of discourse parsers - see [11, 14] for up-to-date overviews
of discourse parsing technology. Studies of discourse analysis in relation to IR and its broader applications are brieﬂy
overviewed below. For a more general overview of discourse
analysis approaches, see Wang et al. [33], section 2.
Sun & Chai [28] investigate the role of discourse processing and its implication on query expansion for a sequence
of questions in scenario-based context question answering
(QA). They consider a sequence of questions as a mini discourse. An empirical examination of three discourse theoretic models indicates that their discourse-based approach
can signiﬁcantly improve QA performance over a baseline of
plain reference resolution.
In a diﬀerent task, Wang et al. [33] parse Web user forum
threads to determine the discourse dependencies between
posts in order to improve information access over Web forum archives. They present three diﬀerent methods for classifying the discourse relationships between posts, which are
found to outperform an informed baseline.
Heerschop et al. [16] perform document sentiment analysis (partly) based on a document’s discourse structure. They
hypothesise that by splitting a text into important and less
important text spans, and by subsequently making use of
this information by weighting the sentiment conveyed by
distinct text spans in accordance with their importance,
they can improve the performance of a sentiment classiﬁer.
A document’s discourse structure is obtained by applying
rhetorical structure theory on a sentence level. They report a 4.5% improvement in sentiment classiﬁcation accuracy when considering discourse, in comparison to a nondiscourse based baseline. Similarly to this study, Somasundaran et al. [26] report improvements to opinion polarity
classiﬁcation when using discourse, and Morato et al. [24]
report a positive dependence between classiﬁcation performance and certain discourse variables. An overview of discourse analysis for opinion detection can be found in Zhou
et al. [36].
In the area of text compression, Louis et al. [21] study the
usefulness of rhetorical relations between sentences for summarisation. They ﬁnd that most of the signiﬁcant rhetorical
relations are associated to non-discriminative sentences, i.e.
sentences that are not important for summarisation. They
report that rhetorical relations that may be intuitively perceived as highly salient do not provide strong indicators of

3. RANKING WITH RHETORICAL
RELATIONS
There may be various ways of considering rhetorical relations in an IR setting. In this work, we view rhetorical relations as non-overlapping text spans, rather than a graph or
a tree with structure and overlapping nodes [27]. We select a
principled integration of rhetorical relation information into
the retrieval model that ranks documents with respect to
queries. The goal is to enable evidence about the rhetorical
relations in a document to have a quantiﬁable impact upon
the estimation of relevance of this document to a query, and
to study that impact.

3.1

Model Derivation

Let q be a query, d a document, D a collection of documents,
and ψg a rhetorical relation in the collection (so that

ψg p(ψg |d) = 1). In probabilistic IR, each d in D can be
ranked by its probability p(d|q) of being relevant to q. Using
Bayes’ law:
p(d|q) =

932

p(q|d)p(d) rank
= p(q|d)
p(q)

(1)

be the smoothing parameter and Ψ would be the collection of all rhetorical relations in D. A similarly Dirichlet
smoothed alternative estimation of Equation 5 would be:
|ψg | f (ψgj ,d)+μ·p(ψgj |D)
log ps (ψg |d) = j=1
. We choose to use
|d|+μ
maximum likelihood instead of Dirichlet to avoid introducing the extra Dirichlet smoothing parameter μ when investigating the eﬀect of rhetorical relations upon retrieval.
Another alternative would be to use Good-Turing smoothing, however doing so would scale down the maximum likelihood estimations in Equations 4-5 by a factor of 1 − E(1)
|ψg |

where the right-hand side of Equation 1 is derived as follows:
p(q) is dropped because it is ﬁxed for all documents, and
p(d) can be dropped on the assumption that it is uniform
in the absence of any prior knowledge about any document.
Using the language modeling approach to IR [9], p(q|d) can
be interpreted as the probability of generating the terms in
q from a model induced by d, or more simply how likely it
is that the document is about the same topic as the query.
p(q|d) can be estimated in diﬀerent ways, for instance using
Dirichlet, Jelinek-Mercer, or two-stage smoothing [35].
We introduce into Equation 1 the probability of generating the query terms from a model induced by d and by its
rhetorical relations ψg ∈ d as follows:
p(q|d) =



p(q|d, ψg )p(ψg |d)

and 1 − E(1)
respectively, where E(1)
(resp. E(1)
) is the
|d|
|ψg |
|d|
estimate of how many items in the numerator of Equation 4
(resp. Equation 5) have occurred once in the sample of the
denominator (see Gale & Sampson [13] for more on GoodTuring smoothing). In eﬀect, for Equation 4 this scaling
down would reduce the probability of the query terms that
we have seen in ψg , making room for query terms that we
have not seen. For our setting this would not be necessary,
because in practice most queries and most rhetorical relations correspond to rather short text spans. Good-Turing
smoothing might be better suited for larger samples [13].
Overall, the model presented in this section can be seen as
a ‘basic model’ for ranking documents (partly) according to
their rhetorical relations. Diﬀerent variations on this basic
model are certainly possible, however we choose to use the
simple maximum likelihood version of this model for this
exploratory investigation into the potential beneﬁts of using
rhetorical relations for IR.

(2)

ψg

We now explain the two components in Equation 2. The
ﬁrst component, p(q|d, ψg ), can be interpreted as the probability of generating the query terms from a model induced
by d and ψg . We estimate p(q|d, ψg ) as a simple mixture of
the probabilities of generating q from d and ψg :
p(q|d, ψg ) = (1 − κ) · p(q|d) + κ · p(q|ψg )

(3)

where p(q|d) is the (baseline) probability of relevance between q and d mentioned in the beginning of this section, κ
is a free parameter, and p(q|ψg ) can be interpreted as the
probability of generating q from a model induced by the
rhetorical relation ψg , or more simply, the ‘likelihood of relevance’ between the terms in the query and the terms in the
rhetorical relation.
The second component of Equation 2, p(ψg |d), is the probability of the rhetorical relation given the document. Similarly to above, this can be interpreted as the probability of
generating the terms in ψg from a model induced by d, or
more simply the likelihood of relevance between the terms
in the rhetorical relation and the terms in the document.

3.2

4. EVALUATION
4.1

Model Induction

To make Equations 2-3 operational we need to compute
p(q|ψg ) and p(ψg |d). One simple way of doing so is using
the respective maximum likelihood estimations:

4.1.1
|q|

f (qi , ψg )
log p(q|ψg ) =
|ψg |
i=1

(4)

|ψg |

 f (ψgj , d)
|d|
j=1

Dataset and Pre-processing

We experiment with the TREC datasets of the Web 2009
(queries 1-50) and Web 2010 (queries 51-100) tracks, that
contain collectively 100 queries and their relevance assessments on the Clueweb09 cat. B dataset2 (50,220,423 web
pages in English crawled between January and February
2009). We choose these datasets because they are used
widely in the community, allowing comparisons with stateof-the-art. We remove spam using the spam rankings of Cormack et al. [8] with the recommended setting of percentilescore < 70 indicating spam3 .
We consider a subset of this collection, consisting of the
top 1000 documents that have been retrieved in response to
each query by the baseline retrieval model on tuned settings

where f (qi , ψg ) is the frequency of the query term qi in ψg ,
and |ψg | is the number of terms in ψg .

log p(ψg |d) =

Experimental Setup

We evaluate our model on the task of re-ranking an initial
list of documents, which has been retrieved in response to
a query. Re-ranking is a well-known IR practice that can
enhance retrieval performance notably [19]. The baseline
of our experiments consists of the top 1000 documents retrieved for each query using a state-of-the-art retrieval model
(language model with Dirichlet smoothing1 [9]). Our approach reranks these documents using Equation 2.

(5)

where f (ψgj , d) is the frequency of the rhetorical relation
term ψgj in d, and |d| is the number of terms in d. In this
work, we use the above equations and, to compensate for
zero-frequency cases, we apply add-one smoothing.
Alternative principled estimations of Equations 4-5 are
possible (e.g. Dirichlet, Good-Turing) and could potentially improve the performance reported in this work. For
instance, one could discount the frequencies in Equations
4-5 by a respective collection model using Dirichlet smooth|q| f (qi ,ψg )+μ·p(qi |Ψ)
ing: log ps (q|ψg ) =
where μ would
i=1
|ψg |+μ

1
We also experimented with Jelinek-Mercer and two-stage
smoothing for the baseline retrieval model. Dirichlet and
two-stage gave higher scores. We chose Dirichlet over twostage because it includes one less parameter to tune.
2
http://lemurproject.org/clueweb09.php/
3
Note that removing spam from Clueweb09 cat B. is known
to give overall lower retrieval scores than keeping spam [3].

933

Table 1: Examples of the 15 rhetorical relations (in bold italics) of our dataset, identiﬁed by the SPADE
discourse parser [27]
Rhetorical relation Example sentences with rhetorical relations italicised and bold
attribution
... the islands now known as the Gilbert Islands were settled by Austronesian-speaking people ...
background
... many whites had left the country when Kenyatta divided their land among blacks ...
cause-result
... I plugged “wives” into the search box and came up with the following results ...
comparison
... so for humans, it is stronger than coloured to frustrate these unexpected numbers ...
condition
... Conditional money based upon care for the pet ...
consequence
... voltage drop with the cruise control switch could cause erratic cruise control operation ...
contrast
... Although it started out as a research project , the ARPANET quickly developed into ...
elaboration
... order accutane no prescription required ...
enablement
... The project will also oﬀer exercise programs and make eye care services accessible ...
evaluation
... such advances will be reﬂected in an ever-greater proportion of grade A recommendations ...
explanation
... the concept called as “evolutionary developmental biology” or shortly “evo-devo” ...
manner-means
... Fill current path using even-odd rule, then paint the path ...
summary
... Safety Last, Girl Shy, Hot Water, The Kid Brother, Speedy (all with lively orchestral scores) ...
temporal
... Take time out before you start writing ...
topic-comment
... Director Mark Smith expressed support for greyhound adoption ...
(described in section 4.1.2) using the Indri IR system4 for
indexing and retrieval. For this subset, we strip HTML annotation using our in-house WHU-REAPER crawling and
web parsing toolkit5 . Rhetorical relations are identiﬁed using the freely available SPADE discourse parser [27]. Table
1 shows the 15 types of rhetorical relations identiﬁed by this
process, with examples taken from the re-ranking dataset.

4.1.2

and background are the most frequent rhetorical relations,
whereas topic-comment is the most infrequent. This happens because quite often in text a topic forms the nucleus
of the discourse, which is then linked by a number of diﬀerent rhetorical relations, for instance about its background,
elaborating on an aspect, or attributing parts of it to some
entity. As a result, several types of other rhetorical relations can correspond to a single topic-comment. Note that
the distribution of rhetorical relations reported here is in
agreement with the literature, e.g. Teufel & Moens [30] also
report a 5% occurrence of contrast, albeit in the domain
of scientiﬁc articles.

Parameter Tuning

Two parameters are involved in these experiments: the
Dirichlet smoothing parameter μ of the retrieval model (used
by both the baseline and our approach) and the mixture
parameter κ of our model. Both parameters are tuned using
5-fold cross validation for each query set separately; results
reported are the average over the ﬁve test sets. μ is tuned
across {100, 500, 800, 1000, 2000, 3000, 4000, 5000, 8000,
10000} (using the range of Zhai & Laﬀerty [35]) and κ is
tuned across {0.1, 0.3, 0.5, 0.7, 0.9}.
Performance is reported and tuned separately for Mean
Average Precision (MAP), Binary Preference (BPREF), and
Normalised Discounted Cumulated Gain (NDCG). These
measures contribute diﬀerent aspects to the overall evaluation: BPREF measures the average precision of a ranked list;
it diﬀers from MAP in that it does not treat non-assessed
documents as explicitly non-relevant (whereas MAP does)
[4]. This is a useful insight, especially for a collection as
large as Clueweb09 cat. B where the chances of retrieving
non-assessed documents are higher. NDCG measures the
gain of a document based on its position in the result list.
The gain is accumulated from the top of the ranked list to
the bottom, with the gain of each document discounted at
lower ranks. This gain is relative to the ideal based on a
known recall base of relevance assessments [17]. Finally, we
test the statistical signiﬁcance of our results using the t-test
at 95% and 99% conﬁdence levels [25].

4.2

4.2.1

Findings

Figure 2 shows the distribution of the rhetorical relations in our re-ranking dataset as a percentage of the total
number of rhetorical relations. Elaboration, attribution
4
5

Retrieval-Enhancing Rhetorical Relations

Table 2 shows the performance of our model against the
baseline, for each rhetorical relation and evaluation measure.
The baseline performance is among the highest reported in
the literature for these setings; for instance Bendersky et al.
[3] report MAP=0.1605 for a tuned language model baseline
with the Web 2009 track queries on Clueweb cat. B without
spam.
We observe that diﬀerent rhetorical relations perform differently across evaluation measures and query sets. The four
rhetorical relations that improve performance over the baseline consistently for all evaluation measures and query sets
(shaded rows in Table 2) are: background, cause-result,
condition and topic-comment. Topic-comment is one of the
overall best-performing rhetorical relations, which in simple
terms means that boosting the weight of the topical part of
a document improves its estimation of relevance.
A closer look at which rhetorical relations decrease performance presents a more uneven picture as no relations
consistently underperform for all measures and query sets.
Some relations, such as explanation and enablement for
Web 2009, and summary and evaluation for Web 2010, are
among the lowest performing, but are not under the baseline
across all measures and both query sets. This implies that
separating rhetorical relations into those that generally can
enhance retrieval performance and those that cannot may
not be straight-forward. Even though exploring the family likeness between useful relations and ones that give no
mileage is an interesting discussion, in the rest of the pa-

http://www.lemurproject.org/
Freely available by emailing the third author.

934

Table 2: Retrieval performance with rhetorical relations and without (baseline). * (**) marks stat. significance at 95% (99%) using the t-test. Bold means > baseline. % shows the diﬀerence from the baseline.
Shaded rows indicate consistent improvements over the baseline at all times.
rhetorical relation
none (baseline)
attribution
background
cause-result
comparison
condition
consequence
contrast
elaboration
enablement
evaluation
explanation
manner-means
summary
temporal
topic-comment

MAP
0.1625
0.1654* +1.8%
0.1646
+1.3%
0.1626
+0.1%
0.1610
-0.9%
0.1632
+0.5%
0.1602
-1.4%
0.1549*
-4.6%
0.1556*
-4.2%
0.1601
-1.4%
0.1632
+0.5%
0.1546
-4.9%
0.1623
-0.1%
0.1626
+0.1%
0.1615
-0.6%
0.1673
+3.0%

Web 2009 (queries 1-50)
BPREF
NDCG
0.3230
0.3893
0.3275** +1.4% 0.3927** +0.9%
0.3291** +1.9% 0.3910
+0.4%
0.3255** +0.8% 0.3900
+0.2%
0.3251*
+0.6% 0.3877
-0.4%
0.3258** +0.9% 0.3903
+0.3%
0.3250
+0.6% 0.3874
-0.5%
0.3269** +1.2% 0.3897
+0.1%
0.3292** +1.9% 0.3866
-0.7%
0.3240
+0.3% 0.3869*
-0.6%
0.3242
+0.4% 0.3886
-0.2%
0.3259*
+0.9% 0.3813
-2.1%
0.3253*
+0.7% 0.3884
-0.2%
0.3241
+0.3% 0.3879
-0.4%
0.3262** +1.0% 0.3887
-0.2%
0.3375
+4.5% 0.3976*
+2.1%

Web 2009 (queries 1-50)
MAP BPREF NDCG
0.1
0.5
0.1
0.2
0.6
0.2
0.3
0.7
0.3
0.4
0.7
0.4
0.3
0.7
0.3
0.5
0.7
0.5
0.3
0.7
0.3
0.1
0.5
0.1
0.1
0.9
0.1
0.5
0.7
0.5
0.5
0.7
0.5
0.5
0.7
0.5
0.5
0.7
0.5
0.1
0.7
0.1
0.5
0.5
0.5

Web 2010 (queries 51-100)
MAP BPREF NDCG
0.3
0.5
0.3
0.3
0.7
0.3
0.5
0.7
0.5
0.3
0.5
0.3
0.3
0.5
0.3
0.5
0.7
0.5
0.3
0.5
0.3
0.3
0.5
0.3
0.3
0.5
0.3
0.5
0.7
0.5
0.5
0.7
0.5
0.5
0.7
0.5
0.3
0.7
0.3
0.3
0.5
0.3
0.5
0.7
0.5

topic-comment
evaluation
consequence
summary
enablement
explanation
comparison
manner-means
cause-result
temporal
contrast
condition

4.2.2 Quantifying the Contribution of Rhetorical
Relations to the Ranking

background

Exactly how much impact each rhetorical relation has on
the ranking can be seen in Table 3. The table lists the
κ values for the best performing tuned runs from Table 2,
where high κ values mean that the rhetorical relations are
given more weight in the ranking (see Equation 3). We see
that none of the values are above 0.5 for MAP and NDCG,
indicating that too much emphasis on the rhetorical rela-

attribution
elaboration

0

5

10

15

0.0986
0.0924
0.1086*
0.1015
0.1017
0.0999
0.0945
0.1103*
0.0951
0.1010
0.0814**
0.1034
0.0986
0.0862
0.0921
0.1090*

per we focus on those rhetorical relations that consistently
improve retrieval performance (for these datasets).
Improvements over the baseline are generally higher for
Web 2010 than Web 2009, possibly because the former baseline is weaker, with potentially more room for improvement.
An interesting trend is that more rhetorical relations improve performance according to BPREF than according to
MAP and NDCG. As BPREF is the only of these evaluation
measures that does not consider non-assessed documents as
non-relevant, this indicates the presence of non-assessed documents in the ranking.
The scores shown in Table 2 are averaged over tens of
queries, meaning that they can be aﬀected by outliers. Figure 3 presents a detailed per-query overview of the performance of each query in relation to the baseline for each of
the 15 rhetorical relations6 . The plotted points represent
the diﬀerence in MAP between our approach and the baseline. Positive points indicate that our approach outperforms
the baseline. The points are sorted.
We observe that although the overall performance of the
Web 2010 query set is lower than that of the Web 2009 query
set, the improvements over the baseline of the 2010 set are
consistently larger. Only in one case, topic-comment, do the
plotted points clearly cross. Overall both query sets show
similar plots with outliers at both ends of the scale. However, the 2009 query set tends to have a somewhat larger
proportion of negative outliers, which goes some way towards explaining the lower improvements over the baseline
observed for Web 2009. The Web 2010 set shows improvements over the baseline for most of the rhetorical relations
and for the majority of the queries.

Table 3: Eﬀect of the rhetorical relation to the retrieval model as indicated by parameter κ (see Equation 3), for the tuned runs of Table 2. Shaded
rows indicate rhetorical relations that consistently
improve performance over the baseline at all times.
rhetorical
relation
attribution
background
cause-result
comparison
condition
consequence
contrast
elaboration
enablement
evaluation
explanation
manner-means
summary
temporal
topic-comment

Web 2010 (queries 51-100)
BPREF
NDCG
0.2240
0.2920
-6.2% 0.2549** +13.8% 0.3008** +3.0%
+10.2% 0.2623** +17.1% 0.3070** +5.1%
+2.9% 0.2491*
+11.2% 0.3079
+5.4%
+3.1% 0.2282
+1.9% 0.3040** +4.1%
+1.3% 0.2470** +10.3% 0.2936
+0.5%
-4.1% 0.2377*
+6.1% 0.2840**
-2.7%
+11.8% 0.2531** +13.0% 0.3069** +5.1%
-3.5% 0.2598** +16.0% 0.3005** +2.9%
+2.4% 0.2316*
+3.4% 0.2992*
+2.5%
-17.4% 0.2313*
+3.3% 0.2902
-0.6%
+4.9% 0.2645** +18.1% 0.3069** +5.1%
- 0.2324*
+3.7% 0.2897
-0.8%
-12.6% 0.2220*
-0.9% 0.2928
+0.3%
-6.6% 0.2546** +13.7% 0.3052
+4.5%
+10.5% 0.2476*
+10.5% 0.3009
+3.1%

MAP

20

25

% of all rhetorical relations

Figure 2: % distribution of rhetorical relations in
our dataset.

6
Similar trends are observed in the corresponding ﬁgures for
BPREF and NDCG, which are not included here for brevity.

935

background

attribution

cause-result

0.08
0.2
0.1

0

0

0

−0.02
comparison

0.05

0.1

0.1

consequence

condition

0
0

0

−0.1

−0.1
contrast

−0.1
elaboration

enablement

0.2
0.1

0

0

0
explanation

evaluation
0.1

0.05

0

−0.2

−0.1
summary

manner-means
0.05

0

0

0

0.05

−0.05

−0.1

0.1

0.15

−0.1
topic-comment

temporal
0.05

0.05

0

0

−0.05
−0.1

−0.05

Web 2009
Web 2010

Figure 3: Sorted per-query diﬀerence in MAP between the baseline and our model (y-axis), for each rhetorical
relation. The horizontal line marks the baseline. + and o mark the 2009 and 2010 query sets.

936

assume that, conditional on λj , the retrieval scores yj have
independent Poisson distributions with means λj xj . Let us
further assume that the λj are independent realisations of a
gamma variable with parameters α and β, and that β itself
has a prior gamma distribution with parameters ν and φ.
Thus
n

(xj λj )yj −xj λj
f (y|λ) =
e
yj !
j=1

tions may not be beneﬁcial to performance. Consistent with
Table 2, BPREF follows a diﬀerent trend than MAP and
NDCG, which could be due to the fact that it is a diﬀerent type of evaluation measure as discussed above in section
4.1.2. With BPREF, non-assessed documents are not explicitly penalised in the evaluation (as in MAP and NDCG) resulting in overall higher κ values for best performing runs,
typically of around 0.5-0.7.
Further we observe that the rhetorical relations that consistently improve performance over the baseline, as indicated
in Table 2, diﬀer in κ values for their best performing runs.
For example, κ = 0.2 - 0.3 for background and κ = 0.5 for
topic-comment. This implies that, to use rhetorical relations successfully for IR, it is not suﬃcient to know which
rhetorical relations should be considered in the ranking and
which not; also knowledge about how much emphasis to put
on each rhetorical relation is needed for optimal IR performance.
Finally, note that the frequency of rhetorical relations
does not aﬀect their impact to retrieval. For instance, the
three best performing rhetorical relations, topic-comment,
background and cause-result constitute respectively approximately ∼1%, 11% and 5% of all rhetorical relations, as
shown in Figure 2.

5.

π(λ|β) =

π(β) =

f (y|λ)f (λ|β)π(β) = c

y +α−1 −λj (xj +β)

{λj j

e

}·β nα+ν−1 e−φβ

(7)
where c is a constant of proportionality.
The conditional density of β can be computed by various numerical approximations, one of which is the Laplace
method [2], which we use here. To ﬁnd the conditional density of β we integrate over the λj to obtain

Rhetorical Relation Selection

n


{(xj +β)−(yj +α) Γ(yj +α)}·β nα+ν−1 e−φβ (8)

j=1

The ﬁndings in section 4.2 show that some rhetorical relations can be more beneﬁcial to retrieval performance than
others. An ideal solution would not consider the lexical
statistics of all rhetorical relations in a document, but rather
it would select to include in the ranking only those rhetorical
relations that have a higher likelihood of enhancing retrieval
performance. This can be formulated as ﬁnding the optimal
rhetorical relation ψˆg that maximises the expected retrieval
scores according to an evaluation measure (e.g. MAP) for a
query-document pair:
ψg ∈Ψ

n


j=1

OPTIMISED RANKING WITH
RHETORICAL RELATIONS

ψˆg = arg max E[y|q, d]

φν β ν−1 −φβ
e
Γ(ν)

so that the joint probability density of the retrieval scores
y, the average performance ratios λ, and β is

f (y, β) = c

5.1

n

β α λα−1
j
e−βλj
Γ(α)
j=1

from which the marginal density of y is obtained by further
integration to give
 ∞
n

Γ(yj + α) ·
e−h(β) dβ
(9)
f (y) = c
j=1

0


where h(β) = φβ − (nα + ν − 1)logβ + (yj + α)log(xj + β).
Let I denote the integral in this expression. In this work,
we take an uninformative prior for β, with ν = 0.1 and
φ = 1 and use α = 1.87 . We then apply Laplace’s method
to I, resulting in the approximate posterior density for β,
π̃(β|y) = I˜−1 e−h(β) .
To calculate approximate posterior densities for λj we
integrate Equation 7 over λi , i = j and then we apply
Laplace’s method to the numerator and denominator integrals of
y +α−1 −λj xj  ∞ −hj (β)
λj j
e
e
dλ
 ∞0
π(λj |y) =
−h(β)
Γ(yj + α) 0 e
dβ

(6)

where E denotes the expectation and y the retrieval score
(rest of notation as deﬁned in section 3).
Bayesian decision theory allows to reason about this type
of expectation, for instance see [32]. In this work, we treat
this as a problem of Bayesian posterior inference, where the
goal is to estimate the retrieval performance associated with
a rhetorical relation, given the observed retrieval scores it
fetches on a number of queries. Then, we can consider
the rhetorical relation associated with the highest retrieval
performance as optimal. For this estimation, we split our
dataset into diﬀerent parts so that we use the observations
from one to make inferences about the other (see section 5.2
for details).
Let n = 15 be the rhetorical relations shown in Table 2,
and xj be the number of queries for which retrieval with
the j th rhetorical relation gets a retrieval score yj . For now
we assume that all rhetorical relations may be expected to
have similar retrieval performance, with the j th rhetorical
relation having an average performance ratio per query λj
y
(estimated as xjj ). Various densities can be used to ﬁt similar data [22], one of which is the Poisson distribution. Let us

where
hj (β) = (φ + λj )β − (nα + ν − 1)logβ +



(yi + α)log(xi + β)

i=j

The resulting denominator is again I˜1 , while the numerator
must be recalculated at each of a range of values for λj .
The output is the (posterior) expected retrieval performance
associated with each rhetorical relation.

5.2
7

Experiments

These values are not tuned; they are the default values
of this approach as illustrated in [10], chapter 11.3, pages
603-604.

937

Table 4: Retrieval performance with optimal rhetorical relations (inferred, observed) and without rhetorical
relations (baseline). (1)-(5) refers to the ﬁve randomised samplings used to infer the optimal rhetorical
relations. Bold marks better than baseline.
rhetorical relation
none (baseline)
optimalinf erred
optimalinf erred
optimalinf erred
optimalinf erred
optimalinf erred
optimalobserved

5.2.1

(1)
(2)
(3)
(4)
(5)

Web 2009 (queries 1-50)
MAP
BPREF
NDCG
0.1625
0.3230
0.3894
0.1879 +15.6% 0.3503
+8.5% 0.4224
+8.5%
0.1948 +19.9% 0.3585 +11.0% 0.4202
+7.9%
0.1984 +22.1% 0.3532
+9.3% 0.4169
+7.1%
0.1952 +20.1% 0.3479
+7.7% 0.4282 +10.0%
0.1950 +20.0% 0.3528
+9.2% 0.4287 +10.1%
0.2157 +32.7% 0.3660 +13.3% 0.4412 +13.3%

Setup

This has no impact on the model presented in section 3, but
it can bias the optimised inference of the model presented
in section 5. The lower the occurrence of a rhetorical relation in the dataset, the fewer the observations of retrieval
performance associated with it, and hence the weaker the
predictions we can infer about whether it is optimal or not.
A fairer setting would be to have the same number of ‘query
- retrieval performance’ observations for all rhetorical relations - however that would imply ﬁddling with the document
distribution of our dataset signiﬁcantly, potentially harming
its quality as a test collection.

The observations required to make the above inference
are triples of rhetorical relation - query number - retrieval
score. To avoid overﬁtting, we pool randomly 50% of the
observations from the 2009 Web query scores and 50% of
the observations from the 2010 Web query scores. We use
this pool to infer the expected retrieval performance of each
rhetorical relation. We repeat this randomised pooling ﬁve
times, each time randomly pertrubing the data, producing
ﬁve diﬀerent sets of observations. We then use each set to
infer the expected best performing rhetorical relation per
query, in accordance to Equation 6. Following this, we use
the model introduced in section 3, Equation 2, to rank documents with respect to queries only for optimal (as inferred)
rhetorical relations. We evaluate the above method using
the same experimental settings described in section 4.1.

6.2 Limitations
A general limitation of discourse analysis is that not all
types of text are susceptible to it. For instance, legal text,
contracts, or item lists often lack rhetorical structure. In this
work, we made no eﬀort to identify and exempt such types
of text from the discourse parsing. We reasoned that, as the
SPADE parser includes a ﬁrst-step grammatical parsing, the
initial grammatical parsing of these types of text would ﬂag
out ill-formed parts (e.g. missing a verb, or consisting of extremely long sentences), which would then be skipped by the
discourse analysis. This was indeed the case, however at a
certain eﬃciency cost. Overall processing speed for SPADE
was approximately 19 seconds per document (including the
initial grammatical parsing), on a machine of 9 GB RAM,
8 core processor at 2.27GHz. One way of improving this
performance would be to update the ﬁrst-step grammatical
parsing. Currently this depends on the well-known Charniak
parser [6], which is one of the best performing grammatical
parsers, however no longer supported. Other state-of-theart faster grammatical parsers, e.g. the Stanford parser8 ,
could be adapted and plugged into SPADE instead.
The choice of applying our model for re-ranking as opposed to ranking all documents was closely related to the
eﬃciency concerns discussed above. Our model is not speciﬁc to re-ranking only, however, using SPADE on approximately 50 million documents was too expensive at this point.
Improving the discourse parser’s eﬃciency is something we
are currently working on, with the aim to apply our model
for full ranking and see if the conclusions drawn from this
work hold.
Finally, the accuracy of the discourse parser was not considered in this work, apart from indications in the literature that SPADE is a generally well-performing parser [27].
Given that the default version of the parser we used is trained
on news articles, one may reason that its accuracy could
improve if we train it on the retrieval collection, or on doc-

5.2.2 Findings
Table 4 shows the runs corresponding to the ﬁve diﬀerent inferences of the best rhetorical relation that use our
model (optimalinf erred (1)-(5) respectively). We also report
the optimal retrieval performance actually observed in the
dataset when using the best rhetorical relation per query
(optimalobserved ). Optimal here means with respect to the
choice of rhetorical relation, not with respect to the Dirichlet
μ parameter of the baseline retrieval model.
Table 4 shows that our optimised ranking model for rhetorical relations is better than the baseline for any of the ﬁve
random inferences on all three evaluation measures. The
probability of getting such a positive result by chance is
1
< 0.05, and thus the improvements are statistically sig25
niﬁcant. The improvements over the baseline are considerable, a very promising ﬁnding given the relatively low number of observations used for optimising the choice of rhetorical relations. Experiments involving larger query sets can
be reasonably expected to perform on a par with state-ofthe-art performance.
More generally, the improvements in Table 4 signal that
rhetorical relations (derived automatically as shown in this
work) could potentially be useful features for ‘linguisticallyuninformed’ learning-to-rank approaches.

6.

Web 2010 (queries 51-100)
MAP
BPREF
NDCG
0.0967
0.2198
0.2890
0.1355 +40.1% 0.2859 +30.1% 0.3347 +15.8%
0.1285 +32.9% 0.2841 +29.3% 0.3394 +17.4%
0.1358 +40.0% 0.2906 +32.2% 0.3388 +17.2%
0.1360 +40.6% 0.2874 +30.8% 0.3336 +15.4%
0.1340 +38.6% 0.2865 +30.3% 0.3322 +14.9%
0.1474 +52.4% 0.2978 +35.5% 0.3569 +23.5%

DISCUSSION

6.1 Rhetorical Relation Distribution
The distribution of the 15 rhetorical relations we identiﬁed in our dataset is not the same for all rhetorical relations
(see Figure 2). Some types, e.g. topic-comment, tend to be
very sparse, whereas relations such as elaboration prevail.

8

938

http://nlp.stanford.edu/software/lex-parser.shtml

retrieval performance notably and consistently for diﬀerent
evaluation measures and query sets: background, causeresult and topic-comment. In retrospect, this is perhaps
not surprising, since these are among the most salient discourse relations on an intuitive basis: the main topic or
theme of a text, its background, causes and results [21]. Future extensions and research directions of this work include
considering more than one rhetorical relation per document,
applying our model for ranking all documents (as opposed
to re-ranking only) and experimenting with alternative estimations of its components.

uments of the same domain. Note that, parsing accuracy
aside, rhetorical relations assignment is not an entirely unambiguous process, even to humans [23]. For the purposes of
this work, this type of ﬁne-grained ambiguity may however
not be important to retrieval performance.

6.3 Future Extensions
Future extensions include primarily making SPADE scalable on large collections of documents as discussed above,
as well as using more than one rhetorical relation per document. For instance, the posterior probabilities estimated in
section 5.1 could be used to weight the text in each rhetorical relation. If those posteriors are too ﬂat, an exponent
could make them peakier. As the exponent goes to inﬁnity, the maximum relation model presented in section 5.2
would be recovered. In addition, we intend to reﬁne the
discourse analysis by considering the nucleus (i.e. central)
versus satellite (i.e. peripheral) rhetorical relations for IR,
as well as to improve the eﬀectiveness of the discourse parser
by training it on data of the same domain. As discussed in
section 3.2, we will also investigate alternative estimations
of Equations 2-3.
An interesting future research direction is the potential
relation between rhetorical relations and user context: for
instance, in a search session including several query reformulations, is there a correlation between the progression of
the information need of the user and the rhetorical relations that the retrieval system should boost in a document
(e.g. elaboration), as indicated by Sun & Chai [28]? Another interesting future extension of this work is in relation
to evaluation measures of graded relevance measures on an
inter-document level, as investigated in XML retrieval [20]
for instance. If parts of a document can be regarded as more
or less relevant, this may be reﬂected to their discourse structure. This might be especially useful for multi-threaded documents, such as multiple-user reviews and opinions, where
the discourse relations tend to shift markedly. Finally, the
current operationalisation of our model is simplistic in the
sense that the term ‘rhetorical relation’ is coerced into meaning ‘non-overlapping text fragment’ and the actual relation
between bits of text is discarded in the process. In future
work we could apply ﬁelded XML retrieval models in order
to investigate nested structuring among rhetorical relations.

8. ACKNOWLEDGMENTS
We thank Kasper Hornbæk, Jakob Grue Simonsen, Raf
Guns, Qikai Cheng and the anonymous reviewers for helping improve this paper. Work partially funded by the Danish International Development Agency DANIDA (grant no.
10-087721) and the National Natural Science Foundation of
China (grant no. 71173164).

9.

REFERENCES

[1] Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, EMNLP
2011, 27-31 July 2011, John McIntyre Conference
Centre, Edinburgh, UK, A meeting of SIGDAT, a
Special Interest Group of the ACL. ACL, 2011.
[2] A. Azevedo-Filho and R. D. Shachter. Laplace’s
method approximations for probabilistic inference in
belief networks with continuous variables. In R. L.
de Mántaras and D. Poole, editors, UAI, pages 28–36.
Morgan Kaufmann, 1994.
[3] M. Bendersky, W. B. Croft, and Y. Diao.
Quality-biased ranking of web documents. In I. King,
W. Nejdl, and H. Li, editors, WSDM, pages 95–104.
ACM, 2011.
[4] C. Buckley and E. M. Voorhees. Retrieval evaluation
with incomplete information. In M. Sanderson,
K. Järvelin, J. Allan, and P. Bruza, editors, SIGIR,
pages 25–32. ACM, 2004.
[5] L. Carlson, D. Marcu, and M. E. Okurowski. Building
a discourse-tagged corpus in the framework of
rhetorical structure theory. In Current Directions in
Discourse and Dialogue, pages 85–112. Kluwer
Academic Publishers, 2003.
[6] E. Charniak. A maximum-entropy-inspired parser. In
Proceedings of the first conference on North American
chapter of the Association for Computational
Linguistics, pages 132–139, San Francisco, CA, USA,
2000. Morgan Kaufmann Publishers Inc.
[7] J. Clarke and M. Lapata. Discourse constraints for
document compression. Computational Linguistics,
36(3):411–441, 2010.
[8] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke.
Eﬃcient and eﬀective spam ﬁltering and re-ranking for
large web datasets. CoRR, abs/1004.5168, 2010.
[9] W. B. Croft and J. Laﬀerty. Language Modeling for
Information Retrieval. Kluwer Academic Publishers,
Norwell, MA, USA, 2003.
[10] A. C. Davison. Statistical Models. Cambridge
University Press, New York, 2009.
[11] D. A. duVerle and H. Prendinger. A novel discourse
parser based on support vector machine classiﬁcation.

7. CONCLUSIONS
Rhetorical relations, e.g. contrast, explanation, condition, indicate the diﬀerent ways in which the parts of a
text are linked to each other to form a coherent whole. This
work studied two questions: Is there a correlation between
certain rhetorical relations and retrieval performance? Can
knowledge about a document’s rhetorical relations be useful to IR? To address these, we presented a retrieval model
that conditions the probability of relevance between a query
and a document on the rhetorical relations occurring in that
document. We applied that model to an IR re-ranking scenario for Web search. Experimental evaluation of diﬀerent
versions of our model on TREC data and standard settings
demonstrated that certain rhetorical relations can be beneﬁcial to retrieval, with >10% improvements to retrieval precision. Furthermore, we showed that these improvements
over the baseline can improve signiﬁcantly, when the optimal rhetorical relation per document is selected for retrieval.
Overall, three rhetorical relations were found to beneﬁt

939

[12]

[13]

[14]

[15]
[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24] J. Morato, J. Llorens, G. Genova, and J. A. Moreiro.
Experiments in discourse analysis impact on
information classiﬁcation and retrieval algorithms. Inf.
Process. Manage., 39:825–851, November 2003.
[25] M. D. Smucker, J. Allan, and B. Carterette.
Agreement among statistical signiﬁcance tests for
information retrieval evaluation at varying sample
sizes. In J. Allan, J. A. Aslam, M. Sanderson, C. Zhai,
and J. Zobel, editors, SIGIR, pages 630–631. ACM,
2009.
[26] S. Somasundaran, G. Namata, J. Wiebe, and
L. Getoor. Supervised and unsupervised methods in
employing discourse relations for improving opinion
polarity classiﬁcation. In EMNLP, pages 170–179.
ACL, 2009.
[27] R. Soricut and D. Marcu. Sentence level discourse
parsing using syntactic and lexical information. In
HLT-NAACL, 2003.
[28] M. Sun and J. Y. Chai. Discourse processing for
context question answering based on linguistic
knowledge. Know.-Based Syst., 20:511–526, August
2007.
[29] N. Suwandaratna and U. Perera. Discourse marker
based topic identiﬁcation and search results reﬁning.
In Information and Automation for Sustainability
(ICIAFs), 2010 5th International Conference on,
pages 119–125, 2010.
[30] S. Teufel and M. Moens. Summarizing scientiﬁc
articles: Experiments with relevance and rhetorical
status. Computational Linguistics, 28(4):409–445,
2002.
[31] D. Y. Wang, R. W. P. Luk, K.-F. Wong, and K. L.
Kwok. An information retrieval approach based on
discourse type. In C. Kop, G. Fliedl, H. C. Mayr, and
E. Métais, editors, NLDB, volume 3999 of Lecture
Notes in Computer Science, pages 197–202. Springer,
2006.
[32] J. Wang and J. Zhu. On statistical analysis and
optimization of information retrieval eﬀectiveness
metrics. In F. Crestani, S. Marchand-Maillet, H.-H.
Chen, E. N. Efthimiadis, and J. Savoy, editors, SIGIR,
pages 226–233. ACM, 2010.
[33] L. Wang, M. Lui, S. N. Kim, J. Nivre, and
T. Baldwin. Predicting thread discourse structure over
technical web forums. In EMNLP [1], pages 13–25.
[34] L.-C. Yu, C.-H. Wu, and F.-L. Jang. Psychiatric
document retrieval using a discourse-aware model.
Artif. Intell., 173:817–829, May 2009.
[35] C. Zhai and J. D. Laﬀerty. Two-stage language models
for information retrieval. In SIGIR, pages 49–56.
ACM, 2002.
[36] L. Zhou, B. Li, W. Gao, Z. Wei, and K.-F. Wong.
Unsupervised discovery of discourse relations for
eliminating intra-sentence polarity ambiguities. In
EMNLP [1], pages 162–171.

In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP: Volume 2 - Volume 2, ACL ’09, pages
665–673, Stroudsburg, PA, USA, 2009. Association for
Computational Linguistics.
C. J. Fillmore, C. F. Baker, and S. Hiroaki. The
framenet database and software tools. In Proceedings
of the 3rd International Conference on Language
Resources and Evaluation (LREC), pages 1157–1160,
2002.
W. A. Gale and G. Sampson. Good-turing frequency
estimation without tears. Journal of Quantitative
Linguistics, 2(3):217–237, 1995.
S. Ghosh, R. Johansson, G. Riccardi, and S. Tonelli.
Shallow discourse parsing with conditional random
ﬁelds. In Proceedings of the 5th International Joint
Conference on Natural Language Processing
(IJCNLP), pages 1071–1079, Chiang Mai, Thailand,
2011.
D. Gildea and D. Jurafsky. Automatic labeling of
semantic roles. In ACL. ACL, 2000.
B. Heerschop, F. Goossen, A. Hogenboom,
F. Frasincar, U. Kaymak, and F. de Jong. Polarity
analysis of texts using discourse structure. In
Proceedings of the 20th ACM international conference
on Information and knowledge management, CIKM
’11, pages 1061–1070, New York, NY, USA, 2011.
ACM.
K. Järvelin and J. Kekäläinen. Cumulated gain-based
evaluation of ir techniques. ACM Trans. Inf. Syst.,
20(4):422–446, 2002.
P. Kingsbury and M. Palmer. From treebank to
propbank. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC), pages x–x, 2002.
E. Krikon and O. Kurland. A study of the integration
of passage-, document-, and cluster-based information
for re-ranking search results. Inf. Retr., 14(6):593–616,
2011.
M. Lalmas. XML Retrieval. Synthesis Lectures on
Information Concepts, Retrieval, and Services.
Morgan & Claypool Publishers, 2009.
A. Louis, A. K. Joshi, and A. Nenkova. Discourse
indicators for content selection in summarization. In
R. Fernández, Y. Katagiri, K. Komatani, O. Lemon,
and M. Nakano, editors, SIGDIAL Conference, pages
147–156. The Association for Computer Linguistics,
2010.
R. Manmatha, T. M. Rath, and F. Feng. Modeling
score distributions for combining the outputs of search
engines. In W. B. Croft, D. J. Harper, D. H. Kraft,
and J. Zobel, editors, SIGIR, pages 267–275. ACM,
2001.
W. C. Mann and S. A. Thompson. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8:243–281, 1988.

940

