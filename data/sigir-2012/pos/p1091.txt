Impact of Assessor Disagreement
on Ranking Performance
Pavel Metrikov

Virgil Pavlu

Javed A. Aslam∗

College of Computer and Information Science
Northeastern University, Boston, MA, USA

{metpavel, vip, jaa}@ccs.neu.edu
ABSTRACT

expected performance of a ranked list optimized for training assessor A but evaluated with respect to testing assessor
B?” We approach this question in two ways, via simulation
and closed form approximation. In the former case, we use
the confusion matrix C to probabilistically generate training
labels A from testing labels B, optimally rank documents according to A, and evaluate with respect to B. In the latter
case, we analytically derive a closed-form approximation to
this limiting performance, as measured by nDCG.
Given a confusion matrix C modeling inter-assessor disagreement, one can apply our results to any learning-to-rank
dataset. The limiting nDCG values obtained correspond to
reasonable upper bounds on the nDCG performance of any
learning-to-rank algorithm, even one given unlimited training data and perfect features. Considering the performance
of existing algorithms on these datasets, and comparing with
the upper bounds we derive, one can argue that learning-torank is approaching reasonable limits on achievable performance.

We consider the impact of inter-assessor disagreement on the
maximum performance that a ranker can hope to achieve.
We demonstrate that even if a ranker were to achieve perfect
performance with respect to a given assessor, when evaluated with respect to a different assessor, the measured performance of the ranker decreases significantly. This decrease
in performance may largely account for observed limits on
the performance of learning-to-rank algorithms.
Categories and Subject Descriptors: H. Information
Systems; H.3 Information Storage and Retrieval; H.3.3 Information Search and Retrieval:Retrieval models
General Terms: Experimentation, Measurement, Theory
Keywords: Inter-assessor Disagreement, Learning-to-Rank,
Evaluation

1.

INTRODUCTION

In both Machine Learning and Information Retrieval, it is
well known that limitations in the performance of ranking algorithms can result from several sources, such as insufficient
training data, inherent limitations of the learning/ranking
algorithm, poor instance features, and label errors. In this
paper we focus on performance limitations due solely to label “errors” which arise due to inter-assessor disagreement.
Consider a training assessor A that provides labels for
training data and a testing assessor B that provides labels
for testing data. Even if a ranker can produce a perfect list
as judged by A, its performance will be suboptimal with
respect to B, given inevitable inter-assessor disagreement.
In effect, no ranking algorithm can simultaneously satisfy
two or more disagreeing assessors (or users). Thus, there
are inherent limitations in the performance of ranking algorithms, independent of the quality of the learning/ranking
algorithm, the availability of sufficient training data, the
quality of extracted instance features, and so on.
We model inter-assessor disagreement with a confusion
matrix C, where cij corresponds to the (conditional) probability that a document labeled i by testing assessor B will be
labeled j by training assessor A, for some given set of label
grades such as {0, 1, 2, 3, 4}. Given such a model of interassessor disagreement, we ask the question, “What is the

2.

ASSESSOR DISAGREEMENT MODEL

Much research in the IR community has focused on addressing the problem of system evaluation in the context of
missing, incomplete, or incorrect document judgments [1].
Soboroff and Carterette [4] provide an in-depth analysis of
the effect of assessor disagreement on the Million Query
Track evaluation techniques. Both assessors and users often disagree on the degree of document relevance to a given
query, and we model such disagreement with a confusion
matrix C as described above. On data sets with multiple
assessments per query-document pair, such as the TREC
Enterprise Track [2], these confusion matrices can be directly estimated from data, and they can be obtained from
user studies as well [3, 5].
For any ranked list returned by a system, the expected
limiting nDCG due to assessor disagreement can be formulated as a function of (1) the disagreement model C and (2)
the number of assessed documents and their distribution
over the label classes. One way to compute this expected
nDCG is numerical simulation: For every document d having testing label id in the ranked list, we randomly draw an
alternative label jd with the probability cij ; we then sort
the ranked list in decreasing order of {jd } and evaluate its
nDCG performance with respect to labels {id }. This simulation is repeated multiple times and the results averaged to
obtain an accurate estimate of the expected limiting nDCG.
In our first experiment, we test whether the inter-assessor
confusion matrix C alone can be used to estimate the limiting nDCG value. We do so by considering data sets that
have multiple judgments per query-document pair, such as
were collected in the TREC Enterprise Track where each

9∗ This work supported by NSF grant IIS-1017903.

Copyright is held by the author/owner(s).
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
ACM 978-1-4503-1472-5/12/08.

1091

Real vs. Simulated NDCG Bounds (MSR C)

1

1

0.9

0.9

Real NDCG Bound

Real NDCG Bound

Real vs. Simulated NDCG Bounds (Enterprise C)

0.8

0.7

SG
BG
GS
BS
GB
SB

0.6

0.5

0.4

0.3
0.3

0.4

0.5

0.6

0.7

0.8

Simulated NDCG Bound

0.9

0.8

0.7

SG
BG
GS
BS
GB
SB

0.6

0.5

0.4

1

Collection
MSLR30K(SIM)
MSLR30K(CFA)
Yahoo(SIM)
Yahoo(CFA)

0.3
0.3

0.4

0.5

0.6

0.7

0.8

0.9

C YanR
0.867
0.869
0.920
0.919

C YanU
0.900
0.898
0.944
0.938

LearnToRank
0.741
0.801

Table 1: nDCG upper bounds derived from disagreement models C applied to popular learning-torank data sets. SIM rows are simulated values; the
CFA rows are closed form approx. Last column is
best known learning-to-rank performance.
number of documents in the rank-list, and Prank (i, r) the
probability that the rank of a given document with reference
label i is r, as ordered by the alternative labels j. One can
then show that the expected nDCG as measured by reference
labels is
h
i
P
P
Prank (i,r)
1
· i∈L ni · gain(i) · n
E[nDCG] = IdealDCG
r=1 discount(r)
h
“P
”i
P
Ψij (h,s)
r−1 Pn−h−1
where Prank (i, r) = j∈L cij ·
s=r−1−h
h=0
s+1

1

Simulated NDCG Bound

Figure 1: Applying C ENT model (left) and C MSR
model (right) to TREC enterprise data. X-axis is
the simulated nDCG upper bound, while Y-axis is
the actual nDCG assessor disagreement measured
between 2 TREC assessors; pairs of assessor type
(“Gold-Silver” as GS) are indicated by colors.
topic was judged by three assessors: a gold assessor G (expert on task and topic), a silver assessor S (expert at task
but not at topic), and a bronze assessor B (expert at neither). Each G, S, and B set of assessments can take on the
role of training or testing assessor, as described above, giving
rise to six possible combinations: GS, GB, SG, SB, BG, BS.
For each such combination, such as GS, the optimal ranked
list can be computed with respect to G and evaluated with
respect to S, resulting in a real suboptimal nDCG. The GS
confusion matrix can also be computed from the data given
and the simulation described above performed, yielding an
estimated limiting nDCG. These actual and estimated limiting nDCG values can then be compared.
Using the TREC Enterprise data, Figure 1 compares the
estimated limiting nDCG obtained through simulation with
a confusion matrix (x-axis) with the real suboptimal nDCG
(y-axis) obtained from different assessors. The left plot uses
a confusion matrix CENT obtained from the TREC Enterprise data itself, as described above, while the right plot
uses a confusion matrix CMSR obtained from a user study
conducted by Microsoft Research [3]. Note that the more
accurate confusion matrix yields better simulated results, as
expected, and that the confusion matrix alone can be used
to accurately estimate limiting nDCG values in most cases.
Given that a confusion matrix alone can be used, via simulation, to estimate limiting nDCG performance, we next consider other data sets and their associated real or estimated
confusion matrices. Yandex [5] conducted user studies to obtain confusion matrices specific to Russian search (CY anR )
and to Ukrainian search (CY anU ), and these were shown
to improve learning-to-rank performance if the learner was
given such models as input. Table 1 presents the estimated
limiting nDCG values when applying three different confusion matrices to two learning-to-rank data sets. For comparison, the last column in the table presents the actual
best known performance of a learning algorithm on these
data sets. Consider the difference between the estimated
limiting nDCG bounds (middle three columns) and known
ranking performance (last column): If CMSR is a good model
of assessor disagreement for these data sets, then the known
learning-to-rank performance is reasonably close to the limiting bound, and little improvement is possible. On the other
hand, if CYanU is a better model of inter-assessor disagreement, then learning algorithms have room for improvement.

3.

C MSR
0.780
0.794
0.861
0.887

and Ψij (h, s) is the probability that other s documents have
the same alternative label j, and other h documents have
alternative label higher than j, given that a particular document with reference label i has alternative label j. Computing Ψij (h, s) straightforwardly is inefficient for even moderately long rank-lists, with a running time of O(n2|L| ).
We instead employ a closed form approximation (CFA)
based on approximating Ψ, a sum-product of binomial conditional distributions, with a Gaussian joint distribution of
two variables (h + s, s). This approximation becomes more
accurate as rank-lists get longer. For a fixed i and j we have
2
´
` σh+s
`h+s´
`
cov h+s,s ´
, and Σ =
∼ N ij (µ, Σ), µ = µh+s
2
µs
s
cov s,h+s
σs
P
P
where (1) aij = k≥j cik , (2) µh+s = −aij + k∈L nk · akj ,
P
(3) µs = −cij + k∈L nk · ckj , and
P
2
σh+s
= −aij · (1 − aij ) + k∈L nk · akj · (1 − akj )
P
σs2 = −cij · (1 − cij ) + k∈L nk · ckj · (1 − ckj )
P
cov s,h+s = −cij · (1 − aij ) + k∈L nk · ckj · (1 − akj ).
We can approximate E[nDCG] in O(n2√
) time given that the
“spread” of the Gaussian grows as O( n) per component.
The CFA rows of Table 1 show closed form approximations
for comparison with simulated nDCG upper bounds.

4.

CONCLUSION

We present a simple probabilistic model of assessor disagreement and results which indicate that the performance
of learning-to-rank algorithms may be approaching inherent
limits imposed by such disagreement.

5.

REFERENCES

[1] P. Bailey, N. Craswell, I. Soboroff, P. Thomas, A. P.
de Vries, and E. Yilmaz. Relevance assessment: Are
judges exchangeable and does it matter? SIGIR, 2008.
[2] P. Bailey, A. P. De Vries, N. Craswell, and I. Soboroff.
Overview of the TREC-2007 Enterprise Track.
[3] B. Carterette, P. N. Bennett, D. M. Chickering, and
S. T. Dumais. Here or there: Preference judgments for
relevance. In ECIR, 2008.
[4] B. Carterette and I. Soboroff. The effect of assessor
error on ir system evaluation. In SIGIR, 2010.
[5] A. Gulin, I. Kuralenok, and D. Pavlov. Winning the
transfer learning track of Yahoo!’s learning to rank
challenge with YetiRank. Journal of Machine Learning
Research, 2011.

CLOSED FORM APPROXIMATION

Let L = {0, 1, 2, 3, 4} be the set of relevance grades, nk the
number of documents with reference label k ∈ L, n the total

1092

