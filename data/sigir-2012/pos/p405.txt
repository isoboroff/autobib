Learning Hash Codes for Efficient Content Reuse
Detection
Qi Zhang, Yan Wu, Zhuoye Ding, Xuanjing Huang
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, P.R.China

{qi_zhang, 10210240075, 09110240024, xjhuang}@fudan.edu.cn
ABSTRACT

ed in the number of user generated content (UGC) rapidly
growing during the last few years.
While the increasing of UGC, content reuse, which is the
practice of using existing content components, occurs frequently in these mediums. It contains various forms including duplicate, near-duplicate, and partial-duplicate. Exact duplicate documents can be easily identiﬁed by standard checksumming techniques. Near-duplicate web pages
contain identical core content but are diﬀerent in framing,
navigation bar, advertisements, footer, or other non-content
parts. Partial-duplicate, which is a more diﬃcult problem,
contains quoted phrases, sentences, or passages from other
documents.
Duplicate or near-duplicate detection can help search engine to reduce storage costs and improve the quality of search
indexes. It may also avoid users to see redundant documents
in search results. Applications including plagiarism detection, information ﬂow tracking, opinion mining, and so on
may beneﬁt from the partial-duplicate detection or involve
it as the basis. Along with the increasing requirements, content reuse detection has received much attention in recent
years. Many eﬃcient and eﬀective algorithms have been
proposed [11, 15, 19, 23, 24, 25, 28, 33].
The challenges of content reuse detection include: 1) reuse
may happen at diﬀerent levels; 2) massive documents should
be eﬃciently processed. Partial-duplicate detection may require diﬀerent algorithms to the approaches proposed for resolving near-duplicate document detection. One of the main
reason for this is that only a small part of a document is taken from others. Content reuse detection algorithms have to
face with enormous documents, due to the rapid growth of
the web. Thus, it is essential that content reuse detection
methods should be eﬃcient and scalable.
In this paper, we investigate a novel approach to detect
sentence level content reuse by mapping sentence to a signature space. Signature of a sentence is created by taking
the bitwise-or of all signatures of words occurs in the sentence. Rather than using traditional hash functions, which
do not consider statistics of words or characters, to assign
hash code for each word/character, we analyze the requirements of what the good codes should satisfy and formalize
it as a constraint optimization problem. Since the task of
ﬁnding optimal codes is NP hard, we relax the optimization problem and introduce a eﬃciency method to calculate
the codes. With the signature generation method, sentences
whose reuse scores are predicted to be less than a given
threshold are eliminated. Experimental results show that
the codes generated by the proposed method outperform

Content reuse is extremely common in user generated mediums. Reuse detection serves as be the basis for many applications. However, along with the explosion of Internet
and continuously growing uses of user generated mediums,
the task becomes more critical and diﬃcult. In this paper,
we present a novel eﬃcient and scalable approach to detect
content reuse. We propose a new signature generation algorithm, which is based on learned hash functions for words. In order to deal with tens of billions of documents, we
implement the detection approach on graphical processing
units (GPUs). The experimental comparison in this paper
involves studies of eﬃciency and eﬀectiveness of the proposed approach in diﬀerent types of document collections,
including ClueWeb09, Tweets2011, and so on. Experimental results show that the proposed approach can achieve the
same detection rates with state-of-the-art systems while uses signiﬁcantly less execution time than them (from 400X to
1500X speedup).

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval - Information Search and Retrieval;
H.3.7 [Digital Libraries]: Collection, Systems Issues

Keywords
Content Reuse Detection, GPUs, Learning to Hash

1. INTRODUCTION
There is a quick expansion in the popularity of user generated content in forums, microblogging sites, blogs, and other
medius in recent years. These broadcast mediums provide
opportunities for users to exchange content. According to
the statistics, users in Twitter send 230 million tweets per
day[8]. Technorati’s State of the Blogosphere Report also
showed that there are about 126 million blogs on the Internet in 2010. The development of these platforms has result-

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$10.00.

405

taneously output the positions where the duplicated parts
occur. In order to handle the eﬃciency problem, they implement their method using three MapReduce jobs.
Local text reuse detection focus on identifying the reused
and modiﬁed sentences, facts or passages, rather than whole documents. Seo and Croft [23] analyzed the task and
deﬁned six categories of text reuse. They proposed a general framework for text reuse detection. Several ﬁngerprinting techniques for the framework were evaluated under the
framework.
The most similar work to ours was proposed by Kim et
al. [15]. They mapped sentences into a point in a high dimensional space and leveraged range searches in this space.
However diﬀerent with us, they simply use MD5 hash function for each word to generate signature ﬁle. In this paper,
we outline and discuss what makes a good code for content
reuse detection, and propose to use learned hash codes to
capture the relations between words/characters to reduce
the false matches.

state-of-the-art approaches. In order to handle millions of
documents, graphical processing units (GPUs) are used to
implement the detection algorithm.
The contributions of this work are as follows: 1) We outline and discuss what makes good codes of words for content
reuse detection. 2) Eﬃcient optimization method is proposed based on several relaxation. 3) We provide parallel
algorithm and its GPU implementation. 4) Evaluations on
six large web collections in both English and Chinese are
used to measure the eﬃciency and eﬀectiveness.
The remaining of the paper is organized as follows: In
section 2, we review a number of related work and the stateof-the-art approaches in related areas. Section 3 presents the
proposed method. Experimental results in test collections
and analyses are shown in section 4. Section 5 concludes
this paper.

2.

RELATED WORK

Our approach relates to three research areas: content
reuse detection, learning to hash and parallel algorithms based on GPUs. In this section, we discuss the related
work on these areas.

2.1

2.2

Learning to Hash

Extensive research on similarity search have been proposed in recent years. Among them hash-based methods
were received more attention due to its ability of solving similarity search in high dimensional space. Recently, several
researches attempted to ﬁnd good data-aware hash functions
through machine learning.
Hinton and Salakhutdinov proposed to train a multilayer
neural network with a small central layer to convert highdimensional input vectors into low-dimensional codes [13].
They used a two-layer network called a Restricted Boltzmann machine(RBM) [14] to do it. Experimental results
showed that it could accelerate document retrieval.
Spectral hashing [30] was deﬁned to seek compact binary
codes in order to preserve the semantic similarity between
codewords. Weiss et al. deﬁned the criterion for a good code
which is related to graph partitioning and used a spectral
relaxation to obtain a solution.
Norouzi and Fleet [20] introduced a method for learning
similarity-preserving hash functions, which is based on latent structural SVM framework. They designed a speciﬁc
loss function taking Hamming distance and binary quantization into account.
Zhang et al. introduced Self-Taught Hashing (STH) approach to semantic hashing [32]. They divided the problem of ﬁnding small codes into two stages. Firstly, they
used unsupervised method, binarised-LapEig, to optimal l bit binary codes for all documents in the given corpus. The
classiﬁers were trained to predict the l -bit code for unseen
documents.
Almost all the current methods for similarity-preserving
hash functions attempt to map the high dimensional data,
which represents the whole document or sentence, onto binary codes. In this paper, we seek good binary codes for
words under the content reuse detection framework.

Content Reuse Detection

Content reuse detection has received much attention in
the past several years. Previous studies on content reuse
detection can be roughly divided into two research directions: representation and eﬃciency. The ﬁrst one focuses
on representing text in diﬀerent levels with or without linguistic knowledge. With the growth of digital documents,
eﬃciency, has also received much more attentions.
Shingling, which was proposed by Broder [4], uses contiguous subsequences to represent documents. It does not
rely on any linguistic knowledge. If sets of shingles extracted
from diﬀerent documents are appreciably overlap, these documents are considered exceedingly similar, which are usually
measured by Jaccard similarity. In order to reduce the complexity of shingling, meta-sketches was proposed to handle
the eﬃciency problem [5].
In order to improve the robustness of shingle-like signatures, Theobald et al. introduced a method, SpotSigs.
It provides more semantic pre-selection of shingles for extracting characteristic signatures from Web documents [28].
SpotSigs combines stopword antecedents with short chains
of adjacent content terms. The aim of it is to ﬁlter naturallanguage text passages out of noisy Web page components.
They also proposed several pruning conditions based on the
upper bounds of Jaccard similarity.
Chowdhury et al. proposed I-Match [7], which ﬁlters the
input document based on collection statistics and compute
a single hash value for the remainder text. If the documents
with same hash value, they are considered as duplicates. It
hinges on the premise that removal of very infrequent terms
and very common terms results good document representations for the near-duplicate detection task. Since I-Match
signatures is respect to small modiﬁcations, Kolcz et al. [16]
proposed the solution of several I-Match signatures, all derived from randomized versions of the original lexicon.
Diﬀerent from the methods focused on document level,
partial-duplicate detection was proposed by Zhang et al. [33].
They converted the task into two subtasks: sentence level
near-duplicate detection and sequence matching. Except for
the similarities between documents, the method can simul-

2.3

GPU-based Algorithms

Graphics programming units is designed for single instruction multiple data(SIMD) paradigm, which is diﬀerent from
general purpose microprocessors. Due to its advantages
on massive parallel, high memory bandwidth, and powerful computing capacity, GPUs have been successfully used
in numerical algorithms.

406

Google
N-gram
Dataset

Hash Code
Generation

Training
Data

Word/Character
Hash Table

Signatures
Sentences

S1

Sentence
Extraction

Data Set

S2

Signature
Generation

…
S1

Sn

010101010000
110001001010
000110000100
110001001101
001010010011

offline

...

110100001010
010001010101
010101011110

Query
Candidate Reuse

Reuse
Sentences

(Doci,Sj)
(Doci,Sj)

Reuse Score
Computation

…
…
…

online
GPU Based Parallel
Candidate Search

Figure 1: An overview of the proposed content reuse detection approach
a number of works to study the eﬀect of cosine similarity [2,
31]. However, directly use these similarity metrics to detect
content reuse in large collections would be very expensive.
Because of this, in recent years, hash-based methods have
been carefully studied and have demonstrated their advantageous for near similarity search in large document collections [27].
In this paper, we follow the method proposed by Kim
et al. [15], which map sentence signature into a point in
a high dimensional space. Within this method, each word
(or character in Chinese corpus) is assigned a ﬁxed-width
bit string. The sentence signature is generated by taking
bitwise-or of all signature of words in the sentence. Figure 2
shows an example of sentence signature generation process.
h(·) represents hash function for words. In [15], two bits
were set for each word using MD5 hash function [22] for
32-bit signatures.
Suppose Si = bi1 bi2 bi3 ...bim , which consists of m-bits, represents a signature extracted from a sentence. We can map
it into a point, pi = (bi1 , bi2 , bi3 , ..., bim ), in m-dimensional
space. Then the candidate sentences can be selected based
on Euclidean distance between sentences, which can be calculated as follow:

m

Distance(pi , pj ) = 
(bik − bjk )2 .

Owens et al. [21] introduced their works on implementing three applications (protein folding simulation, scalable
molecular dynamics, and calculating electrostatic potential
maps). Through these examples, they demonstrated the potential of the GPU for delivering performance gains on real
problems.
The emergence of the NVIDIA CUDA programming model speed up the trend of using GPUs to accelerate algorithms.
Edelkamp et al. [9] introduced their work on accelerating state space search using GPUs. Linear algebra operators were
also implemented to build blocks for more complex numerical algorithms [17]. The results of researches in sort [12],
search [6], linear algebra [10], partial diﬀerential equations
(PDEs) [3], and many other applications have demonstrated
the performance and capabilities of GPUs.
F

3.

OUR APPROACH

The processing ﬂow of the proposed content reuse detection approach is shown in Figure 1. It consists of two distinct
stages:oﬄine and online. Given a collection of documents,
the sentence extraction step splits the documents into sentences. Hash code generation step takes the training data
calculated based on the given collections or open domain
data set to generate data-aware hash codes for words or
characters. Signature generation step uses the hash table to
calculate hash code of sentences from both document corpus
and given queries. The key algorithm of the online stage is
candidate searching, which tries to ﬁlter the sentences whose
reuse scores with the given query are guaranteed not bigger
than a given threshold. The reuse sentences and their corresponding reuse scores are calculate in the ﬁnal step.
Many similarity metrics have been proposed for content
reuse detection. Jaccard similarity has been used in various
works [4, 33]. Metzler et al. proposed to use weighted word
overlap similarity to measure reuse score [18]. There are also

k=1

In other words, it also represents the number of
√ bit diﬀerent
between the two sentences. Given a threshold d, the points
whose distances lie in the range are extracted as candidates.

3.1

Learning Hash Code

As mentioned above, signatures of sentences are generated
based on the hash codes of words. Therefore, how to select
hash codes for words has become one of the key problems in
this task. In the following parts of this section, we discuss
and describe our proposed method.

407

Sentence: Apple announces iPhone 4s.
h(Apple)
h(announce)
h(iPhone)
h(4s)
Signature of Sentence (bitwise-or)

The solving of equation (1) is 0-1 integer programming
program, which is a special case of integer programming.
It is known to be NP-hard. There is no easy solution for
directly optimizing it. Since the number of bits required
by equation (1) does not follow the restrictive assumption
that the bits are uniformly distributed, the spectrum of the
similarity matrix of the data can not be directly used to
get the hash codes as spectral hashing [30]. Relaxation and
approximation methods should be used to solve the largescale problem.
First of all, the constraint yi ∈ {0, 1}m is replaced by 0 ≤
yi ≤ 1. By ignoring the integer constraints, the objective
function in (1) is diﬀerentiable. In other words, the problem
in (1) is relaxed as:

= 0100 0010
= 1100 0010
= 1001 0000
= 0101 0011
= 1101 0011

Figure 2: Example of sentence signature

3.1.1

What good code should satisfy?

The proposed approach tries to ﬁnd as many as possible
reuses under the given upper bound of false matches. In
other words, it aims to maximize the recall rate, ρrec , on the
given false positives rate ρf p . Under the same conditions,
the number of bits, m, and the number, l, of bits set to
1 in the signature eﬀect the detection recall and precision.
In particular,
  m, l, and vocabulary size should follow the
equation ml ≥ w [15], and l is selected as the smallest
value among all candidates. Hence, in order to save the
code space, words which usually occur together should have
the same hash code.
Based on the descriptions above, we seek hash codes which
should satisfy the following properties: (1) the number of
bits set to 1 in the word hash code is low; (2) the number
of bits to code the vocabulary should be small; (3) words
which usually occur together should have same hash codes.
In this paper, we formalize the good code seeking task as a
constraint optimization problem.
Let {yi }n
i=1 be the list of hash codes for n words. yi ∈
{0, 1}m represents m-bits binary vector. #(wi ) represents
the number of sentences containing word wi in the given corpus, #(wi , wj ) indicates the number of sentences containing
both word wi and word wj . sij measures the similarity between word wi and wj , which is formulated as following:
sij = 



minimize:

ij

ij

sij

k=1
m

−μ

m
n 

i

(ln(yik ) + ln(1 − yik ))

(3)

k

,where μ is barrier parameter, which is decreased at each
optimization iteration. The problem in equation 1 can now
be formulated as a sequence of approximate maximization :
minimize:



sij

ij

subject to:

m


m


(yik − yjk )2

k=1
m
n 

i

(yik − yjk )2

subject to: yi ∈ {0, 1} , 1 ≤ i ≤ n
m

yik = l, 1 ≤ i ≤ n

(2)

Since there are usually thousands of yi and m is also bigger
than 32 in practice, the number of parameters tend to be
extremely large. The number of constraints is also linear in
the size of vocabulary. Because of these, the problem in (2)
can not be directly solved within acceptable time either.
In this work, we use an interior-point nonlinear programming algorithm based on a ﬁlter line search to solve the
problem [29]. Based on it, the inequality constraints are
converted to barrier functions which are combined with objective function. We combine the following barrier function
with objective function to replace the constraint 0 ≤ yi ≤
1, 1 ≤ i ≤ n:

−μ


(yik − yjk )2

k=1

k=1

sij equals to binary cosine similarity in which a dimension
receives a score of 1 when the word appears in the sentence
and 0 when it does not appear. The parameter l deﬁnes the
number of bits set to 1 in the hash code. sij can be unsupervised generated based on given corpus. By incorporating all
the constraints together, we obtain the following problem:

minimize:

m


subject to: 0 ≤ yi ≤ 1, 1 ≤ i ≤ n
m

yik = l, 1 ≤ i ≤ n

#(wi , wj )

.
#(wi ) #(wj )

m


sij

(ln(yik ) + ln(1 − yik ))

(4)

k

yik − l = 0, 1 ≤ i ≤ n

k=1

(1)

3.2

Reuse Detection

Algorithm 1 presents the pseudo-code for the detection
part (marked as online in the ﬁgure 1) of the proposed
reuse detection method. SigCol represents a collection of
sentence signatures extracted from a document collection.
Given SigCol and a query sentence, q, the candidate search
step tries to identify a set of candidate sentences, CSet.
The candidate search step selects sentences by searching the
points whose√distances between pq are less than the given threshold d. After that, if the candidate sentences set
CSet is not empty, the reuse score computation step calculates the reuse scores between query sentence with each

k=1

The property (1) is satisﬁed by m
k=1 yik = l, where l is
1
usually set to a small number (in this work, we set l = 16
m).
The number of bits in hash code is predeﬁned by m to follow the property (2). The property (3) is implemented as
the objective function. If two words usually occur together in sentences and have diﬀerent hash code, it would give
negative impact of the objective function.

3.1.2 Optimization

408

Algorithm 1 Pseudo-code of the Reuse Detection
INPUT: a query sentence, q, a distance threshold
signatures of document collection, SigCol
OUTPUT: a set of detected reuse sentences, Oq

Signatures

√

010101010000
110001001010
000110000100
110001001101
001010010011

d, and

Stream Processors

Generate signature Sq for the given query q,
for all Si ∈ SigCol do √
if Distance(Sq , Si ) < d then
CSet = CSet i
end if
end for
return CSet

Candidate Reuse

3

...

110100001010
010001010101
010101011110

Candidate Selection:
1:
2:
3:
4:
5:
6:
7:

1

2

Global Memory

GPU Device

(Doci,Sj)
(Doci,Sj)

…
…
…

Query

Figure 3: Implementation of GPU Based Parallel Candidate
Search

4.

Reuse Score Computing:

4.1

1: for all i ∈ CSet do
2:
Simq,i = Jaccard(Sq , Si )
3:
if Simq,i > θ then
4:
Oq = Oq < i, Simq,i >
5:
end if
6: end for
7: return Qq

candidate based on Jaccard similarity. If the reuse score of
a sentence is greater than the pre-deﬁned threshold, the sentence and its corresponding document is added in the ﬁnal
list.
From analyzing the calculation consumption of algorithm
1, we observe that the most time consuming part is spent on
step 3 of the candidate selection step. Since the number of
sentences in SigCol is usually tens of millions, Distance(Sq , Si )
takes the most computing time of the whole algorithm. Fortunately, GPUs oﬀered us an opportunity to accelerate the
performance of the algorithm. Modern GPUs are massively
parallel processors with extremely high memory bandwidth.
Many operations can be performed in parallel. The distance
calculation part is suitable for implementation on GPUs as
it is fairly simple and consumes a huge part of computing
resources.
The function executed GPUs in parallel is called kernel,
which is driven by threads and grouped together in blocks
and grids. In this work, we implement the step 3, 4, and 5 in
candidate selection part as a kernel function. Based on the
thread index and block index, diﬀerent sentence signature
Si will be justiﬁed in diﬀerent threads. Since signature ﬁle
of the whole corpus is small enough (302MB for 10 million
sentences), it can be easily loaded into the global memory
of modern GPUs entirely at once.
The implement of the candidate selection is shown in ﬁgure 3. The ﬁrst step is to copy signatures extracted from
a collection into global memory. Diﬀerent threads would
parallel process diﬀerent parts of signatures in stream processors. The step 2 and step 3 in the ﬁgure are iterated for
each query. Because the bottleneck of this task on GPUs is
the memory access rather than processing time, if multiple
queries were processed at the same time, the acceleration
ratio to CPU implementation would be further improved.

409

EXPERIMENTS
Collections

We evaluate the proposed method with six corpora TIPSTER (Volume 1-3)1 , ClueWeb09-T09B2 , Tweets2011 Twitter 3 , SogouT 2.04 , Baidu Zhidao5 , Sina Weibo6 . Table 1
shows the statistics of the six collections. TIPSTER collection contains news articles, discourse passages extracted
from Associated Press (AP), Wall Street Journal (WSJ) and
so on. It has been used for evaluations of information retrieval, entity extraction, and many other tasks. Tweets2011
Twitter collection is used by Trec 2011 microblog track,
which contains about 16 million tweets sampled between
January 23rd and February 8th, 2011 7 . TREC Category B dataset (ClueWeb09-T09B), which is a subset of the
ClueWeb09, contains 50 million English pages and has been
used in various TREC tracks. Since the proposed approach
is language independent, we also evaluate the proposed approach on three Chinese collections. Baidu Zhidao is one
of the most popular community Q&A site in China. We
crawled a portion of question and answer pairs of all categories from it, resulting in a local archive of about 33.5
million questions. Sina Weibo is the most popular and the
largest Twitter-like micro-blog site in China. Messages or
comments from approximate 1.78 million users are used in
this work.

4.2

Implementation and Setup

We set the signature lengths(m) to 32 in this work. Following the parameters used in [15], the l is set 2 for 32
bits signature. For English collections, words are used as
the basic units to assign hash codes. The basic units of
Chinese collections are characters. We re-implemented the
baseline qSign algorithm [15] using the MD5 [22] as hash
function to assign hash codes for all the words/characters.
It is labeled as “MD5 ” in the following tables and ﬁgures. To
verify the proposed properties that good hash code should
satisfy, we construct another hash code generation baseline,
which set the same hash code for the words which often oc1

http://www.ldc.upenn.edu/
http://boston.lti.cs.cmu.edu/Data/clueweb09/
3
http://trec.nist.gov/data/tweets/
4
http://www.sogou.com/labs/dl/t.html
5
http://zhidao.baidu.com
6
http://www.weibo.com
7
Since the data is crawled by ourselves with the tools and
tweet lists provided by NIST, about 5.8% tweets are missed
in our collection due to the user name change or other reasons.
2

Table 2: Detailed impact of diﬀerent hash code generation methods. The reuse threshold θ is set to 0.8.
MD5
NER
OPT
bits diﬀ.
#can.
P
R
#can.
P
R
#can.
P
R
d=0
d=1
d=2
d=3
d=4
d=5

19,087
17,2464
992,403
3,955,643
12,049,613
29,990,264

8,881 0.514 0.896
4,704 0.965
64,361 0.073 0.929
7,262 0.648
416,552 0.012 0.968
27,060 0.182
1,889,988 0.002 0.994
139,808 0.036
6,472,842 0.001 1.000
632,570 0.008
17,857,693 0.000 1.000 2,358,244 0.002
(a) TIPSTER
The total number of ground truth reuse sentences for 2,000 queries is 5,095.

bits diﬀ.
d=0
d=1
d=2
d=3
d=4
d=5

0.239
0.027
0.005
0.001
0.000
0.000

0.896
0.928
0.965
0.992
1.000
1.000

MD5
P

#can.
15,686
72,832
310,708
1,146,414
3,775,472
11,025,379

R

0.611
0.224
0.071
0.021
0.006
0.002

#can.

0.6

R

0.2

MD5
NER
OPT

0.6

0.94

0.96

0.98

0.4

0.6
0.4
0.2

0
0.85

1

MD5
NER
OPT

0.8

0.2

0.92

0
0.9

Recall

0.95

0.7

1

0.8

Recall

(a) TIPSTER

0.7

Precision

0.6
0.4

0.6

MD5
NER
OPT

0.8
0.6

MD5
NER
OPT

0.5

Precision

MD5
NER
OPT

1

(c) ClueWeb09-T09B

1

0.8

0.9

Recall

(b) Tweets2011 Twitter

1

0.403
0.687
0.907
0.986
0.998
0.999

1

0.8

0.4

0.9

OPT
P

0.394
0.669
0.907
0.987
0.998
0.999

Precision

MD5
NER
OPT

Precision

Precision

R

1

0.8

Precision

NER
P

15,205 0.631 0.394
15,773 0.623
67,964 0.243 0.678
67,846 0.247
258,559 0.085 0.906
242,141 0.091
862,317 0.027 0.987
727,063 0.033
2,654,901 0.009 0.998 2,001,083 0.012
7,574,689 0.003 0.999 5,306,237 0.004
(b) SogouT 2.0
The total number of ground truth reuse sentences for 2,000 queries is 24,359.

1

0
0.88

#can.

0.891
0.924
0.966
0.988
0.998
1.000

0.4

0.4
0.3
0.2

0.2
0
0.85

0.2

0.9

0.95

Recall

(d) Baidu Zhidao

1

0
0.992

0.1
0
0.994

0.996

Recall

(e) Sina Weibo

0.998

1

0.2

0.4

0.6

0.8

1

Recall

(f) SogouT 2.0

Figure 4: The precision-recall curves of candidate searching based on diﬀerent hash code generation methods in all six corpora.
The reuse threshold θ is set to 0.8.

410

Table 1: Statistics of the evaluation document collections
Corpus
TIPSTER
Tweets2011 Twitter
ClueWeb09-T09B
Baidu Zhidao
Sina Weibo
SogouT 2.0

Language
English
English
English
Chinese
Chinese
Chinese

#Docs
1,078,925
15,204,939
50,220,423
33,497,107
267,612,493
37,205,218

Size
3.25GB
2.13GB
490.4GB
22.8GB
418.6GB
558.0GB

Corpus
TIPSTER

Twitter

ClueWeb09

cur together. “NER” is used to represent this method. For
the proposed approach, although we have proposed several
methods to reduce the computing consumption of the optimization problem, there would also be too many variables
needed to optimize. In this work, we select top 3, 000 words/characters to optimize according to their frequency. The
similarity matrix is calculated based on 300,000 sentences
extracted from each collection. The hash codes of the other words are generated based on MD5. We use “OPT ” to
represent this method in the following.
All the experiments were evaluated on a workstation with
a 2.13G Intel Xeon quad-core processor, 4GB memory, and
an NVIDIA Quadro 4000 graphics card with 2GB global
memory and 256 stream processors. CUDA Toolkit version
4.1 is used to implement the algorithm. For sentence boundary detection, we used around 50 manually written rules to
do it.

4.3

Table 3: The average number of selected candidates per
sentence at diﬀerent recall level. The reuse threshold θ is
set to 0.8.

Baidu Zhidao

Sina Weibo

SogouT 2.0

Recall
Level
0.89
(d=0)
1.00
(d=4)
0.88
(d=0)
1.00
(d=4)
0.77
(d=0)
1.00
(d=4)
0.75
(d=0)
1.00
(d=3)
0.99
(d=0)
1.00
(d=2)
0.90
(d=2)
1.00
(d=4)

Gol.

MD5

NER

OPT

2.5

9.5

4.4

2.4

2.5

6024.8

3236.4

316.3

1.5

1.6

1.6

1.6

1.5

488.1

485.1

477.4

35.1

27.7

27.3

26.9

35.1

1253.0

1006.4

554.1

2.6

3.0

3.0

2.9

2.6

147.2

108.7

94.5

37.5

40.4

39.5

39.6

37.5

145.8

95.0

47.4

12.2

155.4

129.3

121.1

12.2

1887.7

1327.5

1000.5

same recall level, the number of selected candidate sentences
based the hash codes generated by the proposed methods is
only 2.7% to 24.6% of the sentences selected based on MD5
hash code. This indicates that the proposed method can
dramatically improve the eﬀectiveness of candidate searching.
The precision-recall curves graph for all six collections are
shown in Figure 4. The reuse threshold θ is also set to 0.8
in all the experiments for this ﬁgure. In almost all cases, the proposed approach achieves the best result among
the three hash code generation methods. These results also
demonstrate the observations described in the previous section. Although from the view precision-recall curve graph
the precision improvement can not be easily noticed, at a
recall level of 0.999, the proposed approach can further reduce around 51.9% sentences in SogouT 2.0, 55.4% in Sina
Weibo and 27.7% in Baidu Zhidao over baseline method.
Table 3 shows the detailed performance of candidate searching step at diﬀerent recall level in all six collections. The
“Gol.” column represents the average number of ground
truth reuses per sentences. Since the recall level can only be controlled by the bits diﬀerence threshold, we list the
parameter d in the bracket in the third column. From the
table, we can observe that quotations are common in Web
collections. While news articles also contain a large number
of exact quotations. At almost all recall levels, the proposed
hash code generation method achieves the best performance.
It means that the proposed method can beneﬁt the performance of candidate searching step.
In the above experiments, the documents used to calculate similarities between words for our approach are random
selected from each collection. Since in-domain data may not
be pre-collected in some cases, in this experiment we evaluate the performance of candidate selection with hash codes

Effectiveness Evaluation

To compare the candidate searching with diﬀerent hash
code generation methods, for each corpus, we randomly selected 1 million sentences as evaluation data sets. As reuse
detection queries, 2,000 sentences are randomly selected from
them. The similarities between queries and sentences in the
data set are calculated using cosine coeﬃcient. The ground
truth reuse sentences are obtained by comparing queries
with all sentences in corresponding data set.
Table 2 illustrates the impact of candidate searching step
with diﬀerent hash code generation methods. We only list
the detailed result in TIPSTER and SogouT 2.0, due to
space limitations. Figure 4 summarizes results of all six
corpora. In Table 2, d represents the d bits diﬀerence between query and reuse sentences. The reuse threshold θ
is set to 0.8. “#can.” represents the number of candidate
sentences, which are selected using diﬀerent hash code generation methods. From the results, we can observe that candidate searching step really beneﬁts acceleration the reuse
detection system. Instead of comparing with all sentences,
the step can help reduce more than 90% calculation without
losing any correct reuses in most cases.
From Table 2a, we can also observe that diﬀerent hash
code generation methods may highly impact the performance
of candidate searching step. Although the baseline method,
which is also used by previous work qSign [15], already
achieves good results, the simple way which merge the hash
code of similar words can further give more than 50% improvement in all bits diﬀerence level. Comparing to the
baseline methods, hash codes generated based on the proposed learning based method achieve the best results. At the

411

role for the quality of generated hash codes from diﬀerent
domains.

Table 4: The average number of selected candidates per
query sentence with in-domain and out-of-domain data. The
reuse threshold θ is set to 0.8. The recall level is set to 100%,
the corresponding bits diﬀerence d are in the bracket. “OPT IN” represents hash codes generated based on in-domain
data. “OPT OUT” represents hash codes generated based
on out-of-domain data.
Corpus
TIPSTER (d=4)
Twitter (d=4)
ClueWeb09 (d=4)
Baidu Zhidao (d=3)
Sina Weibo (d=2)
SogouT 2.0 (d=4)

MD5
6024.8
488.1
1253.0
147.2
145.8
1887.7

OPT IN
316.3
477.4
554.1
94.5
47.4
1005.5

4.4

OPT OUT
5482.1
492.1
796.4
88.1
49.1
1228.9

Words\Characters Overlap

90%
80%
70%

60%
50%
40%
30%

Baidu Zhidao
Sina Weibo
SogouT 2.0
Twitter
TIPSTER
ClueWeb09

20%
10%
0%

# of Top Frequency Words

Figure 5: The overlap percentage of top frequency words/characters between corpora and “Web 1T 5-gram Corpus”
or “Chinese Web 5-gram”.

generated with similarities matrix calculated from open domain data sets. We use “Web 1T 5-gram Corpus8 ” and “Chinese Web 5-gram9 ” as the open domain data, where 5-grams
are treated as sentences. They contain English and Chinese
word n-grams and their observed frequency counts. Table 4
shows the performance candidate selection with these corpora. From the results, we can observe that in-domain data
performs better than out-of-domain’s in most cases. Out-ofdomain data works better in Chinese than in English. For
the corpus Baidu Zhidao, the performance of hash codes generated based on 5-grams is even better than in-domain data.
We think that the main reason is that the words distributions in “Chinese Web 5-gram” are similar as Baidu Zhidao,
which also contains documents from multiple domains. For
English corpora, the performances of out-of-domain are not
good as in-domains. In order to ﬁnd the reason, we analysis
the overlap of top frequency words. Figure 5 shows the overlap percentage of top frequency words/characters between
corpora and “Web 1T 5-gram Corpus” or “Chinese Web 5gram”. From the statistics, we can observe the reason why
out-of-domain data perform worse in English corpora than
Chinese corpora. Distributions of words play an important
8
9

Efficiency Evaluation

Due to the increasingly growing data, eﬃciency is another
important issue we focused on in this paper. In this subsection, we compare the running time of our approach with
state-of-the-art systems. We note that the running time of
our approach composes of two steps: candidate selection
and post-processing. Candidate selection step can be further divided into two steps: sentence feature generation and
range searching. Because feature generation time for sentences are equal in diﬀerent methods and the calculation
consumption is small, we only evaluate the time of diﬀerent range searching methods. To evaluate the impact of the
number of selected candidates with diﬀerent hash codes, we
also evaluate the post-processing time.
Table 5 shows the running time of three diﬀerent range
searching methods. Brute force, which directly calculates
all the distances between query and reference sentences, is
implemented using a single thread CPU implementation and
GPU implementation. We also adopt PM-Tree10 [26] which
is an indexing technique for eﬃcient similarity searching.
From the results, we can observe that indexing technique can
improve searching eﬃciency. However, brute force method
with GPU implementation can even achieve more than 1500
times speedup. Further more, the brute force methods do
not need the index construction time. We think that the
high memory bandwidth of GPU and naturally parallel algorithm are the main reason of the success of GPU implementation. Figure 6 shows the running time of GPU based
candidate searching. We can observe that the processing
time along the y axis increases as a linear function of the
size of collection.
Figure 7 shows the post processing time at diﬀerent recall
level. We select two corpora “TIPSTER” and “SogouT 2.0”
to evaluate the time using diﬀerent hash codes. Figure 8
shows the post processing time with diﬀerent hash codes
generation methods. From these results, we can observe that
the proposed hash code generation method can beneﬁt the
execution time of post processing. The number of selected
candidates through diﬀerent hash code generation methods
impacts the execution time. Since our proposed method
can ﬁlter more negative candidates than other methods, the
running time are reduced.
7

Time (Seconds)

6
5
4
3
2
1
0
1

2

3

4

5

6

7

8

9

10

# of Reference Sentences (millions)

Figure 6: Total execution time of candidate selection implemented with GPU for 2,000 queries.
10

LDC Catalog No. LDC2006T13
LDC Catalog No. LDC2010T06

We use the source code provided by Tomás Skopal. We set
pivot to 20, and pageSize to 2048 in the experiments.

412

Table 5: Candidate selection time (seconds) for
0.1M 0.2M 0.3M
Brute force (CPU)
468
939
1406
PM-Tree (CPU)
152
297
433
Brute force (GPU) 0.39
0.44
0.49

2,000 queries using diﬀerent range searching methods.
0.4M 0.5M 0.6M 0.7M 0.8M 0.9M
1M
1873 2339 2807 3279 3749 4216 4685
569
704
852
979
1124 1258 1401
0.55
0.61
0.67
0.73
0.79
0.85
0.91

300

500

MD5

250

MD5

400

NER

200

NER

Time (S)

Time (S)

600

OPT

300
200
100

OPT

150
100
50

0

0
0

1

2

3

4

5

6

7

0

1

2

Bits Difference

3

4

5

6

7

Bits Difference

(a) TIPSTER

(b) SogouT 2.0

Figure 7: The post processing time with diﬀerent hash codes generation methods varies with recall level. The reuse threshold
θ is set to 0.8. Each corpus contains 1 million sentences.
14

120

12

MD5

NER

80

Time (S)

Time (S)

100

OPT

60

40

MD5

10

NER

8

OPT

6
4

20

2
0

0
1

2

3

4

5

6

7

8

9

1

10

2

3

4

5

6

7

8

9

10

# Sentences(millions)

# Sentences(millions)

(a) TIPSTER

(b) SogouT 2.0

Figure 8: The post processing time with diﬀerent hash codes generation methods varies with corpus size. The reuse threshold
θ is set to 0.8. The number of bits diﬀerence is set to 3.

5. CONCLUSIONS

ing Academic Discipline Project (B114), and “Chen Guang”
project supported by Shanghai Municipal Education Commission and Shanghai Education Development Foundation
(11CG05).

In this work, we propose a novel approach which improves
the eﬃciency and eﬀectiveness of content reuse detection in
two aspects. We introduce learning to hash method for generating hash codes of words/characters. We also propose
to use GPU implementation to speedup the range searching
task, which is the most time consuming part in candidate
selection step. We evaluate the proposed approach in six
diﬀerent kinds of documents collections. Experimental results show that our method can signiﬁcantly improve the
eﬃciency of content reuse detection and would not impact
the recall any more.

7.

REFERENCES

[1] J. Aberdeen, J. Burger, D. Day, L. Hirschman,
P. Robinson, and M. Vilain. Mitre: description of the
alembic system used for muc-6. In Proceedings of
MUC6, pages 141–155, Morristown, NJ, USA, 1995.
[2] R. J. Bayardo, Y. Ma, and R. Srikant. Scaling up all
pairs similarity search. In Proceedings of the 16th
international conference on World Wide Web, WWW
’07, pages 131–140, 2007.
[3] J. Bolz, I. Farmer, E. Grinspun, and P. Schröoder.
Sparse matrix solvers on the gpu: conjugate gradients
and multigrid. ACM Trans. Graph., 22:917–924, July
2003.

6. ACKNOWLEDGEMENT
The author wishes to thank the anonymous reviewers for
their helpful comments. This work was partially funded
by 973 Program (2010CB327900), National Natural Science
Foundation of China (61003092, 61073069), Shanghai Lead-

413

[4] A. Z. Broder. On the resemblance and containment of
documents. In Proceedings of SEQUENCES 1997,
page 21, Washington, DC, USA, 1997. IEEE
Computer Society.
[5] A. Z. Broder. Identifying and ﬁltering near-duplicate
documents. In Proceedings of COM 2000, pages 1–10,
London, UK, 2000. Springer-Verlag.
[6] B. Bustos, O. Deussen, S. Hiller, and D. Keim. A
graphics hardware accelerated algorithm for nearest
neighbor search. In V. Alexandrov, G. van Albada,
P. Sloot, and J. Dongarra, editors, Computational
Science ĺC ICCS 2006, volume 3994 of Lecture Notes
in Computer Science, pages 196–199. Springer Berlin /
Heidelberg, 2006.
[7] A. Chowdhury, O. Frieder, D. Grossman, and M. C.
McCabe. Collection statistics for fast duplicate
document detection. ACM Trans. Inf. Syst.,
20(2):171–191, 2002.
[8] L. Dugan. 230 million tweets per day, 50 million daily
users and other twitter stats.
WWW.MEDIABISTRO.COM, 2011.
[9] S. Edelkamp, D. Sulewski, and C. Yĺźcel. Perfect
hashing for state space exploration on the gpu. In
ICAPS, pages 57–64. AAAI, 2010.
[10] K. Fatahalian, J. Sugerman, and P. Hanrahan.
Understanding the eﬃciency of gpu algorithms for
matrix-matrix multiplication. In Proceedings of the
ACM SIGGRAPH/EUROGRAPHICS conference on
Graphics hardware, HWWS ’04, pages 133–137, 2004.
[11] A. Gionis, P. Indyk, and R. Motwani. Similarity
search in high dimensions via hashing. In VLDB ’99,
pages 518–529, San Francisco, CA, USA, 1999.
Morgan Kaufmann Publishers Inc.
[12] N. K. Govindaraju, M. Henson, M. C. Lin, and
D. Manocha. Interactive visibility ordering and
transparency computations among geometric
primitives in complex environments. In Proceedings of
the 2005 symposium on Interactive 3D graphics and
games, I3D ’05, pages 49–56, 2005.
[13] G. Hinton and R. Salakhutdinov. Reducing the
dimensionality of data with neural networks. Science,
313(5786):504 – 507, 2006.
[14] G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast
learning algorithm for deep belief nets. Neural
Comput., 18:1527–1554, July 2006.
[15] J. W. Kim, K. S. Candan, and J. Tatemura. Eﬃcient
overlap and content reuse detection in blogs and
online news articles. In Proceedings of the 18th
international conference on World wide web, WWW
’09, pages 81–90, 2009.
[16] A. Kolcz, A. Chowdhury, and J. Alspector. Improved
robustness of signature-based near-replica detection
via lexicon randomization. In Proceedings of SIGKDD
2004, pages 605–610, 2004.
[17] J. Krüger and R. Westermann. Linear algebra
operators for gpu implementation of numerical
algorithms. In ACM SIGGRAPH 2005 Courses,
SIGGRAPH ’05, 2005.
[18] D. Metzler, Y. Bernstein, W. B. Croft, A. Moﬀat, and
J. Zobel. Similarity measures for tracking information
ﬂow. In Proceedings of the 14th ACM international

[19]

[20]

[21]

[22]
[23]
[24]

[25]

[26]

[27]

[28]

[29]

[30]

[31]

[32]

[33]

414

conference on Information and knowledge
management, pages 517–524, 2005.
K. Muthmann, W. M. Barczyński, F. Brauer, and
A. Löser. Near-duplicate detection for web-forums. In
IDEAS ’09, pages 142–151, New York, NY, USA,
2009. ACM.
M. Norouzi and D. Fleet. Minimal loss hashing for
compact binary codes. In L. Getoor and T. Scheﬀer,
editors, Proceedings of the 28th International
Conference on Machine Learning (ICML-11), pages
353–360, June 2011.
J. D. Owens, M. Houston, D. Luebke, S. Green, J. E.
Stone, and J. C. Phillips. GPU Computing.
Proceedings of the IEEE, 96(5):879–899, 2008.
R. Rivest. The md5 message-digest algorithm, 1992.
J. Seo and W. B. Croft. Local text reuse detection. In
SIGIR ’08, pages 571–578, New York, NY, USA, 2008.
N. Shivakumar and H. Garcia-Molina. Scam: A copy
detection mechanism for digital documents. In Digitial
Library, 1995.
N. Shivakumar and H. Garcia-Molina. Finding
near-replicas of documents and servers on the web. In
Proceedings of WebDB 1998, pages 204–212, London,
UK, 1999. Springer-Verlag.
T. Skopal, J. Pokorný, and V. Snásel. Pm-tree:
Pivoting metric tree for similarity search in
multimedia databases. In ADBIS (Local Proceedings),
2004.
B. Stein. Principles of hash-based text retrieval. In
Proceedings of the 30th annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ’07, pages 527–534, 2007.
M. Theobald, J. Siddharth, and A. Paepcke. Spotsigs:
robust and eﬃcient near duplicate detection in large
web collections. In SIGIR ’08, pages 563–570, New
York, NY, USA, 2008. ACM.
A. Wächter and L. T. Biegler. On the implementation
of an interior-point ﬁlter line-search algorithm for
large-scale nonlinear programming. Mathematical
Programming, 106(1):25–57, Mar. 2006.
Y. Weiss, A. Torralba, and R. Fergus. Spectral
hashing. In D. Koller, D. Schuurmans, Y. Bengio, and
L. Bottou, editors, Advances in Neural Information
Processing Systems, pages 1753–1760. MIT Press,
2008.
C. Xiao, W. Wang, X. Lin, and J. X. Yu. Eﬃcient
similarity joins for near duplicate detection. In
Proceeding of the 17th international conference on
World Wide Web, WWW ’08, pages 131–140, 2008.
D. Zhang, J. Wang, D. Cai, and J. Lu. Self-taught
hashing for fast similarity search. In Proceeding of the
33rd international ACM SIGIR conference on
Research and development in information retrieval,
SIGIR ’10, pages 18–25, 2010.
Q. Zhang, Y. Zhang, H. Yu, and X. Huang. Eﬃcient
partial-duplicate detection based on sequence
matching. In Proceeding of the 33rd international
ACM SIGIR conference on Research and development
in information retrieval, SIGIR ’10, pages 675–682,
2010.

