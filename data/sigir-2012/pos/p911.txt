Improving Retrieval of Short Texts Through Document
Expansion
Miles Efron

Peter Organisciak

Katrina Fenlon

University of Illinois at
Urbana-Champaign
501 E. Daniel Street, MC-492
Champaign, IL

University of Illinois at
Urbana-Champaign
501 E. Daniel Street, MC-492
Champaign, IL

University of Illinois at
Urbana-Champaign
501 E. Daniel Street, MC-492
Champaign, IL

mefron@illinois.edu

organis2@illinois.edu

kfenlon2@illinois.edu

ABSTRACT

1.

Collections containing a large number of short documents
are becoming increasingly common. As these collections
grow in number and size, providing effective retrieval of brief
texts presents a significant research problem. We propose a
novel approach to improving information retrieval (IR) for
short texts based on aggressive document expansion. Starting from the hypothesis that short documents tend to be
about a single topic, we submit documents as pseudo-queries
and analyze the results to learn about the documents themselves. Document expansion helps in this context because
short documents yield little in the way of term frequency
information. However, as we show, the proposed technique
helps us model not only lexical properties, but also temporal properties of documents. We present experimental
results using a corpus of microblog (Twitter) data and a
corpus of metadata records from a federated digital library.
With respect to established baselines, results of these experiments show that applying our proposed document expansion method yields significant improvements in effectiveness.
Specifically, our method improves the lexical representation
of documents and the ability to let time influence retrieval.

Collections of short documents present a host of challenges
to information retrieval (IR) systems. The increasing influence of microblogging platforms such as Twitter1 makes this
challenge especially timely. Twitter search exists alongside
other short-text IR problems such as ranking product reviews and advertisement placement. These short documents
join more familiar brief texts such as bibliographic and other
metadata records.
Faced with a corpus of millions of documents, each of
which contains only a few words, traditional IR models run
into difficulty. First, the vocabulary mismatch problem becomes especially worrisome. If documents are very brief,
the risk of query terms failing to match words observed in
relevant documents is large. Second, most IR models rely
on some sort of TF-IDF dynamic, with a term’s frequency
in a document lending evidence to our belief about the document’s relevance. In very short documents, most terms
occur only once, making simple operations such as language
model estimation difficult.
However, we can mitigate these problems. We argue that
a massive document expansion improves retrieval effectiveness for short texts. The mechanism that we propose for
this expansion is simple. Because the documents that we
are concerned with are so brief, each one tends to focus on
only a single topic. A topically homogenous document is
not very different from a query. Thus we propose submitting each document in a corpus as a pseudo-query. We show
that analyzing the results obtained from this pseudo-query
improves the models that underpin IR.
This paper proposes and tests two types of document expansion. Given a corpus of N documents C = D1 . . . DN
and a document of interest D, we augment D’s representation in the index by submitting the text of D as a query to
a search engine over C. This gives a ranked listed of results
R1 . . . Rk . This result set informs two expansions:

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval; H.3.7 [Digital Libraries]: Systems
Issues

General Terms
Algorithms, Experimentation, Performance

Keywords
Information retrieval, microblogs, twitter, Dublin Core, document expansion, language models, temporal IR

INTRODUCTION

1. Lexical: We use the terms in R1 . . . Rk to improve our
estimate of the language model for D.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
Copyright 2012 ACM 978-1-4503-1472-5/12/08... $15.00.

2. Temporal: We use the timestamps associated with
R1 . . . Rk to build a “temporal profile” for D–a probability distribution over time. This probability distribution is helpful during time-sensitive document ranking.
Our chief contribution is a novel approach to representing
collections of short text. Our methods elaborate on re1

911

http://twitter.com

-7.5

Non

The growing quantity of information published in small
texts such as Twitter posts argues for a sustained analysis
of the treatment of brief documents in IR. While this urgency is new, the need for improved retrieval of short texts
is not. For instance, many digital libraries manage repositories of terse metadata records. While these repositories
(such as the DCC repository we describe in this paper) are
nominally searchable, the brevity of their metadata can frustrate effective IR. Other examples of short-text IR include
query-specific advertisement and product review ranking.
But retrieval from collections of short documents is difficult for several reasons. First, brief documents attenuate
one of the primary signals used by modern IR systems–term
frequency. Figure 1 can help us understand why this is the
case. The figure shows data from two TREC collections (described in detail in Section 5). The TREC 8 data included
a corpus of news articles. Tweets2011 is the collection of
Twitter data used for the microblog track at TREC 2011.
Figure 1 shows the distributions of the log-probabilities of
query terms among the first 100 documents retrieved using
a simple query likelihood model. That is, the points that
comprise the distributions are the maximum likelihood estimates of log P (q|D) for each query term q.
Two problematic facts are clear from Figure 1. First,
unlike the longer news documents, tweets lead to a distribution of query term probabilities that is strongly peaked.
The majority of tweets that contain a query word only contain it once, and tweets are of similar length. Second, in
the case of the TREC 8 data, the mean and median logprobability of a query term are higher in relevant documents
than in non-relevant documents. This is not the case for
the Twitter data. Median log P (q|D) for relevant TREC 8
documents is -6.92, with median -7.17 for non-relevant documents. The difference in log-probability in relevant versus
non-relevant TREC8 documents is statistically significant
(Mann-Whitney p  0.001 one-sided). But log P (q|D) for
relevant Twitter documents has median -7.82 and -7.80 for
non-relevant (Mann-Whitney p = 1 one-sided). To the extent that TREC 8 presents a “typical” statistical picture,
Figure 1 shows that Twitter data are qualitatively different
than more familiar TREC collections.
The impact of this difference is easy to see if we consider
the language modeling approach to IR [21]. In the language
modeling approach, we assume that each document in our
collection was generated by a probability distribution–a language model–over terms in the vocabulary. We rank documents against a query Q by the likelihood that their corresponding language models generated the query:

-6.5

-6

log p(w|D)

-4

PROBLEM STATEMENT AND MOTIVATION

P (D|Q) ∝ P (Q|D)P (D).

-8
-10

log p(w|D)

2.

Tweets2001

-5.5

TREC 8

sults from the well-known language modeling approach to
IR, especially the notion of relevance models. But we also
show that the single-topic focus of most short documents
opens avenues for consideration of information other than
language–for example, time.

Rel

Non

Rel

Figure 1: log-Probabilities of Query Terms in Relevant and Non-Relevant Documents in Two Corpora.

If we assume uniform priors over documents and term independence we have:
P (Q|D) =

|Q|
Y

P (wi |D)

(2)

i=1

where |Q| is the number of word tokens in the query.
Key to language modeling is the estimation of P (w|D).
Using multinomial language models, the maximum likelihood estimator Pml (w|D) = n(w,D)
. But when confronted
|D|
with a corpus of brief documents, estimating language models is difficult because the maximum likelihood estimator is
not very expressive. Since |D| is small, n(w, D) cannot be
large and is very often 1. We refer to this as the “headroom
problem:” in brief documents there is little chance for important terms to stand out via repeated usage. Coupled with
a small domain of observed document lengths, the headroom problem has the effect of making many documents’
estimated language models look nearly identical.
To improve our estimates, we typically smooth language
models, re-allocating probability mass away from the maximum likelihood estimator by reference to a background
model, such as P (w|C), the language model of the collection as a whole as described by Zhai and Lafferty in [28].
Though several smoothing techniques are common in the
IR literature, in this paper we select Bayesian updating with
Dirichlet priors. The form of a Dirichlet-smoothed language
model is:
P (w|D) =

|D|
µ
Pml (w|D) +
P (w|C)
|D| + µ
|D| + µ

(3)

for µ ≥ 0. Smoothing in this way improves our estimated
probabilities. But smoothing does little to alleviate the
headroom problem.
Our goal is to improve the representation of short documents in IR. We argue that short text representation can be
improved by an aggressive document expansion. In Section
4.1, we use this expansion to induce improved document language models. However, document expansion has a role to
play in IR for short texts that goes beyond language model
estimation. As an example of an alternative use of expansion information, Section 4.2 extends our results to allow
temporal factors to influence retrieval.

(1)

912

3.

RELATED WORK

of each document in the collection, P (D|D1 ), . . . , P (D|DN ).
We propose analyzing these probabilities to make an augmented representation D0 that provides a better basis for
estimation and prediction than the original D affords.
From a practical standpoint, this amounts to submitting
each document as a pseudo-query and ranking all documents
using the standard query likelihood method. As we shall
show, the analysis of P (D|D1 ), . . . , P (D|DN ) can play a role
similar to relevance feedback, though how literally this comparison holds is variable.
With this in mind, we offer two definitions related to a
document D:

The problematic nature of IR on short documents has
seen little sustained research (though [22, 27] do treat the
topic explicitly). However, recent interest in social media
has drawn attention to this problem [22, 10]. Familiar operations such as measuring inter-document similarity, scholars
have found, is difficult given the brevity of many documents
in collections of user-generated content [20].
Though corpora of brief documents are increasingly common, the estimation problems that our proposed expansion
methods address are not new. Most similar to our own work
are results from research on cluster-based IR. Clusters have
been applied to various points in the IR process, including
relevance feedback [14, 9], rank fusion [12], and as a separate factor used during document ranking [13]. Clusters are
appealing insofar as they afford a level of information about
documents that resides at a level higher than intra-document
word counts, but below the generalities of the collection at
large. For instance, Liu and Croft applied clusters during
document language model smoothing [18]. Ramage, Dumais
and Leibling address the vocabulary mismatch problem in
microblog text using latent Dirichlet allocation (LDA) [23],
allowing the LDA model to supplement observed term probabilities in document representation. Other methods of document expansion have also seen sustained work (e.g. [24]),
though in contexts different from those that we study here.
The most similar work to ours was given by Tao et al.
[26]. In their work, Tao et al. proposed smoothing document
language models by analyzing their lexical neighborhoods.
That is, the model for document D was smoothed with
counts obtained from its k nearest neighbors D1 , . . . , Dk ,
with each document’s influence in the smoothed model being proportional to its cosine similarity with D. Like Tao
et al. we propose improving the representation D by an
analysis of similar documents. However, the approach we
outline in Section 4.1 differs from Tao et al. in its basic theoretical orientation, couching the estimation problem in the
generative semantics of language modeling. Tao et al. define
the neighborhood of a document in geometrical terms, relying on the cosine similarity. With an explicit assumption of
normality, they smooth the model of D based on this neighborhood and this metric. On the other hand, we assume
that D arises from an unseen model D, much as relevance
modeling assumes the influence of an unseen model of relevance. To estimate P (w|D) we combine evidence from other
documents, where the influence that some document Dj exerts on the final model is the likelihood that Dj ’s language
model generated D.
Our work also relies on findings from recent studies on
information retrieval and microblogs [6, 19, 2]. The field of
microblog retrieval is relatively new, but has seen increasing
interest, most noticeably in the 2011 TREC microblog track.
Unlike the methods described in this section, our document expansion technique also invites extension to nonlinguistic features. As we show in Section 4.2, aspects of relevance such as temporality fit naturally into our approach.

4.

Definition 1. Pseudo-query of D: The pseudoquery QD is a representation of the text in D that
we may submit to an IR system.
Definition 2. Result set of D: By submitting
QD as a query against the corpus C, we derive
a ranking RD = RD1 , . . . , RDk . RD consists of
the top k documents retrieved for QD and their
retrieval status value scores. In this paper we
use query likelihood for retrieval, so the scores
are the probabilities P (D|D1 ), . . . , P (D|Dk ).
The use of QD and RD stems from our hypothesis that
most short texts discuss only a single topic. If QD treats
a coherent topical domain, we anticipate that analyzing the
documents in RD will yield information about the topic that
the document of interest D is about.

4.1

Lexical Evidence

Following Lavrenko and Croft’s exposition of relevancebased language models [16], we use RD to induce D0 , a language model associated with D. In typical language modeling, we estimate a document’s language model by combining
evidence from the document itself with information from a
background distribution such as the collection. Given RD ,
however, we can improve this estimate.
The language model D0 is
P (w|D0 )

=

P (w|d1 , . . . , d|D| )

=

P (w, d1 , . . . , d|D| )
.
P (d1 , . . . , d|D| )

(4)

The denominator P (d1 , . . . , d|D| ) does not depend on w,
leaving the joint probability as the quantity of interest. This
joint probability is
X
P (w, d1 , . . . , d|D| ) =
P (Di )P (w, d1 , . . . , d|D| |Di ). (5)
Di ∈C

If we further assume that
P (w, d1 , . . . , d|D| |Di ) = P (w|Di )

|D|
Y

P (dj |Di )

(6)

j=1

then we have
P (w, d1 , . . . , d|D| ) =

DOCUMENT EXPANSION FOR SHORT
TEXTS

X
Di ∈C

P (Di )P (w|Di )

|D|
Y

P (dj |Di ).

j=1

(7)
Let D be a document consisting of |D| word tokens d1 , . . . , d|D| . The last factor in Eq. 7 reminds us that most of the probability mass in this joint distribution will derive from docAlso let C be a corpus of N documents. Using to Eq. 2, we
uments that are lexically similar to the document D. Thus
can calculate the likelihood of D given the language model

913

we can obtain a good estimate of P (w|D0 ) by performing
the summation in Eq. 7 over only the k documents with
the highest likelihood of generating D. Thus the probability of word w under the augmented model is a weighted
average of the observed probabilities of w in the top k documents retrieved by submitting D as a pseudo-query, where
the weights are the likelihoods of D given the retrieved documents’ language models.
Having obtained our augmented representation, we may
then rank documents by the likelihood that their augmented
language models generated the query, P (Q|D0 ) which we
calculate using the query likelihood model as usual, substituting P (w|D0 ) for the maximum likelihood estimator into
Eq. 3. We refer to this method by the abbreviation LExp,
for lexical expansion.
As is common when using relevance models for relevance
feedback, interpolating the expanded model with the originally observed text is likely to be “safer” than relying on
the expanded model alone. Thus we define another lexically
expanded model:

the documents Ri in RD contains a timestamp ti . From
the empirical distribution of t1 , . . . , tk we may estimate the
underlying distribution P (t|D).
To estimate P (t|D) we imagine the following generative
process. A person interested in D samples documents according to P (D1 |D), . . . , P (DN |D). If this choice yields document Di with timestamp ti , we choose time t with probability P (t|ti ). We let this probability follow an exponential
distribution on |ti − t|.
The probability of choosing document Di in our generative process is simply the likelihood of D given Di . To derive
a proper density, we normalize the k likelihoods in RD to
sum to one:
P (D|Di )P (Di )
.
si = P (Di |D) = P
j∈RD P (D|Dj )P (Dj )

Pλ (w|D0 ) = (1 − λ)Pml (w|D) + λP (w|D0 )

The exponential rate parameter r governs the temporal influence; a large r strongly favors times near ti . It is worth
noting that Eq. 9 is similar to a weighted kernel density estimate [25]. It allocates probability mass mostly around the
timestamps of those documents that have a high likelihood
of having generated D.
For each document and query we observe lexical evidence
(i.e. their textual content) and temporal evidence. Let
WQ and WD be the lexical representations of the query
and a document, respectively, and TQ and TD be vectors
of timestamps retrieved by submitting Q as a query or D
as a pseudo-query. It is intuitive to rank on the joint probability P (WQ , WD , TQ , TD ). If we assume conditional independence between lexical and temporal information and a
uniform distribution over temporal profiles we have:

We then have:
P̂ (t|D) =

si · r · e−r·|ti −t| .

(9)

ti ∈RD

(8)

for a parameter λ in [0, 1]. We may then substitute Pλ (w|D0 )
for the maximum likelihood estimator in Eq. 3 for retrieval.
We refer to rankings based on this estimator as LExpλ. For
simplicity, when discussing LExpλ we set λ = 0.5 throughout this paper.

4.2

X

Temporal Evidence

In addition to supplementing the lexical evidence that
we store about documents, the expansion method described
above can create new, extra-lexical features. For example,
information in RD yields actionable information related to
temporal aspects of relevance. Indeed, incorporating temporal evidence into IR entails a research area in its own
right (cf. [1]). We pursue temporality here as an example
of extra-linguistic information that our document expansion
method allows.
We assume that for each document Di , we have a corresponding timestamp ti which is the time at which the
document was published. We also define t0 as the earliest
timestamp in the collection. In this paper we measure time
in fractions of days, such that ti is how many days elapsed
between the initial time t0 and the publication of Di .
The observed timestamp ti provides useful information for
IR. But our goal is to learn additional temporal information
about each document Di . We hypothesize that the empirical distribution of timestamps related to Di via document
expansion helpfully supplements direct use of ti .
Working under a similar scenario, Jones and Diaz defined
the notion of a query’s “temporal profile” [11, 5]. The temporal profile of a query Q is a probability distribution over
time, P (t|Q). Analyzing the empirical distribution of documents retrieved for Q gives information helpful in estimating
P (t|Q), as shown in [4].
Our discussion above suggests that temporal profiles may
be defined not only on queries, but also on documents. For
each document D, we define a temporal profile P (t|D). This
probability distribution expresses the extent to which D is
associated with events that were discussed at various moments in time. Estimating P (t|D) follows the same logic
that we use for query temporal profiles; we submit the text
of D as a query, yielding RD = RD1 , . . . , RDk . Each of

P (WQ , WD , TQ , TD ) ∝ P (WQ |WD ) · P (TQ |TD ).

(10)

This is simply the query likelihood multiplied by the likelihood that the timestamps returned by Q were generated
by the same distribution that generated the timestamps observed in the result set of the pseudo-query for D. Using
our estimate of the temporal profile of D, the likelihood of
TQ given D is:
P (TQ |TD ) =

kQ
Y

P (tQi |D)

(11)

i=1

where kQ is the number of timestamps in the query’s temporal profile; we set this to kQ = 10. We then obtain a
temporally informed retrieval by multiplying Eqs. 2 and 11.
In the following discussion, we refer to this model as TExp
(temporal expansion). Runs labeled TExp use documents’
unexpanded text for WD . We will compare this method to
a baseline using temporal document priors described by Li
and Croft [17] (which we call TPrior). TPrior uses an exponential distribution on the age of documents as the prior
in Eq. 2. Both methods–TExp and TPrior–rely on a rate
parameter r for the exponential. Following Li and Croft,
unless otherwise specified we set r = 0.01.

5.

EXPERIMENTAL DATA

In the discussion that follows we rely mainly on two data
sets. First, we use Tweets2011, the collection used for the

914

TREC 2011 microblog task. Second, we use a collection
of metadata records describing holdings in a large digital library. Both of these collections consist mostly of brief documents, though as we shall show, they have many differences.

5.1

relevant documents (as gauged by the following process) in
the collection were omitted.
Initial assessments of query suitability for our experiments
were made by one of the authors, who is a former DCC
administrator with significant experience in collection evaluation and user interactions. When choosing queries for
inclusion on our test collection, this judge assessed potential relevance generously; if there seemed to be a reasonable
chance that a document could satisfy a query, the document
was considered potentially relevant. If a query accumulated
at least five plausibly relevant documents, the query was included. Redundant queries and queries submitted by DCC
administrators were also culled from the sample. Altogether
we retained 53 queries.
We completed this collection by soliciting relevance judgments via Amazon’s Mechanical Turk4 (AMT) service. To
accomplish this, the same author wrote relevance criteria for
each query. The process of writing relevance criteria is of
course subjective and entails obvious limitations. However,
without explicit knowledge of the information needs of people who created the queries, the author wrote these criteria
to enhance the AMT workers’ understanding of the queries
and results without unduly limiting or biasing their sense of
relevance. For example, relevance criteria for the query USS
Monitor were given as, “Highly relevant documents should
link to photographs or documentation about the Civil-Warera warship, the USS Monitor. Somewhat relevant documents will describe or provide photographic evidence of US
Navy ships from the same era.”
While the process of choosing and describing queries introduced subjectivity into our study, we pursued this method
because we felt that using queries from the DCC’s search
logs was more realistic than writing queries of our own.
Without attendant clickthrough data, this intervention was
necessary to give AMT judges sufficient information for relevance assessment.
Documents to be assessed via AMT were gathered from
pools created by running several retrievals over all 53 queries.
Pools contained results from five of the conditions discussed
in Section 6, as well as a run using BM25 ranking. All together, six runs contributed to the judging pools which were
held to depth of 100.
Judgments were reconciled with two-thirds majority voting. After the first run of AMT ratings, we found that agreement was low with Fleiss kappa=0.24. However, the majority AMT ratings yielded a kappa=0.724 when compared to
a set of oracle ratings by the study authors, suggesting that
the problem of low inter-rater agreement was not systematic but due to scattered unreliable workers. To address
this issue, we identified unreliable raters based on the proportion of each worker’s judgments that were in agreement
with the majority. Using an admittedly arbitrary agreement threshold of 0.67, we divided our workers into “good”
and ”bad” categories. 28 of 131 workers fell into the “bad”
category. We removed all work done by these workers and
re-submitted their tasks to AMT.
After the second judging round we recomputed reliability
scores, this time finding only four bad workers whose sum
proportion of ratings was 0.01. Only two workers who were
good in round one became bad during round two. After
round two, Fleiss kappa was 0.40. While this is lower than

Microblog Data

The Tweets2011 collection uses a corpus of posts (called
“tweets”) made available by the microblogging service Twitter. Instead of distributing the microblog corpus via physical
media or a direct download, TREC organizers distributed
approximately 16M unique tweet ID numbers. Users of the
collection downloaded these ID’s, along with software that
allowed them to fetch the tweets directly from Twitter. We
obtained our data using the supplied HTML scraping tools
on May 25–26, 2011. Our corpus contained 15,653,612 indexable tweets, each containing: the screen name of the author, the time at which the tweet was posted, and the tweet
text itself.
Our microblog data was preprocessed in several ways.
First, in conformance with the track’s guidelines all “retweets”
were removed by deleting documents containing the string
RT. Second, we made a simple pass at removing non-English
tweets. We deleted tweets containing more than four characters with byte values greater than 255. We also defined a list
of 133 words that are common in Spanish, French and German. Tweets containing any of these were removed. After
these operations, the corpus contained 8,320,421 documents.
The 2011 microblog track involved a real-time search problem. Track organizers created 50 test topics2 , each of which
contained query text Q and a timestamp tQ . Only documents posted prior to tQ were assessed for relevance (all
others are non-relevant). Aside from returning only documents published before query-time, for the sake of simplicity,
in this paper we drop additional real-time strictures defined
by the track organizers.

5.2

Digital Library Metadata Collection (IMLS
DCC)

The Institute for Museum and Library Studies Digital
Collections and Content (IMLS DCC) is a large, federated
digital library offering access to the aggregated holdings of
a broad spectrum of digital collections at distributed cultural heritage institutions3 . In collaboration with IMLS
DCC, we obtained a collection of 578,385 brief descriptions
of cultural heritage resources. IMLS DCC administrators
regularly harvest descriptions of several hundred participating institutions’ digital resources via the Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH) [15].
Each of the 578,385 harvested documents is a Dublin Core
metadata record describing a single item such as a photograph, manuscript, sound recording. Details of the process
by which this corpus was built are given in [8].
To enable IR experimentation, we sampled 53 queries
taken from the DCC search engine query logs. It must be
stressed that these were not chosen at random. We used care
in selecting queries to assure that no query returned zero
relevant documents. Because many highly specific queries
returned zero or very few items – making changes in IR
performance difficult to test – queries with fewer than five
2
NIST created relevance judgments using the standard
TREC pooling method. Because it lacked relevant documents, one topic was removed from the final task.
3
http://imlsdcc.grainger.uiuc.edu/

4

915

http://mturk.com

Table 1: Summary Statistics for Experimental Corpora. From left to right, columns give total number of
documents, observed word types, observed word tokens, median document length, mean document length,
standard deviation of document length, and the number of test queries.
Corpus
Tweets2011
DCC

Indexed Docs
8,320,421
578,385

Unique Terms
19,449,151
2,793,371

Tokens
6,506,465,256
114,453,512

Median Doc Len
20
45

Mean Doc Len
21.92
197.89

SD Doc Len
7.30
556.90

# Queries
49
53

we would like, it marks a significant improvement over the
initial work quality.

5.3

Table 2:
Baseline and Experimental Retrieval
Names and Parameters.

Data Statistics

Table 1 summarizes length-related aspects of our two test
data sets. Not surprisingly, documents in the microblog corpus are very short (median=20, compared to median=328 in
TREC 8), with a compressed distribution of lengths. Most
DCC documents are a bit longer than microblog posts (median=45), and their lengths vary more than tweet lengths
do. The corpora analyzed here are similar insofar as their
documents are much shorter than documents in more standard IR collections. But the collections are also different
from each other.

6.

Abbr.
QL
Baselines
FB

TPrior

EXPERIMENTAL ASSESSMENT

LExp

All experiments were done using the Indri search engine
and the Lemur toolkit5 . For efficiency, we used the standard Indri stoplist and a custom Twitter-specific stoplist
when forming document pseudo-queries. But subsequent
retrievals used no stoplists or stemming except to mitigate
common words’ influence in the baseline relevance feedback
condition–feedback models were stripped of words in the
Indri stoplist. Unless otherwise specified, all retrieval (both
queries and pseudo-queries) used the Indri defaults of Dirichlet smoothing with µ = 2500. For each Tweets2011 query we
retrieved 100 documents. We retrieved 50 documents per
DCC query. To assess the merit of our proposed expansion
methods we tested the conditions shown in Table 2.

6.1

Experimental

LExpλ

TExp
LTExp

TBoth

The Effect of Lexical Expansion

Tables 3 and 4 list four effectiveness metrics for the conditions described above, on both of our test data sets6 . The
clearest result from the tables is the improvement over the
QL baseline offered by our lexical expansion methods. Except for one case (P10 for LExp on DCC), the lexical expansion methods always outperform the QL baseline. The
positive effect of document expansion is especially strong
when we interpolated the expanded model with the observed
document model (i.e the TExpλ condition).
We can see the positive effect of document expansion in
Figure 2. Each panel in the figure schematizes the distribution of query terms in relevant and non-relevant documents,
over a variety of data sets. Besides our two short document
collections, as a point of reference we show the TREC 8 data
described above and the small WT10g web collection. As we
discussed earlier, the figure shows that query terms tend to

Details
Basic query likelihood. Dirichlet
smoothing, µ = 2500
Relevance feedback using relevance
models. 20 feedback docs; 15 terms.
Interpolation with original query
λ = 0.5
Temporal priors to promote recent
documents. Exponential rate parameter r = 0.01.
Lexical document expansion. k =
50 expansion documents.
Lexical document expansion with
linear interpolation of expansion
model with MLE. k = 50 expansion. Expansion-MLE mixing proportion λ = 0.5.
Temporal document expansion.
k = 50 expansion documents.
Both lexical and temporal document expansion. i.e. A combination of LExpλ and TExp.
Two types of temporal evidence are
used: the prior of TPrior and the
expansion method of TExp. No lexical expansion.

Table 3: Observed IR Effectiveness on TREC 2011
Microblog Data. The †symbol indicates p < 0.05
on a permutation test against the baseline QL. The
‡symbol indicates p < 0.01.
MAP
Rprec
NDCG P10
QL
0.187
0.275
0.360
0.398
FB
0.189
0.273
0.361
0.394
TPrior 0.198†
0.284†
0.372
0.427
LExp
0.216†
0.301†
0.404‡
0.380
LExpλ 0.226‡
0.319‡
0.415‡
0.431
TExp
0.204†
0.289
0.373
0.414
TBoth 0.206†
0.289†
0.378†
0.427†
LTexp 0.235‡ 0.324‡ 0.428‡ 0.451‡

5

http://lemurproject.org
Reported metrics are mean average precision (MAP), Rprecision (Rprec), NDCG over all positions (NDCG) and
precision at 10 (P10).
6

916

Figure 2: log Probabilities of Query Terms in Relevant and Non-Relevant Documents in Four Corpora.

Rel

Non

Rel

Rel

0
-10

-8

-6

log p(w|D')

-4

-2

0
-6

log p(w|D)

-8
-10
Non

Table 4: Observed IR Effectiveness on DCC Data.
The †symbol indicates p < 0.05 on a permutation
test against the baseline QL. The ‡symbol indicates
p < 0.01.
MAP
Rprec
NDCG P10
QL
0.215
0.287
0.398
0.329
FB
0.205
0.256
0.366
0.300
LExp
0.227
0.292
0.414
0.304
LExpλ 0.302‡ 0.359‡ 0.502‡ 0.402‡

-4

-2

0
-10

-8

-6

log p(w|D')

-4

-2

0
-2
-4

log p(w|D)

-8
-10
Non

DCC (Exp)

Non

Rel

Non

Rel

Mean Avg. Precision

0.30

Figure 3: Mean Average Precision Observed Over
Varied Values of k. The x -axis is the number of
documents used to fit the expansion model D’.

0.18

have a higher probability in relevant documents than in nonrelevant documents. The difference is especially stark for the
standard (TREC 8 and WT10g) corpora. Observed query
term probabilities in the unexpanded microblog corpus give
no consistent evidence of relevance (among retrieved documents). The value of query term probabilities in the unexpanded DCC data is higher than in the Twitter data, but
less so than in TREC 8 and WT10g. However, the panels
labeled Microblog (Exp) and DCC(Exp) show query term
probabilities in expanded document models. In these cases
the estimated probability of query terms in relevant documents is significantly greater than the corresponding probability in non-relevant documents (Mann-Whitney p  0.001
one-sided).
A surprising result of our experiments is the poor performance of feedback using relevance models. Inspection of
the expanded queries that led to these results showed that
the induced query models contained very idiosyncratic terms
such as user names (Twitter) and administrative vocabulary
(DCC). We include the feedback results because it is worth
noting that the expanded document models are capturing semantics that expanded query models cannot, given the conditions encountered in these data. We hypothesize that this
effect arises because, in addition to alleviating the vocabulary mismatch problem, our expansion method yields more
data for estimating language models. Expanded queries alleviate vocabulary mismatch and improve estimates of the
query model, but expanded documents improve our estimates of the document models. In the context of short document retrieval, this effect seems to be crucial.
Figure 3 shows performance over a range of values (from
5 to 50) for k, the number of expansion documents used
to fit an augmented language model D0 (cf. Eq. 7). Con-

Twitter, LExp
Twitter, LExpLambda
DCC LExp
DCC LExpLambda

0.26

Rel

DCC

0.22

Non

Microblog (Exp)

-6

-2
-4

log p(w|D)

-10

-8

-6

-2
-4
-6
-10

-8

log p(w|D)

Microblog

0

WT10g

0

TREC 8

10

20

30

40

50

Number of Expansion Docs.

trary to our expectations, there appears to be little risk in
adding many documents to an expanded model. While a
small decline in performance is visible for the LExp condition on the DCC data after k = 35, when we interpolate
with the original model (LExpλ), the performance does not
decrease over the range from 5–50 documents. We suspect
that this is due to the very small probabilities associated
with documents deep in the retrieved set for D.
Our reliance on the retrieved sets of documents’ pseudoqueries raises the issue of language model smoothing. How
aggressively should we smooth when performing IR for document pseudo-queries? Table 5 shows that during document
pseudo-query retrieval, previous studies of smoothing’s critical role apply [28]. The table shows retrieval effectiveness
statistics at three settings of the Dirichlet smoothing parameter µ during retrieval with document pseudo-queries.
Our experiments use µ = 2500. This setting is the default
in the Indri search engine (we chose it because it obviated
the need for training data) and it has been shown to be effective. Table 5 shows that in this choice we were lucky.
It is clear that 250 leads to under-smoothing, while 5000

917

Table 5: Comparison of Retrieval Effectiveness with Different Selections of µ,
rameter, Applied During Retrieval Based on Document Pseudo-Queries.
Tweets2011
DCC
Metric
µ = 250 µ = 2500 µ = 5000 µ = 250 µ = 2500
REL RET 858
1032
796
441
661
MAP
0.1810
0.2155
0.1792
0.1702
0.2269
Rprec
0.2764
0.3008
0.2584
0.2255
0.2919
NDCG
0.3452
0.4034
0.3454
0.3310
0.4136
P10
0.3735
0.3796
0.3694
0.2264
0.3038
P30
0.3014
0.3190
0.2871
0.1987
0.2994

6.2

µ = 5000
633
0.1990
0.2526
0.3787
0.2906
0.2780

0.05
-0.15

-0.05

TExp - TPrior

0.15

is too high. All declines in MAP, Rprec, and NDCG from
µ = 2500 are statistically significant. This suggests that
document expansion carries some risk. We hypothesize that
much of this risk comes from the conjunctive and disjunctive
semantics of aggressive smoothing. Weak smoothing pushes
retrieval towards a Boolean AND’ing of query terms, while
strong smoothing allows the predominance of only a few
query terms to promote a document. Balancing this economy is clearly important. However, it is also the case that
Table 5 shows very extreme values for µ. Because computational constraints kept us from performing a full parameter
sweep, we chose to test very high and very low smoothing
parameters. In future work we plan to examine the “safe”
region for smoothing in more depth.

the Dirichlet Smoothing Pa-

1

3

5

7

9 11

14

17

20

23

26

29

32

35

38

41

44

47

Figure 4: Comparison of Two Methods of Temporal Influence. Each bar is the difference in Mean
Average Precision between TExp and TPrior (i.e.
MAPT Exp −MAPT P rior on one the the 49 Tweets2011
Queries.

The Effect of Temporal Expansion

Of our two data sets only Tweets2011 has a temporal component; the DCC documents lack any measurable chronology. Thus we tested the temporal expansion described in
Section 4.2 only on the Twitter data. As a baseline we
compared our method against the use of temporal priors
introduced by Li and Croft. Effectiveness results for each
condition are shown in Table 3. Both the baseline method
(TPrior) and our temporal expansion (TExp) gave statistically significant improvements over the non-temporally informed QL baseline. However, the difference in effect between TPrior and TExp are small in Table 3, with both
methods scoring nearly identically (the differences between
them are not statistically significant).
Combining lexical and temporal expansion improves effectiveness further. in Table 3 the method using both expansion sources, LTExp, outperforms all other runs. For
completeness, we compared LTExp against a run using the
LExpλ condition modified with a temporal prior. LTExp
improved on this baseline for all metrics shown in Table 3,
with p < 0.05 for MAP and P10.
An interesting result is visible in Figure 4. The bar plot
shows the difference in mean average precision (between
TExp and TPrior) on a query-by-query basis. Though a
few queries have near-identical scores under both TPrior
and TExp, the majority of queries fare differently under
each method. Both the baseline and proposed methods improve aggregate effectiveness on these data. But they appear
to do so quite differently, suggesting that they are not interchangeable. The run in Table 3 labeled TBoth uses both
temporal priors and temporal expansion. Though its performance is similar to temporal expansion alone, TBoth does
see consistent improvement over TExp (p is 0.83, 0.12, 0.01,
0.15 for MAP, Rprec, NDCG and P10, respectively), sug-

gesting that the prior introduces information distinct from
temporal expansion.
In earlier work [7] it was shown that choosing the rate
parameter r for the exponential distribution when applying
temporal priors has a strong effect on retrieval. However,
we found that a broad range of exponential parameterizations yielded little difference in performance from the data
reported in Table 3. The same was true for the parameter in
our temporal expansion; a broad sweep yielded no statistically significant change. We hypothesize that this robustness
is due to the short temporal span of the Tweets2011 corpus
(two weeks). If we repeated our experiments on collections
spanning months or years, we anticipate seeing more sensitivity to setting r in both the baseline approach and our
own.

7.

DISCUSSION

In our previous exposition we treated temporal and lexical
expansion as two distinct operations. But the same rationale
and the same mechanism underpin both operations. In both
cases, information gleaned from a document’s result set is
used to derive new information about that document. The
influence of each member of the result set is proportional to
P (D|Di ). Thus in addition to lexical and temporal expansion, we could apply this operation on arbitrary features of
documents. For instance, if each document had geolocation
information, we could use the methods presented here to
make an expanded location profile. Our main contribution
is a way to create, for each document, an empirical distribu-

918

• Other settings. Short documents arise in many IR
applications. Searching for relevant advertisements
or product reviews are both domains that we believe
could benefit from the approaches outlined here.

tion of features of whatever type. Our results suggest that
for the feature types explored here (lexical and temporal),
these distributions convey useful information for IR.
A notable advantage of the temporal expansion method
proposed here is its flexibility with respect to the semantics of a query. For instance, the temporal prior method
can account only for “recency queries” where the user seeks
new information. However, our TExp approach assesses the
similarity between the timestamps of documents retrieved
by Q and those obtained from the pseudo-query of a document D. If Q retrieves documents clustered around a window of time in the past, TExp will reward documents whose
pseudo-queries’ results occupy the same window.
One issue that we have not addressed in this paper is the
transformation of a document into a pseudo-query. In the
interest of simplicity, we omitted any transformation other
than removal of stopwords to improve retrieval speed. However, our results suggest room for improvement by a more
thoughtful approach. In particular, we consider the difference between LExp and LExpλ on the DCC data evocative
on this matter. Interpolation with the original document
model helped retrieval to a great extent in this case, a fact
emphasized by Figure 3. We suspect that this is due to
the length of some DCC documents. Many of these documents are more verbose than, for example, tweets. These
results suggest that longer documents do not lead to suitable
pseudo-queries without some improvement.

8.

• Other expansion data. In future work we hope to examine the value of expansion methods for information
such as geolocation data or user-created content.
• Scalable implementation. An efficient way to store and
use document expansion data is crucial for our methods’ viability in a production setting. In particular,
future work will address the matter of changing documents’ expansion data as the collection grows.
Despite these unexplored ideas, the results of this study
show that document expansion is a compelling method for
improving retrieval in corpora of short texts. As people’s
engagement with social media increases the pervasiveness
of abbreviated documents, this problem will grow in importance. Considering short documents as pseudo-queries
provides needed traction on this difficult task.

9.

ACKNOWLEDGEMENTS

This work was supported by a Google academic research
award and Institute of Museum and Library Services LG-0607-0020. Expressed opinions are those of the authors, not
the funding agencies. The project is hosted by the Center for
Informatics Research in Science and Scholarship (CIRSS).

CONCLUSION

We have proposed expanding document representations to
improve retrieval effectiveness on corpora of very brief texts.
Our contribution starts with the idea that short documents
often make productive pseudo-queries. Because brief documents tend to discuss at most one topic, the retrieved set for
a document D is informative with respect to making predictions about the relevance of D. We proposed two methods of
document expansion based on analysis of document pseudoqueries’ result sets. First, we augment the language model
of D, obtaining an expanded language model D0 . Second,
we induce a model of the temporal affinity of D by analyzing
the temporal profile of D’s result set. We find that the likelihood that this density generated the timestamps retrieved
by a query Q yields an effective mechanism for letting time
inform ranking.
In future work we plan to address several issues raised by
this study:

10.

REFERENCES

[1] Omar Alonso, Michael Gertz, and Ricardo
Baeza-Yates. On the value of temporal information in
information retrieval. SIGIR Forum, 41:35–41,
December 2007.
[2] Gianni Amati, Alessandro Celi, Cesidio Di Nicola,
Michele Flammini, and Daniela Pavone. Improved
stable retrieval in noisy collections. In Giambattista
Amati and Fabio Crestani, editors, Advances in
Information Retrieval Theory, volume 6931 of Lecture
Notes in Computer Science, pages 342–345. Springer
Berlin, Heidelberg, 2011.
[3] Adam Berger and John Lafferty. Information retrieval
as statistical translation. In SIGIR ’99: Proceedings of
the 22nd annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 222–229, New York, NY, USA, 1999. ACM.
[4] Wisam Dakka, Luis Gravano, and Panagiotis G.
Ipeirotis. Answering general time-sensitive queries.
IEEE Transactions on Knowledge and Data
Engineering, 24(2):220–235, 2012.
[5] Fernando Diaz and Rosie Jones. Using temporal
profiles of queries for precision prediction. In SIGIR
’04: Proceedings of the 27th annual international
ACM SIGIR conference on Research and development
in information retrieval, pages 18–24, New York, NY,
USA, 2004. ACM.
[6] Miles Efron. Information search and retrieval in
microblogs. Journal of the American Society for
Information Science and Technology, 62(6):996–1008,
2011.
[7] Miles Efron and Gene Golovchinsky. Estimation
methods for ranking recent information. In Proceedings

• Self-retrievability and document priors. We hypothesize that the extent to which D is distinguishable from
other documents in its result set is an indicator of topical coherence. This intuition could inform a prior over
documents.
• Use of longer documents. Though short documents
stand to gain significantly from expansion, there is no
reason the gains we have seen in this study would not
carry over to more standard collections.
• Query expansion vs. document expansion. Studying
more conventional corpora will help us assess the differences between query and document expansion more
thoroughly. We also plan to compare our approach
to other document expansion methods such as Berger
and Lafferty’s translation model [3].

919

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

of the 34th international ACM SIGIR conference on
Research and development in Information, SIGIR ’11,
pages 495–504, New York, NY, USA, 2011. ACM.
Miles Efron, Peter Organisciak, and Katrina Fenlon.
Building topic models in a federated digital library
through selective document exclusion. In Proceedings
of the Annual Meeting of the American Society for
Information Science and Technology, 2011.
Inna Gelfer Kalmanovich and Oren Kurland.
Cluster-based query expansion. In Proceedings of the
32nd international ACM SIGIR conference on
Research and development in information retrieval,
SIGIR ’09, pages 646–647, New York, NY, USA, 2009.
ACM.
Giacomo Inches, Mark Carman, and Fabio Crestani.
Investigating the statistical properties of
user-generated documents. In Henning Christiansen,
Guy De Tr, Adnan Yazici, Slawomir Zadrozny, Troels
Andreasen, and Henrik Larsen, editors, Flexible Query
Answering Systems, volume 7022 of Lecture Notes in
Computer Science, pages 198–209. Springer Berlin /
Heidelberg, 2011.
Rosie Jones and Fernando Diaz. Temporal profiles of
queries. ACM Transactions on Information Systems,
25(3):14, 2007.
Anna Khudyak Kozorovitsky and Oren Kurland.
Cluster-based fusion of retrieved lists. In Proceedings
of the 34th international ACM SIGIR conference on
Research and development in Information Retrieval,
SIGIR ’11, pages 893–902, New York, NY, USA, 2011.
ACM.
Oren Kurland and Lillian Lee. Clusters, language
models, and ad hoc information retrieval. ACM Trans.
Inf. Syst., 27:13:1–13:39, May 2009.
Oren Kurland, Lillian Lee, and Carmel Domshlak.
Better than the real thing?: iterative pseudo-query
processing using cluster-based language models. In
Proceedings of the 28th annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ’05, pages 19–26, New
York, NY, USA, 2005. ACM.
Carl Lagoze and Herbert Van de Sompel. The Open
Archives Initiative: building a low-barrier
interoperability framework. In JCDL ’01: Proceedings
of the 1st ACM/IEEE-CS joint conference on Digital
libraries, pages 54–62, New York, NY, USA, 2001.
ACM.
Victor Lavrenko and W. Bruce Croft. Relevance based
language models. In SIGIR ’01: Proceedings of the
24th annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 120–127, New York, NY, USA, 2001. ACM.
Xiaoyan Li and W. Bruce Croft. Time-based language
models. In CIKM ’03: Proceedings of the twelfth
international conference on Information and
knowledge management, pages 469–475, New York,
NY, USA, 2003. ACM.

[18] Xiaoyong Liu and W. Bruce Croft. Cluster-based
retrieval using language models. In Proceedings of the
27th annual international ACM SIGIR conference on
Research and development in information retrieval,
SIGIR ’04, pages 186–193, New York, NY, USA, 2004.
ACM.
[19] Kamran Massoudi, Manos Tsagkias, Maarten
de Rijke, and Wouter Weerkamp. Incorporating query
expansion and quality indicators in searching
microblog posts. In Proceedings of the 33rd European
conference on Advances in information retrieval,
ECIR’11, pages 362–367, Berlin, Heidelberg, 2011.
Springer-Verlag.
[20] Donald Metzler, Susan Dumais, and Christopher
Meek. Similarity measures for short segments of text.
In Proceedings of the 29th European conference on IR
research, ECIR’07, pages 16–27, Berlin, Heidelberg,
2007. Springer-Verlag.
[21] Jay M. Ponte and W. Bruce Croft. A language
modeling approach to information retrieval. Research
and Development in Information Retrieval, pages
275–281, 1998.
[22] Haoliang Qi, Mu Li, Jianfeng Gao, and Sheng Li.
Information retrieval for short documents. Journal of
Electronics (China), 23:933–936, 2006.
10.1007/s11767-006-0044-2.
[23] Daniel Ramage, Susan Dumais, and Dan Liebling.
Characterizing microblogs with topic models. In
ICWSM, 2010.
[24] Nico Schlaefer, Jennifer Chu-Carroll, Eric Nyberg,
James Fan, Wlodek Zadrozny, and David Ferrucci.
Statistical source expansion for question answering. In
Proceedings of the 20th ACM international conference
on Information and knowledge management, CIKM
’11, pages 345–354, New York, NY, USA, 2011. ACM.
[25] B. W. Silverman. Density Estimation for Statistics
and Data Analysis. Monographs on Statistics and
Applied Probability. Chapman & Hall, Boca Raton,
1996.
[26] Tao Tao, Xuanhui Wang, Qiaozhu Mei, and
Chengxiang Zhai. Language model information
retrieval with document expansion. In Human
Language Technology Conference of the North
American Chapter of the ACL, pages 407–414, New
York, 2006.
[27] Fulai Wang and Jim E. Greer. Retrieval of short
documents from discussion forums. In Proceedings of
the 15th Conference of the Canadian Society for
Computational Studies of Intelligence on Advances in
Artificial Intelligence, AI ’02, pages 339–343, London,
UK, UK, 2002. Springer-Verlag.
[28] Chengxiang Zhai and John Lafferty. A study of
smoothing methods for language models applied to
information retrieval. ACM Transactions on
Information Systems, 2(2):179–214, 2004.

920

