On Judgments Obtained from a Commercial Search Engine
Emine Yilmaz, Gabriella Kazai
Microsoft Research Cambridge, UK

{eminey,v-gabkaz }@microsoft.com

Nick Craswell, S.M.M. Tahaghoghi
Microsoft, Bellevue, WA, USA

{nickcr,saied.tahaghoghi}@microsoft.com

ABSTRACT

procedures, we may reason that judgments from a commercial search engine are likely to lead to different conclusions
than judgments from NIST judges.
We analyze whether judgments obtained from a commercial search engine are reliable, in terms of leading to the same
evaluation conclusions as when using NIST judgements.

In information retrieval, relevance judgments play an important role as they are used for evaluating the quality of
retrieval systems. Numerous papers have been published
using judgments obtained from a commercial search engine
by researchers in industry. As typically no information is
provided about the quality of these judgments, their reliability for evaluating retrieval systems remains questionable.
In this paper, we analyze the reliability of such judgments
for evaluating the quality of retrieval systems by comparing
them to judgments by NIST judges at TREC.

2.

EXPERIMENTAL RESULTS

We use the test collection from the TREC Web Track
Adhoc tasks from 2009 and 2010. This dataset consists of
nearly 50K NIST relevance labels, roughly 25K in each year,
for 50 topics in each year. We took these 100 topics and
Categories and Subject Descriptors
using the topic titles as queries we scraped the top 10 search
H.3 [Information Storage and Retrieval]: H.3.3[Information results from Google and Bing for each query. This gave us
a total of 1603 unique query-URL pairs for the 100 topics.
Search and Retrieval]
We constructed three different collections by obtaining
judgments from three judge groups : judges from (1) the
General Terms
TREC Web track ad-hoc task (NIST), (2) a commercial
Experimentation, Human Factors, Measurement
search engine (ProWeb), and (3) crowdsourcing (Crowd).
ProWeb judges were experienced and highly trained judges,
employed by the search engine company, while crowd workKeywords
ers, recruited via Clickworker, received no prior training on
Crowdsourcing, Evaluation, Test Collection
relevance assessing.
The NIST judgments differ across the two years. In 2009,
1. INTRODUCTION
three relevance levels were used (Highly Relevant, Relevant,
and Not Relevant), while in 2010, five grades of relevance
In information retrieval (IR), test collections are typically
were used (Navigational, Key, Relevant, Non-relevant, and
used to evaluate and optimize the performance of IR sysJunk). The ProWeb and Crowd judgments were obtained
tems. The quality of a test collection can impact the conusing a simple interface that asked judges to rate a search
clusions of the evaluation, where the quality of the relevance
result’s usefulness to a query using a five point scale that
judgments is a key factor. For example, evaluation outcomes
can be viewed as a variation of the 2010 Web Track scale
are shown to be affected by using different judge popula(Ideal, Highly Relevant, Relevant, Somewhat Relevant, Nontions [1] and different judging guidelines [3]. On the other
relevant). Unlike the ProWeb and Crowd judges, the NIST
hand, using different judges from the same population of
judges were given descriptions (topic narrative) about what
NIST judges employed by TREC has been shown to lead to
information need is associated with a particular query.
relatively stable conclusions as to which retrieval algorithm
Using the different sets of judgments and the NDCG meabeats another [4].
sure, we evaluate the effectiveness of the runs submitted to
In recent years, several papers using judgments obtained
the TREC 2009 and 2010 Web Track ad-hoc task. Since
from a commercial search engine have been published [2].
we only have labels for a subset of the retrieved documents,
Most of these papers use such judgments (which are typically
we remove unjudged documents from the runs. To avoid
not publicly available) to validate the superiority of their
variance due to having different documents labeled across
proposed methods over existing algorithms. Since judges
the different judge groups, we only consider documents that
employed by commercial search engine companies are likely
were judged by all three groups.
to come from different populations than NIST judges and
To remove the inconsistency across different judge groups
are likely to be subjected to different training and judging
due to the different levels of relevance scales used, we converted all the judgments to the Web Track 2009 scale using
Copyright is held by the author/owner(s).
the following mapping:Navigational (or Ideal ) judgments to
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
Highly Relevant, Key and Relevant (or Highly Relevant and
ACM 978-1-4503-1472-5/12/08.

1115

1.0

1.0

1.0

0.4

0.6

0.8

1.0

0.8
0.2

0.4

0.6

0.8

1.0

0.0

1.0

1.0

0.8
NDCG Crowd

0.6

0.8
0.6

0.0

0.2

NDCG Crowd

0.4
0.2

0.8

0.8

Kendall's tau = 0.783693

0.0
0.6

NDCG NIST

0.6

1.0

1.0

1.0
0.8
0.6
0.4
0.2

0.4

0.4

NDCG ProWeb

Kendall's tau = 0.538047

0.0

0.2

0.2

NDCG NIST

Kendall's tau = 0.639488

0.0

0.6

NDCG Crowd

0.2
0.0
0.0

NDCG NIST

0.4

0.2

0.4

0.6

NDCG Crowd

0.0

0.2

0.4

0.6

NDCG ProWeb

0.4
0.2
0.0
0.0

NDCG ProWeb

Kendall's tau = 0.928039

0.8

Kendall's tau = 0.860887

0.8

Kendall's tau = 0.896955

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

NDCG NIST

0.4

0.6

0.8

1.0

NDCG ProWeb

Figure 1: Comparisons of evaluation results for runs submitted to TREC 2009 (top) and TREC 2010 (bottom)
using three different sets of judges and their judgements
Relevant) to Relevant, and Non-relevant and Junk (or Somewhat Relevant and Non-relevant) to Not Relevant. We have
also experimented with various other mappings but all supported the same conclusions of this poster.
Figure 1 shows the obtained scatter plots: TREC 2009
results in the top row and TREC 2010 in the bottom row.
For each year, we plot the results obtained from evaluating
the runs using the NIST vs ProWeb (left plot), NIST vs.
Crowd (middle plot) and Crowd vs. ProWeb (right plot)
judgments. In each plot, we also include the Kendall’s τ
correlation between the resulting system rankings, obtained
using the two respective sets of judgments.
We see that for TREC 2009, evaluations using the NIST,
ProWeb and Crowd judgments mostly agree with each other
(Kendall’s τ of 0.86-0.93). This suggests that the judgments
obtained from a commercial search engine are more or less
consistent with NIST: using them will not cause major differences in the evaluation. Crowd judgments may be somewhat
noisier, but still lead to stable evaluations. The differences
in the three plots could be caused by the consistency in
the number and description of relevance grades between the
Crowd and ProWeb judges as compared to the NIST judges.
On the other hand, for TREC 2010, evaluations using the
ProWeb and NIST judgments are quite different (Kendall’s
τ = 0.639). At first glance, one might think that ProWeb
judgments are not reliable at evaluating the systems and the
high agreement on the TREC 2009 data is due to chance.
However, if we compare the agreements between the evaluations using the Crowd and ProWeb judgments, we also
see low agreements. This suggests that the low correlation
is specific to this particular TREC. As the figure suggests,
systems that were submitted to this particular TREC have
very similar performance. Hence, the Kendall’s τ statistic
may be affected by these systems. Furthermore, when we

considered the cases where NIST judgments highly disagree
with the Crowd and ProWeb judgments, we found that there
are quite a few documents that got the best rating by the
Crowd and ProWeb judges but were labeled as non-relevant
by the NIST judges. When we analyzed the disagreement
cases, we realized that these differences could be caused by
the specific topic description that was given to the NIST
judges, which limited the possible intents associated with
a query. Since the Crowd and ProWeb judges were not
given such topic descriptions, they would have considered
all possible intents for a query when assigning labels to the
documents.
Overall, our conclusion is that even though judgments
from a commercial search engine could lead to slightly different conclusions than NIST judges in some settings, evaluations using the two judge groups seem mostly consistent.

3.

REFERENCES

[1] Peter Bailey, Nick Craswell, Ian Soboroff, Paul Thomas,
Arjen P. de Vries, and Emine Yilmaz. Relevance assessment:
are judges exchangeable and does it matter. In Proc. of
ACM SIGIR Conference, pages 667–674. ACM, 2008.
[2] Christopher J. C. Burges, Robert Ragno, and Quoc Viet Le.
Learning to rank with nonsmooth cost functions. In NIPS,
pages 193–200. MIT Press, 2006.
[3] Charles L.A. Clarke, Nick Craswell, Ian Soboroff, and Azin
Ashkan. A comparative analysis of cascade measures for
novelty and diversity. In Proc. of ACM WSDM Conference,
pages 75–84. ACM, 2011.
[4] Ellen M. Voorhees. Variations in relevance judgments and
the measurement of retrieval effectiveness. In Proc. of ACM
SIGIR Conference, pages 315–323. ACM, 1998.

1116

