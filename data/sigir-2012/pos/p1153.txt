Time to Judge Relevance as an Indicator of Assessor Error
Mark D. Smucker

Chandra Prakash Jethani

mark.smucker@uwaterloo.ca

jethani@yahoo-inc.com

Department of Management Sciences
University of Waterloo

Yahoo! Inc.
Sunnyvale, CA, USA

ABSTRACT

2. MATERIALS AND METHODS

When human assessors judge documents for their relevance
to a search topic, it is possible for errors in judging to occur. As part of the analysis of the data collected from a
48 participant user study, we have discovered that when the
participants made relevance judgments, the average participant spent more time to make errorful judgments than to
make correct judgments. Thus, in relevance assessing scenarios similar to our user study, it may be possible to use the
time taken to judge a document as an indicator of assessor
error. Such an indicator could be used to identify documents
that are candidates for adjudication or reassessment.

The user study presented participants with ﬁxed ranked
lists of documents. Given a ranked list, participants were
instructed to search for and save relevant documents. The
user interface was web-based and consisted of two web pages.
The ﬁrst page showed 10 document summaries in a format
similar to a web search engine’s results page. Clicking on a
summary took the participant to a page that displayed the
full document and allowed the participant to save the document as relevant. We did not require relevance judgments to
be made. We treat viewing a full a document and not saving it as a decision to judge the document as non-relevant.
Participants could view the next 10 results by clicking on a
link at the bottom of the document summaries page.
The study used 8 topics from the 2005 TREC Robust
track. The documents came from the AQUAINT newswire
collection. Each participant searched ranked lists with a
uniform precision of 0.6 or 0.3. We use the NIST relevance
judgments as truth.
Participants worked on 4 search topics for 10 minutes
each. The study used a fully balanced design. Participants
were instructed to work as fast as possible while making as
few mistakes as possible. We use data from the second phase
of the study. All participants had already completed phase
1 of the study and gained experience with relevance judging
before participating in the second phase. The result lists
contained near-duplicate documents. We use only the ﬁrst
judgment on a set of near-duplicates.
We measured the time the participants spent on the full
document page before saving the document as relevant or
leaving the page. It is well-known that large variations in
behavior are caused by the search topic and the user. To
allow comparisons of times across users and topics, we standardized a participant’s times on a topic to have a mean of
zero and a standard deviation of 1.
We average a participant’s times on a topic, and then
average across topics for that participant to produce a participant average. We ﬁnally average across participants to
produce average participant times.

Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
Keywords: Relevance judging, assessor error detection

1. INTRODUCTION
We have two motivations for wanting to better understand
assessor behavior and assessor error. First, knowledge of assessor behavior should help IR researchers in the construction of more accurate relevance assessing systems. Second,
while traditional retrieval metrics are relatively insensitive
to assessor error [6], newer metrics that sample documents
for judging, can be dramatically aﬀected by assessor error [1,
7]. A single sampled document can represent thousands of
documents in the ﬁnal estimation, and a single document
falsely judged to be relevant can greatly inﬂate the estimated number of relevant documents. Being able to predict
which documents might have incorrect judgments could be
useful for improving assessor consistency by asking assessors
to judge certain documents a second time [1].
We have previously reported on relevance judging errors
made by the 48 participants who took part in our user
study [5], but we have not previously reported on what behaviors correlate with errors.
In this paper, we present results for a relevance assessing
scenario where there was not an incentive for users to work
faster. In this scenario, where the assessor is not paid per
judgment, but is paid for the time spent, we have found an
interesting result: users take more time to make mistakes
than they do to make correct relevance judgments.

3. RESULTS AND DISCUSSION
When a participant judges a non-relevant document to be
relevant, a false positive error is made. Likewise, when a
relevant document is judged to be non-relevant, a false negative error is made. Table 1 and Figure 1 show the main
results. These results are based on 3614 judgments (2706
correct judgments, 511 false negative errors, 397 false positive errors).

Copyright is held by the author/owner(s).
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
ACM 978-1-4503-1472-5/12/08.

1153

Average Participant Time to Judge Doc. Relevance
Standardized Times
Raw Times in Seconds

Correct
-0.05 ± 0.01
28 ± 2

False Positive
0.26 ± 0.07
39 ± 4

p-value
< 0.001
0.03

False Negative
0.13 ± 0.08
33 ± 3

p-value
0.02
0.16

−0.5

0.0

0.5

1.0

1.5

words, if the time to judge is too long given the length of
the document, this is an indicator of error. We also found
that ranks above about 50 were predictive of making errors.
Reaching a rank over 50 in 10 minutes is likely indicative of
a participant who is not carefully judging documents. For
ranks less than 50, rank was not predictive of errors.
The relevance assessing setup may have a signiﬁcant eﬀect
on assessor behavior. In preliminary analyses of separate
studies where assessors had an incentive to work faster [4]
and relevant documents were highly prevalent (90% relevant) [2], we have seen false negatives be long errors and
false positives be short errors. In this paper, both types of
errors were long errors. In future work we intend to analyze
these studies in more detail by using this paper’s method of
standardizing times.

−1.0

Average Participant Standardized Time to Judge

Table 1: Average participant times to judge document relevance for both standardized times and raw times
in seconds. Shown with the average is the standard error of the mean. We used a two-sided, Student’s t-test
to measure the statistical signiﬁcance of the diﬀerence between the time to make correct judgments and each
of the error conditions: false positive and false negative errors. For the standardized times, both types of
errors took a statistically signiﬁcant longer time (p < 0.05).

False Positive

Correct

False Negative

Figure 1: Average participant standardized times to
judge document relevance for false positive, correct,
and false negative judgments.

4. CONCLUSION
We found in a relevance assessing scenario where the assessor is trusted and paid by time spent, and not by number
of judgments made, that all things equal, the longer an assessor takes to make a relevance judging decision, the more
likely a mistake will be made. We hypothesize that time
to judge a document relative to other documents, gives an
indication of the diﬃculty of judging the document.

We found that on average, participants took signiﬁcantly
longer to judge documents when they made an error compared to when they were correct in their judgment. A priori,
one might guess that participants would make errors by not
taking enough time, but in contrast, we ﬁnd here that time
appears to indicate diﬃculty in making a correct judgment.
We hypothesize that the study participants make judgment errors according to the following process. First the
participants identify a topically relevant document based on
the document’s summary and then click on the summary to
view the full document. Participants then study the document to determine if it is relevant or not to the speciﬁcs of
the search topic. The more diﬃcult it is to determine the
document’s relevance, the longer the participant takes. At
some point, the participant decides to save the document as
relevant or abandon the document without saving it. False
negatives result from not ﬁnding the relevant material in
the document, and false positives are the result of the ﬁnal
decision being a guess.
There are many other factors that can cause a user to
take longer to judge a document. For example, we have
shown that the longer a document, the longer it takes to
judge its relevance [3]. Is it just that mistakes are being
made on longer documents? To investigate this, we modeled the probability that a judgment was an error using logistic regression. For the model, we considered time, document length measured in words, and the rank of the document in the results list. We found that when both time
and document length were used together in the model, document length was not a useful predictor of errors. On the
other hand, when we included the interaction between time
and document length, this interaction was helpful. In other

5. ACKNOWLEDGMENTS
David Hu wrote the software to compute the sets of nearduplicate documents. This work was supported in part by
NSERC, in part by Amazon, and in part by the University
of Waterloo.

6. REFERENCES
[1] B. Carterette and I. Soboroﬀ. The eﬀect of assessor error on
IR system evaluation. In SIGIR, pp. 539–546, 2010.
[2] C. Jethani. Eﬀect of prevalence on relevance assessing
behavior. Master’s thesis, University of Waterloo, 2011.
[3] C. Jethani and M. D. Smucker. Modeling the time to judge
document relevance. In Proceedings of the SIGIR’10
Workshop on the Simulation of Interaction, 2010.
[4] M. D. Smucker and C. Jethani. The crowd vs. the lab: A
comparison of crowd-sourced and university laboratory
participant behavior. In Proceedings of the SIGIR 2011
Workshop on Crowdsourcing for Information Retrieval,
2011.
[5] M. D. Smucker and C. Jethani. Measuring assessor accuracy:
a comparison of NIST assessors and user study participants.
In SIGIR, pp. 1231–1232, 2011.
[6] E. M. Voorhees. Variations in relevance judgments and the
measurement of retrieval eﬀectiveness. IPM, 36:697–716,
September 2000.
[7] W. Webber, D. W. Oard, F. Scholer, and B. Hedin. Assessor
error in stratiﬁed evaluation. In CIKM, pp. 539–548, 2010.

1154

