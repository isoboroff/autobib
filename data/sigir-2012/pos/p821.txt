Mixture Model with Multiple Centralized Retrieval
Algorithms for Result Merging in Federated Search
Dzung Hong

Luo Si

Department of Computer Science
Purdue University
250 N. University Street
West Lafayette, IN 47907, USA

Department of Computer Science
Purdue University
250 N. University Street
West Lafayette, IN 47907, USA

dthong@cs.purdue.edu

lsi@cs.purdue.edu

ABSTRACT

Keywords

Result merging is an important research problem in federated search for merging documents retrieved from multiple ranked lists of selected information sources into a single
list. The state-of-the-art result merging algorithms such as
Semi-Supervised Learning (SSL) and Sample-Agglomerate
Fitting Estimate (SAFE) try to map document scores retrieved from different sources to comparable scores according to a single centralized retrieval algorithm for ranking
those documents. Both SSL and SAFE arbitrarily select a
single centralized retrieval algorithm for generating comparable document scores, which is problematic in a heterogeneous federated search environment, since a single centralized algorithm is often suboptimal for different information
sources.
Based on this observation, this paper proposes a novel
approach for result merging by utilizing multiple centralized retrieval algorithms. One simple approach is to learn a
set of combination weights for multiple centralized retrieval
algorithms (e.g., logistic regression) to compute comparable document scores. The paper shows that this simple
approach generates suboptimal results as it is not flexible
enough to deal with heterogeneous information sources. A
mixture probabilistic model is thus proposed to learn more
appropriate combination weights with respect to different
types of information sources with some training data. An
extensive set of experiments on three datasets have proven
the effectiveness of the proposed new approach.

Federated Search, Result Merging, Mixture Model

1.

INTRODUCTION

Federated search (also known as distributed information
retrieval) [17, 23, 29] is an important research area of information retrieval. Unlike traditional information search systems such as Google or Bing, which index webpages or documents that can be crawled and collected, federated search
targets on information distributed in independent information providers. Many contents in this environment may not
be arbitrarily crawled and searched by traditional search engines, due to various reasons such as copyright, security and
data protection. Only the owners of those documents can
provide a full searching service to their set of documents.
We refer to a collection of documents with its own and customized search engine as an information source. The size
of this type of information (i.e., hidden Web contents) has
been estimated to be many times larger than Web contents
searchable by traditional search engines [3].
Federated search offers a solution for searching hidden
Web contents by building a bridge between users, who have
little knowledge about which kind of information sources
she is looking for, and the information sources that reveal
limited information about their documents through sourcespecific search engines. To achieve this goal, federated search
includes three main research problems: resource representation, resource selection and result merging. Resource representation learns important information about the sources
such as their contents and their sizes. Resource selection selects a subset of information sources which are most useful
for users’ queries. Result merging combines documents retrieved from selected sources into a single ranked list before
presenting the list to the end users.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval

General Terms

Among the above main problems, result merging substantially suffers from the heterogeneity of information sources.
Each information source may adopt a different, customized
retrieval model. A query can also be processed in many
ways. Even if different sources use similar retrieval algorithms, they may have different source statistics (e.g., different values of inverse document frequencies). All of those
make it difficult to compare documents of different sources.
A simple solution that downloads all document contents and
ranks them with a single method for each user query may
yield good results, but it is also costly in an online setting.

Algorithms, Design, Performance

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

821

Other solutions such as downloading parts of the documents
[6] or incorporating scores from the resource selection component into source-specific document scores (e.g., CORI [4])
also suffer when information sources do not provide enough
information or vary greatly in their scales of document ranking scores.

ing. There is a large volume of previous research work in all
of those research problems. This section first briefly introduces most related prior research in resource representation
and resource selection. Then it will provide more details
about the literature of result merging.
Resource representation is to collect information about
each information source. Such information usually includes
sources’ sizes, document frequencies, term frequencies, and
other statistics. The START protocol [9] is one of the first
attempt to standardize the communication between information sources and a broker (or centralized agent) in order to collect, search and merge documents from individual
sources. However, this approach can only work in cooperative environments. In an uncooperative environment, it is
more practical to collect source statistics with sampling algorithms. The query-based sampling method [4] is a popular
algorithm for sampling documents from a set of information
sources. In principal, query-based sampling sends randomly
generated terms as queries to a source, and downloads the
top documents as sample documents for each query. When
this process is done, the set of all sample documents can
be collected in a centralized sample database to build a single index. The centralized sample database is often a good
representative of the (imaginary) complete database of all
documents in a federated search environment.

The state-of-the-art result merging algorithms merge documents by learning how to map document scores in ranked
lists of multiple information sources to comparable document scores. The basic idea is to utilize a centralized sample
database created with all sample documents obtained in resource representation. For each query, these algorithms rank
documents in the centralized sample database with a single
retrieval algorithm, and then build a mapping function between source-specific document scores (or ranks) and comparable document scores. By mapping document scores/ranks
returned from all selected sources to a common scale, it is
possible to construct the final ranked list. Algorithms of this
class such as SSL [27], SAFE [24] and WCF [12] have shown
promising results. However, despite using various learning
algorithms, those methods still do not fully address the heterogeneity of retrieval algorithms in different information
sources. The problem lies in the fact that all these existing
methods arbitrarily select a single fixed centralized retrieval
algorithm for learning the mapping, which is problematic
in a heterogeneous federated search environment, as a single
algorithm is often suboptimal for learning comparable scores
for different sources.

Resource selection is to select a subset of information
sources most relevant to the user’s query. Resource selection has been studied intensively during the last two decades.
Many algorithms have been developed, such as GlOSS [10],
CORI [4], ReDDE [26], CRCS [22], topic models [2], the
classification-based model [1] and many others. The Relevant Document Distribution Estimation (ReDDE) resource
selection algorithm and its variants have been shown to generate good and robust resource selection results in different types of federated search environments. ReDDE selects
relevant sources by first ranking sample documents in the
centralized sample database. Then, each document among
the top of the list can contribute a score to its containing
source. The magnitude of the score depends on both document’s rank and the source’s size. Finally, the relevance
of a source is measured by the combined score of all of its
sample documents.

In this paper, we propose a novel result merging algorithm that utilizes multiple centralized retrieval algorithms.
This method can generate more accurate results in result
merging due to the flexibility of using multiple types of centralized retrieval algorithms for estimating comparable document scores. In particular, the paper shows that it is not
desirable to learn a fixed set of weights (e.g., with a logistic
regression approach) for different centralized retrieval algorithms in estimating comparable document scores. A mixture probabilistic model is proposed to automatically learn
the appropriate weights for different types of information
sources with some training data. The mixture model approach is more flexible in calculating comparable document
scores for a heterogeneous set of information sources. Empirical studies have been conducted with three federated search
datasets to show the advantages of the proposed result merging algorithm. In particular, one new dataset is created from
the Wikipedia collection of ClueWeb data.

Result merging is to collect the ranked list of documents
from each selected source and combine them into a single
ranked list to present to users. Result merging in federated
search is similar to data fusion [32, 25], or merging process
in multilingual information retrieval [30]. In data fusion,
different retrieval models are applied to a single information
source, and the problem is to get the best combination of
retrieval algorithms. Whereas, in federated search, there are
multiple information sources with different (often unknown)
retrieval models. Similar to information fusion, multilingual
information retrieval also assumes that the whole collection
index is available to the merger during the process, which is
not always the case in federated search.

The rest of the paper is organized as follows. Section
2 discusses some research work generally related with the
work in this paper. Section 3 discusses two specific state-ofthe-art results merging algorithms (SSL and SAFE) as they
are directly related with the proposed research. Section 4
proposes the novel result merging algorithm with multiple
centralized retrieval algorithms. Section 5 introduces experimental methodology. Section 6 presents the detailed experimental results and provides some discussions. Section 7
concludes and points out some future research directions.

2.

One scenario is that the broker can download all returned
documents from selected sources, and apply a centralized
retrieval algorithm to produce the final ranked list. However, in practice, this method is rarely used since the high
cost of communication and time may impair user experience. In another simple case, when all sources implement

RELATED WORK

Federated search includes three main research problems:
resource representation, resource selection and result merg-

822

3.

the same retrieval model, documents’ scores (or ranks) returned by the source may be comparable with each other.
Thus, merging their scores (or ranks) directly (also known
as Raw Score Merging), or in a round-robin fashion may give
good results with low cost. However, it is noticed that even
if all sources share the same model, some statistics such as
document frequency of a term are still different across different sources. It is generally not practical to assume that
all independent sources share such a same set of collection
statistics.

3.1

Semi-Supervised Learning Merging

Semi-Supervised Learning Merging (SSL) [27] uses curve
fitting model to calculate comparable document scores from
different sources for result merging. Specifically, given a
user’s query, SSL sends the query to the centralized sample database and retrieves the sample ranked list with relevance score of each document. Upon receiving documents
from a selected information source, SSL checks for overlapping documents exist in the sample database. Those overlapping documents are characterized by two features: the
relevance scores in the central sample database, and the relevance ranks in the specific source. The task is to estimate
the relevance scores of all non-overlapping documents in the
centralized complete database (the imaginary dataset of all
documents of all sources). Assume that there is a linear
mapping between centralized relevance scores and sourcespecific document ranks, then that mapping can be inferred
by using a regression method on the overlapping documents.
Having said that, let Rij be the source-specific rank of document di in source Cj , and Sij be the relevance score of document di in the centralized sample database, we can build
a linear relationship.

Some other algorithms in the early generation of federated search also relied on term statistics for making decision.
Craswell et al. suggested that by partially downloading a
part of the top returned documents, we can approximate
term statistics to build the final rank list [6]. Xu and Croft
requested that document statistics of query terms should be
provided to the broker, in such a way that they can calculate the global inverse document frequencies [34]. However,
these algorithms again require some type of collaboration
from the independent sources, which is often unavailable.
CORI result merging algorithm [4] is a relatively simple,
yet effective algorithm. The intuition is that comparable
document scores should depend on two factors: (i) how good
a document is compared to other documents from the same
source; and (ii) how good the source containing a particular
document is compared to other sources. CORI makes a linear combination of those two factors and gets the final score
of a document as:

D0 =

RESULT MERGING BY SEMI-SUPERVISED
LEARNING & SAMPLE-AGGLOMERATE
FITTING ESTIMATE

Sij = aj × Rij + bj
where aj , bj are two parameters depending on each pair of
an information source and a query.

D + 0.4 × D × C 0
1.4

With enough overlapping documents for a source and a
query, we can train a regression matrix




R1j 1
S1j
 
 R2j 1
 S2j 
aj




 · · · 1 × bj =  · · · 
Snj
Rnj 1

where D0 is the global score, D is the original score within
the source, and C 0 is the normalized source score from the
resource selection step.
The merging algorithm proposed by Rasolofo et al. [19]
also explores the combination between document scores and
source weights. Unlike CORI, their source weights are not
directly related with the sources’ relevance scores. Rather,
the weight of a source depends on the total number of documents that it returns. The algorithm assumes that a source
containing more relevant documents may return a longer
ranked list, which is not always the case for information
sources using different types of ranking algorithms.

In the above equation, let us denote the first matrix by
X, the second matrix by W , and the third matrix by Y . By
minimizing the square loss error, we can derive the solution
to the parameters W as
W = (X T X)−1 X T Y
One main problem of SSL is that if there is not enough
overlapping documents (three requested in the original SSL
work) for building a linear mapping, the model will back off
to the CORI result merging formula, which is often much
less effective.

The intuition of combining document and resource scores
can also be seen in a variant of the PageRank algorithm in
distributed environments [31]. In this work, Wang and DeWitt employed the source’s ServerRank and the document’s
LocalRank to derive the global PageRank values.

3.2

Semi-Supervised Learning (SSL) [27] and the Sample Agglomerate Fitting Estimate (SAFE) [24] result merging algorithms offer a better trade-off in efficiency and effectiveness.
Both methods try to map source-specific document ranks
into comparable document scores generated by a single centralized retrieval algorithm. We will provide more detailed
information about SSL and SAFE in the following section
as they are directly related with the new research in this
paper.

Sample-Agglomerate Fitting Estimate Merging

Sample-Agglomerate Fitting Estimate (SAFE) [24] overcomes the SSL’s problem of not having enough overlapping
documents by estimating the ranks of unoverlapping documents in the centralized sample database. If we assume that
the sampling process is uniform, then each sample document
will represent the same number of unseen documents in the
selected information source. Therefore, a sample document
ranked at position i-th in the source-specific sample ranked

823

Table 1: Transformation Functions
Name
LIN
SQRT
LOG
POW

f (x)
f (x) = x
√
f (x) = x
f (x) = log x
f (x) = 1/x

ple database to learn the comparable document scores by
curve-fitting. However, unlike SSL and SAFE, MoRM employs multiple retrieval algorithms for the centralized sample database. Therefore it is more flexible to address the
heterogeneity of information sources in federated search environments for improving the accuracy of result merging.

Model
S = a ×√R + b
S = a0 × R + b 0
S = a00 × log R + b00
1
S = a000 × R
+ b000

4.1

In this section, we describe the general framework of MoRM
for document merging. The following steps are applied when
a query comes:

|C|
list will have an approximate rank i × |C
in the sources|
specific full rank, where |C| is the source’s estimated size,
and |Cs | is the source’s sample size. By using the estimated
source-specific ranks together with true centralized ranks (of
overlapping documents), SAFE could apply regression with
more information than SSL. A problem may occur when
there are not enough sample documents of a selected information source in the centralized sample database. However,
this is rarely the case, if ReDDE (or its variants) is used
for selecting information sources, since this method usually
selects a source if it has a significant number of documents
in the centralized ranked list with respect to the query.

• A resource selection algorithm such as ReDDE [26] selects a subset of information sources that are most relevant to the user’s query.
• The query is then forwarded to the selected information sources. Each source will return a ranked list of
documents. Scores of the returned documents will help
the model’s performance but are not required. Document ranks are usually sufficient for the next step.

Another contribution of SAFE to SSL is that, instead of
using the raw rank information of documents, SAFE applies
different transformation functions to the rank, in order to
find the best regression. More specifically, there are four
different transformations as in Table 1. Each transformation function is applied to the source-specific ranked list
to learn the set of parameters (aij , bij ). Then, SAFE selects the best transformation by comparing the goodness of
curve-fitting of all models based on their coefficient of determination R2 values [11]. Specifically, for a linear regression
equation X × w = Y , the coefficient of determination is
calculated as follows.
R2 =

• MoRM also issues the query to the centralized sample database and retrieves documents using a set of
predetermined algorithms. At the end of this step, it
obtains a set of ranked lists of sample documents.
• For each ranked list, MoRM tries to learn a mapping
between source-specific document ranks and the centralized document scores. Ranks of sample documents
that are not in the source-specific ranked list are estimated in a similar way as in the SAFE algorithm. All
transformation functions as listed in Table 1 are tested
in order to find the best curve fitting parameters. The
best transformation function is applied to predict the
comparable scores of all returned documents.

||Ŷ ||2
Y TPY
=
2
||Y ||
Y TY

where P = X(X T X)−1 X T

4.

MoRM’s Framework

MIXTURE OF RETRIEVAL MODELS FOR
RESULT MERGING

SSL and SAFE are state-of-the-art algorithms for result
merging in federated search. However, because of their choosing of a single centralized retrieval algorithm for calculating comparable document scores, these algorithms still do
not fully address the heterogeneity of different information
sources in federated search environments . A single centralized retrieval algorithm may have good curve-fittings for
some information sources, but may also be less fit for some
others. This paper proposes to use multiple centralized retrieval algorithms to retrieve a set of ranking scores for each
document. Moreover, rather than assigning a fixed set of
weights to combine the above scores, our model learns a
more appropriate combination of weights with respect to different types of information sources. We assume that there
is an underlying distribution (i.e., latent groups) of sources
according to their adopted retrieval models. Learning the
proposed model thus becomes learning the distribution of
groups and the combination weights associated with each
group. This model is called the Mixture of Retrieval Models
(MoRM) for result merging. MoRM is related with SSL
and SAFE in the way that it uses the centralized sam-

• All comparable scores of a document are combined using a set of combination weights learned from a training dataset. These final scores are used to rank documents.
In the following sections, we will propose a simple logistic
regression model (for learning a single set of combination
weights) and then propose the mixture of retrieval models
(for learning multiple sets of combination weights) for the
task of estimating documents’ comparable scores.

4.2

Logistic Regression for Learning Comparable Scores

A learning algorithm such as logistic regression may address the problem of combining different document scores
seamlessly. We chose logistic regression to demonstrate the
approach of learning a single set of combination weights for
ranking documents. Logistic regression is a discriminative
model that models the probability that a binary event happens by a sigmoid function. In this case, our predictive
functions are:
1
q
= σ(w
~ · ~xqcd ) (1)
P (ycd
= 1|w,
~ ~
xqcd ) =
1 + exp (−w
~ ·~
xqcd )
and
q
q
P (ycd
= −1|w,
~ ~
xqcd ) = 1 − P (ycd
= 1) = σ(−w
~ · ~xqcd )

824

(2)

where our notations for this model are as follows: let the
superscript q refer to the query q, and the subscript cd refer
to the d-th document of source c (such a document is called
Dcd ). We also use ~
xqcd to denote the feature vector of document Dcd (the set of comparable scores of Dcd according to
different centralized retrieval algorithms), and w
~ for the set
q
of weights associated with ~
xqcd . Our target is to predict ycd
,
which is the relevance of document Dcd with respect to the
q
query q. The possible values of ycd
are:
(
1
if document Dcd is relevant to query q
q
ycd
=
−1 otherwise

The probability that a document Dcd has relevance ycd
given all parameters is calculated as follows.
~ ~
P (ycd |θ,
xcd ) =

~ =
L(θ)

q
|w,
~ ~
xqcd ) =
P (ycd

q=1 c=1d=1

4.4

q
w
~ ·~
xqcd )
σ(ycd

Dc X
C Y
K
Y

πk σcdk

(4)

k

Learning MoRM using EM Algorithm

Let zc be a K-dimensional latent variable associated with
source c. zc has only one element which equals to 1 and the
all other elements equal 0 (i.e., a 1-of-K representation).
Therefore, zc must satisfy the following constraints.
PK
for c = 1, 2, · · · , C
k=1 zck = 1

q=1 c=1d=1

where we have combined equations (1) and (2) above. Learning the combination weight w
~ can be done by maximizing
the log-likelihood function using the iterative re-weighted
least squares method [8].

4.3

(3)

where Dc is the number of documents returned by the source
c.
It is difficult to optimize the above function directly, since
taking its logarithm still presents the summation inside the
log. Therefore, we will utilize the Expectation Maximization
(EM) algorithm [7] to learn the parameters. The derivation
of EM algorithm is discussed in the following section.

Given C, the number of sources; Q, the number of queries,
and all the returned documents Dcd with respect to the
training queries, we can write the likelihood function of the
model as
L(w)
~ =

πk σcdk

k=1

c=1 d=1

1
1 + exp(−z)

|Q| C Dc
Y
YY

K
X

The component πk acts as the prior of the clusters’ distribution, which adjusts the belief of relevance according to
each cluster. This equation is also known as the mixture of
logistic regression. Given that model, the likelihood function for the training dataset with respect to one query is as
follows.

and apply this property: σ(−x) = 1 − σ(x).

|Q| C Dc
Y
YY

πk P (y cd |w
~k, ~
xcd ) =

k=1

Finally, in the equations (1) and (2) above, we also use
σ(z) to indicate the sigmoid function
σ(z) =

K
X

where zck is the k-th element of zc .

Mixture of Retrieval Models for Learning
Comparable Document Scores

We then use zc as the indicator of the membership of
source c. If c belongs to cluster k, then zck = 1, and the
other elements of zc equal 0. Given that πk is the prior
distribution of cluster k as above, and note that πck = πk
for all c, we can write the prior distribution of zck as follows.

We now describe the mixture model of retrieval algorithms
(MoRM) for result merging. MoRM offers more prediction
capability by automatically learning multiple sets of combination weights, each of them is associated with a “soft”
information source cluster. The word “soft” means that we
use probability to assign a source to its cluster, rather than
fixing a hard assignment. Specifically, assuming that there
are K of such clusters, and let πck be the probability that the
source c belongs to group k, then the following constraints
must be hold:
PK
for c = 1, 2, · · · , C
k=1 πck = 1

P (zck = 1) = πk

(5)

Then the prior distribution of the whole vector zc can be
written as
P (zc ) =

K
Y

πk zck

(6)

k=1

Define another random variable Z = {zc |c = 1, · · · , C}
associated with all sources. Since each source is independent
of each other, the prior of Z is just the multiplication over
all sources.

To make our formulations simpler, in this section, we will
first derive the formulations for only one query, and drop
the superscript q of ycd and ~
xcd . At the end of this section, we will extend the formulations for the set of training
queries. Furthermore, we will denote σcdk for P (y cd |w
~k, ~
xcd )
(the probability that the document Dcd has relevance ycd to
the query in question, given that the collection c belongs to
cluster k. In short, σcdk = σ(ycd w
~k · ~
xcd )).
~ = {w
Let w
~ k |k = 1, · · · , K}, and π = {πck |c = 1, · · · , C;
~ = {w,
~ π} denote the
k = 1, · · · , K}. Given a query q, let θ
set of parameters, in which each combination weight w
~ k is
associated with the k-th cluster. MoRM assumes the same
combination weight for all sources of a cluster for building
robust combination model with a limited amount of training
data, hence we will set πk = πck = πc0 k for all sources c, c0 .

P (Z) =

C Y
K
Y

πk zck

(7)

c=1k=1

Similarly, if we know that the source c has the membership vector zc , then the probability
Q that thezckdocument Dcd
of that source has relevance ycd is K
, since σcdk is
k=1 σcdk
the conditional probability with respect to cluster k. Therefore, the likelihood function of the model is obtained by
multiplying the above term over all sources and documents.
~ =
P (X, Y |Z, θ)

Dc  Y
K
C Y
Y

σcdk zck

c=1d=1 k=1

825



(8)

where X denotes all document feature vectors, and Y denotes the relevance vector of all documents. Multiplying the
equations (7) and (8) above, one can calculate the complete
likelihood function
~ =
P (X, Y , Z|θ)

Dc
C Y
K 

Y
Y
πk zck
σcdk zck
c=1k=1

Using Lagrange multiplier and setting the gradient to 0,
one can solve the optimal values of πk as
PC
γ(zck )
(14)
πk new = PK c=1
PC
c=1 γ(zck )
k=1

(9)

Searching for the value of w
~ knew is a bit trickier, since we
have to solve the following optimization problem.

d=1

Taking the logarithm of the above function yields the complete log-likelihood as follows.
~ =
log P (X, Y , Z|θ)

C X
K
X

zck {log πk +

c=1 k=1

Dc
X

~ new = arg max
w
~
w

log σcdk } (10)

d=1

Dc X
C X
K
X

γ(zck ) log σcdk

(15)

c=1 d=1k=1

In fact, the gradient of the above objective function with
respect to w
~ k is equal to:

The EM algorithm involves two steps. For the E-step, we
~
need to calculate the posterior probability P (Z|X, Y , θ).
Using (9), we can derive the following relation.

Dc
C X
X

γ(zck )(1 − σcdk )ycd ~
xcd

c=1 d=1
Dc
C Y
K 
zck
Y
Y
~
~ = P (X, Y , Z|θ) ∝
P (Z|X, Y , θ)
πk
σcdk
~
P (X, Y |θ)
c=1k=1
d=1
(11)
where we can use the proportional sign ∝ because the de~ does not depend on Z.
nominator P (X, Y |θ)

Therefore, one can apply a gradient descent algorithm to
find the optimal value of w
~k.
In the implementation of the algorithm discussed so far,
there is an issue about γ(zck ). As equation (12) has shown,
Q c
computing γ(zck ) involves calculating the product D
d=1 σcdk .
This could lead to numerical underflow since σcdk is a probability smaller than 1. Therefore, we need to calculate γ(zck )
under the log space. Let

We wish to calculate the expectation of the variable Z
under the above posterior distribution, since that term will
be useful in the following M step. Given that all zc are
independent, and the right-hand side of equation (11) can
be factorized over c, we can derive the expectation of each
variable zck as
zck
QDc
P
d=1 σcdk
{z ∈{0,1}} zck πk
E[zck ] = P ck
zcj
QDc
d=1 σcdj
{zcj ,1≤j≤K} zcj πj
Q c
πk D
d=1 σcdk
= PK
= γ(zck )
(12)
QDc
π
j
j=1
d=1 σcdj

γ(zck ) ∝ πk

and αmax = maxK
k=1 α(zck ). Therefore
α(zck )
exp{log α(zck ) − log αmax }
γ(zck ) = PK
= PK
α(z
)
cj
j=1
j=1 exp{log α(zck ) − log αmax }
Since each log α(zck ) is computable under the log space,
the above equation will avoid the underflow problem. Finally, we extend our formulations to the set of training
queries. In this case, the E-step becomes:
Q QDc q
πk |Q|
q=1
d=1 σcdk
γ(zck ) = PK
Q|Q| QDc q
j=1 πj
q=1
d=1 σcdj

~new are calcuIn the M-step, the updated parameters θ
lated according to the following formula
(13)

~
θ

where
~old

~ θ
Q(θ,

~ | P (Z|X, Y , θ)
~
) = E log P (X, Y , Z|θ)


In the M-step, the update formula of πk remains the same
(equation (14)), while the optimization function in equation
(15) becomes



~ (as derived
Taking the expectation of log P (X, Y , Z|θ)
in equation (10)) with respect to the posterior distribution
gives us the following objective function for the M-step.
{π

new

~
,w

new

} = arg max
~
π,w

C X
K
X

γ(zck )(log πk +

c=1 k=1

Dc
X

σcdk = α(zck )

d=1

where we have defined a new variable γ(zck ).

~new = arg max Q(θ,
~ θ
~old )
θ

Dc
Y

~ new = arg max
w
~
w

|Q| Dc K
C X
X
XX

q
γ(zck ) log σcdk

c=1 q=1 d=1k=1

and the objective gradient with respect to w
~ k is
log σcdk )

|Q| Dc
C X
X
X

d=1

q
q
γ(zck )(1 − σcdk
)ycd
~
xqcd

c=1 q=1 d=1

To find the new value of πk , we only need to maximize
the first part of the above function
π new = arg max
π

subject to the constraint

C X
K
X

5.

In this section, we will describe the methodology and
datasets of this work. The experiments were conducted
on three datasets: two standard TREC datasets, and one
Wikipedia dataset for federated search based on the ClueWeb.

γ(zck ) log πk

c=1 k=1

PK

k=1 πk

EXPERIMENTAL METHODOLOGY

=1

826

Table 2: Statistics of Three Testbeds
Size
# of
Testbed
(GB) inf. sources
TREC123
3.2
100
TREC4-Kmeans 2.0
100
ClueWeb-Wiki
252
100

# of
Min
0.7
0.3
4.4

documents (x1000) # of # of
Avg
Max
queries Min
10.8
39.7
100
37
5.7
82.7
50
0
58.6
434.5
106
1

relevant docs/query
Avg
Max
483.7
1,994
127.2
416
20.1
93

25

Number of sources

20

15

10

5

0
0

0.5

1

1.5

2
2.5
3
Number of Documents

3.5

4

4.5
5

x 10

Figure 1: Histograms of the Number of Documents per Information Source in ClueWeb-Wiki. The number of bins is 30, the
number of documents ranges from 4,400 to 434,525.

• TREC123-100col-bysource (TREC123): 100 collections (information sources) were created from TREC
CDs 1,2 and 3 [4]. They are organized by publication
source and publication date. This testbed comes with
100 queries (TREC topics 51-150) with judgments.

the number of queries; the max, min and average of the
number of relevant documents per query.
The ClueWeb 09 is a large-scale collection of web documents that was collected in January and February 2009.
The entire dataset consists of about one billion web pages
in ten languages. For its tremendous size, the ClueWeb has
been used in several tracks of the Text REtrieval Conference
(TREC), most notably in the Web track. For distributed
environment (in a different problem setting), ClueWeb has
been used in [15]. It is desired to construct a new dataset
based on ClueWeb for experiments in federated search.

• TREC4-100col-Kmeans (TREC4-Kmeans): 100
collections were created from the TREC 4 data. A twopass K-means clustering algorithm is used to organize
the dataset by topic [33]. This testbed comes with 50
queries (TREC topics 201-250) with judgments.
• Wikipedia-100col-Kmeans (ClueWeb-Wiki): 100
collections were created from the Wikipedia dataset
of the ClueWeb [13]. Similar to TREC4-Kmeans, we
applied clustering algorithm [33] to divide the dataset
into 100 collections. This testbed comes with 106 queries
with judgments1 .

5.1

Three Web tracks of TREC (from 2009 to 2011) have
been using the ClueWeb so far. Each track has provided 50
queries based on which we build the new testbed. Within
the full ClueWeb dataset, Wikipedia is the main contributor
of relevant documents for Web track queries. The total size
of Wikipedia is about 6 million documents, which is reasonable for creating a separate testbed. We extract all Wiki
documents, and apply the same K-means algorithm that was
used for creating the TREC4-Kmeans. We also select only
106 queries which contain at least one relevant Wikipedia
document (out of the 150 provided queries) for training and
testing. In the end, we constructed 100 information sources
for the testbed ClueWeb-Wiki, with statistics provided in
Table 2. The distribution of source sizes is also shown in

ClueWeb Wikipedia Dataset for Federated
Search

Table 2 provides more statistics of the three datasets, including sizes, number of information sources; the max, min
and average of the number of documents of each source. We
also provide the query statistics of each dataset, including
1
The
partition
assignments
are
available
http://www.cs.purdue.edu/homes/dthong/clueweb

at

827

Table 3: High-precision result on TREC123 with 300 sample
documents and top 5 information sources selected for each
query. A * denotes a significant difference at level p < 0.05
compared to the original SAFE algorithm using Indri as the
only centralized retrieval algorithm.

Figure 1. Most of the sources have less 70,000 documents,
and there are 12 sources of more than 100,000 documents.

5.2

Experiment Configuration

Given a set of information sources, we assign each source
a retrieval algorithm chosen from a set of: vector space
TF.IDF with “ltc” weighting [21], a unigram statistical language model with linear smoothing (with smooth parameter
as 0.5) [16] and Okapi [20] in a round robin manner. This
choice is based on the fact that those models are commonly
and widely used in information retrieval. Each information
source is set to return at most 200 documents for each query.
At the centralized sample database, we utilize five models:
the three above models, the Inquery [5] and the Indri [28]
algorithm. All retrieval algorithm implementations use the
Lemur Toolkit [14]. We randomly select a set of queries
for training, and used the other set for testing. For the
TREC123, there are 50 training queries out of 100; those
numbers of TREC4-Kmeans and ClueWeb-Wiki are 25 out
of 50 and 50 out of 106.

Doc
Rank
@5
@10
@15
@20
@30

Doc
Rank
@5
@10
@15
@20
@30

Our metrics for the performance is the high-precision at
document level, which is the percentage of the number of
relevant documents in the final merged ranked list. Given
that list, we measure the precision at top 5, 10, 15, 20 and
30 respectively. In next section, we will present our experimental results of all datasets.

6.1

0.332
0.272
0.267
0.251
0.229

TREC123
LR
MoRM
(+ 23.88 %)
0.344 (+ 28.36
(+ 10.57 %)
0.304 (+ 23.58
(+ 16.31 %)
0.279 (+ 21.54
(+ 20.67 %) * 0.261 (+ 25.48
(+ 9.95 %)
0.232 (+ 11.54

%)
%)
%)
%)
%)

*
*
*
*

Table 4: High-precision result on TREC4-Kmeans with 300
sample documents and top 5 information sources selected
for each query. A * denotes a significant difference at level
p < 0.05 compared to the original SAFE algorithm using
Indri as the only centralized retrieval algorithm.

We choose K, the number of latent groups, to be 3 in
our main results. Some experimental results with different
K values are also presented. For each information source,
we sample at most 300 documents for creating the centralized sample database. For each query, we use ReDDE to
select the top 5 sources for TREC123, TREC4-Kmeans and
ClueWeb-Wiki.

6.

SFI
0.268
0.246
0.229
0.208
0.208

SFI
0.272 0.280
0.244 0.252
0.211 0.227
0.192 0.212
0.177 0.193

TREC4-Kmeans
LR
MoRM
(+ 2.94 %) 0.296 (+ 8.82 %)
(+ 3.28 %) 0.256 (+ 4.92 %)
(+ 7.59 %) 0.227 (+ 7.59 %)
(+ 10.42 %) 0.216 (+ 12.50 %)
(+ 9.02 %)
0.192 (+ 8.29 %)

Table 5: High-precision result on ClueWeb-Wiki with 300
sample documents and top 5 information sources selected
for each query. A * denotes a significant difference at level
p < 0.05 compared to the original SAFE algorithm using
Indri as the only centralized retrieval algorithm.
Doc
Rank
@5
@10
@15
@20
@30

EXPERIMENTAL RESULTS
High-precision Results

We now present the high-precision results on the above
three testbeds. Tables 3-5 show the high-precision results
on TREC123, TREC4-Kmeans and ClueWeb-Wiki respectively. The first column is our baseline using SAFE algorithm with Indri [18] as the single centralized retrieval algorithm. SAFE has been demonstrated to generate accurate
and robust results compared with SSL and other results
merging algorithms. We denote this method as SFI. The
LR column presents the results using the logistic regression
model to learn the combination weights of all centralized
retrieval methods. The last column MoRM presents the results using the proposed mixture of retrieval algorithms. All
precision results of LR and MoRM are compared with the
baseline SFI using paired t-tests at level p < 0.05.

SFI
0.168
0.146
0.139
0.132
0.111

0.182
0.163
0.164
0.150
0.121

ClueWeb-Wiki
LR
MoRM
(+ 8.46 %) 0.204 (+ 21.26
(+ 11.00 %) 0.173 (+ 18.31
(+ 17.95 %) 0.168 (+ 20.53
(+ 13.55 %) 0.153 (+ 15.59
(+ 9.67 %) 0.123 (+ 10.75

%)
%)
%)
%)
%)

have shown the advantage of using multiple centralized retrieval algorithms for learning comparable document scores,
over the previous model that uses only one single centralized retrieval algorithm. It also demonstrates the advantage
of using the mixture model of multiple sets of weights over
the logistic regression model that uses only one single set of
combination weights.
For the Wikipedia dataset based on ClueWeb, the proposed model also consistently outperforms SFI and LR. The
differences however are not significant. Such a significance
may be harder to achieve, since on average, this dataset
contains less relevant documents per query than the other
datasets, as shown in Table 2.

For TREC123, the performance of MoRM is significantly
better than that of SFI. MoRM is also consistently better
than the performance of logistics regression model. In general, both learning methods show improvements over the
baseline method of one single feature. For TREC4-Kmeans,
MoRM also outperforms SFI, although the differences are
not significant as in TREC123. This can be explained as
in TREC4-Kmeans, we only train on 25 queries, whereas in
TREC123, we trained on 50 queries. These above results

6.2

Experiments with Different Number of Latent Variables

In this section, we discuss the experimental results when
the number of latent variable K changes. We only report

828

0.21

0.36
K=1
K=3
K=5
K=10

0.34

K=1
K=3
K=5
K=10

0.2
0.19

0.32
Precision Value

Precision Value

0.18
0.3

0.28

0.17
0.16
0.15

0.26
0.14
0.24
0.13
0.22
5

10

15
20
Document Rank

25

30

0.12
5

(a) TREC123

10

15
20
Document Rank

30

(b) ClueWeb-Wiki

Figure 2: High-precision Results of TREC123 and ClueWeb-Wiki with Different Number of Latent Variables

TREC123 and ClueWeb-Wiki for these experiments, and try
different configuration of K = {1, 3, 5, 10}. Similar pattern
can be observed on the TREC4-Kmeans. K = 1 is actually
equivalent to the logistic regression model. Figure 2 shows
the results of this experiment. It can be seen that the mixture of retrieval algorithms model is quite consistent with
a small range of K values. For ClueWeb-Wiki, the performance of the mixture model with K > 1 is at least equal or
higher than that of the logistic regression. For TREC123,
the performances with different values of K are also stable
for most of the test levels.

7.

tomizing the prior distribution πk independently for each
source, which means a source will be associated with a set
of combination weights independent of the others. However,
this model could require a larger training dataset to learn
the parameters. A hybrid model where a cluster of similar
sources independently uses multiple sets of weights is more
feasible. The similarity between sources will play an important factor in creating those clusters. Another direction is to
build a mixture model based on cluster of queries instead of
cluster of sources, in which each query will trigger a different set of combination weights of all features. Furthermore,
we can combine both of the above methods. It is also interesting to explore other types of evidence, such as the links
between documents from different sources and incorporate
them into the learning model.

CONCLUSION & FUTURE WORK

This paper proposes a novel method of mixture model
with multiple centralized retrieval algorithms for result merging in federated search. Existing result merging algorithms
do not fully address the issue of heterogeneity of information sources in federated search. Their arbitrary choices of
a single centralized retrieval algorithm suffer from the fact
that information sources are inherently different in source
statistics, query processing techniques, and/or document retrieval algorithms. The proposed model attempts to combine various evidence from multiple centralized retrieval algorithms in a mixture model framework, in order to map
source-specific document ranks to comparable scores for result merging. We have shown that a single set of combination weights of the evidence do not offer enough flexibility in dealing with such a heterogeneous environment.
A mixture model that learns multiple sets of combination
weights according to the clusters of sources proves to be a
better choice. A set of experiments has been conducted with
two traditional TREC datasets and a new dataset based
on the ClueWeb. The empirical results in three datasets
have demonstrated the effectiveness of the proposed mixture model with multiple centralized retrieval algorithms.

8.

ACKNOWLEDGMENTS

This work is partially supported by NSF research grants
IIS-0746830, CNS- 1012208 and IIS-1017837. This work also
partially supported by the Center for Science of Information
(CSoI), an NSF Science and Technology Center, under grant
agreement CCF-0939370.

9.

REFERENCES

[1] J. Arguello, J. Callan, and F. Diaz.
Classification-based resource selection. Proceeding of
the 18th ACM conference on Information and
knowledge management, pages 1277–1286, 2009.
[2] M. Baillie and M. Carman. A multi-collection latent
topic model for federated search. Information
Retrieval, 14(4):390–412, Aug. 2011.
[3] M. Bergman. The deep web: surfacing the hidden
value. Technical report, 2001.
[4] J. Callan. Distributed information retrieval. Advances
in Information Retrieval, pages 127–150, 2000.
[5] J. Callan, W. B. Croft, and S. M. Harding. The
inquery retrieval system. In Proceedings of the Third

This model could be extended in many ways. For instance, we could add more flexibility to the model by cus-

829

[6]

[7]

[8]
[9]

[10]

[11]
[12]

[13]
[14]
[15]

[16]

[17]

[18]

[19]

[20]

International Conference on Database and Expert
Systems Applications, 1992.
N. Craswell, D. Hawking, and P. Thistlewaite. Merging
results from isolated search engines. In Proceedings of
the 10th Austrlasian Database Conference, 1999.
A. P. Dempster, N. M. Laird, and D. Rubin.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society,
39(B):1–38, 1977.
R. Fletcher. Practical methods of optimization, volume
1. Wiley, 1987.
L. Gravano, C.-C. K. Chang, H. Garcia-Molina, and
A. Paepcke. Starts: Stanford proposal for internet
meta-searching. In Proceedings of the ACM-SIGMOD
International Conference on Management of Data
(SIGMOD). ACM ACM ACM ACM, 1997.
L. Gravano, H. Garcia-Molina, and A. Tomasic. Gloss:
text-source discovery over the internet. ACM
Transactions on Database Systems (TODS),
24(2):229–264, 1999.
J. Gross. Linear regression, volume 175. Springer
Verlag, 2003.
C. He, D. Hong, and L. Si. A weighted curve fitting
method for result merging in federated search. In
Proceedings of the 34th international ACM SIGIR
conference on Research and development in
Information Retrieval, SIGIR ’11, pages 1177–1178,
New York, NY, USA, 2011. ACM.
http://lemurproject.org/clueweb09/. The clueweb09
dataset.
http://www.lemurproject.org/. The lemur toolkit.
A. Kulkarni and J. Callan. Document allocation
policies for selective searching of distributed indexes.
Proceedings of the 19th ACM international conference
on Information and knowledge management, pages
449–458, 2010.
J. Lafferty and C. Zhai. Document language models,
query models, and risk minimization for information
retrieval. Proceedings of the 24th annual international
ACM SIGIR conference on Research and development
in information retrieval, pages 111–119, 2001.
W. Meng and C. Yu. Advanced metasearch engine
technology. Synthesis Lectures on Data Management,
2(1):1–129, 2010.
D. Metzler and W. B. Croft. Combining the language
model and inference network approaches to retrieval.
Information Processing and Management,
40(5):735–750, 2004.
Y. Rasolofo, F. Abbaci, and J. Savoy. Approaches to
collection selection and results merging for distributed
information retrieval. Proceedings of the tenth
international conference on Information and
knowledge management, pages 191–198, 2001.
S. Robertson, S. Walker, S. Jones,
M. Hancock-Beaulieu, and M. Gatford. Okapi at
trec-3. NIST SPECIAL PUBLICATION SP, pages
109–109, 1995.

[21] G. Salton, E. Fox, and H. Wu. Extended boolean
information retrieval. Communications of the ACM,
26(11):1022–1036, 1983.
[22] M. Shokouhi. Central-rank-based collection selection
in uncooperative distributed information retrieval.
Advances in Information Retrieval, 2007.
[23] M. Shokouhi and L. Si. Federated search. 2011.
[24] M. Shokouhi and J. Zobel. Robust result merging
using sample-based score estimates. ACM
Transactions on Information Systems (TOIS),
27(3):1–29, 2009.
[25] X. M. Shou and M. Sanderson. Experiments on data
fusion using headline information. In Proceedings of
the 25th annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ’02, pages 413–414, New York, NY, USA, 2002.
ACM.
[26] L. Si and J. Callan. Relevant document distribution
estimation method for resource selection. Proceedings
of the 26th annual international ACM SIGIR
conference on Research and development in
informaion retrieval, pages 298–305, 2003.
[27] L. Si and J. Callan. A semisupervised learning method
to merge search engine results. ACM Transactions on
Information Systems (TOIS), 21(4):457–491, 2003.
[28] T. Strohman, D. Metzler, H. Turtle, and C. W. B.
Indri: A language model-based search engine for
complex queries. In Proceedings of the International
Conference on Intelligence Analysis, 2004.
[29] P. Thomas. Server selection in distributed information
retrieval: a survey. To appear in: Journal of
Information Retrieval, 2012.
[30] M. Tsai, H. Chen, and Y. Wang. Learning a merge
model for multilingual information retrieval.
Information Processing & Management,
47(5):635–646, 2011.
[31] Y. Wang and D. J. DeWitt. Computing pagerank in a
distributed internet search system. In VLDB ’04:
Proceedings of the Thirtieth international conference
on Very large data bases, pages 420–431. VLDB
Endowment, 2004.
[32] S. Wu, Y. Bi, and X. Zeng. The linear combination
data fusion method in information retrieval. In
Database and Expert Systems Applications, pages
219–233. Springer, 2011.
[33] J. Xu and J. Callan. Effective retrieval with
distributed collections. Proceedings of the 21st annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 112–120,
1998.
[34] J. Xu and W. B. Croft. Cluster-based language models
for distributed retrieval. Proceedings of the 22nd
annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 254–261, 1999.

830

