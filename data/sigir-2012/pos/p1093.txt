Incorporating Statistical Topic Information in Relevance
Feedback
∗

Karla Caballero

UC, Santa Cruz
Santa Cruz CA, USA

karla@soe.ucsc.edu

Ram Akella
UC, Santa Cruz
Santa Cruz CA, USA

akella@soe.ucsc.edu

ABSTRACT

Relevance Feedback,Topic Models, Language Models

Previous approaches have used statistical topic models to
represent documents according to their latent topic content
and use this representation in information retrieval [1, 4].
Authors in [4], use topics as a form of smoothing the Language Model used in retrieval. However, this approach does
not address the incorporation of relevance feedback. Recent
work from [1] explores the use of topics as a form to perform
the query expansion for relevance feedback. However, this
action might make the query noisier because the top topic
terms might not contribute to a better discrimination of the
relevant documents. In addition, these terms might not be
distinctive across diﬀerent topics.
We propose to include the topic information as feedback
using the document topic mixture instead of the document
word mixture. We ﬁrst estimate the topic mixture for each
document in the corpus using LDA and save it as meta
data. Given an initial query we use a standard retrieval
engine, Language Models for this case, to show the ﬁrst set
of documents to the user and obtain relevance judgments.
Then, we assume that topic mixtures for feedback documents are observed. We then deﬁne two latent Dirichlet distributions: one for relevant documents and another for nonrelevant documents. We ﬁt these distributions iteratively,
by ﬁnding a suﬃcient statistic and maximizing the likelihood of observing this statistic. To score the documents, we
use Bayesian Logistic Regression. This function results in a
very eﬃcient scoring function, and incorporates the beneﬁts
of active learning. Under this model, we incorporate positive and negative feedback, and context based on topics in
the interaction without changing the query. We also provide
eﬃcient updates of the latent distributions based on topics.

1.

2. METHODOLOGY

Most of the relevance feedback algorithms only use document terms as feedback (local features) in order to update
the query and re-rank the documents to show to the user.
This approach is limited by the terms of those documents
without any global context. We propose to use statistical
topic modeling techniques in relevance feedback to incorporate a better estimate of context by including global information about the document. This is particularly helpful for
diﬃcult queries where learning the context from the interactions with the user is crucial. We propose to use the topic
mixture information obtained to characterize the documents
and learn their topics. Then, we rank documents incorporating positive and negative feedback by ﬁtting a latent distribution for each class of documents online and combining
all the features using Bayesian Logistic Regression. We show
results using the OHSUMED dataset for 3 diﬀerent variants
and obtain higher performance, up to 12.5% in Mean Average Precision (MAP).

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval—Relevance Feedback, Document
Filtering

General Terms
Algorithms, Experimentation

Keywords
INTRODUCTION

Relevance feedback has been studied extensively in Information Retrieval as a form of incorporating feedback from
the user to reﬁne the results retrieved. The authors in [5]
concluded that negative feedback is also valuable to improve
the ranking. However, the need to capture broader context
in diﬃcult queries is still a challenge. The authors in [2]
have showed that including global features and using clusters
can improve the retrieval performance signiﬁcantly. Thus,
statistical topic modeling provides a robust and automatic
method to incorporate context to the user feedback.

In this section we describe how we incorporate the topic
mixture of feedback documents as a global measure in contrast to query expansion. To achieve this, we estimate the
topic mixtures of the documents, θi , for K topics in the corpus using LDA oﬀ-line. Given a initial ranking, the user
provides relevance feedback which is used to ﬁt the latent
relevant/non-relevant distributions of topic mixtures. Thus,
we assume two Dirichlet distributions: one for the relevant
set of documents αR , and one for the non-relevant documents αR̄ . Therefore, we have:

K
Γ( K αk,R )  αk,R −1
P (θi |αR ) ∼ Dirichlet(αR ) = K k=1
θi,k
k=1 Γ(αk,R ) k=1

∗Main contact.
Copyright is held by the author/owner(s).
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
ACM 978-1-4503-1472-5/12/08.

for αR and αR̄ . Then, we calculate the log-probability of
the document being generated by those distributions:

1093

LogP (θi |α) = log Γ(

K


αk )−

k=1

K


log Γ(αk )+

k=1

K


(αk −1) log(θi,k )

k=1

We denote these scores as P Ri and P R̄i respectively. To
update the latent distributions of the relevant αR and nonrelevant topics αR̄ , we use the topic content from the documents labeled by the user as relevant, θR , and non-relevant,
θR̄ , after each interaction. The Dirichlet distribution guarantees a unique maximum when the Maximum Likelihood
(ML) is estimated for α. Moreover, a suﬃcient statistics,
SS, can be estimated to update this distribution as more
observations are available. We can update SS eﬃciently
without keeping previous document feedback. The initial
(0)
value of the suﬃcient statistic SSk,R for the relevant topic
k and its update from the interaction j is described by:
1 
(0)
log θi,k
SSk,R = (0)
NR i∈R0
(j)

SSk,R =
(j)

(j−1)

NR

(j)
NR

(j−1)

SSk,R

+

1



(j)
NR i∈Rj

LM+ST W ; LM+ST W +P R; LM+ST W +P R+P R̄. We calculate the initial ranking using LM and asked for feedback
until we have at least one relevant and one non-relevant
documents. We use 10 feedback documents and estimate
precision at 10 (P@10), Mean Average Precision (MAP),
and Discounted Gain (DiscGain). There are two relevance
level labels available in the dataset, {1, 2}, that are assumed
equally, {+1} for P@10 and MAP. However for DiscGain,
we use both labels in the evaluation.
Table 1 shows the results for the variants tested. We observe that the LM+ST W performs better than the baseline.
This score is similar to the LDA-based retrieval proposed
in [4] but the value of the linear combination parameters β
is ﬁtted based on the feedback as opposed to a corpus-wide
parameter. When we incorporate only the score from the relevant distribution of topics P R, the performance decreases.
However, when the score for the non-relevant distribution
P R̄ is incorporated, the performance is the highest. This
shows the value of negative feedback reported previously
in [5]. We notice that, the combination of P R and P R̄ is
equivalent to the log of the likelihood ratio test (probabilistic ranking principle) weighted by β. This also explains why
both scores should be included in the model.
We observe that the combination of the four scores improves the general performance by: 11.6% P@10, 12.8%
MAP, and 4.6% DiscGain respect topic-based language model
(LM+ST W ). This demonstrates, the power of statistical
topic modeling in relevance feedback.

log θi,k

(j−1)

where NR = NR
+ |Rj |, and |Rj | is the total number
(j)
of relevant documents at the j-th interaction. Given SSR
(j)
and SSR̄ , we use the method proposed in [3] to calculate
(j)
(j)
the ML estimator for αR and αR̄ . In addition to these
distributions, we use the topic-based Language Model PT W
for document i as follows:
K

PT W,i (w|θi , φ̂) =
P (w|z = k, φˆk )P (z = k|θi )
k=1

where φ̂ are the word mixture for the topics obtained from
LDA. Thus, the score ST W,i for query Q with terms q is
deﬁned as:

PT W,i (w = q|θi , φ̂)
ST W,i =
q∈Q

To combine the scores from the latent relevant/non-relevant
topic mixtures and the topic-based Language Model, we use
the Bayesian Logistic Regression approach [5]. Let yi =
{+1, −1} be the relevant/non-relevant label for document,
we have the score function:
1
P (yi |β, di ) =
1 + exp(−β T di yi )
where di is the feature vector scores: P Ri , P R̄i , ST W,i . β
is a parameter vector assumed to be normally distributed
and updated in a Bayesian form. Here, the distribution of β
from the j-th iteration is taken as prior distribution for the
next iteration. To approximate the posterior distribution we
use the Laplace approximation as discussed in [5].

3.

Table 1: Results of Topic feedback using 50 topics
in the OHSUMED dataset
Method
P@10
MAP
DiscGain
LM
0.3968
0.4286
0.5660
LM +ST W
0.4206
0.4557
0.6315
LM +ST W +P R
0.2968
0.3307
0.5590
LM +ST W +P R+P R̄ 0.4698 0.5141
0.6580

4. CONCLUSION AND FUTURE WORK
We have presented a method to incorporate statistical
topic information in relevance feedback without changing
the query. Results show that including the mixture of topics
in relevance feedback improves the performance by pruning
the search space, and adding context to the query. As future work, we plan to incorporate a policy to decide when
to update the parameters of the relevant and non-relevant
topic distribution optimally.

5. ACKNOWLEDGMENTS
This work is partially funded by CONACYT grant 207751
and SAP Gift Support

6. REFERENCES

[1] D. Andrzejewski and D. Buttler. Latent topic feedback for
information retrieval. In Proceedings of the 17th ACM
SIGKDD conference, KDD ’11, pages 600–608, 2011.
[2] K. S. Lee, W. B. Croft, and J. Allan. A cluster-based
resampling method for pseudo-relevance feedback. In
Proceedings of the SIGIR conference, pages 235–242, 2008.
[3] T. Minka. Estimating a dirichlet distribution. Technical
report, 2003.
[4] X. Wei and W. B. Croft. Lda-based document models for
ad-hoc retrieval. In Proceedings of the 29th ACM SIGIR
conference, SIGIR ’06, pages 178–185, 2006.
[5] Z. Xu and R. Akella. A bayesian logistic regression model for
active relevance feedback. In Proceedings of the 31st ACM
SIGIR conference, SIGIR ’08, pages 227–234, 2008.

RESULTS

We test our method using the OHSUMED dataset which
consists of 196, 000 medical abstracts and 3, 506 relevance labels for 63 queries from the Document Filtering Track from
TREC 4. As suggested in the track, we assume unobserved
labels as non-relevant. We ﬁt the LDA model using K = 50
topics, which is the number of topics with highest performance based on Empirical Likelihood. To test the impact of
topic information, we use standard Language Model (LM)
with Dirichlet smoothing described in [4] as baseline. This
score is used with Bayesian Logistic Regression. We test
3 variants of the model and the baseline: LM as baseline;

1094

