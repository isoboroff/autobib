On Per-topic Variance in IR Evaluation
Stephen E. Robertson

Evangelos Kanoulas

Microsoft Research
7 JJ Thomson Avenue
Cambridge CB3 0FB, UK

Information School
University of Sheffield
Sheffield, UK

ekanoulas@gmail.com

stephenerobertson@hotmail.co.uk

ABSTRACT

more global way, the set of measurements that we take in
a test, averaged over a set of topics, are the result not of
a simple sampling of queries/topics, but of the interaction
between a sampling of queries/topics and a separate sampling of documents. In [3] this question was investigated in
an empirical way, via bootstrap experiments on experimental results; [9] was concerned with more theoretical issues
involving the nature of IR evaluation metrics, and appealed
only to very limited simulation experiments based on simple
models. In some sense the present paper takes a position
between these two, with experiments based on a variety of
models starting from bootstrapping from empirical results.
The primary question being investigated in this paper is
the question of how we should think about the reliability of
individual per-topic measurements. The question is tricky,
and the paper does not arrive at any firm conclusions. However, it is hoped to shed some light on the question. Specifically, we consider tests of statistical significance for differences between systems. An approach that can make use of
per-topic noise is the mixed-effects models. We propose and
investigate a linear mixed-effects models approach to significance.

We explore the notion, put forward by Cormack & Lynam
and Robertson, that we should consider a document collection used for Cranfield-style experiments as a sample from
some larger population of documents. In this view, any pertopic metric (such as average precision) should be regarded
as an estimate of that metric’s true value for that topic in the
full population, and therefore as carrying its own per-topic
variance or estimate precision or noise. As in the two mentioned papers, we explore this notion by simulating other
samples from the same large population. We investigate different ways of performing this simulation. One use of this
analysis is to refine the notion of statistical significance of a
difference between two systems (in most such analyses, each
per-topic measurement is treated as equally precise). We
propose a mixed-effects model method to measure significance, and compare it experimentally with the traditional
t-test.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]
General Terms: Experimentation, Measurement, Performance
Keywords: information retrieval, evaluation, statistical precision, significance testing, mixed-effects model, simulation

1.

2.
2.1

INTRODUCTION

BACKGROUND
Cormack & Lynam

In this paper [3], a TREC collection of documents was
split randomly into two parts. For each topic, the evaluation
result (in the form of the value of a metric M such as average precision, AP ) in first half of the collection was taken as
predicting the value of M in the second half. In order to estimate the variance (or statistical precision) associated with
this prediction, the authors took repeated bootstrap samples from the first half of the collection, and re-measured M
on each sample. The distribution of M values over samples
(single topic, single original set of results) provides an indication of the statistical precision required. They concluded
that this procedure does indeed adequately model the variance inherent in an AP estimate for an individual topic in a
document collection of a certain size, regarded as a sample
from an infinite collection of potential documents (so that
the split collection is seen as two independent samples from
this population).
Certain technical issues arise: first, it is better to use
M = logit(AP ) rather than M = AP , as it has better distributional properties; second, in the bootstrap sampling it is
necessary to treat very small values of R, the total number

In two recent papers, by Cormack and Lynam [3] and
Robertson [9], it has been suggested that in Cranfield-style
test-collection-based IR evaluations, we should in general
consider the document collection used as a sample from a
population (in addition to considering the set of topics or
queries thus). Such a view suggests (among other things)
that each per-topic measurement (such as of Average Precision) should be considered as carrying its own statistical
precision, or noise, and that this noise might be different for
every topic. This potentially complicates the issue of establishing statistical significance in IR tests. Looked at in a

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

891

of relevant documents in the observed collection, in special
ways. These are discussed further below.

2.2

ple results that we have. A start in exploring these issues
is made in the two papers cited above; the present paper
attempts to take this analysis a step further.
A basic premise of the approach taken in the two cited
papers is that the population of documents looks different
from the point of view of each separate topic – in particular,
with regard to relevance. The simulations in both papers are
based on modelling the document population separately for
each topic. An alternative approach would be to model the
population of documents independently of the topics, and
then model the interaction of each topic with that population. Such an approach would be interesting indeed but very
hard. It would also have to model the interaction of each
system with the population, separately applied to each topic.
The big advantage of the per-topic model of the population
is that both relevance and system responses (e.g. scores)
can be taken as given. Furthermore, it serves to emphasise
an important aspect of the document sample view of IR experiments. While the entire document collection is typically
very large (from which one might be tempted to conclude
that statistical precision issues do not arise), from the point
of view of each topic we see a (typically much smaller) sets of
relevant or retrieved documents, on which all the measures
depend. Precision at cutoff 5, for example, should be seen
as a very imprecise measurement (in the statistical sense)
for a single topic.

Robertson

In this paper [9], different metrics M are investigated theoretically, with a view to establishing whether it makes sense
to pose the infinite-document-population question in terms
of each metric. Some metrics turn out not to be understandable in these terms; a metric as defined on a given finite population may not be generalisable to an infinite population.
In particular, for example, there appears to be no equivalent
to NDCG defined on an infinite population. AP , however,
can be so generalised.
Even if it is generalisable, we should ask the question: is
the metric value observed on the given finite population a
good estimate of its ‘true’ value in the infinite population?
Some limited simulation experiments, using some previously
proposed models of score distributions for relevant and nonrelevant documents, are made in order to investigate this
question. It appears from both theoretical arguments and
these simulation experiments that some such metrics may
provide biased estimates of their true population values. AP
seems to be one of these: AP observed on a given finite document population is likely to be an over-estimate of ‘true’
AP on the infinite population of which the observed finite
population is a sample. The reason postulated for the bias
in AP is as follows. The infinite population equivalent is defined as a particular form of integral over (area under) the
infinite-population recall-precision curve, which we expect to
be smooth. However, in estimating it for an observed sample, we choose to measure the height of the recall-precision
curve only at the points in the ranking where relevant documents occur, i.e. exactly at the peaks of the noisy sample
curve.

2.3

2.4

Variation between topics

It is well understood in the IR research community that
there is a great deal of variation between topics. For example, the analysis of variance in the paper by Banks et al. [1]
finds the topic effect to be greater than the system effect (as
well as a significant interaction effect). The Robust track
at TREC was devoted to investigating hard topics, in the
recognition that there are significant numbers of topics that
appear to be generally hard for systems to perform well on.
If it is accepted that the document collection should be
regarded as sampled in some way from a population, then as
indicated above, each per-topic measurement has some builtin error of estimation because the measurement is based on
the sample rather than the population. One of the aims of
this paper is to get a handle on this estimation error. In particular, we ask the question: does this estimation error vary
substantially between topics? If this variation is substantial,
should this affect how we analyse test results?

Topics and documents

The usual approach to system evaluation in IR experiments is to choose a metric defined on the results of searching on an individual topic or query, and take an average of
this metric over the set of topics, for the system concerned
(there are many such metrics in common use, including AP
and NDCG). The usual approach to statistical significance
is to consider the results for the individual topics as the
(indivisible) units of measurement. So suppose, for example, that the measure is RPrec and the significance test is
the sign test, the fact that system A performs better than
system B by RPrec on topic 1 counts as a definitive result.
The significance question arises only at the aggregation-overtopics level (‘does system A perform better than system B
on significantly more topics?’). We do not ask the question
as to whether or in what sense the individual topic 1 result
was significant. This approach effectively assumes that the
topics were sampled from some population of (possible or
actual) topics, but that the document collection is fixed in
stone for all time.
Stated like this, it must be clear that this (the document
collection part) is a rather drastic assumption. In fact it is
in part mitigated by the general view in IR research that a
result is only good if it works on multiple test collections.
But this is a rather crude approach to the problem. It would
be good to have a much better understanding of what each
individual topic result is telling us, and of how best to draw
general conclusions from the topic sample × document sam-

2.5

Scores and simulations

The method of simulation used in [3] is based on bootstrap
ideas. We start with a given system and topic and the corresponding set of results (documents retrieved and ranked)
from a real test collection of documents. Then we model
the process of drawing other samples from the notional infinite population of documents by repeated sampling with replacement from these actual documents. The method used
in [9] is a parametric approach based on score distributions.
On the back of previous work on fitting distributions to the
scores of relevant and non-relevant documents respectively,
these distributions are then taken as representing the notional population of documents. We can repeatedly sample
from the distributions rather than from any specific documents.
It is clear that the score-distribution approach implies a
smoothing effect which the bootstrap method does not have.

892

Modelling the population as consisting only of instances exactly like the ones we have already seen seems to go against
at least part of the basic idea of samples and populations
– that any sample we take consists only of examples. We
should in some sense expect the population to include other
kinds of examples, not represented directly in the sample
we have. On the other hand, the use of score distributions
typically involves strong assumptions about the nature of
the distributions, and also involves some dependence on the
particular scoring methods used by systems. Systems differ
greatly not only in the kinds of scores they produce, but
also in whether they produce a consistent set of scores at
all. For example, some systems may do an implicit AND
on the query terms, and then use a scoring function which
is defined only within the AND set. Others may rank by
rule or on the basis of local comparisons which do not result
in an overall score. These facts clearly limit the scope of
score-distribution modelling – as against the advantage of
smoothness.
In this paper, accepting the limitations of the score distribution approach, we nevertheless use it to investigate the
smoothness issues. That is, we try to characterise how assuming a smoother population might affect what we can
infer from the sample. But we look at a range of scoredistribution methods, of which one extreme (no smoothing)
corresponds to the Cormack and Lynam bootstrap method.
Thus taking the scores of the retrieved real documents as
given and performing bootstrap samples on them is equivalent to bootstrapping from the documents themselves1 . Then
we introduce various levels of smoothing, from minimal Gaussian noise to fully-fledged parametric distribution fitting.
An alternative method not pursued in this paper might be
to smooth by introducing noise into the documents themselves. Some such methods have been used in the context of
query performance prediction [11].

3.

collections sampled from the same universe, the only way we
can discover how such repetitions might be distributed is by
simulation.

4.

DATA AND METRICS

The document collection used in the experiments reported
here is the one contained in TREC Disk 4 and 5, excluding
the Congressional Record sub-collection, together with the
50 topics used for the TREC 8 Ad Hoc track. The system
runs used are a new set, similar to that used in [6], consisting
of a series of 44 runs on the Terrier system. Four different
weighting models were used (BM25, the Hiemstra language
model, PL2 and TF*IDF), and eleven different query versions (title, title+description, title+description+narrative,
and eight query expansion runs using pseudo relevance feedback, expanding the query by between 4 and 512 terms).
This set may be characterised in various ways: (a) all the
systems (models) generate fairly traditional forms of scores;
(b) all runs use the same initial parsing (including Porter
stemming and stopword removal); (c) the collection of runs
constitutes a systematic matrix of certain variables (weighting schemes and forms of query); (d) all other system variables are held constant.
In the event, four pairs of runs gave the exact same AP
values and thus only one run from each pair was used in
the experiments while the other was discarded. Thus the
analysis below is based on 40 runs.
The primary metric used in this analysis is logit(AP ),
where AP is average precision. As indicated above, following [3], the distributional properties of logit(AP ) seem to be
better than those of AP ; in particular, because (theoretical)
range of logit(AP ) is (−∞, ∞), it makes sense to model distributions of repeated logit(AP ) measurements as Gaussian;
attempting to do the same with AP measurements results
in some logical inconsistencies. In [3], some evidence is presented that such distributions of logit(AP ) measurements
are indeed approximately Gaussian. However, it should be
noted that this does not resolve the issue raised in [9], that
of the bias in estimation from finite samples, which applies
to logit(AP ) as well as to AP . The use of logit necessitates some smoothing, to avoid infinities at either end – we
follow previous practice in using an epsilon correction. However, we note that a new more heavily smoothed version of
logit(AP ), yaAP, is proposed in [8]. This has better distributional properties than logit(AP ) itself, and may therefore
be a better candidate metric for the work reported here;
however, we have not yet rerun the analyses with yaAP.
The 44 runs have MAPs ranging from 0.273 to 0.131, with
a significant bunching to the top end (31 of the runs have
M AP > 0.24). If instead of taking the mean of AP , we
take the mean of logit(AP ) and then convert it back to the
AP scale by reversing the transformation, we see a different
ranking of runs and rather lower ‘AP ’ values, ranging from
0.188 to 0.034 (this is consistent with previous reported results using GMAP). Bunching of runs is not quite so strong
but still evident.

MOTIVATING HYPOTHESIS

To restate the hypothesis that motivates this paper, as
well as the two earlier ones:
A test document collection should be thought of
as a sample from some hypothetical universe of
possible documents.
As is common in statistical analysis of any experimental
results, we assume that the sample that we have is in some
way representative of the universe, which we take to be large
or infinite. The primary object of statistical analysis is to
draw inferences about this universe. This is not to say that
the test collection is representative of other real documents
in the world, but that such a view is a prerequisite for basic
statistical understanding. If we do not know how to draw
inferences correctly for this hypothetical universe, then we
certainly do not know how to draw inferences about the real
world.
The primary concern of this paper is with the variance (error) associated with each observation that we make (a given
metric on a given system-topic pair). Since we cannot normally make multiple repeated observations on different test

5.

1

We note that the assumption that a system would always
give the same score to a given document, even in the context
of a varying collection, is debatable: there is some discussion
in [4]. We do not pursue this question in the present paper.

5.1

SIMULATION MODELS
General simulation principles

In general, we take the results from each topic and system
run (the scores of the top 1000 ranked documents), split into

893

relevant and non-relevant. This is taken as the results from
searching a sample from some (assumed large or infinite)
population of documents, and we wish to simulate what the
search results from other samples from this same large population of documents would look like. Thus we need to infer
a model of the whole population of documents and resample from it. We avoid sampling from the entire supposed
population of documents (which would be intractable) by
sampling separately for the results of each topic. What follows is a brief description of the method, which is similar to
that used in [3], except that it separates the sampling of relevant and non-relevant documents/scores, in order to allow
various smoothing methods. It involves a number of approximations that deserve more discussion than is possible in the
present paper.
Suppose that the size of the original test collection is N ,
this topic has R known relevant documents, of which r are
retrieved in the top 1000 by this system run. For the purposes of the present experiment, we will assume that all samples from the large population of documents are the same
size, N . This assumption is not used directly, but is implicit
in what follows. We first take a stochastic decision about
how many relevant to include in our sample. This theoretically has two steps: deciding how many reldocs should
be in the entire sampled collection, and deciding how many
should be in the top 1000 retrieved set. The first step can
be based on the observed generality of the topic (proportion of the original test collection that is relevant, R/N ),
and the second can be based similarly on the proportion of
these retrieved by the system (r/R). In practice we combine
these two steps, and take a Poisson approximation because
R/N at least is assumed to be small. So we take a sample
from a Poisson distribution with mean r, to define the number rs of relevant retrieved for this topic in this simulation
run. We ignore instances where this number is zero (further
discussion below).
Next we take rs samples from some model of the distribution of relevant document scores, and 1000−rs samples from
some model of the distribution of non-relevant scores. The
details of how we do this vary between models. We sort the
1000 resulting scores into descending order, and treat them
as the ranked document scores from a new test collection,
sampled similarly to the original from the same large population. Since these scores are marked as ‘relevant’ or ‘not
relevant’, we can apply any IR metric to this ranked list.

5.2

tion model as producing reasonable unbiased results; it also
serves to demonstrate the large variation between the variances of different topics, thus confirming our initial premise
that different topics provide measurements with different associated precision (or estimation error or noise). Second, we
take C = 10 samples and submit them to an analysis of
variance process. This provides us with a way of conducting statistical significance tests which take account of this
variation between topics.

5.3

Simulation models

As indicated, the simulation models vary in how they
smooth the actual scores obtained from the system in modelling the two required distributions for each topic. In the
BST (Bootstrap) simulation, we sample with replacement
from the list of actual scores of the top 1000 documents,
split into relevant and non-relevant. In this simulation, the
scores are in effect labels only; it is therefore more-or-less
equivalent to the bootstrap method used in [3]. One difference is in the handling of the zero cases. These turned out
to be a problem in [3], because (a) the corpus used (TREC
6) included 5 topics with R < 5, and (b) the corpus was
split randomly into two parts for their experiments. In the
present corpus, there is one topic for which R = 6, and the
next lowest value is 13; furthermore, we do not split the
collection. Therefore it has been assumed to be safe not to
treat these cases in any special way, and to simply ignore
the bootstrap samples which would have rs = 0.
In the KDE (kernel density estimation) method, we add a
small amount of noise (following some simple distributional
assumption) to each datapoint, and mix the results into a
single smooth distribution. Different noise models may be
used, but a common form (used here) is Gaussian – this can
therefore be thought of as a Gaussian mixture model with
one Gaussian for each observed score. In the present paper, we make use of a Perl module [5], applied separately to
the relevant and non-relevant scores for each topic/returned
document combination. There are various methods of determining the variance to be applied to the noise model; these
are not discussed further in the present paper.
In the GMG (Gaussian Mixture and Gamma) model, developed by Kanoulas et al [6], the relevant distribution is
modelled as a mixture of Gaussians (usually 1, 2 or 3 Gaussians, determined by the data), and the non-relevant as a
Gamma distribution. Estimated parameters provided by
the fitting process are the means and variances of the Gaussians, their relative preponderance, and the shape and scale
parameters of the Gamma, as discussed in the cited paper.
As will be clear, simulation of IR evaluation evaluations is
a complicated process, deserving of much more discussion.
In the present paper, simulation is a means to an end, and
such discussion is deferred to a later paper.

Analysis

For each topic and each run (assume S systems or runs,
and T topics), we repeat the process described above some
number C of times. This results in C simulated values of the
metric M , for each topic and each run. In simulation, these
values represent the outcome of C different test collections,
all of the same size, all taken from the same infinite population of documents, all searched for the same topic by the
same system/run (meaning in this case the same query formulation and the same weighting and scoring method). We
note that we could also vary the size of the simulated collection to investigate the effect of such variation on evaluation
results.
In what follows, we use these C ×T ×S values of M in two
different ways. First, we compare the mean and variance of
M over C = 1000 samples with the observed value of M for
the real test collection. This is used to validate the simula-

6.

RESULTS OF THE SIMULATIONS

We present some results, as follows. In Figure 1 (simulation model BST, run 11), each point represents a single
topic; the x-axis is the observed logit(AP ) on the real test
collection, and the y-axis is the value of the same metric
in the samples. The points represent the means over all
C = 1000 samples, and the bars represent one standard
deviation either side of the mean. We note that (a) the simulated results are very close to the observed ones; (b), more
importantly, the standard-deviation bars are very variable.

894

tinguish it from the variability across topics when evaluating
retrieval systems in a comparative evaluation exercise.
Comparative systems-based evaluation typically uses a
statistical hypothesis test such as the t-test to infer the significance of the difference in the effectiveness of two retrieval
systems. However, the t-test is not adequate to handle multiple sources of variability into a systems-based evaluation.
On the other hand, mixed-effects models, which will be introduced in this section, can incorporate arbitrary sources
of variability into an analysis [7]. 2
We start with a simple model of a single source of variance (topic effect) and extend it to a model that better corresponds to our experiments of repeated measurements per
topic. To illustrate the analysis, we take as example the
simulated results for a single pair of systems.
The linear model that accounts only for the topic effect
is,
yij = βi + bj + ǫij
bj ∼ N (0, σ12 ),

ǫij ∼ N (0, σ 2 )

where yij is the value of an evaluation measure calculated
on topic j for system i, βi is the effect of system i, bj is the
effect of topic j, and ǫij is the residual error. The system
effect is a so-called “fixed effect” since the systems we would
like to compare are fixed and not a sample from some system
distribution. On the other hand the topic effect is a “random
effect”, assuming that the topic is actually sampled from a
population of topics.
The assumption behind this model is that the random
variable, bj , is independent and identically normally distributed with zero mean and σ12 variance. The residual error
is also assumed to be independent and identically normally
distributed with zero mean and σ 2 variance.
In the statistical programming environment R the following procedures fit the same linear effect model (Eq. 1) and
result in equivalent inferences:

Figure 1: Bootstrap simulation of run 11.
In relation to (a), we note also that this is as expected, since
the bootstrap simulation uses the real results very directly;
we have made no attempt to prevent overfitting. However,
we can clearly see consistency; it is entirely plausible, from
the means at least, that the observed results were drawn
from the distributions indicated. Once again, we defer more
detailed discussion of the fit to observation to a future paper
devoted to simulation. Run 11 is the best-performing run
according to this metric; Figure 2 shows similar graphs for
three more runs, distributed over the ranking of runs, with
very similar characteristics.
We note also that the per-topic standard deviation values
are similar for different runs. Taking pairwise correlations
between runs over the set of topic standard deviations, for
the above 5 runs, the correlation values range from 0.60 to
0.97 (the bottom of the range is raised significantly if we
ignore the worst-performing run). Thus (on the whole) the
same topics have the least precise estimates, or the greatest
noise, on the different runs. This result may well be affected
by the choice of runs for the present experiment: a more
varied set of runs might indicate less commonality in this
respect.
Figure 3 shows one run under the Kernel Density simulation and two under the Gaussian Mixture and Gamma
simulation. The variation from the observations is slightly
greater, though not by very much, as a result of the smoothing; the variances are also broadly similar.

7.

(1)

t.test(y ~ system, paired=TRUE, data=data)
lme(y ~ system, data=data, random=~1|topic)

COMPARATIVE EVALUATION

So far we have introduced methods to simulate effectiveness measurements over different document corpora, samples from a very large (or infinite) document collection. Hence,
the effectiveness of a system/run not only varies across topics but it further varies within each topic. The question
that remains to be answered is how can we account for this
new source of variability (within topic variability) and dis-

895

In all cases our data consists of an evaluation measure
calculated over a set of topics for the two systems under
comparison. This is stored in a data frame data with three
fields, the measurement y, the system id system, and the
topic id topic. The first line runs a paired t-test; in the
second one the response variable y is explicitly written as
a function of a fixed effect (system) and a random effect
(Error(topic)). The last line is explicitly fitting a mixedeffects model using the function lme in the nlme package.
The mixed-effects model in Eq. 1 models the effects of
topics over the effectiveness scores of the compared systems.
However, it fails to model a far more important effect, that
of the interaction between the systems and the topics; that
is the fact that a system may find a query hard while the
same query is considered easy by a different system. The
effect of this interaction in the above model is conflated with
the residual error, since we only have one measurement per
system-topic. In the case of repeated measurements this
interaction effect can be decomposed from the random error
and modeled separately,
2

Mixed-effects models has been recently discussed by
Carterette et al. [2]. A similar discussion is provided in this
section.

Figure 2: Bootstrap simulation of runs 19, 38, 29

Figure 3: Kernel Density Estimation simulation of run 11, and Gaussian Mixture and Gamma simulation of
runs 11, 19

yijk = βi + bj + cij + ǫijk
bj ∼ N (0, σ12 ),

cij ∼ N (0, σ22 ),

Random effects:
Formula: ~1 | topic
(Intercept)
StdDev:
1.539644

(2)

ǫijk ∼ N (0, σ 2 )

Formula: ~1 | system %in% topic
(Intercept) Residual
StdDev:
0.6191864 0.6386645

where yijk is the value of an evaluation measure calculated
on topic j for system i on the document collection sample
k, βi is the effect of system i, bj is the effect of topic j,
cij is the interaction effect, and ǫijk is the residual error.
As it can be seen the interaction effect is a random effect
normally distributed with zero mean and σ22 variance. We
can fit Eq. 2 in R by,

Fixed effects: y ~ system
Value Std.Error DF
t-value p-value
(Intercept) -1.3445 0.2438470 846 -5.514077 0.0000
system2
0.0999 0.1343512 46 0.744112 0.4606

> lme1 <- lme(y~system, data=df,
random=~1|topic/system)

Let’s focus on the Random effects section of the output.
We can observe that σ12 , the variance of the topic effect distribution, is estimated to 1.539644, σ22 , the variance of the
system/topic interaction effect distribution is estimated to
0.6191864, while the variance of the error, σ 2 , is estimated
to 0.6386645.3 If we move to the Fixed effects section

The output of R’s lme function is,
> summary(lme1)
Linear mixed-effects model fit by REML
Data: df
AIC
BIC
logLik
2173.5 2197.719 -1081.75

3

Note that even though system %in% topic appears as a
nested effect of the systems within each topic, the function in
fact calculates the interaction effect, because of the crossed

896

of the above results, we can observe that the effect of the
first system is estimated to −1.3445 (intercept), while the
difference to the second system of the pair under comparison is estimated to 0.0999 (system2). The lme function,
further performs a significance test to examine whether this
difference is statistically significant. The p-value over the
system2 line above designates that the difference between
the two systems is not statistically significant.
So far, we have assumed that the errors are independent
and identically normally distributed, with mean zero and
variance σ 2 , and they are independent of the random effects.
This implies that error over all system/topic pairs are i.i.d.
which is the opposite of what we have demonstrated through
Figures 1 and 3. To further examine whether this assumption is valid we first plot the boxplots of residual errors
grouped by system/topic pairs (Figure 4). This plot is useful
for verifying that the errors are centered at zero (i.e., E[ǫ] =
0), have constant variance across groups (V ar(ǫijk ) = σ 2 ),
and are independent of the group levels. Figure 4 indicates
that the residuals are indeed centered at zero, but that the
variability changes with system/topic group.4
We can further verify this by plotting the standardized
residuals against the fitted values (left hand-side plot in Figure 5). As one can observe residuals vary with different
variance across the different groups.
A more general model to better represent our measurements allows different variances by system/topic groups for
the errors. We can define such a model by,
yijk = βi + bj + cij + ǫijk
bj ∼ N (0, σ12 ),

cij ∼ N (0, σ22 ),

Formula: ~1 | system %in% topic
(Intercept) Residual
StdDev:
0.4537618 0.186183
Variance function:
Structure: Different standard deviations per stratum
Formula: ~1 | topic * system
Parameter estimates:
1*1
1*2
2*1
2*2 ...
1.0000000 1.6108387 1.3969085 1.5405710 ...
Fixed effects: y ~ system
Value Std.Error DF
t-value p-value
(Intercept) -1.4385 0.22266286 846 -6.460817 0.0000
system2
0.1834 0.09844907 46 1.863342 0.0688

Apart from the random and fixed effects section, there is a
Variance function section. The values of this section give
the ratio of the standard error of each system/topic group
to the standard error of the first system/topic group. Thus,
for instance, the standard error for the second system when
run against the first topic is about 60% more than that of
the first system when run against the first topic, while the
standard error for the first system run against the second
topic is 40% more than the standard error running against
the first topic. The standard error of the first system/topic
group is itself presented as the Residual standard error, in
the random effect section (0.186183).
We can confirm the adequacy of the heteroscedastic fit by
re-examining plots of the standardized residuals versus the
fitted values by system/topic. As expected, the standardized
residuals in each block now have about the same variability,
Figure 5 (right).
The need for a heteroscedastic model can be formally
tested with the anova method.

(3)

2
ǫijk ∼ N (0, σij
)

This model is called “heteroscedastic”, while the model of
equal variances per system/topic group (Eq. 2) is call “homoscedastic”. The difference between the heteroscedastic
model in Eq. 3 and the homoscedastic model in Eq. 2 is the
fact that the heteroscedastic model allows for different vari2
for each system i topic j group of measurements.
ances σij
This can be expressed in lme by a varIdent parameter that
defines the grouping factors.

anova(lme1,lme2)
Model df AIC BIC logLik
Test L.Ratio p-value
lme1
1 5 2173 2197 -1081
lme2
2 98 1080 1555
-442 1 vs 2 1278.62 <.0001

> lme2 <- lme(y~system, data=df,
random=~1|topic/system,
weights=varIdent(form=~1|topic*system))

The anova method compares the two models in terms
of Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), and Log Likelihood (logLik), with the
smaller the better in all cases. Further it performs a significance test on the likelihood ratio of the two models.
The very small p-value of the likelihood ratio statistic confirms that the heteroscedastic model explains the data significantly better than the homoscedastic model.

The results of fitting the heteroscedastic model in the data
can be viewed below,
> summary(lme2)
Linear mixed-effects model fit by REML
Data: df
AIC
BIC
logLik
1080.872 1555.559 -442.4358

8.

EXPERIMENTAL ANALYSIS

We have seen that the proposed view of evaluation experiments suggests that not all individual topic measurements
have the same status. That is, each one carries its own noise
variance, and these vary significantly between topics. This
suggests in turn that the common approaches to statistical
significance in comparing systems are over-simplified. Tests
like the t-test assign the same status (give the same weight)
to every individual topic measurement.
The view proposed by Cormack and Lynam [3] is that
each topic should in some sense be regarded as a separate
experiment, and that the summarisation of results over sets

Random effects:
Formula: ~1 | topic
(Intercept)
StdDev:
1.447164
design of our experiment, in which the two systems under
comparison are run against all topics.
4
For clarity purposes we only plot the residuals of the two
systems against the first 10 topics.

897

Figure 4: Boxplots of residuals for homoscedastic mixed-effects model by system/topic.

Figure 5: Standardized residuals versus fitted values per system/topic for the homoscedastic (left) and
heteroscedastic (right) mixed-effects model.

898

Figure 6: P-values for the different significance tests
(GMG simulations).
of topics is analogous to a meta-analysis of a group of experiments. We proposed an approach to statistical significance
testing that takes a step in this direction, using mixed-effects
models. For each topic/run pair, we run the simulation 10
times, to obtain 10 values of the metric. In mixed-effects
terms, we regard these values as repeated measurements, in
the sense that the measurements have been taken at random from a population of possible values (i.e. a population
of possible test collections). We are not interested in the differences between these individual values within the groups.
The effect of taking multiple values per topic/run combination is to indicate to the mixed-effects model how reliable
or unreliable each topic is in determining the effectiveness of
this run. One obvious issue is that these measurements are
not real – they are the result of simulations. We can appeal
to the evidence of Section 6, that they are consistent with
the observed results. We need to use the simulations rather
than the directly observed results because only here do we
have the necessary repetitions.
We compare the (heteroscedastic and homoscedastic) mixedeffects models with the t-test. Since we use simulated data
for mixed-effects models, we also use the same data for the
t-test: this is performed on the means of the 10 simulated
samples for each topic. Again, since this is not the usual
way to run a t-test on IR evaluation data, we also run it in
the usual way, on the observed results.
In order to measure the power of the test, we follow Sakai
[10] in performing all possible pairwise tests, sorting by pvalue, and plotting the number of pairs against the p-values:
see Figure 6 for the Gaussian Mixture and Gamma simulation; similar plots are obtained by the other two simulation
model (note that in this section we focus on the GMG simulations, which show the most discrepancy from the observed
results). All these are 2-sided tests. We see that the all four
tests appear to be equally powerful, as indicated by how
much of the curve is below a significance threshold such as
0.05. Hence, decomposing and separately modeling the precision of each topic separately does not lead to a change in
the power of the test.
What is more interesting however is to examine the disagreements between the p-values calculated by the different
tests that could lead in disagreements between which system

899

pairs each test consider as significantly different in performance. Figure 7 presents the scatter plot of the p-values for
the t-test on the means of the simulated measurements, the
homoscedastic mixed-effects model and the heteroscedastic
mixed-effects model against the t-test over the actual logit
AP values (without any simulation). This again uses the
evaluation scores over the Gaussian Mixture and Gamma
simulation. Each point is a pair of systems under comparison. As it can be seen from the plots, there is a significant
disagreement in the p-values computed by the different tests
(but note that the visual effect is somewhat exaggerated because of the large number of superimposed points in the
lower left). The t-test over the actual logit AP values agrees
best with the heteroscedastic model. This is expected since
the t-test over the mean of the repeated measurements per
topic and the homoscedastic model do not handle correctly
the extra source of variability. The points in the areas to
the left and the bottom of the dashed lines in the plots of
Figure 7 represent system pairs that have been found significantly different by at least one of the contrasted tests.
If we focus at the heteroscedastic model vs t-test plot the
percentage of disagreement between the two tests with respect to those system pairs that are found significant by at
least one of the tests reaches about 20%. Examining the
system pairs for which the two tests disagree is of particular
interest; however we leave it as future work.
We also show contingency tables indicating agreement or
otherwise between the tests at the threshold of 0.05 (Table
1). The results over all three simulation methods are summarized in these tables. Similarly to the Figures 6 and 7
we see that all test are comparable regarding their power,
while there are certain system pairs for which different tests
disagree with each other. Note that there is a disagreement
in the results of the t-test on actual values across the three
simulating methods; e.g. in the case of BST and KDE 303
comparisons have been found statistically significant according to the t-test, while for the case of GMG 318 are statistically significant. When simulating sampling the corpus
there are topics for which the simulations produce the same
logitAP value at every iteration. These topics are excluded
when comparing two systems since the homoscedastic and
heteroscedastic models cannot address zero variance within
a topic, even though the t-test is not affected by this. Different simulation methods lead to different sets of zero variance
topics and hence the t-test across different simulation methods is run over different topic sets.

9.

CONCLUSIONS

The experiments reported here reinforce the notion that
we need to worry about the statistical status and validity of
evaluation results on individual topics. The usual approach
in IR evaluation, of regarding each individual topic measurement as an indivisible atom, is not consistent with our use
of particular collections of documents. The size of the document collections we use is actually no insulation against
statistical issues, since measurements for individual topics
typically depend on the small sets of documents judged relevant.
A good approach to this issue has the potential to inform
the measurement of statistical significance in IR evaluations.
We provide some evidence that it will affect significance tests
and inferences of retrieval quality. The evidence presented
is by no means complete, and many other developments and

Figure 7: Scatter plots of p-values for the different tests (GMG simulation)
t-test on actual values
p < 0.05
p ≥ 0.05
287
15
16
462

(BST)

Heterosced.
10 samples

p < 0.05
p ≥ 0.05

(GMG)

Heterosced.
10 samples

p < 0.05
p ≥ 0.05

288
37

(KDE)

Heterosced.
10 samples

p < 0.05
p ≥ 0.05

292
11

t-test on actual values
p < 0.05
p ≥ 0.05
291
2
12
475

t-test on mean
over 10 samples

p < 0.05
p ≥ 0.05

20
435

t-test on mean
over 10 samples

p < 0.05
p ≥ 0.05

281
44

9
446

22
455

t-test on mean
over 10 samples

p < 0.05
p ≥ 0.05

288
15

3
474

Table 1: Contingency table on the agreement of different significance tests for 780 pairs of systems, on 3
simulations: (BST) Bootstrap, (GMG) Gaussian mixture + gamma, (KDE) Kernel Density Estimation.
experiments are required. One future direction would be
to investigate whether we can characterise the high-variance
topics in some simple way. However, the work reported here
is seen as a step towards better understanding of this particular source of error in IR evaluation.

[5] Philipp Jarnet. Statistics-KernelEstimation.
http://search.cpan.org/~janert/
Statistics-KernelEstimation/.
[6] Evangelos Kanoulas, Virgiliu Pavlu, Keshi Dai, and
Javed A. Aslam. Modeling the score distributions of
relevant and non-relevant documents. In Proceedings
of Second International Conference on the Theory of
Information Retrieval, ICTIR 2009, pages 152–163.
Springer, 2009.
[7] Jose C. Pinheiro and Douglas M. Bates. Mixed-Effects
Models in S and S-PLUS. Statistics and computing.
Springer-Verlag New York, Inc., 2000.
[8] Stephen Robertson. On smoothing average precision.
In Proceedings of the 34th European Conference on IR
Research, ECIR 2012, pages 158–169. Springer, 2012.
[9] Stephen E. Robertson. On document populations and
measures of ir effectiveness. In Studies in theory of
information retrieval (Proceedings of ICTIR 2007),
pages 9–12, 2007.
[10] Tetsuya Sakai. Evaluating evaluation metrics based on
the bootstrap. In Proceedings of the 29th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages
525–532. ACM, 2006.
[11] Vishwa Vinay, Ingemar J. Cox, Natasa Milic-Frayling,
and Kenneth R. Wood. On ranking the effectiveness of
searches. In Proceedings of the 29th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages
398–404. ACM, 2006.

Acknowledgments
We would like to thank Jaap Kamps for valuable discussions
on an earlier version of this paper. Further, we gratefully
acknowledge the support provided by the European Commission grant FP7-PEOPLE-2009-IIF-254562.

10.

REFERENCES

[1] David Banks, Paul Over, and Nien-Fan Zhang. Blind
men and elephants: Six approaches to TREC data.
Inf. Retr., 1(1-2):7–34, 1999.
[2] Ben Carterette, Evangelos Kanoulas, and Emine
Yilmaz. Simulating simple user behavior for system
effectiveness evaluation. In Proceedings of the 20th
ACM Conference on Information and Knowledge
Management, CIKM 2011, pages 611–620. ACM, 2011.
[3] Gordon V. Cormack and Thomas R. Lynam.
Statistical precision of information retrieval evaluation.
In Proceedings of the 29th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 533–540. ACM, 2006.
[4] David Hawking and Stephen E. Robertson. On
collection size and retrieval effectiveness. Inf. Retr.,
6(1):99–105, 2003.

900

