Cognos: Crowdsourcing Search for
Topic Experts in Microblogs
Saptarshi Ghosh∗

Naveen Sharma∗

Fabricio Benevenuto

IIT Kharagpur, India
MPI-SWS, Germany

IIT Kharagpur, India
MPI-SWS, Germany

UFOP, Brazil

Niloy Ganguly

Krishna P. Gummadi

IIT Kharagpur, India

MPI-SWS, Germany

ABSTRACT

real-time information on the Web. Recent estimates suggest that 200 million active Twitter users post 150 million
tweets (messages) daily [1]. These messages contain a wide
variety of information, varying from conversational tweets
to highly relevant information on niche topics. The users
posting these messages range from globally popular news
organizations and celebrities to locally popular community
organizers or activists and from domain experts in fields like
computer science and astrophysics to spammers that fake
the identities of well-known users.
As a result, the quality of information posted in Twitter
is highly variable and finding the users that are recognized
sources of relevant and trustworthy information on specific
topics (i.e., topic experts) is an important challenge. Identifying topic experts is also the first step towards finding
authoritative information on the topic. Recognizing this,
Twitter itself has created a topical expert search system
(known as the Twitter Who To Follow (WTF) service [16]).
However, as we show later in this paper, the results from
this service leave a lot of scope for improvement.
In this paper, we present Cognos, a system for finding
topic experts in Twitter. Cognos is based on a new methodology for inferring users’ expertise. Traditional approaches
to identify topical experts in Twitter rely either on the information provided by the user herself (e.g., in the ‘bio’ or
short autobiography of the Twitter account) [17] or on analyzing the network characteristics and tweeting activity of
users [10, 19]. Cognos takes an entirely different approach
to identify topical experts in Twitter utilizing crowdsourced
topical annotation of experts. Specifically, Cognos exploits
the Lists feature in Twitter, using which any user can group
Twitter accounts that tweet on a topic that is of interest
to her, and follow their collective tweets. We observe that
many users carefully create Lists to include other Twitter
users who they consider as experts on a given topic. Furthermore, they generate meta-data, such as List names and
descriptions, that provide valuable semantic cues to the topical expertise of the users included in the List. Our key idea
is to analyze the meta-data of the Lists containing a user to
infer the user’s topics of expertise, which in turn enables us
to identify topical experts.
To build Cognos, we address three key challenges: (1) How
to accurately and comprehensively infer an individual user’s
topics of expertise from Lists? (2) How to rank the relative
expertise of different users identified as experts on a given
topic?, and (3) How to crawl the Lists meta-data for hundreds of millions of Twitter users efficiently and scalably?

Finding topic experts on microblogging sites with millions
of users, such as Twitter, is a hard and challenging problem.
In this paper, we propose and investigate a new methodology for discovering topic experts in the popular Twitter social network. Our methodology relies on the wisdom of the
Twitter crowds – it leverages Twitter Lists, which are often
carefully curated by individual users to include experts on
topics that interest them and whose meta-data (List names
and descriptions) provides valuable semantic cues to the experts’ domain of expertise. We mined List information to
build Cognos, a system for finding topic experts in Twitter.
Detailed experimental evaluation based on a real-world deployment shows that: (a) Cognos infers a user’s expertise
more accurately and comprehensively than state-of-the-art
systems that rely on the user’s bio or tweet content, (b) Cognos scales well due to built-in mechanisms to efficiently update its experts’ database with new users, and (c) Despite
relying only on a single feature, namely crowdsourced Lists,
Cognos yields results comparable to, if not better than, those
given by the official Twitter experts search engine for a wide
range of queries in user tests. Our study highlights Lists as a
potentially valuable source of information for future content
or expert search systems in Twitter.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: selection process; H.3.5 [On-line
Information Services]: Web-based services
General Terms: Algorithms, Design, Experimentation.
Keywords: Twitter, topic experts, Lists, crowdsourcing,
hubs, authorities.

1.

INTRODUCTION

Microblogging sites, out of which Twitter is the most popular, have emerged as an important platform for exchanging
∗

Authors contributed equally to this work.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$10.00.

575

The main contributions of this paper lie in the methodologies we propose to tackle the above challenges.
We present an extensive evaluation of Cognos based
on user feedback obtained using a real-world deployment,
which can be accessed at http://twitter-app.mpi-sws.
org/whom-to-follow/. We summarize here a few highlights
from our evaluation: We find that Cognos performs as well
as or better than the official Twitter WTF service for more
than 52% of the queries. Cognos yields particularly better
search results in those cases in which experts do not provide account bios, or whose bios do not contain information
about the user’s topic of expertise. Moreover, Cognos rarely
produces entirely irrelevant results, unlike the Twitter WTF
service whose top results, at times, include a few users who
are not at all related to the given query, but whose names or
bios contain the terms in the query. Furthermore, as Cognos
is based on the use of a single and simple feature (Twitter
Lists), it is more scalable than prior approaches, which use
computationally intensive machine learning algorithms over
graph and content-based metrics [10, 19].

2.

has become essential given the phenomenal growth rate of
user populations in today’s OSNs [2].
Finally, a few prior studies have used Twitter Lists for
different purposes, such as, to identify popular users in a
few specific topics (e.g., celebrities, bloggers) and study how
they interact with the rest of the Twitter population [20],
or to serve as seed nodes for algorithms like topic-sensitive
Pagerank [18], or to contextualize individual users [11]. The
present study uses Lists for a fundamentally different purpose, which is to find topic experts. In a prior workshop
paper [13], we described how we can use Lists to infer topics
of expertise for individual Twitter users. In this paper, we
not only extend the work to build a fully functional search
system, but also present an in-depth evaluation of the quality of List-based expertise inference and ranking.

3. METHODOLOGY AND CHALLENGES
In this section, we first propose our methodology for finding
topic experts using Twitter Lists. Later we identify the key
design challenges in designing a search system based on the
methodology.

RELATED WORK

As the number of users and information shared in Twitter
grows exponentially, information retrieval techniques, such
as search and recommender systems, are increasingly being
used to find trending topics, interesting users, and valuable
content [14, 16]. A critical component of such systems consists of identifying users who are important sources of information on specific topics (topical experts).
There have been several attempts to measure the influence
of Twitter users and thereby identify influential users or experts [3, 4, 8, 12]. However, none of these efforts attempts
to identify experts in any specific topic. To our knowledge, there have been only two notable efforts that have
approached the problem of identifying experts in specific
topics [10, 19]. Weng et. al. [19] proposed a Page-Rank like
algorithm TwitterRank that uses both the Twitter graph
and processed information from tweets to identify experts
in particular topics. On the other hand, Pal et. al. [10] used
clustering and ranking on more than 15 features extracted
from the Twitter graph and the tweets posted by users.
Apart from the above research studies, there also exist
some services for identifying topical experts in Twitter. Recognizing the importance of searching for experts on specific topics, Twitter itself provides an official “who to follow” (WTF) service [16] where one can search for experts
on a given topic (query). Though the exact details of implementation of the service are not publicly known, it is
reported [17] that Twitter WTF uses several factors such as
the profile information (e.g., name and bio) of users, their
social links, their level of engagement in Twitter, and so on
to identify topical experts.
All the above approaches primarily rely on the information
provided by the user herself (e.g., her account name and bio,
the tweets posted by her) and her social graph to infer the
topics in which she is an expert. In contrast, the present
work relies on the ‘wisdom of the Twitter crowd’ (i.e., how
others describe this user), collected through crowdsourced
Lists. Further, all of the above mentioned research studies
use fixed Twitter datasets collected at a certain point in
time. To the best of our knowledge, this study is the first
to address the practical challenge of keeping an OSN-based
search / recommender system up-to-date, a challenge that

3.1 Methodology: Leverage Twitter Lists
Twitter introduced Lists in late 2009, to help users organize
their followings (i.e., the people whom a user follows) and
the information they post [7]. By creating a List, a user can
group other Twitter users, and view the aggregated tweets
posted by all the listed users in the List timeline. When
creating a List, a user typically provides a List name (free
text, limited to 25 characters) and optionally adds a List
description. For instance, a user can create a List named
“celebrities” and add celebrities to this List. Then, the user
can view tweets posted by these celebrities in the List timeline.
Table 1 presents illustrative examples of Lists, extracted
from our dataset (to be detailed in Section 5.2). The key
observation here is that the List names and descriptions provide valuable semantic cues to the topics of expertise of the
members of the Lists. For example, using List meta-data,
we can associate BarackObama with politics and politicians,
Eminem with music and musicians, and Daniel Tunkelang
with SIGIR. Thus, Lists provide a way to annotate Twitter
users with their topics of expertise. Interestingly, these annotations are generated by arbitrary Twitter users and so
they reflect the collective wisdom of the crowds.
Our methodology relies on extracting the information contained in the crowdsourced Lists to build an expert search
system. Specifically, it has three parts: (i) gather crowdcreated Lists for all Twitter users, (ii) mine List meta-data
to infer the topical expertise of individual Twitter users, and
(iii) for a given query topic, rank the relative expertise of the
users whose topical expertise matches the query.

3.2 Key open questions and design challenges
Our proposed methodology for building a search system for
experts in Twitter raises a number of important questions
and key design challenges, which we enumerate below:
1. How to infer users’ topics of expertise from Lists? Do
Lists contain sufficient information to infer the various
topics of expertise of individual Twitter users both accurately and comprehensively?

576

List Name
News
Music
Tennis
Politics
SIGIR2010

Description
News media accounts
Musicians
Tennis players and Tennis news
Politicians and people who talk about them
People tweeting from SIGIR 2010

Members
nytimes, BBCNews, WSJ, cnnbrk, CBSNews
Eminem, britneyspears, ladygaga, rihanna, BonJovi
andyroddick, usopen, Bryanbros, ATPWorldTour
BarackObama, nprpolitics, whitehouse, billmaher
Daniel Tunkelang, Maria Grineva, Ian Soboroff, James Caverlee

Table 1: Examples of Lists, their description, and some members

2. How to rank the relative expertise of different users
identified as experts on a given topic?

5. As List names and descriptions are typically short, we
consider only unigrams and bigrams as topics.
The above strategy produces a set of topics for each user,
as well as the frequency with which a topic appears in the
names and descriptions of the Lists containing the user.

3. How to crawl the Lists meta-data for tens of millions of
Twitter users (experts) created by hundreds of millions
of other users? How to keep the Lists data up-to-date
as several hundreds of thousands of new users join and
create new Lists every single day [2]?

4.2 Evaluating quality of expertise inference
When evaluating the quality of inferred topics of expertise,
we check for two metrics: (i) accuracy – is the user really an
expert in the inferred topics? (ii) expressiveness – do Lists
comprehensively capture all the different topics in which a
user has expertise?
For our evaluation, we need to gather ground truth information about some Twitter users’ expertise. Since such
ground truth is difficult to obtain for a random set of Twitter
users, we consider the following strategies: First, we evaluate for a select set of popular users whose true topics of
expertise are generally well-known or easily verifiable. Second, for a given set of topics, we collect the top experts
identified by the state-of-the-art research system for identifying topical authorities [10], and by the official Twitter
WTF service [16]. We then check if our methodology identifies these users as experts in the given topics. The results
not only demonstrate the high quality of our expertise inference, but they also uncover drawbacks of the competing
state-of-the-art methods.

We address the above research challenges in each of the subsequent sections. In Section 4, we describe how we use Lists
to infer the topics of expertise of individual Twitter users.
Section 5 presents Cognos, a topical expert search system
for Twitter that leverages the topical expertise inferred using Lists to identify experts on a given topic and rank them.
In Section 6, we propose efficient strategies that minimize
the number of Lists that we need to crawl to keep the Cognos system up-to-date. We conduct an extensive evaluation
of our proposals by comparing their performance with two
systems: (i) the state-of-the-art research system for identifying topical experts in Twitter [10], and (ii) the official
Twitter Who-To-Follow service [16].

4.

USING LISTS TO INFER EXPERTISE

In this section, we first describe our methodology of inferring
the expertise of individual Twitter users and then evaluate
the accuracy and expressiveness of the inferred expertise.

4.2.1 Inferred expertise for selected popular users

4.1 Mining List meta-data to infer expertise

Table 2 shows the top 10 topics (inferred using our Listbased method) for a few Twitter users whose expertise is
well-known. Our inference is accurate and comprehensive
not only for users with millions of followers, but also for
users with hundreds or thousands of followers. For instance,
for Mark Sanderson (Program Committee Chair at SIGIR
2012), even though his Twitter account is included in only
12 Lists, the inferred topics identify that he is a researcher in
computer science (“cs”), specializing in information retrieval,
machine learning (“ml”), search and so on. Again, for US
senators (two examples shown in Table 2 – Chuck Grassley
and Claire McCaskill), our methodology could accurately
identify a variety of topics, such as, their political party (Republicans / Democrats), their state, their gender (‘women’
in case of Claire McCaskill), their political ideology (conservative / progressive) and even a number of the senate
committees of which each senator is a member (‘health’ in
case of Chuck Grassley). We verified the accuracy of our
inference using the Wikipedia pages for these people, and
found them to be almost always accurate (see [13] for details). Thus, List meta-data is often sufficiently rich to yield
high quality expertise inference for a variety of users.

As described in our prior workshop paper [13], our strategy consists of extracting frequently occurring topics (words)
from the List meta-data (names and description) and associating these topics with the listed users. The intuition behind
our strategy is that a user listed by many other users under a certain topic is very likely to be an expert on that
topic. Previous efforts that analyzed Twitter Lists showed
that nouns and adjectives in List names and descriptions are
particularly useful for this purpose [11]. So our strategy to
extract topics from List meta-data consists of the following
steps:
1. Since List names cannot exceed 25 characters, users often
combine multiple words using CamelCase (e.g., TennisPlayers). We separate these into individual words.
2. We apply common language processing techniques such
as case-folding, stemming, and removal of stop-words. In addition to the common stop-words, a set of domain-specific
stop-words are also filtered out, e.g., Twitter, List, and Formulist (a tool frequently used to automatically create Lists).
3. Then, we identify nouns and adjectives using a part-ofspeech tagger.
4. As a significant fraction of List names and descriptions are in languages other than English, we group together
words that are very similar to each other based on editdistance among words, e.g., politics and politica, journalist
and jornalistas, etc. It can be noted that these words are
not unified even by a standard stemmer.

4.2.2 Comparing Cognos with state-of-the-art research system
Next we compare the extent to which the experts identified
by a state-of-the-art research system built by Pal et. al. [10]
can be recalled by our methodology. Pal et. al. use more

577

User
Barack Obama
Ashton Kutcher
Mark Sanderson
Chuck Grassley
Claire McCaskill
BBC News
Linux Foundation
Yoga Journal

# followers
12,481,245
9,479,352
320
34,710
63,687
574,035
46,718
71,689

Most frequent topics
politics, celebs, government, famous, president, news, leaders, noticias, current events
celebs, actors, famous, movies, stars, comedy, funny, music, hollywood, pop culture
information retrieval, ir, cs, ml, semantic, analysis, search, research, nlproc, tech
politics, senator, congress, government, republicans, iowa, gop, officials, conservative, health
politics, senator, government, congress, democrats, missouri, dems, officials, progressive, women
media, news, noticias, journalists, politics, english, newspapers, current, periodicos, london
linux, tech, open, software, libre, gnu, computer, developer, ubuntu, unix
yoga, health, fitness, wellness, magazines, media, mind, meditation, body, inspiration

Table 2: The most common topics of expertise of some well-known Twitter users, as identified from Lists. All topicwords are case-folded to lower case.
User

Extracts from Bio
Query: iphone
macworld
Mac, iPod, iPhone experts
TUAW
Unofficial Apple Weblog
Query: oil spill
kate sheppard
Reporter covering energy, environment
LATenvironment
Environmental news from California
Query: world cup
FIFAWorldCupTM
FIFA soccer world cup tweets
itvfootball
News from ITV football

User
teedubya
macTweeter
Reuters
CBSNews
TIME
huffingtonpost
nikegoal
Flipbooks
channel4news

Table 3: Some of the top 10 results reported by Pal et.
al. [10], for whom the topics inferred using Lists include
the query-topic (iphone / oil spill / world cup)

Extracts from Bio
Query: iphone
Social Strategy Shaman, SEO
Account no longer exists in Twitter
Query: oil spill
latest news from around the world
official Twitter feed of CBS News
Breaking news and current events
The Internet Newspaper
Query: world cup
marketing, music, education, sport
News, Random Information
exclusive stories & breaking news

Table 4: The top 10 results reported by Pal et. al. [10],
for whom the topics inferred using Lists does not include
the query-topic (iphone / oil spill / world cup)

than 15 features extracted from the Twitter social graph
and the content of the tweets posted by users to identify
topical experts. Though an implementation of this system
is not publicly available, their paper lists the top 10 experts
identified for three specific topics – “iphone”, “oil spill” and
“world cup”. We test whether the topics inferred by our
methodology for these experts match with the topic reported
by Pal et. al.
We find that for a majority of the top 10 experts – 8 out
of 10 for “iphone”, 7 out of 10 for “world cup”, and 6 out
of 10 for “oil spill” – in each of the three topics, the set of
topics inferred by us includes the topic for which they are
reported by Pal et. al. Table 3 shows some of these experts,
along with their bio.
However, for the rest of the cases, the topics inferred using
Lists do not contain the topic reported by Pal et. al.. Table 4 lists these users along with their bios. Examining their
bio, it is evident that these users are, in fact, not specifically related to the topic of the corresponding query. For
example, a social media entrepreneur and technology blogger teedubya was identified as an expert on “iphone”, even
though he is not a specialist on Apple products. Similarly,
Reuters, CBSNews and channel4news are popular news media that report on a variety of topics, including those that
are not related to “oil spill” or “world cup”. It is likely that
the algorithm used by Pal et. al. identified these users as
experts because a number of their tweets were related to the
topic in question during the period when the evaluation was
done.
It is worth noting that Pal et. al. had explicitly set out
to discover topical experts that are not just overtly general
and highly followed authorities like popular news media accounts. They highlight the discovery of dedicated specialists
that mostly post tweets related to their specialization. Interestingly, our methodology has successfully recalled all such
experts (i.e., 100% recall) even though it relies only on one
feature, namely Lists. In comparison, Pal et. al. rely on
15 features, which indicates the relative advantages of using
crowdsourced Lists to identify users’ expertise.

4.2.3 Comparing Cognos with Twitter WTF
The official Twitter Who-To-Follow (WTF) service [16]
helps to search for topical experts for a given topic (query),
and is reported to use several factors, such as, the profile information (e.g., name and bio) of users, their social
links, and their level of engagement in Twitter [17]. As part
of a user survey to evaluate our system (detailed in Section 5.3.3), we obtained the top 20 experts returned by the
Twitter WTF service for a few hundred queries generated by
users. We investigated the extent to which our methodology
could recall these experts.
Out of the 3,495 distinct users returned by Twitter as
top 20 results for some given query, the topics inferred using Lists include the corresponding topic (word in the given
query) for 83.4% (2,916) of the users. However, the topics
inferred by the List-based methodology for the other 16.6%
(579) users did not contain the topic (word) in the query.
To understand these missing experts better, we manually
verified 50 randomly selected users out of the 579 users.
We found 9 out of these 50 users (i.e., 18%) to be relevant
experts on the query topics. Our List-based methodology infers topics very similar to the query, but none matching the
exact query-word. Table 5 shows two such examples. For
the Twitter account of the dineLA restaurant, the inferred
topics include ‘food’ and ‘restaurant’ but not the query-word
‘dining’ (for which it was returned by Twitter WTF). Similarly, for the Twitter user HubbleHugger77 who is a space
explorer and has directed the film ‘Saving Hubble’, we identify ‘space’, ‘cosmology’ and ‘nasa’ but not the query-word
‘hubble’. This would appear to suggest that a user’s name
and bio contain clues to the user’s expertise.
However, in 29 out of the 50 cases (i.e., 58%), we found
that the official Twitter WTF service returns wrong results,
i.e., the returned user is not at all related to the topic of the
query for which she is returned. We conjecture that this is
because the query-word appears in the name or bio of the

578

Query

User
Extracts from Bio
Major topics obtained from Lists
Users for whom topics inferred from Lists contain very similar words but not the exact query-word
dining
dineLA
official Twitter account of dineLA
restaurant, food, los angeles, chefs, recipes
hubble
HubbleHugger77
Space Explorer, Director of Film Saving science, tech, space, universe, cosmology,
Hubble
nasa
Wrong results in Twitter WTF top 20 results
astrophysicist
jimmyfallon
astrophysicist
celebs, comedy, funny, actors, famous, humor
cooking
danecook
When I tweet, I tweet to kill
celebs, comedy, funny, famous, actors
origami
ScreenOrigami
Web developer from Germany
webdesign, webkrauts, html, designers
Table 5: Examples of (i) users for whom topics inferred from Lists contain semantically very similar words but not the
exact query-word (ii) wrong results within Twitter WTF top 20 results.

as the only source of information for topical experts – we
have already shown that Lists can be used to accurately infer topics of expertise, now we investigate whether Lists are
also an effective metric to rank topical experts.
The method described in Section 4.1 gives for each individual user, a topic vector {(ti , fi )}, where ti are the set of
topics inferred for the user, and fi is the frequency of occurrence of topic ti in the names and descriptions of the Lists
containing the user. Given a query, we compute a topical
similarity score between this topic vector for a user and the
given query vector, using the algorithm in [5] which computes the cover density ranking between the vectors. We
chose this similarity score (which is suited to queries containing one to three terms) since queries to expert search
systems are almost always short, hence using cosine similarity on tf-idf based representations may not be very effective [9, 10]. Finally, we multiply the topical similarity score
for a user with the logarithm of the number of Lists containing the user – the intuition behind this is that a user who is
included in more number of Lists (by other users) is likely
to be more popular in Twitter.
Thus, given a query (topic), Cognos identifies the set of
experts related to the topic using the List-based methodology discussed in Section 4.1, and then ranks them using the
algorithm described above. In the remainder of this section,
we evaluate this List-based methodology of identifying and
ranking topical experts in Twitter.

user. For instance, the well-known comedian Jimmy Fallon
has (mockingly) described himself as an astrophysicist in his
bio, as a result of which he shows up in the top 20 Twitter
WTF results for the query ‘astrophysicist’. Table 5 shows
other examples of users who are wrongly included within
the top 20 results returned by Twitter WTF. We were not
able to infer the relevance of the expert to the query in the
remaining 12 out of the 50 (24%) manually verified user
accounts, as we found the query to be ambiguous.
Thus, not only does our methodology recall a vast majority (83.4%) of the experts identified by Twitter WTF, but
also a majority of the missing experts were incorrectly identified by Twitter. Our List-based methodology fails to recall
only a small fraction of experts who are actually related to
the given query, and even in those cases, we identify topics
that are semantically quite similar to the query word.

4.2.4 Summary
Our evaluation demonstrates that our proposed methodology of utilizing crowdsourced List meta-data provides an accurate and comprehensive inference of topics of expertise of
individual Twitter users. We also show that in many cases,
the List-based methodology is more accurate compared to
the existing techniques of inferring topics of a user from her
profile or her tweets. In the next section, we describe how
we utilize the topics inferred using Lists, to build a search
system for topical experts in Twitter.

5.

5.2 Building the Cognos experts database

COGNOS EXPERT SEARCH SYSTEM

To populate the Cognos expertise database, we started to
crawl all the Lists containing all Twitter users. We quickly
realized that a brute-force crawl of all Lists for all users
would be prohibitively expensive and would not scale. So we
only crawled the Lists containing all the 54 million Twitter
users in a complete snapshot of the Twitter social network
taken in August 2009 [4]. This is only a small fraction of the
estimated 465 million Twitter users as of January 2012 [2].
We address the challenge of crawling Lists efficiently and
scalably to include experts that joined after 2009, in Section 6.
Of the 54 million Twitter users, we found that 6,843,466
users have been listed at least once. In order to reliably
infer topical expertise of a user from Lists, it is important
that a user has been listed at least a few times. So we
considered only those users who were listed at least 10 times.
There were 1,333,126 users listed more than 10 times. Some
of these users were listed more than a hundred thousand
times. Due to rate-limitations in accessing the Twitter API,
we limited our data collection to at most 2000 Lists per
user. Overall, for the 1.3 million users, we gathered a total
of 88,471,234 Lists. Out of these, 30,660,140 (34.6 %) Lists
had a description, while the others only had the List name.

In this section, we leverage our previously discussed methodology to infer users’ expertise to build Cognos1 , a search
system for topical experts in Twitter. Cognos uses crowdsourced Lists as the only source of information and so its
performance illustrates the potential uses of Lists in finding
experts. We first describe how we rank experts in Cognos
and then present an extensive evaluation of the Cognos system.

5.1 Ranking experts
Ranking of users related to a given topic is a well-studied
problem, and over the years, several ranking algorithms have
been proposed for the Web [6], online topical communities [21], and even for topical experts in Twitter [10,19]. The
expert ranking schemes in Twitter take into account several
metrics extracted from the social graph and the content of
the tweets posted by users. In contrast, we decided to evaluate a ranking scheme that is based solely on the Lists feature,
since one of our objectives is to evaluate crowdsourced Lists
1

The name is derived from the word cognoscenti, i.e., people
who are considered to be especially well informed about a
particular topic.

579

Category
News
Journalists
Politics
Sports
Entertainment
Hobbies
Lifestyle
Science
Technology
Business

Sample queries
politics, sports, entertainment, science, technology, business
politics, sports, entertainment, science, technology, business
conservative news, liberal politicians, USA / German / Brasilian / Indian politicians
F1, baseball, soccer, poker, tennis, NFL, NBA, Bundesliga, LA Lakers
celebrities, movie reviews, theatre, music
hiking, cooking, chefs, traveling, photography
wine, dining, book club, health, fashion
biology, astronomy, computer science, complex networks
iPhone, mac, linux, cloud computing
markets, finance, energy

0.6
0.4
0.2
0

evaluations
results relevant (at least once)
results relevant (majority)

0 5 10 15 20 25 30 35 40 45 50 55
Ranked Queries (w.r.t. fraction of relevant evaluations)

(a) Considering all results

1
0.8
0.6
0.4
0.2
0

evaluations
results relevant (at least once)
results relevant (majority)

0
5 10 15 20 25 30 35 40 45
Ranked Queries (w.r.t fraction of relevant evaluations)

(b) Considering only results that
are evaluated at least twice

Average Precision at 10

1
0.8

Fraction judged relevant

Fraction judged relevant

Table 6: The 55 sample queries used for evaluation of Cognos.

1
0.8
0.6
0.4
majority relevant
at least one relevant

0.2
0
0

5 10 15 20 25 30 35 40 45 50 55
Ranked Queries (w.r.t. average precision)

(c) Average precision at 10

Figure 1: Fraction of evaluations / individual Cognos results that were judged relevant (queries ranked w.r.t. fraction
of relevant judgments) (a) considering all results, and (b) considering only those results for a query, which were
evaluated at least twice; (c) Average precision for the top 10 results returned by Cognos

5.3 Evaluating Cognos expert search system

We found that the fraction of evaluations that judged a result as relevant, for each individual rank out of the top 10
(i.e., considering the results shown at a certain rank for any
of the 55 queries) to be largely invariant – the top 4 results
were judged to be relevant in more than 80% of the evaluations, while the results ranked 5–10 were judged relevant in
more than 75% of the evaluations.
Next we examined the Cognos results that received the
456 (i.e., 21.3%) ‘non-relevant’ judgments. We found a large
amount of subjectivity in these judgments driven by whether
a particular evaluator recognizes a Twitter user as a top
expert on a given topic. We found a number of cases where
the same result for the same query was judged relevant by
some evaluators and non-relevant by others. For example,
for the query ‘cloud computing’, Werner Vogels, who is one
of the principal architects of Amazon’s approach to cloud
computing, was rated as relevant in 4 evaluations, and as
non-relevant in 6 evaluations, possibly because his name was
not known to these evaluators.
To understand the subjectivity in judgments, we consider
for each particular query, (i) what fraction of evaluations
judged a result for this query as relevant, (ii) what fraction
of the top 10 results were judged relevant at least once, and
(iii) what fraction of the top 10 results were judged relevant
in the majority of evaluations. Fig. 1(a) shows the distribution of these fractions for all queries (where queries are
ranked by the fraction of evaluations that judged a result as
relevant). It can be seen that for 37 out of the 55 queries,
every result was judged relevant by at least one evaluation,
and for 30 out of the 55 queries, every result was judged relevant by the majority of the evaluations for that particular
result.
The effects of subjectivity can be seen even more clearly
in Fig. 1(b) where we plot the above three fractions for each
query, considering only those results that were evaluated at
least twice. Note that there are 13 queries (out of the 55)

Judgments on the quality of the results returned by a search
system are to an extent subjective. So we chose to evaluate
Cognos through an user study where a set of human evaluators judged the relevance of the results returned by Cognos, using a web-based feedback service (available at http:
//twitter-app.mpi-sws.org/whom-to-follow/). We also
gathered another set of user evaluations where the results
returned by Cognos were directly compared with those returned by the official Twitter WTF service [16]. We also
compared the top experts returned by Cognos with those
returned by the state-of-the-art research system [10].
The above URL was publicly advertised to a few hundred people at each of the three home institutions of the
authors, which are located across three different continents.
We preferred such an in-the-wild evaluation (instead of a
controlled evaluation with a fixed set of evaluators and few
selected queries, as used by [10]) as it resembles a real-world
deployment of the search system.

5.3.1 Evaluating quality of Cognos results
In this evaluation, an evaluator issues a query, for which
she is shown the top 10 results returned by Cognos. Then
the evaluator gives a binary judgment on each of the top 10
results as to whether it is relevant to the given query. The
queries used for the evaluations could be selected from a
given set of 55 sample queries spread over the 10 categories
shown in Table 6. In the rest of this section, we use the term
‘evaluation’ to indicate a relevant / non-relevant judgment
for an individual result (topical expert) given by Cognos for
a particular query.
Overall, we obtained 2,136 relevance judgments2 over the
top 10 results for the 55 sample queries, out of which 1,680
(i.e., 78.7%) judged the result to be relevant to the query.
2
Despite our request, some of the evaluators did not evaluate
all 10 results for a particular query.

580

User

Cognos results
Extracts from bio

BP America
TheOilDrum
GOHSEP
usoceangov
USCG

BP America
energy, peak oil, sustainability
Emergency Preparedness
National Ocean Service
US Coast Guard

p0sixninja
iH8sn0w
chronicdevteam
MuscleNerd
iPhone News

iPhone Hacker
made f0recast, iREB, iFaith
Hax
iPhone hacker
iPhone news and notes

worldcupscores
EdsonBuddle
thefadotcom
nytimesgoal
herculezg

Live 2010 World Cup Scores
Soccer playerFC Ingolstadt
Website for England Football
New York Times Soccer Blog
US National Team Forward

followers
User
Query: oil spill
35,505
NWF
26,257
TIME
5,295
huffingtonpost
37,866
NOLAnews
20,513
Reuters
Query: iphone
127,631
macworld
105,015
Gizmodo
107,541
macrumorslive
330,625
macTweeter
153,024
engadget
Query: world cup
10,866
TheWorldGame
30,808
GrantWahl
102,536
owen g
11,699
guardian sport
31,454
itvfootball

Results by Pal et. al.
Extracts from bio
National Wildlife Federation
Breaking news, current events
The Internet Newspaper
Latest news and updates
Latest news

followers
76,796
3,231,359
1,574,848
29,433
1,491,852

Mac, iPod, iPhone experts
182,248
Technologies that change
347,667
Updates from Apple events.
170,813
Account no longer exists in Twitter
Twitter account of Engadget
419,583
Australia’s football website
Sports Illustrated writer
Guardian’s Olympics editor
Sport news from Guardian
News from ITV football

11,541
180,290
14,930
121,095
54,395

Table 7: Top 5 results by Cognos and by Pal et. al. [10] for the three queries evaluated by Pal et. al., along with their
bio and number of followers

5.3.3 Comparing Cognos with Twitter WTF

for which no individual result was evaluated twice, hence
Fig. 1(b) shows the other 42 queries. For as many as 40 out
of these 42 queries, every result (that was evaluated at least
twice) was judged relevant by at least one evaluation, and for
33 out of these 42 queries, every result (that was evaluated
at least twice) was judged relevant by the majority of the
evaluations for that result.
Finally, we computed the average precision of the top 10
results returned by Cognos. Fig. 1(c) shows the average precision for all 55 queries, considering two cases – (i) a result
is said to be relevant if it is judged relevant in the majority
of evaluations, and (ii) a result is said to be relevant if it is
judged relevant in at least one evaluation. For 30 out of the
55 queries, all the top 10 results were voted relevant (i.e., average precision of 1) in the majority of the evaluations. Also,
for 45 out of the 55 queries, the average precision was above
0.8 in both cases. The Mean Average Precision (MAP) for
the top 10 Cognos results, considering all 55 queries, was
0.905 in case (i) and 0.939 in case (ii).
The above statistics show that a vast majority of the results returned by Cognos were judged topically relevant to
the given query (topic) by at least some evaluators. Thus,
Cognos can successfully identify relevant experts over a wide
variety of topics.

In this evaluation, when an evaluator issues a query, she is
simultaneously shown the top 10 results returned by Cognos as well as the top 10 results returned by the official
Twitter WTF service for the same query. The results are
anonymized, i.e., the evaluator is not told which result-set
is from which service, in order to prevent bias in judgment.
Then the evaluator indicates which set of results is better
for the given query, or whether both result-sets are equally
good or equally bad.3 Since Cognos uses a Twitter dataset
crawled in 2009 (see Section 5.2), we filtered out from the
Twitter WTF results those user-accounts that were created
after 2009, in order to make the comparison fair.4 In order
to test the performance of Cognos ‘in-the-wild’, we allowed
the evaluators to issue any query of their choice.
We obtained relevance judgments for a total of 325
queries, out of which 259 are distinct. These queries are
evaluator-chosen and they cover a wide variety of topics.
Given the high subjectivity observed in user relevance judgments in the previous section, we choose to focus our evaluation on the 27 distinct queries that were asked at least two
times. In total, these 27 queries were asked 93 times.
Table 8 shows the 27 queries that were asked at least twice.
For each query, we consider the verdict – Cognos better /
Twitter WTF better / tie – based on majority voting. The
queries for which there was a unanimous verdict (i.e., all
evaluations for this query agreed that one or the other was
better) are italicized in Table 8. Cognos was judged to be
better for 12 out of the 27 queries, while Twitter WTF was
judged better for 11, and there was a tie for 4 queries. The
fact that Cognos was judged to be better than the official
Twitter WTF service for 44% of the queries, clearly indicates the potential of crowdsourced Lists (the only feature
used in Cognos) in identifying topical experts in Twitter. It
can be noted that a significant fraction of the cases where
Twitter WTF was unanimously judged better are names of
individuals (celebrities) or organizations. Since such names

5.3.2 Comparing Cognos with state-of-the-art research system
As discussed in Section 4, Pal et. al. [10] give the top 10 experts identified by their algorithm for three specific queries:
oil spill, iphone, and world cup. For these queries, Table 7
compares the top 5 results from Cognos and the top 5 results reported by Pal et. al., along with the bio and number
of followers of each result. Note that while the top results
reported by Pal et. al. contain some general news media
sites (as also discussed in Section 4.2.2), the top Cognos results are much more topic-specific, even if they are not as
popularly followed as the news media sites. Interestingly,
in their paper, Pal et. al. explicitly set out to discover
such specialized topic-specific experts, even if they are not
as highly visible as globally popular celebrities or media accounts. Cognos achieves this goal better than the state-ofthe-art system.

3
The search engines corresponding to the result-sets are revealed to the evaluators after the evaluation is done.
4
The date on which an account was created is available from
the profile information.

581

Cognos better
Linux, computer science,
mac, India, Apple, Facebook, internet, ipad, markets, windows phone, photography, politic journalist

Queries

Average overlap
in top 10 results

Twitter WTF better
politic news, music, Sachin
Tendulkar, Twitter, Alka
Yagnik, Anjelina Jolie,
cloud computing, Delhi,
Harry Potter, metallica,
IIT Kharagpur

Tie
Microsoft – Cognos better: 1, Twitter
better: 1, both good: 1, both bad: 1
Dell, Kolkata – Cognos better: 1, Twitter better: 1
Sanskrit as an official language – both
bad: 2

2.1

3.0

1.83

Table 8: Evaluator-chosen queries (which were asked at least two times) for comparison of Cognos and Twitter WTF,
where the verdict (Cognos better / Twitter better / tie) is given by majority voting. Queries in italics are the ones
for which there was a unanimous verdict.

User

Cognos results
Extracts from bio

Katy Perry
Lady Gaga
taylorswift13
jtimberlake
Pink

i kissed a girl ...
mother monster
Bio not written
Official Justin Timberlake
it’s all happening

BrandonWatson
wmpoweruser
Charlie Kindel
joebelfiore
pocketnow.com

developers on Windows Phone
Windows Phone Power Users
Founder, CTO, Mentor
Runs team doing W. Phone 7
Windows Phone news

followers
User
Query: music
15,016,823
iTunes Music
19,203,867
YouTube
10,994,066
SonyMusicGlobal
8,451,967
50cent
7,128,708
guardianmusic
Query: windows phone
12,462
Windows Phone
10,402
pocketnow.com
8,026
WP Dev Team
15,542
WindowsPhoneNL
42,134
WPCentral

Twitter WTF results
Extracts from bio

followers

Music updates for U.S.
YouTube news, trends, videos
home of Sony Music
It’s the kid 50 Cent
Squashing music

1,903,343
9,220,791
102,753
5,861,243
107,167

Official Windows Phone
Windows Phone news
Windows Phone Dev Team
Windows Phone in Nederland
All thing Windows Phone 7

130,925
42,134
37,344
2,785
18,266

Table 9: Top 5 results by Cognos and Twitter WTF for queries “music” and “windows phone”. While top Cognos
results mostly contain personal accounts, top Twitter WTF results mostly contain organizations / business accounts.

the user’s topic of expertise. In fact, Cognos performs as
good as or better than the official Twitter WTF service for
more than 52% of the queries, even though it is based on a
single and simple feature (Lists).

appear very rarely in the List names / descriptions, Cognos
does not handle these queries well.
Table 8 also shows that the top 10 Cognos results have
very low overlap with top 10 Twitter WTF results across
all queries. This is in spite of the fact that 83.4% out of
the Twitter WTF top 20 results for some query (topic) were
inferred by our List-based methodology to be related to the
same topic (as was reported in Section 4.2.3). This implies
that the low overlap between the top Cognos results and
top Twitter WTF results is primarily due to the List-based
ranking used in Cognos.
In general, we observe that the top Twitter WTF results
mostly include organizations / business accounts while the
top Cognos results mostly include personal accounts. This is
possibly because the Twitter WTF considers the name and
bio of users [17], and organizational / business accounts are
more likely (compared to personal accounts) to have names
or bios containing terms related to their topics of expertise. We present some examples in Table 9 for the queries
‘music’ (for which the majority voted Twitter WTF better),
and ‘windows phone’ (for which the majority voted Cognos
better). As such, these examples again bring out the subjective nature of human judgment, where some evaluators
preferred the personal accounts while others preferred the
organizational accounts.

6. FINDING EXPERTS EFFICIENTLY
In this section, we address the practical challenge of keeping
our Cognos system up-to-date, even as hundreds of thousands of new Twitter accounts and new Lists are created
every day.

6.1 Scalability problem with crawling Lists
We begin by analyzing the scalability of a simple updation
strategy that relies on periodically crawling all the Twitter users and the Lists that contain them. Recent reports
indicate that 200 million new users joined Twitter in the
last 9 months [2], which roughly amounts to 740,000 new
users joining per day. For each user, we would need to make
at least one extra request to crawl her Lists. Thus, just
to keep the system up-to-date by crawling the newly joining
users everyday, a lower-bound rate limit would be of at least
1,480,000 requests per day. Twitter normally rate-limits the
number of API requests from a single machine (IP address)
to 150 per hour [15], i.e., to 3600 user profile crawls per
day. Fortunately, three of our machines were white-listed by
Twitter, which allows each of them to crawl at a significantly
higher rate of 20,000 API requests per hour. Thus, we can
make at most 1,440,000 (20,000 × 3 × 24) API requests per
day from all three of our white-listed machines. Note that
our maximum crawl rate is still lower than the lower-bound
rate we would need to gather the profiles and Lists of all new
users joining Twitter every day. Given that we would need
to periodically crawl the new Lists for the already existing
465 million Twitter users [2], it becomes quite evident that
the strategy of crawling all users’ Lists would not scale.

5.3.4 Summary
Our evaluation of the Cognos search system shows that a
vast majority of its results are relevant for a wide variety of
topics. In fact, Cognos rarely produces entirely irrelevant
results for queries. Comparing Cognos with state-of-the-art
research system by Pal et. al. and the official Twitter WTF
service highlights the advantages of relying on crowdsourced
Lists to identify experts. Cognos yields particularly better
search results in the cases when the bio or tweets posted by
a user does not correspond to or contain information about

582

Fraction of estimated
users discovered

Next, we estimated the number of highly listed users
amongst the 465 million Twitter accounts as of January
2012. Since Twitter assigns userids in an integer sequence
starting from 1, we took a random sample of 300,000 integers
in the range 1 to 465 million, and attempted to crawl the
profiles of Twitter userids in the sample. The distribution of
experts within this large random sample can be expected to
be similar to the distribution of experts among all Twitter
users. For instance, only 363 out of the 300,000 sampled
users (i.e., 0.12%) were listed 100 or more times; hence we
expect the total number of Twitter users who are listed 100
or more times to be 0.12% of the entire Twitter population.
Thus, only a small fraction of all Twitter users are highly
listed experts and once they are identified, it would be possible to crawl the Lists containing them periodically. The
key challenge, however, lies in efficiently identifying these
experts from the large Twitter user population.

1
0.8

users Listed >= 1000
users Listed >= 100
users Listed >= 10

0.6
0.4
0.2
0
3
10

4

5

10
10
Number of top hubs crawled

10

6

Figure 2: Fraction of estimated number of experts who
are included in at least K Lists, that is discovered in the
hub-based crawl, for K = 10, 100, 1000.

lation of 465 million users. Next we calculate the fraction of
the estimated number of users listed at least K times, that
is actually discovered by crawling the Lists created by the
top hubs.
Figure 2 plots the fraction of experts discovered against
the number of top hubs crawled. By crawling the Lists created by the top 1 million hubs, we discover 25,887 experts
who are listed 1000 or more times, which is 70.6% of our estimated total number of experts listed at least 1000 times in
Twitter. Further, we find that crawling the Lists created by
only the top 100,000 hubs is sufficient to discover 53.3% of
the estimated number of experts listed 1000 or more times.
Thus, the hub-based updation methodology can be used to
efficiently discover a large fraction of experts in Twitter.

6.2 Crawling experts efficiently
Our discussion above shows that we cannot exhaustively
crawl Lists for all Twitter users. However, we can crawl
Lists for the small fraction of expert users, if we somehow
identified them from the Twitter user population. We now
propose and evaluate a strategy to efficiently identify expert
users.
We begin by observing that the Twitter social network
consists of a number of hubs, users who follow a large number
of popular experts and include them in Lists. Our strategy
is to first identify popular hubs in an older snapshot of the
network (when the network was considerably smaller) and
then leverage the Lists created by the top hubs in order to
find new authorities. It can be noted that this strategy also
relies on crowdsourcing – we expect the Twitter crowd (in
particular, the top hubs) to discover experts who newly join
Twitter, and we can utilize their discovery by periodically
crawling the Lists created by the top hubs.
We used the well-known HITS algorithm to identify the
top hubs in the snapshot of the Twitter network gathered
in 2009 [4] (introduced in Section 5.2), when the network
had only 54 million users. We then crawled the Lists created by the top 1 million hubs in the network to efficiently
discover experts. In all, the top 1 million hubs had created
479,129 Lists, which taken together contained 4,100,367 distinct users. Out of these, 2,064,373 (i.e., 50.3%) have been
included in 10 or more Lists. In comparison, only 1.13% of
all the users in our large random sample of Twitter users are
listed 10 or more times. The difference clearly indicates that
our strategy is effective in focusing our crawls on experts in
Twitter. The crawl for the Lists created by the top 1 million
hubs took about 3 weeks (January 20 – February 8, 2012)
using three machines white-listed by Twitter, and hence, it
can be repeated every month to discover new experts.

6.3.2 Coverage of newly joined experts
Our expert discovery strategy is also effective in discovering
newly joined experts. Even though our top hubs were selected using a 2009 snapshot of the Twitter network, more
than 42.3% of the 4,100,367 users in the Lists created by
these hubs have joined Twitter after 2009. Further, we
show some examples of very recently created Twitter accounts that our hub-based crawl could discover, in Table 10.
Our crawl of Lists created by the top 1 million hubs, which
ended on February 8, 2012, discovered some experts who
joined Twitter as recently as Feb 6 or Feb 4 (i.e., while the
crawl was going on). This not only validates our hypothesis that the top hub nodes quickly discover newly joined
experts and add them to their Lists, but it also shows the
effectiveness of the hub-based updation strategy.
Account
MartiRiverola
annekirkbride
AaronAStanford
Shay Given
CourteneyCox
PMOIndia

Bio / Description
F.C.Barcelona
English Actress
Canadian Actor
Ireland goalkeeper
American actress
Prime Minister India

Listed
67
23
32
107
294
309

Created
Feb 6
Feb 4
Feb 1
Jan 27
Jan 24
Jan 23

Table 10: Examples of very recently created expert accounts discovered by our Hub-based crawl (which ended
on Feb 8, 2012)

6.3 Evaluating coverage of our crawls
In this section, we estimate the fraction of experts covered
by our strategy to crawl Lists created by top hubs.

6.3.3 Coverage of experts identified by other systems
We evaluate whether our updation methodology can discover topical experts returned by the Pal et. al. research
system and Twitter WTF service. Out of the 30 topical experts stated by Pal et. al. (for the three topics “oil spill”,
“world cup” and “iPhone”), 29 are included in the crawls of
Lists created by the top 1 million hubs (the 1 remaining account no longer exists in Twitter). Next, we consider the

6.3.1 Coverage of most listed users
We measure the fraction of most listed users in Twitter that
is covered by our methodology as follows. First, we estimate
the number of Twitter users listed at least K times by computing the number of such users in our random sample of
300,000 users, and then scaling it to the total Twitter popu-

583

Fraction of top 20
covered in crawl

8. REFERENCES

[1] There Are Now 155m Tweets Posted Per Day, Triple the
Number a Year Ago. http://rww.to/gv4VqA, April 2011.
[2] Twitter to hit 500 million accounts by February.
http://bit.ly/twitpopulation, Jan 2012.
[3] E. Bakshy, J. M. Hofman, W. A. Mason, and D. J. Watts.
Everyone’s an influencer: quantifying influence on Twitter.
In Proceedings of ACM Conference on Web Search and
Data Mining (WSDM), 2011.
[4] M. Cha, H. Haddadi, F. Benevenuto, and K. P. Gummadi.
Measuring User Influence in Twitter: The Million Follower
Fallacy. In Proceedings of AAAI Conference on Weblogs
and Social Media (ICWSM), May 2010.
[5] C. Clarke, G. Cormack, and E. Tudhope. Relevance
ranking for one to three term queries. Information
Processing and Management, 36(2), 2000.
[6] T. H. Haveliwala. Topic-sensitive pagerank. In Proceedings
of ACM Conference on World Wide Web (WWW), 2002.
[7] N. Kallen. Twitter blog: Soon to Launch: Lists.
http://blog.twitter.com/2009/09/soon-to-launchlists.html, Sep 2009.
[8] C. Lee, H. Kwak, H. Park, and S. Moon. Finding
influentials based on the temporal order of information
adoption in Twitter. In Proceedings of ACM Conference on
World Wide Web (WWW), 2010.
[9] D. Metzler, S. Dumais, and C. Meek. Similarity measures
for short segments of text. In Proceedings of European
Conference on Information Retrieval, 2007.
[10] A. Pal and S. Counts. Identifying topical authorities in
microblogs. In Proceedings of ACM Conference on Web
Search and Data Mining (WSDM), 2011.
[11] R. Pochampally and V. Varma. User context as a source of
topic retrieval in Twitter. In Proceedings of Workshop on
Enriching Information Retrieval (with ACM SIGIR), 2011.
[12] D. M. Romero, W. Galuba, S. Asur, and B. A. Huberman.
Influence and passivity in social media. In Proceedings of
ACM Conference on World Wide Web (WWW), 2011.
[13] N. Sharma, S. Ghosh, F. Benevenuto, N. Ganguly, and
K. Gummadi. Inferring Who-is-Who in the Twitter Social
Network. In Proceedings of Workshop on Online Social
Networks (with ACM SIGCOMM), 2012.
[14] J. Teevan, D. Ramage, and M. R. Morris. #twittersearch: a
comparison of microblog search and web search. In
Proceedings of ACM Conference on Web Search and Data
Mining (WSDM), 2011.
[15] Rate Limiting | Twitter Developers.
https://dev.twitter.com/docs/rate-limiting.
[16] Twitter: Who to Follow.
http://twitter.com/#!/who_to_follow.
[17] Twitter Improves “Who To Follow” Results & Gains
Advanced Search Page. http://selnd.com/wtfdesc.
[18] M. J. Welch, U. Schonfeld, D. He, and J. Cho. Topical
semantics of Twitter links. In Proceedings of ACM
Conference on Web Search and Data Mining (WSDM),
2011.
[19] J. Weng, E.-P. Lim, J. Jiang, and Q. He. Twitterrank:
finding topic-sensitive influential twitterers. In Proceedings
of ACM Conference on Web Search and Data Mining
(WSDM), 2010.
[20] S. Wu, J. M. Hofman, W. A. Mason, and D. J. Watts. Who
says what to whom on Twitter. In Proceedings of ACM
Conference on World Wide Web (WWW), 2011.
[21] J. Zhang, M. S. Ackerman, and L. Adamic. Expertise
networks in online communities: Structure and algorithms.
In Proceedings of ACM Conference on World Wide Web
(WWW), 2007.

1
0.8
0.6
0.4
0.2
0
0

50
100
150
200
250
Ranked Queries (ranked w.r.t. Y-axis)

300

Figure 3: Distribution of the fraction of Twitter WTF
top 20 results that is covered in our hub-based crawl (for
the queries discussed in Section 5.3.3)

top 20 experts returned by Twitter WTF for each of the
259 queries obtained by our user-survey (see Section 5.3.3)
and calculate what fraction of these experts are covered by
our hub-based crawls. Figure 3 plots the distribution of
the fraction of Twitter WTF top 20 results included in our
hub-based crawls, across all queries. It can be seen that our
crawls include all Twitter WTF top 20 results for more than
50% of the queries and at least 15 out of the Twitter WTF
top 20 results for close to 80% of the queries.
The above results indicate that our hub-based strategy –
periodically discovering experts through the Lists created by
top hubs – can be used to efficiently discover newly joined
experts, and keep our expert search system up-to-date in
the face of rapid increase in the Twitter population.

7.

CONCLUSION

As Twitter emerges as a popular platform for finding realtime information on the Web, an important research challenge lies in identifying experts in specific topics. In this paper, we show that an effective solution to this hard problem
lies in exploiting the wisdom of the Twitter crowds. Specifically, we observe that individual Twitter users, for their own
convenience, annotate and classify experts in various topics
using the Lists feature. By aggregating the List information
for individual Twitter users, we can obtain an extremely rich
characterization of their topical expertise as perceived by the
Twitter crowds. We leverage this insight to build and deploy
Cognos, a topical expert search system. Through extensive
evaluation, we demonstrate that even though Cognos is built
utilizing only the Lists feature, it can compete with the commercial who-to-follow system deployed by Twitter itself. We
believe that crowdsourced Lists provide a valuable foundation for building future content search / recommendation /
discovery services in Twitter.
Our current List-based methodology is vulnerable to List
spamming, where malicious users can create fake Lists including a target user to manipulate the inferred attributes
for the user. To date, we did not find much evidence of such
attacks. However, such attacks can easily be launched in
the future. Defending against such attacks would require
the List-based methodology to consider the reputation of
the users who create the Lists – a crucial challenge that we
plan to address in future work.
Acknowledgment
The authors thank the anonymous reviewers whose suggestions helped to improve the paper. This research was supported in part by a grant from the Indo-German Max Planck
Centre for Computer Science (IMPECS).

584

