Summarizing the Differences from Microblogs
Dingding Wang

Mitsunori Ogihara

Tao Li

University of Miami
1365 Memorial Drive
Coral Gables, FL 33146

University of Miami
1365 Memorial Drive
Coral Gables, FL 33146

Florida International University
11200 SW 8th ST
Miami, FL 33199

d.wang1@miami.edu

ogihara@cs.miami.edu

ABSTRACT

have been made to the newly emerging microblogs. Different from traditional news reports, microblogs are short
noisy text streams and are of the dynamic and social natures. In this paper, we propose a framework to generate
short summaries capturing the diﬀerences from microblogs.
The proposed framework consists of three modules: keyword
selection, query expansion, and sentence selection. First of
all, discriminant keywords are selected via an unsupervised
approach which combines document clustering and keyword
selection. Then the keywords are used as the original query
to perform query expansion, which is important to deal with
the unique characteristics of microblogs. Finally, a querybased sentence selection is conducted using a strategy of
maximizing the relevances between the selected sentences
and the expanded query and at the same time minimizing
the redundancy.

With the rapid growth of social media websites, microblogging has become a popular way to spread instant news and
events. Due to the dynamic and social nature of microblogs,
extracting useful information from microblogs is more challenging than from the traditional news articles. In this paper
we study the problem of summarizing the diﬀerences from
microblogs. Given a collection of microblogs discussing an
event/topic, we propose to generate a short summary delivering the diﬀerences among these microblogs, such as the
diﬀerent points of view for a news topic and the changes and
evolution of an ongoing event.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.7 [Artiﬁcial Intelligence]:
Natural Language Processing

2. THE METHODOLOGY

Keywords

2.1 Keyword Selection via Weighted Feature
Subset Non-negative Matrix Factorization

Summarizing diﬀerences, Microblogs

1.

taoli@cs.fiu.edu

In order to ﬁnd the diﬀerences among microblogs, we use
weighted feature subset non-negative matrix factorization
(WFS-NMF) proposed in [6] to cluster the microblogs into diﬀerent topics and simultaneously extract discriminant
keywords for these topics. This is an unsupervised approach
to combine data clustering and feature selection. Unlike the
keywords selected from co-clustering or latent semantic analysis which usually strongly associate with one cluster, the
keywords selected by WFS-NMF are the most representative
and discriminant terms among the topics.
Given a term-microblog matrix X, let G represent the
cluster indicator matrix for terms and F be the cluster indicator matrix for microblogs. WFS-NMF can be formalized
as follows.

INTRODUCTION

Nowadays, microblogs provide a new communication channel for users to share information and report news. Compared to regular blogs and traditional media, microblogging
spreads ﬁrst-hand reports on real-life events faster and more
widely. For a hot event, thousands even millions of individuals join the discussion and express their opinions through
popular microblogging web sites such as Twitter. A natural
and interesting question is what are the diﬀerences among
the discussion. In this paper, we explore the problem of
generating a short summary of the diﬀerences among microblogs.
This problem is related to document summarization which
aims to extract sentences from documents to form a brief
summary. However, traditional document summarization
usually selects sentences delivering the majority of information or query-relevant information from the documents,
while in our task the diﬀerences need to be addressed. Recently there are some research works on comparative document summarization [7, 5] which also try to summarize the
diﬀerences from news articles. However, all of the existing
research focuses on traditional news articles, few attempts

min

W ≥0,F ≥0,G≥0

||X − F GT ||2W , s.t.


j

Wjα = 1

(1)

where W is a diagonal matrix indicating the weights of
the terms in X, and α is a parameter. The terms with the
highest weights are the most discriminant keywords.

2.2 Query Expansion using Pseudo Relevance
Feedback
The original query is usually short and vague, to enhance
the query expressibility, query expansion is used to replace
the original query by a high-quality query. In this paper, we
adopt the pseudo relevance feedback methods implemented

Copyright is held by the author/owner(s).
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
ACM 978-1-4503-1472-5/12/08.

1147

in the Terrier information retrieval platform [3]. The methods extract informative terms from the top-ranked documents retrieved using the original query and use them for
query expansion.
Here we model each microblog m in the corpus C as θm ,
and model each query Q as θQ . The expanded query is
deﬁned as Q . In a pseudo relevance manner, suppose the
few top-ranked microblogs m+ by the initial query Q build
a relevant model θF . We set the new query to be a linear
combination of original query Q and relevant model θF :
p(w|θQ ) = (1 − α)p(w|θQ ) + αp(w|θF ),

0.06

0.05

0.04

0.03

0.02

0.01

0

(2)



p(m+)p(w|m+)p(Q|m+),


q∈Q

(3)

p(q|m+).

Once we obtain the expanded query, we perform a querybased sentence selection under the max-relevance and minredundancy framework [4]. The sentences are selected one
by one, and each selected sentence should be highly related
to the query and be dissimilar to the other selected sentences
so that to remove the redundancy. This problem then becomes an optimization problem as follows.
Suppose set S represents the set of sentences and we already have Vk−1 , the selected sentence set with k-1 sentences, then the task is to select the k-th sentence from the
set {S − Vk−1 }. In the following formula, we see that minimizing the redundancy and maximizing the relevance can
be achieved concordantly [4].
max

[Sim(si ; query) −

1
k−1



Sim(sj ; si )],

Ours

3.3 Experimental Results
Figure 1 shows the Rouge scores of diﬀerent systems. From
the results, we observe that (1) DocSum achieves the poorest results since it focuses on delivering the major information in the tweets and can not handle the requirement of
ﬁnding diﬀerences; (2) CoRank and CDS outperform DocSum because they are designed to extract diﬀerent themes
from documents and the common aspects among the documents are removed; (3) our proposed method achieves the
highest scores because we consider the unique characteristics of microblogs which are usually short and noisy, and the
discriminant keyword selection, query expansion, and the
optimization based sentence selection schemes used in our
method can eﬀectively address the problem.

(4)

si ∈Gk−1

4. ACKNOWLEDGMENTS

where Sim is the cosine similarity between a pair of sentences. The computational complexity of this incremental
sentence selection method is O(|V | × |S|) = O(km).

3.

CDS

We compare our proposed method with the following existing summarization systems: (1) DocSum - one of the most
widely used traditional document summarization methods [1]; (2) CoRank - a method summarizing the diﬀerences
from multilingual news articles [5]; (3) CDS - a comparative document summarization proposed in [7]. We compare
the human created summaries with the summaries generated by diﬀerent systems using the ROUGE toolkit (version
1.5.5) [2].

2.3 Query-based Sentence Selection

sj ∈S−Vk−1

CoRank

3.2 Implemented Systems and Evaluation Measures

m+∈F

where p(Q|m+) =

DocSum

Figure 1: Comparison of diﬀerent summarization
systems.

where α controls the degree of coherence of the new query
to the pseudo relevance. The relevance model method approximates θF as the query model, and each pseudo relevant
microblog is a sample from the query model. Therefore the
relevance model method deﬁnes term distribution in θF as
the likelihood of generating terms from the pseudo relevance:
p(w|θF ) ∝

Rouge−2
Rouge−SU

This work is in part supported by NIH/NIA 1P30DA02782801A1 and NSF grants DBI-0850203 and CNS-1126619.

5. REFERENCES

EXPERIMENTS

[1] G. Erkan and D. R. Radev. Lexpagerank: Prestige in
multi-document text summarization. In EMNLP, 2004.
[2] C. Y. Lin. ROUGE: A package for automatic evaluation of
summaries. In Proceedings of the Workshop on Text
Summarization Branches Out (WAS), 2004.
[3] I. Ounis, G. Amati, V. Plachouras, B. He, C. Macdonald, and
D. Johnson. Terrier information retrieval platform. In ECIR,
2005.
[4] H. Peng, F. Long, and C. Ding. Feature selection based on
mutual information: criteria of max-dependency, max-relevance,
and min-redundancy. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 2005.
[5] X. Wan, H. Jia, S. Huang, and J. Xiao. Summarizing the
diﬀerences in multilingual news. In SIGIR, 2011.
[6] D. Wang, T. Li, and C. H. Q. Ding. Weighted feature subset
non-negative matrix factorization and its applications to
document understanding. In ICDM, 2010.
[7] D. Wang, S. Zhu, T. Li, and Y. Gong. Comparative document
summarization via discriminative sentence selection. In CIKM,
2009.

3.1 Data Annotation and Preprocessing
First of all, we select 10 hot topics of world news and technology such as “Japanese earthquake and Fukushima nuclear disaster”, “swine ﬂu outbreak”, and “Siri for iphone 4s”,
etc. For each topic, we collect 1000 tweets which have the
most replies and retweets from Twitter. Three human labelers manually create reference summaries describing the
diﬀerences of the tweets for each topic. The length of each
summary is limited to 250 words.
In the preprocessing, mentions (@somebody) are removed
from the vocabulary. Non-English tweets containing less
than one English word with more than 2 characters are
ﬁltered. Explicit re-tweets, empty tweets, and forbidden
tweets are also ﬁltered.

1148

