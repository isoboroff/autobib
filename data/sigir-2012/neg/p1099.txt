Learning to Select a Time-aware Retrieval Model
Nattiya Kanhabua

Klaus Berberich

Kjetil Nørvåg

L3S Research Center
Leibniz Universität Hannover
Hannover, Germany

Max-Planck Institute for
Informatics
Saarbrücken, Germany

kanhabua@L3S.de

kberberi@mpi-inf.mpg.de

Dept. of Computer Science
Norwegian University of
Science and Technology
Trondheim, Norway

ABSTRACT

words) and a temporal part dtime composed of its publication time PubTime(d), and temporal expressions {t1 , . . . , tk }
mentioned in d, denoted ContentTime(d). A temporal query
q consists of keywords qtext , and temporal expressions qtime .
PT-Rank and CT-Rank both employ a mixture model that
linearly combines textual similarity and temporal similarity
between q and d as S(q, d) = (1 − α) · S 0 (qtext , dtext ) + α ·
S 00 (qtime , dtime ) using a mixing parameter α. The textual
similarity S 0 can be determined using any existing termbased retrieval model (e.g., tf.idf or a unigram language
model). The temporal similarity S 00 is determined assuming that temporal expressions in the query are generated
independently from a two-step generative model, i.e.:
X
Y
1
P (tq |td ) .
S 00 (qtime , dtime ) =
|dtime |
t ∈q

Time-aware retrieval models exploit one of two time dimensions, namely, (a) publication time or (b) content time (temporal expressions mentioned in documents). We show that
the effectiveness for a temporal query (e.g., illinois earthquake
1968) depends significantly on which time dimension is factored into ranking results. Motivated by this, we propose
a machine learning approach to select the most suitable
time-aware retrieval model for a given temporal query. Our
method uses three classes of features obtained from analyzing distributions over two time dimensions, a distribution
over terms, and retrieval scores within top-k result documents. Experiments on real-world data with crowdsourced
relevance assessments show the potential of our approach.
Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms Algorithms, Experimentation
Keywords temporal queries, time-aware ranking prediction

1.

q

time

td ∈dtime

For CT-Rank [1] the probability P (tq |td ) is estimated according to the LMTU method based on content time. For
PT-Rank [4] P (tq |td ) is estimated based on publication time
using an exponential decay function. For both methods
Jelinek-Mercer smoothing eliminates zero probabilities.

INTRODUCTION

Previous work [1, 4] has shown that the retrieval effectiveness of temporal queries can be significantly improved
by modeling and taking into account publication time (i.e.,
when a document was published) or content time (i.e., what
time a document refers to). The right choice of time-aware
retrieval model can make a huge difference for a given temporal query, as we observe empirically. Thus, the model
based on publication time proposed in [4] (labeled PT-Rank )
performs best for temporal queries like iraq 2001 and mac
os x 24 march 2001, whereas the model based on content
time from [1] (labeled CT-Rank ) performs best for temporal
queries like sound of music 1960s and michael jackson 1982.
Our contribution in this work is a novel machine learning approach to select the most suitable time-aware retrieval
model for a given temporal query – to the best of our knowledge the first approach tackling this objective. It uses three
classes of features obtained from analyzing distributions over
two time dimensions, a distribution over terms, and retrieval
scores within top-k result documents. We further present experimental results, showing the significance of the problem
addressed and the effectiveness of our approach.

2.

noervaag@idi.ntnu.no

3.

SELECTING A RETRIEVAL MODEL

Given a temporal query q, we will predict which timeaware retrieval model achieves the best effectiveness by learning a prediction model using three classes of features:
Temporal KL-divergence, originally proposed in [3],
measures the difference between the distribution of publication time within a set of top-k result documents Dq and
their distribution in the overall document collection C. This
definition thus only considers publication time, and we further refer to it as KLP T . While it gives a strong signal,
for instance, when all relevant documents were published
around the occurrence of an important real-world event (e.g.,
a sports tournament), it does not capture when they all refer
to a common time period (e.g., the 19th century). We therefore adapt temporal KL-divergence to also consider content
P
(t|q)
, where
time as KLCT (Dq ||C, q) = t∈TC P (t|q) · log PP(t|T
C)
TC is a set of all temporal expressions in C. P (t|TC ) is the
probability of a temporal expression t in C. P (t|q) is the
probability of generating a temporal expression t given q:
P
P P (q|d)
P (t|q) =
, where P (q|d) is a
d∈Dq P (t|d) ·
P (q|d0 )
0

MODEL

d ∈Dq

retrieval score of d wrt. a particular retrieval model. Since a
document can contain more than one temporal expression,
, where c(t0 , d) is the number of occurP (t|d) = P 0 c(t,d)
c(t0 ,d)

A document d consists of a textual part dtext (a bag of

t ∈d

Copyright is held by the author/owner(s).
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
ACM 978-1-4503-1472-5/12/08.

rences of t0 in d. Again, we employ Jelinek-Mercer smoothing when estimating P (t|d) to avoid zero probabilities.

1099

As suggested in [3], temporal features alone could not
achieve high accuracy for query classification. Thus, we also
employ a clarity score [2] for measuring the KL-divergence
between the distribution of terms within top-k results Dq
and their distribution in the overall document
Pcollection C.
A clarity score can be computed as Clarity = w∈V P (w|q)·
(w|q)
, where w is a term from the vocabulary V of all
log PP(w|C)
distinct terms in C. P (w|q) is the probability of generating
w given q and P (w|C) is the probability of w in C.
Retrieval scores can also be exploited to select a retrieval model, as proposed in [5]. We employ different features obtained from analyzing/comparing the retrieval scores
of a term-based baseline model that is not time-aware, PTRank, and CT-Rank, namely: 1) average score of the baseline (AVGbase ), 2) average score of PT-Rank (AVGPT-Rank ),
3) average score of CT-Rank (AVGCT-Rank ), and 4) the divergence of retrieval scores according to PT-Rank and CTRank from those produced by the baseline (JSPT-Rank and
JSCT-Rank ). We employ Jensen-Shannon divergence to measure the extent to which the time-aware models alter the
scores of the baseline retrieval model, formally:
X
Sb (q, d)
.
JS(Sb ||Sr , q) =
Sb (q, d) · log 1
·
S
(q,
d) + 12 · Sr (q, d)
b
2
d∈D

Table 1: Accuracy of query classification.
Feature
Clarity
KLPT
KLCT
AVGBase
AVGPT-Rank
AVGCT-Rank
JSPT-Rank
JSCT-Rank
Clarity+KLPT +KLCT
Clarity+JSPT-Rank +JSCT-Rank

inclusive
100
500
0.59
0.60
0.60
0.59
0.60
0.60
0.63
0.56
0.60
0.60
0.60
0.59
0.74
0.64
0.60
0.60
0.61
0.61
0.75
0.61

Table 2: Effectiveness of different retrieval models.
Method
CT-Rank
PT-Rank
PR
MAX

P@1
0.55
0.63
0.68
0.83

exclusive
P@5
0.50
0.53
0.53
0.61

MAP
0.53
0.55
0.59
0.64

P@1
0.58
0.63
0.70
0.78

inclusive
P@5
0.55
0.58
0.58
0.62

MAP
0.56
0.61
0.64
0.67

results show that prediction accuracy tends to be better
when using k = 100 rather than k = 500. One reason
for this is that with the larger number of top-k documents,
more irrelevant documents are introduced into the analysis.
The performance among different feature classes shows that
JSPT-Rank performs well in most case. For exclusive, using
a small number of top-k documents is better than a large
number of top-k documents. For top-100, JSPT-Rank outperforms the baseline classifier and other features significantly
(accuracy=0.72). For top-500, all single features perform
worse compared to the baseline classifier. For inclusive, the
performance of top-100 is better than top-500. For top-100,
the best performing feature is the combination of Clarity,
JSPT-Rank and JSCT-Rank , which achieves an accuracy of 0.75.
Retrieval results. For each query, we determined retrieval results using a model chosen according to the best
prediction model determined in the previous experiment,
such as, 1) JSPT-Rank for retrieval in exclusive, and 2) Clarity+ JSPT-Rank +JSCT-Rank for retrieval in inclusive. Table 2
shows the effectiveness of different retrieval models, where
PR is the retrieval model based on our prediction model.
MAX is the maximum (or optimal) effectiveness that can be
achieved, that is, if a prediction model performs accurately
100%. The retrieval results are compared with the baseline
method CT-Rank. The results show that our predictionbased retrieval model (PR) outperforms the baseline significantly in P@1 and MAP. However, we note that it is difficult
for PR to achieve the optimal effectiveness because of the
classification accuracy as explained above.

q

where Sb (q, d) is the retrieval score of d according to the
baseline Sb . Sr (q, d) is the score of d when ranked using a
time-aware retrieval model Sr ∈ {PT-Rank, CT-Rank}.

4.

exclusive
100
500
0.51
0.53
0.53
0.53
0.53
0.53
0.53
0.53
0.53
0.53
0.53
0.53
0.72
0.42
0.38
0.42
0.54
0.65
0.42
0.65

EXPERIMENTS

We conducted two sets of experiments: 1) evaluate our
prediction model as classification accuracy, and 2) demonstrate how an accurate choice of the retrieval model can improve retrieval effectiveness. We used the New York Times
Annotated Corpus containing 1.8M documents published
between 1987 and 2007, and the 40 temporal queries and
relevance assessments from [1]. Temporal expressions were
extracted using the TARSQI Toolkit. Documents were indexed and retrieved with Apache Lucene 2.9.3 using its default similarity function as a baseline retrieval model. We
consider both inclusive and exclusive modes of evaluating
queries, described in [1], that differ in whether temporal
expressions are also treated as textual query terms. The
mixture parameter α was determined empirically: α = 0.5
for PT-Rank and α = 0.6 for CT-Rank in inclusive, and
α = 0.5 for PT-Rank and α = 0.1 for CT-Rank in exclusive.
The parameters for TSU were: DecayRate = 0.5, λ = 0.5,
and µ = 6 months. For LMTU, smoothing γ was 0.75. For
temporal KL-divergence, smoothing was set to 0.1.
For classification, each query was labeled according to
whether PT-Rank or CT-Rank performs best on it. More
precisely, we assumed the model with the best MAP as a
query label. We excluded queries with a small difference in
MAP of two time-aware models. We learned a prediction
model using several algorithms: decision tree, Naı̈ve Bayes,
neural network and SVM, using 10-fold cross-validation with
10 repetitions. We measured statistical significance using a
t-test with p < 0.05. In the tables, bold face indicates statistically significant difference from the respective baseline.
Classification results. The baseline method for query
classification is the majority classifier. The accuracy of the
baseline is 0.54 for exclusive and 0.60 for inclusive. Table 1 shows the accuracy of the best-performing classifiers,
i.e., SVM for exclusive and decision tree for inclusive. The

5.

CONCLUSIONS

We have demonstrated that selecting the right time-aware
retrieval model can have a significant impact on the retrieval
effectiveness of temporal queries. We proposed a novel machine learning approach to do so automatically and demonstrated its effectiveness through extensive experiments.

6.

REFERENCES

[1] K. Berberich, S. Bedathur, O. Alonso, and G. Weikum. A
language modeling approach for temporal information needs. In
Proceedings of ECIR’2010, 2010.
[2] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting
query performance. In Proceedings of SIGIR’2002, 2002.
[3] R. Jones and F. Diaz. Temporal profiles of queries. ACM Trans.
Inf. Syst., 25, July 2007.
[4] N. Kanhabua and K. Nørvåg. Determining time of queries for
re-ranking search results. In Proceedings of ECDL’2010, 2010.
[5] J. Peng, C. Macdonald, and I. Ounis. Learning to select a
ranking function. In Proceedings of ECIR’2010, 2010.

1100

