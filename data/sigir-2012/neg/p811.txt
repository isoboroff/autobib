Content-based Retrieval for Heterogeneous Domains:
Domain Adaptation by Relative Aggregation Points
Makoto P. Kato

Hiroaki Ohshima

Katsumi Tanaka

Kyoto University, Japan

{kato,ohshima,tanaka}@dl.kuis.kyoto-u.ac.jp
ABSTRACT

General Terms

We introduce the problem of domain adaptation for content-based
retrieval and propose a domain adaptation method based on relative
aggregation points (RAPs). Content-based retrieval including image retrieval and spoken document retrieval enables a user to input
examples as a query, and retrieves relevant data based on the similarity to the examples. However, input examples and relevant data
can be dissimilar, especially when domains from which the user
selects examples and from which the system retrieves data are different. In content-based geographic object retrieval, for example,
suppose that a user who lives in Beijing visits Kyoto, Japan, and
wants to search for relatively inexpensive restaurants serving popular local dishes by means of a content-based retrieval system. Since
such restaurants in Beijing and Kyoto are dissimilar due to the difference in the average cost and areas’ popular dishes, it is difficult
to find relevant restaurants in Kyoto based on examples selected in
Beijing. We propose a solution for this problem by assuming that
RAPs in different domains correspond, which may be dissimilar
but play the same role. A RAP is defined as the expectation of instances in a domain that are classified into a certain class, e.g. the
most expensive restaurant, average restaurant, and restaurant serving the most popular dishes. Our proposed method constructs a new
feature space based on RAPs estimated in each domain and bridges
the domain difference for improving content-based retrieval in heterogeneous domains. To verify the effectiveness of our proposed
method, we evaluated various methods with a test collection developed for content-based geographic object retrieval. Experimental results show that our proposed method achieved significant improvements over baseline methods. Moreover, we observed that the
search performance of content-based retrieval in heterogeneous domains was significantly lower than that in homogeneous domains.
This finding suggests that relevant data for the same search intent
depend on the search context, that is, the location where the user
searches and the domain from which the system retrieves data.

Algorithms, Experimentation

Keywords
Content-based retrieval, domain adaptation

1. INTRODUCTION
Content-based retrieval enables a user to input examples as a
query, and retrieves relevant data based on the similarity to the examples. In multimedia retrieval, it is difficult to represent complex
search intents by using keywords due to the difference in medium
between input and output; thus, content-based retrieval has been
successfully applied to a wide range of multimedia retrieval including image retrieval [17] and spoken document retrieval [9], as well
as geographic object retrieval [15].
Content-based geographic object retrieval proposed by Kato et
al. [15] provides a means by which a user can select geographic
objects (e.g. restaurants) as a query in a domain s/he knows well
(denoted by source domain D(S) ) for retrieving objects from a domain s/he does not know well (denoted by target domain D(T ) ).
This search method is helpful, especially when the searcher has little knowledge of the target domain, as it is usually difficult to make
a concrete query without knowledge on the domain in which the
user wants to search [2]. Suppose that a user who lives in Beijing
visits Kyoto, Japan, and wants to search for restaurants, say, relatively inexpensive restaurants serving popular local dishes. If the
user has little knowledge of Kyoto and no idea about the average
cost and what the popular local dishes are there, it might be difficult
for him/her to specify attributes (e.g. a category and price range
of restaurants), or make a keyword query that represents his/her
search intent. By using a content-based geographic object retrieval
system, a user is able to make an example query without knowledge
of the target domain by selecting familiar source domain examples
that are relevant to his/her search intent.
A challenge in content-based geographic object retrieval is to
find relevant instances from a target domain D(T ) based on examples selected from a heterogeneous source domain D(S) . When the
two domains are dissimilar, examples selected from D(S) are not
always similar to relevant instances from D(T ) . For example, assume that the user who visits Kyoto selects relatively inexpensive
restaurants serving popular local dishes in Beijing. Such restaurants in Beijing and Kyoto are dissimilar since the average cost
and popular local dishes are different in the two domains. Therefore, the content-based geographic object retrieval system has to
find instances semantically similar to the given examples against
superficial dissimilarity between instances from heterogeneous domains. Although similar problems have been recognized as domain

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

811

Blitzer et al. proposed structural correspondence learning (SCL)
for domain adaptation [4, 5]. SCL first extracts pivot features,
which behave in the same way in different domains. The correlation with the pivot features is then used to identify correspondences
among features from different domains. Each instance is represented by the correspondence together with its own features. SCL
has been successfully applied to part of speech tagging and sentiment classification, and is considered a state-of-the-art method for
domain adaptation. Since our proposed method is similar to SCL
in terms of incorporating corresponding points not corresponding
features, we included SCL as a baseline and compared it with our
proposed method.
Several domain adaptation methods for document classification
tasks have been proposed recently. Dai et al. proposed a domain
adaptation method based on co-clustering [10]. The method clusters words and documents simultaneously in a target domain, restricting word clusters by labels from a source domain and transferring the labels to documents in the target domain. Ling et al.
incorporated spectral clustering for domain adaptation to construct
a new feature space based on the clustering results [16]. Spectral clustering is applied to documents in a source and target domain under the source domain supervision, so that documents in
the target domain are separated as much as possible. Their method
then represents documents by their vector that encodes clusters to
which a document belongs. Xue et al. proposed a method based
on probabilistic latent semantic analysis (pLSA) for domain adaptation [28]. They assumed that different domains include common
latent topics, applied pLSA to documents under the source domain
supervision, and classified documents from a target domain using
common topics obtained through the pLSA. Wang et al. tackled a
problem of cross-language document classification [27]. They argued that cross-language document classification is a challenge because of cultural discrepancies and translation ambiguities. It is an
interesting and similar motivation to our geographic object retrieval
example in which the cultural discrepancy may prevent training an
accurate model.
In addition to document classification tasks, some studies have
focused on domain adaptation for learning to rank tasks. Cai et al.
proposed active query selection for ranking adaptation, which seeks
instances to be labeled in the target domain for an effective training [7]. Wang et al. tackled the problem of learning to rank, assuming that there is a common latent space in different domains, and
mapped instances onto the space preserving the label order [26].
Gao et al. proposed a method based on re-weighting labeled instances for learning to rank in heterogeneous domains [11]. The
main idea of their approach is that the label information of source
domain instances similar to ones from a target domain is more helpful in predicting the label of target domain instances than dissimilar
source domain instances. Although we also focus on an information retrieval task, our problem described in the following section
uses pairs of an instance and binary label; thus, we cannot simply
adapt their methods to our problem and compare them with our
proposed method.
Nakajima and Tanaka proposed a relative query processing
method [18]. A relative query consists of a set of instances and
an instance in the set. Their relative query processing method represents an instance x in a set X as a vector from the centroid of
X to x. The differences between domain adaptation for contentbased retrieval and the relative query processing method are the
following two aspects: (i) a content-based retrieval system returns
ranked instances, while the relative query aims to find only a relevant instance, and (ii) domain adaptation for content-based retrieval addresses a problem that the same features may have dif-

adaptation [20], domain adaptation for content-based retrieval has
not been addressed in the literature. We discuss related work on
domain adaptation in Section 2, and formalize and characterize domain adaptation for content-based retrieval in Section 3.
We propose a method for accurately predicting relevant instances
in a domain based on selected examples in another heterogeneous
domain. We assume that different domains include corresponding
points that may not be the same in terms of their representation but
play the same role in heterogeneous domains. For the Kyoto and
Beijing example, both domains include the most expensive restaurant, the average restaurant, and a restaurant serving the most popular dishes there. These kinds of points are identified through aggregation based on relative and domain-independent operation; we
call these points relative aggregation points (RAPs). We assume
that RAPs in different domains correspond, and construct a new
feature representation for each instance based on RAPs from the
source and target domains. Several kinds of similarities are measured between RAPs and instances, each of which is used to represent instances in the new feature space. Incorporating features represented by RAPs bridges the gap between heterogeneous domains
and enables us to capture the place of an instance in a domain.
Our experiment was designed to provide answers to two research
questions on domain adaptation for content-based retrieval. The
first research question is whether domain adaptation for contentbased retrieval is necessary. In other words, are relevant data
different for the same search intent in different locations and
datasets? We answered this research question based on a test collection developed for content-based geographic object retrieval,
which includes relevant data for 100 queries (20 search intents in
five domains). Our experimental result showed that mean average precision (MAP) and normalized discounted cumulative gain
(nDCG) [12] in out-domain settings (D(S) = D(T ) ) were significantly lower than those in in-domain settings (D(S) = D(T ) ). This
finding suggests that relevant data can vary in different search locations and datasets and has important implication for relevance judgment in information retrieval tasks. The second research question
is whether domain adaptation based on RAPs can bridge the domain difference and significantly improve the search performance
of content-based retrieval in heterogeneous domains. In our experiment based on the aforementioned test collection, our proposed
method showed significant improvements on MAP and nDCG over
baseline methods.
The remainder of this paper is structured as follows. We describe
related work on domain adaptation in Section 2, and formalize and
characterize domain adaptation for content-based retrieval in Section 3. In Section 4, we propose a method for domain adaptation
based on RAPs. In Section 5, we describe the test collection developed for content-based geographic object retrieval and show experimental results in Section 6. We conclude with a summary of our
findings and future work in Section 7.

2.

RELATED WORK

Domain adaptation [20] has been addressed in several tasks such
as part of speech tagging [5], text classification [4, 16, 27, 28], and
learning to rank [7, 11, 26]. The motivation behind domain adaptation is that a model trained in a source domain D(S) does not work
well in a heterogeneous target domain D(T ) . The aforementioned
content-based retrieval problem is also a domain adaptation problem since it is a process for modeling a search intent from examples
selected in a domain, and inferring relevant instances in another by
using the model. In this section, we describe related work on domain adaptation and clarify our contributions of this paper.

812

ferent meanings in different domains, while the relative query processing method does not and only addresses the relativity of the
feature value. Moreover, we included their method as a baseline in
our experiment and showed the difference in effectiveness between
our proposed method and the relative query processing method.
We assume that different domains include points that play the
same role, like the assumption of common features in most of the
previous work. However, we would like to emphasize a fundamental difference; we do not assume that common features in different
domains correlate with labels in the same way. The reason for this
is that even the same features may indicate different meanings in
different domains. For example, sushi restaurants in Beijing and
Kyoto that are exactly the same may be relevant in Kyoto, but irrelevant in Beijing for a user who wants a restaurant serving local
dishes. Moreover, a test collection we developed for the contentbased geographic object retrieval is different from commonly used
test collections such as 20 Newsgroups1 in some aspects. Our test
collection includes different types of features: continuous, such as
the cost and distance from the nearest station, discrete, such as category information, and high-dimensional, such as the description of
a restaurant. The label distribution is usually balanced in the sentiment and category classification tasks addressed in previous work,
while the amount of relevant instances to a search intent is relatively small in our test collection and is different in different search
intents and domains. The unbalanced label distribution might impair the performance of previously proposed methods, as revealed
in our experiment.

3.

an arbitrary information retrieval metric is applied to a totally ordered instance set (X (T ) , ≺).
For example of content-based geographic object retrieval, a user
can select a region (e.g. Beijing) as the source domain D(S)
and another (e.g. Kyoto) as the target domain D(T ) . In this
case, their feature spaces X (S) and X (T ) are the same: a set
of all possible restaurant instances. A set of restaurant instances
(S)
(S)
(S)
in D(S) is denoted by X (S) = {x1 , x2 , x3 }, and we assume that the set is obtained by sampling instances following
the marginal distribution P (X (S) ). The user can select a sub(S)
(S)
set Q = {x1 , x2 } of X (S) , which can be represented as
(S)
(S)
(S)
D(S) = {(x1 , +1), (x2 , +1), (x3 , −1)}. A content-based
geographic object retrieval system receives D(S) as a query, and
(T )
(T )
(T )
ranks a set of restaurant instances X (T ) = {x1 , x2 , x3 } sam(T )
pled from D .
The most important difference from domain adaptation tasks addressed in previous work is that no label information in the target
domain is available during the training process in content-based retrieval. Relevant instances in the target domain depend on the user’s
search intents; thus, it is not realistic to prepare labels for all possible search intents. Note that in many cases all the instances from
the target domain are not available for training in ad-hoc information retrieval due to the amount of data. Instead of using all the
data, one can sample some data from the target domain or re-rank
top ranked instances using them as target instances. For contentbased geographic object retrieval, it is feasible to use all the target
instances because the size of instances in a region the user focuses
on is generally small (1,000 at most).
The difference between domains is characterized by the difference in feature spaces and marginal distributions. In content-based
geographic object retrieval, for example, features spaces in different domains are the same, i.e. X (S) = X (T ) . A restaurant instance is represented as three elements in any domain: the price
range, category, and description of a restaurant in natural language.
On the other hand, marginal distributions in different domains can
be different, i.e. P (X (S) ) = P (X (T ) ). For example, there are
many expensive restaurants in Tokyo, many Japanese-style restaurants in Kyoto, and many restaurants serving spicy dishes in Beijing. Recent studies proposed methods for quantifying the domain
difference by using A-distance for estimating the learning bound
for domain adaptation [3, 21]. In our experiment, we demonstrated
the domain difference measured by A-distance dA for ensuring that
domains we used are different from each other. We used the inequation D(S) = D(T ) if and only if the A-distance dA (D(S) , D(T ) ) is
greater than a threshold .
In summary, domain adaptation for content-based retrieval is defined as follows: given label-instance pairs D(S) from a source domain D(S) and instances X (T ) from a target domain D(T ) such that
D(S) = D(T ) , domain adaptation for content-based retrieval is the
problem of predicting a ranking function f that maximizes an information retrieval metric of a totally ordered instance set (X (T ) , ≺).

DOMAIN ADAPTATION
FOR CONTENT-BASED RETRIEVAL

In this section, we define the problem of domain adaptation for
content-based retrieval and discuss domain difference.
A domain consists of a feature space X and a marginal distribution P (X) [20]; thus, we represent a domain as D = (X , P (X)).
A label space is denoted by Y . Instances X ⊂ X are sampled following P (X), and a label y ∈ Y is assigned to each
instance x ∈ X. Label-instance pairs are denoted by D =
{(x1 , y1 ), (x2 , y2 ), . . .}.
We set two domains in content-based retrieval: a source domain
D(S) = (X (S) , P (X (S) )), from which instances to be selected as a
query are sampled, and a target domain D(T ) = (X (T ) , P (X (T ) )),
from which instances to be retrieved are sampled. From instances
X (S) sampled from the source domain D(S) , the user can select
a subset Q ⊂ X (S) that consists of instances relevant to his/her
search intent. The example selection can be interpreted as a labeling process on the instances X (S) , and label-instance pairs are
denoted by D(S) = {(x(S) , +1) | x(S) ∈ Q} ∪ {(x(S) , −1) |
x(S) ∈ (X (S) − Q)}, where a label space Y (S) comprises −1
(nonselected examples) and +1 (selected examples). Therefore, a
query in content-based retrieval is represented as the set of labelinstance pairs D(S) . On another front, instances sampled from the
target domain D(T ) are denoted by X (T ) , and label-instance pairs
are denoted by D(T ) , in which a label represents binary relevance
or graded relevance for a search intent; thus, a label space Y (T ) is
a set of integer values. A content-based retrieval system ranks the
instances X (T ) based on a ranking function f : X (T ) → R trained
by label-instance pairs D(S) and target instances X (T ) , where a
(T )
(T )
total order ≺ on the instances X (T ) is given by xi ≺ xj ⇔
(T )

f (xi

4. DOMAIN ADAPTATION BASED ON
RELATIVE AGGREGATION POINTS
We propose a domain adaptation method for content-based retrieval in this section. Our assumption is that different domains
include corresponding points that may not be the same in terms of
their representation, but play the same role in heterogeneous domains. We first introduce RAPs that we believe to be such points
and then propose a method for constructing a new feature representation for each instance based on RAPs. We also introduce several

(T )

) ≤ f (xj ). To evaluate content-based retrieval systems,

1
http://people.csail.mit.edu/jrennie/
20Newsgroups/

813

Source Domain

Target Domain

categories of the maximum cost points in different domains as the
(S)
(T )
same, i.e. c3 = c3 . For the example of Kyoto and Beijing,
the maximum cost point in Kyoto and that in Beijing are different:
a restaurant serving Kyoto cuisine for 10,000 JPY and one serving Beijing duck for 5,000 JPY. By using the maximum cost point
in Kyoto and Beijing, we assume that 10,000 JPY corresponds to
5,000 JPY and Kyoto cuisine corresponds to Beijing duck. Kyoto
cuisine in Kyoto is considered the same as Beijing duck in Beijing since those dishes are delicacies in each domain. Based on the
above assumption, we can model each instance by the similarity
to RAPs, where instances in different domains are similar if their
features are similar to ones of RAPs.
We give a concrete feature representation based on RAPs. Let
H = {h1 , h2 , . . . hm } denote a set of similarity functions between
two instances, and A = {a1 , a2 , . . . an } denote a set of RAPs in
a domain D. For example, H includes the similarity in terms of
the cost and that in terms of the category. By using a function
φ : X → Rmn , we transform an instance x ∈ X into a mndimensional vector φ(x). The function φ is defined as follows:

Category

Category
3
2

2

1

3

1

Cost

Cost

Figure 1: RAPs and construction of a new feature representation. Black stars are RAPs, and white circles are instances.
Stars that have the same number are corresponding.
examples of the RAP for geographic object retrieval at the end of
this section. Although this domain adaptation method is a framework and can be applied to any type of instances, we use an example of a restaurant instances for simplicity. A simplified restaurant
instance x has two attributes: the cost xb (continuous) and category
xc (a set of category values such as Japanese-style and sushi), and
can be represented as a d-dimensional vector x if needed.

φ(x) = (φ1 (x), φ2 (x), . . . , φn (x))

(3)

where φi (x) = (h1 (x, ai ), h2 (x, ai ), . . . , hm (x, ai )) and each dimension corresponds to the similarity to a RAP measured by a
similarity function. Combining the vector representation x of an
instance x and the feature representation based on RAPs φ(x), we
obtain an augmented feature representation (x, λφ(x)), where λ is
a parameter that determines the importance of the feature representation based on RAPs. Note that the vector x and mn-dimensional
vector φ(x) are normalized by their L2 -norm in practice.
RAPs are estimated separately in the source and target domains.
Letting φ(S) denote a function based on RAPs estimated in the
source domain and φ(T ) denote one in the target domain, a source
instance x(S) ∈ X (S) is converted into (x(S) , λφ(S) (x(S) )), while
x(T ) ∈ X (T ) is converted into (x(T ) , λφ(T ) (x(T ) )). Thus, our
method enables us to take into account ordinary similarity (i.e. the
similarity between x(S) and x(T ) ), as well as the similarity based
on RAPs (i.e. the similarity between φ(S) (x(S) ) and φ(T ) (x(T ) ).
The motivating example, relatively inexpensive restaurants serving popular local dishes, is processed by using RAPs in the following way. Relatively inexpensive restaurants should be similar to
the minimum cost point in terms of the cost, and restaurants serving local dishes may be similar to restaurants of a category that
frequently appears in a domain. It is also possible that expensive
or average restaurants serve popular local dishes, and the similarity to such restaurants in terms of the category is helpful to capture
what the popular local dishes are. Therefore, we can find relevant
restaurants in another heterogeneous domains by calculating the
similarity between similarities to those RAPs.

4.1 Feature Representation based on Relative
Aggregation Points
A RAP is the expectation of instances in a domain that are classified into a certain class. Let us consider the maximum cost point in
a domain D = (X , P (X)), which is the most expensive restaurant.
Given a set of instances X sampled from the domain D, we can easily identify the most expensive restaurants within X, and represent
these restaurants as ψMAX (X) = argmaxx∈X xb , where xb is the
cost of x. However, a set of instances X might not be representatives sampled from the domain D. For example, the set of instances
X may include an extremely expensive out-lier despite the low average cost of the domain. Therefore, we estimate the expectation of
the most expensive instances in all possible instance sets sampled
from a domain.
Given a domain D and a subset function ψ : 2X → 2X (2X is a
collection of subsets of X ), the RAP aψ is defined as follows:
Z
aψ =
xP (x|ψ)dx,
(1)
x∈X

where x is the vector representation of an instance x, and P (x|ψ)
is the probability that x is sampled from a subset ψ(X) of any instance set X sampled from the domain D. The probability P (x|ψ)
is given by marginalizing out a set of instances X:
Z
P (x|ψ) =
P (x|ψ, X)P (X)dX,
(2)
X∈2X

4.2 Examples of Relative Aggregation Point

where P (x|ψ, X) is the probability that x is sampled from a subset
ψ(X). Intuitively, the subset function ψ indicates the deterministic membership of an instance, while probability P (x|ψ) indicates
probabilistic membership, which is more stable than the deterministic one.
RAPs are relative because they can vary in different domains for
the same subset function ψ. Figure 1 illustrates RAPs as black stars
in two domains, where the feature space includes two dimensions:
cost and category (e.g. the Japaneseness of a restaurant). Black star
1 is the minimum cost point, star 2 is the average cost point, and
star 3 is the maximum cost point, all of which are different between
the two domains. We assume that RAPs in different domains correspond and features of RAPs also correspond. In Figure 1, we regard
(S)
(T )
costs of the minimum cost points as the same, i.e. b1 = b1 , and

Our proposed method is a framework, and effective RAPs highly
depend on the task. We present some examples of the RAPs that
are effective for the content-based geographic object retrieval. Note
that methods for estimating RAPs from finite samples are not essential for the purpose of this paper; thus, we describe these methods in Appendix A.

AVG. The most normal restaurant may be considered equivalent
in different domains. The average (AVG) RAP is the average of
instances in a domain. The subset function ψAVG is defined as
ψAVG (X) = {x|x ∈ X}; thus, the probability P (x|ψAVG ) is
equal to the marginal distributionR P (x). Therefore, the AVG RAP
is defined as follows: aψAVG = x∈X xP (x)dx.

814

MAX/MIN. The maximum (MAX) (and minimum (MIN)) cost
may correspond in different domains as mentioned earlier. The
MAX RAP is the expectation of instances that has the maximum cost in a domain. The subset function ψMAX is defined
as ψMAX (X) = argmaxx∈X xb , where xb is the cost of an inRstance x. The probability P (x|ψMAX ) is given by P (x|ψMAX ) =
P (x|b)P (b|ψMAX )db, using the probability P (b|ψMAX ) of
b∈R
cost b, which is the maximum cost in any instance set, and the probability of x conditionedR by b. Therefore,
the MAX RAP is defined
R
as follows: aψMAX = x∈X x b∈R P (x|b)P (b|ψMAX )dbdx.
The MIN RAP is obtained in the same way as the MAX
RAP, where the subset function ψMIN is defined as ψMIN (X) =
argminx∈X xb .

Table 1: The number of instances (#), the average cost (Avg.),
and relatively frequent category values in each domain.
Domain
Kyoto
Tokyo
Sapporo
Fukuoka
Nagoya

Avg. (JPY)
4,530
6,410
3,490
3,760
3,590

Frequent category values
Japanese-style
Western-style
Seafood
Nabe, Seafood
Izakaya

words) are text, which were processed by MeCab4 , a Japanese morphological analyzer. Only nouns and adjectives were used to construct a text vector xt whose elements represent the term frequency
in those text attributes. Feature selection was carried out excluding terms that occured less than three times in our test collection.
The category attribute is a set of category values, e.g. {bar, sushi,
Japanese-style}. We treated the category attribute the same as text
attributes (i.e. bag-of-words), and represented a category vector as
xc . Although category values are hierarchically structured in the
GourNavi Web service (e.g. top categories include Japanese-style
and Western-style, and sub categories under the Japanese-style category include sushi and tempura), we did not use the hierarchical
structure for simplicity. The text and category attributes were separately processed and were weighted by the tf-idf method. To be
more specific, the i-th element of the text vector xt for an instance
x is tf(ti ) log (N/df(ti )), where tf(ti ) is the frequency of a term
ti in the instance x, df(ti ) is the number of instances that include
the term ti , and N is the number of instances in our test collection.
The category vector xc is weighted in the same manner. To process the cost attribute together with text and category attributes, we
discretized the cost and represented it as a probability distribution
on discrete values. The cost b was assumed to follow a normal distribution N (z; b, σ), and was discretized by the probability of each
z in a set of real values Z. The cost b was represented by a |Z|dimensional vector xb whose element corresponds to each element
z ∈ Z and is N (z; b, σ). In our test collection, Z includes every
500 from −5,000 to 40,000, and σ = 1,000. This discretization
is not essential but is convenient for computation of the similarity
between restaurant instances. Through the above processes, an instance was represented as a d-dimensional vector x = (xt , xc , xb ),
where d =15,670 in our test collection.

FREQ. Relatively frequent (FREQ) categories may correspond
in different domains. For example, Japanese-style in Kyoto and
Chinese-style in Beijing represent the same meaning, i.e. a popular category in a domain. The subset function ψFREQ is defined as ψFREQ (X) = {x|x ∈ X ∧ (∀c ∈ xc )freqX (c) >
freqA (c)}, where freqX (c) is the frequency of category value
c in a set of instances X, and A is a set of instances sampled from the entire domain (or all the domains we have). This
subset function ψFREQ returns instances with categories that are
more frequent in X than those in A. A FREQ RAP is obRtained asR follows in a similar manner to MAX/MIN: aψFREQ =
x C∈C P (x|C)P (C|ψFREQ )dCdx, where C is a collection
x∈X
of subsets of category values.

CLS. RAPs can be automatically defined through clustering. The

subset function ψCLS is defined as ψCLS (X) = {x|x ∈ X ∧ x ∈
S}, where a set of instances S is obtained through clustering applied to instances in both the source and target domains. Since
P (x|ψCLS ) is independent from a way of
R samplings, the CLS RAP
is simply defined as follows: aψCLS = x∈X xP (x|S)dx.

5.

#
510
511
514
500
529

TEST COLLECTION

In this section we describe a method for developing our test collection for content-based geographic object retrieval2 , and show the
statistics of the test collection including the degree of differences
between domains.

5.2 Domains
We chose five major cities (Kyoto, Tokyo, Sapporo, Fukuoka,
and Nagoya) in Japan. We selected a region from each city that
contain about 500 restaurant instances, and used the five regions as
domains in our test collection. Table 1 shows the number of instances, the average cost, and relatively frequent category values in
each domain. The average cost in Tokyo is especially high, that in
Kyoto follows, and there is no significant difference across other
three domains. There are many Japanese-style restaurants in Kyoto, many Western-style ones in Tokyo, and many izakaya ones in
Nagoya. The number of seafood restaurants is relatively large in
Sapporo and Fukuoka, and that of nabe restaurants is slightly large
in Fukuoka. Those differences in the average cost and category
indicate the difference between marginal distributions, and may affect the performance of content-based geographic object retrieval.
To measure the difference between the five domains and to ensure that domains in our test collection are significantly different
from each other, the A-distance dA was calculated between all the
pairs of domains in our test collection. The A-distance was introduced by Ben-David et al. [3], and was defined as the upper bound
of the difference between the two marginal distributions P (X (S) )

5.1 Instances
Our test collection includes 46,945 restaurant instances crawled
from a Japanese restaurant Web site GourNavi3 , which is one of
the biggest sites that provide restaurant information in Japan. We
opted to target at restaurant data for several reasons. Restaurant
search is usually conducted with an ambiguous search intent that
cannot be easily expressed by using keywords. Moreover, the difference between domains (or regions) is originally derived from the
cultural difference and inherits its properties; thus, the difference is
generally large and difficult to understand. Therefore, restaurant
search is one of the most attractive applications of content-based
geographic object retrieval.
Restaurant instances in our test collection include several attributes such as the cost, category, name, genre, and introduction
of a restaurant. The name (e.g. Fujiyama), genre (e.g. traditional
Japanese sushi bar), and introduction (a detailed description of a
restaurant written in natural language, which comprises about 200
2
The dataset is available at http://www.mpkato.net/
datasets/.
3
http://www.gnavi.co.jp/

4

815

http://mecab.sourceforge.net/

and P (X (T ) ). To compute the A-distance from finite samples of
the source and target domains, we approximated the distance by a
method introduced by Rai et al. [21].
We calculated the A-distance of all the pairs of domains in
our test collection, which ranges from 0.537 (Fukuoka-Nagoya) to
0.745 (Kyoto-Tokyo). Rai et al. [21] reported the A-distance across
eight types of user reviews in the multi-domain sentiment dataset5 ,
and the A-distance ranges from 0.0459 (Kitchen-Apparel) to 0.762
(DVD-Book). Comparing the A-distance in our test collection and
the multi-domain sentiment dataset, we found that our domains are
as different from each other as domains used in some previous
work [4, 21], even though all the instances are restaurants and all
the domains are domestic cities. Therefore, we will use the inequation D(S) = D(T ) for any pair of the domains hereinafter since it
was revealed that any pair of the domains is significantly different,
i.e. dA (D(S) , D(T ) ) > , as mentioned in Section 3.

which correspond to the five domains, Kyoto, Tokyo, Sapporo,
Fukuoka, and Nagoya. We asked the subjects to select relevant
instances in a domain corresponding to their home area because
it is difficult to judge the relevance without domain knowledge.
Note that the recruitment was controlled so that the number of subjects is uniform for each sex, age, and region combination, i.e. 20
subjects were sampled from each (male and female), (20’s, 30’s,
40’s, 50’s, and 60’s), and (Kyoto, Tokyo, Sapporo, Fukuoka, and
Nagoya) combination. The online questionnaire service we used is
hosted by a domestic company, which delivered a mail to users and
asked to visit our Web system. Subjects were rewarded through the
company if they finished the whole task we instructed.
The task we asked the subjects to work on consists of two steps.
In the first step, the subjects were asked to examine 30 restaurant
instances randomly sampled from a domain for subjects’ better understanding of the domain. To make the subjects carefully examine each restaurant instance, we asked subjects to find a couple of
restaurants they wanted to visit. In the second step, two search intents were assigned and presented in random order to each subject.
The subjects were asked to select as many restaurants relevant to a
presented search intent as possible. The subjects were allowed to
use a simple search engine, and can search for restaurants by specifying the upper and/or lower limits of the cost, the top category (e.g.
Japanese-style, Western-style, and Chinese-style), and sub category
(e.g. sushi, Italian, and French). The search engine returns a list of
restaurant instances that satisfy all the specified conditions, and the
search result ranking was randomized to reduce the ranking bias.
Relevance judgments for each search intent in each domain were
conducted by 20 subjects, where two subjects were selected from
each sex and age group.

5.3 Search Intents
To select relevant instances in our test collection, which were
used as a query in the source domain and as the ground truth in the
target domain, search intents had to be documented for being understood by subjects. To this end, we manually gathered questions
from the restaurant category in Yahoo! Chiebukuro 6 (Japanese
Yahoo! Answers), where the asker asks restaurants that are relevant to his intents. We opted to use Yahoo! Chiebukuro as the
resource of search intents because complex and realistic intents are
posted there. A hundred of questions were extracted as search intents from Yahoo! Chiebukuro, and were classified into two types
in terms of domain-dependency for characterizing each search intent. Domain dependency is the degree of variability of a search
intent in different domain, and was subjectively evaluated by two
assessors we hired only for this task. We asked them to score the
extracted search intent on a scale from one (independent from the
domain) to five (dependent on the domain). As their agreement
on 100 questions was low: 0.233 in terms of quadratic weighted
kappa [24], we separated the labels into domain-dependent (a score
is three or more) and domain-independent (a score is less than
three), and excluded all the questions on which the two assessors
disagreed. As a result, 57 questions remained, of which 20 (35.1%)
questions were domain-dependent and 37 (64.9%) questions were
domain-independent. We selected ten questions each from the sets
of domain-dependent and domain-independent questions, and included them as 20 search intents in our test collection.
An example of domain-dependent search intents is “I’m going to
<v> for business trip with my boss. Do you have any recommendations for dinner? It would be great if you would tell us restaurants
for from 3,000 to 5,000 JPY that provide <v>-specific dishes with
Japanese sake. We are 30’s and 40’s. Either meat or fish dish is
OK.” On the other hand, an example of domain-independent search
intents is “Please show me Western restaurants in <v>. I want a
Italian or French restaurant that has a lunch course for 5,000 JPY
without wine,” where the variable <v> is replaced with the name
of a domain, or a region (e.g. Kyoto).

5.5 Post-Processing of Relevance Judgments
As Alonso and Baeza-Yates indicated [1], it is important to keep
the quality of relevance judgments when the assessment is conducted through an online service, or crowd sourcing. We ensured
the quality of our test collection by conducting the following two
steps. First, subjects who finished a task within five minutes were
filtered out, as it was supposed to take more than five minutes in
a preliminary system test. Second, we manually assessed all the
relevant instances for each search intent in each domain, and removed subjects who made obviously wrong relevance judgments.
For example, we removed a subject who selected a restaurant for
1,000 JPY as relevant when the subject was asked to find restaurants for 5,000 JPY. After the removal of low-quality subjects, there
remained 579 subjects and 6,082 relevance judgments. On average,
11.6 subjects worked on each search intent in each domain, and
made 121 relevance judgments. Finally, we merged duplicate relevance judgments and generate graded relevance by regarding the
multiplicity as the grade. The average number of relevant instances
per domain and search intent was 42.1, and the standard deviation
was 33.3.
Our test collection contains many subjective search intents and
needed as many subjects as possible for avoiding the individual
bias, whereas it was difficult to ask individual to evaluate all the
restaurant instances due to the cost limitation. To gain the coverage of relevance judgments, we assigned 20 subjects to each search
intent and randomized the ranking result returned by the simple
search system. However, the relevance judgment we conducted is
not always complete since it is not guaranteed that all the restaurant
instances in a domain were examined by the subjects. As Buckley
and Voorhees’s study on retrieval evaluation with incomplete information suggests [6], traditional evaluation metrics such as MAP

5.4 Procedure of Relevance Judgment
We recruited 1,000 subjects living in five regions through an online questionnaire service, and asked them to select instances relevant to each search intent in their home area. The five regions are
Kyoto city in Kyoto, 23 special wards in Tokyo, Sapporo city in
Hokkaido, Fukuoka city in Fukuoka, and Nagoya city in Aichi,
5
http://www.cs.jhu.edu/~mdredze/datasets/
sentiment/
6
http://chiebukuro.yahoo.co.jp/

816

elected examples, which are considered typical content-based retrieval methods. SVM is a method based on a linear support vector machine using label-instance pairs D(S) , which trains a model
for binary classification, and the ranking function f is defined as
f (x) = wT x + b, where w and b are parameters that determine
the decision boundary of the SVM.
Baseline domain adaptation methods are transductive support vector machine (TSVM), structural correspondence learning
(SCL), and relative cluster mapping (RCM), all of which use
knowledge of both the source and target domains. TSVM proposed
by Joachims [13] has been used as a baseline for domain adaptation
tasks [16, 28]. The ranking function of this baseline is predicted in
the same way as the SVM method. SCL was also compared with
our proposed method, which is a state-of-the-art method for domain adaptation [4, 5]. SCL constructs a new feature space and
represents each instance with additional features generated based
on pivots (features that behave similarly in the source and target
domains). We applied the SVM method to augmented vectors of
instances for predicting the ranking function f . RCM was selected
as a baseline since the problem definition and motivation are similar to ours [18]. RCM represents an instance by the vector from the
centroid of a set of instances to a vector representing the instance
itself. We then applied the SVM in the same manner as SCL.
Our proposed RAP-based domain adaptation method includes
a parameter λ, and has variants that use different RAPs. We set
λ = 1 (default) and included all the RAPs mentioned in Section
4.2 for the comparison. The best setting for those factors were
explored after comparison. A set of similarity functions involves
the cosine of the cost xb , that of the category xc , that of the text
xt , and that of x. The CLS RAPs were obtained by using CLUTO7
with default parameters. The number of clusters was set to 30. The
training was conducted using the SVM in the same manner as the
baseline domain adaptation methods.
The SVM and TSVM methods were implemented using
SVMlight8 , and the OSVM was implemented using LIBSVM9 . All
parameters were set to default by the software. SCL was implemented according to the paper authored by Blitzer et al. [4, 5]10 .
For SCL, we selected features that occur more than five times in
the source and target domains and showed the highest mutual information with respect to labels in the source domain. The number
of dimensions h was set to 50 according to a study using SCL for
comparison [19], and the number of pivots was set to 100, which is
smaller than the one used in the comparison study, due to the lack
of frequently occurring features in our test collection.

and precision at ten are not robust to substantially incomplete relevance judgments.
To assess the completeness of our relevance judgments, we used
a method of estimating the population size by multiple census proposed by Schumacher and Eschmeyer [23]. Their method conducts multiple samplings, which correspond to individual’s relevance judgments in our case, and estimates the population size,
which corresponds to the number of relevant instances in our case.
Having estimated the number of relevant instances, we can estimate the fraction of judged relevant instances to all the relevant
instances. In our test collection, 62.9% of relevant instances were
estimated to be actually judged by the subject. According to the
Buckley and Voorhees’s work [6], the moderate completeness of
our test collection possibly affects the system ranking based on
the evaluation metric: the Kendall correlation between the system
ranking based on MAP with incomplete relevance judgments and
that with complete relevance judgments was 0.9 on average in three
test collections from the Text REtrieval Conference (TREC). Although we cannot argue that our test collection is complete and is
able to accurately estimate the effectiveness of methods, the effect
of the moderate completeness of our test collection is supposed to
be small. Moreover, we used an evaluation metric for graded relevance, where relevant instances with a low grade make less impact
on the evaluation result than ones with a high grade. As the maximum grade per search intent and region is 4.34 on average in our
test collection, relevant instances that were not judged by any subjects are not seriously affect the evaluation metric compared to the
binary relevance case.

6.

EXPERIMENT

We answer the following two research questions through our experiment: whether domain adaptation for content-based retrieval
is necessary, and whether domain adaptation based on RAPs can
significantly improve the search performance of content-based retrieval in heterogeneous domains.

6.1 Experimental Settings
We selected a domain as source D(S) and another as target D(T )
from our test collection. The input in our problem was a set of pairs
of a binary label and instance D(S) from the source domain D(S)
as mentioned in Section 3; thus, a label +1 was assigned to relevant
(selected) source domain instances, while −1 was assigned to irrelevant (nonselected) source domain instances, where the grade for
relevance was ignored since the user does not grade but just selects
instances as a query in content-based retrieval. A content-based
retrieval method receives D(S) and target instances X (T ) and predicts an optimal ranking function f to rank instances X (T ) from
the target domain D(T ) . We used both MAP and nDCG [12] in our
experiment for evaluating the results in terms of binary relevance
and graded relevance. For MAP, relevant instances were defined
as ones that were judged as relevant by at least one subject. For
nDCG, we set the rank threshold to 10 because it is the common
number of search results shown in a search engine result page.
Simple baselines are methods without any knowledge of the
target domain: nearest neighbor search (NNS), one-class SVM
(OSVM), and SVM. NNS ranks target instances based on the similarity to the centroid of selected instances in D(S) . OSVM is
a method incorporating the one-class SVM [8, 22], which trains
a model for binary classification using only selected instances in
D(S) , and the ranking function f is defined as f (x) = wT x − ρ.
Note that x is a vector representation of an instance x, and w
and ρ are parameters that determine the decision boundary of the
one-class SVM. The NNS and OSVM methods do not use nons-

6.2 Is Domain Adaptation Necessary for
Content-based Retrieval?
We first present an answer to the first research question: is domain adaptation for content-based retrieval necessary? The SVM
method, which is a baseline that does not use any domain adaptation technique, was applied to all the search intents. In an indomain setting (D(S) = D(S) ), we used a method similar to k-fold
cross validation, where instances in a domain are split into k bins,
of which k − 1 bins are used as the source and the rest are used
as the target. In our study, we set k to 5 and recorded the average
7
http://glaros.dtc.umn.edu/gkhome/views/
cluto/
8
http://svmlight.joachims.org/
9
http://www.csie.ntu.edu.tw/~cjlin/libsvm/
10
To test our implementation of SCL, we applied it to a dataset
Blitzer et al. developed [4], which is the one we referred to in
Section 5.2. The average accuracy was 0.774, which is close to the
average accuracy Blitzer et al. reported.

817

Kyoto
Tokyo
Sapporo
Fukuoka
Nagoya

Kyoto
0.620
0.460
0.492
0.505
0.476

Tokyo
0.463
0.596
0.432
0.472
0.466

Target
Sapporo Fukuoka
0.520
0.511
0.449
0.501
0.638
0.524
0.510
0.588
0.538
0.527

Table 3: The average MAP for each search intent in the outdomain setting.

Domain-dependent

Source

Table 2: The average nDCG@10 over all the search intents
in each combination of the source and target domains. The
bold font indicates in-domain settings, while the others are outdomain settings.
Nagoya
0.431
0.389
0.506
0.477
0.599

Domain-independent

nDCG@10 over k runs. For an out-domain setting (D(S) = D(T ) ),
we applied a similar method to the in-domain setting for comparison. A domain was used as the source and another was used as the
target, where instances in the source and the target domains were
split into k bins, of which k − 1 bins in the source domain were
used as input and a bin in the target domain was used as instances
to be retrieved. The query in our experiment was different for the
combination of the source and search intent. The number of unique
queries was 100 (5 domains × 20 intents). The target domain was
selected from the five domains; thus, we conducted 500 runs (5
domains × 100 queries).
Table 2 lists the average nDCG@10 over all the search intents
in each pair of the source and target domains. It is clear that the
average nDCG@10 in out-domain settings (D(S) = D(T ) ) was
lower than that in in-domain settings (D(S) = D(S) ). There was
a negative correlation between the average nDCG@10 and the Adistance (Pearson’s coefficient r = −0.678, p < 0.05). The negative correlation indicates that the more different domains are, the
more difficult it is to achieve high search performance. The average nDCG@10 for the in-domain and out-domain settings are
0.608 and 0.482, respectively. With a Welch’s t-test, we found a
significant difference between the in-domain and out-domain settings, t(179) = 5.89, p < 0.00111 . Moreover, the effect size of the
domain setting was 0.588 in terms of Cohen’s d, which is considered medium.
The significant difference in search performance between the indomain and out-domain settings suggests that domain adaptation
for content-based retrieval is necessary. This result also implies
that relevant data can vary even for the same search intent when
the search context is different. The difference in search context in
our case consists of the difference between datasets from which the
system retrieves data, the difference between search locations, and
the difference between user’s home areas. Recently, studies have
suggested that relevant data can vary for different users [25] and
different search devices [14]. In addition to such hypotheses, our
results pose new hypotheses stating that relevant data can vary for
different datasets and/or locations where the user searches, which
are still open questions since we did not separate the two effects.

Intent
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
Total

NNS
0.539
0.387
0.347
0.409
0.488
0.198
0.078
0.192
0.488
0.261
0.442
0.180
0.586
0.115
0.354
0.416
0.270
0.256
0.586
0.288

Inductive
OSVM
0.546
0.452
0.319
0.350
0.458
0.227
0.083
0.219
0.491
0.293
0.401
0.177
0.571
0.114
0.293
0.399
0.270
0.226
0.595
0.286

SVM
0.490
0.522
0.317
0.672
0.512
0.327
0.155
0.254
0.548
0.335
0.630
0.280
0.645
0.161
0.545
0.702
0.379
0.401
0.641
0.238

TSVM
0.417
0.439
0.344
0.558
0.506
0.329
0.150
0.206
0.561
0.305
0.581
0.233
0.627
0.109
0.506
0.609
0.317
0.312
0.532
0.203

Transductive
SCL
RCM
0.482
0.490
0.512
0.521
0.321
0.317
0.688
0.672
0.515
0.513
0.324
0.327
0.153
0.155
0.251
0.254
0.548
0.548
0.329
0.335
0.631
0.630
0.284
0.280
0.638
0.645
0.161
0.161
0.541
0.545
0.686
0.701
0.384
0.379
0.398
0.401
0.646
0.641
0.236
0.238

RAP
0.490
0.574
0.320
0.668
0.522
0.333
0.144
0.260
0.570
0.337
0.654
0.284
0.640
0.156
0.538
0.705
0.385
0.399
0.645
0.239

0.344†

0.338†

0.438†

0.392†

0.436†

0.443

0.438†

Domain-independent

Domain-dependent

Table 4: The average nDCG@10 for each search intent in the
out-domain setting.
Intent
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
Total

NNS
0.191
0.173
0.149
0.145
0.182
0.118
0.024
0.213
0.199
0.130
0.208
0.148
0.201
0.068
0.285
0.164
0.107
0.222
0.305
0.118

Inductive
OSVM
0.192
0.249
0.123
0.087
0.167
0.162
0.046
0.251
0.184
0.178
0.172
0.121
0.198
0.056
0.230
0.136
0.135
0.174
0.345
0.109

SVM
0.175
0.406
0.084
0.389
0.242
0.267
0.174
0.303
0.254
0.277
0.589
0.288
0.295
0.184
0.483
0.487
0.418
0.368
0.492
0.127

TSVM
0.176
0.277
0.151
0.315
0.250
0.213
0.121
0.219
0.223
0.226
0.582
0.162
0.262
0.038
0.513
0.375
0.230
0.206
0.378
0.081

Transductive
SCL RCM
0.171 0.177
0.415 0.406
0.103 0.084
0.367 0.386
0.260 0.242
0.261 0.267
0.170 0.174
0.311 0.303
0.258 0.254
0.269 0.278
0.617 0.589
0.296 0.288
0.283 0.295
0.194 0.184
0.509 0.483
0.496 0.486
0.475 0.418
0.356 0.368
0.493 0.492
0.128 0.120

RAP
0.173
0.420
0.086
0.395
0.283
0.268
0.168
0.309
0.282
0.266
0.631
0.295
0.325
0.183
0.492
0.486
0.440
0.323
0.540
0.121

0.168†

0.166†

0.315

0.250†

0.322

0.324

0.315

edge of the target domain, while “Inductive” methods do not.
The bold font indicates the highest score per search intent. Oneway ANOVA showed a significant effect of methods on MAP and
nDCG, F (6, 2793) = 16.1, p < 0.001 and F (6, 2793) = 40.0,
p < 0.001, respectively. With a paired t-test12 , we found significant
differences between baselines and the RAP-based domain adaptation method at α = 0.05, which are indicated by a dagger † at the
“Total” row in Tables 3 and 4.
Our proposed RAP-based domain adaptation method achieved
significant improvements over all the baseline methods in terms of
MAP, and over NNS, OSVM, and TSVM in terms of nDCG@10.
It can be seen that the RAP-based domain adaptation method
achieved high MAP and nDCG@10 especially for domaindependent search intents. SCL achieved comparable nDCG@10
scores but failed to improve the MAP. SCL selected some features
as pivots such as “private room” and “seafood,” which are considered common in any domains. The MAP and nDCG@10 of

6.3 How Effective Is Domain Adaptation
based on Relative Aggregation Points?
We compared our RAP-based domain adaptation method with
baselines, NNS, OSVM, SVM, TSVM, SCL, and RCM, to demonstrate its effectiveness. We conducted 400 runs (4 out-domains
× 100 queries) in this experiment. Tables 3 and 4 list the average MAP and nDCG@10 for each search intent in the outdomain setting, respectively. “Transductive” methods use knowl11

We also tested the difference between the in-domain and outdomain settings in terms of MAP (0.609 vs. 0.479) and found
a significant difference, t(157) = 5.35, p < 0.001, Cohen’s
d = 0.585.

12

Holm’s method was used to adjust the significance level for multiple comparisons.

818

0.328

Table 5: The relative MAP and nDCG@10 for all the combination of RAPs.

nDCG@10

0.326
0.324

CLS, MAX/MIN, AVG
CLS, MAX/MIN
CLS, MAX/MIN, Mode
CLS, MAX/MIN, AVG, FREQ
MAX/MIN, AVG
CLS, FREQ
CLS, AVG, FREQ
CLS, AVG
CLS
AVG
(None)
MAX/MIN, AVG, FREQ
MAX/MIN, FREQ
MAX/MIN
AVG, FREQ
FREQ

0.322
0.320
0.318

0.5

1.0

λ

1.5

2.0

Figure 2: The average nDCG@10 curve for the parameter λ.
RCM was almost the same as SVM, since the centroid vector of
instances had small values for all the dimensions in our case: the
value distribution was flattened by taking an average over all the
instance vectors. The TSVM was worse than SVM in many of the
search intents, probably because the number of relevant instances
per search intent was not balanced across domains, the same label distribution in the source and target domains is assumed with
TSVM. SVM showed higher MAP and nDCG@10 than methods
using only selected examples, i.e. NNS and OSVM. The difference between SVM and those methods is that SVM did use nonselected examples. Therefore, this result suggests that using not
only selected examples but also nonselected examples can improve
the search performance of content-based retrieval when most of the
relevant instances are selected in the source domain.
Our proposed RAP-based domain adaptation method showed
considerable improvements of nDCG@10 for search intents 5, 11,
and 19. Search intent 5 was “Please tell me a tranquil restaurant
appropriate for entertaining foreign guests in <v>. Our budget is
around 8,000 JPY per person,” search intent 11 was “I am looking for Western restaurants in <v> for Christmas dinner for 7,500
JPY per person (but not picky about the cost),” and search intent
19 was “Please show me Japanese restaurants or ones serving meat
for celebrating my father’s birthday in <v>. He cannot eat Italian dishes or drink alcohol. Our maximum budget is 40,000 JPY
for three people.” These intents include ambiguous conditions, e.g.
a category in search intent 5, and budget in the search intent 11,
which can be different in different domains. For example, Japanese
restaurants were often judged as relevant in Kyoto for search intent 5, while Western restaurants were usually relevant in Tokyo.
As shown in Table 1, Kyoto include many Japanese restaurant and
is famous for Japanese-style restaurants because it is one of the
oldest cities in Japan. On the other hand, there are many Westernstyle restaurants in Tokyo because it is a modern international city
in Japan. The difference in frequent categories might be lessened
by RAPs in our proposed method, which leads to high search performance. Similarly, relevant restaurants were more expensive in
Tokyo than those in Kyoto for search intent 11, and the cost difference was probably bridged by RAPs related to the cost.

MAP
100.0%
99.7%
99.4%
98.3%
96.1%
95.2%
93.9%
92.5%
91.2%
37.7%
0.0%
-68.7%
-139.2%
-144.1%
-225.9%
-383.6%

nDCG@10
98.4%
95.4%
94.5%
95.9%
16.1%
97.3%
94.5%
99.6%
100.0%
-8.7%
0.0%
-19.7%
-27.9%
-26.3%
-102.0%
-105.8%

percentage indicates the gain from (None), a method with no RAP,
normalized by the maximum gain of MAP or nDCG@10. The
FREQ RAP was the least effective in our test collection and rather
impaired search performance. In contrast, the CLS RAP was the
most effective in our experiment and improved search performance
even when only CLS was used. The MAX/MIN and AVG achieved
high MAP scores when they were used together. The best performance in terms of both MAP and nDCG@10 was obtained by the
combination of CLS, MAX/MIN, and AVG. As future work, we
will develop a method of finding effective RAPs for a given task.

7. CONCLUSIONS
We introduced the problem of domain adaptation for contentbased retrieval, and proposed a domain adaptation method based on
RAPs. We developed a test collection for content-based geographic
object retrieval, and provided answers to two research questions.
The first research question was whether domain adaptation for
content-based retrieval is necessary. Our experimental results
showed that evaluation metrics in an out-domain setting (D(S) =
D(T ) ) were significantly lower than those in an in-domain setting
(D(S) = D(T ) ). Therefore, we conclude that the answer to the
first research question is yes: search performance without domain
adaptation significantly decreases when the source and target domains are heterogeneous, and domain adaptation for content-based
retrieval is necessary.
The second research question was whether domain adaptation
based on RAPs can bridge the domain difference and significantly
improve the search performance of content-based retrieval in heterogeneous domains. Our experimental results answered the second research question by showing significant improvements over
baseline methods.
Future work involves applying the RAP-based method to other
datasets. In practice, developing a method for finding effective
RAPs is the most important part of future work, as it is difficult
to find effective RAPs manually. Although we assume that feature
spaces in the source and target domains are the same in this paper,
it is possible to apply our proposed RAP-based domain adaptation
method to heterogeneous feature spaces such as image and music feature spaces. The RAP-based method requires only a withindomain similarity function not a between-domain similarity function for domain adaptation, and enables a user to find popular music
by selecting popular pictures. There is also a need to further study
the effect of the domain differences, such as the difference between
datasets from which the system retrieves data, between search locations, and between user’s profiles, on search tasks.

6.4 Parameter Tuning
There are a parameter λ and choices of RAPs in our proposed
RAP-based domain adaptation method. We tested several values
for λ using all the RAPs mentioned in the experimental setting.
The average nDCG@10 over all the search intents are plotted in
Figure 2. The setting λ = 0.5 was relatively close to the SVM (no
RAP was used), while the setting λ = 2.0 was relatively close to
where only features derived from RAPs were used. The best λ was
1.3 in our test collection. These results suggest that both original
and domain-dependent features are necessary to better model the
similarity between instances in different domains.
We also tested the effectiveness of each RAP. Table 5 lists the
relative MAP and nDCG@10 for all combinations of RAPs. The

819

8.

ACKNOWLEDGMENTS

[21] P. Rai, A. Saha, H. Daumé III, and S. Venkatasubramanian.
Domain adaptation meets active learning. In Proc. of the
NAACL HLT 2010 Workshop on Active Learning for Natural
Language Processing, pages 27–32, 2010.
[22] B. Schölkopf, J. Platt, J. Shawe-Taylor, A. Smola, and
R. Williamson. Estimating the support of a high-dimensional
distribution. Neural computation, 13(7):1443–1471, 2001.
[23] F. X. Schumacher and R. W. Eschmeyer. The estimation of
fish populations in lakes and ponds. Journal of the Tennessee
Academy of Sciences, 18:228–249, 1999.
[24] J. Sim and C. Wright. The kappa statistic in reliability
studies: use, interpretation, and sample size requirements.
Physical therapy, 85(3):257–268, 2005.
[25] J. Teevan, S. Dumais, and E. Horvitz. Potential for
personalization. ACM Transactions on Computer-Human
Interaction, 17(1):1–31, 2010.
[26] B. Wang, J. Tang, W. Fan, S. Chen, Z. Yang, and Y. Liu.
Heterogeneous cross domain ranking in latent space. In Proc.
CIKM, pages 987–996, 2009.
[27] H. Wang, H. Huang, F. Nie, and C. Ding. Cross-language
web page classification via dual knowledge transfer using
nonnegative matrix tri-factorization. In Proc. of SIGIR, pages
933–942, 2011.
[28] G. Xue, W. Dai, Q. Yang, and Y. Yu. Topic-bridged plsa for
cross-domain text classification. In Proc. of SIGIR, pages
627–634, 2008.

This work was supported in part by the following projects:
Grants-in-Aid for Scientific Research (Nos. 24240013, 24680008,
and 22·4687) from MEXT of Japan, and a Kyoto University
GCOE Program entitled “Informatics Education and Research for
Knowledge-Circulating Society.”

9.

REFERENCES

[1] O. Alonso and R. Baeza-Yates. Design and implementation
of relevance assessments using crowdsourcing. In Proc. of
ECIR, pages 153–164, 2011.
[2] N. J. Belkin. Helping people find what they don’t know.
Communications of the ACM, 43:58–61, 2000.
[3] S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira.
Analysis of representations for domain adaptation. In Proc.
of NIPS, pages 137–144, 2006.
[4] J. Blitzer, M. Dredze, and F. Pereira. Biographies,
Bollywood, Boom-boxes and Blenders: Domain Adaptation
for Sentiment Classification. In Proc. of ACL, pages
440–447, 2007.
[5] J. Blitzer, R. McDonald, and F. Pereira. Domain adaptation
with structural correspondence learning. In Proc. of EMNLP,
pages 120–128, 2006.
[6] C. Buckley and E. M. Voorhees. Retrieval evaluation with
incomplete information. In Proc. of SIGIR, pages 25–32,
2004.
[7] P. Cai, W. Gao, A. Zhou, and K. Wong. Relevant knowledge
helps in choosing right teacher: active query selection for
ranking adaptation. In Proc. of SIGIR, pages 115–124, 2011.
[8] Y. Chen, X. Zhou, and T. Huang. One-class svm for learning
in image retrieval. In Proc. of ICIP, pages 34–37, 2001.
[9] T. Chia, K. Sim, H. Li, and H. Ng. A lattice-based approach
to query-by-example spoken document retrieval. In Proc. of
SIGIR, pages 363–370, 2008.
[10] W. Dai, G.-R. Xue, Q. Yang, and Y. Yu. Co-clustering based
classification for out-of-domain documents. In Proc. of
KDD, pages 210–219, 2007.
[11] W. Gao, P. Cai, K.-F. Wong, and A. Zhou. Learning to rank
only using training data from related domain. In Proc. of
SIGIR, pages 162–169, 2010.
[12] K. Järvelin and J. Kekäläinen. Cumulated gain-based
evaluation of ir techniques. ACM Transactions on
Information Systems, 20(4):422–446, 2002.
[13] T. Joachims. Transductive inference for text classification
using support vector machines. In Proc. of ICML, pages
200–209, 1999.
[14] M. Kamvar, M. Kellar, R. Patel, and Y. Xu. Computers and
iphones and mobile phones, oh my!: a logs-based
comparison of search users on different devices. In Proc. of
WWW, pages 801–810, 2009.
[15] M. P. Kato, H. Ohshima, S. Oyama, and K. Tanaka. Search
as if you were in your home town: geographic search by
regional context and dynamic feature-space selection. In
Proc. of CIKM, pages 1541–1544, 2010.
[16] X. Ling, W. Dai, G.-R. Xue, Q. Yang, and Y. Yu. Spectral
domain-transfer learning. In Proc. of KDD, pages 488–496,
2008.
[17] Y. Liu, D. Zhang, G. Lu, and W. Ma. A survey of
content-based image retrieval with high-level semantics.
Pattern Recognition, 40(1):262–282, 2007.
[18] S. Nakajima and K. Tanaka. Relative queries and the relative
cluster-mapping method. In Proc. of DASFAA 2004, pages
843–856, 2004.
[19] S. Pan, X. Ni, J. Sun, Q. Yang, and Z. Chen. Cross-domain
sentiment classification via spectral feature alignment. In
Proc. of WWW, pages 751–760, 2010.
[20] S. Pan and Q. Yang. A survey on transfer learning. IEEE
Transactions on Knowledge and Data Engineering,
22(10):1345–1359, 2010.

APPENDIX
A.

ESTIMATION OF RELATIVE AGGREGATION POINTS

We approximately estimated RAPs by using finite samples X
from a domain.
Expectation E[x]P
can be approximated as follows:
R
1
E[x] = x∈X xP (x)dx ≈ |X|
x∈X x, where |X| is the number of instances X. This approximation is directly applied to the
equation for the AVG RAP.
Similarly, expectation E[x|S] can be approximated
by instances
R
XS = X ∩ S as follows: E[x|S] = x∈X xP (x|S)dx ≈
P
1
x∈XS x, which is applied to the equation for the CLS RAP.
|XS |
The equations for the MAX/MIN and FREQ RAPs
Rare the sameR form and can be written as follows:
P (w|ψ) x∈X xP (x|w)dxdw, where w is an attribute
w∈W
value (i.e. a cost or a category) and W is the entire set of w. By
using the approximation for the CLS RAP, we can approximate the
P
P (w|ψ) P
equation as follows:
w∈WX |Xw |
x∈Xw x, where Xw is a
set of instances that have the attribute value w, and WX is a set of
attribute values included in the instance set X.
For the MAX RAP, we assume that the cost follows a normal distribution g, and the number of sampled instances from a
domain is constant l. The probability of the cost b that is the
maximum cost in a set of sampled instance is then defined as
P (b|ψMAX ) = {G(b)}l , where G is the cumulative distribution
function of the normal distribution g. For the MIN RAP, the probability P (b|ψMIN ) is defined as P (b|ψMIN ) = 1 − {1 − G(b)}l .
For the FREQ RAP, we assume that the frequency of a category
value c follows a binomial distribution B, and is independent from
the frequency of other values. In addition, the number of sampled
instances from a domain is assumed to be constant l. The probability of a category value c that is more frequent in instances sampled

from a domain D than in ones
Pl sampled
Pl from another D is then

defined as P (c|ψFREQ ) =
i=0
j=i+1 BD (i)BD (j), where
BD (i) is the probability that a category value c occurs i times in
l instances sampled Q
from D. Thus, P (C|ψFREQ ) is represented
as P (C|ψFREQ ) = c∈C P (c|ψFREQ ) based on the independent
assumption.

820

