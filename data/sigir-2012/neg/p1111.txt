On Automatically Tagging Web Documents from Examples
Nicholas Woodward

Weijia Xu

Kent Norsworthy

Texas Advanced Computing Center
The University of Texas at Austin

Texas Advanced Computing Center
The University of Texas at Austin

The University of Texas Libraries
The University of Texas at Austin

nwoodward@utexas.edu

xwj@tacc.utexas.edu

kentn@mail.utexas.edu

H.3.3 [Information Search and Retrieval]: Retrieval models

In this case study, the corpus for our experiment comes from
the Latin American Government Documents Archive (LAGDA1)
that contains over 70 million of documents, totaling more than 5.8
terabytes, gathered from government websites of 18 Latin
American and Caribbean countries. Because the corpus is
comprised of web-archived data culled from numerous sources
over the course of several years, it presents several challenges to
traditional content-based approaches for document representation.
For example, variations in subject matter limit the effectiveness of
using named entities to compare documents. Metadata such as file
or folder naming conventions, as well as titles and descriptions of
the documents, are extremely limited. And finally, links and
anchor text, where they can be ascertained, tend to vary greatly
across country, administration and department.
Our problem is then an issue of finding documents that may be
of the same type despite the fact that they contain different
content. We attempt to solve this problem with a hybrid approach
that uses multiple n-grams to find the terms that correlate to the
functions of documents in a training set and then compare them to
documents in the larger corpus. By tagging the documents in the
training set using words related to their function, we can create a
model for classifying other documents. Our implementation
divides and distributes computations across multiple nodes for
parallel processing.

General Terms

2. BACKGOUND AND CHALLENGES

Abstract
An emerging need in information retrieval is to identify a set of
documents conforming to an abstract description. This task
presents two major challenges to existing methods of document
retrieval and classification. First, similarity based on overall
content is less effective because there may be great variance in
both content and subject of documents produced for similar
functions, e.g. a presidential speech or a government ministry
white paper. Second, the function of the document can be defined
based on user interests or the specific data set through a set of
existing examples, which cannot be described with standard
categories. Additionally, the increasing volume and complexity of
document collections demands new scalable computational
solutions. We conducted a case study using web-archived data
from the Latin American Government Documents Archive
(LAGDA) to illustrate these problems and challenges. We
propose a new hybrid approach based on Naïve Bayes inference
that uses mixed n-gram models obtained from a training set to
classify documents in the corpus. The approach has been
developed to exploit parallel processing for large scale data set.
The preliminary work shows promising results with improved
accuracy for this type of retrieval problem.

Categories and Subject Descriptors

Algorithms, Document Representation, Information Retrieval.

Traditional webpage classification assigns categorical labels to
each webpage, which is often treated as a text document of the
same text content [1]. Among these approaches, while different
classification algorithms might be used, only textual information
is used as the feature of each webpage. Although state-of-art full
text-based web classification can achieve relatively high accuracy,
it is not suitable for our practical needs. In this particular problem,
the documents of interest often do not share a high similarity in
terms of their overall text content.
Another general approach for improving web page
classification is to utilize link information. In this approach, the
neighbor pages, which are pages linked from and to the page, are
used to enhance classification. Research efforts have been made
on how neighbor pages can be used. [2]. The overall effectiveness
of those approaches is limited for our problem. First, many web
documents are generated dynamically from a source such as a
relational database. Therefore, all URLs of a website that are
pulled from the same source are similar and only differ by some
unique identifier. Secondly, most documents of interest in our
experiment are terminal documents in the form of Microsoft
Office or PDF-formatted documents, and they generally do not
have any outgoing links. The anchor text and pages that may lead
to the document of interests are sometime created dynamically

Keywords
Web archive, Naïve Bayesian classification

1. INTRODUCTION
Information is frequently gathered from internet for various
research fields. A common task in the social sciences is to identify
a set of web documents belonging to an abstract concept. For
example, a researcher may be interested in collecting all available
transcripts of the presidential speeches from a set of web
documents across multiple countries and a wide time span. Since
the content of those documents varies greatly, with term index
based on content, a researcher has to try multiple combinations of
keyword searches and manually sift through search results. Still,
the approach often yields incomplete results with very low
accuracy. In this poster, we investigate how to develop a
computational method to automatically tag documents of interest
based on a training set that is capable of effectively processing
large-scale data in a reasonable amount of time.

Copyright is held by the author/owner(s).
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
ACM 978-1-4503-1472-5/12/08.

1

1111

http://lanic.utexas.edu/project/archives/lagda/

and can vary across different governments, departments and time
periods.
Therefore, existing web page classification methods are not
very effective in meeting our challenges. Our proposed approach
seeks alternative feature representation of each webpage in order
to improve the performance. Our approach uses selected n-grams
of variable length as a representation of the document. Strotgen et
al proposed a novel document similarity model based on events
extracted from the document. In their approach, a document can
be represented as a set of events extracted from the document and
compared based on geological and spatial references [3, 4].
Mladenic first proposed a document representation consisting of
n-grams where n is up to 5 [5]. In this approach, each web
document is first processed with all unigram to select a subset of
unigram as features. In a subsequent step, the length of n-gram is
increased by 1 and features of the set of n-gram are selected based
on the previous results. The process continues until the 5-gram is
used. The features that were selected at each step are combined as
a vector representation of the web document.

calculate the ROC score for the highest ranked 25 and
50documents from each n-gram model, and compare these scores
to determine the most effective model for classification[6].
We used our approach to identify potential presidential
speeches in the general corpus, dividing the training documents
into 90% used to create classification models with n-grams from
length one to five and 10% set aside for testing purposes. The
models were used to rank all documents in the collection
according to their probability scores. For scalability of the
processing, our implementation uses a divide-and-conquer
strategy to distribute the corpus over multiple computing nodes
for parallel processing. We compared our rankings to rankings
obtained from the Naïve Bayesian classifier in the Apache
Mahout library by calculating the ROC scores of the two result
sets. However, the Mahout classifier was unable to rank any of the
documents set aside in the first 1,000 results for any n-gram of
length one to five during our test runs. Our method can return true
positive results as early as within the first seven results, on
average. Table-1 details average accuracy results of identifying
five presidential speech records using our approach over 10 runs.
In terms of performance, our approach uses multiple strategies
(limited I/O, single pass through text, reusing data structures to
store n-gram models) to achieve a superior runtime to that of
Mahout, finishing in approximately 55%-60% of the time.

3. PRELIMINARY WORK
The corpus for this experiment comes from a single LAGDA
crawl taken from August 16th-19th, 2006. It is a subset of the
crawl, consisting of 109 ARC files (14GB). The training
documents are divided among four types of documents:
presidential speeches, ministerial documents, ministerial reports
and annual state of the union speeches.
The corpus contains 160,638 entities pulled from the web,
96,423 of which we consider as documents (HTML, Word and
PDF). The documents represent approximately 26 different
languages, though the vast majority documents are in Spanish
(83%). Document length in the general corpus ranges from a few
hundred terms (in HTML files) to more than 60,000 terms (Word
and PDF files), with an average of approximately 850 terms per
document.
The training documents are divided into two groups: 90% of
them are used to create a model and 10% are set aside to test the
performance of the model. With the corpus text prepared, we
iterate over the training documents, creating separate structures to
store n-grams from one to five terms. We measure the frequency
of each n-gram, t, in the training set and the general corpus, and
its relative importance to the training model is determined by the
equation (1). We use the log function to smooth the effects of
differences between n-gram frequencies.
score t

#

log

#

Table 1. Accuracy of using various n-grams
N=1
N=2
N=3
N=4
N=5
ROC25 0
.192
.288
.224
.296
ROC50 .088
.396
.596
.596
.620

4. CONCLUSION
We have presented a case study that represents an increasingly
common search problem: locating documents by function rather
than content. Our hybrid approach based on Bayes inference that
uses models derived from multiple n-grams is a first step towards
solving this type of research task in a manner that is well-suited
for parallel implementation to handle large-scale data. We are
continuing to refine our approach to utilize the structure
information and limited metadata available with each document to
improve the results.
This project has been supported by National Science
Foundation (NSF-OCI-0504077 and NSF-DBI-0735191).

5. REFERENCES

(1)

Using the most relevant n-grams as tags related to the function
of the training set documents, we can then calculate a probabilistic
measure for every document in the larger corpus. We iterate over
the entire collection, computing the score of document D as the
sum of all n-gram, ti in D (equation 2) and then normalize all
scores, dividing them by the document’s length to dampen the
effects of longer documents.
∑

(2)

Documents from the corpus are ranked separately by their score
for each n-gram model, and we search for the remaining 10% of
the training set. These training documents are labeled as true
positives, while all other documents are false positives. We then

1112

[1] X. Qi, and B. D. Davison, “Web page classification: Features
and algorithms,” ACM Comput. Surv., vol. 41, no. 2, pp. 1-31,
2009.
[2] M. Richardson, and P. Domingos, “The Intelligent surfer:
Probabilistic Combination of Link and Content Information in
PageRank,” in Advances in NIPS 14, 2002, pp. 1441-1448.
[3] J. Strotgen, M. Gertz, and C. Junghans, “An event-centric
model for multilingual document similarity,” in Proceedings
of the 34th international ACM SIGIR, Beijing, China, 2011,
pp. 953-962.
[4] H. Wang, H. Huang, F. Nie et al., “Cross-language web page
classification via dual knowledge transfer using nonnegative
matrix tri-factorization,” in Proceedings of the 34th
international ACM SIGIR, Beijing, China, 2011, pp. 933-942.
[5] D. MLADENIC, “Turning Yahoo into an automatic Web-page
classifier,” in 13th European Conference on Artificial
Intelligence (ECAI), 1998, pp. 473-474.
[6] T. Fawcett, “An introduction to ROC analysis,” Pattern
Recogn. Lett., vol. 27, no. 8, pp. 861-874, 2006.

