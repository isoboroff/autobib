GLASE 0.1: Eyes Tell More than Mice
*

Viktors Garkavijs*†, Mayumi Toshima*, and Noriko Kando†*
†

Department of Informatics, The Graduate University for Advanced Sciences, National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101-8430, Japan

{gvb,mamitako,kando} @ nii.ac.jp
Our developed prototype system bases its relevance ranking on
learning from gaze data. The experiment results show that it
satisfied user needs 13.7% faster than two mouse-driven baseline
systems and obtained 39% more feedback data from interaction
with users.

ABSTRACT
This paper proposes a prototype system called Gaze-LearningAccess-and-Search-Engine 0.1 (GLASE), which can perform
image relevance ranking based on gaze data and within-session
learning. We developed a search user interface that uses an eyetracker as an input device and employed a relevance re-ranking
algorithm based on the gaze length. The preliminary experimental
results showed that using our gaze-driven system reduced the task
completion time an average of 13.7% in a search session.

2. SYSTEM
The system’s UI was designed to be simple and concise. It
includes a standard query box, a search button, and a stop button
to stop eye tracking. The images are displayed as thumbnails with
a maximum edge size of 150 pixels. Right clicking on the SERP
brings a context menu with an option to reset the index.

Categories and Subject Descriptors
H.3.3. [Information Search and Retrieval]: Relevance feedback
H.5.2. [User Interfaces]: Input devices and strategies

Clicking on a thumbnail produces a pop-up window with the
image title a list of its tags and a zoomed-up version of an image,
which can be closed by a mouse click anywhere on the screen. For
gaze-driven version, a user can instead fix his gaze on a thumbnail
for more than 800 ms to bring the pop-up, which can be closed by
moving the gaze outside the pop-up window area.

General Terms
Experimentation, Measurement

Keywords
Image search, gaze-based interaction, within session learning

Data set. We indexed 786126 images extracted from the
MIRFLICKR-1M image collection, which has 1 million Flickr
images. For the tags, we used a "tags_raw" fileset that contains
original tags obtained from users. To narrow the index tree
breadth, we discarded the tags that don’t exist in the ispell-enwl3.1.20 dictionary fileset. This resulted in removing 213874 image
files from the index.
Engine. Images are indexed by tags. We implemented a trie-like
structure to access the image IDs faster. The trie branches store
tag characters, and the leaves store lists of image IDs and the
scores generated for respective tags. The initial scores of the tags
are 0.
We employed two scoring mechanisms: "dwell-time" and
"binary". The former is used in both mouse-driven and gazedriven UIs. In this mechanism, the scores are being added to the
image tags based on the time in milliseconds the image pop-up
was opened, distributed among the tags equally on each iteration:
, where i is iteration, s is score, l is the length of the time
| |

1. INTRODUCTION
The interactivity of most search user interfaces (UI) is mostly
supported by a keyboard and a mouse. Our prototype system,
GLASE 0.1, provides a gaze-driven search UI and personalizes
the search results based on within-session learning.
Gaze data can be used as feedback for learning to rank and re-rank
based on the dynamic interest and attention of users. Since the
provided data are continuous, compared to discrete mouse click
events, learning can be performed in continuous time rather than
by discrete iterations. Gaze conveys information that mouse clicks
don't, including the gaze length, the direction, and the speed of
movements. The data are more accurate and can capture the user
attention and interest toward the items that were not clicked on;
(this phenomena is called "good abandonment" [4]) and can also
convey such information as whether users are actually looking at
the screen while they are performing multiple tasks. Mouse "dwell
time" fails to track this. Gaze-driven navigation is a great support
when a user's hands are occupied during the search process.
Finally, such input can greatly benefit people with sensory system,
motor system, or cognitive disorders.

interval the preview was open, and

is a set of tags of image x.

The "binary" mechanism adds a score to each tag,

|

|

,

regardless of the time the pop-up was shown. This mechanism is
used only in binary version of a mouse-driven UI.
To adapt to user interactions, we introduced an overlapping
sliding window. In preliminary experiments, we determined that a
window size of 5 is optimal for most cases; however this will be
investigated in future studies. The total tag score thus can be
∑ s,
S
if i w
calculated like this:
, where w is
S
S
s s
if i
the window size and S is the total score after the i-th iteration.
Based on these scores we calculate the relevance score of image x:
∑ ∑
, where R is the relevance score, x is the image,

The search process has a high cognitive load, especially during
query formulation and relevant document tagging [1]. Using
implicit feedback, which requires nothing more from users than
looking at items on a search engine results page (SERP), may
lower the cognitive load and simplify the search task. Users are
freed from clearly formulating their intents, if the system can learn
them fast during within-session interaction. This can be especially
effective in exploratory search for images.
Many related researches exist that prove that eye-tracking is an
effective means both as an input device in an interactive UI as
well as a data source for determining object relevance [2,3,5].

q is the query term, X is a set of images that include tag q and T
is a set of tags that occur for image set X. After calculating the
scores for every image in X , we sort the images in descending
order and present them on the UI.

Copyright is held by the author/owner(s).
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
ACM 978-1-4503-1472-5/12/08.

1085

3. EXPERIMENT
3.1 Setup

Gaze
n=245,
t=245.585

Mouse all
n=182,
t=303.737

Mouse dwell
n=54, t=150.593

An experiment explored the effects of using eye-tracker as an
input device in an image search user interface.
Six Japanese university students from various majors were
recruited for the experiment; two were female.
Each participant performed four search tasks: 2 with an eyetracker-driven (ET) UI and 2 with a mouse-driven UI. We used
two types of relevance recalculation methods for the mousedriven interface: "binary" and "dwell-time". The experiment
organizers assigned the participants to either the "binary" (MB) or
"dwell-time" (MD) interfaces. The tasks were counterbalanced
among the participants.
Before performing the tasks, the system was briefly explained and
a training session was held. After that the participants performed
search tasks. The participants were asked to search for the images,
that are similar to the sample images (these are the images from
MIRFLICKR with the following IDs: 859626, 585623, 926255,
781859). The search session started with an input of a query.
Depending on the task, the navigation had to be performed using
gaze or a mouse. There was no time restriction for the tasks;
however the participants were told that the tasks should not take
longer than five minutes and that the overall experiment duration
should not exceed 1 hour.

3.2 Results
The task completion time means and standard deviations, shown
in the Table 1 are floored down to the nearest integer. The task
completion time for ET is smaller than for the mouse-driven UI by
13.7%.

Legend

Mouse
131 (48)
1.7 (2.0)

ET
113 (24)
1.0 (1.3)

High

<<--

----

----

-->>

Low

Zero

Figure 1. Heat maps of retrieved item positions. Number of
pop-ups is n, and pop-up show length sum is t.
Even though out of all kinds of the rich gaze data, we used only
gaze length to learn user's intents, the task completion time was
shorter comparing to that of a conventional mouse-driven UI. In
the future it is worth investigating if it is possible to use brainwave data for relevance calculation, as well as using other gaze
information to infer user intentions. We are also considering
implementation of negative feedback handling feature for one of
the future versions of our prototype as well as conducting
experiment with a larger number of users.

Table 1. Means and standard deviations of task completion
times and pop-up show lengths, in seconds.
Measure
Task completion time
Pop-up show length

Pop-up show lengths

Mouse Binary
n=128,
t=153.114

Click counts

p-value
0.26
< 0.01

We generated heat maps of pop-up-counts and pop-up-viewlengths of the images on the UI (Fig. 1). The SERP consisted of
five lines of images, seven images per line.
The total number of pop-ups on gaze-driven UI was 1.35 times
bigger than on the mouse UI, but the total duration of the pop-ups
was 1.24 times smaller on the gaze-driven UI. The cases of "good
abandonment," when users looked at the images without clicking
on them were captured by the gaze-driven UI. So the system
learned this action; however the mouse-driven UI failed to capture
these events. Thus, the system satisfied the information needs of
users in a shorter time.
The total time effort of the users is smaller on the gaze-driven UI,
but the computational cost required for relevance recalculation is
slightly bigger due to the larger number of iterations. This
increase is, however, negligible and was not noticed by the users
during the experiment.

5. REFERENCES
[1] Gwizdka, J. 2010. Distribution of Cognitive Load in Web Search.
In JASIST, vol. 61, issue 11, (Nov. 2010), 2167-2187.

[2] Kandemir, M., Saarinen, V.-M., Kaski, S.. 2010. Inferring object
relevance from gaze in dynamic scenes. In Proceedings of the
2010 Symposium on Eye-Tracking Research Applications
(ETRA '10). ACM, New York, NY, USA, 105-108.
[3] Kumar, M., Paepcke A., and Winograd, T. 2007. EyePoint:
Practical Pointing and Selection Using Gaze and Keyboard. In
Proceedings of the SIGCHI conference on Human factors in
computing systems (CHI '07). ACM, New York, NY, USA, 421430.
[4] Li, J., Huffman, S., and Tokuda, A. 2009. Good abandonment in
mobile and PC internet search. In Proceedings of the 32nd
international ACM SIGIR conference on Research and
development in information retrieval (SIGIR '09). ACM, New
York, NY, USA, 43-50.
[5] Oliveira, F. T.P., Aula, A., and Russell, D. M. 2009.
Discriminating the relevance of web search results with
measures of pupil size. In Proceedings of the 27th international
conference on Human factors in computing systems (CHI '09).
ACM, New York, NY, USA, 2209-2212.

4. CONCLUSION
Using an eye-tracker as a relevance feedback input device can
help save time in image search tasks. The information of
relevance that is otherwise lost due to the phenomena of "good
abandonment" can be retrieved during search sessions and used to
learn the user's search intent. User experiments showed that the
total duration of a search session can be decreased by an average
of 13.7% using our proposed system, the gaze-driven UI of
GLASE 0.1.

1086

