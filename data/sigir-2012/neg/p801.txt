When Web Search Fails, Searchers Become Askers:
Understanding the Transition
Qiaoling Liu§ , Eugene Agichtein§ ,
Gideon Dror† , Yoelle Maarek† , Idan Szpektor† ,
§

Emory University, Atlanta, GA, USA
†
Yahoo! Research, Haifa, Israel

{qliu26, eugene}@mathcs.emory.edu, {gideondr, idan}@yahoo-inc.com, yoelle@ymail.com
ABSTRACT

In fact, Hitwise in August 2011 reported that only 6680% of the searches are successful1 , and Hassan et al. [15]
obtained a similar success rate of search goals (73%) via human labeling. We argue that many such unsatisfied searches
could be addressed by asking people via Community Question Answering (CQA) services, such as Baidu Knows, Quora,
or Yahoo! Answers. It already happens in practice. For
example, we have observed that about 2% of web search
sessions performed by users who are also members of the
Yahoo! Answers community, lead to a question posted to
the community. Consider Figure 1a, which depicts a sample
search submitted to a major search engine. The searcher is
not satisfied with the results, and eventually posts a related
question on the Yahoo! Answers site, which is then answered
to the searcher’s satisfaction. Understanding and improving
the synergy between searching and community question answering is at the heart of this work.
Specifically, our goal is to better understand the behavior of these users, as well as characterize the types of Web
searches that could be effectively handled by CQA sites.
Insights acquired during such analysis should bring multiple benefits to both search engines and CQA systems. On
one hand, search engines always need to better understand
when searchers are unsatisfied by the returned results. More
specifically, Web search engines would find value in analyzing the search session patterns of such unsuccessful queries,
the associated underlying query intents, and possibly reflect
these findings in search effectiveness metrics. One can even
imagine new search experiences that would allow users to
turn to the community for certain types of needs better addressed by people than by traditional Web search. Additionally, CQA systems could potentially improve the asking
experience by taking advantage of the context provided by
unsuccessful queries preceding a posted question. One can
imagine several ways to leverage this context, such as automatically giving examples of irrelevant answers to clarify
the question to the community.
To the best of our knowledge, our work is the first to perform a large-scale study of the transformation of searchers
into askers. That is, we start our analysis with web search
sessions, trace the searcher through her visit to a CQA site,
and analyze the resulting questions posted for the community.
We focus on one of the most visited, and more mature,
CQA systems existing today, namely Yahoo! Answers, which

While Web search has become increasingly effective over the
last decade, for many users’ needs the required answers may
be spread across many documents, or may not exist on the
Web at all. Yet, many of these needs could be addressed
by asking people via popular Community Question Answering (CQA) services, such as Baidu Knows, Quora, or Yahoo!
Answers. In this paper, we perform the first large-scale analysis of how searchers become askers. For this, we study the
logs of a major web search engine to trace the transformation
of a large number of failed searches into questions posted on
a popular CQA site. Specifically, we analyze the characteristics of the queries, and of the patterns of search behavior
that precede posting a question; the relationship between the
content of the attempted queries and of the posted questions;
and the subsequent actions the user performs on the CQA
site. Our work develops novel insights into searcher intent
and behavior that lead to asking questions to the community, providing a foundation for more effective integration of
automated web search and social information seeking.

Categories and Subject Descriptors
H.3.3 [Information Systems]: Information Storage and
Retrieval

Keywords
query analysis, community question answering

1.

INTRODUCTION

While Web search engines have significantly progressed in
effectiveness and efficiency over the last decade, there still
exist certain user needs that cannot be satisfied. This could
be due to a number of reasons, such as the difficulty of expressing a complex need as a short search query, the lack
of existing relevant content on the Web (e.g., for unique
or “tail” needs that keep appearing), and for more “social”
needs, for which the user prefers to interact with a real human.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

1
www.hitwise.com/us/about-us/
press-center/press-releases/
experian-hitwise-reports-google-share-of-searche/

801

(a)

(b)

Figure 1: Example search (a) followed by a question posted by the same user on the Yahoo! Answers site
with a satisfactory answer from the community (b).
with more than 1 billion posted answers2 is highly visible
in most search engines result pages. We built a corpus of
query-to-question transitions and studied it in order to understand when and why searchers become askers. A privacypreserving subset of the data has recently been made publicly available through Yahoo’s Webscope program3 .
More specifically, our study is organized around the following three research questions, each associated with a set
of hypotheses:

such as occurrence of personal pronouns or sentiment
indicators (Section 4).
Research Question 3: How do searchers behave after
transferring to the CQA site?
• Hypothesis 5: We further hypothesize that the content and the topics of the questions posted after a
search session differ substantially from the general question distribution (Section 5.1).

Research Question 1: When do searchers turn to CQA
for answers?

• Hypothesis 6: We hypothesize that the question sessions after switching from searching, exhibit different
characteristics than general question sessions and search
sessions explored in depth in previous research work
[17][10]. We study these different types of sessions in
terms of duration and persistence for specific users and
examine their behavior over time (Section 5.2).

• Hypothesis 1: Queries and information needs of search
sessions that lead to posting questions are hypothesized to share common characteristics, and differ from
general web searches in words and information needs
(Section 3.1).
• Hypothesis 2: We hypothesize that searchers who
switch to CQA exhibit common search behavior. For
instance, they tend to click more on CQA results on
the search result page, and their search sessions are
longer, allowing to characterize different types of users
in the same spirit as [7] (Section 3.2).

The rest of this paper is dedicated to answering the above
questions and verifying the associated hypotheses.

2.

ACQUIRING DATA

In order to understand how searchers become askers, we
collected a dataset that contains both the search session part
and asking session part of each user who conducted a search
Research Question 2: How do search queries relate to the
session that resulted in posting a question. Our dataset is
associated questions posted on CQA sites?
derived from joining a sample of the query logs of the Yahoo!
• Hypothesis 3: Queries and questions follow different
search engine and the Yahoo! Answers question logs, both
word distributions. More specifically, words in queries
for June 2011.
are hypothesized to follow different distributions than
To create this dataset, we first created a mapping of users
those appearing in questions, and a clear vocabulary
between the two logs, based on the clicks on the same Yahoo!
gap between these can be observed (Section 4).
Answers question page following the same query on the same
time frame, as they appear in both logs. Then, we extracted
• Hypothesis 4: Questions are typically more specific
user actions from the query and question logs, e.g. posting
than queries and include additional context (e.g., perqueries and clicking on results from query logs, as well as
sonal background) absent from the original queries.
posting questions and re-viewing them from the question
We hypothesize that these differences are reflected in
logs. We constructed search sessions from these extracted
the lexicographic differences between questions and queries, actions, with a 30 minutes timeout as a session boundary.
2
Question sessions have no temporal boundary, since every
http://yanswersblog.com/index.php/archives/2010/
action in the session unambiguously refers to the question
05/03/1-billion-answers-served/
3
http://webscope.sandbox.yahoo.com/
posted by the asker.

802

Table 1: Statistics of the constructed datasets.
Description

Query length distribution (cumulative)
0.0
0.2
0.4
0.6
0.8
1.0

Total sampled search sessions
SearchOnly sessions
Search sessions that include question sessions
SearchAsk sessions: search sessions with a single relevant question posted after searching

Once we had search sessions and question sessions, and
mapping between some of them, we created two datasets.
The first, termed SearchAsk dataset, contains search sessions that turned into question sessions. We only kept such
sessions that resulted in posting one and only one question
for simplicity of analysis later on. In addition, we only kept
sessions in which the posted question is “relevant” to a previously issued query (if the query and the question share
at least one non-stopword, they were considered relevant).
By observing that some users actually searched for Yahoo!
Answers to navigate to its home page before they posted a
question there, we deleted such special navigational clicks
and corresponding queries from the user action sequences.
The second dataset, termed SearchOnly dataset, consists of
search sessions that did not turn into question sessions. In
both datasets, we only kept sessions for users that posted at
least once in Yahoo! Answers, since these users are aware of
the site and know how to post a question there, thus removing the potential investment of effort for newcomers to join
the site, and filtering our the users that simply do not know
where to ask questions.
Table 1 reports the statistics of the datasets we obtained.
As shown in the table, 95.8% of all the search sessions are
SearchOnly sessions, while SearchAsk sessions account for
1.65%. Despite the sparsity of the SearchAsk sessions, we
still believe that understanding how searchers become askers
in such sessions can be helpful for improving the search experience of these users and perhaps more users. Indeed,
the two datasets allow us to investigate the differences between sessions in which users posted a question following
attempted searches, mainly due to search failure or searcher
frustration, and sessions in which users that have experience
of asking questions on Yahoo! Answers did not bother or did
not need to ask questions at that time. Recall, that the users
in these datasets satisfy two conditions: (1) having clicked
at least a Yahoo! Answers question page within this month;
and (2) having asked at least one question on Yahoo! Answers within the month. Yet, we believe that such users still
represent the general, though somewhat experienced, web
searchers.

3.

SearchAsk
SearchOnly

1

2
5
10
20
Query length (number of words)

50

Figure 2: Distribution of query length

Table 2: Statistics of words per query
Avg # Avg # Avg % Avg
words
stopstopword
words
words
length
SearchAsk queries
6.5
2.4
28%
5.1
SearchOnly queries 3.4
0.72
11%
6.0

extract the queries that are issued before the question is
posted, and are relevant to the question. We call such
queries SearchAsk queries. For comparison, we also extract the queries in each SearchOnly session which are called
SearchOnly queries. In the following, we explore how SearchAsk
queries are different from SearchOnly queries in terms of
length, words, frequency, and results.

Query Length Distribution.
Figure 2 compares the distribution of query length (in
terms of number of words in the query) for the SearchAsk
and SearchOnly queries. We can see that SearchAsk queries
tend to be longer than SearchOnly queries, as 85% of the
SearchOnly queries contain at most 5 words, while about
50% of the SearchAsk queries contain more than 5 words.
Therefore, searchers issuing longer queries are more likely to
turn to Yahoo! Answers to post a relevant question.
Table 2 compares the average word length per query and
the average number of stopwords for the SearchAsk queries
and SearchOnly queries. We can see that, on average, queries
turning to questions tend to contain more words (but shorter
words) than queries that do not turn to questions. The main
reason could be that SearchAsk queries contain more stopwords (which are often short) than SearchOnly queries. Indeed, the percentage of stopwords in SearchAsk queries is
over 2.5 times higher than in SearchOnly queries.

FROM SEARCHING TO ASKING:
QUERY AND BEHAVIOR ANALYSIS

As a first step, we study the characteristics of queries leading to a question post on Yahoo! Answers (Section 3.1), and
the characteristics of searcher behavior before question asking (Section 3.2).

3.1

Number of sessions
1,287,238 (100%)
1,233,279 (95.8%)
53959 (4.2%)
21231 (1.65%)

Characteristics of Queries leading to Questions

The first interesting question is which queries are more
likely to be unsuccessful for automated search, but instead
are more amenable to be answered by a CQA site. To
get such queries, we examine each SearchAsk session, and

803

Search
Ask
SearchAsk
SearchOnly

Search
Only

Query frequency distribution (cumulative)
0.5 0.6 0.7 0.8 0.9 1.0

Query SERP with url from Yahoo! Answers
Query SERP without url from Yahoo! Answers

0.0

1

10
100
1000 10000
Query frequency in 1-month query log

0.2

0.4
0.6
Probability

0.8

1.0

Figure 4: Distribution of query results

Figure 3: Distribution of query frequency

site, the searcher might realize that a community might be
able to answer her information need, and try posting a question.

Query Words Distribution.
To better understand the difference between the content
of SearchAsk queries and SearchOnly queries, we compare
their word distributions and show the main difference in Table 3. We can see that SearchOnly queries are more likely
to be navigational, e.g., to reach websites like Facebook or
YouTube, or to find information related to the searcher’s
common tasks such as looking up the weather, hunting for
coupons, or finding a cooking recipe. In contrast, SearchAsk
quries are more likely to start with question words (e.g.,
‘how’, ‘what’), and tend to use more verbose natural language to express the needs of the searchers (e.g., ‘want’,
‘to’, ‘know’) rather than using only keywords.

Summary of Query Characteristics.
As a summary of the above analysis, we conclude that
queries that are more likely to fail in search and lead to
a question post on Yahoo! Answers tend to be longer, and
use more verbose natural language to express the searchers’
needs. The needs behind such queries tend to be more
unique and complex than those associated with SearchOnly
queries.

3.2

Searcher Behavior Before Asking Questions

To understand how searchers become askers, we analyze
the searcher behavior in search sessions, with an associated
question posted by the same user on Yahoo! Answers.

Query Frequency Distribution.
To verify the hypothesis from the above word distribution analysis that SearchAsk queries are more likely to be
unique, we compute the frequency4 of SearchAsk queries
and SearchOnly queries in our 1-month query log. Figure 3
shows the results. We can see that over 90% of SearchAsk
queries are tail (actually unique) queries, indicating the variety of the needs of searchers and the ways to express them.
In contrast, SearchOnly queries contain more popular queries,
e.g., around 20% of SearchOnly queries occur in more than
100 search sessions.

Last Action Before Question Asking.
First, we examine what searchers do right before they
start question asking, i.e., we examine the last user action
prior to a question being posted. We found that the last
search action before question asking is a click on Yahoo!
Answers question result in 47.8% of the sessions, a click on
other result in 31.2% of the sessions, and a query in 17.4%
of the sessions. We notice that in about half of the sessions, the searcher posts a question right after clicking on
a Yahoo! Answers question page from the search engine results. There may be several reasons for this. First, such
a click indicates that the query is relevant to the clicked
question, and therefore it probably carries an information
need that would benefit from a human response. Second,
when the clicked Yahoo! Answers question page cannot satisfy the search need, it encourages the user to post a new
question on Yahoo! Answers. Of course, it is also possible
that a searcher had already decided to post a question when
seeing the original SERP, and she then clicked on a Yahoo!
Answers question result simply to navigate to the Yahoo!
Answers site.

Query Results Distribution.
To better understand user needs behind SearchAsk queries,
we further examine the results returned in their search engine result pages (SERPs). We found a significant difference between SearchAsk and SearchOnly queries based on
whether a SERP contains a Yahoo! Answers question page.
As shown in Figure 4, a Yahoo! Answers question page occurs in the SERPs for half of the queries that eventually
turn to questions, but for only 13% of SearchOnly queries.
It is clear that SearchAsk queries are more likely to have
a Yahoo! Answers question page in the SERP. This is not
surprising. First, having a Yahoo! Answers question page
in search results indicates that the query could be relevant
to an existing Yahoo! Answers question. Therefore, answers
from a human might be more suitable to address the need
behind the query, encouraging the searcher to post a question on Yahoo! Answers. Second, more impressions often
leads to more clicks. After landing on the Yahoo! Answers

Distribution of Clicks.
To better understand the effects of clicking on a Yahoo!
Answers question result on the transformation of searchers
into askers, we compute and compare the likelihood of such
clicks in SearchAsk and SearchOnly sessions. Figure 5 shows
the results. First, 21% of SearchOnly sessions and 81% of
SearchAsk sessions contain a Yahoo! Answers question page
in the SERPs. Next, after seeing a Yahoo! Answers question
page in the SERPs, 81% of the searchers who turned to

4
The frequency of a query in this paper is computed as the
number of search sessions containing the query.

804

Table 3: Frequent words in SearchAsk queries and SearchOnly queries
Words
First words
Content words

Words
First words
Content words

More likely in SearchAsk queries
to, a, be, i, how, do, my, can, what, on, in, the, for, have, get, with, you, if, yahoo, it
how, what, can, be, why, i, do, my, where, yahoo, if, when, 0000, a, will, 00, best, who, which, should
yahoo, 00, use, 0, work, song, old, help, make, need, like, change, year, good, long, mail, answer, email,
want, know
More likely in SearchOnly queries
facebook, youtube, google, lyric, craigslist, free, online, new, bank, game, map, ebay, county, porn, tube,
coupon, recipe, home, city, park
facebook, youtube, google, craigslist, ebay, the, you, gmail, casey, walmart, amazon, *rnrd, justin, facebook.com, mapquest, netflix, face, fb, selena, home
facebook, youtube, google, craigslist, lyric, free, bank, map, ebay, online, county, porn, tube, coupon, recipe,
anthony, weather, login, park, ca

0.12 / 0.13

1+ clicks on Yahoo! Answers question result
session SERPs with url from Yahoo! Answers
session SERPs without url from Yahoo! Answers

Click
ques
result

Search
Ask

0.15 / 0.03
0.25 / 0.40

Search
Only

Begin

0.0

0.2

0.4
0.6
Probability

0.8

1/1

0.06 / 0.01
0.14 / 0.23

Query

0.30 / 0.23

1.0

0.47 / 0.21

0.04 / 0.05
0.51 / 0.43

0.48 / 0.66

Click
other
result

Ask
/End

0.09 / 0.29

Figure 5: Distribution of clicks
0.30 / 0.25

Figure 6: Transition probabilities for actions in
SearchAsk (in red, before the slash symbol) and
SearchOnly (in black, after the slash symbol) sessions. Note that two other actions (Pagination and
Click interface) are ignored for simplicity.

askers had clicked on a Yahoo! Answers question result while
19% of them hadn’t; in contrast, 43% of the searchers in
SearchOnly sessions seeing a Yahoo! Answers question result
clicked on it while 57% of them didn’t. Therefore, users in
SearchAsk sessions are about twice as likely as in SearchOnly
sessions to click on a Yahoo! Answers question page in the
search results once seeing it. This indicates that searchers
are more likely to post a question once clicking on a Yahoo!
Answers question result.

Action Sequences Before Question Asking.

Transitions between Actions.
To better understand searcher actions, we further compute the probability of transitions between actions in SearchAsk
and SearchOnly sessions respectively, and compare them
in Figure 6. The transition probability between two actions ai and aj in SearchAsk (SearchOnly) sessions is computed using Maximum Likelihood estimation: P (ai , aj ) =
Nai ,aj /Nai , where Nai ,aj is the number of transitions from
action ai to action
aj in all SearchAsk (SearchOnly) sessions,
P
and Nai = ak Nai ,ak . SearchAsk transition probabilities
are shown in red before the slash symbol, while SearchOnly
transition probabilities are shown in black after the slash
symbol. If we look at the transitions for SearchOnly sessions
from the figure, we can see that after issuing a query, the
searcher is very likely to click on other result, then with perhaps more queries and clicks on other result, and then ends
the session. Clicking on a Yahoo! Answers question result is
very unlikely. However, in SearchAsk sessions, the searcher
has a higher probability on clicking a Yahoo! Answers question result. After the click, the searcher in SearchAsk sessions would post a question on Yahoo! Answers for around
half of the time.

805

To better understand how searchers become askers, we
examine the user action sequences in SearchAsk sessions before the question post, and compare them with action sequences in SearchOnly sessions. Table 4 shows a sample of
top frequent user action sequences. The top frequent path
in SearchOnly sessions indicates navigational needs of the
searchers, i.e., they issue a query, click on a search result and
leave the session. Such navigational cases account for 30%
of total SearchOnly sessions. In contrast, the top frequent
path in SearchAsk sessions indicates more “social” needs of
the searchers, i.e., they issue a query, click on a search result
of Yahoo! Answers question page, and then ask a question
on Yahoo! Answers. Yet, the path distribution is more balanced for SearchAsk sessions. Moreover, clicks on Yahoo!
Answers question results are common in the paths.

Session Size Distribution.
Finally, we compare the distribution of session sizes for
SearchAsk and SearchOnly sessions. Session size can be
measured in several ways, e.g., by the number of (unique)
queries issued by the searcher in the session, by the number of actions performed in the session, or by the duration
that the session lasts. We use the first option in this paper.
The results are shown in Figure 7. While only one query

Table 5: Statistics of length difference between a
query and its associated question (number of words).
Median Avg Max
|question| - |query|
42
66
1431
|subject| - |query|
3
4
27
|content| - |query|
31
55
1428
Table 6: Overlap of content words (CW) between a
query and its associated question.
CW?
CW?
CW?
CW?
CW?

⊃ CWquery
= CWquery
⊂ CWquery
6⊃ CWquery
6⊂ CWquery

?=question

?=subject

?=content

31.4%
1.8%
0.7%
66.1%

14.6%
6.2%
3.7%
75.5%

14%
0.4%
17.1%
68.5%

SearchAsk
SearchOnly

0.0

Sessioin size distribution (cumulative)
0.2
0.4
0.6
0.8
1.0

Table 4: Top frequent user action sequences in
SearchAsk sessions and SearchOnly sessions (B: Begin a session, Q: Query, Cqr : Click on a Yahoo! Answers question result, Cor : Click on other result, A:
Ask a question, E: End a session)
SearchAsk sessions
Distribution
B Q Cqr A
10%
B Q Cor A
3.8%
3.3%
B Q Q Cqr A
BQA
2.8%
B Q Cor Q Cqr A
2.0%
SearchOnly sessions Distribution
B Q Cor E
30.2%
B Q Cor Q Cor E
7.1%
BQE
6.1%
B Q Cor Cor E
3.6%
3.6%
B Q Q Cor E

1
2 3
5
10
20
50
Session size (number of unique queries)

Figure 7: Distribution of session size
Figure 8: Word distributions over question words
is issued in the half of SearchOnly sessions, at least three
different queries are issued in the half of SearchAsk sessions.
The average session size is 2.5 for SearchOnly sessions and
3.8 for SearchAsk sessions. This shows that searchers tend
to issue more queries in SearchAsk sessions, possibly because SearchOnly sessions contain more navigational needs,
while SearchAsk sessions are associated with more difficult
or complex needs, and thus require more effort in finding
answers.

4.

limited in length, additional knowledge of the problem to be
solved is added. Interestingly, the subject of the question is
very close in length to the query, which shows that searchers
still think in search-style writing for the subject. However,
the content part of the question is significantly longer, and
much more information is added in this question part.
We next look at word distribution differences, since they
may point at the lexical gap between queries leading to questions and their associated posted questions. Figure 8 depicts the word occurrence distribution over word ranking
by frequency for search-related questions. The most notable difference between the two distributions is that questions tend to be more personal and verbose, as captured by
the abundant usage of the pronouns such as ‘I’, ‘me’, ‘it’
and ‘this’, connectives such as ‘but’, ‘because’, ‘recently’, and
‘just’, as well as sentiment indicators such as ‘help’, ‘please’,
and ‘thanks’. Queries, on the other hand, tend to focus more
on the things or actions that are searched for, with content
words like ‘best’, ‘free’, ‘download’ and ‘games’ as well as
question words like ‘how’, ‘why’ and ‘what’ occurring more
frequently than in the associated questions corpus. Interestingly, one to four digit figures, such as car model years,

QUERIES VS. QUESTIONS: CONTENT
ANALYSIS

After discovering the unique attributes of queries that lead
to asking a question, we next want to understand better the
process of turning a search session, as captured by a query,
into a question posted on Yahoo! Answers.
The most expected difference between queries and questions is their length. Table 5 shows these differences. From
the table we can see that a question has 66 more words
than its associated query on average. This indicates two
things: first, as expected, questions are much more verbose,
being natural language expressions, compared to the concise queries; second, since Yahoo! Answers questions are not

806

Table 7: Examples showing semantics difference between the query and the question.
ID

1

Type of
context
added
N/A

Query

Question (Category, Subject, and Content)

what to serve with
chicken salad
best nba players without a championship

Food & Drink>Cooking & Recipes
what can you serve with chicken salad?
Sports>Basketball
Greatest NBA players to never win championship?
Patrcik ewing, reggie miller, charles barkley, karl malone? Who else?
Cars & Transportation>Car Makes>Chrysler
how much does it cost to fix an ac system in a pt cruiser?

2

thought

3

task

pt cruiser ac fix

4

task

solve nˆ2-2n-3=5000

5

limit

chocolate
menlo park

6

situation,
task

chicago fried chicken

7

situation,
task

douglas az

8

attribute,
situation,
task

how many bottles to
buy for a newborn

croissant

Education & Reference>Homework Help
Algebra question, Need Help Pls!!!!?
An owner of a key rings company found that the profit earned (in thousands of
dollars) per day by selling n number of key rings is given by nˆ2 - 2n - 3, where
n is the number of key rings in thousands. Find the number of key rings sold on
a particular day when the total profit is $5000. Thanx
Dining Out>United States>San Jose
Where can I get a good Chocolate Croissant near Menlo Park, CA?
Something with thick, dark chocolate? And please, don’t say La Boulanger.
Dining Out>United States>Chicago
Where can I get really good fried chicken in the Lakeview area in
Chicago?
I really want fried chicken after watchin a special on TV. But I cant find any place
near me that has decent priced chicken thats not fast food and is homemade and
delicious. Any one know of a place?
Education & Reference>Higher Education (University +)
Radiology schools in Arizona?
Does any one know any schools in az that offer radiology degree programs, I
moved to Douglas az and don’t know any schools near to study radiology. If any
one can help that would be great :)
Pregnancy & Parenting>Newborn & Baby
How many bottles should I purchase for my new baby? And what
brand is best?
I am 9 mo. pregnant and still need to buy bottles. I will be trying to breast
feed but I am unsure of how many bottles and what sizes I should buy. Is there
anything else I will need for feeding and what brand do you recommend? Thanks!

also appear more in question-related queries than in their
associated questions, probably since they capture much of
the essence of the target information need.
To further understand the semantic difference between
composing a query and its related question, we measured
the distribution of query-question pairs in which the same
words are used for both query and question, the pairs in
which one is included in the other, and those pairs in which
each contains words that do not occur in the other. Table 6
presents these statistics, while Table 7 provides examples of
such pairs, annotated with the type of context added when
switching from query to question, as been classified by [23],
i.e., task, situation, attribute, limit, and thought.
Some interesting question composition patterns are evident from this analysis. First, in the majority of pairs (66%),
both queries and questions contain unique words that do not
occur in the other. This is somewhat surprising, since we
would expect more complete inclusion of the query terms
in the question. However, it seems that with the freedom of
writing a free text question, searchers tend to rephrase some
of the terms they used in their queries. For example, abbreviations and short terms are turned into their more complete
forms, e.g. ‘AZ’ into ‘Arizona’ and ‘newborn’ into ‘new baby’
(see example 7 and 8 in Table 7). In addition, while 31%
of the pairs do show complete inclusion of the query terms
in the question, many times the query terms do not all appear in the question’s subject or content, but spread in both
question parts. Table 7 shows that most of the extensions of

the query into a question include additional details that are
related to the search task. Yet, many times details of the
personal situation are added, such as the state of mind, e.g.
“after watching a special on TV ” (example 6 in Table 7).
One interesting future research is to automatically generate questions from queries [25][26]. However, adding context
information to the question, such as the situation or limit is
a difficult challenge. Still, expanding the query expression
to an explicit question form may be possible for many cases,
e.g. examples 1 and 3 in Table 7.

5.

ASKING AFTER SEARCHING: QUESTION
ANALYSIS

As our final analysis, we are interested in discovering unique
activity patterns in Yahoo! Answers that searchers posting
a question have, compared to typical asker behavior in Yahoo! Answers. Specifically, we first examine the differences
in lexicon, that is whether different words are used when
composing a question (Section 5.1). Then, we analyze the
difference in asker behavior after posting such questions, in
terms of “traditional” CQA activities (Section 5.2).

5.1

Characterizing Questions Posted after a
Search Session

As expected, we find that there is a large difference between the word distribution for the corpus of all questions
posted in June 2011 and the distribution of the corpus of

807

Table 8: Categories with largest differences in
assignment probability between questions coming
from search and general questions
Categories more likely for
general questions
Polls & Surveys (Entertainment & Music)
Singles & Dating
Religion & Spirituality
Politics
Friends
Mathematics
Diet & Fitness
Lesbian, Gay, Bisexual, and
Transgendered
Other - Beauty & Style
Basketball
Baby Names
Adolescent

Table 9: Statistics of words in SearchAsk questions
and sampled general questions
Avg. corpus
Sampled general SearchAsk
statistics
questions
questions
# words
78.3
73.7
# words per sentence
13.5
13.7
# sentences
5.8
5.4
% stopwords
66.3
65.0
word length
4.22
4.17

Categories more likely for
questions following search
Maintenance
&
Repairs
(Cars)
Law & Ethics
Dogs (Pets)
Pregnancy
Maintenance
&
Repairs
(Home & Garden)
Renting & Real Estate
Accounts & Passwords
Other - Yahoo! Mail

Table 10: Statistics about user follow-up activities
around their posted questions.
SearchAsk Ask Search
Avg duration
30h
32h 19.4m
Median duration
2.2h
3.7h 11.6m
Avg #actions
6.41
7.45
Median #actions
5
5
-

Military
Problems with Service
Garden & Landscape
Cooking & Recipes

questions posted by searchers. In addition, the entropy of
generating a word from the search-related question corpus is
much lower, showing a more focused vocabulary. But what
are the reasons for this large difference? It turned out to be
mainly topical.
To measure this topical difference between the two types
of questions, we looked at the distribution of categories to
which the questions in the two compared corpora are assigned. Table 8 shows the categories with largest differences
in assignment probability, those that are preferred more in
the general question corpus and in the search-related question corpus respectively. These lists show that searchers
tend to ask informational questions [14] to get fact- or adviceoriented answers, such as how to fix the car or maintain one’s
garden, how to bake cookies, but also questions related to
Yahoo products, such as Yahoo! Mail. On the other hand,
regular askers are more likely to ask conversational questions
[14] with a social flavor, such as discussions around music or
sports events, politics and religions, and opinions on possible
baby names. We manually labeled 100 questions randomly
sampled from the search-related question corpus, and found
none are conversational, showing a very different distribution compared to that 38% of Yahoo! Answers questions are
conversational as reported in [14]. We conjecture that this is
because searchers usually turn to search engines to find information instead of starting conversations. Another kind of
questions that are less likely searched first over the web are
personal questions, in which the asker is interested in adding
very personal details. These include topics such as diet and
fitness advices, dating and style opinions. Finally, there are
questions that are too complex, for which the asker knows
the answer cannot be found on the web. A good example
are Math questions, such as example 4 in Table 7.
To further investigate the differences between the two
question types, we removed the strong bias caused by the
different category distributions within the two corpora by
sampling questions from the general question corpus based
on the category distribution of the search-related question
corpus. By comparing the word distribution between the
sampled corpus and the search-related question corpus, we
found that hardly no topical differences remained. That is,
the topical variation in the two corpora is more or less completely captured by the level of assigned categories, without

more subtle topical differences evident. Still, there may be
stylish variations in question composition between searchers
and typical askers. Table 9 provides the stylish statistics
for the general-sampled and search-related question corpora.
The significant difference between the two corpora is the
number of words per question: for the same topics, general questions contain 6% more words compared to searchrelated questions. Yet, interestingly, this attribute is due
to more sentences that are written on average per general
question, while if we look at the number of words per sentence, we see that surprisingly search-related questions have
slightly more words in each sentence. This could be related
to more information-focused nature of the questions posted
after a search session, and suggests further investigation.

5.2

Asker Follow-up Activity after a Search

As our final question behavior analysis, we wanted to test
whether a searcher interacts more or less with Yahoo! Answers after posting the question. To that end, we measured
both the number of actions that both searchers and regular
askers perform around a specific question they posted, as
well as the duration of this set of actions. Follow-up actions
after a posted question include: browsing the question page
(e.g. checking for new answers), adding more details to the
question, selecting a best answer, reporting abusive answers,
voting for answers, and deleting the question.
Table 10 provides the average statistics of these actions,
while Figures 9 and 10 depict the distribution of number of
actions and their duration for searchers and regular askers.
From the table we can see that searchers perform fewer yet
similar number of actions as typical askers do, but in a much
shorter duration. As can be seen by Figure 9, in terms of
number of actions, the difference of about one more action
on average for regular askers is small though constant. For
the duration of the interaction, regular askers spend about
7% more time on average around the question, but looking
at the median, the difference is substantially larger, with
half the searchers spending 2.2 hours or less while the typical askers tend to spend about 68% more time, or 3.7 hours,
at the median. As an interesting comparison, we also measured the average time the searchers spent searching before
asking questions, to show the substantial difference between

808

1.0
Cumulative distribution function
0.2
0.4
0.6
0.8

queries [21] have also received special research attention. Besides, searcher satisfaction and frustration [11][15][2][16] has
also been actively studied, which utilized query log information for satisfaction prediction, such as relevance measures,
as well as user behavior during the search session, including
mouse clicks and time spent between user actions.
Donato et al. [10] identified the research missions that
often associate with complex information needs and require
collecting information from many pages. In our work we
focused on studying the types of queries that arguably are
difficult for a web search engine to satisfy, often require human to answer [19][20][18], and thus could be better handled
by CQA sites. Liu et al. [18] argued that some of these needs
can be satisfied with existing answers from CQA archives by
harnessing the unique structure of such archives for detecting web searcher satisfaction. Our work in this paper further
observed that many searchers not satisfied with search results finally posted a related question on a CQA site, which
inspired our analysis of how searchers become askers.
White and Dumais [24][12] studied search engine switching behavior and developed models to predict the switching
and its rationale. Although different types of searchers are
focused on (they focused on searchers who turn to another
search engine and issue more queries, while we focused on
searchers who turn to CQA sites and post questions), we are
both interested in characterizing the types of queries and
searcher behavior that lead to the switchings. Our analysis
shows both similar (e.g. longer sessions are more likely to
involve a switch) and different characteristics (e.g. different
last action before switching) compared to their study.
On the CQA side, there is also research effort devoted to
question analysis, e.g. distinguishing conversational and informational questions [14], identifying high quality questions
[3], and investigating the effects of contexts in questions on
answer quality [23]. In our work, we use their classification
of contextual factors to analyze the semantic difference between the query and question posted by the same user for
the same need. There is also some previous work related
to asker behavior analysis. For example, Adamic et al. [1]
analyzed the content properties and user interaction patterns across different Yahoo! Answers categories. Gyongyi
et al. [13] studied several aspects of user behavior in Yahoo!
Answers, including users’ activity levels, interests, and reputation. Yet, they did not study the effort that askers spend
in tracing their posted questions as we studied in this paper.

0.0

SearchAsk
Ask

1
2
5
10
20
50
# of user actions on question after it is posted

0.8
0.6
0.4
0.2

SearchAsk
Ask
Search

0.0

Cumulative distribution function

1.0

Figure 9: Count of user actions in questions

0

10m

1h

1d

1m

Duration of user actions on the question after it is posted

Figure 10: Duration of user actions in questions
an interactive search session and an offline asking session, a
difference that is clear to the searchers, since they are willing to spend several hours waiting for an answer to arrive,
compared to a few minutes actively searching.
In summary, we showed that searchers are expecting a
faster response time for their questions, which often aim to
address practical problem solving tasks. On the other hand,
general Yahoo! Answers askers are willing to put more effort
in following up their questions. One possible reason for this
behavior is that Yahoo! Answers site is often viewed by users
from a more social perspective, as indicated by many users
asking socially-focused (e.g., conversational) questions.

6.

7.

CONCLUSIONS

Web search needs are becoming increasingly sophisticated,
and the expectations have grown accordingly. As a result,
quite a few search sessions end in posting a question in a
Community Question Answering service, as the searcher realizes that such a service could better answer her need.
This work studies the unique properties of SearchAsk sessions: search sessions that turn into question composition.
To the best of our knowledge, this paper presents the first
large-scale analysis of the user transition from searching to
asking. What makes our work unique is the study of the
explicit connection between the search query and the corresponding question from the same user for the same need.
It provides insights into some specific needs that searchers
try to express on search engines, yet are not satisfied by
search results, and turn to human answerers instead. We
analyzed the various aspects of SearchAsk sessions, includ-

RELATED WORK

As we study the transformation of unsatisfied searches
into questions posted on a popular CQA site, our work is
related to the work on query log analysis, searcher behavior
and satisfaction prediction, and CQA question analysis.
On the search side, significant research has been done on
analysis of queries and searcher behavior based on query
logs. For example, understanding query intent and user
goals has attracted much research effort [22][6]. Difficult
queries [9][8], long and tail queries [4][5], and question-like

809

ing the differences between general search-engine queries and
those belonging to a SearchAsk session, the transformation
of a query into a natural language question and the question composition patterns, as well as other asking behavior
of searchers, compared to general askers in a CQA service.
Our findings may contribute both to search-engine optimization, as well as to better user experience in CQA sites.
For example, we found out that searchers are not as patient
as regular askers when waiting for answers to their questions.
This finding may influence CQA sites to promote questions
coming from searchers, if they want to retain their engagement. As another example, our analysis of the transitions
between user actions in SearchAsk sessions, and especially
the fact that question asking is typically preceded by viewing
a CQA page, may help search engines. They might decide to
detect such cases and explicitly promote the option of asking a question to the searcher, even before she resorts into
doing it on her own. Furthermore, as this paper demonstrates, modeling the transformation of a query meant for
an automated search engine into a fully specified question
meant for human, provides a valuable tool for query intent
and satisfaction analysis.
In future work, we intend to develop some of the directions mentioned above. One of the most intriguing ones in
our view is for search engines to automatically trigger a dialog for posting questions in the right CQA forum, whenever
a SearchAsk need is detected. This is just one application
made possible by our study, which lays a foundation for more
effective integration of automated web search and social information seeking.

8.

[8] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg.
What makes a query difficult? In SIGIR, 2006.
[9] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.
Predicting query performance. In SIGIR, 2002.
[10] D. Donato, F. Bonchi, T. Chi, and Y. Maarek. Do you
want to take notes?: identifying research missions in
yahoo! search pad. In WWW, pages 321–330, 2010.
[11] H. A. Feild, J. Allan, and R. Jones. Predicting
searcher frustration. In SIGIR, pages 34–41, 2010.
[12] Q. Guo, R. W. White, Y. Zhang, B. Anderson, and
S. T. Dumais. Why searchers switch: understanding
and predicting engine switching rationales. In SIGIR,
pages 335–344, 2011.
[13] Z. Gyongyi, G. Koutrika, J. Pedersen, and
H. Garcia-Molina. Questioning yahoo! answers.
Evolution, 2008.
[14] F. M. Harper, D. Moy, and J. A. Konstan. Facts or
friends?: distinguishing informational and
conversational questions in social q&a sites. In CHI,
pages 759–768, 2009.
[15] A. Hassan, R. Jones, and K. L. Klinkner. Beyond dcg:
user behavior as a predictor of a successful search. In
WSDM, pages 221–230, 2010.
[16] S. B. Huffman and M. Hochster. How well does result
relevance predict session satisfaction? In SIGIR, 2007.
[17] R. Jones and K. L. Klinkner. Beyond the session
timeout: automatic hierarchical segmentation of
search topics in query logs. In CIKM, 2008.
[18] Q. Liu, E. Agichtein, G. Dror, E. Gabrilovich,
Y. Maarek, D. Pelleg, and I. Szpektor. Predicting web
searcher satisfaction with existing community-based
answers. In SIGIR, pages 415–424, 2011.
[19] M. R. Morris, J. Teevan, and K. Panovich. A
comparison of information seeking using search
engines and social networks. In ICWSM, 2010.
[20] M. R. Morris, J. Teevan, and K. Panovich. What do
people ask their social networks, and why?: a survey
study of status message q&a behavior. In CHI, 2010.
[21] B. Pang and R. Kumar. Search in the lost sense of
“query”: question formulation in web search queries
and its temporal changes. In HLT, 2011.
[22] D. E. Rose and D. Levinson. Understanding user goals
in web search. In WWW, pages 13–19, 2004.
[23] S. Suzuki, S. Nakayama, and H. Joho. Formulating
effective questions for community-based question
answering. In SIGIR, pages 1261–1262, 2011.
[24] R. W. White and S. T. Dumais. Characterizing and
predicting search engine switching behavior. In CIKM,
pages 87–96. ACM, 2009.
[25] S. Zhao, H. Wang, C. Li, T. Liu, and Y. Guan.
Automatically generating questions from queries for
community-based question answering. In IJCNLP,
pages 929–937, 2011.
[26] Z. Zheng, X. Si, E. Y. Chang, and X. Zhu. K2q:
Generating natural language questions from keywords
with user refinements. In IJCNLP, 2011.

ACKNOWLEDGMENTS

This work was supported by the National Science Foundation grant IIS-1018321 and by the Yahoo! Faculty Research and Engagement Program. The authors also would
like to thank Elad Yom-Tov for the helpful discussion and
the anonymous reviewers for their valuable comments.

9.

REFERENCES

[1] L. A. Adamic, J. Zhang, E. Bakshy, and M. S.
Ackerman. Knowledge sharing and yahoo! answers:
everyone knows something. In WWW, 2008.
[2] M. Ageev, Q. Guo, D. Lagun, and E. Agichtein. Find
it if you can: a game for modeling different types of
web search success using interaction data. In SIGIR,
pages 345–354, 2011.
[3] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and
G. Mishne. Finding high-quality content in social
media. In WSDM, pages 183–194, 2008.
[4] P. Bailey, R. W. White, H. Liu, and G. Kumaran.
Mining historic query trails to label long and rare
search engine queries. ACM Trans. Web, 4:15:1–15:27,
September 2010.
[5] M. Bendersky and W. B. Croft. Analysis of long
queries in a large scale search log. In WSCD, 2009.
[6] A. Broder. A taxonomy of web search. SIGIR Forum,
36:3–10, September 2002.
[7] G. Buscher, R. W. White, S. Dumais, and J. Huang.
Large-scale analysis of individual and task differences
in search result page examination strategies. In
WSDM, pages 373–382, 2012.

810

