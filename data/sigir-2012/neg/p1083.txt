Genre Classification for Million Song Dataset Using
Confidence-Based Classifiers Combination
Yajie Hu

Mitsunori Ogihara

Department of Computer Science
University of Miami
Coral Gables, Florida, USA

Department of Computer Science
University of Miami
Coral Gables, Florida, USA

yajie.hu@umail.miami.edu

ogihara@cs.miami.edu

ABSTRACT
We proposed a method to classify songs in the Million Song
Dataset according to song genre. Since songs have several
data types, we trained sub-classiﬁers by diﬀerent types of
data. These sub-classiﬁers are combined using both classiﬁer authority and classiﬁcation conﬁdence for a particular
instance. In the experiments, the combined classiﬁer surpasses all of these sub-classiﬁers and the SVM classiﬁer using concatenated vectors from all data types. Finally, the
genre labels for the Million Song Dataset are provided.

a correct decision regarding a particular item are also different. Hence, the voting result of an instance is related to
both the authority of the classiﬁer and the conﬁdence of the
classiﬁer to classify the particular instance.
In this paper, we extract features from audio, artist terms,
lyrics and social tags to represent songs and train sub-classiﬁers. The trained sub-classiﬁers are combined to classify song
genre. The songs with missing data in certain data types are
classiﬁed by the remaining data without any negative inﬂuence by the missing data.

Categories and Subject Descriptors

2. METHOD

I.5.2 [Design Methodology]: Classiﬁer design and evaluation; H.5.5 [Sound and Music Computing]: Methodologies and techniques

We apply each data source to train a sub-classiﬁer and
we assume that classiﬁer set C contains n sub-classiﬁers,
namely, C = {c1 , c2 , . . . , cn }. Furthermore, we assume that
songs are distributed into m genres, G = {g1 , g2 , . . . , gm }.
The voting result is shown in Equation 1.

General Terms
Algorithms, Experimentation

G (Ik ) = arg max

Keywords

gj

Classiﬁer Combination, Song Genre Classiﬁcation

1.

⎧
|C|
⎨
⎩

i=1

⎫
⎬
[Auth (ci ) · Conf (ci , gj , Ik )]
(1)
⎭

Auth (ci ) denotes the authority of the classiﬁer ci from
0.0 to 1.0. Auth (ci ) is estimated by the accuracy of the
classiﬁcation in the validation test.
Conf (ci , gj , Ik ) is the conﬁdence of the classiﬁer ci to classify the instance Ik to genre gj . For Naı̈ve Bayes, the posterior probability is seen as the conﬁdence for a class. Neural
Net has normalized real value output from -1.0 to 1.0. A positive value means the conﬁdence to assign the instance to a
positive label. We employ the method proposed by Lee [2] to
estimate the conﬁdence for logistic regression. The margin
from the instance location to the classiﬁcation hyperplane is
considered to be the conﬁdence of the SVM classiﬁer. The
conﬁdences of classiﬁers are normalized into [0.0, 1.0]. The
conﬁdence for invalid data is set to 0.0, in order to avoid
negative eﬀect caused by invalid data.

INTRODUCTION

In music information retrieval, many methods see song
genre as important metadata for retrieving songs. As the
largest currently available dataset, the Million Song Dataset
(MSD) is a collection of audio features and metadata for a
million contemporary popular music tracks. However, none
of records have any genre labels. The goal of this paper is to
automatically classify songs in the MSD according to genre.
Some papers have discussed the importance of multiple
data sources in genre classiﬁcation and proposed methods to
use them. Most of these methods [3] concatenated features
from diﬀerent data sources into a vector to represent the
song. However, for a huge scale dataset, it is impossible
that every instance will have valid data in all data sources.
It is inevitable for the classiﬁcation results to be demoted
due to data missing inﬂuence in the concatenated vector.
If we imagine classiﬁers as experts in voting, the accuracy of each classiﬁer represents the authority of the expert.
Because the types of input data are diﬀerent, the views of
experts are not same. Therefore, the conﬁdences to make

3. EXPERIMENT
In our experiment, we applied MSD, MusiXmatch and
Last.fm tag datasets to extract features, as shown in Table 1.
The records in these data sources are matched via trackID.
AllMusic.com provides genre taxonomy, which consists of
10 major genres with sample songs. 1,138 songs are collected
from AllMusic.com and they have valid records in MSD as
the ground truth. The distribution of the songs according
to genre is shown in Figure 1.

Copyright is held by the author/owner(s).
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
ACM 978-1-4503-1472-5/12/08.

1083

Bules 52

38

1

1

24

1

30

13

6

3

2

0

1

5

4

6

6

3

2

Bules 36

20

10

16

16

13

17

13

15

13

Bules 47

12

11

16

16

9

23

13

9

13

Country 36

75

0

1

17

0

62

15

2

2

Country

6

185

0

1

0

1

14

2

0

1

Country 17

35

19

12

17

18

48

16

17

11

Country

7

103

12

6

13

6

32

17

6

8

Electronic

0

1

2

1

1

0

9

2

3

0

Electronic

0

1

13

0

1

0

4

0

0

0

Electronic

0

3

2

0

0

2

5

3

2

2

Electronic

0

1

1

1

3

0

7

1

4

1

International

2

0

0

4

2

0

4

1

1

0

nternational

0

0

1

8

2

1

2

0

0

0

International

0

1

0

3

2

4

2

0

1

1

International

1

2

0

5

0

0

3

1

2

0

Jazz 21

18

0

2

56

3

14

9

1

0

Jazz

2

0

0

3

110

2

6

1

0

0

Jazz 11

11

14

12

15

11

17

9

12

12

Jazz 10

9

9

5

56

5

6

7

10

7

1

0

0

0

5

0

5

0

1

0

Latin

0

1

0

0

4

5

2

0

0

0

Latin

0

1

1

0

2

3

2

1

0

2

Latin

0

1

0

0

3

4

1

1

1

1

Pop/Rock 20

43

6

3

14

4

252

34

11

4

Pop/Rock

6

16

1

11

7

77

253

12

6

2

Pop/Rock 46

46

27

25

25

30

90

42

30

30

Pop/Rock 10

9

11

15

9

10

308

7

4

8

R&B 17

21

1

1

6

1

46

16

6

6

R&B

3

0

0

0

2

10

6

92

8

0

R&B 14

9

9

3

7

11

29

17

8

14

R&B 11

11

8

4

16

10

31

13

9

8

Rap

7

1

0

0

2

0

13

3

22

0

Rap

0

0

0

0

1

1

5

0

41

0

Rap

1

7

0

3

4

6

12

2

9

4

Rap

4

3

3

5

5

4

5

5

10

4

Reggae

3

3

0

1

1

0

9

3

3

7

Reggae

0

0

0

0

1

4

3

1

0

21

Reggae

3

5

1

1

4

6

5

3

1

1

Reggae

1

2

5

1

0

5

5

0

5

6

Latin

e
ga
eg
R

(d)

Logistic Regression by lyrics

ap
R

&B k
R
oc
R
p/
Po
tin
La
al
zz
Ja tion
a
rn
te ic
In
n
tro
ec
El y
r
nt
ou
C
s
le
Bu

e
ga
eg
R

(c)

ap
R

Neural Net by artist terms

&B k
R
oc
R
p/
Po
tin
La
al
zz
Ja tion
a
rn
te ic
In
n
tro
ec
El y
r
nt
ou
C
s
le
Bu

(b)

e
ga
eg
R

Neural Net by audio features

ap
R

&B k
R
oc
R
p/
Po
tin
La
al
zz
Ja tion
a
rn
te ic
In
n
tro
ec
El y
r
nt
ou
C
s
le
Bu

e
ga
eg
R

ap
R

&B k
R
oc
R
p/
Po
tin
La
al
zz
Ja tion
a
rn
te ic
In
n
tro
ec
El y
r
nt
ou
C
s
le
Bu

(a)

Bules 140

Naı̈ve Bayes by social tags

Figure 2: Confusion matrixes of four sub-classifiers

0

5

5

3

2

0

0

0

0

20

1

0

1

Electronic

3

0

3

1

1

2

6

2

0

1

Electronic

0

1

11

0

1

0

6

0

0

0

International

1

2

1

7

1

0

1

0

1

0

International

1

0

1

8

1

0

3

0

0

0

Jazz 17

7

5

6

57

6

9

5

4

8

Jazz

3

2

0

3

109

0

5

2

0

0

5

0

0

2

0

1

2

2

0

0

Latin

0

1

0

0

5

4

2

0

0

0

Pop/Rock 13

13

16

11

14

13

265

14

21

11

Pop/Rock

5

16

1

0

3

1

348

9

6

2

R&B 17

8

4

7

9

7

38

17

7

7

R&B

3

1

1

0

4

0

23

84

5

0

Rap

5

6

5

2

2

4

5

2

12

5

Rap

0

0

0

0

0

0

6

0

42

0

Reggae

1

3

4

1

1

2

7

1

4

6

Reggae

0

0

0

0

1

0

7

1

3

18

In

Ja

La

Po

R

R

R

C

El

In

Ja

La

Po

R

R

Latin

eg
e

oc

ga

R
k

na

c

k

io

ni

at

ry

tro

rn

ec

s

nt

le

e

oc

ga

R

l

l

na

c

io

ni

at

SVM by long vector

ou

Bu

eg

p/

rn

ry

tro

s

nt

le

ec

Bu

(a)

R

4

183

ap

0

5

Bules 145

p/

0

Country

&B

5

5

tin

7

12

zz

7

11

te

16

35

ap

29

8

&B

11

13

tin

13

10

zz

11

6

te

8

92

ou

17

El

MuisXmatch
Last.fm tags

Bules 50
Country 18

C

Name
MSD

Table 1: Data sources
Extracted information Number of records
Audio features,
1,000,000
artist terms
Lyrics features
237,662
Social tags
505,216

(b)

Combined classifier

Figure 3: Confusion matrixes by all data
Table 2: Experiment result comparison
Data
Method
Accuracy
Audio
Neural Net
42.70%
Artist terms
Neural Net
76.27%
Lyrics
Logistic Regression
18.54%
Social Tags
Naı̈ve Bayes
48.59%
All data
SVM
44.82%
All data
Combined classiﬁers
83.66%

Figure 1: Genre Samples in AllMusic.com
In order to improve classiﬁcation performance, we convert genre classiﬁcation into a series of binary classiﬁcations.
Thus, the classiﬁcation result of a song is a vector of conﬁdence to classify the song into a genre. The predicted genre
is the one whose conﬁdence is highest.
We extract features from diﬀerent data sources and trained
individual classiﬁers by each type of features using Naı̈ve
Bayes, Rule Induction, LDA, Neural Net, Logistic Regression and SVM, respectively. The classiﬁers’ performances
are evaluated by 5-folder cross validation. The best performance classiﬁers in diﬀerent types of features are shown in
Figure 2.
The four sub-classiﬁers are combined based on classiﬁcation authority and conﬁdence. The resultant combined classiﬁer is signiﬁcantly better than sub-classiﬁers and the SVM
classiﬁer using concatenated vectors from four data sources
as shown in Figure 3 and Table 2. The result is encouraging
regarding to the result of genre classiﬁcation task in MIREX
[1]. Furthermore, we apply the combined classiﬁer to classify all of the songs in the MSD and the result is available at
http://web.cs.miami.edu/home/yajiehu/resource/genre.

4. CONCLUSION
Based on classiﬁer authority and classiﬁcation conﬁdence,
the combined classiﬁer integrates sub-classiﬁers, which are
good at classiﬁcation of certain data sources. The combined
classiﬁer performs with higher accuracy than sub-classiﬁers
and the SVM classiﬁer using concatenated vectors.

5. REFERENCES
[1] http://www.music-ir.org/mirex/wiki/2009.
[2] C.-H. Lee. Learning to combine discriminative
classiﬁers: conﬁdence based. In Proceedings of the 16th
ACM SIGKDD, KDD ’10, pages 743–752, New York,
USA, 2010.
[3] C. McKay, J. A. Burgoyne, J. Hockman, J. B. L.
Smith, G. Vigliensoni, and I. Fujinaga. Evaluating the
genre classiﬁcation performance of lyrical features
relative to audio, symbolic and cultural features. In
Proceedings of the 11th ISMIR, ISMIR ’10, pages
213–718, Utrecht, Netherlands, 2010.

1084

