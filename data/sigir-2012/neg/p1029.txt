A Study of Term Weighting Schemes Using Class
Information for Text Classification
Youngjoong Ko
Dept. of Computer Engineering, Dong-A Univ., 840, Hadan 2-dong, Saha-gu, Busan, 604-714, Korea

yjko@dau.ac.kr
Categories and Subject Descriptors:

I.5.4 [Pattern

2. NEW TERM WEIGHTING SCHEMES

Recognition]: Applications – Text processing

Unlike the previous work, the idf part of new term weighting
schemes is replaced by using the probability estimations on a
relevant class (positive class) and other classes (negative classes),
and the log-odds ratio of them as follows:

General Terms: Experimentation, Performance
Keywords: Text Classification, IDF, Term Weighting
1. INTRODUCTION

 P( wi | c j )

twi  (log(tf i )  1)  log
 
 P( wi | c j )




Text classification is the task of automatically assigning
unlabeled documents into predefined categories. In text
classification, text representation transforms the content of
textural documents into a compact format so that the documents
can be recognized and classified by a classifier. In the vector
space model, a document is represented as a vector in the term
spaces, d  ( w1,..., w|V | ) , where |V| is the size of vocabulary. The

(1)

where tfi is the number of times term wi occurs in a document, d,
cj is a class of the document, d, c j is all of other classes of cj, and
 is a constant value of the base of this logarithmic operation
because it makes the logarithmic value a positive value.

value of wi between [0,1] represents how much the term wi
contributes to the semantics of the document d. Text classification
has borrowed the traditional term weighting schemes from
information retrieval field, such as tf, tf.idf [1] and its variants.
This research starts with a question, “Can we make a better
term weighting scheme than ones of information retrieval for text
classification?” We believe that text classification should utilize
class information better than information retrieval because
supervised learning based text classification has labeled training
data. However, inverse document frequency, idf, is a measure of
general importance of a term; there is no use of class information.
Therefore, this paper focuses on how we can improve text
classification by effectively applying class information to a term
weighting scheme.
Recently, researchers have attempted to use this prior
information, class information, for term weighting. There are two
representative term weighting schemes: rf (relevance frequency)
[2] and Delta tf.idf [3,4]. The former was proposed to use the ratio
of term occurrences of the positive class and the negative class for
calculating term weights. However, they did not discuss how they
make the representation of test documents. Since a test document
does not have any prior class information, it is hard to represent
the test document using class information. The latter provided a
solution to use class information for sentiment classification by
localizing the estimation of idf to the documents of one or the
other class and subtracting the two values. However, this
approach is limited to classification problems with only two
classes just like sentiment classification.
This paper proposes new term weighting schemes for multiclass text classification, which include term weighting methods
for test documents. Then it was compared to the previous studies
[2,3,4] and tf.idf. As a result, the proposed schemes achieved the
best performance on all of data sets and classifiers.

This new idf part is called Term Relevance Ratio (TRR). TRR
can be estimated by the following two different ways.

 tf
P(w | c ) 
 
|Tc j |

i

ik
k 1
|V |
|Tc j |

j

l 1

k 1

 tf
, P( w | c ) 
 
|Tc j |

i

tf lk

j

|V |

ik
k 1
|Tc j |

l 1

k 1

(2)
tf lk

where V denotes the vocabulary set of total training data and
Tc j is the document set of positive class, cj, and Tc j is the
document set of negative classes, c j . To resolve zero divisor
problem, we assign the smallest probability value, which can be
estimated in training data set, to the divisors of both equations.
This is maximum likelihood estimation (MLE) and this
estimation misses term distribution in a document. Thus we
reform these equations as follows:


P( w | c )  
P( wi | c j ) 
i

j

|Tc j |

k 1
|Tc j |
k 1

P( wi | d k )  P(d k | c j ) ,

(3)

P( wi | d k )  P(d k | c j )

where P ( wi | d k ) is estimated by MLE like equation (2) and
P(d k | c j ) is estimated as a uniform distribution.

The next issue is how to represent test documents. We need to
develop a document representation method because test
documents do not have any class information and our term
weighting schemes require it. A test document can be first
represented as |C| different vectors by using estimated distribution
of each class, cj, and then it has to be represented as one vector
that well describes the document in our proposed vector space; |C|
is the number of classes. We consider the following three
solutions for this problem and you can see experimental evidences
in the next section.

Copyright is held by the author/owner(s).
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
ACM 978-1-4503-1472-5/12/08.

1029

1.
2.
3.

Word Max (W-Max): the term weight of each word is chosen
by the maximum value among |C| estimated term weights.
Document Max (D-Max): the sum of all term weights in
each vector is first calculated and then one vector with the
maximum sum value is selected by a representative vector.
Documents Two Max (D-TMax): the sum of all term weights
in each vector is calculated and then two vectors with the
highest and second highest sum values are selected. Then a
vector is created by choosing the term weight with the higher
score between two term weights of selected vectors for each
term.

3.1.2 Term weighting schemes for test data
Table 2 shows the performances of three different term
weighting methods for test documents: W-Max, D-Max and DTMax. D-TMax achieved the best performance in Reuters while
D-Max the best performance in NG. We think it is very natural
results because many documents in Reuters have two or more
labels. For example, if a document with two labels is represented
by using only one class distribution between two labels,
classifiers could has some difficulty to classify it in the other class.
Table 2. Comparison of term weighting schemes for test data

3. EXPERIMENTS
Two widely-used data sets were used as the benchmark data
sets: Reuters and 20 Newsgroups data sets, and two promising
learning algorithms, which have shown better performance than
other algorithms, were chosen in the experiments: kNN and SVM.
The one-against-the-rest method was used for setting up positive
examples and negative examples for each class.
The Reuters 21578 data set (Reuters) was split as training and
test data according to the standard ModApte split and the top 10
largest categories were used in the experiments. The 20
Newsgroups data set (NG) is a collection of approximate 20,000
newsgroup data evenly divided among 20 discussion groups. For
fair evaluation, we used five-fold cross validation.
These data sets have quite different characteristics. Reuters has
a skewed class distribution and many documents have two or
more class labels. On the other hand, NG has uniform class
distribution and its documents have only one class label. Thus,
two different measures were used to evaluate various term
weighting schemes on two classifiers for our experiments. For
Reuters, we used the micro-averaging Break Even Point (BEP)
measure which is a standard information retrieval measure for
binary classification and, for NG, the performances are reported
by the micro-averaging F1 measure.

Reuters

SVM
kNN
NG
SVM

NG

First of all, the proposed term weighting schemes (TRR) are
compared with the traditional idf scheme. All of these schemes
did not use any tf information. As a result, the proposed schemes
achieved better performances than the idf scheme. We here need
to discuss the results of TRR because TRR has two different
estimation methods (Cat-MLE by equation (2) and Doc-MLE by
equation (3)) and they showed different performance aspects in
each data set; Cat-MLE obtained the best performances in Reuters
while Doc-MLE the best performances in NG. It can be caused by
the skewed distribution of the document length in Reuters; many
documents in Reuters consist of small number of sentences such
as just two or three. It could make a bad probability estimation in
the Doc-MLE scheme. We can also observe the same results in the
following experiments; Cat-MLE is better in Reuters while DocMLE in NG.

NG

kNN
SVM
kNN
SVM

tf.idf
92.46
94.87
81.20
87.07

log tf.idf
93.29
94.86
85.17
87.74

rf
92.07
94.40
83.60
87.46

Delta
91.96
93.00
86.01
86.64

Proposed
94.90
95.30
87.75
88.43

4. CONCLUSIONS AND FUTURE WORK
In this work, we utilized class information for term weighting
for text classification. As a result, the proposed schemes
performed consistently well on the two benchmark data sets and
kNN and SVM classifiers.
In the future, we would like to explore these schemes to apply
and evaluate it on different classifiers and different data sets.

5. REFERENCES
[1] G. Salton and C. Buckley. Term-weighting approaches in
automatic text retrieval. Inf. Process. Manage. 24(5):513-523.
[2] M. Kan, C.-L. Tan and H.-B. Low. Proposing a new term
weighting scheme for text categorization. In AAAI 2006, pp.
763-768.
[3] J. Martineau and T. Finin. Delta TFIDF: an improved feature
space for sentiment analysis. In AAAI 2009.
[4] G. Paltoglou and M. Thelwall. A study of information
retrieval weighting schemes for sentiment analysis. In ACL
2010, pp. 1386-1395.

Table 1. Comparison of TRR (Cat-MLE & Doc-MLE) and idf
Reuters

D-TMax
94.90
94.72
95.30
95.12
86.03
86.36
87.54
87.75

Table 3. Final experiment results
Reuters

Cat-MLE
94.22
94.69
86.93
87.39

D-Max
94.51
93.97
94.83
94.65
87.15
87.75
87.93
88.42

Table 3 shows the final experimental results and comparison of
the proposed scheme and other schemes. The proposed scheme
achieved better performance than other schemes. Note that tf.idf
used raw term frequency while log tf.idf used log(tf)+1 like
equation (1). Delta and rf also used the proposed document
representation methods for test data and their best performances
were chosen and shown in Table 3.

3.1.1 Comparison of the proposed schemes and idf

idf
92.86
94.11
86.39
87.61

W-Max
94.33
94.54
94.83
94.72
84.19
84.30
86.60
86.24

3.1.3 Comparison of the proposed scheme and other
term weighting schemes

3.1 Experimental Results

kNN
SVM
kNN
SVM

kNN

Cat-MLE
Doc-MLE
Cat-MLE
Doc-MLE
Cat-MLE
Doc-MLE
Cat-MLE
Doc-MLE

Doc-MLE
93.93
94.29
87.20
87.77

1030

