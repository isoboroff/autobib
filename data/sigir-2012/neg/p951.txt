Confidence-Aware Graph Regularization
with Heterogeneous Pairwise Features
∗

Yuan Fang

Bo-June (Paul) Hsu

Kevin Chen-Chuan Chang

Univ. of Illinois at Urbana-Champaign
201 N Goodwin Avenue
Urbana, IL 61801, USA

Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA

Univ. of Illinois at Urbana-Champaign
201 N Goodwin Avenue
Urbana, IL 61801, USA

fang2@illinois.edu

paulhsu@microsoft.com

kcchang@illinois.edu

ABSTRACT

1. INTRODUCTION

Conventional classification methods tend to focus on features of individual objects, while missing out on potentially valuable pairwise
features that capture the relationships between objects. Although
recent developments on graph regularization exploit this aspect, existing works generally assume only a single kind of pairwise feature, which is often insufficient. We observe that multiple, heterogeneous pairwise features can often complement each other and
are generally more robust in modeling the relationships between
objects. Furthermore, as some objects are easier to classify than
others, objects with higher initial classification confidence should
be weighed more towards classifying related but more ambiguous
objects, an observation missing from previous graph regularization
techniques. In this paper, we propose a Dirichlet-based regularization framework that supports the combination of heterogeneous
pairwise features with confidence-aware prediction using limited
labeled training data. Next, we showcase a few applications of
our framework in information retrieval, focusing on the problem
of query intent classification. Finally, we demonstrate through a
series of experiments the advantages of our framework on a largescale real-world dataset.

Many applications in information retrieval (IR) call for effective
classification techniques. For instance, the problem of text categorization is fundamental to a myriad of real-world tasks, such as
automated email organization, spam detection, and information filtering. As another example, fine-grained query intent classification
enables a search engine to direct users to the intended search verticals, greatly enhancing user experience.
While any conventional machine learning algorithm can be used
for classification in IR applications, straightforward adaptation is
generally unfruitful, since there exist challenges beyond traditional
classification tasks. One prominent issue is the sparsity of feature vectors. For instance, in our query intent classification dataset,
about 95% of the queries contain no more than five words.
Recent developments in graph-based regularization [20, 18, 2, 3]
somewhat tackles the problem of feature sparsity. Instead of objectlevel features that are independently extracted from each object, we
now consider pairwise features that are extracted from each pair of
objects. Hence, each object can potentially be paired with any other
object for feature extraction. In this work, we employ pairwise features derived from similarity functions between objects, such that a
graph can be constructed by linking similar objects with edges. The
edges can be used to propagate classification evidence from one object to another based on the key assumption of consistency—similar
objects are likely to have similar class labels [20]. In other words,
the classification of an object can be “helped” or regularized by
its neighbors on the graph. In comparison, conventional classification algorithms cannot easily incorporate pairwise features without
blowing up the parameter space and subsequently requiring more
labeled training data.
Given the above benefits, graph-based regularization has also
been adopted in IR applications, most notably through the use of
a click graph [12, 7, 11] in query intent classification. A click
graph usually consists of queries and websites as its vertices. A
query q and a website w are connected by an edge if q results in a
click landing on w. Intuitively, two queries connected to the same
website are related, and thus should share a similar prediction.
While making the same assumption of consistency, we are motivated by two core observations, which entail key intuitions towards
a more effective graph-based regularization framework.
First, most existing regularization framework only deals with a
single kind of pairwise feature. However, heterogeneous pairwise
features often exist. For instance, in query intent classification,
queries can be related through not only clicks, but also other pairwise features, including lexical similarity (“acer laptop” and “laptop” are related due to word overlap), co-session (the second query
could be a refinement of the first in the same session), and the simi-

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval

General Terms
Algorithms, Design, Experimentation, Performance

Keywords
graph regularization, pairwise features, confidence, query intent
classification, applications in information retrieval
∗Work done during an internship at Microsoft Research.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$10.00.

951

2. RELATED WORK

larity of search result pages (two queries that retrieve similar pages
may be similar themselves). These different pairwise features potentially complement each other, as some may only cover a small
fraction of all queries (e.g., not all shopping queries have associated product clicks). In addition to sparsity, some pairwise features
may lack the robustness to capture similarities between two objects. E.g., while both queries “hp 3 in 1” and “canon printer” refer
to printers, they contain no common words and thus their similarity cannot be represented by lexical pairwise features. To support
heterogeneous pairwise features, the challenge lies in how different
similarity functions can be aggregated to optimize the regularization, which is missing from previous regularization frameworks.
Second, in existing graph regularization, the extent to which the
classification of an object o influences o0 only depends on their similarity. Greater similarity between o and o0 means that they have
stronger influences on each other. While this is reasonable, a second factor, largely overlooked by previous work, is the confidence
of classification. If we are more confident about the prediction on
o, we expect it to influence its neighbors on the graph more. On the
other hand, if we are unsure about the prediction on o, we should
minimize its influence. In other words, we use objects that are easier to classify (with higher classification confidence) to help predict
harder ones (with lower classification confidence), but less so the
other way round. Existing regularization works do not provide for
a mechanism to incorporate classification confidence. In contrast,
our modeling of objects with Dirichlet priors allows us to interpret
the observation counts as confidence.
Finally, another challenge of classification lies in the requirement of a large amount of labeled training data, especially in realworld IR applications with a large number of classes. Our experiments on query intent classification involve 2043 classes, and thus
require much more labeled training queries than, say, a binary classification task. Our framework deals with the shortage of labeled
training data in two ways. To begin with, like other graph-based
regularization frameworks (e.g., [20, 18]), classification evidence
is propagated across the graph along the edges. By using more unlabeled data, we generally obtain a denser graph that promotes such
propagation, and hence improve the performance. Our experiments
show that adding more unlabeled data yields superior results, despite using the same set of labeled training data. Next, our framework allows for fewer parameters, independent of the number of
classes. Hence, even with limited training data and a large number
of target classes, our technique is effective.
In this paper, we propose a regularization framework that can be
applied to different applications, hinged on our insights above. To
summarize, we make the following contributions:

Pairwise features. Many IR classification tasks are plagued by
feature sparsity. Thus, a significant amount of research has studied
augmenting the feature vector (e.g., [4, 16, 15, 5] for query-intent
classification). Unfortunately, these techniques are generally not
universal to other classification tasks. Driven by the hypothesis that
“similar objects share similar labels” [20], we consider pairwise
features that allow for feature extraction from any pair of objects,
potentially expanding the feature space.
Although it is difficult to incorporate pairwise features in conventional classification algorithms without blowing up the parameter space, they have been used in recent graph-based regularization
frameworks [20, 18, 2, 3] and related random walk approaches [17,
7]. Examples of their applications in IR include the use of click
graphs [12, 10, 7, 11] in query intent classification, and document
affinity matrices [8, 9] in text retrieval. However, unlike this paper, most of these works do not consider heterogeneous pairwise
features, which complement each other and thus play an important
role in improving classification accuracy. While a recent work [11]
explores a content-based pairwise feature in addition to co-clicks,
its treatment is limited as it lacks a mechanism to effectively aggregate arbitrary pairwise features. In particular, their parameters are
manually selected, making it difficult to extend to more pairwise
features. In comparison, we learn the parameters via an iterative
optimization process.
Confidence. Existing regularization frameworks [20, 18, 2, 3] and
their task specific realizations (e.g., [12, 10, 7, 11] for query intent classification and [8, 9] for text retrieval) do not recognize the
importance and hence take advantage of the classification confidence associated with objects. We observe the need of confidenceaware regularization, and exploit it to improve classification accuracy. Specifically, we are more confident about objects that are
easier to classify, which should have a larger influence towards the
prediction of their neighbors.
Limited training data. To deal with the shortage of labeled training data, many task specific techniques exist (e.g., [1, 12, 13, 15]
for query-intent classification). However, they are often highly
tailored and cannot be easily extended to a generic framework.
In our framework, we benefit from using more unlabeled data by
exploiting the relationships between objects, leveraging the semisupervised nature of graph-based regularization [20, 18, 2, 3]. In
addition, our framework involves a small parameter space independent of the number of target classes, and thus performs well even
with limited training data and thousands of target classes.

• Motivation: We recognize the need for heterogeneous pairwise features that complement each other, and confidenceaware regularization that distinguishes objects of different
initial classification confidence. (Sect. 1)
• Framework: We develop a generic graph-based regularization framework. The framework supports arbitrary heterogeneous pairwise features, is confidence-aware, and works with
a limited amount of training data. (Sect. 3)
• Applications: We showcase how our framework can be applied to a few applications in IR, primarily focusing on query
intent classification in the shopping domain. (Sect. 4)
• Experiments: We conduct extensive experiments on a realworld dataset for query intent classification. Our approach
substantially outperforms traditional regularization methods
which do not consider heterogeneous pairwise features or
confidence, validating our observations. (Sect. 5)

3. REGULARIZATION FRAMEWORK
In this section, we develop our graph regularization framework.
It is universal to different classification tasks, including our example application on query intent classification in Sect. 4.
Given a set of objects O and a set of classes C, each object o ∈ O
has some distribution over C. We regularize the class distribution
of o by the class distributions of objects similar to o, i.e., adjust
the prediction on o according to objects similar to it. To start, we
capture objects and their pairwise similarity features using a graph.
Next, based on the graph, we introduce our regularization model.

3.1 Object-Relationship Graph
We model objects and their pairwise features with a graph G =
(O, R). O is the set of objects that define the vertices. R is the
set of relationships between objects that describe the edges of the
graph, which are derived from the pairwise features between ob-

952

canon
sd1000

sd1000

scribe later, such a prior may be derived from an initial object-level
classifier or previous iterations of the regularization algorithm.

Target classes
(product categories)
digital-camera
Inkjet-printer
laptop
…

canon

3.2.1 Regularization by Neighbors
Given a prior over possible class distributions for an object o,
we can treat its neighbors as additional evidence to support its classification. In particular, we consider the neighbors as observation
data, weighted by their similarity, from which we can compute a
posterior distribution over possible class distributions for o. Alternatively, we can interpret this as a form of regularization in that the
original belief on o is regularized by the belief on its neighbors.
Let N (o) denote the set of neighbors of o on the graph. Specifically, for each neighbor o0 ∈ N (o), we can interpret its Dirichlet
prior Dir(αo0 ) as observing αo0 [i]−1 instances of class i for each
of the K classes when drawing σo0 instances from the underlying
multinomial distribution for o0 . Under the assumption that similar
objects share similar labels and hence observations, we deem that
each neighbor o0 contributes to the classification of o an observation
of S(o, o0 )(αo0 [i] − 1) counts for each of the K classes, weighted
by the similarity S(o, o0 ) between o and o0 . As o can be connected
to o0 via multiple pairwise features, we define the similarity between the two objects as a linear combination of their similarity
W (o, o0 , τ ) across all pairwise features:
X
S(o, o0 ) ,
λτ W (o, o0 , τ ),
(2)

hp 3 in 1
inkjet
printer

canon inkjet
printer
Relationship based on lexical pairwise feature
Relationship based on co-click pairwise feature

Figure 1: Example graph for classification of shopping queries.
jects. We use the toy graph in Fig. 1 for the task of query intent
classification in the shopping domain as our running example.
Vertices. Each object or the target entity of classification is a vertex
on the graph. For example, the query “canon” in Fig. 1 is a vertex
to be classified as a digital camera or inkjet printer, among others.
Edges. An edge on the graph is represented as a triplet (o, o0 , τ ) ∈
R, which models the relationship between two objects o and o0
based on a pairwise feature τ . Unlike traditional object-level features that are independently extracted from each object (e.g., words
in each document in text classification), τ is in fact a feature defined
on a pair of objects that describes their mutual similarity. Conceptually, τ can be understood as a particular similarity function on
two objects. For instance, as Fig. 1 illustrates, it can be the cosine
similarity between the word vectors of two queries (lexical pairwise feature), or the number of co-clicks for two queries (co-click
pairwise feature). Thus, there can exist as many edges between
two objects as the number of pairwise features, such as between
“canon” and “canon sd1000” in Fig. 1.
The strength of a relationship is encoded as the non-negative
weight of its corresponding edge (o, o0 , τ ), and is thus a function of the two objects o, o0 and the pairwise feature τ , denoted
as W (o, o0 , τ ). W (o, o0 , τ ) can be understood as the similarity between o and o0 with respect to τ . By convention, W (o, o0 , τ ) = 0
iff the edge (o, o0 , τ ) ∈
/ R. In this paper, we consider bi-directional,
symmetric relationships, which have equal strengths in both directions, i.e., W (o, o0 , τ ) = W (o0 , o, τ ).

τ

where each λτ is a parameter specifying the overall influence of
τ towards S(o, o0 ). We will learn these parameters in Sect. 3.3 to
optimize the regularization.
Since the Dirichlet distribution is conjugate to the multinomial
distribution, factoring in additional multinomial observations still
results in a Dirichlet posterior. Specifically, given a Dirichlet prior
Dir(α), the posterior distribution after observing β[i] counts in
each class i is simply Dir(α + β), where β = (β[1], . . . , β[K]).
Hence, for an object o with prior Dir(αo ), the additional weighted
observation data from each neighbor o0 would give us the Dirichlet
b o ), where:
posterior Dir(α
X
b o = αo +
α
S(o, o0 )(αo0 − 1)
(3)
o0 ∈N(o)

Similar to the prior confidence (Eq. 1), we can compute the posterior confidence for the object o as below:

3.2 Regularization Model
Given K target classes {1, . . . , K}, each object o ∈ O has an
underlying probability distribution over the K classes that captures
the possible user intents. E.g., in Fig. 1 the distribution for the query
“canon” could be (digital-camera:0.3, inkjet-printer:0.2, . . . ), indicating that the user may be looking for a digital camera with probability 0.3, an inkjet printer with probability 0.2, etc.
As such a distribution is inherently latent, we undertake a “distribution over distributions” instead. Specifically, we model the class
distribution of each object o using a Dirichlet prior Dir(αo ), parameterized by a vector αo = (αo [1], . . . , αo [K]). Informally,
it describes the distribution over all possible latent class distributions of o, given that each class i ∈ {1, . . . , K} has been observed
αo [i] − 1, potentially fractional, times. The Dirichlet prior also
conveniently allows us to treat the total count of observations σo as
the classification confidence—we are more confident in the prior if
σo is larger. We call σo the prior confidence of o:
σo ,

K
X
i=1

(αo [i] − 1) =

K
X

αo [i] − K

σ
bo =

K
X
i=1

b o [i] − K = σo +
α

X

S(o, o0 )σo0

(4)

o0 ∈N(o)

However, since all observations are potentially noisy in practice,
having more neighbors and hence more observation data does not
necessarily imply a higher posterior confidence. As a heuristic,
we normalize the observation data by the number
P of contributing
objects weighted by their similarity, So = 1 + o0 ∈N(o) S(o, o0 ).
e o ) and a norThis results in a normalized Dirichlet posterior Dir(α
malized posterior confidence σ
eo for each object o:


X
bo − 1
α
1
0
eo − 1 =
α
=
αo − 1 +
S(o, o )(αo0 − 1) (5)
So
So
0
o ∈N(o)



σ
bo
1 
σ
eo =
=
σo +
So
So

(1)

i=1

X

o0 ∈N(o)

0



S(o, o )σo0 

(6)

Intuitively, this normalization step causes observations from neighbors of o to “adjust” our confidence in o (Eq. 6), rather than to raise
it monotonically in the unnormalized version (Eq. 4).

In particular, we only consider unimodal Dirichlet priors, i.e.,
∀i ∈ {1, . . . , K}, αo [i] > 1, such that σo > 0. As we shall de-

953

3.2.2 Regularization by Indirectly Related Objects

We can understand Eq. 5 as a form of regularization. Informally,
our original belief on object o, as prescribed by its prior Dir(αo ),
is regularized by its neighbors, which change our belief and proe o ). Note that in the special
duce the regularized distribution Dir(α
case that o has no neighbors (N (o) = ∅), Eq. 5 would give us
e o = αo , i.e., no regularization is done.
α

In the above model (Eq. 5), an object o is only regularized by
its neighbors on the graph. In most scenarios, indirectly related
objects, such as “neighbors of neighbors”, also provide additional
evidence for o. The evidence from “neighbors of neighbors” can be
e o ) again. In general, we propose an
captured by regularizing Dir(α
iterative regularization algorithm, which can potentially model the
evidence from any object with a path to o on the graph.

Confidence-aware prediction. Given the regularized Dirichlet dise o ) for an object o, we can make a prediction on o
tribution Dir(α
by first choosing the most likely class distribution, i.e., the mode
e o of Dir(α
e o ). Subsequently, we assign classes to o
distribution m
e o , say, by taking the top k classes with the largest
according to m
e o , or all classes above a given probability threshprobabilities in m
e o [i] > 1, ∀i, the mode has a closed form solution:
old. Since α
e o = (α
e o − 1)/e
m
σo ,

e o)
Iterative regularization. As our regularized distribution Dir(α
is also Dirichlet, we can feed it back as input for regularization
again in the exact same way as we have done in Sect. 3.2.1, treate o ) as the new Dirichlet prior for o. The process can be
ing Dir(α
repeated for any number of iterations.
(0)
Let αo , αo denote the Dirichlet parameter of the initial prior
(t)
for an object o. Furthermore, let αo be the regularized Dirichlet
parameter after t iterations, ∀t > 0. Similar to Eq. 5, it is easy to
see that ∀t ≥ 1,




X
1
(t−1)
α(t−1)
−1+
S(o, o0 ) αo0
− 1  (12)
α(t)
o
o −1 =
So
0

(7)

where σ
eo is the normalized posterior confidence of o in Eq. 6.
Unlike previous regularization frameworks [20, 18, 2, 3], the
e o is confidence-aware, as we will now demonstrate.
prediction by m
Dividing both sides of Eq. 5 by σ
eo , we obtain:
P
αo − 1 + o0 ∈N(o) S(o, o0 )(αo0 − 1)
eo − 1
α
=
(8)
σ
eo
σ
eo So

o ∈N(o)

As shown, we only need to know the Dirichlet parameters from
(t)
the previous iteration to compute αo . To start the iterative pro(0)
cess, we bootstrap from t = 0 by specifying αo , ∀o, for the initial prior of each object. How they are initialized is an orthogonal
issue to our regularization framework. Different strategies may be
applied based on the application, as we will discuss in Sect. 4.
Finally, we make predictions using the mode distributions after
(t)
(t)
(t)
t iterations of regularization, mo = (αo − 1)/σo , in the same
manner as discussed in Sect. 3.2.1.

e o . On the right hand side,
In Eq. 8, the left hand side is simply m
αo − 1 = σo mo , ∀o ∈ O, where mo is the mode distribution of
Dir(αo ). Moreover, taking Eq. 6 into consideration, Eq. 8 can be
further expressed as:
P
σo mo + o0 ∈N(o) S(o, o0 )σo0 mo0
P
eo =
(9)
m
σo + o0 ∈N(o) S(o, o0 )σo0

Similar to the harmonic method [20], Eq. 9 implies that the rege o is a weighted average of the original (unregularularized mode m
ized) modes mo and mo0 , ∀o0 ∈ N (o). However, unlike [20], the
weight on each neighbor o0 is S(o, o0 )σo0 , which factors in both
the similarity S(o, o0 ) and the confidence σo0 . Hence, our model is
confidence-aware, i.e., objects with higher confidence σo0 will be
weighed more in the regularization. As motivated in Sect. 1, such
a property is desirable, since an object with low confidence would
potentially introduce noise and thus its influence should be downplayed. On the other hand, [20] and other regularization models
[18, 2, 3] only assign weights according to the similarity between
the objects, without utilizing the confidence.

Number of iterations. The iterative regularization process can be
interpreted as a form of information propagation on the graph. In
each iteration, information is propagated from each object to its
neighbors. The propagation is repeated such that indirectly related
objects are also affected through longer-range dependencies.
The total number of iterations T is an important parameter to decide, with similar effects as the length parameter in random walks
[17, 7]. In particular, when T = 1, it reduces to a simple weighted
(T )
voting by neighbors. But as T → ∞, αo becomes the same for
every object o in each connected component of the graph, similar
to what have been discussed in [17, 7].
Intuitively, if T is too small, only very short-range dependencies
can be captured, ignoring potentially useful long-range ones. On
the other hand, if T is too large, all dependencies regardless of its
range will be captured, making the result less discriminative. Thus,
an optimal T would ensure a desirable trade-off between short and
long-range dependencies. While the optimal T can be selected either manually [7] or heuristically [17], we propose to learn it in the
next subsection.

Alternative interpretation. Our regularization (Eq. 5) is equivalent to the minimization of a cost function, the essence of many
graph-based regularization frameworks [20, 18, 2, 3]. Specifically,
e o , ∀o ∈ O:
we minimize the following function over α


X
1 X
2
0
2
e o − αo k +
e o − αo0 k
E=
, (10)
kα
S(o, o )kα
2 o∈O
0
o ∈N(o)

where k · k is the Euclidean distance. Intuitively, the regularization
should not change the original belief on o too much, i.e., we want
e o ) to be close to the prior Dir(αo ). Additionally, Dir(α
e o)
Dir(α
should be close to the prior Dir(αo0 ) of each neighbor o0 ∈ N (o),
from which observations are contributed towards o.
To see the equivalence, we minimize E by setting its derivative
e o to zero:
with respect to α
∂E
e o − αo +
=α
eo
∂α

X

o0 ∈N(o)

e o − αo0 ) = 0,
S(o, o0 )(α

3.3 Parameters Learning
In addition to the total number of iterations T , each pairwise
feature τ has a parameter λτ that specifies the overall influence of
τ towards the similarity between two objects (Eq. 2). Thus, our
parameters are T and Λ, where Λ , {λτ : ∀τ }.
We propose to learn the parameters with a heuristic objective
function defined on only T and Λ. Since |Λ| is generally small as
restricted by the number of possible pairwise features between objects (in our experiments |Λ| = 2), only a few labeled objects are
needed for learning. In comparison, traditional supervised classification techniques train a model for each target class. In real-world

(11)

which is algebraically equivalent to Eq. 5.

954

applications where the number of target classes is large (K = 2043
in our experiments), an enormous amount of training data would be
required, as we would expect at least a few labels for each class. On
the other hand, with our heuristic objective function, we require far
fewer than K to achieve good classification accuracy.
Our first attempt defines a global error function to optimize the
parameters. However, due to computational challenges, we subsequently propose a local error function for the dynamic selection
of parameters in each iteration. As the parameters are chosen to
minimize the error functions, any existing numerical optimization
technique can be used. In particular, we apply Powell’s method
[14] to solve the optimization problem.

Although in the regularization step, we still need to update the
entire graph, it is acceptable as it is done only once per iteration.
In contrast, to solve the optimization problem in each minimization step, the error function has to be computed numerous times.
Thus it is important the error function involves only light-weight
computation. Our local error function (Eq. 14) is defined using the
global error function (Eq. 13) with T = 1, which can be easily
computed using only the neighbors of the objects in the training set
OL , where typically |OL |  |O|. E.g., in most of our experiments,
|OL | is less than 0.1% of |O|.
Another distinction in our iterative optimization is that we do
not explicitly learn T , the maximum number of iterations. Instead, we terminate the iterations when the minimum local error
(t)
minΛ Lerr (·) converges. Although such convergence is guaranteed
as established below, similar to EM, there is no guarantee that the
global minimum minΛ,T Gerr (·) can be achieved.

3.3.1 Global Optimization
Given a labeled training set OL ⊂ O, and an initialization for
(0)
the Dirichlet parameters Ω = {αo : o ∈ O} at t = 0, we can
define an error function as the minimization objective:
2
1 X
|Ω,Λ)
Gerr (OL , Ω, Λ, T ) =
m(T
− u∗o ,
(13)
o
|OL | o∈O

Proposition 1: As defined in Eq. 14, the sequence of local mini(t)
mum errors {minΛ Lerr (OL , Λ)}∞
t=1 converges to a finite limit.

L

P ROOF : Let Λ0 denote that all parameters λτ are zero. Then,

(T |Ω,Λ)

where mo
denotes the mode after T iterations of regularization (using Eq. 12), with the initialization Ω and parameters
Λ. u∗o is the “gold standard” distribution derived from the labels
of o. k · k is the Euclidean distance. (Note that alternative distance measures such as KL-divergence may also be used.) We select parameters that minimize this global error, i.e., {Λ∗ , T ∗ } =
arg minΛ,T Gerr (OL , Ω, Λ, T ).

∗

(t)
(t−1)
min L(t)
(OL , Λ).
err (OL , Λ) ≤ Lerr (OL , Λ0 ) = min Lerr
Λ

Λ

Note that (∗) holds since Λ0 implies that no regularization is
done—we get the same error as the minimum error of the previous
iteration. Thus, the sequence is monotonically non-increasing. Additionally, the minimum error is bounded from below by zero. Any
bounded monotone sequence converges to a finite limit.

(T |Ω,Λ)

When T = 1, mo
can be computed for all o ∈ OL using
only the neighbors of objects in the labeled training set OL . When
T = 2, we need to consider “neighbors of neighbors” as well. For
larger T ’s, we potentially need to compute iteratively over the entire graph of labeled and unlabeled objects. This makes the computation of the error function very costly. As many optimization techniques require repeated computation of Gerr (·) for different values
of Λ and T , such an error function is computationally infeasible.

4. APPLICATIONS IN IR
We now discuss the applications of the regularization framework
on real-world classification tasks in information retrieval. We first
formalize what components of the regularization framework must
be realized in any application. Next, we showcase a specific application on query intent classification in the shopping domain, followed by brief discussions of applications on other tasks. Finally,
we describe the target scenarios of our applications.

3.3.2 Iterative Optimization
As an efficient alternative to minimizing the global error, we
propose an algorithm where we regularize the Dirichlet parameters
(t)
αo (Eq. 12) and update the parameters Λ in alternating fashion. In
each iteration, we regularize the classification using the Λ learned
from the previous iteration. As the Dirichlet distribution on every
object changes after regularization, we update Λ by minimizing the
error function for the next iteration. This is similar in spirit to the
Expectation-Maximization (EM) algorithm. However, unlike EM
which maximizes the likelihood function of the parameters, we are
minimizing an error function. Our approach involves the following
two steps in each iteration t, ∀t ≥ 0:

4.1 Realization of the Framework
Realizing our regularization framework requires a vertex model
and one or more edge models.
Vertex model. As we see in Eq. 12, to enable iterative regularization, we need an initial model for t = 0, characterized by the
(0)
Dirichlet parameters αo , ∀o ∈ O. Since each object o is a vertex in the graph, we call this initialization a vertex model. As
(0)
(0)
(0)
αo = σo mo + 1, which entails both the mode distribution
(0)
and the confidence, we can equivalently set αo by initializing
(0)
(0)
mo and σo separately, as we do in Sect. 4.2.

(0)

(1) Regularization step. If t = 0, initialize αo , ∀o ∈ O, ac(0)
cording to the given Ω(0) = {αo : ∀o ∈ O}. If t > 0,
(t)
(t)
compute Ω = {αo : ∀o ∈ O} from Ω(t−1) (Eq. 12), using
parameters Λ(t) . Note that Ω(t−1) and Λ(t) are already computed from the previous iteration.

Edge model. In Sect. 3, the strength of a relationship between
two objects o, o0 is abstracted into a weight function W (o, o0 , τ ),
where τ is the pairwise feature that induces the relationship. Since
relationships are edges on the graph, we call the realization of the
weight function W (q, q 0 , τ ) for a pairwise feature τ an edge model.

(2) Minimization step. To update Λ for the next iteration, we
minimize a local error function over Λ:

4.2 Example Applications

L(t+1)
(OL , Λ) , Gerr (OL , Ω(t) , Λ, 1),
err

4.2.1 Query Intent Classification

(14)

We address the problem of query intent classification in the shopping domain. Given a query q ∈ Q from a query log of an ecommerce website, and a set of K predefined product categories,
the task is to map the query to a product category, with a limited

which is the global error for T = 1 with initialization Ω(t) . We
(t+1)
find Λ(t+1) = arg minΛ Lerr (OL , Λ) for the next iteration.
We continue the iteration until the minimum error converges.

955

Lexical Edge Model. Given two queries q and q 0 from a query log,
we can view each as a set of words, denoted by φ(q) and φ(q 0 ),
respectively. We can then define the lexical edge model as follows:

1 φ(q) ⊆ φ(q 0 ) or φ(q 0 ) ⊆ φ(q)
W (q, q 0 , τlex ) =
(16)
0 else

amount of labeled queries. Additional resources such as existing
product metadata may also be leveraged. In the following, we
present how we can realize the vertex and edge models for this
task. Note that the set of queries Q in the given query log form the
vertices on the graph instead of the generic objects O in Sect. 3.
Vertex models. Any conventional query classification algorithm
(0)
can be used to initialize the mode mq for each query q ∈ Q, as
long as the output of the algorithm can be converted to a probability
distribution over the K categories, which is taken as the most likely
(0)
distribution, i.e., the mode distribution mq . We first introduce
a vertex model based on unigram language models, followed by
possible alternatives.

This edge model is a symmetric, binary similarity measure: we
draw an edge between two queries if one of them contains all words
in the other. E.g., W (“canon”, “canon camera”, τlex ) = 1, whereas
W (“canon printer”, “canon camera”, τlex ) = 0.
Although this edge model is simple, it is more effective in preliminary experiments than other more sophisticated models such as
cosine similarity. One possible explanation is that our binary similarity results in less noise than cosine similarity.

Language Vertex Model. As input, we leverage existing product
metadata from online shopping sites such as Bing Shopping1 . Each
product is associated with metadata that include attributes such as
name, brand and description, as well as the category to which the
product belongs. From this data, we build a unigram language
model θi for each category i ∈ {1, . . . , K} based on all observed
words from the attribute values of the products in that category,
(0)
weighted by product popularity. Given a query q, to estimate mq ,
we evaluate the query likelihood p(q|θi ), ∀i, and convert it to a
probability distribution over categories by applying Bayes’ rule:
p(θi |q) ∝ p(q|θi )p(θi ), using the prior of the categories as estimated by the total popularity of all products in each category.
On the other hand, to initialize the confidence, we build a background model θb by considering all products in our metadata. The
intuition is that queries with a lower background model likelihood
are easier to classify, as a low likelihood means that the words in
the query are rare and more likely to be identified as belonging
to those few categories that contain these rare words, resulting in
higher classification confidence. On the other hand, a high likelihood implies that the query words are very common and appear in
many categories, leading to lower confidence classification. Empirical study shows that the probability of a correct classification of a
query is fairly correlated with its negative log likelihood. Thus, we
initialize the confidence as follows:
σq(0) = − log p(q|θb )

Co-click Edge Model. We can also establish relationships between
queries using co-click pairwise features. Intuitively, if two queries
have more clicks that land on product pages belonging to the same
category, they are more closely related. In many cases, we may
only be able to deduce that two clicks lead to the same category
(e.g., they land on the same URL, or URLs with the same prefix
such as example.com/digital_camera/...), but we may not know the
actual category, or how to map the vendor category to ours. Formally, we define the co-click edge model as follows, adapted from
the co-citation/click measures in [19, 10]:
!
X
N
0
0
W (q, q , τclick ) = log 1 +
#(q, c)#(q , c)
(17)
Nc
c

In this edge model, each c is a category, which could be a “virtual
category” with no direct correspondence to a target category, for
reasons discussed above. ∀q ∈ Q, #(q, c) denotes the number of
clicks associated with query q that leads to category c. N is the
total number of clicks across all queries, whereas Nc is the number
of total clicks that lead to category c. Thus, N/Nc has an effect
similar to inverse document frequency, i.e., more popular categories
contribute less to the similarity between two queries. We also use
a logarithm function to model a sublinear growth of the similarity
with respect to the number of clicks.

(15)

Additional Edge Models. The following pairwise features can also
be considered, although they are not used in our experiments.

The language vertex model only requires a weak supervision
from the product metadata, which is available from shopping websites. No training labels for individual queries are needed.

• User sessions. In the same user session, the second query
is often a refinement of the first. Thus, whether two queries
co-occur in the same session and the frequency of such cooccurrence can be another pairwise feature.

Alternative Vertex Models. To initialize the mode distribution, we
can also use supervised but substantially more accurate classifiers.
The disadvantage of using a supervised classifier is that it requires
a large amount of labeled training queries to achieve good classification accuracy. However, after applying our regularization framework, using the weakly supervised unigram models can achieve
comparable results to using a well-trained fully supervised classifier for the vertex model, as we shall observe in the experiments.
Eq. 15 is only a simple heuristic for the purpose of initializing the
distribution confidence. Other heuristics can be used, such as the
query length (longer queries provide additional features to guide
its classification), or the consistency of the outputs from multiple
classifiers (queries with more consistent outputs are arguably easier to classify). However, developing more theoretical confidence
estimation methods, such as those explored by works on query difficulty [6], is beyond the scope of this paper.

• Search results. On an existing e-commerce system, a query
can retrieve a set of related products (i.e., the search results).
A pairwise feature between two queries could be the similarity of their search results. Alternatively, search results from
a generic search engine can also be used, where similarity
between retrieved pages can be measured instead.

4.2.2 Other Applications
We briefly discuss how we can realize our regularization framework on other applications in IR. As discussed in Sect. 4.2.1, the
vertex model is often easy to realize using an existing classifier for
the initial mode distribution and various heuristics for the initial
confidence. Hence, for the following applications, we only discuss
the choice of the pairwise features for their edge models, which are
often neglected in conventional classification.

Edge models. Among many possible pairwise features between
queries, we focus on the lexical (τlex ) and co-click (τclick ) features.
1

Offer classification. Given a product offer from a vendor, which
is presented as a list of attribute-value pairs (e.g., product name,
brand, image, description, as well as product-specific attributes such

http://www.bing.com/shopping

956

(a) “canon 35”
canon 35 mm lens
canon 35 f 2
35 mm wide angle 1.4 canon lens

as focal length for cameras), the task is to map it to our predefined
categories. (Imagine we are an e-commerce website receiving feeds
on offers from different vendors, whose attribute schemas and category taxonomies vary.) The following pairwise features could be
used: (i) the graphical similarity between two product images; (ii)
the overlap of product-specific attribute names, since products from
the same category often share similar attribute names even across
vendors; (iii) lexical similarity between the names and descriptions
of two products.

Figure 2: Examples of (a) lexical and (b) co-click neighbors.

5.1 Experimental Setting
Dataset. We obtain a hierarchy of product categories from Bing
Shopping. As our regularization framework targets generic classification problems, which may not have hierarchical categories as
labels, we use only the most detailed 2043 leaf categories (i.e.,
K = 2043), instead of taking advantage of the category taxonomy.
Thus, our task is to predict the leaf category for each query.
In our experiments, we use a query log containing four million
distinct queries. Some of these queries have been labeled by human
judges, which we split into two disjoint subsets for training and
testing. Specifically, the training set consists of 10K labeled queries
for parameter learning, and the test set consists of over 10K labeled
queries for evaluation. Although we reserve 10K labeled queries
for training, we only use 1K of them in all experiments except in
Sect. 5.2.4, where we vary the number of training queries.
We also use clickthrough data to build the co-click edge model.
There are about 11 million clicks associated with about 1/4 of all
queries. The fact that most queries do not have any click motivates
the necessity of heterogeneous pairwise features.

Text classification. This is a classic task in IR that assigns each
document to a class. With additional resources such as access
statistics, social media and embedded hyperlinks, we suggest the
following pairwise features: (i) co-readership, the number of readers who accessed both of the two documents within some time window; (ii) social tagging, the distributional similarity of the user tags
on social sharing sites; (iii) hyperlinks in the documents, where
similar documents likely point to similar pages; (iv) lexical similarity in the contents of two documents.

4.3 Target Application Scenarios
As our regularization framework requires multiple iterations over
the entire graph G = (O, E), it is not intended for online classification. Instead, it is designed for offline computation, where the
precomputed predictions for each object in O is stored in an index.
Given a seen object o (i.e., o ∈ O), its precomputed prediction
is directly fetched from the index. If o is previously unseen (i.e.,
o∈
/ O), we can use the vertex model to classify it on-the-fly. Alternatively, we may potentially look up o’s neighbors on the graph
using an inverted index, locality sensitive hash, or other means, and
subsequently regularize the classification of o using its neighbors
for one iteration online.
As an example, let us consider a real scenario involving a live
search system on the task of query intent classification. In this
situation, as time passes, the live system collects a growing amount
of data, including queries and clickthroughs. Thus, it is ideal if our
regularization framework can be applied periodically offline (say,
on a daily or weekly basis), in order to leverage more data and
update the precomputed index accordingly. As our experiments
show (Sect. 5.3), our approach runs fast enough on a single machine
to be computed daily for at least tens of millions of queries.
Alternatively, we can apply our framework and take objects with
highly confident predictions as labeled training data. Using these
automatically generated training data, we can iteratively train a supervised classifier, extending the approach described in [12].

5.

(b) “hp laptop hard drive”
hard drive 1tb
seagate harddrive
western digital 2tb external

Vertex model. Unless otherwise stated, we use the language vertex
model (Sect. 4.2.1) in all experiments, which is weakly supervised
and requires no labeled training queries. However, to demonstrate
the robustness of our framework, we also compare it with an alternative vertex model in Sect. 5.2.2.
Evaluation metrics. For each query, we sort the categories according to their predicted probabilities, from which we can apply
the following metrics.
• Top-k accuracy: the fraction of test queries which contain a
correct result within the top k hypothesized categories;
• Precision-recall plot: a plot of precision against recall while
varying the prediction cut-off threshold.
• Optimal f-score: the best f-score achievable among all points
in the precision-recall plot.
• Precision @ 0.5 recall: precision at a recall level of 0.5.
In practical scenarios, higher precision is usually more important than higher recall—we would rather not predict than
suggest a wrong category.

EXPERIMENTS

5.2 Accuracy Study

To analyze the performance of our regularization framework,
we conduct extensive experiments on our showcase application of
query intent classification, using a large scale real-world dataset.
The objective of our experiments is to validate that the proposed
framework can utilize heterogeneous pairwise features as well as
classification confidence to improve the performance, especially in
a scenario with limited training data. This objective cannot be accomplished by comparing our approach with existing state-of-theart techniques for query intent classification. Instead, we choose
different schemes of our framework as the baselines (e.g., using the
same framework, but we assume only one kind of pairwise feature
or uniform confidence). Note that in each iteration, our framework
makes a similar form of updating on the predictions (Eq. 9) as the
well-known graph regularization method Harmonic [20]. Hence,
different schemes of our framework are well represented baselines
for existing graph regularization techniques.

5.2.1 Case Study
We first present some illustrative results, providing insights on
why our approach works.
Query 1: “canon 35”. This query is misclassified as camcorder
by the unigram language model, since both words in the query are
not sufficiently discriminative. However, its lexical neighbors reveal some interesting information, as illustrated in Fig. 2(a). Given
these neighbors, it is not surprising that our approach can correctly
classify this query as camera-lens.
Query 2: “hp laptop hard drive”. This query is misclassified as
laptop by the unigram model, since the words “hp laptop” strongly
suggest laptops. This time, its co-click neighbors, as shown in
Fig. 2(b), allow our approach to correctly classify it as hard-drive.

957

1.0

Algorithm
Unigram
Reg-Lex
Reg-Click
Our approach

0.9
0.8

Precision

0.7

Top-3 accuracy
0.663
0.735 (+10.8%)
0.731 (+10.1%)
0.763 (+15.0%)

Optimal f-score
0.564
0.660 (+17.1%)
0.659 (+16.9%)
0.728 (+29.2%)

Prec @ 0.5 recall
0.465
0.676 (+45.4%)
0.712 (+53.3%)
0.821 (+76.7%)

Figure 4: Accuracy comparison.

0.6

(a) Queries with clicks

0.5
0.4

Unigram

0.3

Reg-Lex

Algorithm
Unigram
Reg-Lex
Reg-Click
Our approach

Reg-Click

0.2

Our approach

Top-3 accuracy
0.721
0.795 (+10.2%)
0.832 (+15.3%)
0.836 (+15.9%)

Optimal f-score
0.609
0.704 (+15.6%)
0.798 (+30.9%)
0.801 (+31.5%)

Prec @ 0.5 recall
0.593
0.780 (+31.6%)
0.920 (+55.1%)
0.925 (+56.0%)

0.1
0.1

0.2

0.3

0.4

0.5

0.6

(b) Queries without clicks

0.7

Recall

Algorithm
Unigram
Reg-Lex
Reg-Click
Our approach

Figure 3: Precision-recall plot.

5.2.2 Effect of Heterogeneous Pairwise Features

Top-3 accuracy
0.573
0.641 (+11.9%)
0.573 (+0.0%)
0.648 (+13.1%)

Optimal f-score
0.495
0.583 (+17.9%)
0.495 (+0.0%)
0.592 (+19.7%)

Prec @ 0.5 recall
0.238
0.445 (+87.2%)
0.238 (+0.0%)
0.483 (+102.9%)

We compare our approach to the vertex model and regularization
baselines that only consider one kind of pairwise feature.

Figure 5: Accuracy comparison for queries with/without clicks.

• Unigram: The unigram model used in our vertex model.

fluence and improve the performance of more queries via heterogeneous long-range dependencies.

• Reg-Lex: The same framework as ours, but with only the
lexical pairwise features (i.e., the lexical edge model).

Alternative vertex model. To demonstrate the robustness of our
framework, we also evaluate our approach with an alternative regression vertex model. Specifically, we initialize the modes using
a supervised logistic regression model (Regression) trained from
a large number of labeled queries, with query words as features.
To initialize the confidence, we use the query length as a heuristic,
as empirical study suggests a correlation between longer queries
and better classification accuracy of the regression model—longer
queries contain more word features to guide the classification.
The results of using the regression vertex model are shown in
Fig. 6. Similar to using the language vertex model as reported in
Fig. 4, our approach outperforms the baselines by a clear margin,
even though the improvements are not as substantial. We expect
such results, since the regression model is a much stronger model
than the unigram model, and is thus harder to improve upon. This
is analogous to ensemble methods in machine learning, where it is
generally easier to improve over weak learners than strong learners. Interestingly, despite initializing from a much stronger vertex
model, the final results from regularizing the regression model is
only slightly better than those from regularizing the unigram model.
Thus, subsequent experiments use the language vertex model, which
enjoys the additional benefit of being weakly supervised.

• Reg-Click: The same as Reg-Lex but with co-click pairwise
features instead (i.e., the co-click edge model).
Overall results. The results are reported in Fig. 3 and 4. While
all regularization methods generally outperform the unigram model
especially at high recall levels, we make two further observations.
First, neither Reg-Lex nor Reg-Click clearly dominates the other,
which only uses lexical and co-click features, respectively. Normally we would expect Reg-Click to perform much better than
Reg-Lex, since the former uses co-click pairwise features, a form
of user feedback that may be more indicative than pairwise lexical features. For instance, many queries may be lexically similar
yet unrelated (e.g., “laptop” and “laptop bag”), or related but exhibit no lexical similarity (e.g., “hp 3 in 1” and “canon printer”).
Instead, our results can be explained by the sparsity of the clickthrough data, where only 1/4 of the queries have received at least
one click, but almost all queries have lexically similar neighbors.
Second, by using both pairwise features, our approach performs
substantially better than Reg-Lex and Reg-Click. One reason is that
a single kind of pairwise feature cannot cover all queries. A second
reason is that the combined evidence of both pairwise features is
more informative than any single one alone, thus directly improving queries that have both features and indirectly improving queries
that connect to neighbors with both features.

5.2.3 Effect of Confidence
Next, we investigate the effect of the confidence initialization on
regularization. Specifically, we compare our heuristic method in
Eq. 15 (Heuristic) with the following two strategies.
We call the first strategy NoConf, which uses the same framework as ours, except that no confidence information is applied. In
other words, uniform confidence is used to initialize each query.
We label the second strategy Simulated, which also uses the
same framework as ours. However, we initialize the confidence
differently, since our confidence estimation (Eq. 15) is only a simple heuristic. To minimize the adverse effect of weak confidence

Result breakdown. We further analyze the performance on two
subsets of queries—queries with clicks and ones without—as shown
in Fig. 5. As expected, Reg-Click performs well for queries with
clicks, but has no effect on queries without clicks. On the other
hand, Reg-Lex improves the accuracy for both subsets of queries. It
is not surprising that our approach outperforms both Reg-Click and
Reg-Lex for queries with clicks, since in this subset, both pairwise
features improve the classification accuracy. However, we also outperforms all baselines for queries without clicks, although using
co-click pairwise features alone has no effect on this subset. In
this case, because classification evidence is iteratively propagated
across all edge models in our approach, improvements to queries
with clicks will influence their lexical neighbors, not all of which
may have clicks. By combining heterogeneous pairwise features,
we can extend the reach of each individual pairwise feature to in-

Algorithm
Regression
Reg-Lex
Reg-Click
Our approach

Top-3 accuracy
0.719
0.746 (+3.9%)
0.773 (+7.5%)
0.784 (+9.1%)

Optimal f-score
0.680
0.697 (+2.5%)
0.722 (+6.2%)
0.729 (+7.3%)

Prec @ 0.5 recall
0.739
0.773 (+4.6%)
0.809 (+9.5%)
0.823 (+11.4%)

Figure 6: Accuracy comparison with regression vertex model.

958

(b) Optimal f-score

(c) Prec @ 0.5 recall

(a) Training queries

0.850

0.9

0.66

0.74

0.835

0.8

0.65

0.73

0.820

0.7

0.64

0.72

0.805

0.63

0.71

Metrics

0.75

0.8

0.6
Top-3 accuracy
Optimal F-score
Prec @ 0.5 recall

0.5

0.790

(b) Total queries
0.9

Metrics

(a) Top-1 accuracy
0.67

Top-3 accuracy

0.6

Optimal F-score
Prec @ 0.5 recall

0.4

0.5
1%
0%

Unigram 10
100
1000 10000
Number of training queries used

Figure 7: Accuracy with different confidence initializations.

0.7

20%

40%

60%

80%

100%

Fraction of all queries used

Figure 8: Accuracy versus amount of training/total queries.

estimation, we generate more reliable confidence values via a simulation. Specifically, for each labeled query, we sample a random
value x from two Gaussian distributions. If the top-1 category predicted by the unigram model is correct, then x ∼ N (0.8, 0.1);
otherwise, x ∼ N (0.2, 0.1). Subsequently, we initialize the confidence of the labeled query with x, clipped to the range [0, 1]. For
each unlabeled query, we initialize its confidence to 0.512, which
is the top-1 accuracy of the unigram model on the test queries.
The results of the different confidence initialization strategies are
presented in Fig. 7. Although Heuristic only uses a simple heuristic (Eq. 15) to initialize the confidence, it performs better than NoConf, which is not confidence-aware. On the other hand, Simulated
performs the best, since it initializes confidence in a more reliable
way. Thus, with more sophisticated confidence estimation, we can
expect even better results from our framework.

(b) Our approach
0.85

0.75

0.75
Metrics

Metrics

(a) Reg-Click
0.85

0.65
Top-3 accuracy
Optimal F-score
Prec @ 0.5 recall

0.55
0.45

0.65
Top-3 accuracy
Optimal F-score
Prec @ 0.5 recall

0.55
0.45

0%

20% 40% 60% 80% 100%
Fraction of clickthroughs used

0%

20% 40% 60% 80% 100%
Fraction of clickthroughs used

Figure 9: Accuracy versus the amount of clickthroughs.
most of the improvement is achieved when the amount of queries
is initially increased from 1% to 20%, further increases result in
smaller but continual improvements. From this trend, we expect
that the accuracy can be further improved, but to a lesser extent,
with an even larger query set.

5.2.4 Effect of Varying Amount of Data
In the following experiments, we vary the amount of data in three
aspects: the number of labeled training queries, total queries, and
clickthroughs. It is important to study the effects of varying the
amount of data, since in practical scenarios, data is collected from
a live system gradually.

Number of clickthroughs. Finally, we investigate the effect of
varying the number of clickthroughs. Specifically, we randomly
sample between 0% and 100% of clickthroughs from our query
log, while using all queries and the same set of training queries.
Fig. 9(a) and (b) illustrate the effect the amount of clickthroughs
has on Reg-Click and our approach2 , respectively. In both methods,
using a larger number of clickthroughs improves accuracy. Similar
to the effect of increasing the number of total queries, the improvements diminish as more clickthroughs are used. Nevertheless, the
improvements have not converged, suggesting that additional clickthrough data will further increase performance.

Number of training queries. Recall from Sect. 3.3, we require
some labeled queries as training data in order to learn the parameters Λ. We examine the number of training queries needed to reach
convergence on accuracy. Varying from 10 to 10K training queries,
we record the corresponding performance in Fig. 8(a). The results
reveal that with merely 10 training queries, we can already achieve
a substantial improvement over the unigram baseline. Furthermore,
with as few as 100 training queries, the performance has nearly
converged. Considering that 100 is much smaller than the 2043
categories in this classification task, we demonstrate that our model
requires very few training queries indeed. This is not surprising as
we are not directly modeling the categories, which would require
2000+ training queries at the minimum, one for each category. Instead, we use the training queries only to learn Λ, the weights of
pairwise features. Coupled with the weakly supervised language
vertex model, our approach reaches convergence with very few
training queries in total.

5.3 Efficiency Study
Although our regularization framework is designed to be applied
offline periodically (Sect. 4.3), efficiency is still important. For
instance, to apply it daily, it shall not take longer than a day. The
following experiments evaluate the efficiency of our approach.

5.3.1 Rate of Convergence
As our approach is iterative, the number of iterations required to
achieve convergence is crucial to efficiency. Our experiments show
that convergence is fast, with 2 to 4 iterations in most settings. In
particular, when using all available data, Fig. 10(a) illustrates the
convergence of classification accuracy with a precision-recall plot.
Iteration-0 corresponds to the initial vertex model. Regularization
in Iteration-1 results in a larger improvement, whereas Iteration2 shows smaller improvement. There is virtually no improvement
from Iteration-3 (not shown in the figure), which indicates that the

Number of total queries. The majority of our queries are unlabeled and not directly used for parameter learning. However, as we
increase the number of unlabeled queries, we also improve the connectivity of the graph. To evaluate the effect, we randomly sample
a subset of all queries, and apply our approach using this subset as
the total set of queries. The sample size varies between 1% and
100% of all queries. To eliminate any differences due to labeled
training queries, our samples include the same set of 1K training
queries throughout. The results are presented in Fig. 8(b). Despite
using the same set of training queries, the accuracy of our approach
increases as the amount of total queries used increases. Although

2
The number of clickthroughs does not affect the performance of
Reg-Lex, which only utilizes lexical pairwise features.

959

(a) Precision-recall plot

(b) Minimum error

Minimum error

0.7
Precision

the shopping domain. Finally, we conducted extensive experiments
on a large-scale real-world dataset to verify our observations. With
limited training data, our approach effectively leverages heterogeneous pairwise features and classification confidence.

0.75

0.9

0.5

0.3

Iteration-0
Iteration-1
Iteration-2

0.1
0.1

0.3

0.65

7. REFERENCES
0.55

[1] S. Beitzel, E. Jensen, O. Frieder, D. Lewis, A. Chowdhury,
and A. Kolcz. Improving automatic query classification via
semi-supervised learning. In ICDM, pages 42–49, 2005.
[2] M. Belkin, I. Matveeva, and P. Niyogi. Regularization and
semi-supervised learning on large graphs. Learning Theory,
pages 624–638, 2004.
[3] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold
regularization: A geometric framework for learning from
labeled and unlabeled examples. J. of Machine Learning
Research, 7:2399–2434, 2006.
[4] A. Broder, M. Fontoura, E. Gabrilovich, A. Joshi,
V. Josifovski, and T. Zhang. Robust classification of rare
queries using web knowledge. In SIGIR, pages 231–238,
2007.
[5] H. Cao, D. Hu, D. Shen, D. Jiang, J. Sun, E. Chen, and
Q. Yang. Context-aware query classification. In SIGIR, pages
3–10, 2009.
[6] D. Carmel and E. Yom-Tov. Estimating the query difficulty
for information retrieval. Synthesis Lectures on Information
Concepts, Retrieval, and Services, 2(1):1–89, 2010.
[7] N. Craswell and M. Szummer. Random walks on the click
graph. In SIGIR, pages 239–246, 2007.
[8] F. Diaz. Regularizing query-based retrieval scores.
Information Retrieval, 10(6):531–562, 2007.
[9] F. Diaz. Improving relevance feedback in language modeling
with score regularization. In SIGIR, pages 807–808, 2008.
[10] X. He and P. Jhala. Regularized query classification using
search click information. Pattern Recognition,
41(7):2283–2288, 2008.
[11] M. Ji, J. Yan, S. Gu, J. Han, X. He, W. Zhang, and Z. Chen.
Learning search tasks in queries and web pages via graph
regularization. In SIGIR, pages 55–64, 2011.
[12] X. Li, Y. Wang, and A. Acero. Learning query intent from
regularized click graphs. In SIGIR, pages 339–346, 2008.
[13] X. Li, Y. Wang, and A. Acero. Extracting structured
information from user queries with semi-supervised
conditional random fields. In SIGIR, pages 572–579, 2009.
[14] W. Press, S. Teukolsky, W. Vetterling, and B. Flannery.
Numerical Recipes. Cambridge Univ. Press, 3rd edition,
2007.
[15] D. Shen, Y. Li, X. Li, and D. Zhou. Product query
classification. In CIKM, pages 741–750, 2009.
[16] D. Shen, J. Sun, Q. Yang, and Z. Chen. Building bridges for
web query classification. In SIGIR, pages 131–138, 2006.
[17] M. Szummer and T. Jaakkola. Partially labeled classification
with markov random walks. In NIPS, volume 14, pages
945–952, 2001.
[18] D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Schölkopf.
Learning with local and global consistency. In NIPS, pages
595–602, 2004.
[19] D. Zhou, B. Schölkopf, and T. Hofmann. Semi-supervised
learning on directed graphs. NIPS, 17:1633–1640, 2005.
[20] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised
learning using Gaussian fields and harmonic functions. In
ICML, pages 912–919, 2003.

0.45
0.5
Recall

0

0.7

1

2

3

Iterations

Figure 10: Convergence of (a) accuracy; (b) minimum error.

Running time (minute)

20

15

10

5

0
20%

40%

60%

80%

100%

Fraction of all queries used

Figure 11: Execution time versus amount of total queries.
accuracy has converged. From another perspective, Fig. 10(b) illustrates the sequence of minimum errors in each minimization step
(Eq. 14), which converges after only 3 iterations.

5.3.2 Execution Time
We start with a simple complexity analysis. As we scan all edges
and vertices of the graph once in each iteration, the complexity is
O((E+V )T ), where T is the number of iterations, E is the number
of edges and V is the number of vertices (i.e., queries). E potentially increases faster than V —the number of edges introduced by
a new query potentially grows with the number of existing queries.
Hence, the overall execution time would grow faster than a linear
rate as we introduce more queries. Note that we do not consider
the time to build the graph, which can be incrementally constructed
whenever a new query is collected. Thus, it is unnecessary to build
the graph from scratch every time.
In Fig. 11, we record the execution time on a single 8-core PC
with 32GB memory. Consistent with our analysis above, the execution time grows slightly faster than a linear rate against the number
of total queries. Nevertheless, when all four million queries are
used, the execution time is just under 20 minutes. This is a reasonable time frame for our framework to be applied frequently (e.g.,
on a daily basis), at least on the scale of tens of millions of queries.
Furthermore, as the number of new queries within a fixed time
interval tends to decrease over time, the total number of queries
should grow sublinearly. Lastly, as Sect. 5.2.4 shows, including
more queries brings in gradually less boost in accuracy. Thus, we
do not expect to handle an arbitrarily large number of queries, since
it would not achieve a noticeable benefit beyond a certain point.

6.

CONCLUSION

In this paper, we proposed a Dirichlet-based graph regularization framework, driven by our key observations on heterogeneous
pairwise features and confidence-aware classification. Building on
the proposed framework, we discussed example realizations of several real-world IR tasks, particularly query intent classification in

960

