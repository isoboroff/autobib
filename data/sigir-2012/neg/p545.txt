Modeling User Posting Behavior on Social Media
Zhiheng Xu, Yang Zhang, Yao Wu and Qing Yang
Institute of Automation
Chinese Academy of Sciences
Beijing, 100190, China

{xuzhiheng,yagzag,wuyao,qyang}@nlpr.ia.ac.cn

ABSTRACT

websites. There are also studies focused more on individual
user behavior, by analyzing the content users have created
[30, 37, 41] or inferring from their social friends [9, 14, 40],
to help users ﬁnd interesting information or people.
While previous works on individual user behavior [9, 14,
30, 37, 40, 41] have simply assumed that users tend to publish content they are interested in or make friends with similar interest, however, reality is much more complicated due
to diﬀerent usage patterns and user intentions. For example, it is reported that users are easily attracted by breaking
news [1, 27], and are likely to create conversation with their
intimate friends [15, 20]. On the other side, friendship on
social media does not necessarily indicate similar interest,
since it may arise from diﬀerent sources such as inﬂuence,
homophily, environment and reciprocity [4]. All these problems require a more comprehensive model of user behavior
on social media, which is the task we deal with in this paper.
Inspired by those early works [1, 15, 20, 27], we believe
that when a user publishes a post, he is probably inﬂuenced
by three factors: breaking news happens at that moment,
posts published by his friends recently and his intrinsic interest. In light of this, we equally divide our experimental time
period into time intervals, and within each time interval,
we compute the distribution of breaking news and friends’
timeline for each user, which are assumed as two external
factors that might aﬀect his posting behavior in the same
time interval. Subsequently, by modeling user interest as a
distribution over latent topics, we use a mixture latent topic
model to represent user posting behavior, and present the
inference of our model based on collapsed Gibbs sampling.
Our experiment is based on Twitter, a popular social media website. Since Twitter has attracted thousands of individuals and organizations with business intents (e.g., news
channels, online brands and social spammers), we ﬁrst built
a dataset of 11,358 common users, and then collected all
tweets published by those users and samples of their social
friends during a 70-days experimental time period. We tested our model on this dataset and showed its superiority over
the competitors. Although our work has been done in the
context of Twitter, we expect the same results would hold
for many other similar applications, such as Facebook updates and Google Buzz.
The main contributions of our work include:

User generated content is the basic element of social media
websites. Relatively few studies have systematically analyzed the motivation to create and share content, especially
from the perspective of a common user. In this paper, we
perform a comprehensive analysis of user posting behavior
on a popular social media website, Twitter. Speciﬁcally,
we assume that user behavior is mainly inﬂuenced by three
factors: breaking news, posts from social friends and user’s
intrinsic interest, and propose a mixture latent topic model
to combine all these factors. We evaluated our model on
a large-scale Twitter dataset from three diﬀerent perspectives: the perplexity of held-out content, the performance
of predicting retweets and the quality of generated latent topics. The results were encouraging, our model clearly
outperformed its competitors.

Categories and Subject Descriptors
H.4 [Information Systems Applications]: Miscellaneous;
H.3.3 [Information Search and Retrieval]: Information
ﬁltering—performance measures

General Terms
Algorithms, Experimentation

Keywords
Twitter, user modeling, user behavior, topic model

1.

INTRODUCTION

With the rising popularity of social media, better understanding of user posting behavior has become crucial for
many personalization and information ﬁltering applications, as well as for better site design and advertising policies.
Towards this goal, existing works have examined the workloads of various social media websites [7, 13, 21], aimed at
providing a global picture of user activity patterns on these

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$10.00.

1. We build a large and reasonable dataset for analyzing
user posting behavior on Twitter.
2. A simple but eﬀective method is used to recognize
breaking news from Twitter streams in a certain time period.

545

generated by users and their social friends, and the SVM
classiﬁcation framework was leveraged in [5, 6, 23] to detect
spammers and content promoters on social media.
Within the research area of Twitter, few have been done
to systematically analyze individual user behavior. Previous eﬀorts about user modeling on Twitter simply built a
“bag-of-words” proﬁle for each user based on his tweets, and
extracted key words [9], entities [2], categories [30] or latent
topics [17, 41] for that user. Although existing works can
to some extent help recognize important information about
users, however, they failed to capture the real motivation
of users to publish content, as user behavior can easily be
aﬀected by some external factors other than user interest.
To reach a comprehensive model of user behavior, we propose a mixture model which incorporates three important factors that might trigger user posting behavior, namely
breaking news, friends’ timeline and user interest. Our model is under the framework of latent topic models, since the
entity-based and category-based user modeling frameworks
would require external knowledge bases such as Wikipedia
and AlchemyAPI 1 , which are time and resources consuming. Inspired by previous works on multiple text streams
modeling [8, 18, 33], we present the inference of our model
based on collapsed Gibbs sampling, and further test it on a
large-scale Twitter dataset from three diﬀerent tasks.

3. We analyze the inﬂuence of diﬀerent social relationships
on user posting behavior, and quantitatively measure the
inﬂuence between users.
4. We propose a mixture topic model to analyze user
posting behavior, and demonstrate the superiority of our
model from three diﬀerent tasks.
The remainder of this paper is organized as follows: section 2 provides a brief review of related work, section 3
describes the way to build our dataset, section 4 formally
presents our mixture model, followed by the results of our
experiments in section 5. Finally we conclude in section 6.

2.

RELATED WORK

2.1 Social Media
Social media has become indispensable to users recently.
A rich set of studies has been conducted on various forms of
social media, such as blogs, photo and video sharing communities, question/answering portals and social bookmarking sites, focused on diﬀerent properties and applications of
them. For example, Gruhl et al. studied the dynamics of
information propagation in blogspace [12], Leskovec et al.
analyzed the network structure and evolution on diﬀerent
information networks [24, 25], Agichtein et al. introduced a
classiﬁcation framework to extract high quality content on
question/answering portals [3] and Benevenuto et al. tried
to discover spammers on a video sharing community [6].
Among the various successful social media websites, Twitter, a microblogging service, has attracted considerable attention from research area recently. With a limit of 140
characters for each message, Twitter enables an even faster
mode of communication and information propagation. Early works [15, 20] examined the usage patterns and network
properties of Twitter, and revealed that Twitter was mainly
used in two diﬀerent ways: as an information platform or as
a social network. Subsequently, to better leverage its great
wealth of both textual and social information, researchers
have used Twitter to discover breaking news [28, 36], detect
natural disasters [35, 39], improve realtime web search [11],
characterize media events [10] and identify inﬂuential users
[41] or interesting content [9].

3. DATASET PREPARATION
We started by using Twitter’s streaming API 2 to collect
a random sample of the public tweets from March 10, 2011
to May 19, 2011 (the streaming API would return about 1%
of all tweets each day, and is widely used for analyzing news
on Twitter). After removing non-English tweets, the stream
dataset contained 56,415,430 tweets published by 9,292,345
users, with an average of 805,935 tweets each day. This
stream dataset was used to extract breaking news for each
time interval on Twitter. Since Twitter imposes a rate limit
on crawling posts of a speciﬁc user, it is diﬃcult for us to
analyze large amount of users. Thus we would like to build
a relatively small dataset of common active users.
Speciﬁcally, we assumed that a user was common and active if he had (100-3000) friends/followers, (10-200) tweets per week and has been listed (1-50) times. Most common and active Twitter users were believed to fall into this
category. We randomly picked 11,358 ordinary users as
our experimental users, and crawled all their tweets during the 70-days experimental time period, yielding a dataset
of 7,843,190 tweets. For each user, we crawled his entire
social graph, including his followees, listers (people in the
user-generated lists) and listfollowees (people in the userfollowed lists). As it was diﬃcult for us to collect tweets from
all those friends, we created a sample of friends for each user,
including all friends that have been retweeted or mentioned
more than 4 times by him (on average 30 friends were chosen
for each user), and 5 randomly picked followees, listers and
listfollowees respectively. Finally the entire dataset of social
friends contained 179,456 unique users, and we crawled all
their posts during the experimental time period, yielding a
dataset of 86,815,267 tweets.
Admittedly, our samples of social friends only contain small proportion of users’ friends. However, as presented

2.2 User Modeling
The massive amount of data generated by social media
users has provided researchers with insights into user behavior. For instance, by analyzing workloads from three
information networks, Guo et al. showed that users’ posting
behavior exhibited strong daily and weekly patterns. They
also pointed out that diﬀerent types of content would have
diﬀerent characteristics [13]. Benevenuto et al. used clickstream data from a social network aggregator to compare user behavior across diﬀerent online social networks, and they
further investigated social interactions on those networks [7].
These macroscopic analysis of user behavior provided interesting observations about general usage patterns on social
media websites, but they might lack interpretations at an individual level. To reach a better understanding of individual
user behavior, work [32] investigated the causality between
individual behavior and social inﬂuence by observing the information diﬀusion among users, work [27] predicted a user’s
news interest from the user activities and the news trends, work [37] proposed a user interest model based on tags

1
2

546

http://www.alchemyapi.com/
https://dev.twitter.com/docs/streaming-api/

later in this paper, users’ top retweeted/mentioned friends
are much more inﬂuential than other social friends. Thus we
believe that our dataset (which includes users’ top retweeted/mentioned friends) is still to some extent reasonable for
analyzing the inﬂuence of social friends.

4.

Table 1: Bursty Words in 3 Time Intervals
Time interval
Bursty Words
tsunami japan earthquake 90999 quake
2nd
redcross hawaii tsunamis prayers tokyo
kate royal wedding william middleton
50th
abbey prince duchess mcqueen wills
osama laden bin obama dead
53th
abbottabad death obl islamabad 1945

MIXTURE MODEL OF USER BEHAVIOR

Imagine the situation when a common user publishes a
post about iphone, the reason behind this behavior might
be: (1) he is a fan of smartphones and has a long time focus on iphone (2) he is reminded by some big events about
iphone, such as the release of Iphone 4S (3) he is attracted by
a discussion about iphone raised by his close social friends.
In light of this, user posting behavior can be represented as
a mixture model of three diﬀerent factors: breaking news,
posts from social friends and the user’s intrinsic interest.
Speciﬁcally, given a user a in time interval T , the likelihood
for him to generate a word w is regarded as a sample of
the following mixture model (based on the bag-of-words assumption).
pT a (w) = λB p(w|θB ) + (1 − λB )(λa2 pT (w|θN )
+λa3 pT (w|θaF ) + λa0 p(w|θaI ))


user participation



wedding of William


death of Laden







(1)

Figure 1: User Participation in 3 Events.
where N T (w) represents the number of tweets containing
word w in time interval T , N T is the number of tweets
in T , N (w) is the number of tweets containing word w in
the entire dataset,
 and N is the number of tweets in the
entire dataset.
N T (w) is used to promote the scores of
high frequency words and punish low frequency words. We
set a threshold S for score(w) (300 is chosen when T is 24
hours), and discard words below the threshold. It is worth
to mention that we also try to set diﬀerent threshold S, and
ﬁnd that there are no obvious change for the experimental
results when S is larger than 50.
Table 1 gives the top 10 bursty words in the 2nd, 50th
and 53th time intervals when T is set to be 24 hours, well
represent 3 real world events: japan earthquake, wedding of
prince William and the death of Osama bin Laden. Figure 1
shows user participation in the 3 events. Among the 11358
experimental users, 46.5% of users published tweets about
japan earthquake in the 2nd time interval, 50.9% of users
talked about the wedding of prince William in the 50th time
interval and the death of Osama bin Laden attracted 52.4%
of users in the 53th time interval, which demonstrate that
breaking news has great impact on user posting behavior.
For each time interval T , we model the distribution of

breaking news according to equation (3), where w stands

for any word that meets score(w ) ≥ S.

4.1 Influence of Breaking News

pT (w|θN ) =

To identify emerging news on Twitter, we borrow the idea
from TwitterMonitor [28], where news is represented by a
group of bursty keywords that suddenly appear in tweets at
an unusually high rate. For each time interval T , a set of
bursty keywords is extracted using equation (2):
N T (w) N 
N T (w)
N T N (w)

japan earthquake





In the formula above, θB is the background smoothing
model using word frequency from the entire dataset, and λB
is the mixing weight of the background model θB . pT (w|θN )
denotes the distribution of breaking news in time interval
T , and pT (w|θaF ) is the distribution of friends’ timeline for
user a in time interval T . All breaking news and friends’
posts in the same time interval are assumed to have inﬂuence
on user posting behavior, since it will be computationally
expensive to consider what has happened before each user
behavior. p(w|θaI ) means the distribution of a’s interest,
and is represented by a distribution over latent topics in this
paper. λa2 , λa3 and λa0 are the mixing weights of breaking
news, friends’ timeline and user interest for a respectively.
Notice that, for each user, the model uses diﬀerent mixing
weights, considering the diﬀerence between users in usage
patterns. For instance, some users regard Twitter as an
instant messaging tool and use it to communicate with their
friends (where λa3 should be big). On the other hand, some
people consider Twitter to be an information platform and
use it to release or seek information they are interested in
(where λa0 should be big). As presented later in this section,
all mixing weights can be automatically learned during the
training process.
In the following of this section, we ﬁrst separately analyze
the inﬂuence of breaking news and friends’ timeline on user
posting behavior, and compute their corresponding distributions. Then we view user interest as a distribution over
latent topics, and use a latent topic model framework to
represent our mixture model of user posting behavior.

score(w) =







⎧
score(w)
⎪
⎪
⎨   score(w )
⎪
⎪
⎩

score(w) ≥ S

w

0

(3)
score(w) < S

4.2 Influence of Social Friends
Twitter introduces a directed social relationship named
“follow”, which enables users to follow others to receive their
tweets. To further help users organize their followees and ﬁlter incoming tweets, Twitter has launched another feature

(2)

547







mentioned proportion



retweeted proportion




followees



listers







followees
listers
listfollowees



listfollowees




















Figure 2: Influence of Friends on Retweet Behavior.

Figure 3: Influence of Friends on Mention Behavior.




topRetweeted


named “list” since November 2, 2009, which can group sets
of users into categories. Users can create their own lists,
add and delete list members, or just follow other users’ lists. Besides these two explicit relationships, there are two
implicit relationships indicated by tweets, namely “retweet”
and “mention”. The retweet operates as a citation of another
user’s tweet, with the form “RT @username”, while mention
acts as a response to another user’s tweet, with the form
“@username”. Both retweet and mention are strong signals
of social inﬂuence [22].
In our dataset, on average, each user has 644 followees.
39.8% of users have created at least 1 list, with an average
of 169 people in each list, 37.2% of users have followed at
least 1 list, with an average of 577 people in each list. 44.7%
of users do not use list, which means list has not been used
as widely as follow yet. Figure 2 analyzes the inﬂuence of
followees, listers and listfollowees on user retweet behavior.
During the 70-days experimental time period, on average,
21% of users’ followees have been retweeted by them, 28.5%
of listers have been retweeted and only 10% of users’ listfollowees have been retweeted by them. The inﬂuence on user
mention behavior is similar, as reported in ﬁgure 3, 28.8%
of followees, 37.6% of listers and 13.3% of listfollowees have
been mentioned respectively. The results show that listers have a little greater impact on users than followees, but
listfollowees are far less important. However, most of users’
social friends have not been retweeted or mentioned, which
means that the explicit relationships on Twitter do not necessarily indicate strong inﬂuence [19]. As inﬂuence mainly
exsits in the form of retweet and mention on Twitter, we assume that for each user, the more times a friend is retweeted
or mentioned by him, the more inﬂuence that friend has on
the user. To approximately verify this assumption, we build
a “bag-of-words” proﬁle for each user based on his tweets,
and use TF-IDF algorithm to determine the word weight.
Only the top 200 words are selected. For each experimental
user, we compute the cosine similarities with his top retweeted friends, top mentioned friends, the random sample of his
friends in section 3 (i.e., 5 listers, 5 followees and 5 listfollowees) and 5 random users that are not directly connected
with him. As demonstrated in Figure 4, on average, the
similarities with top retweeted and top mentioned friends
are clearly higher than other friends, which proves that our
assumption is reasonable. On the other side, the similarities
with random listers, followees and listfollowees are almost
the same as random users, which is consistent with our previous conclusion that explicit relationships on Twitter are

topMentioned



randomFollowees

similarity

randomListers
randomListFollowers



randomUsers













Figure 4: Similarity between Friends.

not strong symbols of inﬂuence. Based on the assumption,
we use equation (4) to measure the inﬂuence of friend j on
user i:
Xj,i
(4)
Inf luence(j, i) =
maxj  Xj  ,i
Where Xj,i is as:
Xj,i =

N R(j, i) + N M (j, i) + 1
log(N (j) + 2)

(5)

Here N R(j, i) is the number of times friend j is retweeted by
user i, N M (j, i) is the number of times friend j is mentioned
by user i and N (j) is the total number of tweets posted by
user j. Due to the similar performance in ﬁgure 4, we view
retweet and mention the same in equation (5).
Our measure of social inﬂuence is computationally eﬃcient, and generally captures the strength of communications between friends, which is shown to accurately reﬂect
the strength of relationship between friends [26, 40]. Admittedly, there are many other works on measuring inﬂuence in
social networks. Since we mainly focus on modeling user
posting behavior rather than computing social inﬂuence between friends, we leave it as future work to compare diﬀerent
measures of social inﬂuence.
For each user i in time interval T , we compute the distribution of his friends’ timeline as:

j inf luence(j, i) ∗ NjT (w)
pT (w|θiF ) =  
(6)


j
w inf luence(j, i) ∗ NjT (w )
In the equation above, NjT (w) means the number of times
word w is tweeted by friend j during the time interval T .

548

If word w has never been tweeted by any friends during T ,
equation (6) is set to be 0.

d

d

F
B

4.3 Mixture Latent Topic Model Framework
We use a latent topic model framework to represent our
mixture model of user posting behavior, where user interest
is represented as a random mixture over latent topics, and
can be automatically inferred during the training process.
Figure 5 shows the Bayesian graphical framework of the
proposed model. The model can be viewed as an extension
of author-topic model [34], a widely used variation of Latent Dirichlet Allocation (LDA) [29] to integrate authorship
information of documents into topic modeling. The authortopic model assumes that each author in the document collection is represented by a distribution over topics, and each
word is associated with two latent variables: an author and
a topic. To generate each document from a document collection, it ﬁrst chooses an author from a document’s author
list, samples a topic from topic distribution associated with
the selected author, and then picks a word from the topic
speciﬁc word distribution. As each tweet has only 1 author,
the author-topic model here acts as to collect a document for
each user based on all his tweets, and uses LDA to extract
the topic distribution of this document.
The proposed model has a similar general structure to
the author-topic model, but with additional machinery to
handle the distribution of breaking news, friends’ timeline
and background words respectively. In particular, a latent
random variable x is associated with each word, acts as a
switch to determine whether the word is generated from the
distribution of background model, breaking news, posts from
social friends or user’s intrinsic interest. x is sampled from
a user-speciﬁc multinomial distribution λa , which in turn
has a symmetric Dirichlet prior, η. A indicates the set of
authors, T1 is the set of latent topics, Nd means the length
of tweet d and D is the set of tweets. The generative process
of this model is as follows:
1.
2.
3.
4.

For
For
For
For

A

A

N

T

T1

Nd

Nd

a

D

Figure 5: Bayesian Graphical Framework of the
Mixture Model.
sampling using the following updating rules:
p(xi = 0, zi = t) ∝
ea,0,−i + η ma,t,−i + α nt,wi ,−i + β
(1 − λB )
ea,−i + 3η ma,−i + αK nt,−i + βW

(7)

p(xi = 1) ∝ λB p(wi |θB )

(8)

p(xi = 2) ∝ (1 − λB )pT (wi |θN )

ea,2,−i + η
ea,−i + 3η

(9)

ea,3,−i + η
ea,−i + 3η

(10)

p(xi = 3) ∝ (1 − λB )pT (wi |θaF )

where ea,j,−i and ea,−i are computed as:
ca,j,−i
ea,j,−i = 
(j = 0, 2, 3)
|θj |
ea,−i =

each topic k, draw ϕk from Dir(β)
each author a, draw θa from Dir(α)
each author a, draw λa from Dir(η)
the ith word wi posted by a during T
(a) Sample xi from Multinomial(λa )
(b) If xi = 0
A. Sample topic zi from Multinomial(θa )
B. Sample wi from Multinomial(ϕzi )
(c) Else
⎧
⎪
xi = 1
⎨p(w|θB )
Sample wi from pT (w|θN )
xi = 2
⎪
⎩p (w|θ )
xi = 3
T
aF



ea,j,−i

(j = 0, 2, 3)

(11)

(12)

j

Here ca,j,−i is the number of words written by a assigned to
stream j (excluding the ith word), |θj | is the size of stream
j, which means the number of non-zero words in the distribution of stream j (for the stream of user interest, we use
ca,0,−i as an approximation). While previous works in [8, 18,
33] simply use ca,j,−i to denote the importance of stream j,
in our work we smooth it with the size of the corresponding
stream, since diﬀerent streams have diﬀerent size. ma,t,−i
is the number of words posted by a assigned to topic t (excluding the ith word) and nt,w,−i is the number of times
word w assigned to topic t (excluding the current one). ma
denotes the total number of words posted by a and nt is the
total number of words under topic t. K means the number
of latent topics and W is the number of words.
The other parameters can be estimated as follows:

4.4 Model Inference
Our inference of the latent variable x is inspired by previous works on multiple text streams modeling [8, 18, 33],
where a word is “split” into diﬀerent streams, and a latent
variable is sampled to indicate which stream the word belongs to. For each word in a document, the assignment of the
latent variable is decided by two factors: the distribution of
streams in the document, and the importance of the word in
each stream. Based on this idea, we view the distribution of
background model, user interest, breaking news and friends’
timeline as four diﬀerent streams, and apply collapsed Gibbs

549

θa,t =

ma,t + α
ma + αK

(13)

ϕt,w =

nt,w + β
nt + βW

(14)

λa,j =

ea,j + η
ea + 3η

(j = 0, 2, 3)

(15)

0.2

5600

0.19

5500

0.18
0.17

5300

0.16

our model

0.15

5200

AT model

0.14

5100

0.13

AT1 model

5000

0.12
0.11

4900

0.1

4800

0.09

4700

0.08

0

50

100

150

200

number of topics

250

300

0

Figure 6: Perplexity of Held-out Content.

p(w|θaI ) =



θa,t ∗ ϕt,w

100

150

200

number of topics

250

300

5.2 Performance of Predicting Retweets

(16)

The main purpose of user modeling is to help users ﬁnd
interesting information from the overwhelming information
streams. While in the context of Twitter, retweet is the
most important signal of user interest, as users are prone
to broadcast their favorite tweets to their followers. Thus,
the performance of predicting retweets is a good standard
to judge the performance of a user model.
Speciﬁcally, for each user in every time interval T , we
randomly select a tweet that is retweeted by him (if exists),
and mix it with 10 other tweets that are not retweeted by
him. All the 11 tweets are published by his top retweeted/mentioned friends in the same time interval. This experiment can be seen as a real information ﬁltering application:
when a user is viewing his Twitter stream consists of 11
new tweets, he might ﬁnd a tweet interesting and retweet it
to his followers, on the other side, the other 10 tweets are
not important to him relatively (since all the 11 tweets are
published in the same time interval by his close friends, it
is reasonable to assume that the user can see all of them
at once). The task of a user model is to accurately predict which tweet can attract the user’s attention and will be
retweeted by him.

Hyper-parameters like α, β and η can be estimated using
standard methods introduced in [31]. It is worth to mention that we also try to compute the weight of background
model automatically using formula similar to (15), however
the result is not as good as to set a reasonable weight before
training starts (but it still outperforms our baseline).

EXPERIMENT

We examine the proposed model from three diﬀerent perspectives: the perplexity of held-out content, the performance of predicting retweets and the quality of generated
latent topics.

5.1 Perplexity of Held-out Content
The perplexity in this study means the performance of
prediction for new tweets, which is a widely used method to
judge the performance of a topic model. We compare the
perplexities of two topic models: our model and the authortopic model. We randomly split the tweets of each user into
90% training tweets and 10% test tweets, and compute the
perplexity of all test tweets according to:
 
1
exp(− 
log(pa (wtest )))
test
m
a
a
test
a

50

Figure 7: Precision of Predicting Retweets.

t

5.

our model
TF-IDF
AT model
entity

precision

perplexity

5400

5.2.1 Compared with other user models
We ﬁrst compare the performance of our model with three
user models: the author-topic model in [41], the TF-IDF algorithm used in [9] and the entity-based user proﬁle in [2]
(The AlchemyAPI is used to extract entities). The predictive probability of each tweet d is computed according to
equation (18) in our model, and the one with the highest
probability is predicted as the retweet. T is set to be 24
hours and λB is set to be 0.3 again. For the three competitors, tweets are ranked based on their cosine similarities
with user proﬁles. We repeat the experiment ﬁve times on
diﬀerent random sample sets and the results are the average
value. Figure 7 shows the predictive precision of our model
and all the other competitors.
1 
p(d) =
pa (w)
(18)
Nd w∈d

(17)

w

where the predictive probability of a test word is denoted by
pa (wtest ), and is computed by equation (1) in our model. A
lower perplexity indicates better performance. We run each
model ﬁve times and the perplexity of each model is average
value. Figure 6 shows the results for the proposed model
and author-topic model (AT model) with diﬀerent number
of topics. T is set to be 24 hours and λB is set to be 0.3 in the
mixture model. As a result, the proposed model outperforms
the author-topic model. We also add tweets published by
users’ close friends (top retweeted/mentioned friends) into
the training data of author-topic model, however, as denoted
by AT1 model, directly use posts from social friends even
lower the performance. Notice that, the perplexities of both
models do not change apparently when the number of topics
is greater than 50.

As a result, about 17.2% of retweets are correctly predicted in our model, which is clearly better than our competi-

550

1

1

0.9

0.9

0.8

0.8

0.7

0.7

0.6

0.6

0.4
0.3

our model
16 features
17 features

recall

recall

our model
AT model
TF-IDF
entity

0.5

0.5
0.4
0.3

0.2

0.2

0.1

0.1
0

0
1

2

3

4

5

6

n

7

8

9

10

1

11

2

3

4

5

6

n

7

8

9

10

11

Figure 8: Recall of Predicting Retweets (5.2.1).

Figure 9: Recall of Predicting Retweets (5.2.2).

tors. The precision of TF-IDF algorithm, entity-based user
proﬁle and author-topic model are 10.4%, 9.8% and 9.2%
respectively, which are only little better than random selection (9.1%). However, we must point out that the task
is diﬃcult since all candidate tweets are published by users’ closest friends. Furthermore, all the four methods tend
to reﬂect the similarities of tweets with users’ general posting behavior, which are not especially intended for retweet
prediction settings. Thus the relatively low precision of all
models are reasonable. Nevertheless, the result still shows
that our model can reach a better understanding of user
posting behavior than the three competitors.
To avoid missing too much retweets, we should provide
more candidates to increase the recall of all models. Assume
that each time we provide the top n results returned by all
methods, ﬁgure 8 gives the recall of retweets with diﬀerent
n. The number of topics is set to be 50 in our model. As
demonstrated, our model outperforms the three competitors
for all diﬀerent n.

predict retweets, tweets are ranked based on their retweet
probabilities returned by logistic regression.
Figure 9 gives the recall of predicting retweets with different n. The performance of our model remains unchanged
to ﬁgure 8 because of the same test set. As shown in ﬁgure
9, the retweet prediction model (denoted by 16 features) indeed has large superiority over our model, since the proposed
mixture model is not quite intended for retweet prediction
tasks (our model only tries to reﬂect the likelihood of users
to generate content, where other important factors associated with retweets are not considered, such as the global
inﬂuence of the author and the syntactic features of tweets).
However, our mixture model can still provide an important feature for retweet models. As denoted by 17 features,
the performance of the retweet model is improved after using the predictive probability of our model as a new feature (e.g., the recall of retweets is improved from 37.8% to
41.2% for top 1 result), which means that our model is of
great importance for retweet prediction models. Notice that,
in the 17-features retweet model, if we remove 64% tweets
from a user’s Twitter stream, we can still reach a recall of
retweets over 80%, which is quite meaningful for social media websites like Twitter, where information overwhelming
has already become a serious problem.

5.2.2 Compared with a retweet prediction model
We compare the performance of our model with a retweet
prediction model. While retweet is recognized as the key
mechanism for information diﬀusion on Twitter, a rich set
of studies has been conducted to predict retweets [16, 38],
mainly based on classiﬁcation frameworks which incorporate
diﬀerent features related to tweets or authors.
We use logistic regression to build a retweet prediction
model, leveraging 16 diﬀerent features that are found to be
important in previous retweet prediction models [16, 38],
including author-based features (i.e., # of followers, # of
followees, # of times listed, is he a veriﬁed user, his account
age, # of tweets published totally and # of tweets published
per day), tweet-based features (i.e., # of urls, # of hashtags,
# of users mentioned, # of words, is the tweet a reply, is the
tweet a retweet itself) and content-based features (i.e., the
TF-IDF, entity and latent topic similarities of tweets with
users’ past tweets, which are the three competitors in 5.2.1).
We use the same test set in 5.2.1, and build another different training dataset based on the same method: for each
group of 11 tweets, the retweet is labeled as positive example
and the other 10 tweets are viewed as negative examples. To

5.3 Impact of Model Parameters
We investigate the impact of λB and T on the model performance. Figure 10 shows the results of diﬀerent λB when
T is set to be 24 hours. The vertical axis on the left side of
the graph is the perplexity of held-out content and the one
on the right side means the precision of predicting retweets.
As shown in ﬁgure 10, the model performance is generally
similar when λB is small, and drops dramatically when λB
is greater than 0.5. The best value for λB is between 0.2
and 0.3. Even if we set λB to be zero, the result is still satisfactory, which means the background model is not quite
important to our method. We further ﬁx λB to be 0.3 and
analyze the inﬂuence of diﬀerent time interval T in ﬁgure
11. When T is 24 hours, the model performance is the best.
With T get longer, the model becomes coarse-grained and
the performance drops. On the other side, the performance
will also drop when T is shorter than 24 hours. According to

551

9000

8000

0.2

5400

0.2

0.18

5300

0.18

0.16

0.16

5200

0.14

7000

0.1

6000

5000

0.12

perplexity
precision

5000
0.1

0.08

4900

0.06

4800

perplexity
precision

0.08

0.04

4000

0.14

5100

0.12

0.06

4700

0.04

0.02

4600

0.02

4500

0

0

3000
0

0.1

0.2

0.3

0.4

0.5

Ȝȕ

0.6

0.7

0.8

0.9

1

ALL

Figure 10: Influence of λB .

RN

RF

D50

Figure 12: Importance of News and Friends’ Posts.

5400

0.19

18

5300

0.18

16

0.17

14

our model

12

AT model

5200

0.16

5100

0.15
5000
0.14
4900

perplexity
precision

17

10
8

7

0.13

6

4800

0.12

4

4700

0.11

4600

0.1
3h

6h

12h

T

24h

48h

2

96h

0

Figure 11: Influence of T .

Figure 13: Labeled Result.

our observation, it might probably due to the data sparsity
problem in each short time interval.
We further analyze the importance of breaking news and
friends’ timeline in our model respectively. As shown in ﬁgure 12, when removing breaking news from our model (denoted by RN), the performance remains almost the same,
and when removing friends’ timeline from our model (denoted by RF), the performance drops dramatically. This indicates that the distribution of friends’ timeline contributes
a lot to the mixture model, but the distribution of breaking
news is of little importance. The little impact of breaking
news might due to two reasons: 1. the distribution of breaking news is really sparse 2. it is quite possible that users’
friends will publish posts about breaking news, which might
also lower the importance of breaking news. However, the
existence of breaking news is still meaningful, for example,
as denoted by D50, the performance of our model is clearly
better than average on the 50th day, when the wedding of
prince William happened.

t topics between two models. We present the top 50 words
of each latent topic to three labelers and ask them to label
which one can better represent a topic. Due to the limit of
space, table 2 only displays the top 10 words for each salient
topic, and ﬁgure 13 gives the labeled result. On average,
71% of topics generated by the proposed model are labeled
better, which shows that our model can reach a better understanding of latent topics behind Twitter streams.

5.5 Discussion
The results of our experiments prove that the proposed
model clearly outperforms other user models. By comparing
the perplexity of held-out content and the quality of generated latent topics, our mixture model is shown to be better
than the traditional author-topic model, and within the task
of predicting retweets, its superiority over other competitors
is still obvious. Although our model is not comparable to
retweet models in predicting retweets, however, it can still
provide an important feature for them.
To reach a microscopic understanding of our mixture model, we empirically analyze how diﬀerent factors work in our
model based on a random sample of 30 users. First, we
ﬁnd that words associated with breaking news are generally
well recognized. Second, for users with obvious interest (11
users are found to often publish tweets about some areas),
our model can discover words related to user interest very
well. Take a technology fan as an example, words such as
“ipad” and “android” are easily assigned to his interest by
our model. On the other side, if a user does not show strong

5.4 Quality of Latent Topics
Another typical method to judge the performance of topic
models is to print top words for the latent topics and judge
them by experience. We design an experiment to compare
the latent topics generated by author-topic model and the
proposed model. Speciﬁcally, we set the number of topics to
be 50 and manually extract the same salient topics for both
models. As a result, 8 latent topics are extracted, and the
rest of latent topics are either meaningless topics or diﬀeren-

552

Table 2: Top 10 Words for Latent Topics
Topic
mixture model
film
movie
films
screening
director
trailer
movies
festival
interview
comedy
Topic
mixture model
app
ipad
iphone
google
apple
android
apps
data
code
web

0
AT model
film
movie
game
awesome
games
comic
time
trailer
play
video
4
AT model
app
google
ipad
iphone
apple
mobile
twitter
android
facebook
video

Topic
mixture model
libya
war
forces
arab
killed
gaddafi
military
israel
libyan
security
Topic
mixture model
music
song
album
video
show
playing
songs
listen
listening
tour

1
AT model
libya
people
news
israel
killed
forces
police
military
arab
bin
5
AT model
music
show
tonight
album
playing
band
song
tour
night
great

Topic
mixture model
canada
election
canadian
toronto
vote
campaign
government
politics
harper
vancouver
Topic
mixture model
social
media
twitter
facebook
blog
post
web
marketing
online
google

2
AT model
canada
vote
election
toronto
harper
party
campaign
canadian
debate
labour
6
AT model
social
media
great
business
top
stories
daily
twitter
blog
post

Topic
mixture model
game
team
coach
fans
play
players
sports
games
basketball
season
Topic
mixture model
food
dinner
wine
eat
chocolate
lunch
cheese
restaurant
delicious
chicken

3
AT model
state
game
coach
draft
nfl
team
football
players
ncaa
big
7
AT model
food
wine
dinner
beer
eat
great
lunch
chocolate
love
cheese

6. CONCLUSION

interest in any area, words assigned to user interest will be
less reasonable, since it is diﬃcult to model his topics of interest accurately. Third, the factor of social inﬂuence mainly
captures words that are recently published by users’ friends
(and are not quite related to users’ interest). Most words inspired by friends can be captured successfully, despite there
are also many noise words included. Finally, some words
out of all three factors can be handled by the background
model, most of which are high-frequency words appear in
daily life posts and conversations.

In this paper, we propose a mixture model to analyze user
posting behavior on social media. By assuming that user behavior is mainly inﬂuenced by three factors: breaking news,
posts from social friends and user’s intrinsic interest, our
method is able to reach a more comprehensive model of user
posting behavior on social media. We demonstrate the superiority of the proposed model on a popular social media
website, Twitter, from three diﬀerent tasks: the perplexity
of held-out content, the performance of predicting retweets
and the quality of generated latent topics. The results are
satisfactory and our model clearly outperforms other traditional methods.
Our future work lies in several areas. First, our basic
idea is not limited to latent topic models, and it will be
an interesting direction to test it under other frameworks, such as the entity-based or category-based user modeling
frameworks. Second, while it is widely agreed that user interest will change with time, our method does not model
the change of user interest explicitly since the experimental
time period is relatively short. However, to achieve a long
time understanding of user posting behavior, it is necessary
to incorporate time factors into user interest model. Third,
there might be some special terms besides words in tweets,
such as URLs, hashtags and usernames. We tend to investigate whether the presence of those terms can help improve
our model performance. Finally, the distribution of breaking news and friends’ timeline computed in our model are
simple, and more accurate methods are worth further exploration. For example, to compute breaking news around
users’ geographic location, or try diﬀerent measures of social
inﬂuence in computing the distribution of friends’ timeline.

What is the potential value of our model for some real
tweets recommendation systems? Admittedly, as demonstrated in the low accuracy of predicting retweets, it might
still not be accurate enough to recommend tweets only based
on our model. We believe that a good tweets recommendation system should based on classiﬁcation frameworks which
incorporate various of features (just like the retweet model
in 5.2.2), and our model can be used as one feature to reﬂect
the similarities of tweets with users’ general posting behavior, as well as other user modeling frameworks. Furthermore, there are also many other important factors should
be considered, such as the geographic information of users
and their entire social graph, which we leave as future work
due to the restriction of our current dataset.
Finally, we must point out that it is quite a diﬃcult task
to model user posting behavior on Twitter, since users can
easily generate content with any intentions at diﬀerent time
and place. On account of this, our model is still a macrolevel modeling of user behavior, as only three factors are
included in it. Thus, the interpretation of our model at
micro-level might not be good enough sometimes, especially
for users with no obvious interest. Nevertheless, as shown
in the results of our experiments, our model can still reach a
better understanding of user posting behavior in most cases,
and lay out a foundation to personalization and information
ﬁltering applications on Twitter.

7. REFERENCES
[1] F. Abel, Q. Gao, G.-J. Houben, and K. Tao.
Analyzing temporal dynamics in twitter proﬁles for
personalized recommendations in the social web. In
Proc. of WebSci, 2011.

553

[22] A. Leavitt, E. Burchard, D. Fisher, and S. Gilbert.
New approaches for analyzing inﬂuence on twitter. a
publication of the Web Ecology project.
[23] K. Lee, J. Caverlee, and S. Webb. Uncovering social
spammers: Social honeypots + machine learning. In
Proc. of SIGIR, 2010.
[24] J. Leskovec, L. Backstrom, R. Kumar, and
A. Tomkins. Microscopic evolution of social networks.
In Proc. of KDD, 2008.
[25] J. Leskovec, K. J.Lang, A. Dasgupta, and
M. W.Mahoney. Statistical properties of community
structure in large social and information networks. In
Proc. of WWW, 2008.
[26] C.-Y. Lin, K. Ehrlich, V. GriﬃthsFisher, and
C. Desforges. Smallblue: People mining for expertise
search. IEEE Multimedia Magazine, 2008.
[27] J. Liu, P. Dolan, and E. R. Pedersen. Personalized
news recommendation based on click behavior. In
Proc. of IUI, 2010.
[28] M. Mathioudakis and N. Koudas. Twittermonitor:
Trend detection over the twitter stream. In Proc. of
SIGMOD, 2010.
[29] D. M.Blei, A. Y.Ng, and M. I.Jordan. Latent dirichlet
allocation. Journal of Machine Learning Research,
2003.
[30] M. Michelson and S. A. Macskassy. Discovering users’
topics of interest on twitter: A ﬁrst look. In Proc. of
AND, 2010.
[31] T. P. Minka. Estimating a dirichlet distribution. 2009.
[32] M. Papagelis, V. Murdock, and R. van Zwol.
Individual behavior and social inﬂuence in online
social systems. In Proc. of HT, 2011.
[33] M. Paul and R. Girju. Cross-cultural analysis of blogs
and forums with mixed-collection topic models. In
Proc. of EMNLP, 2009.
[34] M. Rosen-Zvi, T. Griﬃths, M. Steyvers, and
P. Smyth. The author-topic model for authors and
documents. In Proc. of UAI, 2004.
[35] T. Sakaki, M. Okazaki, and Y. Matsuo. Earthquake
shakes twitter users: real-time event detection by
social sensors. In Proc. of WWW, 2010.
[36] J. Sankaranarayanan, H. Samet, B. E. Teitler,
M. D.Lieberman, and J. Sperling. Twitterstand: News
in tweets. In Proc. of GIS, 2009.
[37] J. Stoyanovich, S. Amer-Yahia, C. Marlow, and C. Yu.
Leveraging tagging to model user interests in
del.icio.us. In Proc. of AAAI, 2008.
[38] B. Suh, L. Hong, P. Pirolli, and E. H. Chi. Want to be
retweeted? large scale analytics on factors impacting
retweet in twitter network. In Proc. of SocialCom,
2010.
[39] S. Vieweg, A. L. Hughes, K. Starbird, and L. Palen.
Microblogging during two natural hazards events:
what twitter may contribute to situational awareness.
In Proc. of CHI, 2010.
[40] Z. Wen and C.-Y. Lin. On the quality of inferring
interests from social neighbors. In Proc. of KDD, 2010.
[41] J. Weng, E.-P. Lim, J. Jiang, and Q. He. Twitterrank:
Finding topic-sensitive inﬂuential twitterers. In Proc.
of WSDM, 2010.

[2] F. Abel, Q. Gao, G.-J. Houben, and K. Tao.
Analyzing user modeling on twitter for personalized
news recommendations. In Proc. of UMAP, 2011.
[3] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and
G. Mishne. Finding high-quality content in social
media. In Proc. of WSDM, 2008.
[4] A. Anagnostopoulos, R. Kumar, and M. Mahdian.
Inﬂuence and correlation in social networks. In Proc.
of KDD, 2008.
[5] F. Benevenuto, G. Magno, T. Rodrigues, and
V. Almeida. Detecting spammers on twitter. In Proc.
of CEAS, 2010.
[6] F. Benevenuto, T. Rodrigues, and V. Almeida.
Detecting spammers and content promoters in online
video social networks. In Proc. of SIGIR, 2009.
[7] F. Benevenuto, T. Rodrigues, M. Cha, and
V. Almeida. Characterizing user behavior in online
social networks. In Proc. of IMC, 2009.
[8] C. Chemudugunta, P. Smyth, and M. Steyvers.
Modeling general and speciﬁc aspects of documents
with a probabilistic topic model. In Proc. of NIPS,
2006.
[9] J. Chen, R. Nairn, L. Nelson, M. Bernstein, and E. H.
Chi. Short and tweet: Experiments on recommending
content from information streams. In Proc. of CHI,
2010.
[10] N. A. Diakopoulos and D. A. Shamma. Characterizing
debate performance via aggregated twitter sentiment.
In Proc. of CHI, 2010.
[11] A. Dong, R. Zhang, P. Kolari, J. Bai, F. Diaz,
Y. Chang, Z. Zheng, and H. Zha. Time is of the
essence : Improving recency ranking using twitter
data. In Proc. of WWW, 2010.
[12] D. Gruhl, R.Guha, D. Liben-Nowell, and A. Tomkins.
Information diﬀusion through blogspace. In Proc. of
WWW, 2004.
[13] L. Guo, E. Tan, S. Chen, X. Zhang, and Y. Zhao.
Analyzing patterns of user content generation in
online social networks. In Proc. of KDD, 2009.
[14] J. Hannon, M. Bennett, and B. Smyth. Recommending
twitter users to follow using content and collaborative
ﬁltering approaches. In Proc. of RecSys, 2010.
[15] C. Honeycutt and S. C.Herring. Beyond
microblogging: Conversation and collaboration via
twitter. In Proc. of HICSS, 2009.
[16] L. Hong, O. Dan, and B. D. Davison. Predicting
popular messages in twitter. In Proc. of WWW, 2011.
[17] L. Hong and B. D. Davison. Empirical study of topic
modeling in twitter. In Proc. of SOMA, 2010.
[18] L. Hong, B. Dom, S. Gurumurthy, and
K. Tsioutsiouliklis. A time-dependent topic model for
multiple text streams. In Proc. of KDD, 2011.
[19] B. A. Huberman, D. M. Romero, and F. Wu. Social
networks that matter: Twitter under the microscope.
2008.
[20] A. Java, X. Song, T. Finin, and B. Tseng. Why we
twitter: Understanding microblogging usage and
communities. In Proc. of WEBKDD, 2007.
[21] H. Kwak, C. Lee, H. Park, and S. Moon. What is
twitter, a social network or a news media? In Proc. of
WWW, 2010.

554

