Exploring Example-Based Person Search in Email
1

Tan Xu1,3 and Douglas W. Oard1,2,3

College of Information Studies/2 UMIACS, University of Maryland, College Park
Human Language Technology Center of Excellence, Johns Hopkins University

3

{tanx|oard}@umd.edu

ABSTRACT

2. A PROBABILISTIC RANKING MODEL

This paper describes an entity ranking model for examplebased person search in email. Evaluation by comparison
to manually resolved named references in Enron email yield
results that correspond to typically placing the correct entity
in the ﬁrst or second rank.

Our task is to rank all e ∈ E in decreasing order of the
probability of e being the correct resolution given m and d:
P (e | m, d). By applying the chain rule, this can be inferred
as in Formula 1:
P (e | m, d) =

Categories and Subject Descriptors

where the prior probability of an email message P (d) can
be assumed to be uniform, and P (e | d) is the probability
of entity e being mentioned in email message d. There are
several types of complementary evidence that could help us
to estimate P (e | d), suggesting a log-linear model:

H.3 [Information Storage and Retrieval]: Information
Search and Retrieval

General Terms
Experimentation, Measurement

P (e | d; λ) ∝ exp

Keywords

λk fk (e, d)

(2)

where, fk (e, d) is one of K ways of estimating P (e | d) and
λk is some weight for fk (e, d). In this paper we allocate
uniform weights to all λk , but in general these weights can
and should be learned from training data.
To compute P (m | e, d), we simplify it as P (m | e) by
assuming that the probability of seeing some form of mention
for an entity will not be aﬀected by its surrounding email.
Of course, this may not be true. For example, when writing
to a family member, people might refer to another family
member using a nickname that they would not normally use
to refer to that same person in an email sent to a business
colleague. We leave accounting for that factor to future
work.
We estimate P (m | e) from normalized occurrence counts
in the entity model C  (lm ), which count the number of times
each lexical form is observed in a header, salutation or signature. We use the Dice coeﬃcient s(lm , m) to account for
partial string matches between the actual mention m and
canonical lexical forms lm :
 
P (m | e) :=
C (lm ) · s(lm , m)
(3)

INTRODUCTION

The usual formulation of the person search task requires
ﬁnding a unique page (e.g., the principal home page) for a
named person, where the name is provided in isolation as the
query [1, 2], but other formulations are possible. In this paper we focus on resolving named mentions in context, a task
that we refer to as example-based person search. We model
our task in a manner similar to the entity linking task in the
Text Analysis Conference (TAC),1 but with a speciﬁc mention in an email messages as the “query,” and automatically
constructed entity models as the items to be ranked. Specifically, given an email message d where a person’s name m
is mentioned, and a collection of person entities E, ﬁnd the
entity e ∈ E to which m refers. We report Mean Reciprocal
Rank (MRR) rather than TAC’s modiﬁed B-Cubed score
as our principal evaluation measure because our focus is on
formative rather than summative evaluation (thus preferring a ranked measure) and because in our present work we
test only with mentions that human annotators are able to
resolve (obviating the need to score what TAC calls “NILs”).
1

K

k=1

Entity retrieval, name resolution, email

1.

P (e, m, d)
∝ P (d) · P (e | d) · P (m | e, d) (1)
P (m, d)

lm ∈e

3. EXPERIMENTS

http://nlp.cs.qc.cuny.edu/kbp/2011/

We take the Enron email collection [5] hosted by CMU2 as
our email collection; this collection contains messages but no
2

Copyright is held by the author/owner(s).
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
ACM 978-1-4503-1472-5/12/08.

1067

http://www.cs.cmu.edu/∼enron/

Figure 1: Data Structure for Entity e.

Figure 2: Evaluation Results (MRR)

attachments. We obtained the identity models built automatically for this collection by Elsayed and Oard [4], which
includes 123,773 unique entities. Figure 1 shows the data
structure for each entity. The bag-of-words representation
is stored separately for words in the body of messages sent,
received-as-to by the entity, and received-as-cc by the entity
(with quoted text removed). The communication graph contains a list of all entities e with whom entity e exchanged at
least one message in each direction (i.e., at least one message
sent as “to” or “cc” to e and at least one message received
as “to” or “cc” from e ).
As ground truth we use a publicly available set of 470
single-token mentions (in 285 unique messages) that have
been manually resolved to email addresses.3 The reported
inter-annotator agreement is 81% and the median lexical
ambiguity (i.e., the number of identity models with an exact
string match) is 116 [3].
We tried four ways of estimating fk (e, d). Method (B1 ),
based on Balog’s “model 1” [1], ﬁnds entities who typically
use similar words, regardless of whether those words are
used together in one email. The words in the body of d
are taken as query terms (with no removal of quoted text),
and Lucene’s standard analyzer and index searcher are used
to score each entity. Method (B2 ), based on Balog’s “model
2” [1] works similarly, but indexing individual messages rather
than the concatenation of all messages sent or received by
each entity. For the top-scoring 50 messages dr retrieved
for dq , each entity in the “from”, “to” or “cc” ﬁeld of dr receives an equal portion of dr ’s retrieval score and these partial scores are then summed over all 50 documents. Method
(QE ) directly uses dq ’s “from” “to” and “cc” ﬁelds, assigning each such entity e equal weight. These entities are later
used in the communication graph to retrieve their one-hop
neighbors e , with each valued by their normalized number of
messages exchanged with e. Using this same communication
graph, Method (EE ) is constructed by projecting weights to
entities that are directly connected by any ranked entity
collected from the three previous methods. Each method’s
results are ﬁrst renormalized to sum to 1, and then combined
as described in equation (3). Contributions to the same entity are summed, and the ﬁnal results are renormalized to
sum to 1.

tomated entity resolution system (which will also leverage
intra-document consistency evidence in subsequent iterative
steps), we interpret this as a promising result. Our processing requires an average of 0.78 seconds per query (one core,
2.66 GHz CPU, 4 GB RAM), which meets our eﬃciency
requirements for a preprocessing stage. Elsayed, using different techniques, similarly found the communication graph
to be the best source of evidence (MRR=0.785), but at the
cost of the substantially greater computational cost to simultaneously resolve all 1.3 million named references to people
in the Enron collection [3].
Comparing Methods (B1 ) and (B2 ), our results parallel those Balog et al report for other content types, with
(B2 ) markedly better than (B1 ) [1]. Combining evidence
from Methods (B1 ) and (B2 ) yields no improvement over
Method (B2 ) alone, suggesting that further work on combination methods is called for. Zaragoza et al found the
entity graph to be useful for ranking relevant entities [6];
our failure to replicate that by Method (EE ) suggests that
we should exlore graph-based entity ranking measures and
that learned weights are likely needed for a log-linear combination in this application. We plan to explore those ideas
in future work.

4.

5. ACKNOWLEDGMENTS
We appreciate the advice and suggestions of Tim Finin,
Tim Oates, Dawn Lawrie, Paul McNamee, Jim Mayﬁeld,
Ves Stoyanov and William Webber.

6. REFERENCES
[1] K. Balog, L. Azzopardi, and M. de Rijke. A language
modeling framework for expert ﬁnding. Information
Processing & Management, 45(1):1–19, 2009.
[2] A. P. de Vries et al. Overview of the INEX 2007 entity
ranking track. In INEX, pages 245–251. 2007.
[3] T. Elsayed. Identity resolution in email collections.
PhD thesis, University of Maryland, 2009.
[4] T. Elsayed and D. Oard. Modeling identity in archival
collections of email: A preliminary study. In Conference
on Email and Anti-Spam, pages 95–103. 2006.
[5] B. Klimt and Y. Yang. The Enron corpus: A new
dataset for email classiﬁcation research. In ECML,
pages 217–226. 2004.
[6] H. Zaragoza, H. Rode, P. Mika, J. Atserias,
M. Ciaramita, and G. Attardi. Ranking very many
typed entities on wikipedia. In CIKM, pages
1015–1018. 2007.

RESULTS AND CONCLUSIONS

As Figure 2 shows, combining Methods (B2 ) and (QE )
does rather well, achieving an MRR of 0.667. Since our
ultimate goal is to use this as the ﬁrst stage of an au3

http://www.cs.umd.edu/∼telsayed/research.htm

1068

