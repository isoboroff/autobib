Experimental Methods for Information Retrieval
Donald Metzler

Oren Kurland

Google Inc.
Venice, CA, USA

Faculty of Industrial Engineering and
Management
Technion — Israel Institute of Technology
Haifa, Israel

metzler@google.com

kurland@ie.technion.ac.il

Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Experimentation
Keywords: information retrieval, experimental methods,
evaluation

1.

searchers from other disciplines, and industrial practitioners
who are interested in experimentally validating the effectiveness of search engines in a rigorous, scientifically sound
manner.
The primary goals of this tutorial are as follows:
• Highlight the importance of experimental evaluation
in IR. This will be accomplished by providing a brief
explanation of the importance of experimental evaluations as from the early days of IR, and the ramifications
of poorly executed experimental studies.

OVERVIEW

Experimental evaluation plays a critical role in driving
progress in information retrieval (IR) today. There are few
alternative ways of exploring, in depth, the empirical merits
(or lack thereof) of newly devised search techniques. Careful evaluation is necessary for advancing the state-of-theart; yet, many published papers present work that was illevaluated. Indeed, this phenomenon has garnered attention
from the community recently, after the publication of a controversial, but eye-opening, study by Armstrong et al. that
suggested ad hoc search quality has not meaningfully advanced since 1984 [1]. The authors noted that the root of the
problem was generally lax evaluation methodologies (e.g.,
weak baselines, etc.). Furthermore, many submissions to
top IR research venues (e.g., SIGIR, CIKM, ECIR, WSDM,
WWW, etc.) are rejected primarily due to insufficient or
inappropriate evaluation.
There is therefore a strong need to educate students, researchers, and practitioners about the proper way to carry
out IR experiments. This is unfortunately not something
that is taught in IR courses or covered in IR textbooks.
Indeed, to the best of our knowledge, there is very little
written work that lays down some principles for running an
IR experiment [3]. More specifically, there have not been
any recent tutorials or written works that have specifically,
and comprehensively, addressed the question of “how to run
an IR experiment” in terms of effectiveness evaluation. This
has potentially yielded a number of detrimental effects, as
described above.
The goal of the tutorial is to provide an initial set of training material for researchers interested in rigorous evaluations
of information retrieval systems. Although the primary focus is on ad hoc retrieval experiments, the principles and
concepts described in the tutorial are general and can easily
be applied to a wide range of experimental scenarios both
within, and beyond, the field of information retrieval.
The tutorial is primarily inteded for graduate students, re-

• Provide attendees with an in-depth overview of the
fundamental IR evaluation paradigm. The tutorial
will explain all of the key “tools” (e.g., test collections,
baselines, statistical significance testing, result analysis, etc.) that make up an “experimental toolbox”. We
will cover a number of case studies and provide specific examples of how the various “tools” can be used
to evaluate new ad hoc search techniques.
• More broadly, we hope that this tutorial will be an
important first step towards developing a culture of
strong experimental evaluations within the IR community. We will make our slides publicly available to
help fill the gap in knowledge left by the fact the topic
is largely ignored within IR courses and textbooks.
It is important to note that this is not a tutorial about devising evaluation measures or building test collections. Those
issues are orthogonal to the topics covered by this tutorial.
After the tutorial, attendees should be able to distinguish
between strong and weak experimental evaluation methodologies and be able to design strong, convincing experimental evaluations for their own projects.

2.

MATERIALS

The tutorial material will consist of the following:
• A web page that contains a description of the tutorial
and relevant pointers.
• A comprehensive set of (downloadable) slides available
in both PowerPoint and PDF format.
• An extensive bibliography that will allow attendees to
read more about topics of interest in more detail after
the tutorial has concluded.

Copyright is held by the author/owner(s).
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
ACM 978-1-4503-1472-5/12/08.

1185

3.

OUTLINE

4.

CONCLUSIONS

Given the highly applied nature of information retrieval
research, this tutorial will take a first step towards developing a broader pedagogical movement that will help instill
the importance of rigorous, scientifically sound experimental evaluations. It is also our hope that this tutorial will
help inspire others to develop similar training material (e.g.,
tutorials, courses, books, etc.) in the future.

• Introduction
– The critical role of (proper) empirical evaluation
in IR [3, 1]
• A high level view of empirical evaluation
– The fundamental evaluation paradigm (task definition, benchmarks, ground truth, reference comparisons, statistical significance testing, macro and
micro analysis of experimental results, etc.)

5.

– Brief, high level, examples of using the paradigm
for several IR applications (e.g., ad hoc retrieval,
question answering, filtering, text classification)
– Brief survey of different types of evaluation (crowdsourcing, user studies, online evaluation, TRECbased evaluation, etc.)
• In-depth exploration of the empirical evaluation paradigm
– Main focus: TREC-based evaluation [7, 4] of ad
hoc retrieval approaches
– A running example will be used for demonstrating
specific principles (e.g., devising a novel pseudofeedback-based query expansion method)
– Devising an experimental setup
∗ Full specification (reproducibility)
∗ Effectiveness versus efficiency
∗ Using open-source toolkits for implementation (e.g., Ivory, Lemur/Indri, Lucene, Terrier)
∗ Selecting corpora (newswire versus Web, scale)
∗ Selecting queries (titles vs. descriptions, how
many queries? [2])
∗ Relevance judgments (binary vs. graded)
∗ Data pre-processing (tokenization, stemming,
stopword removal)
∗ Free parameters
∗ Reference comparisons (baselines)
∗ Evaluation measures
∗ Statistical significance tests [5, 6, 8]
– Analyzing experimental results
∗ What counts as a “meaningful” performance
difference? [2]
∗ Studying the effect of varying free-parameter
values
∗ Exploring specific instances of the proposed
methods (e.g., by using extreme values of free
parameters)
∗ Micro-analysis (e.g., failure analysis, use cases
for specific queries)
• Summary of important evaluation principles

1186

REFERENCES

[1] T. G. Armstrong, A. Moffat, W. Webber, and J. Zobel.
Improvements that don’t add up: ad-hoc retrieval
results since 1998. In Proceedings of CIKM, pages
601–610, 2009.
[2] C. Buckley and E. M. Voorhees. Evaluating evaluation
measure stability. In Proceedings of SIGIR, pages
33–40, 2000.
[3] A. Moffat and J. Zobel. What does it mean to ”measure
performance”? In Proceedings of WISE, pages 1–12,
2004.
[4] M. Sanderson. Test collection based evaluation of
information retrieval systems. Foundations and Trends
in Information Retrieval, 4(4):247–375, 2010.
[5] M. Sanderson and J. Zobel. Information retrieval
system evaluation: effort, sensitivity, and reliability. In
Proceedings of SIGIR, pages 162–169, 2005.
[6] M. D. Smucker, J. Allan, and B. Carterette. A
comparison of statistical significance tests for
information retrieval evaluation. In Proceedings of
CIKM, pages 623–632, 2007.
[7] E. M. Voorhees and D. K. Harman. TREC:
Experiments and evaluation in information retrieval.
The MIT Press, 2005.
[8] W. Webber, A. Moffat, and J. Zobel. Statistical power
in retrieval experimentation. In Proceedings of CIKM,
pages 571–580, 2008.

