Advances on the Development of Evaluation Measures
Ben Carterette

Evangelos Kanoulas

Emine Yilmaz

University of Delaware
101 Smith Hall
Newark, DE, USA

Information School
University of Sheffield
Sheffield, UK

Microsoft Research
7 JJ Thomson Avenue
Cambridge, UK

carteret@cis.udel.edu

ekanoulas@gmail.com

ABSTRACT

models the utility of an engine to its users. We discuss traditional approaches to effectiveness evaluation based on test
collections, then transition to approaches based on test collections along with explicit models of user interaction with
search results.

The goal of the tutorial is to provide attendees with a comprehensive overview of the latest advances in the development of information retrieval evaluation measures and discuss the current challenges in the area. A number of topics
are covered, including background in traditional evaluation
paradigm and traditional evaluation measures, evaluation
measures based on user models, advanced models of user interaction with search engines, measures based on these models, measures for novelty and diversity, and session-based
measures.

2.

TUTORIAL OBJECTIVES
The primary objectives of the tutorial are to
• describe models of user interactions with retrieval systems and measures constructed on the top of such
models;
• introduce complex retrieval tasks that require the construction of advanced user models;
• highlight recent advances in the field that has influenced the evaluation measures adopted by forums such
as TREC;
• enable attendees to use advanced evaluation measures,
tailored to their own tasks;
• identify open questions in modeling user interactions
and constructing evaluation measures and motivate
work in the field.

Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems
and Software—Performance Evaluation

General Terms
Experimentation, Measurement

Keywords
evaluation, measures, user models, novelty, diversity, cascade model, sessions

1.

eminey@microsoft.com

3.

ANNOTATED OUTLINE
1. Test collections and traditional evaluation measures [17,
18, 19, 20, 16]
2. Basic User Model and Measures

TUTORIAL DESCRIPTION

The effectiveness of a retrieval system, i.e. its ability to
retrieve items that are relevant to the information need of
an end user, is one of the most important aspects of retrieval quality. A number of different experimental frameworks have been designed in IR to measure retrieval effectiveness. The systems-based approach of using a test collection comprising canned information needs and static relevance judgments to compute evaluation measures has so far
served well IR experimentation. The availability of query
logs that can demonstrate the interactions of a user with
a retrieval system has led to an increasing interest in better modeling user needs and user interaction with an engine
and building measures on the top of such models. This approach has led to measures that are better correlated with
the actual utility that a system is offering to an end user.
This tutorial focuses on methods of measuring effectiveness, in particular focusing on recent work that more directly

(a) Cooper’s Expected Search Length [7]
(b) Robertson’s Interpretation of Average Precision [14]
(c) Graded Relevance [15]
3. Cascade User Model and Measures
(a) Cascade Model
(b) Ranked Bias Precision [13]
(c) Normalised Discounted Cumulative Gain [8, 9, 3,
2]
(d) Expected Reciprocal Rank [5]
(e) Expected Browsing Utility [22]
4. Models and Measures for Novelty and Diversity
(a) Information Nuggets
(b) Subtopic Recall and Precision [23]

Copyright is held by the author/owner(s).
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
ACM 978-1-4503-1472-5/12/08.

(c) Intent-Aware Family [1]
(d) α-NDCG[6]

1200

(e) Intent-aware ERR [4]
5. Models and Measures for Session Evaluation

[7]

(a) TREC Session Track [11]
(b) Session DCG [10]
[8]

(c) Expected Utility [21]
(d) Session Precision, Recall and Average Precision [12]

4.

BIOGRAPHIES

Ben Carterette is an assistant professor of Computer and Information Sciences at the University of Delaware in Newark,
Delaware, US. He completed his PhD in Computer Science
at the University of Massachusetts Amherst in 2008. His
work on information retrieval evaluation has been recognized
with several Best Paper Awards at conferences such as SIGIR, ECIR, and ICTIR. With Evangelos Kanoulas, he has
been actively involved in coordination of the TREC Million
Query Track and the TREC Session Track.

[9]
[10]

[11]

[12]

Evangelos Kanoulas is a postdoctoral research scientist at
Google, Switzerland. Prior to that he was a Marie Curie fellow in the Information School at the University of Sheffield.
Evangelos received his PhD from Northeastern University,
Boston. He has published extensively in the field of information retrieval evaluation in SIGIR, CIKM and ECIR. He was
actively involved in coordinating the TREC Million Query
and TREC Session Track.

[13]
[14]

Emine Yilmaz is a researcher at Microsoft Research Cambridge. She obtained her Ph.D. from Northeastern University in 2008. Her main interests are information retrieval and
applications of information theory, statistics and machine
learning. She has published research papers extensively at
major information retrieval venues such as SIGIR, CIKM
and WSDM. She has also organized several workshops on
Crowdsourcing and served as one of the organizers of the
ICTIR Conference.

5.

[15]

[16]

REFERENCES

[17]

[1] Rakesh Agrawal, Sreenivas Gollapudi, Alan Halverson, and
Samuel Ieong. Diversifying search results. In Ricardo A.
Baeza-Yates, Paolo Boldi, Berthier A. Ribeiro-Neto, and
Berkant Barla Cambazoglu, editors, WSDM, pages 5–14.
ACM, 2009.
[2] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt
Deeds, Nicole Hamilton, and Greg Hullender. Learning to
rank using gradient descent. In ICML ’05: Proceedings of
the 22nd international conference on Machine learning,
pages 89–96, New York, NY, USA, 2005. ACM Press.
[3] Ben Carterette. System effectiveness, user models, and user
utility: a conceptual framework for investigation. In SIGIR,
pages 903–912, 2011.
[4] Olivier Chapelle, Shihao Ji, Ciya Liao, Emre Velipasaoglu,
Larry Lai, and Su-Lin Wu. Intent-based diversification of
web search results: metrics and algorithms. Inf. Retr.,
14(6):572–592, 2011.
[5] Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre
Grinspan. Expected reciprocal rank for graded relevance. In
In Proceedings of the 18th ACM Conference on
Information and Knowledge Management (CIKM), 2009.
[6] Charles L.A. Clarke, Maheedhar Kolla, Gordon V.
Cormack, Olga Vechtomova, Azin Ashkan, Stefan Büttcher,
and Ian MacKinnon. Novelty and diversity in information
retrieval evaluation. In SIGIR ’08: Proceedings of the 31st
annual international ACM SIGIR conference on Research

[18]

[19]

[20]
[21]

[22]

[23]

1201

and development in information retrieval, pages 659–666,
New York, NY, USA, 2008. ACM.
William S. Cooper. Expected search length: a single
measure of retrieval effectiveness based on the weak
ordering action of retrieval systems. American
Documentation, 19:30–41, 1968.
Kalervo Järvelin and Jaana Kekäläinen. IR evaluation
methods for retrieving highly relevant documents. In SIGIR
’00: Proceedings of the 23rd annual international ACM
SIGIR conference on Research and development in
information retrieval, pages 41–48, New York, NY, USA,
2000. ACM Press.
Kalervo Järvelin and Jaana Kekäläinen. Cumulated
gain-based evaluation of IR techniques. ACM Transactions
on Information Systems, 20(4):422–446, 2002.
Kalervo Järvelin, Susan L. Price, Lois M. L. Delcambre,
and Marianne Lykke Nielsen. Discounted cumulated gain
based evaluation of multiple-query ir sessions. In ECIR,
pages 4–15, 2008.
Evangelos Kanoulas, Ben Carterette, Paul Clough, and
Mark Sanderson. Session track overview. In The Nineteenth
Text REtrieval Conference Notebook Proceedings (TREC
2010), December 2010.
Evangelos Kanoulas, Ben Carterette, Paul Clough, and
Mark Sanderson. Evaluating multi-query sessions.
Submitted to the 34th international ACM SIGIR
conference on Research and development in information
retrieval (SIGIR 2011), 2011.
Alistair Moffat and Justin Zobel. Rank-biased precision for
measurement of retrieval effectiveness. ACM Trans. Inf.
Syst., 27(1):1–27, 2008.
Stephen Robertson. A new interpretation of average
precision. In SIGIR ’08: Proceedings of the 31st annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 689–690, New
York, NY, USA, 2008. ACM.
Stephen E. Robertson, Evangelos Kanoulas, and Emine
Yilmaz. Extending average precision to graded relevance
judgments. In SIGIR ’10: Proceeding of the 33rd
international ACM SIGIR conference on Research and
development in information retrieval, pages 603–610, New
York, NY, USA, 2010. ACM.
Karen Sparck Jones and C. J. van Rijsbergen. Information
retrieval test collections. Journal of Documentation,
32(1):59–75, 1976.
Jean Tague. The pragmatics of information retrieval
evaluation. pages 59–102.
Jean Tague-Sutcliffe. The pragmatics of information
retrieval experimentation, revisited. Inf. Process. Manage.,
28(4):467–490, 1992.
Jean Tague-Sutcliffe. The pragmatics of information
retrieval experimentation, revisited. Readings in
Information Retrievalnformation retrieval, pages 205–216,
1997.
Ellen M. Voorhees and Donna K. Harman. TREC:
Experiment and Evaluation in Information Retrieval. MIT
Press, 2005.
Yiming Yang and Abhimanyu Lad. Modeling expected
utility of multi-session information distillation. In Leif
Azzopardi, Gabriella Kazai, Stephen E. Robertson,
Stefan M. Rüger, Milad Shokouhi, Dawei Song, and Emine
Yilmaz, editors, ICTIR, pages 164–175, 2009.
Emine Yilmaz, Milad Shokouhi, Nick Craswell, and
Stephen E. Robertson. Incorporating user behavior
information in IR evaluation. In in Understanding the user
- Logging and interpreting user interactions in information
retrieval. Workshop in Conjunction with the ACM SIGIR
Conference on Information Retrieval, 2009.
ChengXiang Zhai, William W. Cohen, and John D.
Lafferty. Beyond independent relevance: methods and
evaluation metrics for subtopic retrieval. In SIGIR, pages
10–17, 2003.

