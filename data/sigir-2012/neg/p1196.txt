SIGIR 2012 Tutorial: Query Performance Prediction for IR
David Carmel

Oren Kurland

IBM Research - Haifa Labs
Haifa 31905, Israel

Faculty of Industrial Engineering and
Management
Technion, Haifa 32000, Israel

carmel@il.ibm.com

kurland@ie.technion.ac.il

ABSTRACT

can invoke alternative retrieval strategies for diﬀerent
queries according to their estimated diﬃculty. For example, heavy query-analysis procedures that are not
feasible for all queries due to time response restrictions, may be invoked selectively for diﬃcult queries
only.

The goal of this tutorial is to expose participants to current research on query performance prediction. Participants
will become familiar with state-of-the-art performance prediction methods, with common evaluation methodologies of
prediction quality, and with potential applications that can
utilize performance predictors. In addition, some open issues and challenges in the ﬁeld will be discussed.
This tutorial is an updated version of the SIGIR 2010
tutorial presented by David Carmel and Elad Yom-Tov on
the same subject [7]. This year we intend to expand on
new results in the ﬁeld, in particular focusing on recently
developed frameworks that provide a uniﬁed model for performance prediction.
Categories and Subject Descriptors: H.3.3 [Information Search
and Retrieval]: Retrieval models
General Terms: Algorithms, Experimentation
Keywords: query-performance prediction

1.

OBJECTIVES

Many information retrieval (IR) systems suﬀer from a
radical variance in performance when responding to users’
queries. Even for systems that succeed very well on average, the quality of results returned for some of the queries
is poor. Thus, it is desirable that IR systems will be able
to identify “diﬃcult” queries in order to handle them properly. Understanding why some queries are inherently more
diﬃcult than others is essential for IR, and a good answer
to this important question will help search engines to reduce the variance in performance, hence serving better their
customer needs.
The high variability in query performance has driven a
new research direction in the IR ﬁeld on estimating the expected quality of the search results, i.e., the query diﬃculty,
when no relevance feedback is given. Such an estimation is
beneﬁcial for many reasons:

• As feedback to the system administrator: The administrator can identify queries related to a speciﬁc subject that are “diﬃcult” for the search engine, and to
expand the collection of documents to better answer
poorly covered subjects. Identifying missing content
queries is especially important for commercial search
engines which should better identify, as soon as possible, popular emerging user needs that cannot be answered appropriately due to missing relevant content.
• For IR applications. For example, diﬃculty estimation can be used by a distributed search application as
a method for merging the results retrieved from diﬀerent datasets by weighing the results according to their
estimated quality.

• As feedback to the users: The IR system can provide
the users with an estimate of the expected quality of
the results retrieved for their queries. The users can
then rephrase queries that were found to be “diﬃcult”,
or alternatively, resubmit the “diﬃcult” query to alternative search resources.

Estimating the query diﬃculty is a signiﬁcant challenge
due to the numerous factors that impact retrieval performance. Many prediction methods have been proposed throughout the last years. However, as many researchers observed,
the prediction quality of state-of-the-are predictors is still
too low to be widely used by IR applications. The low prediction quality is due to query ambiguity, missing content,
vocabulary mismatch, and many other factors. This complexity burdens the estimation task and calls for new prediction methods that will be able to cope with the complex
prediction challenge.
In this tutorial we will ﬁrst discuss the reasons that cause
search engines to fail for some of the queries. Then, we will
overview several state-of-the-art approaches for estimating
query diﬃculty. We will also describe common methodologies for evaluating the prediction quality of query-performance
prediction methods, and present some experimental results
for the prediction quality of some predictors, as measured
over several TREC’s benchmarks. We will cover a few potential applications that can utilize query diﬃculty estimators. Finally, we will summarize with a discussion on open
issues and challenges in the ﬁeld.

• As feedback to the search engine: The search engine

2. PRESENTERS:

Copyright is held by the author/owner(s).
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
ACM 978-1-4503-1472-5/12/08.

David Carmel. is a Research Staﬀ Member at the Information Retrieval group at IBM Haifa Research Lab. David’s

1196

research is focused on search in the enterprise, query performance prediction, social search, and text mining. David has
published more than 80 papers in IR and Web journals and
conferences, organized a number of workshops, and taught
several tutorials at SIGIR, and WWW. David is co-author
of the book “Estimating the Query Diﬃculty for Information Retrieval”. David earned his PhD in Computer Science
from the Technion – Israel Institute of Technology in 1997.

[15] Ronan Cummins, Joemon M. Jose, and Colm O’Riordan.
Improved query performance prediction using standard
deviation. In Proceedings of SIGIR, pages 1089–1090, 2011.
[16] Ronan Cummins, Mounia Lalmas, and Colm O’Riordan. The
limits of retrieval eﬀectiveness. In Proceedings of ECIR, pages
277–282, 2011.
[17] Fernando Diaz. Performance prediction using spatial
autocorrelation. In Proceedings of SIGIR, pages 583–590, 2007.
[18] Donna Harman and Chris Buckley. The NRRC reliable
information access (RIA) workshop. In Proceedgins of SIGIR,
pages 528–529, 2004.
[19] Claudia Hauﬀ, Leif Azzopardi, and Djoerd Hiemstra. The
combination and evaluation of query performance prediction
methods. In Proceedings of ECIR, pages 301–312, 2009.
[20] Claudia Hauﬀ, Djoerd Hiemstra, and Franciska de Jong. A
survey of pre-retrieval query performance predictors. In
Proceedings of CIKM, pages 1419–1420, 2008.
[21] Claudia Hauﬀ, Diane Kelly, and Leif Azzopardi. A comparison
of user and system query performance predictions. In
Proceedings of CIKM, pages 979–988, 2010.
[22] Claudia Hauﬀ, Vanessa Murdock, and Ricardo Baeza-Yates.
Improved query diﬃculty prediction for the web. In
Proceedings of CIKM, pages 439–448, 2008.
[23] Ben He and Iadh Ounis. Inferring query performance using
pre-retrieval predictors. In Proceedings of SPIRE, pages 43–54,
2004.
[24] Oren Kurland, Anna Shtok, David Carmel, and Shay Hummel.
A uniﬁed framework for post-retrieval query-performance
prediction. In Proceedings of ICTIR, pages 15–26, 2011.
[25] Josiane Mothe and Ludovic Tanguy. Linguistic features to
predict query diﬃculty. In ACM SIGIR 2005 Workshop on
Predicting Query Diﬃculty - Methods and Applications, 2005.
[26] Joaquı́n Pérez-Iglesias and Lourdes Araujo. Standard deviation
as a query hardness estimator. In Proceedings of SPIRE, pages
207–212, 2010.
[27] Daniel Sheldon, Milad Shokouhi, Martin Szummer, and Nick
Craswell. Lambdamerge: merging the results of query
reformulations. In Proceedings of WSDM, pages 795–804, 2011.
[28] Anna Shtok, Oren Kurland, and David Carmel. Predicting
query performance by query-drift estimation. In Proceedings of
ICTIR, pages 305–312, 2009.
[29] Anna Shtok, Oren Kurland, and David Carmel. Using statistical
decision theory and relevance models for query-performance
prediction. In Proccedings of SIGIR, pages 259–266, 2010.
[30] Natali Soskin, Oren Kurland, and Carmel Domshlak.
Navigating in the dark: Modeling uncertainty in ad hoc
retrieval using multiple relevance models. In Proceedings of
ICTIR, pages 79–91, 2009.
[31] Stephen Tomlinson. Robust, Web and Terabyte Retrieval with
Hummingbird Search Server at TREC 2004. In Proceedings of
TREC-13, 2004.
[32] Vishwa Vinay, Ingemar J. Cox, Natasa Milic-Frayling, and
Kenneth R. Wood. On ranking the eﬀectiveness of searches. In
Proceedings of SIGIR, pages 398–404, 2006.
[33] E. M. Voorhees. Overview of the TREC 2003 robust retrieval
track. In Proceedings of TREC-12, 2003.
[34] E. M. Voorhees. Overview of the TREC 2004 robust retrieval
track. In Proceedings of TREC-13, 2004.
[35] Mattan Winaver, Oren Kurland, and Carmel Domshlak.
Towards robust query expansion: Model selection in the
language model framework to retrieval. In Proceedings of
SIGIR, pages 729–730, 2007.
[36] Elad Yom-Tov, Shai Fine, David Carmel, and Adam Darlow.
Learning to estimate query diﬃculty: including applications to
missing content detection and distributed information retrieval.
In Proceedings of SIGIR, pages 512–519, 2005.
[37] Yun Zhou and W. Bruce Croft. Ranking robustness: a novel
framework to predict query performance. In Proceedgins of
CIKM, pages 567–574, 2006.
[38] Yun Zhou and W. Bruce Croft. Query performance prediction
in web search environments. In Proceedings of SIGIR, pages
543–550, 2007.

Oren Kurland. is a Senior Lecturer in the Faculty of Industrial Engineering and Management at the Technion, Israel
Institute of Technology. The information retrieval research
group that Oren leads at the Technion focuses on developing formal models for information retrieval. Oren published
more than 30 peer-reviewed papers in IR conferences and
journals. He served as a senior program committee member
(area chair) for the SIGIR and CIKM conferences. Oren also
serves on the editorial boards of the Journal of Information
Retrieval and the Journal of Artiﬁcial Intelligence Research
(JAIR).

3.

REFERENCES

[1] Giambattista Amati, Claudio Carpineto, and Giovanni
Romano. Query diﬃculty, robustness, and selective application
of query expansion. In Proceedings of ECIR, pages 127–137,
2004.
[2] Javed A. Aslam and Virgiliu Pavlu. Query hardness estimation
using Jensen-Shannon divergence among multiple scoring
functions. In Proceeding of ECIR, pages 198–209, 2007.
[3] Niranjan Balasubramanian and James Allan. Learning to select
rankers. In Proceedings of SIGIR, pages 855–856, 2010.
[4] Niranjan Balasubramanian, Giridhar Kumaran, and Vitor R.
Carvalho. Exploring reductions for long web queries. In
Proceedings of SIGIR, pages 571–578, 2010.
[5] Niranjan Balasubramanian, Giridhar Kumaran, and Vitor R.
Carvalho. Predicting query performance on the web. In
Proceedigns of SIGIR, pages 785–786, 2010.
[6] David Carmel and Elad Yom-Tov. Estimating the Query
Diﬃculty for Information Retrieval. Synthesis Lectures on
Information Concepts, Retrieval, and Services. Morgan &
Claypool Publishers, 2010.
[7] David Carmel and Elad Yom-Tov. Estimating the query
diﬃculty for information retrieval. In Proceedings of the 33rd
international ACM SIGIR conference on Research and
development in information retrieval, SIGIR ’10, pages
911–911, New York, NY, USA, 2010. ACM.
[8] David Carmel, Elad Yom-Tov, Adam Darlow, and Dan Pelleg.
What makes a query diﬃcult? In Proceedings of SIGIR, pages
390–397, 2006.
[9] David Carmel, Elad Yom-Tov, and Haggai Roitman. Enhancing
digital libraries using missing content analysis. In Proceedings
of JCDL, pages 1–10, 2008.
[10] Kevyn Collins-Thompson and Paul N. Bennett. Predicting
query performance via classiﬁcation. In Proceedings of ECIR,
pages 140–152, 2010.
[11] Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft.
Predicting query performance. In Proceedings of SIGIR, pages
299–306, 2002.
[12] Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft. A
language modeling framework for selective query expansion.
Technical Report IR-338, Center for Intelligent Information
Retrieval, University of Massachusetts, 2004.
[13] Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft.
Precision prediction based on ranked list coherence.
Information Retrieval, 9(6):723–755, 2006.
[14] Ronan Cummins. Predicting query performance directly from
score distributions. In Proceedings of AIRS, pages 315–326,
2011.

1197

