Category Hierarchy Maintenance: a Data-Driven Approach
Quan Yuan‚Ä† , Gao Cong‚Ä† , Aixin Sun‚Ä† , Chin-Yew Lin‚Ä° , Nadia Magnenat-Thalmann‚Ä†
‚Ä†

School of Computer Engineering, Nanyang Technological University, Singapore 639798
{qyuan1@e., gaocong@, axsun@, nadiathalmann@}ntu.edu.sg
‚Ä°
Microsoft Research Asia, Beijing, China 100080
{cyl@microsoft.com}

ABSTRACT

browsing at many news portal websites. Figure 1 shows a small
portion of Yahoo! Answers hierarchy. Questions in the same category are usually relevant to the same topic.
Hierarchy enables not only easy document browsing, but also
searching of the documents within user deÔ¨Åned categories or subtrees of categories. Additionally, hierarchy information can be utilized to enhance retrieval models to improve the search accuracy [4].
On the other hand, users‚Äô information needs can only be satisÔ¨Åed
with the documents accessible through the hierarchy (but not the
category hierarchy itself). That is, the usefulness of a hierarchy
heavily relies on the eÔ¨Äectiveness of the hierarchy in properly organizing the existing data, and more importantly accommodating
the newly available data into the hierarchy. Given the fast growth
of text data, continuously accommodating large volume of newly
available text data into a hierarchy is nontrivial. Automatic text
classiÔ¨Åcation techniques are often employed for eÔ¨Écient categorization of newly available documents into category hierarchies.
However, hierarchy often evolves at a much slower pace than its
documents. Two major problems often arise after adding many
documents into a hierarchy after some time.

Category hierarchies often evolve at a much slower pace than the
documents reside in. With newly available documents kept adding
into a hierarchy, new topics emerge and documents within the same
category become less topically cohesive. In this paper, we propose a novel automatic approach to modifying a given category
hierarchy by redistributing its documents into more topically cohesive categories. The modiÔ¨Åcation is achieved with three operations
(namely, sprout, merge, and assign) with reference to an auxiliary
hierarchy for additional semantic information; the auxiliary hierarchy covers a similar set of topics as the hierarchy to be modiÔ¨Åed.
Our user study shows that the modiÔ¨Åed category hierarchy is semantically meaningful. As an extrinsic evaluation, we conduct experiments on document classiÔ¨Åcation using real data from Yahoo!
Answers and AnswerBag hierarchies, and compare the classiÔ¨Åcation accuracies obtained on the original and the modiÔ¨Åed hierarchies. Our experiments show that the proposed method achieves
much larger classiÔ¨Åcation accuracy improvement compared with
several baseline methods for hierarchy modiÔ¨Åcation.

Categories and Subject Descriptors

‚Ä¢ Structure Irrelevance. A category hierarchy may well reÔ¨Çect
the topical distribution of its data at the time of construction.
However, as new topics always emerge from the newly coming documents, there is no proper category in the hierarchy to
accommodate these new documents, leading to putting these
documents in less relevant categories. As the result, some
categories contain less topically cohesive documents. Moreover, some categories become less discriminative with respect to the current data distribution. One example is the two
categories Printers and Scanners in YA, for there emerged
many questions about multi-functional devices which are related to both printers and scanners, leading to ambiguity between these two categories.

H.3.3 [Information Storage and Retrieval]: Information Filtering

Keywords
Category Hierarchy, Hierarchy Maintenance, ClassiÔ¨Åcation

1. INTRODUCTION
With the exponential growth of textual information accessible,
category hierarchy becomes one of the most widely-adopted and
eÔ¨Äective solutions in organizing large volume of documents. Hierarchy provides an organization of data by diÔ¨Äerent levels of abstraction, in which each node (or category) represents a topic that is
shared by the data in it. The connection between two nodes denotes
supertype-subtype relation. Examples include Web directories provided by Yahoo! and Open Directory Project (ODP), hierarchies
for community-based question-answering services by Yahoo! Answers (YA) and AnswerBag (AB), product hierarchies by online
retailers like Amazon and eBay, as well as the hierarchies for news

‚Ä¢ Semantics Irrelevance. Semantics may change over time which
calls for a better organization of the documents [17]. For instance, when creating the hierarchy, experts are more likely
to put category Petroleum under Geography. However, after
the disaster of BP Gulf Oil Spill, a lot of news articles in category Petroleum are about the responsibility of the Obama
Administration. These documents have stronger connection
to category Politics than Geography. It is therefore more reasonable to put these documents under Politics for better document organization.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proÔ¨Åt or commercial advantage and that copies
bear this notice and the full citation on the Ô¨Årst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc
permission and/or a fee.
SIGIR‚Äô12, August 12‚Äì16, 2012, Portland, Oregon, USA.
Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

These two problems not only hurt user experiences in accessing
information through the hierarchy, but also result in poorer classiÔ¨Åcation accuracy for the classiÔ¨Åers categorizing newly available
documents because of the less topically cohesive categories [15].

791



 

  



 

 



 

categories in the modiÔ¨Åed hierarchy. The assign operation rebuilds
the parent-child relations in the modiÔ¨Åed hierarchy. The category
labels in the modiÔ¨Åed hierarchy are either borrowed from or generated based on both the original and the auxiliary hierarchies. We
emphasize that the reuse of category labels from original and auxiliary hierarchies largely ensures semantically meaningful category
labels in the modiÔ¨Åed hierarchy. When such an auxiliary hierarchy is unavailable, the given hierarchy can be used as an auxiliary
hierarchy. Because of the three operations (i.e., sprout, merged,
and assign), we name our approach the SMA approach. The main
contributions are summarized as follows.
1) We propose a novel data-driven approach SMA to automatically modify a category hierarchy making it better reÔ¨Çect the topics
of its documents. The proposed approach exploits the semantics of
the given hierarchy and an auxiliary hierarchy, to guide the modiÔ¨Åcation of the given hierarchy.
2) We evaluate the proposed approach using data from three realworld hierarchies, Yahoo! Answers, Answerbag, and ODP. The
user study shows that the modiÔ¨Åed hierarchy Ô¨Åts with the data better than the original one does. As we argue that the categories in
the modiÔ¨Åed hierarchy are more topically cohesive compared to
the original hierarchy, we employ text classiÔ¨Åcation as an extrinsic evaluation. Our experimental results show that the classiÔ¨Åers
trained on the modiÔ¨Åed hierarchy achieve much higher classiÔ¨Åcation accuracy (measured by both macro-F1 and micro-F1 ), than the
classiÔ¨Åer built on the original hierarchy, or the classiÔ¨Åers modeled
on the hierarchies generated by three baseline methods, including
the state-of-the-art method in [17] and the hierarchy generation
method in [2].
The rest of this paper is organized as follows. Section 2 surveys
the related work. We describe the research problem and overview
the proposed approach in Section 3. The three operations are detailed in Section 4. The experimental evaluation and discussion of
the results are presented in Section 5. Finally, we conclude this
paper in Section 6.

 



 

 

Figure 1: Portion of Yahoo! Answers Hierarchy
Consequently, the poorer classiÔ¨Åcation accuracy further hurts user
experience in browsing and searching documents through the hierarchy. This calls for category hierarchy maintenance, a task to
modify the hierarchy to make it better reÔ¨Çect the topics of its documents, which in turn would improve the classiÔ¨Åcation accuracy.
Although a category hierarchy is relatively stable, many websites
have modiÔ¨Åed or adjusted their hierarchies in the past. In May
2007, Yahoo! Answers added a new category Environment into her
hierarchy, and added several categories like Facebook and Google
under Internet later. By comparison, eBay adjusted her hierarchy
more frequently, because there always emerge new types of items,
like tablets and eReaders.
Hierarchy modiÔ¨Åcation is nontrivial. Manual modiÔ¨Åcation of
category hierarchy is a tedious and diÔ¨Écult task, because it is hard
to detect the semantic changes as well as the newly emerged topics.
This motivates the data-driven automatic modiÔ¨Åcation of a given
hierarchy to cope with semantic changes and newly emerged topics.
This is a challenging task because of at least two reasons, among
others. First, the resultant modiÔ¨Åed category hierarchy (hereafter
called modiÔ¨Åed hierarchy for short) should largely retain the semantics of the existing hierarchy and keep its category labels semantically meaningful. Second, the categories in the modiÔ¨Åed hierarchy shall demonstrate much higher topical cohesiveness, which
in turn enables better classiÔ¨Åcation accuracy in putting new documents into the modiÔ¨Åed hierarchy.
To the best of our knowledge, very few work has addressed the
hierarchy modiÔ¨Åcation problem (see Section 2). Tang et al. propose
a novel approach to modifying the relations between categories
aiming to improve the classiÔ¨Åcation accuracy [17]. However, their
proposed method does not change the leaf categories of the given
hierarchy, and thus cannot solve the aforementioned problems. For
example, the method may move the leaf category Petroleum to be
child category of Politics. However, it is more reasonable to partition the documents in Petroleum into two categories: one being
the child category of Geography, and the other child category of
Politics. The method [17] fails to do so since it is unable to detect
the newly emerged hidden topic "Petroleum politics".
In this paper, we propose a data-driven approach to modify a
given hierarchy (also called as the original hierarchy) with reference to an auxiliary hierarchy using three operations (namely,
sprout, merge, and assign). An auxiliary hierarchy is a category
hierarchy that covers a similar set of topics as does the given hierarchy (e.g., the Yahoo! hierarchy and ODP can be used as auxiliary
hierarchy to each other). Similar to the concept of bisociation [8],
our approach discovers Ô¨Åner and more elaborate categories (also
known as hidden topics) by projecting the documents in the given
hierarchy to the auxiliary hierarchy. This operation, similar to a
cross-product operation between the categories from the given hierarchy and the categories from the auxiliary hierarchy, is named
sprout 1 . The similar hidden topics are then merged to form new

2.

RELATED WORK

Hierarchy Generation. Hierarchy generation focuses on extracting a hierarchical structure from a set of categories, each containing
a set of documents. The generation process can be either fully automatic [2, 5, 11] or semi-automatic [1, 7, 22]. The semi-automatic
approaches involve interaction with domain experts in the hierarchy
generation process. In the following, we review the fully automatic
approaches in more detail.
Aggarwal et al. use the category labels of documents to supervise hierarchy generation [2]. They Ô¨Årst calculate the centroids of
all categories and use them as the initial seeds. Similar categories
are merged and clusters with few documents are discarded. The
process is iterated to build the hierarchy. User study is employed to
evaluate the quality of the generated hierarchy.
Punera et al. utilizes a divisive hierarchical clustering approach,
which Ô¨Årst splits the given set of categories into two sets of categories, and each such set is partitioned recursively until it contains
only one category [11].
An algorithm for generating hierarchy for short text is proposed
by Chuang et al. [5]. They Ô¨Årst create a binary-tree hierarchy by
hierarchical agglomerative clustering, and then construct a multiway-tree hierarchy from the binary-tree hierarchy. They use both
classiÔ¨Åcation measurement and user evaluation to evaluate the generated hierarchy.
Recently, Qi et al. employ genetic algorithms to generate hierarchy [12]. Given a set of leaf categories, a group of hierarchies
are randomly generated as seeds, and genetic operators are applied

1
We would like to thank an anonymous reviewer for suggesting the connection with
bisociation [8] and the name sprout

792

to each hierarchy to generate new ones. The newly generated hierarchies are evaluated and the hierarchies with poor classiÔ¨Åcation
accuracy are removed. The process is repeated until the classiÔ¨Åcation accuracy is not improved.
DiÔ¨Äerent from hierarchy generation which assumes a set of categories as input, our hierarchy modiÔ¨Åcation method takes a hierarchy as the input. Hierarchy generation does not change the given
categories hence it cannot solve the structure irrelevance problem.


 





  








Figure 2: Overview of SMA
Algorithm 1: SMA algorithm for hierarchy modiÔ¨Åcation
Input: Hc : category hierarchy to be modiÔ¨Åed
Ha : auxiliary hierarchy
Œª : minimum coverage ratio
Œ∏ : maximum loss ratio
Output: Hn : modiÔ¨Åed category hierarchy

Hierarchy ModiÔ¨Åcation. Tang et al. present a method of modifying a hierarchy to improve the classiÔ¨Åcation accuracy [17]. The
method introduces three operations. The promote operation lifts a
category to upper level; the merge operation generates a new parent for a category and its most similar sibling; the demote operation either demotes a category as a child of its most similar sibling,
or makes the sibling a child of the category. For each category
in the given hierarchy, promote operation is tested, followed by
merge and demote operations, in a top-down manner. The operation comes into eÔ¨Äect if it can improve the classiÔ¨Åcation accuracy.
The approach iterates the process until no improvement can be observed or some criterion is met. In experiments, this method outperforms clustering-based hierarchy generation method in terms of
classiÔ¨Åcation accuracy. However, this method does not change the
leaf categories, leaving the topically incohesive leaf categories untouched. In addition, the method [17] has a high time-complexity.
Due to the high time complexity of the method [17], Kiyoshi et al.
propose an approach [10] to addressing the eÔ¨Éciency issue.

1
2
3
4
5
6
7
8
9
10
11

Discussion. With the existing work on either hierarchy generation
or hierarchy modiÔ¨Åcation, the leaf categories in the modiÔ¨Åed hierarchy (i.e., either generated or modiÔ¨Åed) remain unchanged. Clearly,
without changing leaf categories, the topical incohesiveness among
documents in the same leaf category remains unaddressed. Consequently, the likely poorer classiÔ¨Åcation accuracy for these topically
incohesive categories results in poorer document organization in
the hierarchy. In this paper, we therefore propose an automatic approach to modify a given hierarchy where the leaf categories could
be split or merged so as to better reÔ¨Çect the topics of the documents
in the hierarchy.

Hn ‚Üê Hc ;
h ‚Üê number of levels of Hc ;
foreach Level  from 2 to h of Hn do
foreach Category Ci of Hn on level  do
Œ¶Ci ‚Üê ProjectedCategories(Ci , Ha , Œª , Œ∏ );
Sprout(Ci , Œ¶Ci , Hn , Ha );
n ‚Üê number of categories on level  of Hc ;
Merge(, n , Hn );
Assign(, Hn );
Hn ‚ÜêRelabel(Hc , Ha , Hn );
return Hn

erarchy during the modiÔ¨Åcation process, and Ha is the auxiliary
hierarchy.
Auxiliary Hierarchy. BrieÔ¨Çy introduced in Section 1, an auxiliary
hierarchy Ha is a hierarchy covering similar topics as the given hierarchy Hc . For example, Yahoo! hierarchy and ODP hierarchy
can be auxiliary hierarchy to each other. Similarly, Yahoo! Answers and AnswerBag can be auxiliary hierarchy to each other.
The auxiliary hierarchy Ha plays an essential role in Ô¨Ånding hidden topics. Note that the hidden topics are not readily present in
the auxiliary hierarchy, and our approach does not simply use the
structure of auxiliary hierarchy as part of the modiÔ¨Åed hierarchy.
Instead, they contain semantics from both the original hierarchy
and the auxiliary hierarchy. We use the following example to illustrate. Suppose that the original hierarchy has two categories,
Action movie and Comedy movie, and the auxiliary hierarchy contains two categories America and Asia. Our approach will Ô¨Ånd that
Action movie has two hidden topics, namely American action movie
and Asian active movie; Comedy movie also has two hidden topics,
namely American comedy movie and Asian comedy movie.
The auxiliary hierarchy also plays an important role in guiding
the merge operation, which is to merge similar hidden topics to
generate the categories of the modiÔ¨Åed hierarchy. Continue with
the earlier example, after merging the generated hidden topics, we
may get new categories‚ÄìAmerican movie and Asian movie (if "action vs. comedy" is evaluated to be less discriminative compared
with "American vs. Asian"). The semantics of the hierarchy to
be modiÔ¨Åed, together with the semantics of the auxiliary hierarchy,
will be exploited to deÔ¨Åne the similarity between hidden topics.
Validated in our experiments (Section 5), our approach is equally
applicable when the original hierarchy Hc is used as the auxiliary
hierarchy to itself.

3. SMA APPROACH OVERVIEW
We observe that each category in a hierarchy may contain several "hidden topics", each of which is topically cohesive, e.g., category Computer contains hidden topics like Internet Programming,
Operating Systems, etc. With more documents adding to a category hierarchy, new "hidden topics" emerge within a single category leading to topical incohesiveness among its documents (see
Section 1). Our proposed approach, therefore, aims to Ô¨Ånd the hidden topics within each category and then sprout categories based
on its hidden topics, merge similar hidden topics to form new categories, and then assign the parent-child relation among categories.
We name our approach SMA after its three major operations.
The key challenges in the approach include: (i) How to detect
the "hidden topics" at the appropriate granularity? (ii) How to evaluate the similarity between "hidden topics"? and (iii) How to assign the parent-child relation between the unmodiÔ¨Åed and modiÔ¨Åed
categories? Further, recall from Section 1, the modiÔ¨Åed hierarchy
has to largely retain the semantics of the existing hierarchy, with
meaningful category labels and topically cohesive categories. In
the following, we give a high-level overview of the SMA approach
and then detail the three major operations in the next Section.
The framework of our SMA algorithm is illustrated in both Figure 2 and Algorithm 1, where Hc is the category hierarchy to be
modiÔ¨Åed, Hn is the modiÔ¨Åed hierarchy, Hn is the intermediate hi-

Algorithm Overview. Shown in Figure 2 and Algorithm 1, Hn
is Ô¨Årst initialized to Hc (line 1). In a top-down manner, the SMA
algorithm modiÔ¨Åes the hierarchy level by level. Note that the root

793

ments projected from category Ci to category Ca by œÄ (Ci ‚Üí Ca ). If
Ca is a leaf category, then œÄ (Ci ‚Üí Ca ) denotes the set of documents
from Ci that are projected into Ca ; if Ca is a non-leaf category, then
œÄ (Ci ‚Üí Ca ) denotes the set of documents projected into any of the
descendent leaf categories of Ca in Ha .

category is the only category at level 1. Starting from level 2, for
each category Ci in this level, the documents contained in Ci is projected to the auxiliary hierarchy Ha . A set of categories from Ha
each of which contains a reasonable number of documents originally from Ci is identiÔ¨Åed to represent Ci ‚Äôs hidden topics (line 5).
The two parameters, minimum coverage ratio Œª and maximum loss
ratio Œ∏ , adjust the number of hidden topics. New Ô¨Åner categories
are then sprouted from Ci according to the hidden topics and the
documents in category Ci are assigned to these Ô¨Åner categories (or
hidden topics) (line 6). Given the expected number of categories
n on level  (line 7), the merge operation forms n number of new
categories on level  of the intermediate hierarchy Hn (line 8). If
the current level is not the lowest level in the hierarchy, the parentchild relations between the modiÔ¨Åed categories and the unmodiÔ¨Åed
categories on the next level are assigned (line 9). The last step in
the SMA algorithm is to generate category labels with reference to
both the original and auxiliary hierarchies (line 10).

4.

Candidate Topic Tree. Based on the projection, we select categories from Ha to represent the hidden topics of Ci . A selected
category can be either a leaf category or a non-leaf category. Before describing the selection process, we introduce the notions of
major category and minor category. Let Œª denote the minimum
coverage ratio parameter.
Definition 1 (Major Category). A category Ca from Ha is a
major category for category Ci if |œÄ (Ci ‚Üí Ca )|/|Ci | ‚â• Œª .
Definition 2 (Minor Category). A category Ca from Ha is a
minor category for category Ci if |œÄ (Ci ‚Üí Ca )|/|Ci | < Œª .


SPROUT, MERGE, AND ASSIGN

We detail the three operations to address the challenges in the
SMA framework (i.e., to identify hidden topics, evaluate the similarity between hidden topics, and assign the parent-child relation).

4.1

 



 



  

 
 

Sprout Operation

The sprout operation Ô¨Årst discovers the hidden topics for the documents in a category Ci and then sprouts the category. Without loss
of generalization, a leaf category is represented by all documents
belonging to the category; a non-leaf category is represented by all
documents belonging to any of its descendent categories.

4.1.1


  

 



 
 

 

 ! "
#$ %




  &
" 
 

 # 
 
 ' "'  $  !
%
!(!"' 

Figure 3: Generate candidate topic tree for Ci using Ha
For example, suppose Œª = 15%. As shown in Figure 3, category
Ci is projected to the categories in Ha . In the left tree, in which each
number besides a node represents the percentage of documents of
Ci projected to the node, the nodes in dark color are major categories while the others are minor categories.
Naturally, only the major categories are considered candidate
categories to represent hidden topics of Ci because a good number of documents in Ci are projected into them. However, not all
major categories need to be selected because of two reasons. First,
let Cp be the parent of a major category Ca . By deÔ¨Ånition, the parent
of a major category is also a major category. Selecting both Ca and
Cp would lead to semantic overlap. Second, assume all Cp ‚Äôs other
child categories are minor categories, but altogether those minor
categories contain a large number of documents. Then selecting Ca
but not Cp would lead to a signiÔ¨Åcant loss of documents from Ci
(hence semantic loss). We therefore deÔ¨Åne the notion of loss ratio.

Discovery of Hidden Topics

Ideally, for a category we Ô¨Ånd a set of its hidden topics, which are
comprehensive and cohesive, and have no overlap. This is however
a challenging task. We proceed to give an overview of the proposed
method. Given a category Ci in the intermediate hierarchy Hn during the modiÔ¨Åcation process (see Algorithm 1), we assign all its
documents into the categories of the auxiliary hierarchy Ha , and
get a set of candidate categories from Ha in a tree-structure. Each
candidate category contains a number of documents from Ci . Then,
with the consideration of both cohesion and separation, we select a
set of categories from the tree as hidden topics. The selection process is modeled as an optimization problem. We now elaborate the
details.
Document Projection. To assign documents from Ci to Ha , we
represent a document by its word feature vector, and a category
in Ha by its centroid vector. Based on cosine similarity between
the document and the centroids, we recursively assign each document d ‚àà Ci to Ha from its root to a leaf category along a single
path of categories. If a good number of documents from Ci are assigned to a category Ca in Ha , then the topic of Ca is relevant to
Ci , and the semantics of Ca can be used to describe a hidden topic
of Ci . Thus, multiple categories in Ca can be identiÔ¨Åed to describe
all hidden topics of Ci . For example, large number of documents
from category Programming assigned into two categories Security
and Network in an auxiliary hierarchy, implies that Programming
has two hidden topics: Network Programming and Security Programming. We have also tried to build a classiÔ¨Åer on Ha to assign
documents from Ci to Ha using Naive Bayes and support vector
machine, respectively, and the set of generated hidden topics is almost the same.
The process of assigning documents from a category Ci in Hn to
categories in Ha is called projection. We denote the set of docu-

Definition 3 (Loss Ratio). The loss ratio of a leaf category is
Œº
deÔ¨Åned as 0. For a non-leaf category Ca , let Ca be the set of minor
categories among Ca ‚Äôs child categories. The loss ratio of Ca with
respect to Ci , the category being projected, is the ratio between
the projected documents in all its child minor categories and Ca ‚Äôs
projected documents, i.e.,

Œº
C ‚ààCa

|œÄ (Ci ‚ÜíC )|

|œÄ (Ci ‚ÜíCa )|

.

We set a threshold maximum loss ratio Œ∏ . After projecting documents from Ci to categories in Ha , we only keep the major categories whose parent‚Äôs loss ratio is smaller than Œ∏ . Note that, if a
non-leaf category is not selected in the above process, the subtree
rooted at this category is not selected. After the selection, we obtain
a sub-hierarchy from Ha containing only eligible major categories,
which is called candidate topic tree for Ci , denoted by TCi .
For example, suppose Œ∏ = 30%. The candidate topic tree for Ci
is shown on the right hand side of Figure 3. Although node C5 is
a major category, it is not part of the candidate topic tree since the
loss ratio ((10%+10%)/50% = 40%) of its parent node C3 is larger
than Œ∏ .

794

Hidden Topic Selection. We next present how to choose a set of
nodes from TCi to represent hidden topics of Ci . Ideally, we expect
the hidden topics to be comprehensive but not overlap with each
other. Hence, we use tree-cut to deÔ¨Åne the selection [18].

Procedure ProjectedCategories
Input: Ci : the category to be sprouted
Ha : the auxiliary hierarchy
Œª : minimum coverage ratio
Œ∏ : maximum loss ratio
Result: Œ¶Ci []: the list of projected categories for Ci

Definition 4 (Tree-Cut). A tree-cut is a partition of a tree. It
is a list of nodes in the tree, and each node represents a set of all
leaf nodes in a subtree rooted by the node. The sets in a tree-cut
exhaustively cover all leaf nodes of the tree, and they are mutually
disjoint.

1
2
3
4
5
6

There exist many possible tree-cuts for TCi to generate hidden
topics. Two example tree cuts for the candidate topic tree in Figure 3 are {C1 , C2 } and {C1 , C3 }. Among all possible tree-cuts, we aim
to choose the tree-cut such that each resultant hidden topic (category) is cohesive and well separated from other categories in the
tree-cut. In the following, we prove that the tree-cut containing
only leaf nodes of the candidate topic tree satisÔ¨Åes this requirement. Note that a leaf node in TCi is not necessary a leaf category
in Ha . For example, in Figure 3, C3 is leaf node of the candidate
topic tree, but not a leaf category in the auxiliary hierarchy.
We proceed to show the proof. We Ô¨Årst deÔ¨Åne the Sum of Square
Error (SSE) of cohesion for a category Ca .

(d ‚àí ca )2 ,
SSE(Ca ) =

7
8
9
10
11
12
13
14
15

Œ¶Ci [] ‚Üê {root category of Ha };
repeat
Ca ‚Üê Œ¶Ci [].getNextUnprocessedCategory();
C_List[] ‚Üê child categories of Ca ;
M_List[] ‚Üê {};
foreach Category C of C_List[] do
if |œÄ (C|Ci ‚ÜíC)|
‚â• Œª then
i|
M_List[].add(C);
Œ£

|œÄ (C ‚ÜíC)|

i
if 1 ‚àí C‚ààM_List[]
< Œ∏ then
|œÄ (Ci ‚ÜíCa )|
Œ¶Ci [].add(M_List[]);
Œ¶Ci [].remove(Ca );

else
mark Ca as processed
until No more unprocessed category in Œ¶Ci []
return Œ¶Ci []

d‚ààCa

where d is a document and ca is the centroid of category Ca .
Given a set of categories {Ca } (1 ‚â§ a ‚â§ k), the Total-SSE and
Total Sum of Square Between
(Total-SSB), denoted

 by ŒµE and ŒµB
respectively, are ŒµE = ka=1 SSE(Ca ) and ŒµB = ka=1 |Ca |(c ‚àí ca )2 ,
where c is the centroid of documents in all categories {Ca }. It is
veriÔ¨Åed that, given a set of documents,
the sum of ŒµE and ŒµB is a
 
constant value [16]: ŒµE +ŒµB = ka=1 d‚ààCa (d‚àíc)2 . Thus, maximizing
separation is equivalent to minimizing cohesion error. We therefore
formulate the problem of selecting categories from TCi to represent
hidden topics for category Ci as following:

SSE(Ca ), where S is a tree-cut on TCi . (1)
Œ¶Ci = arg min
S

Lemma 1 enables us to use an eÔ¨Écient method to solve Eq.1
as follows. According to Lemma 1, specializing a category by its
child categories can reduce Total-SSE. That is, among all possible
tree-cuts in TCi , the cut that contains only leaf categories has the
minimum value of Total-SSE.
In summary, for a category Ci in Hn , we build a candidate topic
tree TCi and the leaf nodes of TCi are used to represent the hidden
topics of Ci . The pseudocode is given in Procedure 2 ProjectedCategories. As discussed, according to Lemma 1, we only need
the leaf nodes of the candidate topic tree TCi as the result. Instead
of explicitly building TCi and then Ô¨Ånding TCi ‚Äôs tree cut containing
only leaf nodes, we Ô¨Ånd TCi ‚Äôs leaf nodes directly in our procedure.
More speciÔ¨Åcally, we start from the root category of Ha in a topdown manner (the root node is a major category by deÔ¨Ånition as its
coverage ratio is 1). Each time we get a unprocessed categories Ca
from the list of projected categories Œ¶Ci [], and check its child categories (lines 3-4). The major categories among the child categories
are put into a major category list (lines 6-8) for further testing on
loss ratio. If the loss ratio of Ca is smaller than maximum loss ratio
Œ∏ , then Ca is replaced by its child major categories (lines 9-11); otherwise, Ca is selected as a candidate category (line 13). We iterate
the process until all major categories are processed (line 14).

Ca ‚ààS

This problem can be reduced to the maximum Ô¨Çow problem by
viewing TCi as a Ô¨Çow network. Thus, it can be solved directly by
Ford-Fulkerson method [6]. However, its complexity is relatively
high. Note that we need to solve the optimization problem for every
category in the original hierarchy, and thus an eÔ¨Écient algorithm is
essential.
Lemma 1: The SSE of a category is not smaller than the Total-SSE
of its child categories.
Proof: Suppose there is a category Cp with k child categories {Ci }ki=1 .
For a child category Ci , the Sum ofSquare Distance (SSD) of its
data to a data point x is: SSD(Ci ) = d‚ààCi (d ‚àí x)2 . We get the mini

mum value when x = |C1i | d‚ààCi d = ci which let dxd d‚ààCi (d‚àíx)2 = 0.
Thus, when x is the mean of data in Ci (or ci ), the SSD of Ci becomes SSE of Ci , and gets its minimum value. One step further, we
have


(d ‚àí ci )2 ‚â§
(d ‚àí c p )2 ,
d‚ààCi

4.1.2 Sprout Category
For a category Ci of Hn , we sprout it based on the projected categories Œ¶Ci returned by Procedure ProjectedCategories. Recall that
each of the projected category Ca ‚àà Œ¶Ci represents a hidden topic of
Ci and contains a good number of documents projected from Ci , i.e.,
œÄ (Ci ‚Üí Ca ). We sprout Ci with |Œ¶Ci | number of categories. However, not all documents from
 Ci are contained in all these newly
sprouted categories, i.e., Ca ‚ààŒ¶C |œÄ (Ci ‚Üí Ca )| ‚â§ |Ci |. For those
i
documents in Ci but not contained in any of the newly sprouted categories, we assign them to their nearest sprouted categories. As the
result, each document in Ci is now contained in one and only one
sprouted category of Ci .

d‚ààCi

where c p is the mean of data of Cp . This demonstrates that the SSE
of Ci is smaller than the SSD of data of Ci to the overall mean, and
this result leads to
k 
k 


(d ‚àí ci )2 ‚â§
(d ‚àí c p )2 .
i=1 d‚ààCi

i=1 d‚ààCi

This demonstrates that the SSE of a category is not smaller than the
Total-SSE of its child categories.

795

$X[LOLDU\
KLHUDUFK\

1HWZRUN
6HFXULW\



0RGLI\LQJ
KLHUDUFK\


&RPSXWHU

1HWZRUN

:$1

&DEOH
+L

HQ

OLW

GG

WR

6S



SL
FV



3URWRFRO

FD
WHJ
RU

\

1HWZRUN

3URJUDPPLQJ

1HWZRUN
3URWRFRO

 0HUJH FDWHJRU\

6HFXULW\

documents over categories of Hc and Ha , and (ii) the similarity
between the categories within Hc and Ha , respectively.
Let Lc be the set of categories on level  in the original hierarch
Hc , Ls be the set of categories sprouted from Lc , and La be the
set of projected categories in auxiliary hierarchy representing
the

hidden topics of the categories in Lc . That is, La = Ci ‚ààLc Œ¶Ci .
For a sprouted category Cs ‚àà Ls , its document distribution over
Lc is deÔ¨Åned to be the ratios of its documents in each of the categories in Lc . That is, the document distribution of Cs can be modeled as a |Lc |-dimensional vector vcs . The j-th element of vcs is
|Cs ‚à©C j |
(i.e., the portion of Cs ‚Äôs documents also contained in C j ),
|Cs |
where C j is the j-th category of Lc . Similarly, we get the data distribution vector vas for Cs over La based on the ratio of documents
in Cs projected to each of the categories in La . Because vcs and
vas usually have diÔ¨Äerent dimensionality for diÔ¨Äerent sprouted category Cs ‚Äôs, we extend vcs to be |Hc |-dimensional (each category is
one dimension) by Ô¨Ålling up zeros for corresponding categories in
Hc but not in Lc . Similarly vas is extended to be |Ha |-dimensional.
We use two matrices Mc , Ma to represent the similarity between
categories of Hc and Ha , respectively. Mc is a |Hc |- by-|Hc | matrix
and Ma is a |Ha |-by-|Ha | matrix. Each element mi j in a matrix
represents the similarity in the corresponding hierarchy between
a pair of categories Ci and C j , which is deÔ¨Åned by Inverted Path
1
, where path(Ci , C j ) is the length of
Length [14]: mi j = 1+path(C
i ,C j )
path between Ci and C j in the hierarchy.
Considering both document distribution and structural similarity from the two hierarchies, the similarity between two sprouted
categories C1 and C2 on level  of Hn is deÔ¨Åned as:

&RPSXWHU

&RPSXWHU

6HFXULW\
3URJUDPPLQJ

3URWRFRO
3URJUDPPLQJ

&RPSXWHU

1HWZRUN 6HFXULW\
6HFXULW\ 3URJUDPPLQJ

:$1 6HFXULW\



1HWZRUN
&DEOH

 $VVLJQ FDWHJRU\ UHODWLRQ

1HWZRUN
&DEOH

1HWZRUN 3URWRFRO
3URWRFRO 3URJUDPPLQJ

:$1 3URWRFRO
0RGLILHG KLHUDUFK\ EHIRUH UHODEHO

Figure 4: SMA operations by example. The hidden topics and
sprouted categories for a category of the original hierarchy are
in the same color. The Network and Programming categories
have 3 and 2 hidden topics, respectively, leading to 5 sibling
sprouted categories. These 5 categories are merged into 3 categories and the category WAN is reassigned to 2 of the merged
categories.
Example 4.1: Shown in Figure 42 , suppose after applying procedure ProjectedCategories, we Ô¨Ånd Network is projected to Security, Protocol, and Cable. According to the three hidden topics,
we sprout Network into three categories Network Security, Network
Protocol, and Network Cable.

The sprout operation may be reminiscent of the work on hierarchy integration, aiming to integrate a category from a source hierarchy into similar categories in the target hierarchy, which has a different purpose from our mapping. Most of proposals (e.g., [3, 21])
on hierarchy integration employ a hierarchical classiÔ¨Åer built on the
target hierarchy to classify each document in the source hierarchy
into a leaf node of the target hierarchy, which is too Ô¨Åne a granularity to represent hidden topics as in our approach. Frameworks that
can map a category to categories on proper levels in target hierarchy
are proposed (e.g., [19]). However, they do not take the cohesion
and separation between mapping categories into account, which are
essential to Ô¨Ånd good hidden topics in our approach. Thus, they
cannot be applied to our work.

Sim(C1 , C2 ) = (vTc1 ¬∑ Mc ¬∑ vc2 ) + (vTa1 ¬∑ Ma ¬∑ va2 ).
This similarity deÔ¨Ånition considers both the similarity estimated
based on Hc and the similarity estimated based on Ha . With similarity between two sprouted categories deÔ¨Åned, we proceed to detail
the merge operation.
We Ô¨Årst explain the notion of sibling sprouted category through
an example. Let Ci1 and Ci2 be the two categories sprouted from
category Ci , and C j1 and C j2 be the two categories sprouted from C j .
If Ci and C j in Hm are both children of category Cp , then naturally,
all the newly sprouted categories Ci1 , Ci2 , C j1 , C j2 are children of
Cp . These four example categories are known as sibling sprouted
categories. All the Ô¨Åve sprouted categories shown in Figure 4 are
sibling sprouted categories.
The merge operation is as follows. We Ô¨Årst calculate the similarity between sibling sprouted categories on level . Then, we
pick up the pair of categories with the largest similarity, and merge
them into a category, and recompute its similarities with its sibling sprouted categories. The process iterates until the number of
remaining categories on  equals n , the number of categories on
level  of the original hierarchy Hc . When all the sibling sprouted
categories under the same parent node are merged into a single category, we shrink the single category into its parent node. Note that
we cannot merge two sprouted categories on level  if they have
diÔ¨Äerent parent node.

4.2 Merge Operation
The sprout operation in Section 4.1 generates a set of sprouted
categories, each representing a hidden topic. The merge operation
aims to combine the newly sprouted categories with similar hidden
topics.
Suppose we are now working on level  of the intermediate modiÔ¨Åed hierarchy Hn and we have a set of sprouted categories originated from the categories on level . Our task is to merge some
of these sprouted categories such that the number of resultant categories on level  is the same as before (i.e., n ). Note that the
number of resultant categories can also be speciÔ¨Åed by users. To
ease the presentation, the modiÔ¨Åed hierarchy has the same size as
the given hierarchy in our discussion.
During merge, we need to consider an important constraint ‚Äî we
can only merge categories under the same parent category. Thus,
existing clustering algorithms need to be modiÔ¨Åed to accommodate such a constraint. Another key issue here is how to deÔ¨Åne the
similarity between two sprouted categories by considering their semantics enclosed in Hc and Ha . In the following, we Ô¨Årst deÔ¨Åne a
similarity measure and then describe our merge method.
We consider two aspects when deÔ¨Åning the similarity for a pair of
sprouted categories C1 and C2 on level : (i) the distribution of their
2

Example 4.2: Recall Example 4.1. We sprout Network into three
categories Network Security, Network Protocol, and Network Cable. Suppose there is another category Programming on the same
level of Network and sprouted into Security Programming, and Protocol Programming (see Figure 4). Based on the similarity, Network
Security and Security Programming are merged together (both are
about the Security topic), and Network Protocol and Protocol Programming are merged to generate a new category about protocol.


For clarity, we recommend viewing all diagrams in this paper from a color copy.

796

4.3 Assign Operation

Table 1: Statistics of the three hierarchies
Hierarchy
HYA
HAB
HODP
Number of documents
421,163 148,822 203,448
75
195
460
Number of leaf nodes
40
70
98
Number of non-leaf nodes
4
5
4
Height

After modifying categories (by sprout and merge) on level  of
H n , the original parent-child relations between the categories on
level  and the categories on next level  + 1 do not hold any longer.
Hence we need to reassign the parent-child relation.
Based on the fact that a non-leaf category of a hierarchy subsumes the data of all its descendants, we rebuild the children for
each of the modiÔ¨Åed categories on  by checking document containment. If the documents of a category on level  are also contained
in a category on level  + 1, then the latter category is assigned to be
the children of the former category. In other words, for each category C on , we calculate the intersection of documents between C
and the categories on  + 1. The intersections form new children for
category C. Because one category has only one parent category, if
a category has intersections with more than one category on level ,
the category will be split into multiple categories, each containing
the intersection with one category on level .


HODP , respectively. Since the modiÔ¨Åed category hierarchy contains
a diÔ¨Äerent set of leaf nodes, the labels for documents given in the
original dataset do not stand in modiÔ¨Åed hierarchy. Manual annotation of documents in the modiÔ¨Åed hierarchy is therefore unavoidable. To make the annotation manageable, we selected the documents from two major topics Sports and Computers from these
three hierarchies (because the annotators are familiar with both topics). Nevertheless, the number of documents in the two major topics in the three hierarchies ranges from 148,822 to 421,163, and the
number of categories ranges from 115 to 558. These numbers are
large enough for a valid evaluation. Table 1 reports the statistics on
the three hierarchies.
HYA : Obtained from the Yahoo! Webscope datatset3 , HYA contains 421,163 documents (or questions) from Sports and Computers & Internet categories.
HAB : We collected 148,822 questions from Recreation & Sports
and Computers categories from AnswerBag to form HAB . Categories with fewer than 100 questions are pruned and all aÔ¨Äected
questions are moved to their parent categories.
HODP : The set of 203,448 documents from Sports and Computers categories are collected4 in HODP . Categories containing fewer
than 15 documents or located on level 5 or deeper are removed in
our experiments.
The preprocessing of the documents in all three hierarchies includes stopword removal and stemming. Terms occurred no more
than 3 times across the datasets are also removed.

Example 4.3: Recall Example 4.2. Suppose WAN was a child category of Network before sprout (see Figure 4). After sprout and
merge, Network no longer exists and WAN lost its parent. We compare the documents of WAN and the two newly formed categories
after merge (i.e., Network Security & Security Programming and
Network Protocol & Protocol Programming). If WAN has overlap
with both categories, then WAN have two hidden topics (about security and protocol). Thus, we divide WAN into two categories and
assign them to diÔ¨Äerent parent nodes, shown in Figure 4.


4.4 Relabel
Unlike most of previous work, our approach is able to automatically generate readable labels for every modiÔ¨Åed category. By projecting documents from Hn to Ha , we can consider the three hierarchies Hn , Hc and Ha , which contain the same set of data. For
every document in Hn , we trace its labels in Hc and Ha , and use
them together as the label of the document; the semantic of the new
label is the intersection of semantics of its two component labels.
We then aggregate such labels for all documents in a category of
Hn , and use them as candidate labels for the category. The candidate labels for a category are ranked according to the proportion of
documents in their corresponding original categories from Hc and
Ha . In this paper, the top-1 ranked label is chosen.
The labels generated in this way are mostly readable and semantically meaningful, as reÔ¨Çected in our user study (see Section 5.3)
and case study (see Section 5.4). Nevertheless, a manual veriÔ¨Åcation of the labels for the newly generated categories can be employed when the proposed technique is used in real applications.

5.2

Evaluation by ClassiÔ¨Åcation

The proposed SMA algorithm modiÔ¨Åes a category hierarchy to
better reÔ¨Çect the topics of its documents, which in turn should improve the classiÔ¨Åcation performance. Following the experimental
setting in [17], we evaluate the eÔ¨Äectiveness of hierarchy modiÔ¨Åcation by comparing the classiÔ¨Åcation accuracies obtained by the
same hierarchical classiÔ¨Åcation model applied on the original category hierarchy and the modiÔ¨Åed hierarchy, respectively.
Another three methods for hierarchy modiÔ¨Åcation are employed
as the baselines, namely, Bottom Up Clustering (BUC), Hierarchical Acclimatization (HA) [17], and Supervised Clustering (SC) [2].
Table 2 gives a summarized comparison of the three baselines with
the proposed SMA, and Section 5.2.1 briefs the baseline methods.
The modiÔ¨Åed hierarchies by all the methods evaluated in this paper have the same size as the original hierarchy (i.e., same number
of levels, and same number of categories in each level). For each
hierarchy modiÔ¨Åcation method, we evaluate the percentage of classiÔ¨Åcation accuracy increment obtained by the same classiÔ¨Åcation
model (e.g., Support Vector Machine) on the modiÔ¨Åed hierarchy
over the original hierarchy. The classiÔ¨Åcation accuracy is measured by both micro-average F1 (Micro-F1 ) and macro-averaged F1
(Macro-F1 ) [20]. The former gives equal weight to every document
while the latter weighs categories equally regardless the number of
documents in each category.
We remark that this is a fair evaluation for all the methods, each
generating a hierarchy with the same size as that of the original

5. EXPERIMENTS
We designed two sets of experiments. The Ô¨Årst set of experiment, similar to that in [17], is to evaluate whether the modiÔ¨Åed
hierarchy improves the classiÔ¨Åcation accuracy. Discussed in Section 1, if a category hierarchy better reÔ¨Çects the topics of its contained documents and each category in the hierarchy is topically
cohesive, then better classiÔ¨Åcation accuracy is expected than that
on a hierarchy with less topically cohesive categories. The second
set of experiments employs a user study to manually evaluate the
semantic quality of the modiÔ¨Åed hierarchy following the settings
in [2,5]. Finally, we report a case study comparing a part of Yahoo!
Answers hierarchy with its modiÔ¨Åed hierarchy.

5.1 Data Set

3

We use data from three real-world hierarchies: Yahoo! Answers,
AnswerBag, Open Directory Project, denoted by HYA , HAB and

4

797

Available at http://research.yahoo.com/Academic_Relations.
Available at http://www.dmoz.org/rdf.html.

good a modiÔ¨Åed hierarchy is and manual assessment of every modiÔ¨Åed hierarchy for parameter tuning is impractical. We therefore
adopt a bootstrapping like approach described below.
After the test data selected, the remaining data is used for hierarchy modiÔ¨Åcation. We split the remaining data of HYA into 3 parts:
P1 , P2 , P3 , and the proportion of their sizes is 12:3:1. Using P1 for
HYA and a given auxiliary hierarchy, we obtain a modiÔ¨Åed hierarchy HnYA . Naturally, all documents in P1 have category labels from
hierarchy HnYA . We then build a classiÔ¨Åer using all documents in
P1 and their labels from HnYA . The classiÔ¨Åer classiÔ¨Åes documents
in P2 and P3 . Assume that the classiÔ¨Åer gives reasonably good classiÔ¨Åcation accuracy, then all documents in P2 and P3 have their category labels assigned according to HnYA . With these labels, we can
evaluate the classiÔ¨Åcation accuracy of documents in P3 by the classiÔ¨Åer built using P2 on HnYA . Intuitively, if a hierarchy H1 better
organizes documents than another hierarchy H2 , then the classiÔ¨Åer
trained on H1 is expected to have higher classiÔ¨Åcation accuracy for
P3 than a classiÔ¨Åer built on H2 . We then select the parameters leading to the best classiÔ¨Åcation accuracy for P3 . In our experiments,
the parameters (i.e., Œª and Œ∏ ) set for SMAYA|YA , SMAYA|AB , and
SMAYA|ODP are (0.29 and 0.11), (0.17 and 0.17), (0.38 and 0.08),
respectively.

Table 2: Comparison of baseline methods with SMA
Aspect/Methods
BUC HA [17] SC [2]
SMA
‚àö
‚àö
Utilize original hierarchy
√ó
√ó
‚àö
‚àö
√ó
√ó
Change leaf category
√ó
√ó
√ó
Optional
Utilize auxiliary hierarchy
hierarchy, where the same classiÔ¨Åcation method is applied to the
modiÔ¨Åed hierarchies to evaluate the improvement of each modiÔ¨Åed
hierarchy over the original one in terms of classiÔ¨Åcation accuracy.

5.2.1 Baseline Methods
Baseline 1: Bottom Up Clustering (BUC). In this method, each
leaf category is represented by the mean vector of its contained documents. The categories are then clustered in a bottom-up manner
using K-means to form a hierarchy.
Baseline 2: Hierarchical Acclimatization (HA). The HA algorithm is reviewed in Section 2, In simple words, it employs promote, demote and merge operations to adjust the internal structure,
but leaves the leaf nodes unchanged [17].
Baseline 3: Supervised Clustering (SC). Given a set of documents with labels, SC Ô¨Årst calculates the mean vector of each category as the initial centroid and then reassigns the documents to
the categories based on the cosine similarity with their centroids.
Then, similar categories are merged and minor categories are removed. These procedures are repeated, and during each iteration, a
constant portion of features with smallest term-frequencies are set
to zero (projected out) [2]. The process stops when the number of
features left is smaller than a pre-deÔ¨Åned threshold. This method
cannot generate category labels. We take the most frequent words
in a category to name it.

Test Data Annotation. With the chosen parameters, each SMA
setting generated a modiÔ¨Åed hierarchy using P1 as HYA and its corresponding auxiliary hierarchy. The preselected 500 test questions
are used as test data to fairly evaluate the modiÔ¨Åed hierarchies by
the three SMA settings and the baseline methods. Recall that the
500 questions are not included in the three parts (P1 , P2 , and P3 ) for
parameter setting. Because BUC and HA do not change the leaf
categories, the original labels of the 500 questions remain applicable. For SC and SMA, both changing leaf categories, we invited
two annotators to label the 500 questions to their most relevant leaf
categories in the modiÔ¨Åed hierarchies. We synthesized the results
of the annotators, and assigned the labels for questions. If two annotators conÔ¨Çicted about a label, a third person made the Ô¨Ånal judgment. The dataset and their annotations are available online 5 .

5.2.2 Experiments on Yahoo! Answers
From the data of HYA , we randomly selected 500 questions as
test data (used for classiÔ¨Åcation evaluation with manual annotations). To evaluate the possible improvement in classiÔ¨Åcation accuracy, the same set of test documents are classiÔ¨Åed on the original (or
unmodiÔ¨Åed) HYA , and the modiÔ¨Åed HnYA ‚Äôs. Two classiÔ¨Åers, multinominal Naive Bayes (NB) and Support Vector Machine (SVM)
classiÔ¨Åers are used as base classiÔ¨Åers for hierarchical classiÔ¨Åcation. We build Single Path Hierarchical ClassiÔ¨Åer (SPH) [9] as it
performs better than other hierarchical classiÔ¨Åcation methods for
question classiÔ¨Åcation according to the evaluation [13]. In the training phase of SPH, for each internal node of the category tree, SPH
trains a classiÔ¨Åer using the documents belonging to its descendent
nodes. In the testing phase, a test document is classiÔ¨Åed from the
root to a leaf node in the hierarchy along a single path.

ClassiÔ¨Åcation Results. ClassiÔ¨Åcation accuracy measured by MicroF1 using NB and SVM as base classiÔ¨Åers for the six methods (i.e.,
three baselines BUC, HA, SC, and the three SMA settings) on modiÔ¨Åed hierarchies is reported in Figure 5(a). For comparison, the
classiÔ¨Åcation accuracy on the original (or unmodiÔ¨Åed) Yahoo! Answers hierarchy is also reported under column named HYA . Figure 5(b) reports Macro-F1 .
As shown in Figures 5(a) and 5(b), all the three settings of SMA
achieve signiÔ¨Åcant improvement over the results obtained on the
original hierarchy. For example, using NB as the base classiÔ¨Åer,
SMAYA|YA improves Micro-F1 over the results on the original hierarchy by 41.0% and improves Macro-F1 by 40.3%. NB achieves
better accuracy than SVM probably because NB was used as the
base classiÔ¨Åer for parameter setting. The three baseline modiÔ¨Åcation methods only slightly improve the classiÔ¨Åcation accuracy over
the original hierarchy and even deteriorate the accuracy in some
cases. Recall that SC and SMA modify leaf categories while BUC
and HA only modify internal structures of hierarchy without changing leaf categories. All the methods that change leaf categories outperform the methods that keep leaf categories unchanged. Note that
SMAYA|YA signiÔ¨Åcantly outperforms SC. One possible reason could
be that SMAYA|YA utilizes the semantics of the original hierarchy in
hierarchy modiÔ¨Åcation while SC does not.
We observe that the auxiliary hierarchies employed by SMA
have eÔ¨Äect on the classiÔ¨Åcation accuracy of the modiÔ¨Åed hierarchy.

SMA Settings. Recall that SMA uses auxiliary hierarchy in the
modiÔ¨Åcation process. We evaluated SMA with three settings, to
modify HYA using HYA , HAB , and HODP as auxiliary hierarchy, respectively. The three settings are denoted by SMAYA|YA , SMAYA|AB ,
and SMAYA|ODP , respectively. The Ô¨Årst setting is to evaluate the effectiveness of using the original hierarchy as auxiliary hierarchy,
and the last two are to evaluate the eÔ¨Äectiveness of using external
hierarchies.
Parameter Setting. Before evaluating the test documents on the
modiÔ¨Åed hierarchies, we set the parameters required by SMA for
hierarchy modiÔ¨Åcation. Recall that SMA requires two parameters: minimum coverage ratio Œª and maximum loss ratio Œ∏ . Usually parameters are set using a development set or through crossvalidation. In our case, however, there is no ground truth on how

5

798

http://www.ntu.edu.sg/home/gaocong/datacode.htm

0.9
NB
0.85 SVM

Table 4: Comparison on appropriateness of category labels
Judgement
Number of documents
HnYA is better than HYA
12
1
HnYA is not as good as HYA
81
Both are equally good
6
Neither is good

MicroF1

0.8
0.75
0.7
0.65
0.6
0.55
0.5

HYA

BUC

HA

SC

Table 5: Averaged scores of HYA
Measure/Hierarchy
Cohesiveness
Isolation
Hierarchy
Navigation Balance
Readability

SMAYA|YA SMAYA|AB SMAYA|ODP

(a) Micro-F1
0.8
NB
0.75 SVM

MacroF1

0.7
0.65

and HnYA (by SMAYA|AB )
HYA HnYA
5.00 6.00
4.00 4.67
5.00 5.33
4.50 4.50
6.00 5.67

0.6
0.55

Through the study, we aim to quantify both the appropriateness of
the category labels and the structure of the modiÔ¨Åed hierarchy.

0.5
0.45
0.4

HYA

BUC

HA

SC

SMAYA|YA SMAYA|AB SMAYA|ODP

(b) Macro-F1
Figure 5: Micro-F1 and Macro-F1 on the modiÔ¨Åed hierarchies
by three baselines and three SMA settings, and on the original
Yahoo! Answers hierarchy.
Table 3: ClassiÔ¨Åcation accuracy on modifying AnswerBag
Measure/Hierarchy
HAB
SMAAB|YA Improvement(%)
Macro-F1 (NB)
0.3444
0.5638
63.7%
0.3691
0.4933
33.6%
Macro-F1 (SVM)
Micro-F1 (NB)
0.4671
0.6669
42.8%
0.4371
0.5697
30.3%
Micro-F1 (SVM)
Measured by Macro-F1 , SMAYA|YA without using an external hierarchy slightly outperforms its counterpart SMAYA|AB or SMAYA|ODP ,
which uses an external hierarchy; while in terms of Micro-F1 , SMAYA|YA
performs worse than do its counterparts.

5.2.3 Experiments on AnswerBag

Category Labels. Following a similar setting as in [2], we randomly selected 100 questions from the labeled test set originated
from Yahoo! Answers. For each question, we gave the path of the
categories in HYA from the second level category to the leaf category, and similarly the category path from HnYA (by SMAYA|AB ).
We asked three students to annotate which category path better reÔ¨Çects the topic of the question. Which hierarchy a category path
was originated from was not provided to the annotators. Given a
question, each volunteer is asked to rate each path from 1(lowest)
to 5(highest) based on its quality. hen, we select one of the following choices based on the averaged ratings (rYA and rnYA for the two
paths, respectively). (1)HnYA is better than HYA , if rnYA , rYA ‚àà [3, 5]
and rnYA > rYA ; (2)HnYA is not as good as HYA , if rnYA , rYA ‚àà [3, 5];
(3)Both are equally good, if rnYA , rYA ‚àà [3, 5] and rnYA = rYA ;
(4)Neither is good, if rnYA , rYA ‚àà [1, 3). The statistics of the labels are reported in Table 4. The table shows that the number of
questions having better labels in HnYA is larger than that in HYA although for majority of questions, the category paths from the two
hierarchy are equally good. This result also suggests that the generated labels well reÔ¨Çects the content of categories.
Category Structure. Following the evaluation approaches in [5],
we evaluate the quality of Yahoo! Answers hierarchy and the modiÔ¨Åed hierarchy by Ô¨Åve measures. Cohesiveness: Judge whether the
instances in each category are semantically similar. Since it is impractical to read all questions in a large category, we randomly select 50 questions from each category for cohesiveness evaluation.
Isolation: Judge whether categories on the same level are discriminative from each other.We also use the 50 randomly selected questions to represent each category. Hierarchy: Judge whether the
concepts represented by the categories become Ô¨Åner from top to
bottom. Navigation Balance: Judge whether the number of child
categories for each internal category is appropriate. Readability:
Judge whether the concept represented by each category is easy to
understand.
We invited three students to evaluate the two hierarchies, HYA
and HnYA (by SMAYA|AB ) and assigned scores ranging from 0 to 7
on each measure. The mean of the scores is reported in Table 5.
The cohesiveness of the modiÔ¨Åed hierarchy is better than the
original one. A possible reason is that our approach detected the
hidden topics and merged the most similar ones together. The isolation of the modiÔ¨Åed hierarchy is slightly better. This is probably
because the proposed method takes isolation into consideration. To
Ô¨Ånd out the reasons that caused the relatively low isolation of the
original hierarchy, we get the list of categories with low scores from

In this set of experiments, SMA is used to modify the AnswerBag
hierarchy. The main purpose is to evaluate whether SMA remains
eÔ¨Äective when the size of the auxiliary hierarchy is smaller than the
one to be modiÔ¨Åed. SpeciÔ¨Åcally, we use HYA as auxiliary hierarchy
to modify HAB and evaluate the classiÔ¨Åcation accuracy as we did in
the earlier set of experiments. Note that the HAB has 265 categories
which is more than twice of the 115 categories contained in HYA .
The parameters Œª and Œ∏ were set as 0.17 and 0.08, respectively,
using the parameter setting approach described earlier. The classiÔ¨Åcation accuracy is reported in Table 3. Observe that SMAAB|YA improves the classiÔ¨Åcation accuracy (Macro- and Micro-F1 ) by 30%
to 63%, compared with the result obtained before hierarchy modiÔ¨Åcation. This demonstrates that the proposed SMA approach is
eÔ¨Äective for diÔ¨Äerent hierarchies, even if the size of the auxiliary
hierarchy is smaller than the hierarchy to be modiÔ¨Åed.

5.3 User Study
A good category hierarchy must be semantically meaningful: (i)
Its category labels should be easy to understand, facilitating data
browsing; and (ii) Its category structure should reÔ¨Çect the topics
of its data. We would like to note that it is challenging to evaluate
these. We evaluate the modiÔ¨Åed hierarchy by SMAYA|AB through two
types of user study by following the methods [2, 5], respectively.

799



 

 



 

 

 


 

 

  

hierarchy with better classiÔ¨Åcation accuracy improvement over the
original hierarchy than baseline methods. Additionally, user study
shows that the modiÔ¨Åed category hierarchy is topically cohesive
and semantically meaningful.

&&&

 





 
 

  

7.


  !"
#  ! 






#

 

 
$ #

 
 

 

%
"
 

 



 

&&&

 


ACKNOWLEDGEMENTS

Quan Yuan would like to acknowledge the Ph.D. grant from the
Institute for Media Innovation, Nanyang Technological University,
Singapore. Gao Cong is supported in part by a grant awarded by
Microsoft Research Asia and by a Singapore MOE AcRF Tier 1
Grant (RG16/10).

  

 

   

8.

REFERENCES

[1] G. Adami, P. Avesani, and D. Sona. Bootstrapping for hierarchical
document classiÔ¨Åcation. In CIKM, pages 295‚Äì302, 2003.
[2] C. C. Aggarwal, S. C. Gates, and P. S. Yu. On the merits of building
categorization systems by supervised clustering. In KDD, pages
352‚Äì356, 1999.
[3] R. Agrawal and R. Srikant. On integrating catalogs. In WWW, pages
603‚Äì612, 2001.
[4] X. Cao, G. Cong, B. Cui, C. S. Jensen, and Q. Yuan. Approaches to
exploring category information for question retrieval in community
question-answer archives. ACM Trans. Inf. Syst., 30(2):1‚Äì38, 2012.
[5] S.-L. Chuang and L.-F. Chien. A practical web-based approach to
generating topic hierarchy for text segments. In CIKM, pages
127‚Äì136, 2004.
[6] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein.
Introduction to Algorithms, Second Edition. The MIT Press and
McGraw-Hill Book Company, 2001.
[7] S. C. Gates, W. Teiken, and K.-S. F. Cheng. Taxonomies by the
numbers: building high-performance taxonomies. In CIKM, pages
568‚Äì577, 2005.
[8] A. Koestler. The Act of Creation. Penguin Books, New York, 1964.
[9] D. Koller and M. Sahami. Hierarchically classifying documents
using very few words. In ICML, pages 170‚Äì178, 1997.
[10] K. Nitta. Improving taxonomies for large-scale hierarchical
classiÔ¨Åers of web documents. In CIKM, pages 1649‚Äì1652, 2010.
[11] K. Punera, S. Rajan, and J. Ghosh. Automatically learning document
taxonomies for hierarchical classiÔ¨Åcation. In WWW (Special interest
tracks and posters), pages 1010‚Äì1011, 2005.
[12] X. Qi and B. D. Davison. Hierarchy evolution for improved
classiÔ¨Åcation. In CIKM, pages 2193‚Äì2196, 2011.
[13] B. Qu, G. Cong, C. Li, A. Sun, and H. Chen. An evaluation of
classiÔ¨Åcation models for question topic categorization. JASIST,
63(5):889‚Äì903, 2012.
[14] G. Siolas and F. d‚ÄôAlch√© Buc. Support vector machines based on a
semantic kernel for text categorization. In IJCNN (5), pages
205‚Äì209, 2000.
[15] A. Sun, E.-P. Lim, and Y. Liu. What makes categories diÔ¨Écult to
classify?: a study on predicting classiÔ¨Åcation performance for
categories. In CIKM, pages 1891‚Äì1894, 2009.
[16] P.-N. Tan, M. Steinbach, and V. Kumar. Introduction to Data Mining.
Addison-Wesley, 2005.
[17] L. Tang, J. Zhang, and H. Liu. Acclimatizing taxonomic semantics
for hierarchical content classiÔ¨Åcation from semantics to data-driven
taxonomy. In KDD, pages 384‚Äì393, 2006.
[18] N. Tomuro. Tree-cut and a lexicon based on systematic polysemy. In
NAACL, 2001.
[19] W. Wei, G. Cong, X. Li, S.-K. Ng, and G. Li. Integrating community
question and answer archives. In AAAI, 2011.
[20] Y. Yang and X. Liu. A re-examination of text categorization methods.
In SIGIR, pages 42‚Äì49, 1999.
[21] D. Zhang and W. S. Lee. Web taxonomy integration through
co-bootstrapping. In SIGIR, pages 410‚Äì417, 2004.
[22] L. Zhang, S. Liu, Y. Pan, and L. Yang. Infoanalyzer: a
computer-aided tool for building enterprise taxonomies. In CIKM,
pages 477‚Äì483, 2004.

Figure 6: Portion of Yahoo! Answers hierarchy and its modiÔ¨Åed hierarchy.
the annotators. As an example, a number of questions that are related to motor-cycling were put under Other - Auto Racing by their
askers, resulting in low isolation between the two categories. The
modiÔ¨Åed hierarchy does not deteriorate hierarchy quality, navigation balance, and readability of the original hierarchy on average.
In summary, the modiÔ¨Åed hierarchy is of high quality comparable
to the original hierarchy generated by domain experts.

5.4 Case Study
As a case study, we select three categories Software, Internet and
Hardware from Yahoo! Answers as an example to illustrate the dif-

ferences before and after modifying HYA . The modiÔ¨Åed hierarchy
HnYA is by SMAYA|AB utilizing AnswerBag as auxiliary hierarchy.
The two hierarchies are shown in Figure 6. We make the following observations: 1) DiÔ¨Äerent from the original hierarchy, Software
and Internet become three categories ‚Äì Operating System & Application Software, Internet & E-mail and Internet Software. The third
category is formed based on the overlapping part of the original
two categories, which contains questions about instant messaging
(IM) and blog software. This demonstrates that the proposed approach can discover and detach the overlapping hidden topics. 2)
Two pairs of categories of the original hierarchy, (Laptops & Notebooks and Desktops), and (Printers and Scanners), are merged into
two categories in the modiÔ¨Åed hierarchy, because of the high similarity between the categories within each pair. This shows that
categories with high overlap in semantics are merged. 3) For Hardware, some hidden topics are discovered and new categories are
formed, like Storage and CPU & Memory & Motherboard, whose
questions come from Desktops, Add-ons and Other - Hardwares in
the original hierarchy. These newly formed categories are more
isolated from each other.

6. CONCLUSION
Category hierarchy plays a very important role in organizing data
automatically (through classiÔ¨Åers built on the hierarchy) or manually. However, with newly available documents added into a hierarchy, new topics emerge and documents within the same category become less topically cohesive. Thus the hierarchies suÔ¨Äer
from problems of structure irrelevance and semantic irrelevance,
leading to poor classiÔ¨Åcation accuracy of the classiÔ¨Åers developed
for automatically categorizing the newly available documents into
the hierarchy, which in turn leads to poorer document organization.
To address these problems, we propose a novel approach SMA to
modify a hierarchy. SMA comprises three non-trivial operations
(namely, sprout, merge, and assign) to modify a hierarchy. Experimental results demonstrate that SMA is able to generate a modiÔ¨Åed

800

