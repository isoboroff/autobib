SimFusion+: Extending SimFusion Towards Efﬁcient
Estimation on Large and Dynamic Networks
Weiren Yu†‡ , Xuemin Lin† , Wenjie Zhang† , Ying Zhang† , Jiajin Le
†


The University of New South Wales, Australia
East China Normal University, China
‡

NICTA, Australia
Donghua University, China

{weirenyu, lxue, zhangw, yingz}@cse.unsw.edu.au
ABSTRACT

P2

D2
P3

P4

D3
P5

G1

SimFusion has become a captivating measure of similarity between
objects in a web graph. It is iteratively distilled from the notion
that “the similarity between two objects is reinforced by the similarity of their related objects”. The existing SimFusion model usually exploits the Uniﬁed Relationship Matrix (URM) to represent
latent relationships among heterogeneous data, and adopts an iterative paradigm for SimFusion computation. However, due to the
row normalization of URM, the traditional SimFusion model may
produce the trivial solution; worse still, the iterative computation
of SimFusion may not ensure the global convergence of the solution. This paper studies the revision of this model, providing a full
treatment from complexity to algorithms. (1) We propose SimFusion+ based on a notion of the Uniﬁed Adjacency Matrix (UAM), a
modiﬁcation of the URM, to prevent the trivial solution and the divergence issue of SimFusion. (2) We show that for any vertex-pair,
SimFusion+ can be performed in O(1) time and O(n) space with
an O(km)-time precomputation done only once, as opposed to the
O(kn3 ) time and O(n2 ) space of its traditional counterpart, where
n, m, and k denote the number of vertices, edges, and iterations
respectively. (3) We also devise an incremental algorithm for further improving the computation of SimFusion+ when networks are
dynamically updated, with performance guarantees for similarity
estimation. We experimentally verify that these algorithms scale
well, and the revised notion of SimFusion is able to converge to a
non-trivial solution, and allows us to identify more sensible structure information in large real-world networks.

D1

P1

lejiajin@dhu.edu.cn
lim SimFusion(k) =
k→∞
⎡
⎤
D1
1 .219 .219 .219 .219
⎥
⎢
⎢ .219 1 .219 .219 .219 ⎥
⎥
D2 ⎢
⎢ .219 .219 1 .219 .219 ⎥
⎥
⎢
⎥
⎢
⎣ .219 .219 .219 1 .219 ⎦
D3
.219 .219 .219 .219 1
D1

D2

D3

lim SimFusion+(k) =

k→∞

⎤
1 .290 .194 .139 .100
⎥
⎢
⎢ .290 1 .304 .217 .156 ⎥
⎥
⎢
⎢ .194 .304 1 .145 .105 ⎥
⎥
⎢
⎥
⎢
⎣ .139 .217 .145 1 .075 ⎦
.100 .156 .105 .075 1
⎡

D1

D2

D3

Figure 1: Trivial SimFusion on Heterogeneous Domain
based analysis” or “structural similarity search”, and it has been
extensively studied by different communities with a proliferation
of emerging applications. Examples include collaborative ﬁltering, hyper-text classiﬁcation, graph clustering and proximity query
processing. Recently, while the scale of the Web has dramatically increased our need to produce large graphs, the study of efﬁciently computing object similarity on such large graphs becomes
a desideratum. In practice, an effective similarity measure should
not only correlate well with human intuition but also scale well for
large amounts of data.
Among the existing metrics, SimFusion [1] can be regarded as
one of the attractive ones on account of the following reasons. (i)
Similar to PageRank [2] and SimRank [3], SimFusion is based on
hyperlinks and follows the reinforcement assumption that “the similarity between objects is reinforced by the similarity of their related objects”, which is fairly intuitive and conforms to our basic understandings. (ii) Unlike other measures (e.g., PageRank and SimRank) that explore the linkage patterns merely from a single data
space [2–4], SimFusion has the extra beneﬁts of incorporating both
inter- and intra-relationships from multiple data spaces in a uniﬁed
manner to measure the similarity of heterogeneous data objects.
(iii) SimFusion offers more intuitive and ﬂexible ways of assigning weighting factors to each data space that reﬂects their relative
importance, as opposed to the PageRank and SimRank measures
that need to determine a damping factor. (iv) SimFusion provides
a general-purpose framework for measuring structural similarity
in a recursive fashion; other well-known measures, such as CoCitation [5] and Coupling [6] are just special cases of SimFusion.
However, existing work on SimFusion has the following problems. Firstly, although the basic intuition behind the SimFusion
model is appealing, it seems inappropriate to use the Uniﬁed Relationship Matrix (URM) to represent the relationships of heterogeneous objects. The main problem is that, according to the deﬁnition
of URM L in [1], the sum of each row of L is always equal to 1.
Since the product of L and the matrix 1 whose entries are all ones
is equal to the matrix 1 of all ones, there always exists a trivial solution S = 1 to the original SimFusion formula S = L · S · LT [1],
as illustrated in Example 1. The same phenomena of yielding such
a trivial solution may occur in our experimental results in Section 6.
To address this issue, we shall revise the original SimFusion model.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information Storage
and Retrieval; G.2.2 [Graph Theory]: Discrete Mathematics

Keywords
Similarity Computation, SimFusion, Web Ranking Algorithm

1. INTRODUCTION
The conundrum of measuring similarity between objects based
on hyperlinks in a graph has fueled a growing interest in the ﬁelds of information retrieval. This problem is also known as “link-

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.

E XAMPLE 1 (T RIVIAL S OLUTION ). Figure 1 depicts a graph
G1 partly extracted from Cornell CS Department. Each vertex Pi

365

G2

P2

P4

P1
D 1 P3

P6
P5

lim SimFusion(2k+1)
lim SimFusion(2k)
⎤
⎤ ⎡k→∞
⎡ k→∞
.38 0 .38 0 0 .38
.46 0 .46 0 0 .46
⎢ 0 .38 0 .38 .38 0 ⎥ ⎢ 0 .46 0 .46 .46 0 ⎥
⎥
⎥ ⎢
⎢
⎢.46 0 .46 0 0 .46⎥ = ⎢.38 0 .38 0 0 .38⎥
⎥
⎥ ⎢
⎢
⎥
⎥ ⎢
⎢
⎢ 0 .38 0 .38 .38 0 ⎥ ⎢ 0 .46 0 .46 .46 0 ⎥
⎥
⎥ ⎢
⎢
⎣ 0 .38 0 .38 .38 0 ⎦ ⎣ 0 .46 0 .46 .46 0 ⎦
.38 0 .38 0 0 .38
.46 0 .46 0 0 .46

lim SimFusion+(k)
⎤
1 0 .25 0 0 .25
⎢ 0 1 0 .25 .18 0 ⎥
⎥
⎢
⎢.25 0 1 0 0 .18⎥
⎥
⎢
⎥
⎢
⎢ 0 .25 0 1 .25 0 ⎥
⎥
⎢
⎣ 0 .18 0 .25 1 0 ⎦
.25 0 .18 0 0 1
⎡

incremental algorithm retains O(δn) time and O(n) space for
handling a sequence of δ edge insertions or deletions.
5. We experimentally verify the effectiveness and scalability of the
algorithms, using real and synthetic data (Section 6). The results
show that SimFusion+ can govern the convergence towards a
meaningful solution, and our algorithms achieve high accuracy
and signiﬁcantly outperform the baseline algorithms.

k→∞

Figure 2: Divergent SimFusion on Homogeneous Domain
denotes a web page, and each edge a hyperlink. There are three
categories: D1 = {P1 } (student), D2 = {P2 , P3 } (staff), and
D3 = {P4 , P5 } (faculty). We want to retrieve the top-3 similar
pairs of web pages in G1 . However, the naive SimFusion fails to
correctly ﬁnd them. We observe that the SimFusion solution is a
(trivial) matrix whose entries are all the same. In fact, vertices in
G1 do not have the identical neighbor structures. Hence, the trivial
solution is non-semantic in real communities.
Secondly, it is rather expensive to compute SimFusion similarities. The existing approach for SimFusion computation deploys
a ﬁxed-point iteration: S(k+1) = L · S(k) · LT , which requires
O(kn3 ) time and O(n2 ) space [1]. This impedes the scalability
of SimFusion on large graphs. Worse still, the iterative computation of SimFusion do not always converge. The convergence of the
SimFusion iterations heavily depends on the choice of the initial
guess S(0) , as shown in Example 2.
E XAMPLE 2 (D IVERGENCE S IM F USION ). Consider the disease transmission graph G2 , where each vertex is an organism Pi
which can carry the disease, and an edge represents one organism
spreading it to another. One wants to ﬁnd the three most similar
organisms to P2 in G2 . However, the iterative computation of SimFusion does not work properly. We observe the following:
(i) When S(0) is set to an n×n identity matrix (according to [1])
the “even” and “odd” subsequences of {S(k) } are convergent
respectively, but they do not converge to the same limit, which
makes the full sequence {S(k) } divergent.
(ii) Choosing S(0) = 1n (i.e., an n × n matrix of all 1s) instead,
we observe that the full SimFusion sequence {S(k) } is always
convergent to 1n regardless of the graph structure.
This suggests that the original SimFusion iterations may be divergent or converge to a trivial solution, not to mention its scalability. This highlights the need to ﬁnd a feasible way to guarantee the
convergence of the SimFusion iterations, but it is hard to devise an
efﬁcient algorithm for the revised SimFusion computation.
Thirdly, it is a big challenge to incrementally compute SimFusion on dynamic graphs. The traditional method [1] has to recompute the similarity from scratch when edges in a graph change over
time, which is not adaptive to many evolving networks. Fortunately, we have an observation that the size of the areas affected by
the updates is typically small in practice. To this end, we propose
an incremental algorithm that fully utilizes these affected areas to
compute SimFusion on dynamic graphs.
Contributions. This paper proposes SimFusion+, a revised notion of SimFusion, to provide a full treatment of SimFusion for the
convergence issues and to improve its computational efﬁciency. In
summary, we make the following contributions.
1. We formalize the problem of SimFusion+ computation (Section
2). The notion of SimFusion+ revises the divergence and nonsemantic convergence worries of the traditional model [1].
2. We present optimization techniques for improving the computation of SimFusion+ to O(1) time and O(n) space for every pair
of vertices, plus an O(km)-time precomputation run only once
(Section 3).
3. We devise an efﬁcient algorithm to compute the SimFusion+
similarity with better accuracy guarantees (Section 4). An error
estimate is also given for the SimFusion+ approximation.
4. We devise an incremental algorithm for further optimizing the
SimFusion+ computation when edges in networks are dynamically updated (Section 5). We show that the update cost of the

Related Work. The link-based similarity has become increasingly popular since the famous result of Google PageRank [2] on
ranking web pages. Since then, there has been a surge of papers
focusing on web link analysis. In particular, a growing interest has
been witnessed in the SimFusion model over the past decade [1, 7]
as it provides a useful measure of similarity that supports different
kinds of intra- and inter-node relations from multiple data spaces.
The iterative computation of SimFusion was proposed in [1] with
several problems left open there. In comparison, this work extends [1] by (i) addressing the divergent and trivial solution of the original SimFusion, (ii) optimizing the time and space complexity of
similarity computation, and (iii) supporting incremental update on
evolving graphs, none of which was considered in [1].
It is worth mentioning that Jeh and Widom have proposed a similar structural measure called SimRank [3], predicated, as SimFusion is, on the idea that vertices are similar if they have similar
neighbor structures. The essential difference between the two models is the notion of the convergence principle. SimFusion ensures
the existence of the stationary distribution and ergodicity convergence to this distribution, whereas SimRank hinges on a damping
factor 0 < c < 1 to govern the convergence.
Optimization techniques have been devised for SimRank computation (e.g., [8–11]). The best-known SimRank algorithm yields
n3
}) time [8]. The performance gain is mainly
O(k min{nm, log
n
achieved by a partial sum function for amortization; as for SimFusion, the conventional matrix multiplication in its iterative formula
misled its complexity, which was previously considered O(kn3 )
time and O(n2 ) space. The idea of the dominant eigenvector in
this work signiﬁcantly improves its computation to O(km) time
and O(kn) space, which is more efﬁcient than SimRank [8].
There has also been work on link-based similarity computation.
A uniﬁed framework of link-based analysis was addressed in [7],
which extends PageRank and HITS by (i) considering both interand intra-type relationships, and (ii) bringing order to data objects
in different data spaces. It differs from this work in that the focus is on ﬁnding attribute values of a single object, rather than on
improving the complexities for similarity estimation. Extensions of similarity reinforcement assumption were studied in [12], by
spreading multiple relationship similarities over interrelated data
objects to enhance their mutual reinforcement effects. Nevertheless, neither of these deduces rigorous mathematical formulae, and
the rationales behind the integration approaches are different from
this work. Recently, a closed-form solution to P-Rank (PenetratingRank) formula was addressed in [13]. Cai et al. [13] showed that
when the damping factor c = 1 and weighting factor γ = 0, PRank can be reduced to SimFusion. However, this reasoning is
based on the ﬂawed assumption that the diagonal entries diag(S)
of the P-Rank similarity matrix were not considered. We argue that
P-Rank is deﬁned recursively, and hence, the omission of diag(S)
has an impact on the similarity of a vertex with itself, and recursively, it has an impact on the similarity of different pairs of vertices.

2.

SIMFUSION ESTIMATION REVISED

In this section, we ﬁrst revisit the deﬁnition of data space and data relation. We then introduce the notion of the Uniﬁed Adjacency
Matrix (UAM) to revise the SimFusion model.

2.1

Data Space and Data Relation

Graphs studies here are digraphs having no multiple edges.

366

Data Space. A data space is the ﬁnite set of all data objects
where S is called the Uniﬁed Similarity Matrix (USM) whose
(vertices) with the same data type, denoted by D = {o1 , o2 , · · · }.
(i, j)-entry represents the similarity score between object i and j.
|D| denotes the number of data objects in D. Two nonempty data
The uniqueness and existence of the SimFusion+ solution S to
spaces D and D are said to be disjoint if D ∩ D  = ∅.
Eq.(1) can be established by the power iteration [14, pp.381]. A
Throughout the paper, we shall use the following notations. (i)
detailed proof will be shown in Proposition 1 (Section 3).
The entire space D in a network is the union of N disjoint daThe revised notion of SimFusion utilizes UAM (rather than

URM) to represent data relations because UAM can effectively
ta spaces D1 , · · · , DN such that D = N
i=1 Di and Di ∩ Dj =
avoid divergent and trivial similarity solutions while well preserv∅ (i = j). (ii) The total size |D| of the entire space, denoted by
ing the intuitive reinforcement assumption of the original modn, is the sum of the number ni of the data objects in each data space
el [1]. We observe that the root cause of the ﬂawed solution to
Di , i.e., n = N
n
with
n
=
|D
|
(∀i
=
1,
·
·
·
,
N
).
i
i
i=1 i
the original SimFusion is the “row normalization” of URM. Thus,
Intuitively, for heterogeneous networks, the distinct spaces Di
by using UAM, we have an opportunity to postpone the operation
form a partition of D into classes. For homogenous networks, the
of “row normalization” in a delayed fashion. To this end, we utilize
partition of D is itself.
the matrix 2-norm A · S · AT 2 to squeeze similarity scores in
Data Relation. A data relation on D is deﬁned as R ⊆ D × D,

S into [0, 1]. The obtained similarity results in USM can not only
where (o, o ) ∈ R is a connection (a directed edge) from object
prevent the divergence issue and the trivial solution but effectively
o to o . Data objects in the same data space are related via intracapture the reliability of the similarity evidence between data obtype relations Ri,i ⊆ Di × Di . Data objects between distinct data
jects. For instance, the SimFusion+ USMs in Examples 1 and 2 are
spaces are related via inter-type relations Ri,j ⊆ Di ×Dj (i = j).
nontrivial and intuitively explainable.
Intuitively, the intra-type relation carries connected information
in each data space (e.g., co-citation between web pages); the intertype relation represents interlinked information between different
3. COMPUTING SIMILARITY VIA DOMIdata spaces (e.g., making user requests). As an example, in Figure
NANT EIGENVECTOR
1 there are three data spaces : D = {P1 } ∪ {P2 , P3 } ∪ {P4 , P5 },
A
conventional
approach for ﬁnding the SimFusion+ solution S
where (P2 , P2 ), (P2 , P3 ), (P3 , P2 ), (P4 , P5 ), (P5 , P4 ) are intrato
Eq.(1)
is
to
employ
the following ﬁxed-point iteration: 1
type relations; (P1 , P2 ), (P2 , P1 ), (P3 , P1 ), (P1 , P3 ), (P1 , P4 ), (P4 , P1 )
are inter-type relations.
A · S(k) · AT
S(k+1) =
.
(2)
A · S(k) · AT 2

2.2 Uniﬁed Adjacency Matrix

However, as the matrix multiplication may contain O(n3 ) operations, it requires O(kn3 ) time and O(n2 ) space to compute Eq.(2)
for k iterations, which may be quite expensive.
In this section, we study the optimization techniques to improve
the computation of SimFusion+. Our key observation is that SimFusion+ computation can be converted into ﬁnding the dominant
eigenvector of the UAM A. The idea is to calculate the dominant
eigenvector of A once, ofﬂine, for the preprocessing, and then it
can be effectively memorized to compute similarity at query time.
We ﬁrst revisit the deﬁnition of the dominant eigenvector.

Let us now introduce the uniﬁed adjacency matrix (UAM). Consider a graph G = (D, R) with data space D and data relation R.
Uniﬁed Adjacency Matrix (UAM). The matrix A = Ã + 1/n2
of size n × n is said to be a uniﬁed adjacency matrix of the relation
R whenever
⎛
⎞
λ1,1 A1,1

⎜ λ2,1 A2,1
Ã = ⎜
.
⎝
.

.
λN,1 AN,1

λ1,2 A1,2
λ2,2 A2,2
.
..
λN,2 AN,2

···
···
..
.
···

λ1,N A1,N
λ2,N A2,N ⎟
⎟,
.
⎠
..
λN,N AN,N

D EFINITION 1 ( [14, P.379]). The dominant eigenvector of
the X is an eigenvector, denoted by σmax (X), corresponding to
the eigenvalue λ of the largest absolute value of X such that

where (i) 1 is the n × n matrix of all ones; (ii) Ai,j is the submatrix of A whose (o, o )-entry equals 1 if there is an edge from
data object o to o , i.e., ∃ (o, o ) ∈ R, n1j if data object o has no
neighbors in Dj , or 0 otherwise; and (iii) λi,j is called the weighting factor between data space Di and Dj with 0 ≤ λi,j ≤ 1 and
N
j=1 λi,j = 1 (∀i = 1, · · · , N ).
Intuitively, Ai,j represents the intra- (i = j) or inter- (i = j)
relation from data space Di to Dj . λi,j reﬂects the relative importance between data spaces Di and Dj .

X · σmax (X) = λ · σmax (X) with σmax (X)

⎡ 1  1  1  ⎤ ⎡ 1
1 6 11 3 10
2
2
⎢     1 1 ⎥ ⎢ 1
⎢ 1
⎥ ⎢
7 1 1
1 2 2 ⎥
⎢1
⎢6
⎢6
⎢
12
4 1 1 ⎥
⎥ = ⎢ 16
1
10
⎦ ⇒ Ã = ⎢
⎢   
⎢
 2 2  ⎥
⎢ 1
⎥
⎢1
1 1
⎣1
⎣3
1 2 2
5 0 1 ⎦

Λ=

D1
D2 ⎣
D3

2
1
6
1
3

6
7
12
1
4

3
1
4
5
12

3

0

4

1 1
2 2

12

10

0

1 1 1
0
6 6 3
7 7 1 1
12 12 8 8
7
0 18 18
12
1 1
5
0 12
8 8
1 1 5
0
8 8 12

P ROPOSITION 1. Let A be the UAM of network G = (D, R).
The SimFusion+ matrix S can be computed as
[S]i,j = [σmax (A)]i × [σmax (A)]j ,

A · S · AT
A · S · AT

,

(3)

where []i,j denotes the (i, j)-entry of a matrix, and []i denotes
the i-th entry of a vector.
P ROOF. We shall use the knowledge of Kronecker product (⊗)
and vec operator (see [15, p.139] for a detailed description).

⎤

(i) We ﬁrst prove that vec(S) = σmax (A ⊗ A).

⎥
⎥
⎥
⎥
⎥
⎥
⎥
⎦

Taking vec() on both sides of Eq.(2) and applying Kronecker
property vec(BCDT ) = (D ⊗ B) · vec(C) [15, p.147] yield
vec(S(k+1) ) =

SimFusion+ Model. In light of the UAM A, we next propose
the revised model of SimFusion, termed SimFusion+, as follows:
S=

= 1.

The dominant eigenvector of the UAM can be utilized for speeding
up SimFusion+ computation based on the following proposition.

E XAMPLE 3. In Figure 1, the relative importance between data
space Di and Dj is denoted by a weighting matrix Λ = (λi,j )3×3 .
Then, the UAM A of G1 can be derived from A = Ã + 1/n2 ,
where Ã is computed from Λ as follows.
D1 D2 D3
⎡ 1 1 1 ⎤

2

1

(A ⊗ A) · vec(S(k) )
.
(A ⊗ A) · vec(S(k) ) 2

(4)

Note that the uniqueness of the SimFusion+ solution guarantees
that S is insensitive to the initial guess S(0) . For convenience, we
choose S(0) = 1 of all 1s, which can be interpreted as “initially, no
other vertex-pair is presumably more similar than itself”.

(1)

2

367

Let x(k)  vec(S(k) ) and M  A ⊗ A. Then Eq.(4) having
the form x(k+1) = Mx(k) / Mx(k) 2 ﬁts the power iteration
paradigm [14, pp.381], which follows that the sequence {x(k) }
converges to the dominant eigenvector of M. This in turn implies
vec(S)  lim vec(S(k) ) = σmax (A ⊗ A).
k→∞

One caveat is that the convergence of vec(S(k) ) is ensured by the
positivity of A⊗A [16, p.508] 2 . This is true because A is positive
and the self-Kronecker product of two positive matrices preserves
positivity.
(ii) We next show that σmax (A ⊗ A) = σmax (A) ⊗ σmax (A).
Since A · σmax (A) = λ · σmax (A), it follows that
(A ⊗ A) · (σmax (A) ⊗ σmax (A)) = (Aσmax (A)) ⊗ (Aσmax (A))
= (λ · σmax (A)) ⊗ (λ · σmax (A)) = λ2 · (σmax (A) ⊗ σmax (A)).
This implies that the dominant eigenvector of A ⊗A is actually the
self-Kronecker product of the dominant eigenvector of A. Hence,
vec(S) = σmax (A ⊗ A) = σmax (A) ⊗ σmax (A).
It can be noticed that the (i, j)-entry of the matrix S (i.e., the
((i−1)×n+j)-th entry of the vector vec(S)) is exactly the product
of the i-th and j-th entries of σmax (A). Thus, Eq.(3) holds.

C OROLLARY 1. The SimFusion+ matrix S is a rank 1 matrix.
P ROOF. Applying Eq.(3) to S, we obtain that for any two rows
of S, ∃ ω = [σmax (A)]x /[σmax (A)]y s.t. [S]x,∗ = ω × [S]y,∗ .
Hence, the rank of S is 1.

Proposition 1 provides the efﬁcient technique for accelerating
SimFusion+ computation. The central point in optimizing S computation is that only matrix-vector multiplication is used for computing σmax (A). Once calculated, the vector σmax (A) is memorized and thus will not be recomputed when subsequently required,
as opposed to the naive matrix-matrix multiplication in Eq.(2).

4.

E XAMPLE 4. Consider the graph G1 in Fig.1 with its UAM A
already computed in Example 3. The dominant eigenvector of A is
σmax (A) = [.431

.673

.451

.322

T

.232] .

Note that σmax (A) is calculated only once for the preprocessing
and can be used for computing any entry of S at query time, e.g.,
[S]1,2 = [σmax (A)]1 × [σmax (A)]2 = .431 × .673 = .290.

L EMMA 1 ( [17, PP.25-33]). Let A be an n × n matrix.
Then, for every k = 1, 2, · · · , we have the following results.
(a) There exists a unique k × k almost triangular matrix Tk s.t.

[S]1,3 = [σmax (A)]1 × [σmax (A)]3 = .431 × .451 = .194.
Regarding computational complexity, our approach only needs
O(km) preprocessing time and O(n) space to compute σmax (A)
by using the following power iteration [14, pp.381]:
ξ

= e,

ξ

(k+1)

Aξ(k)
=
Aξ(k)

2

Ãξ(k) + γ (k) e
=
,
Ãξ(k) + γ (k) e 2

VkT AVk = Tk ,

(6)

where Vk = [v1 v2 · · · vk ] is an n × k matrix consisting of k
orthonormal column-vectors vi ∈ Rn (i = 1, · · · , k).
(b) The difference between AVk and Vk Tk is a zero matrix
except the last column. Precisely, there exist a small scalar δk and
an orthonormal vector vk+1 ∈ Rn such that

(5)

(k)
where e  (1, · · · , 1)T ∈ Rn and γ (k)  n12 n
]i . 3 The
i=1 [ξ
existence and uniqueness of the dominant eigenvector σmax (A) is

AVk − Vk Tk = δk vk+1 eTk ,

2

According to the Perron-Frobenius theorem [14, p.383], the positivity of A ⊗ A ensures that there exists a unique dominant eigenvector of A⊗A associated with its eigenvalue being strictly greater
in magnitude than its other eigenvalues.
3
The correctness of Eq.(5) can be proved as follows:
Aξ(k) = (Ã+ n12 eeT )ξ(k) = Ãξ(k) +γ (k) e with γ (k) =

ESTIMATING SIMFUSION+ WITH BETTER ACCURACY

After the dominant eigenvector σmax (A) has been suggested to
speed up SimFusion+ computation, the algorithm presented in this
section can guarantee more accurate similarity results.
The main idea of our approach is to leverage an orthogonal subspace for “upper-triangularizing” the UAM A (n × n dimension)
n. Due to Tk
into a small matrix Tk (k × k dimension) with k
small size and almost “upper-triangularity”, computing the dominant eigenvector σmax (Tk ) is far less costly than straightforwardly computing σmax (A). We show that the choice of k provides a
user-controlled accuracy over the similarity scores. The underlying rationale is that the dominant eigenvector of a matrix can be
well-preserved by an orthogonal transformation.
We ﬁrst use the technique of the Arnoldi decomposition [17] to
build an order-k orthogonal subspace for the UAM A.

Then using Eq.(3) for computing S yields
⎡
⎤
.186 .290 .194 .139 .100
⎢.290 .453 .304 .217 .156⎥
⎢
⎥
S = ⎢.194 .304 .203 .145 .105⎥ .
⎣.139 .217 .145 .104 .075⎦
.100 .156 .105 .075 .054

(0)

guaranteed by the combination of Perron-Frobenius theorem [14,
p.383] and the positivity of A. Thus, by applying the power
method, the sequence {ξ(k) } converges to σmax (A). Then, with
σmax (A) being memorized, only O(1) time is required at query
stage for computing each entry of S via Eq.(3). Indeed, due to
S symmetry, only n(n + 1)/2 entries [S]i,j (i ≤ j) need to be
computed. In contrast to the O(kn3 ) time and O(n2 ) space of the
conventional iterations, our approach is a signiﬁcant improvement
achieved by σmax (A) computation.
Our method of memorizing σmax (A) can extra accelerate SimFusion+ computation when only a small portion of similarity values of S need to be computed. Speciﬁcally, for certain applications like K-nearest neighbor (KNN) queries, given a vertex i as a
query, one needs to retrieve the top-K ( n) most similar vertices
in a graph by computing the i-th row of S. Before Proposition 1
is introduced, computing the similarity of only one vertex-pair still
requires O(kn3 ) time. In contrast, using the memorized σmax (A),
we only need O(1) time for computing a single entry of S at query
time. In fact, for KNN queries, after σmax (A) is memorized with
its entries sorted in an descending order for the preprocessing, it only takes constant time to retrieve the top-K results at query stage.
Proposition 1 also gives an interesting characterization of the
SimFusion+ matrix.

T

(7)

k

where ek = (0, · · · , 0, 1) ∈ R is a unit vector.

1 T (k)
e ξ .
n2

368

As depicted in Fig.3, by using the upper-triangularization process 4 , the matrix A ∈ Rn×n can be transformed into the small almost triangular Tk ∈ Rk×k by the n × k orthonormal ma4
From the computational viewpoint, Vk , Tk , vk and δk in Eq.(7)
can be obtained by an algorithm in our later developments.

n

almost upper
triangular

k

n
k

k

k

·

VkT

A

n

·

Vk n

=

≈

n

Tk k

·

Vk n

Algorithm 1: SimFusion+ (G, , (u, v))

k

Input : Network G = (D, R), accuracy , vertex pair (u, v).
Output: Similarity score s(u, v).
1 compute the matrix Ã ∈ Rn×n of the UAM A in G ;
T
2 initialize e ← (1, 1, · · · , 1) ∈ Rn , and v1 ← √1n e ;

σmax (Tk )

σmax (A)

Rn×n → Rk×k

3 foreach iteration k = 1, 2, · · · do
4
initialize the auxiliary vector w ← Ãvk + n12 (eT vk )e ;

Figure 3: Upper Triangular Process of UAM
trix Vk for every iteration. As k increases, Tk+1 can be iteratively obtained by bordering the matrix Tk at the last iteration
(i.e., Tk+1 = [ Tk  ]), and Vk+1 by augmenting the matrix Vk at
the last iteration with the vector vk+1 (i.e., Vk+1 = [Vk vk+1 ]).
When k = rank(A), it follows that δk = 0.

5
6
7
8
9
10
11
12

E XAMPLE 5. Consider the network G1 in Fig.1 and its UAM
A = Ã + 1/52 in Example 3. For k = 3, ∃ V3 = [v1 v2 v3 ] ∈
R5×3 mapping A ∈ R5×5 into T3 ∈ R3×3 s.t. T3 = V3T AV3 ,
where
⎡1
2

⎢1
⎢6
⎢
Ã = ⎢ 16
⎢1
⎣3

0

⎤
1 1 1
0
6 6 3
7 7 1 1 ⎥
12 12 8 8 ⎥
⎥
7
0 18 81 ⎥ ,
12
⎥
5 ⎦
1 1
0
8 8
12
1 1 5
0
8 8 12

⎡

⎤
.447 .125 −.089
⎢
⎥
1.08 .298 0
⎢ .447 .750 .044 ⎥
⎥.
T3 = ⎣.298 .190 .359 ⎦ , V3 = ⎢
−.125
.710
.447
⎢
⎥
⎣ .447 −.125 −.696 ⎦
0 .359 −.083
.447 −.625 .032
⎡

⎤

∃ δ3 = .231, v4 = [−.881 .328 .137 .280 .135]T s.t. Eq.(7) holds.
(see Example 7 for a detailed iterative process)

Ŝ3 − S

(8)

≤ k ,

Notice that if k = rank(A) ( n), 5 then k = 0 and the kapproximation similarity matrix Ŝk becomes the conventional exact USM S. From this perspective, the k-approximation similarity
can be regarded as a generalization for the conventional similarity.
One of the possible ways of choosing an appropriate low order
k for achieving the desired accuracy  is to calculate the estimation
error k from Eq.(10) in an a-posteriori fashion after each iteration
k = 1, 2, · · · . 6 The iterative process stops once k ≤ . Due
to k decreasing monotonicity, such k is the minimum low order
s.t. Ŝk − S 2 ≤ . More concretely, the residual δk in Eq.(7)
(Lemma 1) approaches 0 as k is increased to n, which implies

(9)
(10)

(Please refer to the Appendix for a detailed proof.)
The parameter k is intended as a user control over the difference between the approximate and the exact similarity matrices,
and hence k is generally chosen by a user. Provided that k is selected to satisfy Eq.(10), Proposition 2 states that the gap between
the approximate and the exact similarity scores does not exceed k .

k = 2 × |δk | × |[σmax (Tk )]k | ≤ 2 × |δk |.
Hence, the condition k ≤  (with k being obtained from Eq.(10))
can be used as a stopping criterion for determining the minimum
low order k needed for the desired accuracy .
Capitalizing on Eq.(8) and Proposition 2, below we provide an
algorithm for SimFusion+ computation with accuracy guarantee.
Algorithm. SimFusion+ is shown in Algorithm 1. It takes as
input a network G = (D, R), a desired accuracy , and a vertex
pair (u, v); it returns the approximate similarity ŝ(u, v) such that
|ŝ(u, v) − s(u, v)| ≤  with s(u, v) being the exact value.
Before illustrating the algorithm, we ﬁrst present the notations it uses. (a) [Tk ]i,j is the (i, j)-entry of the matrix Tk , and
[σmax (Tk )]i is the i-th entry of the eigenvector σmax (Tk ). (b)

E XAMPLE 6. Consider the network G1 in Fig.1 and the matrix
T3 , V3 , δ3 given in Example 5. For k = 3, we have
σmax (T3 ) = [.945 .316 .089]T .
Therefore, V3 · σmax (T3 ) = [.454 .663 .447 .321 .228]T . Then
applying Eq.(8) and the exact S in Example 4 yields
Ŝ3 = {Using Eq.(8)} =

.206
⎢.301
⎢
⎢
⎢.203
⎢
⎣.146
.103

.301
.440
.296
.213
.151

.203
.296
.199
.143
.102

.146
.213
.143
.102
.073

⎤
.103
.151⎥
⎥
⎥
.102⎥
⎥
.073⎦
.051

= .0257,

k = 2 × |.231 × .089| = .0411.

and δk is a small scalar given in Eq.(7); [σmax (Tk )]k is the k-th
entry of the dominant eigenvector of Tk .

⎡

2

which is smaller than k (using Eq.(10) with δ3 = .231)

where
k = 2 × |δk × [σmax (Tk )]k |,

σ̂max (A) ← [v1 |v2 | · · · |vk ] · σmax (Tk ) ;

16 free v1 , · · · , vk , σmax (Tk ) ;
17 compute the approximate similarity score of (u, v)

We note that the gap between S and Ŝk for k = 3 is actually

P ROPOSITION 2. For every k = 1, 2, · · · , the following estimate holds:
2

14 free w, Tk , vk+1 , δk ;
15 compute the approximate dominant eigenvector σ̂max (A)

ŝ(u, v) ← [σ̂max (A)]u × [σ̂max (A)]v ;

where Vk and Tk can be obtained from Lemma 1.
To differentiate Ŝk from S, we shall refer to S as exact similarity.
The following estimate for the approximate similarity Ŝk with
respect to the exact S can be established.

Ŝk − S

compute the residual scalar δk ← w2 ;
ﬁnd the dominant eigenvector σmax (Tk ) ;
estimate the error k ← 2 × |δk × [σmax (Tk )]k |;
if k ≤  then exit for ;
compute the residual vector vk+1 :
w ← w/δk and vk+1 ← w ;
free δk , σmax (Tk ) ;

18 return ŝ(u, v) ;

In light of Lemma 1, we next provide an error estimate for SimFusion+ similarity when using σmax (Tk ) to compute σmax (A).
Error Estimation. We deﬁne a k-approximation similarity matrix Ŝk over a low-order parameter k:
[Ŝk ]i,j = [Vk · σmax (Tk )]i × [Vk · σmax (Tk )]j ,

13

for i = 1, 2, · · · , k − 1 do
compute the almost upper triangular matrix Tk ∈ Rk×k :
[Tk ]i,k−1 ← vkT · w ;
orthogonalize w s.t. w⊥span{v1 , · · · , vk } :
w ← w − [Tk ]i,k−1 · vk−1 ;

Ŝ3 − S = {Using S in Example 4} =
⎡
⎤
2.02 1.09 .83 .67 .34
⎢1.09 −1.33 −.75 −.41 −.51⎥
⎢
⎥
⎢
⎥
.01 × ⎢ .83 −.75 −.40 −.21 −.30⎥
⎢
⎥
⎣ .67 −.41 −.21 −.09 −.17⎦
.34 −.51 −.30 −.17 −.20

5
When k is set to argmink {δk = 0} (which is practically much
smaller than rank(A)), k = k+1 = · · · = n = 0.
6
As increased by 1 per iteration, the low order parameter k equals
the iteration number.

369

#-line
4
6
7
8
9
10
12

time
O(m)
O(n)
O(n)
O(n)
O(k)
O(1)
O(n)

memory
O(n)
O(n)
O(n)
O(n)
O(k)
O(k)
O(n)

operation
sparse matrix-vector multiplication
vector dot product
vector addition and scalar multiplication
computing the 2-norm of a vector
using the power iteration
getting the vector component
scaling the vector

memorized, yielding O(n) space; v1 , · · · , vk and σmax (Tk ) are
not used subsequently and thus can be freed (line 16).
(ii) For the on-line query (lines 17-18), ŝ(u, v) can be computed
in O(n) space with σ̂max (A) memorized.
Taking (i) and (ii) together, the total space is bounded by O(kn).

5.

Table 1: Running Time & Memory Space Required per Iteration for Algorithm SimFusion+ in Lines 4-12
span{v1 , · · · , vk } is the set of all linear combinations of vectors
v1 , · · · , vk . (c) σ̂max (A) denotes the approximation of σmax (A).
The algorithm SimFusion+ works as follows. It ﬁrst computes
A and initializes v1 (lines 1-2). Using A, it then computes Tk
(lines 4-6), δk (lines 7-8) and vk+1 (line 12) by orthonormalizing the vector Avk with respect to v1 , · · · , vk for every iteration; SimFusion+ also calculates σmax (Tk ) (line 9), and utilizes
δk and σmax (Tk ) to estimate the error k (line 10). The process
(lines 3-13) iterates until k ≤ , i.e., the minimum low order k
is found s.t. k meets the desired accuracy  (line 11). For such
k, the matrix-vector product [v1 |v2 | · · · |vk ] · σmax (Tk ) is used
to approximate the dominant eigenvector of A, and is memorized
to compute σ̂max (A) (line 15). The product of the u-th and v-th
entries of σ̂max (A) is collected in ŝ(u, v), which is returned as the
estimated similarity between vertex u and v (lines 17-18).

T HEOREM 1. The incremental SimFusion+ estimating problem
is solvable in O(δn) time and O(n) space for every vertex pair,
where δ is the number of edges affected by the update Ḡ.
As we shall see later, δ captures the size of areas in a graph G that
is affected by updates Ḡ; hence δ is much smaller than n when Ḡ is
small. That is, the incremental SimFusion+ can be performed more
efﬁciently than computing similarities in G  . This suggests that we
compute the eigenvector of A in G once, and then incrementally
compute SimFusion+ when G is updated.
To prove Theorem 1, we ﬁrst introduce a notion of incremental
UAM. We then devise an incremental algorithm for handling batch
edge updates with the desired bound.

E XAMPLE 7. We show how SimFusion+ estimates the similarity in G1 of Example 1. Given the desired accuracy  = 0.05,
SimFusion+ ﬁrst initializes the UAM A (in Example 3). It then
iteratively computes Tk , vk+1 , δk , σmax (Tk ) and k as follows:
k

Tk

vk+1

δk

σmax (Tk )

0

−

[.447 .447 .447 .447 .447]T

−

−

−

1

[1.08]

[.125 .750 −.125 −.125 −.625]

.298

[1]

.596



2
⎡
3

1.08 .298
.298 .190



⎤
1.08 .298 0
⎣ .298 .190 .359 ⎦
0 .359 −.083

T

T

[−.089 .044 .710 −.697 .032]

.359
T

[−.881 .329 .137 .280 .135 .231]

.231



.957



.290
⎡
⎤
.945
⎢
⎥
⎣.316⎦
.090

INCREMENTAL SIMFUSION+

For certain applications like social networks, graphs are frequently modiﬁed [9]. It is too costly to recalculate similarities every time when edges in the graphs are updated. This motivates us to
study the following incremental SimFusion+ estimating problem.
Given a network G, the eigen-information in G, and a list Ḡ of
updates (edge deletions and insertions) to G, it is to compute the
new USM S in G  . Here G  is the updated G, denoted by G + Ḡ.
The idea is to maximally reuse the eigen-information in G when
computing S . The observation is that Ḡ is often small in practice;
hence, S (= S + S̄) is slightly different from S. It is far less costly
to ﬁnd the change S̄ to the old S than to recalculate the new S from
scratch. The main result in this section is the following.

k

5.1

Incremental Uniﬁed Adjacency Matrix

Consider an old network G = (D, R) and a new G  = (D, R ).
Incremental UAM. The matrix Ā is said to be the incremental
UAM of the update Ḡ (= G  − G, i.e., a list of edge insertions and
deletions) iff Ā = A − A, where A and A are the UAMs of the
old network G and the new G  , respectively.
Intuitively, the nonzero entries of Ā can identify the edges in
G that is affected by updates Ḡ. Typically, Ā is a sparse matrix
when δ is small. Indeed, the number of nonzero entries in Ā is
bounded by O(δn), which represents the costs that are inherent to
the incremental problem itself, i.e., the amount of work absolutely
necessary to be performed for the problem.
Using Ā, we next provide a strategy for incrementally computing SimFusion+ similarity.

.208

.041

The iteration terminates at k = 3 because the estimation error
3 = .041 ≤  (= .05). SimFusion+ then memorizes σ̂max (A) =
[v1 |v2 |v3 ] · σmax (T3 ) and returns the similarity ŝ(u, v), i.e., the
(u, v) entry of Ŝ3 , as shown in Example 6.
We next analyze the time and space complexity of SimFusion+ .
Running Time. The algorithm consists of two phases: preprocessing (lines 1-16), and on-line query (lines 17-18).
(i) For the preprocessing, (a) it takes O(m) time to compute Ã
(line 1) and O(n) time to initialize v1 (line 2). (b) The total time
of the for loop is analyzed in Table 1 (line 3-13), which is bounded
by O(m + 4n + k + 1) for each iteration. (c) It takes O(kn) time
to compute σ̂max (A) (line 15). Hence, the total time in this phase
is O(m + k(m + 4n + k + 1) + kn), which is bounded by O(km).
(ii) The on-line query phase (lines 17-18) can be done in constant
time for each query by virtue of σ̂max (A) memorization.
Combining (i) and (ii), the query time of SimFusion+ is in
O(1), plus an O(km)-time precomputation.

P ROPOSITION 3. Given a network G and an update Ḡ to G, let
A be the UAM of G, and Ā the incremental UAM of Ḡ. Then the
new USM S of the new network G  (= G + Ḡ) can be computed as
[S ]i,j = [ξ ]i · [ξ ]j with [ξ ]i = [ξ1 ]i +
cp =

T
ξp
·η

αp −α1

n
p=2

and η = Ā · ξ1 .

cp × [ξp ]i
(11)

where ξp is the eigenvector of A corresponding to the eigenvalue
αp with ξp 2 = 1, and ξ1 is the dominant eigenvector of A.

Memory Space. (i) In the precomputation, (a) initializing Ã and
v1 takes O(n) space (lines 1-2). (b) For each iteration k, the space complexity is analyzed in Table 1, which is bounded by O(n)
(line 3-13). (c) As the for loop terminates, only v1 , · · · , vk and
σmax (Tk ) are kept in memory, yielding O(kn + k) space; the
other intermediate results can be freed (line 14). (d) Computing
σ̂max (A) takes O(k) space (line 15). Once computed, σ̂max (A) is

(Please refer to the Appendix for a detailed proof.)
The main idea in incrementally computing S is to reuse Ā and
the eigen-pair (αp , ξp ) of the original A. From the computational perspective, memorization techniques can be applied to Eq.(11)
for an extra speed-up in computing [ξ ]i . Once η is computed, it
can be memorized for computing c2 , · · · , cn . When c2 , · · · , cn are
calculated, they can be memorized for computing [ξ ]i and [ξ ]j .

370

λ

except for their u-th entries to 0 if
entries of [Āj,i ],v to nj,i
i
|NDi (v)| = 1; (iii) we set [Āi,j ]u,v = −λi,j otherwise.

Algorithm 2: IncSimFusion+ (G, A, (αp , ξp ), Ḡ, (u, v))

1
2
3
4
5
6
7
8

Input : Network G = (D, R), the old UAM A of G,
eigen-pairs (αp , ξp ) of A, the update Ḡ to G, query (u, v).
Output: New similarity score s (u, v).
compute the incremental UAM Ā for the update Ḡ :
Ā ← UpdateA (G, A, Ḡ) ;
initialize a ← [ξ1 ]u , b ← [ξ1 ]v ;
compute η ← Ā · ξ1 ;
free Ā, ξ1 ;
for p ← 2, · · · , n do
compute t ← ξpT · η, cp ← t/(αp − α1 ) ;
compute a ← a + cp × [ξp ]u , b ← b + cp × [ξp ]v ;
free αp , ξp ;

Complexity. The algorithm IncSimFusion+ is in O(δn) time
and O(n) space for handling δ edge updates in Ḡ. (i) The procedure UpdateA can be bounded by O(δn) time and O(n) space
(line 1). For each edge update in Ḡ, it is in at most O(n) time
and O(n) intermediate space to update the corresponding entries
of Ā. (ii) Computing η requires an O(δ)-time and O(n)-space
sparse matrix-vector multiplication Ā · ξ1 (line 3). (iii) For every
cp , it takes O(δ) time and O(n) space to calculate ξpT · η (line 6)
since η is a sparse vector with only O(δ) nonzeros; and c2 , · · · , cp
are memorized for computing [ξ ]i , which requires O(δn) time and
O(n) space in total. (iv) Computing a, b and s (u, v) needs constant time and space (lines 7 and 10). Thus, combining (i)-(iv) ,
the total complexity is bounded by O(δn) time and O(n) space.

9 free η, t ;
10 compute s (u, v) ← a × b ;
11 return s (u, v) ;

E XAMPLE 8. We show how IncSimFusion+ works. Consider
the graph G1 in Fig.1 with its UAM A in Example 3. Suppose two
edges (P1 , P2 ) and (P2 , P1 ) are removed from G1 . The new USM
S is updated as follows.
First, UpdateA is invoked for precomputing the incremental Ā
and the eigen-pairs (αp , ξp ) of A in an off-line fashion:

5.2 An Incremental Algorithm for SimFusion+
We next prove Theorem 1 by providing an incremental algorithm, referred to as IncSimFusion+, for handling δ edge updates.
Algorithm. The algorithm accepts as input a network G, the
UAM A of G, the eigen-pairs (αp , ξp ) of A, an update Ḡ (a list of
edge insertions and deletions) to G, and a vertex pair (u, v).
It works as follows. (a) IncSimFusion+ ﬁrst computes the incremental UAM Ā for the update Ḡ by using procedure UpdateA (line
1). UpdateA incrementally ﬁnds all the changes to the old UAM
A in the presence of a list of edge updates to G. (b) For the given
vertex pair (u, v), IncSimFusion+ initializes a and b based on the
dominant eigenvector ξ1 of A (line 2) ; it computes η once and
memorizes η for computing c2 , · · · , cn (line 3). (c) Once computed, c2 , · · · , cn are memorized for calculating the u-th and v-th
entries of the dominant eigenvector ξ of the new UAM,which is
collected in a and b, respectively (lines 4-9). IncSimFusion+ returns a × b as the similarity ŝ(u, v) (lines 10-11).
Edge Update. The procedure UpdateA is used for incrementally updating the UAM A by virtue of Ḡ. An update Ḡ is represented
as a sequence of 2-tuples (D × D, op) that records every single
action of the edge update, in which D × D is a set of δ edges to
be inserted or deleted, and op is either “+” (edge insertion) or “−”
(edge deletion). For instance, after the edge (P3 , P5 ) is added and
(P1 , P2 ) is removed from G1 in Fig.1, the update Ḡ is denoted by

⎡

0
⎢−1
⎢ 6
⎢
Ā = ⎢ 0
⎢
⎣ 0
0

⎤
000
0 0 0⎥
⎥
⎥
0 0 0⎥
⎥
0 0 0⎦
000

p

αp

1

1.184

[.431 .673 .451 .322 .232]

ξp

cp

2

.503

[.708 −.522 −.242 .388 .132]

3

-.480

[−.256 −.020 .095 .716 −.641]

4

-.366

[−.021 −.507 .853 −.119 .017]

5

.242

[.497 .127 .037 −.467 −.719]

T
T
T
T

T

−
.062
-.018
-.025
.069

IncSimFusion+ next computes η from Ā and ξ1 (line 3):
η = Ā · ξ1 = [−.112

−.072

0

0

T

0] .

Then, cp can be derived from the memorized η and (αp , ξp ), e.g.,
c2

=

ξ2T · η/(α2 − α1 ) = −.0419/(.503 − 1.184) = .062,

c3

=

ξ3T · η/(α3 − α1 ) = .030/(−.480 − 1.184) = −.018.

Once computed, c2 , · · · , c5 are memorized for calculating [ξ ] :
5
T
cp × ξp = [.327 .703 .485 .326 .266] .
ξ  = ξ1 +
p=2

Hence, applying [ξ ] to the new USM S (line 10) yields

Ḡ = {(P3 , P5 , +), (P1 , P2 , −)}.

⎡

.107
⎢.230

S =⎢
⎣.159
.107
.087

UpdateA identiﬁes the incremental Ā in two phases. (i) It ﬁrst
ﬁnds the affected nodes and the data spaces for each edge update in
Ḡ using a breadth-ﬁrst search. (ii) It then updates the corresponding
entries of Ā based on the following. We abuse the notation ND (u)
to denote all the neighbors of object u in the data space D, i.e.,
ND (u) = {v ∈ D| (u, v) ∈ R}.

− 16
0
0
0
0

6.
N

.230
.494
.341
.230
.187

.159
.341
.235
.158
.129

.107
.230
.158
.107
.087

⎤

.087
.187⎥
.129⎥
⎦.
.087
.071

EXPERIMENTAL EVALUATION

In this section, a comprehensive empirical study of the proposed
similarity estimating methods is presented.

Based on the partition of the entire data space D = i=1 Di with
ni = |Di |, the incremental UAM Ā can be accordingly partitioned
into N 2 submatrices Āi,j .

6.1

Experimental Setting

Datasets. We used three real-life datasets and a synthetic
dataset.
(1) MSN Data. 7 The MSN search log data were taken from
“Microsoft Live Labs: Accelerating Search in Academic Research”. This dataset was also used in the prior work [1]. It contains
about 15M user queries from the United States in May 2006 and the
corresponding clickthrough URLs. The dataset was formatted by
showing each query, the URLs of the associated web pages, and the
number of clickthroughs by query, as depicted below.

• For each edge insertion (u, v, +) ∈ Ḡ with u ∈ Di and
λ
v ∈ Dj , (i) we set all entries of [Āi,j ]u, to − ni,j
except
j
for their v-th entries to 0 if NDj (u) = ∅; (ii) we set all
λ
except for their u-th entries to 0
entries of [Āj,i ],v to − nj,i
i
if NDi (v) = ∅; (iii) we set [Āi,j ]u,v = λi,j otherwise.
• For each edge deletion (u, v, −) ∈ Ḡ with u ∈ Di and
λ
v ∈ Dj , (i) we set all entries of [Āi,j ]u, to ni,j
except
j
for their v-th entries to 0 if |NDj (u)| = 1; (ii) we set all

7

371

http://research.microsoft.com/ur/us/fundingopps/RFPs/Search_2006_RFP.aspx

URL
www.ebay.com

Clicks
1,859

SF+

The 15K most common queries in the search log were chosen, and
the hyperlinks from the contents of the top 32K popular web pages
were parsed. We built a network G = (D, R), which consists of a
web page space Dw and a query space Dq .
(2) DBLP Data. 8 This dataset was derived from a snapshot of
the computer science bibliography (from 2001 to 2010). We selected the research papers published in the following conference proceedings: “SIGIR”, “KDD”, “VLDB”, “ICDE”, “SIGMOD” and
“WWW”. Choosing a time step of two years, we built 5 DBLP web
graphs Gi (i = 1, · · · , 5) with the sizes listed below:
G2 : 01-04

G3 : 01-06

G4 : 01-08

G5 : 01-10

1,838
7,103

3,723
14,419

5,772
29,054

9,567
45,310

12,276
64,208

U1 : CO
867
1,496

U2 : TE
827
1,428

U3 : WA
1,263
2,969

0.2
Query

SR

PR

0.6
0.4
0.2
0

Webpage

SF+

SF

CSF

Query

SR

1

1

0.8

0.8

0.6
0.4

Webpage

0

PR

0.6
0.4
0.2

0.2
1

3

5

7 9 11 13 15
Query

0

1

3

5

7 9 11 13 15
Webpage

Figure 5: The detailed query-by-query and page-by-page comparisons for NDCG10 on MSN data

algorithms. The results are shown in Figure 4, in which the x-axis
categorizes the objects according to query and web page. We ﬁnd
the following. (i) In most cases, SF seems hardly to get sensible
similarities because with the increasing number of iterations, all
the similarities of SF will asymptotically approach the same value. This veriﬁes the convergence issue of the original model [1].
(ii) When SF did not fail, SimFusion+ always gave more accurate
estimation on average than the other algorithms. For instance, for
the top 10 queries, the average NDCG10 of SimFusion+ (0.79) is
10x better than SF (0.07), 39% better than CSF (0.57), 58% better than SR (0.50), and 15% better than PR (0.69), whereas for
the top 30 web pages, the average NDCG30 of SimFusion+ (0.64)
is 12x better than SF (0.05), 45% better than CSF (0.44), 33%
better than SR (0.48), and 21% better than PR (0.53). This is because substituting UAM for URM effectively avoids divergent or
trivial solutions, thus improving the quality and reliability of SimFusion+ similarity, as expected.
To further verify the accuracy, we randomly selected another 15
queries and 15 web pages from MSN data. In Figure 5, the queryby-query and page-by-page comparisons are shown for NDCG10
of the ﬁve algorithms. We observe that (i) for 12 out of 15 queries,
SimFusion+ achieved highest accuracy of the ﬁve algorithms;for
13 out of 15 web pages, SimFusion+ outperformed the other algorithms in its accuracy, and was slightly less accurate than PR for
only 2 pages. (ii) For all the queries and web pages, SimFusion+ showed the best accuracy performance on average, PR the
second, and SF the worst. This is because SimFusion+ uses UAM
to encode the intra- and inter-relations in a comprehensive way,
thus making the results unbiased.
We also evaluated the performance of SimFusion+ on DBLP and W EB KB datasets. In DBLP experiments, 20 authors
were randomly chosen from G1 :01-02, G3 :01-06 and G5 :01-10 data, respectively. We compared the similarity of the top 10 authors
in Gi (i = 1, 3, 5) estimated by the ﬁve algorithms. The results
of the average NDCG10 are depicted in Figure 6. It can be seen
that SimFusion+ again achieved better accuracy on DBLP data.
For instance, SimFusion+ (0.88) on G3 :01-06 was 13x better than
SF (0.06), 95% better than CSF (0.45), 26% better than SR (0.7),
and 19% better than PR (0.74). In W EB KB experiments, we computed NDCG within 10 web pages for each object in each university data (CO,TE,WI,WA) and evaluated the average scores. Figure

U4 : WI
1,205
1,805

6.2 Experimental Results
6.2.1 Accuracy
We ﬁrst evaluated the accuracy of SimFusion+ vs. SF, CSF,
SR and PR in estimating the similarity, using real-life data.
We randomly chose 50 queries and 40 pages from the MSN query
log, and compared the average NDCG10 (and NDCG30 ) of the ﬁve
9

CSF
0.8

Figure 4: Comparing SimFusion+ with other ranking algorithms for the average NDCG10 and NDCG30 on MSN data

(4) Synthetic Data. The data were produced by the C++ boost
graph generator, with 2 parameters: the number of vertices and
the number of edges. Varying the graph parameters, we used this
dataset to represent homogenous networks for an in-depth analysis.
Compared Algorithms. The following algorithms were implemented in C++: (1) SimFusion+ and IncSimFusion+ ; (2) SF,
a SimFusion algorithm via matrix iteration [1]; (3) CSF, a variant
of SF, which leverages PageRank stationary distribution [13]; (4)
SR, a SimRank algorithm via partial sums function [8]; (5) PR, a
Penetrating-Rank algorithm encoding both in- and out-links [4].
Evaluation Metrics. For evaluating the performance of the
algorithms, we used Normalized Discounted Cumulative Gain
(NDCG) metrics [13]. The NDCG at a rank position p is deﬁned as
p
1
2ranki −1
NDCGp = IDCG
i=1 log2 (1+i) , where ranki is the graded relep
vance of the similarity result at rank position i, and IDCGp is the
normalization factor to guarantee that NDCG of a perfect ranking
at position p equals 1.
Twelve IT experts were hired to judge the similarity of the ﬁve
algorithms. The ﬁnal judgment was rendered by a majority vote.
All experiments were run on a machine with a Pentium(R) DualCore (2.00GHz) CPU and 4GB RAM, using Windows Vista. The
algorithms were implemented in Visual C++. Each experiment was
repeated over 5 times, and the average is reported here.

8

0.4

0

For each graph Gi = (Di , Ri ), two data spaces were used: paper
space Dpi and author space Dai .
(3) W EB KB Data. 9 This dataset collects web pages from the
computer science departments of four universities: Cornell (CO),
Texas (TE), Washington (WA) and Wisconsin (WI). It was also
used in the previous work [13] for link-based similarity estimation. For each university, a network GUi = (Di , Ri ) was built, in
which (a) the web pages in Di were classiﬁed into 7 categories (data spaces): student, faculty, staff, department, course, project and
others, and (b) the UAM of Ri represented the hyperlink adjacency
matrix. The sizes of these networks are as follows:
|D|
|R|

0.6

NDCG10

|D|
|R|

G1 : 01-02

SF

0.8

AVG NDCG30

Clicks
2,375

NDCG10

URL
shopping.yahoo.com

AVG NDCG10

Query
Shopping

http://www.informatik.uni-trier.de/˜ley/db/
http://www.cs.cmu.edu/afs/cs/project/theo-20/www/data/

372

CSF

PR

0.6
0.4
0.2

0.8
0.6
0.4
0.2
0

01-02 01-06 01-10
DBLP Dataset

50K
0
01-02 01-04 01-06 01-08 01-10

100

200

CO

TX

WI

4
2
0

WA

SF+
SF
CSF
SR
PR

CO

TX

WI

WA

Figure 8: Comparing the CPU time and memory of the ranking
algorithms on W EB KB data
SF+

SF+
SF
CSF
SR
PR

CPU Time (sec)

100K

300
Memory (MB)

CPU Time (sec)

150K

SF+
SF
CSF
SR
PR

SF+
SF
CSF
SR
PR

200

0
CO TX WI WA
WebKB Dataset

Figure 6: Comparing SimFusion+ with other ranking algorithms for the average NDCG10 on DBLP and W EB KB data
200K

6

300
CPU Time (sec)

AVG NDCG10

AVG NDCG10

0.8

0

SR
1

Memory (MB)

SF

100
0
01-02 01-04 01-06 01-08 01-10

Figure 7: Comparing the CPU time and memory of the ranking
algorithms on DBLP data

SF

105
104
103
102

CSF
Memory (MB)

SF+
1

Query

Webpage

SR
10

PR

3

102
101
100

Query

Webpage

Figure 9: Comparing the CPU time and memory of the ranking
algorithms for the given query and web page on MSN data

6 shows that SimFusion+ outperformed the other 4 algorithms on
CO, WI and WA data, except that PR (0.8) did 6% better than
SimFusion+ (0.75) on TX data. This tells that SimFusion+ accuracy performance is consistently stable on different experimental
datasets.

sion+ on real datasets. Given a sequence of edge updates (δ edge
insertions and deletions in Ḡ), we compared the running time of
IncSimFusion+ with that of SimFusion+ ; the latter had to recalculate the UAM when edges were updated, and the computational cost was counted. In these experiments, the eigen information
of the old UAM can be preconditioned in an off-line fashion and
shared by all the updated graphs for incremental computation, and
hence their costs were not counted at the query stage. Due to space
limitations, below we only reported the results on MSN dataset.
Varying δ (the number of the edges to be updated) from 600 to
3000, we ﬁrst evaluated the running time of IncSimFusion+ and
SimFusion+ over MSN data, respectively, for estimating all the
similarities between the query objects in Dq . Figure 10 shows that
IncSimFusion+ outperformed SimFusion+ when δ < 2800, but
SimFusion+ performed better for larger δ, as expected. This is
because the small value of δ often preserves the sparseness of the
incremental UAM, and hence reduces the computational cost of η
when the USM was incrementally updated.
To further validate the performance of IncSimFusion+ , we estimated the similarity among the web page in Dw and tested its
CPU time on MSN data. In Figure 10, the result shows that IncSimFusion+ was highly efﬁcient for the small number of edge
updates. When δ > 7700, SimFusion+ did better than IncSimFusion+ . This tells that increasing the number of updated edges
induces more nonzeros in Ā, thus increasing the difﬁculty of incremental computation. We also noticed that the SimFusion+ time
was less sensitive to the small number of updated edges, whereas
the IncSimFusion+ time was linearly increased with δ, as expected. This is because once the edges are changed, SimFusion+ has
to recompute all the similarities from scratch. In contrast, IncSimFusion+ only computes the similarities from the affected area of
edge updates. In light of this, IncSimFusion+ scales well with δ.

6.2.2 CPU Time & Memory Space
We then evaluated the running time and memory space efﬁciency
of SimFusion+ , SF, CSF, SR and PR using real datasets.
Figure 7 shows the CPU time and memory consumption for the
ﬁve algorithms on DBLP. The total time and memory for each algorithm showed an increasing tendency with the growing size of
DBLP. It can be noted that the time for SimFusion+ was at least
one order of magnitude faster than CSF and SR on average, and
more than 20x faster than PR and SF, whereas the space for SimFusion+ and SR increased linearly with the size of DBLP, in contrast with the quadratic increase in memory for SF, CSF, PR, as
expected. This drastic speedup and decrease in RAM is due to the
memorization of σmax (Tk ) for computing USM, thus saving much
time and space for repetitive matrix multiplications.
To further evaluate the efﬁciency, we compare the time and memory of the ﬁve ranking algorithms on W EB KB. In Figure 8, the
results indicate that SimFusion+ took about 10x less time than
SF and PR, and 6x less time than CSF and SR on average. The
memory space for SimFusion+ was also efﬁcient and scaled well
with the size of W EB KB. It can be seen that SF also took small
memory space (approx. 1M) when the data size was small (e.g., CO
and TX). However, when the data size increased (e.g., WI and WA),
SF was less useful since large memory storage (about 2.5M) was
required to keep the intermediate result of the k-th iterative USM.
In all the cases, SimFusion+ performed the best.
On large datasets, the effect of SimFusion+ is even more pronounced. Figure 9 reports the average time and memory of the
ﬁve algorithms on MSN data, in which the y-axis is log-scale. We
chose 50 queries and 40 web pages from Dq and Dw , respectively.
For each oq ∈ Dq (resp. ow ∈ Dw ), we estimated the similarity
between oq (resp. ow ) and other object o ∈ Dq ∪ Dw . We see that
SimFusion+ was highly efﬁcient on large datasets (nearly 2 orders of magnitude than SF and PR, and 1 order of magnitude than
CSF in both time and space). This validates that the performance
of SimFusion+ is fairly stable among different datasets.

Effect of 
We used 9 web graphs with the number of vertices increased
from 600K to 1.4M. We varied  from 0.01 to 0.0001 and ran SimFusion+ on each graph. The results are reported in Figure 11. It
can be seen that the computational time and memory consumption
for SimFusion+ was sensitive to . The smaller the  is, the larger amounts of the CPU time and memory space are, as expected.
These conﬁrmed our observation in Section 4, where we envisage
that the small choice of  imposes more iterations on computing
Tk and vk , and hence increases the estimation costs.
6.2.4

6.2.3 Incremental Performance
We next evaluated the incremental performance of IncSimFu-

373

3K
2K

15K

IncSF
SF+

|D| = 47, 000
|R| = 127, 600
|Dq | = 15, 000

1K
0
600

CPU Time (sec)

CPU Time (sec)

4K

10K

Appendix: Proofs

IncSF
SF+

Proof of Proposition 2.
P ROOF. Let ψ(x) = Ax − α(x) · x be a vector function of x ∈
Rn , with α (x) being a real function of x. To simplify notations,
we shall denote by αk the dominant eigenvalue of Tk , and

|D| = 47, 000
|R| = 127, 600
|Dw | = 32, 000

5K
0
800 2400 4000 5600 7200 8800

1200 1800 2400 3000
#-edges Updated

ηk = σmax (Tk ), ξk = Vk · σmax (Tk ), ξ = σmax (A).

#-edges Updated

Using Tk ηk = αk ηk and Eq.(7) in Lemma 1, we have

CPU Time (sec)

200K
150K

 = 0.0001
 = 0.001
 = 0.01

100K
50K
0
0.6M 0.8M 1M 1.2M 1.4M

Memory Space (MB)

Figure 10: Comparing the CPU time of IncSimFusion+ with
that of SimFusion+ on MSN data

ψ (ξk ) = Vk Tk ηk + δk vk+1 eTk ηk − αk Vk ηk
= δk vk+1 (eTk ηk ) = δk [ηk ]k vk+1 ,

1,000

500

where [ηk ]k denotes the k-th entry of ηk . Hence,




ξk − ξ 2 ≤ ψ (ξk ) 2 = δk [ηk ]k  vk+1 2 = δk [ηk ]k .

250

Since vec(Sk ) = ξk ⊗ ξk and vec(S) = ξ ⊗ ξ, we have

750

 = 0.0001
 = 0.001
 = 0.01

Sk − S

0
0.6M 0.8M 1M 1.2M 1.4M

Figure 11: Effect of  for SimFusion+ on synthetic data

7. CONCLUSIONS

2

= vec(Sk ) − vec(S) 2 = ξk ⊗ ξk − ξ ⊗ ξ
= ξk ⊗ (ξk − ξ) + (ξk − ξ) ⊗ ξ 2
≤ ξk 2 · ξk − ξ 2 + ξk − ξ 2 · ξ 2
  
  
≤1

We present SimFusion+, a revision of SimFusion, for preventing
the trivial solution and the divergence issue of the SimFusion model. We propose efﬁcient techniques to improve the time and space
complexity of SimFusion+ computation with accuracy guarantees.
We also devise an incremental algorithm to compute SimFusion+
similarity on dynamic graphs when edges are frequently updated.
The empirical results on both real and synthetic datasets show that
our methods achieve high performance and result quality.
We are currently studying the vertex-updating methods for incrementally computing SimFusion+. We are also to extend our techniques to parallel SimFusion+ computing on GPU.

2

≤1

= 2 × ξk − ξ

2

≤ 2 × |δk × [ηk ]k |,

which completes the proof.
Proof of Proposition 3.
P ROOF. For the new graph G  , let ξ be the dominant eigenvector of A with its eigenvalue α , and ξ̄1 = ξ − ξ1 , ᾱ1 =
α − α1 , Ā = A − A. Then, A ξ = α ξ can be rewritten as
(A + Ā)(ξ1 + ξ̄1 ) = (α1 + ᾱ)(ξ1 + ξ̄1 ).
Expanding the above equation, eliminating Āξ̄1 and ᾱ1 ξ̄1 (the
high-order inﬁnitesimals of ξ̄1 ), and using Aξ1 = α1 ξ1 , we have
Aξ̄1 + η = α1 ξ̄1 + ᾱ1 ξ1 with η = Āξ1 .

8. REFERENCES

[1] W. Xi, E. A. Fox, W. Fan, B. Zhang, Z. Chen, J. Yan, and D. Zhuang,
“SimFusion: Measuring similarity using uniﬁed relationship matrix,” in
SIGIR, pp. 130–137, 2005.
[2] L. Page, S. Brin, R. Motwani, and T. Winograd, “The PageRank citation
ranking: Bringing order to the web,” technical report, Stanford InfoLab,
November 1999.
[3] G. Jeh and J. Widom, “SimRank: A measure of structural-context
similarity,” in KDD, pp. 538–543, 2002.
[4] P. Zhao, J. Han, and Y. Sun, “P-Rank: A comprehensive structural similarity
measure over information networks,” in CIKM, pp. 553–562, 2009.
[5] H. Small, “Co-citation in the scientiﬁc literature: A new measure of the
relationship between two documents,” J. Am. Soc. Inf. Sci., vol. 24, no. 4,
pp. 265–269, 1973.
[6] B. Jarneving, “Bibliographic coupling and its application to research-front
and other core documents,” J. Informetrics, vol. 1, no. 4, pp. 287–307, 2007.
[7] W. Xi, B. Zhang, Z. Chen, Y. Lu, S. Yan, W. Ma, and E. A. Fox, “Link
Fusion: A uniﬁed link analysis framework for multi-type interrelated data
objects,” in WWW, pp. 319–327, 2004.
[8] D. Lizorkin, P. Velikhov, M. N. Grinev, and D. Turdakov, “Accuracy
estimate and optimization techniques for SimRank computation,” VLDB J.,
vol. 19, no. 1, pp. 45–66, 2010.
[9] C. Li, J. Han, G. He, X. Jin, Y. Sun, Y. Yu, and T. Wu, “Fast computation of
SimRank for static and dynamic information networks,” in EDBT,
pp. 465–476, 2010.
[10] G. He, H. Feng, C. Li, and H. Chen, “Parallel SimRank computation on
large graphs with iterative aggregation,” in KDD, pp. 543–552, 2010.
[11] W. Yu, W. Zhang, X. Lin, Q. Zhang, and J. Le, “A space and time efﬁcient
algorithm for SimRank computation,” World Wide Web, vol. 15, no. 3,
pp. 327–353, 2012.
[12] G. Xue, H. Zeng, Z. Chen, Y. Yu, W. Ma, W. Xi, and E. A. Fox, “MRSSA:
An iterative algorithm for similarity spreading over interrelated objects,” in
CIKM, pp. 240–241, 2004.
[13] Y. Cai, M. Zhang, C. H. Q. Ding, and S. Chakravarthy, “Closed form
solution of similarity algorithms,” in SIGIR, pp. 709–710, 2010.
[14] G. Williams, Linear Algebra with Applications. Jones and Bartlett
Publishers, 2007.
[15] A. J. Laub, Matrix Analysis for Scientists and Engineers. SIAM: Society for
Industrial and Applied Mathematics, 2004.
[16] R. A. Horn and C. R. Johnson, Matrix Analysis. Cambridge University
Press, February 1990.
[17] Y. Saad, Iterative Methods for Sparse Linear Systems, Second Edition.
Society for Industrial and Applied Mathematics, 2 ed., April 2003.

(12)

n

Since ξ1 , · · · , ξn of A constitute a basis for R , there exist scalars
c1 , · · · , cn s.t. ξ̄1 = c1 ξ1 + · · · + cn ξn . Substituting this into
Eq.(12) and pre-multiplying both sides by ξjT produce
ξjT

n


ci αi ξi +ξjT η = α1 ξjT

i=1

n


ci ξi +ᾱ1 ξjT ξ1 .

(j = 2, · · · , n)

i=1

Due to ξi orthonormality (i = 1, · · · , n), we have
cj αj ξjT ξj +ξjT η = cj α1 ξjT ξj + ᾱ1 ξjT ξ1 .
  
     
=cj αj

=cj α1

=0

(ξjT η)/(αj

Hence, cj =
− α1 ) with η = Āξ1 (j = 2, · · · , n).
T
To determine c1 , we use the identity (ξ1 + ξ̄1 ) (ξ1 + ξ̄1 ) = 1.
Expanding the left-hand side, eliminating the high-order inﬁnitesimal ξ̄1T ξ̄1 , and replacing ξ̄1 with c1 ξ1 + · · · + cn ξn , we have
n
n


c i ξi +
ci ξiT ξ1 = 1.
ξ1T ξ1 + ξ1T
  
i=1
i=1
=1
     
=c1

=c1

Due to ξ1 , · · · , ξn orthonormality, it follows that c1 = 0. Thus,
vec(S ) = ξ ⊗ ξ with ξ = ξ1 + ξ̄1 = ξ1 +

n

p=2

cp =

374

ξpT η
, and η = Āξ1 .
αp − α1

c p · ξp ,

