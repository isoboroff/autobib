Social Annotations: Utility and Prediction Modeling
Patrick Pantel, Michael Gamon

Omar Alonso, Kevin Haas

{ppantel,mgamon}@microsoft.com

{omalonso,kevinhaa}@microsoft.com

Microsoft Research
One Microsoft Way
Redmond, WA, USA

Microsoft Corp
1065 La Avenida
Mountain View, CA, USA

ABSTRACT
Social features are increasingly integrated within the search
results page of the main commercial search engines. There
is, however, little understanding of the utility of social features in traditional search. In this paper, we study utility
in the context of social annotations, which are markings indicating that a person in the social network of the user has
liked or shared a result document. We introduce a taxonomy of social relevance aspects that inﬂuence the utility of
social annotations in search, spanning query classes, the social network, and content relevance. We present the results
of a user study quantifying the utility of social annotations
and the interplay between social relevance aspects. Through
the user study we gain insights on conditions under which
social annotations are most useful to a user. Finally, we
present machine learned models for predicting the utility
of a social annotation using the user study judgments as
an optimization criterion. We model the learning task with
features drawn from web usage logs, and show empirical evidence over real-world head and tail queries that the problem
is learnable and that in many cases we can predict the utility
of a social annotation.

Figure 1: Example social annotation for the query
“maui hotels”.
Bing or Google may yield reviews from your friends about
the restaurant, or about nearby restaurants liked by your
friends. A query for “maui hotels” may yield a result indicating that your colleagues like the Royal Lahaina Resort on
Facebook and a query for “luau history” may yield a social
annotation on the result “Hawaii Luau History” stating that
your friend has liked or shared the document.
The user can beneﬁt from such social experiences in various ways, including: (a) discovery of socially vetted recommendations; (b) personalized search results; (c) connecting
to the lives of their friends; (d) result diversity; and (e) emotionally connecting with an otherwise static and impersonal
search engine. There is, however, very little understanding
whether these social features are useful or detrimental to the
whole-page user experience, or, for that matter, how to even
measure the utility of social features.
Consider a social annotation feature, such as the one
depicted in Figure 1, where some results on the search results
page (SERP) are enriched with markings indicating some
of your friends that have previously liked or shared that
result1 . Are such endorsements from dearest friends more
relevant to the user than from acquaintances or coworkers?
Are expert opinions or those from friends who live in the
vicinity of the restaurant more valuable? Do annotations on
irrelevant results amplify their negative perception?
Studying such aspects and their eﬀect on social relevance
form the basis of this paper. We begin by enumerating a
taxonomy of social relevance aspects, i.e. cues or criteria
that inﬂuence the perceived utility of social annotations.
We consider aspects related to the user query, the social
connection, and the relevance of the related content (e.g., a
document returned by the search engine). We then deﬁne
measures of social annotation utility and present the results
of a large controlled user study of the interplay between each
of these aspects on head and tail queries drawn from several
months of real-world queries issued to a commercial search

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval

General Terms
Algorithms, Design, Experimentation, Measurement

Keywords
Social relevance, social aspects, web search

1.

INTRODUCTION

Social oﬀerings are becoming table stakes for the major
search engines. Querying for your local pizza restaurant on

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGIR’12, August 12–16, 2012, Portland, Oregon, USA.
Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$10.00.

1
Social networks have diﬀerent terms to indicate explicit
positive user interest of a document, such as like, +1, and
tag. Herein we use the term like generically to indicate any
of these, and dislike to indicate negative interest.

285

engine. We show evidence that social annotations add perceived utility to users in varying degrees according to the
social relevance aspects. Finally, we turn our attention to
the task of automatically predicting the utility of a social
annotation. We model the prediction task using a multiple
additive regression tree model over features available to a
standard search engine. We show empirical evidence that
the task is learnable and that we can automatically predict
the utility of a social annotation. The major contributions
of our research are:

Recently, social features are appearing as explicit userfacing features such as in: (1) annotations, where interest
by the searcher’s social network is visibly marked on an existing search result (v.s. Figure 1); (2) injected results,
where social data, such as tweets or status posts, are presented in a manner similar to and within the existing search
results, but sourced outside of the web corpus; and (3) independent results, where the social data is presented in a
manner not to be mistaken for one of the web search results,
such as in a web answer or direct display. This paper specifically focuses on measuring utility in the context of social
annotations and leaves the analysis of other visualizations
to future work.

• We introduce a taxonomy of social relevance aspects,
drawn from the query, social connection and content,
which inﬂuence the utility of social annotations on the
search results page.
• We conduct a user study to quantify the inﬂuence
and interplay of the social relevance aspects, over realworld social annotation impressions drawn from a commercial search engine.
• We propose a machine learned discriminative model
for predicting the expected added value of a social annotation in an online scenario.
• We empirically show that our model can accurately
predict the relevance of a social annotation.

2.

2.2

Relevance is a multidimensional dynamic concept and there
is a wide range of factors that inﬂuence a user’s perception
of relevance. Via an extensive user study, Barry and Schamber proposed a set of categories capturing users’ relevance
criteria or cues (e.g., accuracy, speciﬁcity, expertise, presentation, etc.) in the context of an information seeking
task [3]. More recently, Borlund provided a framework for
examining relevance in the context of information retrieval
evaluation [4]. The relevance and usefulness of results returned by web search engines are typically evaluated using
variants of nDCG [16], expected reciprocal rank [7], mean
average precision, relative information gain, and a variety of
click/feedback metrics [25, 15], such as (1) clicks on lowerranked documents indicate that higher-ranked documents
are less relevant for the query; and (2) clicks to documents
which are quickly abandoned by the user for other search
results are deemed less relevant for the query. However,
use of these traditional metrics can present challenges when
personalizing and annotating web search results, as higherranked search results may be passed up for lower-ranked
search results with social annotations. As shown in [8] and
[12], annotations and other modiﬁcations to captions can
alter the success rate for users independent of where the
document was ranked in the result set. Fidel and Crandall [10] show factors beyond the document that aﬀect the
perception of relevance, including recency, detail, and genre;
but they do not discuss social factors. This paper extends
their work and also proposes metrics by which the utility of
a social annotation feature can be measured.

RELATED WORK

Most relevant to our work is that of Muralidharan et
al. [18] who show through user studies and eyetracking analysis that the presentation of the social annotation, such
as the size of the proﬁle thumbnail, greatly impacts user
engagement of the annotation. Through anecdotal feedback, participants from their studies conjectured that annotations would be useful in certain social and subjective
topics (e.g., restaurant and shopping queries) or when presented by friends believed to be topical authorities (e.g., a
ﬁtness trainer annotating a ﬁtness web document) or when
in a close relationship with the searcher; but for other situations the participants believed annotations were not helpful,
such as when there is no label explaining why an annotation
was present. In this paper, we build upon their work by
exploring a taxonomy of inﬂuential social relevance aspects,
including query class, content relevance, and social network
aspects; and quantifying their utility and interplay.

2.1

Measuring the Utility of a Search Result

2.3

Social Features in Web Search

Predicting Relevance

At the core of a search engine is the ability to learn to
rank candidate documents according to their relevance to a
user query. As such there is a plethora of work on modeling, feature extraction, selection, and model adaptation, see
Liu [17] for a comprehensive survey and Burges et al. [5] for a
description of the system that won the recent Yahoo! Learning to Rank Challenge. In our work, we introduce a complementary task, that of learning to predict the relevance
of a social annotation. As such, we make use of a state-ofthe-art learning algorithm [27] and predict social relevance
based on runtime features from web usage logs and query
classiﬁers commonly used in web search ranking, as well as
social relevance cues deﬁned in this paper.

Evans and Chi [9] performed a detailed user study about
the role that social interactions play in collaborative search
tasks. Outside of collaborative search, social signals have
primarily been explored as implicit ranking features hidden deep inside the ranking functions [20, 26], for example by examining how users would beneﬁt from personalized search results considering implicit behavior similarity
attributes such as from click-based measures [22, 23, 1]. Bao
et al. [2] further argue that the quality of a web page can
be improved by the amount of del.icio.us annotations and
Carmel et al. [6] show that personalized search improves
the quality of intranet search results. For traditional web
search, Heymann et al. [13] predicted that while social bookmarks can provide ancillary data not present in the web
page, the majority of tags are also present in the document,
or inlinks/outlinks, and therefore would have limited use as
ranking features.

3.

ASPECTS OF SOCIAL RELEVANCE

Consider a search results page consisting of ranked content (e.g., documents, videos, images) in response to a user

286

3.2.1

query. Formally, we deﬁne a social annotation as a tuple, {q, u, c, v}, consisting of a query q, content u, a social
network connection c and the connection’s interest valence
v in the content (e.g., like, dislike or share). For example, Figure 1 illustrates such a social annotation impression
where q is “maui hotels”, u is a relevant Expedia hotel page,
c is “Tim Harrington” and v is like.
In this section we propose a taxonomy of aspects that inﬂuence the utility of a social annotation, spanning the query,
the social connection, and the content.

3.1

The circle of a connection refers to the relation of the
connection to the searcher. Intuitively, a co-worker’s interest
in an article related to the workplace might hold more value
than a family member or friend’s interest. We consider the
following circles: work colleague (wkc), family member
(fam), and friend (frn). Other interesting circles out of
scope for this paper include school friends, college friends,
church friends, and sports club friends.

3.2.2

Query Aspects (QA)

Afﬁnity (SA-AFF)

The aﬃnity between a searcher and a connection refers to
their degree of closeness. Although aﬃnity has a continuous
range, in this paper we consider two aﬃnities: close (cls)
and distant (dst). As conjectured in [18] we generally expect the closeness of a connection to greatly inﬂuence the
perceived utility of a social annotation.

3.1.1

Query Intent (QA-INT)
We divide queries into two sets based on whether they
are navigational (nav) or non-navigational (nnv). We
expect that social annotations will be less valuable when the
user is simply looking for the url of a web page she wants to
reach. Other possible intents that are not explicitly studied
in this paper include informational and transactional intents,
which we grouped within our nnv intent, and which partially
overlap with our QA-CLS aspects described next.
3.1.2

Circle (SA-CIR)

3.2.3

Expertise (SA-EXP)
Whether a connection is an expert (exp) or non-expert
(nex) on the search topic may inﬂuence the value of a social
annotation especially when issuing informational or transactional queries. Other possible values not considered in this
paper include hobbyist and enthusiast.

Query Class (QA-CLS)

The utility of a social annotation may be inﬂuenced by
the class of the issued query. For example, although one
might ﬁnd value in knowing the interest of a social connection when querying for a movie, song, or book, we may
expect only certain connections such as experts to be of interest for a health query. Similarly for local queries, interest
from connections in the vicinity of the target location are
likely more valuable than distant connections.
We focus our analysis on the following query classes covering the majority of social annotations found on a commercial
search engine:

3.2.4

Geographical Distance (SA-GEO)

For local queries, we expect that a connection living near
(nea) the target location will add more value than if living
far (far). For non-local queries, the value of this aspect is
not applicable (n/a). One might also consider the geographic distance between the searcher and the connection,
but we leave this aspect for consideration in future work.

3.2.5

Interest Valence (SA-INT)
Social annotations require both a social network connection and the interest valence of that connection with respect
to the annotated search result. Most experiences today classify the valence as either a like (lik) or a share (shr), i.e.
that the user has shared the document with someone. In
this paper, we also consider a third valence, dislike (dis).

• Commerce (com): Product-related queries seeking
speciﬁcations, prices, comparisons, transactions, and
reviews.
• Health (hea): Queries on health-related topics such
as symptoms, procedures, and treatments.

3.2.6

Out of Scope Aspects

Other aspects may inﬂuence the value of a social annotation, such as the connection’s gender, age, and general
interests. We leave these for further study in future work.

• Movies (mov): Movie title queries.
• Music (mus): Queries for musical artists, bands, and
song lyrics.

3.3

Content Aspects (CA)
Finally, the social annotation is inﬂuenced by the relevance of the document to the intent of the user query. We
consider graded relevance according to the following scale,
similar to that proposed by Järvelin and Kekäläinen [16]:

• Restaurant (res): Local queries related to restaurants, cafes, and drinking establishments.

3.2

Social Connection Aspects (SA)
The social network connection is at the heart of the social annotation and it clearly inﬂuences its utility. Consider
the query “korean restaurant” and a set of resulting links
to nearby korean restaurants. We expect that a connection
who is an expert on korean cuisine may increase the utility
of an annotation, and that a connection living near the localized neighborhood is more important than one far away.
A dear friend’s interest in the restaurant may also hold more
weight than a more distant work colleague. And ﬁnally, the
interest valence, whether positive, negative or neutral might
aﬀect relevance. Below we summarize each of these aspects
and their value ranges.

• Perfect (per): The page is the deﬁnitive or oﬃcial
page that strongly satisﬁes the most likely intent.
• Excellent (exc): The page satisﬁes a very likely or
most likely intent.
• Good (goo): The page moderately satisﬁes a very
likely or most likely intent.
• Fair (fai) The page weakly satisﬁes a very likely or
most likely intent.
• Bad (bad): The page does not satisﬁes the intent.
• Detrimental (det): The page contains content that
is inappropriate for a general audience.

287

COLLE
AGU
ES

SOCIAL RELEVANCE

This section presents the results of a user study quantifying the utility of a social annotation. We analyze the
interplay between the social relevance aspects presented in
Section 3 and identify situations where a social annotation
is more relevant than others. Our approach is to sample social annotation impressions on a commercial search engine
and to have human annotators judge the utility of each impression. In the following sections, we describe our process
for sampling social annotation impressions (i.e., {q, u, c, v}tuples) and the guidelines for judging their relevance. Then,
in Sections 4.4 and 4.5, we present our analysis.

4.1

ILY
AM

F

Cynthia
Chris

Di
ann
ne
e
Dianne

Roger
g

Lisa

Alice

D

Query-URL Sampling

Peterr
Pete

Bob

IS

CLOS

TA
NT
FRIE
NDS

Fernando

DS

Jill

E

FR

I

Figure 2: Simulated social network.

We deﬁne U as the universe of all social annotation impressions observed on a commercial search engine during three
weeks spanning three months of US English web usage logs:
10/7/2011-10/14/2011, 11/4/2011-11/11/2011, and 12/2/
2011-12/9/2011. We admitted only queries that were classiﬁed by the search engine to be in our domains of interest,
namely commerce, health, movies, music, or restaurants (see
Section 3.1.2). We applied a low classiﬁcation threshold to
admit a larger number of queries since we later manually
annotate the query class of all queries in our test set. We
further rejected any query suspected of being bot-generated.
We deﬁne the head of U as all tuples where q is in the
top-20% of the most frequent queries, and the tail as all
tuples where q is in the bottom 30%. A query-frequency
weighted sample of queries from both sets yield our test
queries, referred to as HEAD and TAIL consisting of 2388 and
1375 queries respectively.
For each query in HEAD and TAIL, we randomly selected
one url from U using an impression-weighted sampling (i.e.,
a url with many social annotation impressions in the usage
logs for the query is more likely to be chosen than a url with
fewer social annotation impressions for the query).
We used a crowdsourcing tool to have each query in HEAD
and TAIL manually classiﬁed according to the query classes
deﬁned in Section 3.1.2 as well as an other class. We obtained three judgments per query (7526 judgments) from a
total of 32 independent annotators and kept the majority
vote. The inter-annotator agreement as measured by Fleiss’
κ was 0.495 (moderate agreement).
We further manually judged the relevance of each url to
the associated query in HEAD and TAIL. The relevance categories and guidelines are those listed in Section 3.3. As
this task is known to be more diﬃcult than query classiﬁcation, we employed seven professional independent annotators with experience in search engine relevance testing. The
observed inter-annotator agreement as measured by Fleiss’
κ was 0.176 (slight agreement).

4.2

Emily
John

EN

4.

colleagues have a distant aﬃnity. Friends can have aﬃnity either close or distant.
For each query-url pair in HEAD and TAIL we assigned a
social annotation as follows. First, we randomly sampled a
circle (either work colleague, family, or friend). Then,
we randomly selected an individual from that circle, which
also determines the aﬃnity of the connection (either close
or distant). Next, we randomly picked whether the connection was an expert or non-expert with respect to the
document and we randomly chose the interest valence (either like, dislike or share). Finally, if the query was
annotated as a local query, we randomly determined if the
connection lived near or far from the intended target location. Otherwise, we set the geo-distance aspect to n/a.
Deploying our user study over the actual social networks of
the participants is preferable, however several problems arise
that make this infeasible, most notably privacy concerns.
Also, since our study requires signiﬁcant training and expertise, we found it necessary to hire professional annotators,
thus limiting the total number of independent annotators.
Using their personal networks would not only cause privacy
concerns, but it would also bias towards a non-representative
set of search users. Simulating a social network as we do carries its own risks. Firstly, we expect people’s personal social
networks to vary in terms of attribute value distributions
(for example, some people have only work colleagues in their
network while others have mostly friends and family) as well
as diversity distribution. Although in our setup we assumed
uniform priors for all attributes, if given the true priors it is
trivial to reweight the ﬁndings. Secondly, the degree of an
individual in their social network (i.e., average number of
network connections) is signiﬁcantly larger than the twelve
in our virtual network, and also varies signiﬁcantly2 . To
balance the cognitive load on our judges, the reliability of
judgments, and reducing the risks of simulating a social network, we chose to keep the connection degree small enough
whilst ensuring diversity in age, gender, and ethnicity. Finally, by explicitly drawing the judges’ attention to social
aspects, there is a risk of overemphasizing their importance
and thus inﬂuencing the judgments, though we expect this
inﬂuence to be minimal.

Social Annotation Sampling

In order not to bias the social annotations to the speciﬁc
social networks of our human annotators, we simulated a
social network for our judges. We used this network to create
the connections c and interest valences v for our random
query-url pairs in HEAD and TAIL.
Figure 2 illustrates the virtual social network. It consists
of twelve connections spanning the social circles and aﬃnities deﬁned in Section 3.3. We assume in this network that
family members always have a close aﬃnity whereas work

4.3

Annotation Task

The judges were presented with the following scenario:
2

Hill and Dunbar [14] estimate the average connection degree at 153 and Ugander et al. [24] measured the median
degree of Facebook users in May 2011 at 99.

288

HEAD
Oct 11 Nov 11 Dec 11
Test Cases
Judgments
sig-util
some-util
no-util
dont-know
error

R(T)

TAIL
Oct 11 Nov 11 Dec 11

770
1540

823
1646

795
1590

423
846

486
972

466
932

402
734
364
5
35

634
610
377
10
15

428
785
315
11
51

273
397
164
1
11

363
341
241
22
5

225
449
225
0
33

RRel (T)
RPrec (T)

Table 1: Summary of the test datasets and the human labels broken down by judgment category.

ω function deﬁnition
⎧
⎨ sig − util :
some − util :
ωRel =
⎩ no − util :
⎧
⎨ sig − util :
some − util :
ωP rec =
⎩ no − util :

1
0.5
0
1
1
0

Table 2: Utility metrics of social annotations.
Table 1 summarizes the breakdown of the resulting judgments. Inter-annotator agreement as measured by Fleiss’ κ
on this annotation task was 0.395 (0.487 on HEAD and 0.236
on TAIL), considered moderate to fair agreement. For our
ﬁnal datasets HEAD and TAIL we omit the 2.7% of the judgments that were judged as dont-know and error.

Imagine you issue a search query to a commercial search engine. Amongst the results set, you
see the result below, which someone in your social network has either liked, disliked or shared.
Your task is to judge the utility of this social annotation. Value can be assessed on that the annotation is relevant to you, is useful to you, or
is generally interesting to you.

4.4

Utility Analysis

Let T be a set of test tuples, such as those from HEAD
and TAIL, where ti = {qi , ui , ci , vi }, let A be a set of aspect
values and TA be the subset of tuples in T matching an
aspect value in A. For example, if A = {QA-CLS-hea, SACIR-fam, SA-INT-lik}, then TA is the set of all test tuples
in HEAD and TAIL where q is a health query, and c is a family
connection that has liked u. Finally, let J(t) be the set of
judges that annotated a test tuple t.
We deﬁne R(T ), the expected utility of social annotations
in T , as the average utility of each tuple in T :


j∈J(t) ω(t,j)

The annotators were also presented with their virtual social network from Figure 2 along with a textual description of each of the twelve individuals in the network. For
each test case, the social annotation was graphically presented as though returned from a search engine, similar to
what is illustrated in Figure 1. A textual description of the
annotation is also presented to the annotator, stating the
connection’s circle, aﬃnity, expertise, interest valence and
geo-distance (if the query was a local query, such as for a
restaurant).
For each {q, u, c, v} tuple in HEAD and TAIL the annotation
task is to assess the utility of the social annotation according
to the following guidelines:

R(T ) =

t∈T

|J(t)|

|T |

(1)

where ω : J → R is a real-valued utility function mapping
judgments J to real numbers in the range [0,1], where J is the
set {sig-util, some-util, no-util} deﬁned in Section 4.3.
Table 2 lists the two variants of R(T ) that we report on
in this paper. RRel (T), our relevance utility metric, assigns
a graded utility score to each judgment similar to that done
for query-url relevance judgments [16]. RPre (T) expresses a
binary utility where a social annotation has positive utility
if it is judged as either signiﬁcantly or somewhat relevant,
otherwise negative utility. RP re (T ) can be thought of as a
measure of social annotation precision.
Table 3 lists the overall utility and per aspect utility breakdown, over the HEAD and TAIL data sets4 . The expected overall relevance, at 0.543 indicates that social annotations are
generally somewhat relevant, useful or of interest to users.
Each individual aspect, however, inﬂuences the utility in
very diﬀerent ways. Those aspects with signiﬁcantly more
utility are bolded with a ‡ symbol and those with less utility
are bolded with a † symbol.
Overall, we observe no statistically signiﬁcant diﬀerence
between the expected utility of head vs. tail queries (note
that health queries and restaurant queries seem to have
higher utility on tail, but this is not statistically signiﬁcant).
It is surprising that the query class aspects (QA-CLS) generally do not show diﬀerent utility with respect to the average,
however further analysis in Section 4.5 reveals signiﬁcantly
diﬀerentiated inﬂuence in combination with social aspects.
Also counter to our expectations, judges found equal utility between navigational and non-navigational queries. For
the content aspects (CA), although we observe no statistical

• Signiﬁcant utility (sig-util). The annotation is
substantially relevant, useful, or of interest to you.
• Some utility (some-util). The annotation is somewhat relevant, useful or of interest to you.
• No utility (no-util). The annotation is not relevant,
useful or of interest to you.
• Don’t know (dont-know). I don’t have enough information to assess this annotation.
• Non English/Service error (error). Can’t judge
because content is non-English or there is a service
error (e.g., 404 message, image didn’t load, etc.)
Annotators were encouraged to enter a comment justifying
their judgments and were required to do so if their judgment
was dont-know or error.
Each test tuple in HEAD and TAIL was annotated by two
judges randomly drawn from a pool of seven paid professional independent annotators, for a total of 7532 judgments.
We trained the judges by iterating on the guidelines over an
independent dataset 3 . In cases where the judges disagreed,
we adjudicated as follows. If one judge added a comment
that we determined clearly justiﬁed the judgment, we adjudicated the test tuple to that judge’s decision. If both
judges added a comment deemed clearly justifying their decision or if both judges omitted a comment, then we retained
the disagreeing judgments.
3
The training phase was necessary to achieve a fair interannotator agreement. As such, although it would have been
desirable to crowdsource the judgments we deemed the task
too diﬃcult and that paid professional independent judges
were necessary.

4

289

CA-det had too few judgments to report results.

Samples

QA

SA

CA

HEAD
RRel

RPrec

Samples

TAIL
RRel

RPrec

ALL

2388

0.543 ± 0.011

0.771 ± 0.013

1375

0.541 ± 0.014

0.763 ± 0.016

QA-INT-nav
QA-INT-nnv

785
1603

0.539 ± 0.019
0.545 ± 0.014

0.775 ± 0.022
0.769 ± 0.015

93
1282

0.513 ± 0.054
0.543 ± 0.015

0.753 ± 0.066
0.764 ± 0.017

QA-CLS-com
QA-CLS-hea
QA-CLS-mov
QA-CLS-mus
QA-CLS-res
QA-CLS-oth

701
169
233
674
159
452

±
±
±
±
±
±

0.785 ± 0.023
0.772 ± 0.047
0.740 ± 0.041
0.809‡ ± 0.023
0.730 ± 0.052
0.723† ± 0.030

208
36
61
762
51
257

SA-CIR-wkc
SA-CIR-fam
SA-CIR-frn

818
788
782

0.494† ± 0.016
0.654‡ ± 0.019
0.484† ± 0.021

0.782 ± 0.021
0.841‡ ± 0.019
0.690† ± 0.025

470
431
474

0.498† ± 0.022
0.625‡ ± 0.024
0.508 ± 0.026

0.779 ± 0.027
0.810‡ ± 0.027
0.706† ± 0.031

SA-AFF-cls
SA-AFF-dst

1169
1219

0.657‡ ± 0.015
0.434† ± 0.014

0.845‡ ± 0.015
0.701† ± 0.019

689
686

0.622‡ ± 0.020
0.460† ± 0.019

0.804‡ ± 0.021
0.722 ± 0.025

SA-EXP-exp
SA-EXP-nex

1194
1194

0.635‡ ± 0.016
0.451† ± 0.014

0.819‡ ± 0.016
0.723† ± 0.019

686
689

0.609‡ ± 0.020
0.474† ± 0.019

0.799 ± 0.022
0.728 ± 0.025

SA-GEO-nea
SA-GEO-far
SA-GEO-n/a

57
75
2256

0.469 ± 0.067
0.593 ± 0.063
0.543 ± 0.012

0.711 ± 0.087
0.787 ± 0.070
0.772 ± 0.013

13
17
1345

0.673‡ ± 0.098
0.588 ± 0.081
0.539 ± 0.015

0.923‡ ± 0.098
0.853 ± 0.108
0.761 ± 0.017

SA-INT-dis
SA-INT-lik
SA-INT-shr

740
853
795

0.570 ± 0.019
0.620‡ ± 0.018
0.436† ± 0.019

0.818‡ ± 0.021
0.861‡ ± 0.017
0.631† ± 0.024

454
498
423

0.526 ± 0.024
0.628‡ ± 0.024
0.455† ± 0.025

0.771 ± 0.029
0.827‡ ± 0.024
0.680† ± 0.032

CA-per
CA-exc
CA-goo
CA-fai
CA-bad
CA-det

63
390
644
577
710
4

0.603 ± 0.075
0.561 ± 0.025
0.572 ± 0.021
0.535 ± 0.023
0.509† ± 0.021

0.786 ± 0.075
0.809 ± 0.028
0.816‡ ± 0.023
0.758 ± 0.025
0.719† ± 0.025
−

86
215
337
235
496
6

0.628‡ ± 0.059
0.627‡ ± 0.032
0.582 ± 0.029
0.547 ± 0.032
0.458† ± 0.024
−

0.826 ± 0.055
0.877‡ ± 0.031
0.812 ± 0.033
0.783 ± 0.037
0.660† ± 0.030
−

0.544
0.549
0.526
0.570
0.524
0.517

−

0.021
0.040
0.037
0.020
0.045
0.027

0.556
0.611
0.549
0.532
0.608
0.531

±
±
±
±
±
±

0.035
0.073
0.070
0.019
0.073
0.034

0.784
0.847
0.762
0.755
0.843
0.745

±
±
±
±
±
±

0.039
0.085
0.077
0.022
0.069
0.041

Table 3: Utility of social annotation on SERP vs. social relevance aspects from the query, network connection,
and content, with 95% conﬁdence intervals. Bold indicates statistical signiﬁcance over all test tuples (ALL)
with † indicating lower utility and ‡ indicating higher utility. (-) indicates too few judgments.
signiﬁcance within the class, the descending utility trend follows the graded relevance judgments, with higher utility on
both HEAD and TAIL for perfect and excellent content versus
lower utility for fair and bad content.
The social aspects SA generally have signiﬁcant diﬀerentiating inﬂuence. The social aﬃnity (SA-AFF) shows the most
inﬂuence on utility followed by expertise (SA-EXP) and connection circle (SA-CIR), where colleagues and friends have
equal utility but family members have much higher expected
utility. For interest valence (SA-INT), knowing that a connection has liked (lik) a link shows more utility than average, but a share (shr) shows a negative utility inﬂuence.
Interestingly, disliking a link (dis) has little eﬀect on utility, which is further conﬁrmed in our analysis in Section 4.5.
Since only 7% of the queries in HEAD and 4% in TAIL were
local queries, our sampling resulted in too few geo-distance
(SA-GEO) instances that were near or far. The result is large
conﬁdence bounds and the only statistically signiﬁcant utility diﬀerence is shown in the TAIL where knowing near-ness
is more valuable than not. Further investigation on this aspect is warranted because we expect geographical distance
to have inﬂuence on social annotations of local queries.
We performed feedback analysis by inspecting a random
sample of the comment boxes ﬁlled by our judges. For the
no-util judgment, the feedback was mainly split between
poor content matches or vague queries, and connections of
very poor perceived utility. For the latter, expertise was
the main discussed social cue followed by affinity and interest valence. Example feedback includes: “Because he
is a distant friend, neutral and a non-expert, his opinion
is not going to be useful to me.” and “Even though the
connection is a family member, being a non-expert and neutral on the page would make me think that they do not
know a lot on the result.” For the some-util judgment,

feedback mainly revolved around the circle and dislike
aspects as the driving cues for utility. Example comments
include: “Chris’ dislike might get me to click another link,
even though it’s what I’m looking for, it could be a bad
quality link.”; “Even though Bob is neutral, since he’s an
expert there is added value in his annotation.”; and “Anytime one of my friends ‘dislikes’ a website, it might make me
dig further to see why.” Finally, for the sig-util judgment,
expertise and affinity drove the utility cues. Example
feedback includes: “There is a lot of added value to this annotation because Bob is my close friend and an expert.” and
“She is only a colleague, but she is an expert on the result
and her opinion matters to me because she knows what she
is talking about.”
Finally, we further analyzed the utility of all pairwise combinations of aspects. We list the top-20 and bottom-20 in
terms of RRel utility in Table 4, computed over THEAD∩TAIL .
The main conclusion from this analysis is that social annotations have very large variations in utility depending on the
aspects deﬁned in Section 4, with relevance utility ranging
from 0.336-0.754 in pairwise combinations. These should be
leveraged in deciding when to impress a social annotation to
a user.

4.5

Aspect Interplay Analysis

We turn now to measuring the interplay between social
relevance aspects. In a production system, some aspects are
more expensive than others to obtain. For example, most
search engines will already have query classiﬁers in place
whereas it may be harder to obtain some of the social aspect
values such as a connection’s aﬃnity or expertise. In this
section, we are interested in answering the question: what
is the value of a set of aspects if we know another set of
aspects?

290

Top-20 Pairs by RRel
SA-AFF-cls
SA-CIR-fam
SA-AFF-cls
SA-AFF-cls
SA-CIR-fam
SA-CIR-fam
SA-EXP-exp
SA-INT-lik
SA-CIR-fam
SA-CIR-fam

∩
∩
∩
∩
∩
∩
∩
∩
∩
∩

CA-per
CA-per
SA-INT-lik
SA-EXP-exp
SA-INT-lik
SA-EXP-exp
SA-INT-lik
CA-per
SA-GEO-far
CA-exc

SA-CIR-frn
SA-EXP-nex
SA-INT-shr
SA-CIR-frn
SA-CIR-wkc
SA-AFF-dst
SA-CIR-wkc
SA-INT-shr
SA-EXP-nex
SA-AFF-dst

∩
∩
∩
∩
∩
∩
∩
∩
∩
∩

CA-bad
CA-bad
QA-CLS-mov
SA-EXP-nex
SA-EXP-nex
QA-CLS-mov
SA-INT-shr
CA-bad
SA-GEO-nea
CA-bad

0.754
0.736
0.734
0.731
0.727
0.726
0.713
0.696
0.689
0.686

SA-AFF-cls
SA-CIR-fam
SA-AFF-cls
SA-AFF-cls
SA-EXP-exp
SA-AFF-cls
SA-INT-lik
SA-GEO-far
SA-CIR-fam
SA-EXP-exp

∩
∩
∩
∩
∩
∩
∩
∩
∩
∩

CA-exc
CA-goo
CA-goo
SA-GEO-far
CA-per
QA-CLS-res
CA-exc
SA-INT-lik
QA-CLS-res
CA-goo

0.683
0.680
0.679
0.679
0.674
0.671
0.669
0.669
0.668
0.667

SA-GEO-nea
CA-bad
CA-fai
SA-INT-shr
SA-GEO-nea
SA-INT-shr
SA-EXP-nex
SA-INT-shr
SA-INT-shr
SA-AFF-dst

0.396
0.396
0.395
0.394
0.388
0.385
0.363
0.355
0.355
0.336

over both HEAD and TAIL of knowing the row aspect given the
column aspect, i.e. RGHEAD ∪TAIL (row  column). For example, given the query class movie, knowing that the connection is in the family circle increases the relevance utility by
a factor of 0.27, whereas if it is in the work colleague circle
reduces the relevance utility by a factor of 0.132. Dashed entries represent either <0.001 utility gain/loss or impossible
combinations (e.g., since family members are always considered to have close aﬃnity in our setting then Tfamily∩distant
is an empty set).
Generally, knowing all social aspects with the exception of
friend and dislike yields signiﬁcant utility gain (or loss)
over the query classes and content relevance aspects. The
top-3 social aspects resulting in the most overall gains are
the family circle and the affinity aspects. We computed
RG between all combinations of affinity and circle aspects (table omitted for lack of space). We found that if we
knew the circle then further knowing affinity leads to a
utility diﬀerence of a factor of 0.319. In contrast, if we knew
ﬁrst the affinity, then further knowing the circle leads
to a utility diﬀerence of a factor of only 0.243. Clearly circle and affinity are not independent. Hence, if one has to
choose, investing in obtaining the affinity of a connection
is more valuable than obtaining circle. If the aﬃnity is
known to be close then there is no value in also knowing
that the circle is a friend (i.e., a social annotation from a
close friend or family member has equal utility in our data).
If the aﬃnity is distant, however, then there is signiﬁcant
value in determining if the circle is work colleague (0.116
gain) or friend (0.243 loss).
Expertise, as expected, aﬀects utility relatively more than
other social aspects for health queries. With respect to
music queries, expertise along with affinity, interest
and the family circle equally most inﬂuence utility. Content
that was shared has more inﬂuence on utility than other
social aspects for music queries.

Bottom-20 Pairs by RRel
0.422
0.417
0.412
0.410
0.410
0.409
0.405
0.397
0.396
0.396

SA-EXP-nex
SA-AFF-dst
SA-GEO-nea
SA-CIR-frn
SA-CIR-frn
SA-GEO-nea
SA-AFF-dst
SA-EXP-nex
SA-AFF-dst
SA-CIR-frn

∩
∩
∩
∩
∩
∩
∩
∩
∩
∩

Table 4: Top-20 and Bottom-20 social relevance aspect combinations in terms of RRel utility.

4.5.1

Relative Gain

More formally, given a corpus of tuples T and a set of
aspect values A1 , we are interested in the expected relative
gain or loss in social annotation utility, RGT (A2  A1 ), if
we learn another set of aspect values A2 :
RGT (A2  A1 ) =

R(TA1 ∩ TA2 ) − R(TA1 )
R(TA1 )

(2)

Consider for example A1 = {health} (query class) and A2 =
{family} (circle). Using Eq. 1, we can compute the expected
utility of a social annotation in T{health} as R(T{health} ). We
can similarly compute the expected utility if we also knew
the family aspect, R(T{health} ∩ T{family} ). Eq. 2 measures
the ratio between these two utilities, which captures the expected gain or loss of knowing family given that we knew
health. RGT is non symmetric, that is RG(TA1  TA2 ) =
RG(TA2  TA1 ). Positive gain is indicated by the sign of
RGT , and no gain is observed if RGT = 0.

Relationship to information gain.

5.

The interplay between social relevance aspects can also
be expressed in terms of information gain. Although we
report only values of RGT in this paper and prefer its interpretability (it can be directly interpreted as the expected
ratio increase in utility), for completeness we derive the information gain criteria. First, let PT (v) be the probability
that a tuple t ∈ T is judged as v ∈ J:


φj (t, v)
t∈T
 j∈J(t)

(3)
PT (v) =
t∈T
j∈J(t) 1
where φj (t, v) indicates if tuple t is judged as v by annotator
j. Then the information gain of A2 given A1 is deﬁned as:
IGT (A1 , A2 ) = H(TA1 ) − H(TA1 ∩ TA2 )

PT (v) log2 PT (v)
H(T ) = −

(4)
(5)

v∈J

4.5.2

PREDICTING SOCIAL RELEVANCE

In the previous sections we have identiﬁed and examined
the inﬂuence of social relevance aspects on the utility of social annotations, and our user study conﬁrmed that this inﬂuence is of a diﬀerentiated and complex nature. In this section we aim to build on these results by asking the question:
can we predict automatically whether a social annotation
adds utility to a search result? To address this question, we
develop discriminative models by learning from signals obtained in two diﬀerent ways: (1) oﬄine features, obtained
from the social relevance aspects used in our user study in
Section 4; and (2) online features, obtained from signals
available at runtime in a commercial search engine, to examine how well we can perform in our prediction task in a
real-world scenario without access to features such as a connection’s circle and affinity or gold judgments on query
class and content relevance.

Results

5.1

We focus our analysis on the relative gain of social aspects
with respect to each other, the query class aspects (QA-CLS)
and content aspects (CA). We omit the geo-distance aspect
for lack of space and since very few gains were signiﬁcant.
Table 5 and Table 6 list the relative utilty gains of SA vs.
QA-CLS and CA, respectively. Bolded entries indicate statistically signiﬁcant gains. The value of any cell can be interpreted as the utility gain or loss (in terms of RRel and RP rec )

Ofﬂine Features

Our 16 oﬄine features are derived from the social relevance aspects presented in Section 3. The query class aspects (QA-CLS) are mapped to ﬁve binary features, namely
commerce, health, movie, music, and restaurant. The social aspects each become a categorical feature as follows:
circle = {wkc, fam, frn}; affinity = {cls, dst}; expertise = {exp, nex }; geo-distance = {nea, far, n/a}; and

291

QA-CLS-com
RGRel
RGPrec

QA-CLS-hea
RGRel
RGPrec

QA-CLS-mov
RGRel
RGPrec

QA-CLS-mus
RGRel
RGPrec

QA-CLS-res
RGRel
RGPrec

SA-CIR-wkc
SA-CIR-fam
SA-CIR-frn

−0.124
0.215
−0.078

−0.011
0.093
−0.074

−0.029
0.14
−0.081

0.043
0.056
−0.091

−0.132
0.27
−0.103

−0.019
0.148
−0.12

−0.059
0.144
−0.086

0.037
0.059
−0.096

−0.092
0.247
−0.139

−0.011
0.15
−0.132

SA-AFF-cls
SA-AFF-dst

0.211
−0.213

0.089
−0.09

0.16
−0.125

0.063
−0.049

0.237
−0.225

0.118
−0.112

0.145
−0.154

0.057
−0.061

0.251
−0.208

0.134
−0.111

SA-EXP-exp
SA-EXP-nex

0.159
−0.161

0.052
−0.053

0.169
−0.167

0.062
−0.062

0.141
−0.126

0.056
−0.05

0.141
−0.137

0.05
−0.048

0.191
−0.161

0.075
−0.063

SA-INT-dis
SA-INT-lik
SA-INT-shr

0.028
0.149
−0.185

0.057
0.112
−0.17

−0.022
0.146
−0.134

0.02
0.129
−0.155

0.044
0.189
−0.193

0.063
0.143
−0.174

0.012
0.14
−0.188

0.029
0.084
−0.138

0.034
0.133
−0.184

0.062
0.119
−0.199

Table 5: Relative utility gain (RRel and RP rec ) of social aspects (rows) vs. query aspects (columns) over HEAD
∪ TAIL. Bold indicates that the relative utility gain is statistically signiﬁcant with 95% conﬁdence.
CA-per
RGRel
RGPrec

CA-exc
RGRel
RGPrec

CA-goo
RGRel
RGPrec

CA-fai
RGRel
RGPrec

CA-bad
RGRel
RGPrec

SA-CIR-wkc
SA-CIR-fam
SA-CIR-frn

−0.052
0.248
−0.107

0.042
0.153
−0.123

−0.039
0.126
−0.072

0.041
0.056
−0.09

−0.093
0.172
−0.082

−0.014
0.08
−0.064

−0.102
0.186
−0.075

0.014
0.081
−0.091

−0.09
0.235
−0.151

0.039
0.09
−0.15

SA-AFF-cls
SA-AFF-dst

0.247
−0.193

0.143
−0.112

0.153
−0.14

0.068
−0.063

0.175
−0.192

0.09
−0.098

0.183
−0.189

0.066
−0.068

0.209
−0.188

0.074
−0.067

SA-EXP-exp
SA-EXP-nex

0.129
−0.146

0.044
−0.049

0.154
−0.149

0.036
−0.034

0.159
−0.155

0.063
−0.062

0.151
−0.174

0.061
−0.071

0.145
−0.135

0.057
−0.053

SA-INT-dis
SA-INT-lik
SA-INT-shr

−0.066
0.166
−0.101

−0.004
0.074
−0.072

−
0.153
−0.16

0.046
0.08
−0.122

0.018
0.169
−0.215

0.047
0.108
−0.176

0.026
0.159
−0.215

0.039
0.115
−0.176

0.044
0.11
−0.158

0.043
0.108
−0.155

Table 6: Relative utility gain (RRel and RP rec ) of social aspects (rows) vs. content aspects (columns) over
HEAD ∪ TAIL. Bold indicates that the relative utility gain is statistically signiﬁcant with 95% conﬁdence.

5.3

interest-valence = {lik, shr, dis}. Finally, the content
relevance aspects (CA) are mapped to six binary features,
namely perfect, excellent, good, fair, bad, detrimental.

5.2

Model

For all our prediction experiments we use the Multiple
Additive Regression Trees (MART) [27] algorithm, which is
based on the Stochastic Gradient Boosting paradigm [11].
We used log-likelihood as the loss function, steepest-descent
as the optimization technique and binary decision trees as
the ﬁtting function. MART oﬀers a range of crucial advantages: it has been proven to yield high accuracy, it does not
require any feature normalization and can handle any mix
of real-valued, multi-valued and binary features, and ﬁnally,
through its use of decision trees it can handle non-linear dependencies between the features. The latter advantage is
of particular importance in our case since we have already
found in Section 4 that combinations of social relevance aspects are predictive for social relevance. We cast our task as
a supervised learning problem for predicting the utility of a
social annotation t = {q, u, c, v} (see Section 3). For reasons
of simplicity, we cast the prediction task as a binary task
(i.e., a social annotation is either relevant or not).

Online Features

We turn now to features that are available to a search engine at runtime, which we call online features. Although
no human annotator can provide relevance or query classiﬁcation judgments at runtime, most search engines today
have proxies from automatic query classiﬁers [21] and content rankers [19]. Social aspects are also generally unavailable at runtime. However, there are a multitude of other
measurements computed by search engines, and usage logs
are routinely collected. The total number of features we
collect in our online models runs at over 150. Below we
describe the major classes of features and list examples.
• Query Classes: Our search engine evaluates each
query by a set of automatic query classiﬁers, which
assign a score to each query/class pair. We selected 55
of these classes as real-valued features, including iscommerce, is-local, is-health, and is-movie. While
these automatic classiﬁers are not perfectly accurate,
they serve as a proxy for the coarser (but more reliable)
query-class annotation in our user study.
• Session Metrics: For user sessions containing the
query, we measure features such as average session
duration and average page view count.
• User Metrics: For users that have issued the query,
we measure features such as average click count and
average page view count.
• Query Metrics: We measure query-level aggregate
features such as average dwell time, average time
to first click, and issues per day.
• Result Metrics: We measure query-url aggregate
metrics such as average dwell time and abandonment.

5.4

Experimental Setup

We use the data produced by our user study in Section 4.
For each judged social annotation tuple t in the data, we
extract a feature vector according to Sections 5.1 and 5.2.
The online aggregate features are extracted from US English
Web search usage logs from the same 3-month period as the
samples drawn for our user study (see Section 4.1). The online features are directly obtained from the annotated tuples
from our user study. We retain a total of 2380 HEAD tuples
and 1371 TAIL tuples5 .
For each tuple, we mapped the two annotator judgments
from the space J to a binary value indicating whether the social annotation in the tuple was relevant (1) or not (0). To
this end, we use a conservative minimum-based approach,
where we label any tuple as relevant if the minimum anno5
12 of the 3763 tuples were discarded where online feature
extraction failed.

292

Relevance Prediction Performance on TAIL
1

0.95

0.95

0.9

0.9

0.85

0.85

precision

precision

Relevance Prediction Performance on HEAD
1

0.8
0.75
0.7

0.7

0.65

0.65

0.6
0

0.2
offline

0.4

recall
online

0.6

0.8

0.6

1

0

0.2

0.4

offline

offline + SA

Figure 3: Prediction performance on HEAD using feature sets: oﬄine, online, online + social aspects.

recall
online

0.6

0.8

1

offline + CA

Figure 4: Prediction performance on TAIL using feature sets: oﬄine, online, online + social aspects.

tated utility is some-util, i.e. where none of the judges has
labeled the case as no-util. In HEAD this results in 909 class
0 (non-relevant) examples and 1471 class 1 (relevant) examples. In TAIL we have 552 non-relevant and 819 relevant
examples.
For the MART learner, we train for 100 iterations (resulting in 100 trees) and restrict the decision tree stumps to 10
leaf nodes that cover a minimum of 25 samples. All reported
results are based on 10-fold cross-validation.

5.5

0.8
0.75

Feature Ablation on HEAD for "online" Setting
0.85

precision

0.8
0.75
0.7

0.65

Results

0.6
0

We present the results of our prediction experiments separately for HEAD and TAIL tuples since we observe systematically diﬀerent behavior between the two in our prediction
task. Figures 3 and 4 illustrate the precision-recall characteristics of the relevance prediction with diﬀerent feature
sets. Overall, the task is learnable and as could be expected,
the prediction model that uses oﬄine features outperforms
all other models on both HEAD and TAIL.
It is common in practice to impress a social annotation
anytime one is available for a query-url context. Looking
at the split between positive and negative examples in the
test data, one would achieve 61.8% relevance precision on
HEAD and 59.7% relevance precision on TAIL by impressing
every available social annotation to a user. Using our oﬄine
model on HEAD, one could increase the rate of relevance by a
factor of 13% while maintaining 88% recall, or by a factor of
25% at 50% recall. On TAIL, the same model would increase
the rate of relevance by a factor of 7% at 87% recall, or by
a factor of 29% at 50% recall.
We analyzed the importance of the oﬄine features in our
model by observing the weights assigned by MART in its
training log ﬁles. For HEAD, the social aspects were consistently ranked highest: circle, aﬃnity and expertise are the
three most important features, followed by content relevance
judgments. For TAIL, the picture is diﬀerent. Here, the
content relevance features are dominant. The top-ranked
feature is whether the content relevance is marked as bad,
which is not surprising since instances of non-relevant urls
are higher in TAIL and social annotations on irrelevant documents have less utility. This relevance feature is followed in
the importance ranking by a number of social aspects (aﬃnity, circle and expertise), followed by whether the content
relevance is excellent, followed by query class features.
We also observe that there is enough signal in the online
features for predictions up into the 70-75% precision range,
albeit at much lower recall rate than for oﬄine features.

0.2
online

0.4
recall
online - Query Classes

0.6

0.8

1

online - Result Metrics

Figure 5: Ablation of two most predictive online
feature familes on HEAD: Query Classes and Results
Metrics.
Prediction ability is highest on TAIL nearly achieving the
performance of oﬄine features. Somewhat surprisingly, in
both HEAD and TAIL, we observed that the Query Class
features were by far the most predictive, followed by the
Results Metrics features - see ablation results in Figure 5
for HEAD (TAIL omitted for lack of space). In fact for HEAD
queries, only using query class features produces results close
to those using all online features. In TAIL, the picture is
more diﬀerentiated, where adding other online features to
the query classiﬁer features improves results.
Based on this analysis of feature importance and the ﬁnding that online features are predictive but not as predictive
as oﬄine features, we also experimented with adding the
most predictive oﬄine features (aspect family) to the set
of online features. For HEAD queries we added the social
aspect features (circle, affinity, expertise, etc.), and
for TAIL queries we added the content relevance features
(perfect, excellent, fair, etc.) Results are also shown
in Figures 3 and 4. We observe that we can increase the
performance of online features by adding social aspects and
content relevance features. We take this as an encouraging
result since proxies of both these feature families may be
made available at runtime. Content relevance features such
as PageRank scores can be assessed, which - while not as
accurate as human judgments - may provide additional signal. Similarly, social relevance features could be amenable
to statistical modeling from observable data such as social
network characteristics or proﬁle modeling at runtime, an
area that we plan to investigate in future research.

293

6.

CONCLUSION AND FUTURE WORK

We presented a taxonomy of aspects that inﬂuence the
perceived utility of social annotations in a Web search scenario, drawn from the query, social connection, and content
relevance. Via a user study, we took a ﬁrst step at quantifying the utility of social annotations and gained insights on
the complex interplay between the social relevance aspects.
We concluded that there are large variations in utility depending on the aspects in play and that these should be
leveraged when deciding to impress a social annotation to a
user. We learned that social aspects are most inﬂuential in
perceived utility, in particular aﬃnity, expertise and interest valence. We further established that close social connections and experts in the search topic provide the most utility,
whereas distant friends and friends that show no positive or
negative interest valence provide the least utility, by a factor
of over 50%.
We also showed that we can automatically predict whether
a social annotation is relevant for a given query/url pair.
We cast the task as a binary supervised learning problem
over a stochastic gradient boosting model. In an oﬄine experiment, we drew features from the manually labeled user
study and established that we can accurately predict social utility. In a conﬁguration simulating an online scenario,
we drew session-, query-, document-, and user-level features
from query classiﬁers and web usage logs, which can be computed at runtime by commercial web search engines. In this
online setting, we established the prediction task as learnable and approaching the performance of the oﬄine model.
Finally, by adding the social aspects and content relevance
aspects from the oﬄine features to the online features, we
gained predictive performance over just the online features.
A promising avenue of future work is to develop social
aspects classiﬁers in order to increase our ability to predict
the utility of a social annotation. Other directions include
investigating the inﬂuence of other social aspects such as
age, gender, user location, and school; and applying our
framework to other social features such as interleaved results. Perhaps most valuable, however, is to broaden our
concept of utility, which we have limited to a search result,
to the whole-page user experience, as outlined in Section 1,
and further our understanding of how social features aﬀect
the overall information seeking, discovery, and sensemaking
processes.

7.

[7]

[8]

[9]

[10]
[11]

[12]
[13]

[14]
[15]
[16]

[17]

[18]
[19]

[20]
[21]

[22]

REFERENCES
[23]

[1] E. Agichtein, E. Brill, and S. Dumais. Improving web
search ranking by incorporating user behavior
information. In SIGIR, 2006.
[2] S. Bao, G. Xue, and X. e. a. Wu. Optimizing web
search using social annotations. In WWW, 2007.
[3] C. L. Barry and L. Schamber. Users’ criteria for
relevance evaluation: A cross-situational comparison.
Inf. Process. Manage., 34(2-3):219–236, 1998.
[4] P. Borlund. The concept of relevance in ir. JASIST,
54(10):913–925, 2003.
[5] C. J. C. Burges, K. M. Svore, P. N. Bennett,
A. Pastusiak, and Q. Wu. Learning to rank using an
ensemble of lambda-gradient models. Journal of
Machine Learning Research, 14:25–35, 2011.
[6] D. Carmel, N. Zwerdling, I. Guy, S. Ofek-koifman,
N. Har’el, I. Ronen, E. Uziel, S. Yogev, and

[24]

[25]

[26]

[27]

294

S. Chernov. Personalized social search based on the
user’s social network. In CIKM, pages 1227–1236,
2009.
O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan.
Expected reciprocal rank for graded relevance. In
CIKM, pages 621–630, 2009.
C. Clarke, E. Agichtein, S. Dumais, and R. White.
The inﬂuence of caption features on clickthrough
patterns in web search. In SIGIR, 2007.
B. M. Evans and E. H. Chi. An elaborated model of
social search. Inf. Process. Manage., 46(6):656–678,
2010.
R. Fidel and M. Crandall. Users’ perception of the
performance of a ﬁltering system. In SIGIR, 1997.
J. Friedman. Greedy function approximation: a
gradient boosting machine. Annals of Statistics, 29,
2001.
K. Haas, P. Mika, P. Tarjan, and R. Blanco. Enhanced
results for web search. In SIGIR, 2011.
P. Heymann, G. Koutrika, and H. Garcia-Molina. Can
social bookmarking improve web search. In WSDM,
2008.
R. A. Hill and R. I. M. Dunbar. Social network size in
humans. Human Nature, 14(1):53–72, 2003.
S. Huﬀman and M. Hochster. How well does result
relevance predict session satisfaction? In SIGIR, 2007.
K. Järvelin and J. Kekäläinen. Cumulated gain-based
evaluation of ir techniques. ACM Trans. Inf. Syst.,
20:422–446, October 2002.
T.-Y. Liu. Learning to rank for information retrieval.
Foundations and Trends in Information Retrieval,
3:225–331, March 2009.
A. Muralidharan, Z. Gyongyi, and E. H. Chi. Social
annotations in web search. In SIGCHI, 2012.
L. Page, S. Brin, R. Motwani, and T. Winograd. The
pagerank citation ranking: Bringing order to the web.
In WWW, 1998.
J. Pitkow and H. e. a. SchÂĺutze. Personalized search.
In ACM, volume 45(9), 2002.
D. Shen, J. Sun, Q. Yang, and Z. Chen. Building
bridges for web query classiﬁcation. In Proceedings of
SIGIR, 2006.
J. Teevan, S. Dumais, and D. Liebling. To personalize
or not to personalize: Modeling queries with variation
in user intent. In SIGIR, 2008.
J. Teevan, S. T. Dumais, and E. Horvitz. Personalizing
search via automated analysis of interests and
activities. In SIGIR, pages 449–456, 2005.
J. Ugander, B. Karrer, L. Backstrom, and C. Marlow.
The anatomy of the facebook social graph. CoRR,
abs/1111.4503, 2011.
K. Wang, T. Walker, and Z. Zheng. Pskip: estimating
relevance ranking quality from web search
clickthrough data. In SIGKDD, 2009.
S. Wedig and O. Madnani. A large-scale analysis of
query logs for assessing personalization opportunities.
In SIGKDD, 2006.
Q. Wu, C. J. Burges, K. M. Svore, and J. Gao.
Ranking, boosting and model adaptation. Microsoft
Research Technical Report, 2008.

