Finding Readings for Scientists from Social Websites
Jiepu Jiang, Zhen Yue, Shuguang Han, Daqing He
School of Information Sciences, University of Pittsburgh

jiepu.jiang@gmail.com, zhy18@pitt.edu, shh69@pitt.edu, dah44@pitt.edu
ABSTRACT

each user can maintain a collection of articles as the userâ€™s personal library. Here we assume CiteULike users are scientists and
their personal libraries are their collections of useful readings.
Previous studies found co-occurrences of articles in usersâ€™ libraries can be used for clustering articles into research fields [2]. Our
dataset includes titles and abstracts for 913,846 unique articles
posted to CiteULike by users in 2010 (99.2% of all articles posted
by users in 2010) and 54,402 usersâ€™ personal libraries.

Current search systems are designed to find relevant articles, especially topically relevant ones, but the notion of relevance largely depends on search tasks. We study the specific task that scientists are searching for worth-reading articles beneficial for their
research. Our study finds: usersâ€™ perception of relevance and preference of reading are only moderately correlated; current systems
can effectively find readings that are highly relevant to the topic,
but 36% of the worth-reading articles are only marginally relevant
or even non-relevant. Our system can effectively find those worthreading but marginally relevant or non-relevant articles by taking
advantages of scientistsâ€™ recommendations in social websites.

2.2 Algorithms

Let R be an article (reading). Given a query q, we rank readings by P(R|q), which is equivalent to P(R, q) in ranking. Further,
we model P(R, q) as: what is the probability that a scientist working on the problems described by q will read a reading R? P(R, q)
is calculated by two steps. First, we find a list of peer scientists by
P(q|u) and P(u): P(u) is considered equal for all users, i.e. 1/|{u}|;
P(q|u) is calculated as (2) using expert finding â€œmodel 2â€ in [3]. In
(2), Lu is the list of articles in uâ€™s personal library, which is used to
model uâ€™s expertise. Second, we let each peer scientist u vote for
reading R by the probability P(R|u, q). We assume a peer scientist
u will vote for R as a worth-reading article for q if R is in both Lu
and the search results of query q (referred to as Sq), as in (3). The
whole method is referred to as RUL, which is summarized in (1).
We can further set a cutoff value n in (3) so that only the top n
retrieved results for query q are considered for voting (referred to
as RULn). We use two ad hoc search models for comparison: sequential dependence model (SDM) and relevance model 3 (RM3
[4]). We also merge rankings of RULn and SDM/RM3 by (4).

Categories and Subject Descriptors

H.2.8 [Database Applications]: scientific databases.

General Terms

Algorithms, Measurement, Performance, Experimentation.

Keywords

Scientific articles; scientific readings; social search.

1. INTRODUCTION

Scientists in nowadays not only read articles closely related
to their main research fields but also those from other fields. This
makes automatically finding useful readings more difficult than
simply matching articles with queries because: 1) many useful
readings could come from topics that the scientists do not have
adequate knowledge to formulate effective queries; 2) not all topically relevant articles are worth reading; and 3) some useful readings may not contain query terms.
We believe that a useful search system for solving this problem should have the following key features: 1) the scientists are
not required to formulate queries related to the topics of the readings, but rather queries about the problems or research topics they
are working on, which should be a relatively easier task; 2) the
system can find not only readings that are topically relevant, but
also those beyond the topics that the scientists are working on.
Here we propose a method of finding readings by looking at what
scientistsâ€™ peers are reading about in social websites, which generates a list of candidate readings not limited to topically relevant
articles. Specifically, we study the following research questions:
(1) What are the relations between usersâ€™ perception of relevance and preference of reading?
(2) Can peer scientistsâ€™ libraries help find worth-reading articles beyond the queriesâ€™ topics?

RUL:

ğ‘ƒ(ğ‘…, ğ‘) = ï¿½ ğ‘ƒ(ğ‘…, ğ‘|ğ‘¢)ğ‘ƒ(ğ‘¢) =
ğ‘¢

Finding peer scientists:
Voting for articles:
Merge rankings:

1
ï¿½ ğ‘ƒ(ğ‘|ğ‘¢)ğ‘ƒ(ğ‘…|ğ‘¢, ğ‘)
|{ğ‘¢}|
ğ‘¢

ğ‘ƒ(ğ‘|ğ‘¢) = ï¿½ ğ‘ƒ(ğ‘|ğ‘‘, ğ‘¢) âˆ™ ğ‘ƒ(ğ‘‘|ğ‘¢)
ğ‘‘âˆˆğ¿ğ‘¢

(1)
(2)

1 if ğ‘… âˆˆ ğ¿ğ‘¢ ğ‘ğ‘›ğ‘‘ ğ‘… âˆˆ ğ‘†ğ‘
(3)
0
1
1
(4)
ğ‘†ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘…) =
+
ğ‘Ÿğ‘ğ‘›ğ‘˜(RULğ‘› ) ğ‘Ÿğ‘ğ‘›ğ‘˜(SDM/RM3)
ğ‘ƒ(ğ‘…|ğ‘¢, ğ‘) = ï¿½

2.3 User Judgments

We recruited 10 subjects in academia for judgments (1 faculty member, 1 postdoc and 8 PhD students). Every participant was
asked to generate 3 queries, each of which described a research
problem related to his/her research. Seven runs were pooled
(depth=10) for each query: SDM, RM3, RUL, and RULn with n =
20, 50, 100, and 200. On average 31 articles were pooled for each
query. The participants need to answer the following questions:
Q1. Do you think this article is relevant to your query? (1not relevant, 2-somewhat relevant, 3-relevant)
Q2. Do you want to read this article? (1-I already read it, 2yes, I want to read it, 3-no, I don't want to read it)
Q3. (If Q2=1) I already read it (1-I like it, 2-netural, 3-I
don't like it)
For each query, after judging all pooled articles, we asked the
participant to select at most 3 articles he/she wanted to read first if
he/she is only given limited time. We assign relevance scores (rel)
to articles based on answers to Q1 (rel = 2 for â€œrelevantâ€; rel = 1
for â€œsomewhat relevantâ€; and rel = 0 for â€œnot relevantâ€). For read-

2. EXPERIMENT DESIGN
2.1 Dataset

We built a dataset [1] based on articles and usersâ€™ libraries in
CiteULike, a social reference management website. In CiteULike,
Copyright is held by the author/owner(s).
SIGIRâ€™12, August 12â€“16, 2012, Portland, Oregon, USA.
ACM 978-1-4503-1472-5/12/08.

1075

performed not much better than SDM. The reasons can be: 1) if
the original query is not effective, it is unclear whether or not
pseudo-relevance feedback can produce high quality expanded
terms; and 2) expanded terms may enrich representations of the
queryâ€™s topic, but do not necessarily help matching of crossdisciplinary articles.
Figure 2 shows per topic differences of nDCG@10 between
SDM, RM3, RUL20, and RUL20+SDM/RM3. It shows that RM3
improves and hurts nearly the same number of topics compared
with SDM, but RUL20 (+SDM/RM3) can not only improve topics
hurt by RM3, but can also get as much as improvements in topics
improved by RM3. We find the reasons are related to the voting
mechanisms in RULn. On the one hand, voting can reduce wrong
expansions of topics. For example, for topic 28 â€œscience mapping
intellectual structureâ€, SDM performs effective but RM3 wrongly
emphasizes on â€œmappingâ€ and matches articles such as â€œGenetic
mapping in human diseaseâ€. However, since peers found by RULn
do not read and vote for the article, it is still lower ranked in results of RULn. On the other hand, voting is also a useful way of
finding good readings beyond topical level. Still, for topic 28, the
user wants to read the article titled â€œScholarly research and information practices: a domain analytic approachâ€, which is not
ranked in top position by SDM or RM3 for the lack of query
terms in title and abstract. Instead, RULn votes a high score for
this article because it has been read by many scientists. We also
find, however, RULn works poorly if the topic is associated with
peers with diversified interests, e.g. topic 7 â€œcollaborative information seekingâ€.
To conclude, our evaluation results demonstrate that current
search models can effectively find readings that are also topically
relevant, because in such cases article relevance and preference of
reading are highly correlated. In comparison, our system can more
effectively find readings beyond the queryâ€™s topic by taking advantages of peer scientistsâ€™ recommendations in social websites.

ing preference score (read), we consider two cases: if the participant has read the article (Q2=1), we assign read score based on
Q3 (read = 2 for â€œI like itâ€; read = 1 for â€œneutralâ€; and read = 0
for â€œI donâ€™t like itâ€); if the participant has not read the article, we
assign read = 2 to the three articles the participant selected to read
first, and read = 1 to other articles the participant would like to
read (Q2=2). Table 1 shows statistics of participantsâ€™ judgments.
For the 477 articles judged as worth-reading (read â‰¥ 1), 173 (36%)
are only marginally relevant or not relevant to the queriesâ€™ topics
(rel â‰¤ 1). After removing 370 articles that are neither relevant to
the topic nor worth-reading (rel = 0; read = 0), we find a moderate
correlation (r = 0.357) between articlesâ€™ rel and read scores. This
indicates the fact that good readings are not necessarily relevant to
the queriesâ€™ topics and vice versa.
Table 1. Article judgments statistics.
rel = 2
166
138
13
317

read = 2
read = 1
read = 0
sum

rel = 1
40
111
76
227

rel = 0
6
16
370
392

sum
212
265
459
936

3. RESULTS AND DISCUSSIONS

Our evaluation includes two stages:
(1) At the first stage, we use readings that are highly relevant
(rel = 2; read â‰¥ 1) for evaluation (HRelRead). As shown in Table
1, over 90% of the highly relevant articles (rel = 2) are also worthreading (read â‰¥ 1). Current search systems (SDM and RM3) may
perform well in finding HRelRead articles because the HRelRead
readings are mostly also highly relevant articles. We also evaluate
by article relevance (QRel) for comparison.
Left and center parts of Figure 1 show the results using QRel
and HRelRead for evaluation. Ad hoc search models (SDM and
RM3) performed very effectively (nDCG@10 â‰¥ 0.6 and 0.5). In
comparison, RULn performed worse than ad hoc search models,
but is still very effective (when n = 20, nDCG@10 â‰¥ 0.45). This is
not surprising considering read and rel scores are highly correlated in HRelRead readings. RULn does not explicitly model topical
relevance, but SDM and RM3, as two of the best ad hoc search
models, model topical relevance very well. Thus, in general, ad
hoc search models can effectively find the HRelRead readings,
but their success may come from better modeling of topical relevance rather than better modeling the task of finding readings.
(2) At the second stage, we use readings that are marginally
relevant or not relevant for evaluation (referred to as MRelRead;
rel <= 1, read >= 1). Since such readings are not necessarily relevant, ad hoc search models may not perform effectively. The right
part of Figure 1 shows evaluation results for MRelRead readings.
It indicates that ad hoc search algorithms (SDM and RM3) cannot
perform as effectively as they did in finding HRelRead readings
(nDCG@10 < 0.35). In comparison, when n = 20, RULn can perform significantly better than SDM (nDCG@10+13.36%; p<0.05).
Combining RULn with SDM/RM3 further improved performance.
As expected, ad hoc search models cannot effectively solve
the problems of finding MRelRead readings because in such cases
article relevance and userâ€™s preference of reading diverge a lot. It
seems unlikely to solve the problem by query expansion, as RM3
nDCG@10
0.75
0.70
0.65
0.60
0.55
0.50
10

SDM
RULn
RULn+RM3

RM3
RULn+SDM

n
20

50

100

200

nDCG@10
0.65
0.60
0.55
0.50
0.45
0.40
0.35
10

0.4
0.3
0.2
0.1
0.0
-0.1
-0.2
-0.3
-0.4

nDCG@10 RM3 - SDM
nDCG@10 RUL20 - SDM

nDCG@10 RUL20+RM3 - SDM
nDCG@10 RUL20+SDM - SDM

28 3 16 6 21 17 26 1 29 8 11 2 15 7 10 13 31 32 33 4 14 12 5 9 25 30 20 19 18
topic id

Figure 2. Per topic nDCG@10 differences of algorithms.

4. ACKNOWLEDGMENTS

This work was supported in parts by National Science Foundation grant IIS-1052773 and III-COR 0704628.

5. REFERENCES

[1] http://www.sis.pitt.edu/~jjiang/data/rul/
[2] J. Jiang, D. He, C. Ni. 2011. Social reference: aggregating
online usage of scientific literature in CiteULike for clustering academic resources. In JCDL '11: 401-402.
[3] K. Balog, L. Azzopardi, M. de Rijke. 2006. Formal models
for expert finding in enterprise corpora. In SIGIR '06: 43-50.
[4] Y. Lv, C. Zhai. 2009. A comparative study of methods for
estimating query language models with pseudo feedback. In
CIMK '09: 1895-1898.

SDM
RULn
RULn+RM3

RM3
RULn+SDM

SDM
RULn
RULn+RM3

nDCG@10
0.45
0.40

RM3
RULn+SDM

0.35
0.30
n
20

50

100

200

n

0.25
10

20

50

100

200

Figure 1. nDCG@10 of SDM, RM3, and RULn (+SDM/RM3) evaluated by QRel (left), HRelRead (center), and MRelRead (right).

1076

