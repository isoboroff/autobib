A Content based Approach for Discovering Missing
Anchor Text for Web Search
Xing Yi and James Allan
Center for Intelligent Information Retrieval
Computer Science Department
University of Massachusetts, Amherst, MA, USA

{yixing,allan}@cs.umass.edu
ABSTRACT

# of web pages
# of pages having inlinks
# of pages having original
or enriched inlinks[14]

Although anchor text provides very useful information for
web search, a large portion of web pages have few or no
incoming hyperlinks (anchors), which is known as the anchor text sparsity problem. In this paper, we propose a language modeling based technique for overcoming anchor text
sparsity by discovering a web pageâ€™s plausible missing anchor text from its similar web pagesâ€™ in-link anchor text.
We design experiments with two publicly available TREC
web corpora (GOV2 and ClueWeb09) to evaluate diï¬€erent
approaches for discovering missing anchor text. Experimental results show that our approach can eï¬€ectively discover
plausible missing anchor terms. We then use the web named
page ï¬nding task in the TREC Terabyte track to explore the
utility of missing anchor text information discovered by our
approach for helping retrieval. Experimental results show
that our approach can statistically signiï¬cantly improve retrieval performance, compared with several approaches that
only use anchor text aggregated over the web graph.

GOV2
25,205,179
376,121 (1.5%)
977,538 (3.9%)

ClueWeb09-T09B
50,220,423
7,640,585 (15.2%)
19,096,359 (38.0%)

Table 1: Summary of in-link statistics on two TREC
web corpora used in our study.
hereâ€ and â€œnextâ€), many times anchor text provides succinct description of the destination URLâ€™s content, e.g. â€œSIGIR 2010(Geneva, Switzerland)â€ is from an anchor linked
to http://www.sigir2010.org/. Anchor text instances are
usually reasonable queries that web users may issue to search
for the associated URL and have been used to simulate plausible web queries relevant to the associated web pages in
some web search research [15]. Therefore, anchor text is
highly useful for bridging the lexical gap between user issued web queries and the relevant web pages. It is arguably
the most important piece of evidence used in web ranking
functions[14].
However, previous research has shown that the distribution of the number of inlinks on the web follows a power law
[1], where a small portion of web pages have a large number
of inlinks while most have few or no inlinks. Thus, most web
pages do not have in-link associated anchor text, a problem
originally referred to as the anchor text sparsity problem by
Metzler et al. [14]. This problem presents a major obstacle for any web search algorithms that want to use anchor
text to improve retrieval eï¬€ectiveness. Table 1 shows the anchor text sparsity problem in two large TREC1 web corpora
(GOV22 and ClueWeb09-T09B3 ). To address this problem,
Metzler et al. [14] proposed aggregating, or propagating, anchor text across the web hyperlink graph so that web pagesâ€™
lack of anchor text can be enriched with their linked web
pagesâ€™ associated anchor text. Table 1 shows that the number of URLs associated with some anchor text (original or
propagated) in the two TREC web corpora is signiï¬cantly
increased by using their linked-based anchor text enrichment
approach. Nevertheless, in Table 1 we notice that large portion of web pages still do not have any associated anchor
text after having been enriched. This observation motivated
us to consider another possible approach, which utilizes the
content similarity between web pages, to alleviate anchor
text sparsity.

Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and
Retrievalâ€“Search process,Retrieval models;H.3.5 [Information Storage and Retrieval]:Online Information Servicesâ€“
Web-based services
General Terms: Algorithms, Experimentation
Keywords: anchor text, anchor text sparsity, language
models, relevance models, content similarity, web search

1. INTRODUCTION
There are rich dynamic human generated hyperlink structures on the web. Most web pages contain some hyperlinks,
referred to as anchors, that point to other pages. Each anchor consists of a destination URL and a short piece of text,
which is called anchor text. Anchors play an important role
in helping web users conveniently navigate to their interested web information. Although some anchor text only
functions as a navigational shortcut which does not have direct semantic relation to the destination URL (e.g.,â€œclick

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGIRâ€™10, July 19â€“23, 2010, Geneva, Switzerland.
Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

1

http://trec.nist.gov/
http://ir.dcs.gla.ac.uk/test_collections/
gov2-summary.htm
3
http://boston.lti.cs.cmu.edu/Data/clueweb09/
2

427

Speciï¬cally, we hypothesize that the anchor text associated with a web pageâ€™s inlinks typically has close semantic
relations to the web page so that web pages that are similar in content may be pointed to by anchors having similar
anchor text. Under this assumption, in this paper we propose a language modeling based technique for discovering a
web pageâ€™s plausible missing in-link anchor text by using its
most similar web pagesâ€™ in-link anchor text. We then test
the eï¬€ectiveness of our approach by using the discovered
missing anchor text information for some TREC web search
tasks. We ï¬nd that even on the GOV2 data where a serious anchor text sparsity problem exists as shown in Table
1, our approach can signiï¬cantly improve retrieval performance. Our content based approach can be combined with
the hyperlink based approach to further reduce anchor text
sparsity and beneï¬t web search. Our enriched document
and anchor text representations can also be used for many
other tasks beyond web search, including estimating better
document models and extracting advanced textual features
for content match and document classiï¬cation.
Our work has four chief contributions: 1) although content similarity has been used widely in other applications,
we are the ï¬rst to propose using web content similarity to
address the anchor text sparsity problem. 2) We develop a
language modeling based technique, which stems from ideas
in one eï¬€ective retrieval technique â€“ relevance based language models [10], to eï¬€ectively discover plausible missing
anchor text information and use it for retrieval. 3) We empirically show that our approach performs better than Metzler et al.â€™s linked-based approach [14] in terms of discovering
plausible missing anchor terms in two standard large TREC
web corpora. 4) We show that our approach statistically
signiï¬cantly improves retrieval eï¬€ectiveness, compared with
several approaches that only use aggregated anchor text over
the web graph, in the web named page ï¬nding task of the
TREC Terabyte track [4].
We begin by reviewing related work in Â§2. Next, we
describe diï¬€erent approaches of discovering missing anchor
text to enrich document representations in Â§3. Then we
describe the experimental setup and results of evaluating
diï¬€erent approaches for anchor text discovery in Â§4. After
that, we present how to use discovered anchor text information for retrieval in a language modeling approach and
report the experimental results in Â§5. We conclude in Â§6.

but focus on discovering a plausible associated anchor language model for web pages with no or few inlinks. Our approach can be easily used together with any language model
based retrieval model (e.g., Ogilvie and Callanâ€™s model [16])
that takes document structure into account.
Our approach of overcoming anchor text sparsity stems
from ideas in the relevance based language models(RMs),
proposed by Lavrenko and Croft [10]. Their original work
introduces the RMs to ï¬nd plausible useful terms missing
in the original query for query expansion. Here we adapt
the RMs to compute a web content dependent associated
anchor language model for discovering missing anchor terms
and using anchor text for retrieval. Thus, our approach, although similar in spirit to, diï¬€ers from document expansion
[18] and graph-based document smoothing[13].

3.

DISCOVERING MISSING ANCHOR TEXT

We now describe three diï¬€erent approaches for discovering plausible missing anchor text for web pages with few or
no inlinks. The goal of each is to produce a ranked list of
plausible anchor text terms for a page.

3.1

Aggregating Anchor Text

To overcome anchor text sparsity, Metzler et al.[14] originally proposed to augment web pages with auxiliary anchor
text (denoted as ğ´ğ‘ğ‘¢ğ‘¥ ) that is derived by aggregating anchor
text over the web graph. We ï¬rst brieï¬‚y review the procedure they have used to build ğ´ğ‘ğ‘¢ğ‘¥ , which is very important
for our discussions and comparisons in this research. Given
a web page ğ‘ƒ0 â€™s URL ğ‘¢ğ‘ƒ0 , the procedure ï¬rst collects all
pages ğ‘ƒğ¼ğ‘› (ğ‘ƒ0 ), within the same site (domain), that link to
ğ‘¢ğ‘ƒ0 . These links are known as ğ‘¢ğ‘ƒ0 â€™s internal inlinks. Then
the procedure collects all anchor text ğ´ from pages (denoted
as ğ‘ƒğ´ğ‘¢ğ‘¥ (ğ‘ƒ0 )) that are linked to any page in ğ‘ƒğ¼ğ‘› (ğ‘ƒ0 ) from
outside the site. The anchor text set ğ´ is known as external
anchor text and is used as ğ´ğ‘ğ‘¢ğ‘¥ for ğ‘¢ğ‘ƒ0 .
Figure 1 illustrates the procedure by using a real-world
example from the TREC GOV2 collection. We collect the
auxiliary anchor text ğ´ğ‘ğ‘¢ğ‘¥ for the page ğ‘ƒ0 . ğ‘ƒ0 â€™s original
anchor text (denoted as ğ´ğ‘œğ‘Ÿğ‘–ğ‘” ), which comes from all pages
(denoted as ğ‘ƒğ‘‚ğ‘Ÿğ‘–ğ‘” (ğ‘ƒ0 )) that are directly linked to ğ‘ƒ0 from
outside the site, consists of lines including â€œOptima National
Wildlife Refugeâ€ and â€œOptima NWRâ€. ğ‘ƒ0 â€™s ğ´ğ‘ğ‘¢ğ‘¥ consists of
lines including â€œOklahoma Refuge Websitesâ€ and â€œOklahoma
National Wildlife Refugesâ€.
Note that the above procedure does not use any anchor
text associated with internal inlinks, because internal inlinks
are typically generated by the owner of the site for navigational purposes and their associating anchor text tends to
be navigational in nature (e.g., â€œhomeâ€,â€œnext pageâ€, etc.; refer to [14] for more discussions on this issue). We emphasize
that in the remainder part of this paper we follow Metzler et
al. and do not use the anchor text associated with internal
inlinks in any way.
In this paper we are speciï¬cally interested in the eï¬€ectiveness of using ğ´ğ‘ğ‘¢ğ‘¥ to serve as a surrogate for possibly missing
original anchor text. In other words, we consider how eï¬€ectively we may use ğ´ğ‘ğ‘¢ğ‘¥ to discover plausibly missing original
anchor text of the URL of the interest so that anchor text
sparsity can be eï¬€ectively reduced. Therefore, we focus on
the discovered anchor terms themselves in the ğ´ğ‘ğ‘¢ğ‘¥ . We use
two typical methods to rank the relative importance of each
anchor term ğ‘¤. The ï¬rst method, denoted as AUX-TF, is
to use each term ğ‘¤â€™s term frequency ğ‘¡ğ‘“ğ‘ğ‘¢ğ‘¥ (ğ‘¤) in the ğ´ğ‘ğ‘¢ğ‘¥ .

2. RELATED WORK
Metzler et al. [14] ï¬rst directly addressed the anchor text
sparsity problem by using the web hyperlink graph and propagating anchor text over the web graph. Our work also addresses the same problem but using a diï¬€erent approach,
which is based on the content similarity between web pages.
Our approach is similar in nature to other similarity based
techniques, such as cluster-based smoothing from the language modeling framework[8, 9, 11], except we focus on enriching web documentsâ€™ anchor text representation by using
their similar documentsâ€™ associated anchor text.
Anchor text can be modeled in many diï¬€erent ways. Westerveld et al. [20] and Nallapati et al. [15] model anchor
text in the language modeling approach [17] and calculate
an associated anchor language model to update the original
document model for retrieval. Fujii [6] further considers differently weighting each line of anchor text associated with
the same page thus obtaining a more robust anchor language
model. Here, we also adopt the language modeling approach

428

http://saltplains.fws.gov/just4kid s.html

http://saltplains.fws.gov/index.html

auxiliary anchortext
(aggregated)

Oklahoma
Refuge
Websites
â€¦

P5

Pages within
the same site

Oklahoma
National
Wildlife
Refuges

P6

P2

Similar Pages:

P3

P0

P8

Buffalo
Lake
NWR

P7

P1

Optima
National
Wildlife
Refuge

http://ifw2irm2.irm1.r2.fws.gov/texas.html

original anchortext

â€¦..
Optima
NWR
â€¦..

P9

P4

PIn (P0)={P1, P2 }
POrig (P0)={P8 , P 9}
PAux (P0 )={P5, P6 }
POrig (P4 )={P7 }

http://ifw2es.fws.gov/Oklahoma/refuges.htm l http://ifw2irm2.irm1.r2.fws.gov/toklahoma.html
P 0 : http://southwest.fws.gov/refuges/oklahoma/optima.html. P 1 :http://southwest.fws.gov/oklahoma.html .
P 2 : http://southwest.fws.gov/refuges/okrefuges.html .
P 4 : http://southwest.fws.gov/refuges/texas/buffalo.html .

Figure 1: Illustration of how to aggregate anchor text over the web graph or use similar web pagesâ€™ anchor
text for discovering more anchor text for a web page (ğ‘ƒ0 in this example). The page ğ‘ƒ0 is a GOV2 web page,
whose DocID is GX010-01-9459902 and URL is http://southwest.fws.gov/refuges/oklahoma/optima.html.
The second method, denoted as AUX-TFIDF, is to use
each term ğ‘¤â€™s ğ‘¡ğ‘“ğ‘ğ‘¢ğ‘¥ â‹… ğ‘–ğ‘‘ğ‘“ (ğ‘¤) score, computed by multiplying
ğ‘¡ğ‘“ğ‘ğ‘¢ğ‘¥ (ğ‘¤) with ğ‘¤â€™s ğ‘–ğ‘‘ğ‘“ score in the web collection. The quality of the discovered anchor term rank lists produced from
these two link based approaches implies the eï¬€ectiveness of
using auxiliary anchor text as a surrogate of missing original
anchor text. We will compare these two approaches with our
content based approach in Â§4.

where ğ‘¤ is a word from the vocabulary ğ’± of ğ’. Similarly,
given an target page ğ‘ƒ0 , our approach aims to calculate a
relevant anchor text language model (RALM) ğ‘(ğ‘¤âˆ£ğ´0 ) by:
ğ‘(ğ‘¤âˆ£ğ´0 ) =

Note that in the link based approach, a web page ğ‘ƒ0 still
cannot obtain the auxiliary anchor text if it has no internal
inlinks or if all pages in its ğ‘ƒğ¼ğ‘› (ğ‘ƒ0 ) have no external anchor
text. Indeed, Metzler et al. reported only 38% anchor text
sparsity reduction on a web sample with the link based approach[14]. Therefore, we propose a content based approach,
which does not have speciï¬c link structure requirements on
the target web page, to discover its plausible missing anchor text. Intuitively, our approach assumes that web pages
that are similar in content may be described by similar associated anchor text. For example, in Figure 1, the target
page ğ‘ƒ0 , which is about Optima national wildlife refuge, is
similar in content with the page ğ‘ƒ4 , which is about Buï¬€alo
Lake national wildlife refuge. We observe that the anchor
term â€œNWRâ€, which appears in ğ‘ƒ0 â€™s and ğ‘ƒ4 â€™s ğ´ğ‘œğ‘Ÿğ‘–ğ‘” but not
in ğ‘ƒ0 â€™s ğ´ğ‘ğ‘¢ğ‘¥ , can be used to partially describe both ğ‘ƒ0 and
ğ‘ƒ4 although two pages are concerned about diï¬€erent places.
We consider a language modeling approach to better use
document similarity and anchor text information, based on
ideas from the relevance-based language models (RM)[10].
In brief, given a query ğ‘, RM ï¬rst calculates the posterior
ğ‘(ğ·ğ‘– âˆ£ğ‘) of each document ğ·ğ‘– in the collection ğ’ generating
the query ğ‘, then calculates a query dependent language
model ğ‘(ğ‘¤âˆ£ğ‘):
âˆ‘

ğ‘(ğ‘¤âˆ£ğ·ğ‘– ) Ã— ğ‘(ğ·ğ‘– âˆ£ğ‘),

ğ‘(ğ‘¤âˆ£ğ´ğ‘– ) Ã— ğ‘(ğ´ğ‘– âˆ£ğ´0 ),

(2)

where ğ´ğ‘– denotes the complete original anchor text that
should be associated with ğ‘ƒğ‘– but may be missing, ğ’œ denotes the complete original anchor text space for all pages,
ğ‘(ğ‘¤âˆ£ğ´ğ‘– ) is a multinomial distribution over the anchor text
vocabulary ğ’±ğ’œ . To compute ğ‘(ğ´ğ‘– âˆ£ğ´0 ) in Equation 2 where
ğ´0 and ğ´ğ‘– information may be missing, we view each page
ğ‘ƒğ‘– â€™s content as its anchor text ğ´ğ‘– â€™s context and use ğ‘ƒğ‘– â€™s document language model ğ‘ğ‘– = {ğ‘(ğ‘¤âˆ£ğ‘ƒğ‘– )} as ğ´ğ‘– â€™s contextual
model. Then we can calculate a translation model ğ‘¡(ğ´ğ‘– âˆ£ğ´0 )
by using ğ´0 and ğ´ğ‘– â€™s contextual models and use ğ‘¡(ğ´ğ‘– âˆ£ğ´0 )
to approximate ğ‘(ğ´ğ‘– âˆ£ğ´0 ). This contextual translation approach is also used in Wang and Zhaiâ€™s work [19].
When calculating a page ğ‘ƒğ‘– â€™s document language model
{ğ‘(ğ‘¤âˆ£ğ‘ƒğ‘– )}, we employ Dirichlet smoothing on the maximum
likelihood (ML) estimate of observing a word ğ‘¤ in the page
(ğ‘ğ‘€ ğ¿ (ğ‘¤âˆ£ğ‘ƒğ‘– )) with the wordâ€™s collection probability ğ‘(ğ‘¤âˆ£ğ’):

3.2 Discovering Anchor Text through Finding
Similar Web Pages

ğ‘(ğ‘¤âˆ£ğ‘) =

âˆ‘
ğ´ğ‘– âˆˆğ’œ

ğ‘(ğ‘¤âˆ£ğ‘ƒğ‘– ) =

ğ‘ğ‘ƒğ‘–
ğœ‡
ğ‘ğ‘€ ğ¿ (ğ‘¤âˆ£ğ‘ƒğ‘– ) +
ğ‘(ğ‘¤âˆ£ğ’),
ğ‘ğ‘ƒğ‘– + ğœ‡
ğ‘ğ‘ƒğ‘– + ğœ‡

(3)

where ğ‘ğ‘ƒğ‘– is the length of ğ‘ƒğ‘– â€™s content and ğœ‡ is the Dirichlet
smoothing parameter (ğœ‡ = 2500 in our experiments). Then
given two pages ğ‘ƒ0 and ğ‘ƒğ‘– , we use the Kullback-Leibler
divergence (KL) ğ·ğ‘–ğ‘£(â‹…âˆ£âˆ£â‹…) between their document models
ğ‘0 and ğ‘ğ‘– to measure their similarity and view that as the
contextual similarity between the associated anchor text ğ´0
and ğ´ğ‘– . Then the contextually based translation probability
ğ‘¡(ğ´ğ‘– âˆ£ğ´0 ) is calculated by:

(1)

ğ‘¡(ğ´ğ‘– âˆ£ğ´0 ) =

ğ·ğ‘– âˆˆğ’

429

âˆ‘exp(âˆ’ğ·ğ‘–ğ‘£(ğ‘0 âˆ£âˆ£ğ‘ğ‘– )) .
ğ‘– exp(âˆ’ğ·ğ‘–ğ‘£(ğ‘0 âˆ£âˆ£ğ‘ğ‘– ))

(4)

This ğ‘¡(ğ´ğ‘– âˆ£ğ´0 ) is then used to approximate ğ‘(ğ´ğ‘– âˆ£ğ´0 ) in Equation 2 to get:
ğ‘(ğ‘¤âˆ£ğ´0 ) â‰ˆ

âˆ‘

ğ‘(ğ‘¤âˆ£ğ´ğ‘– ) Ã— ğ‘¡(ğ´ğ‘– âˆ£ğ´0 ).

4.

(5)

We now compare the capability of discovering missing anchor text by diï¬€erent approaches described in Â§3, including
two link based approaches (AUX-TF and AUX-TFIDF), our
content based approach (RALM), and three keyword based
approaches (DOC-TF, DOC-TFIDF and DOC-OKAPI).

(6)

4.1

ğ´ğ‘– âˆˆğ’œ

A few transformations of Equation 4 can obtain:
âˆ
ğ‘¡(ğ´ğ‘– âˆ£ğ´0 ) âˆ ğ‘(ğ‘¤âˆ£ğ‘ƒğ‘– )ğ‘(ğ‘¤âˆ£ğ‘ƒ0 ) ,
ğ‘¤

3.3 Using Keywords as Anchor Text
The keyword based approaches come from the intuition
that important keywords in a web page may be good description terms for the page, thus may be arguably used as
anchor text. We use three typical term weighting schemes
to identify the keywords and rank the words in a web pageâ€™s
content. The ï¬rst method, denoted as DOC-TF, uses each
word ğ‘¤â€™s term frequency ğ‘¡ğ‘“ğ‘ƒ0 (ğ‘¤) in the page ğ‘ƒ0 for term
weighting. The second method, denoted as DOC-TFIDF,
uses each word ğ‘¤â€™s ğ‘¡ğ‘“ğ‘ƒ0 â‹… ğ‘–ğ‘‘ğ‘“ (ğ‘¤) score, computed by multiplying ğ‘¡ğ‘“ğ‘ƒ0 (ğ‘¤) with ğ‘¤â€™s ğ‘–ğ‘‘ğ‘“ score in the web collection. The
third method, denoted as DOC-OKAPI, uses each word
ğ‘¤â€™s Okapi BM25 score ğµğ‘€ 25ğ‘ƒ0 (ğ‘¤), computed by:
ğ‘¡ğ‘“ğ‘ƒ0 (ğ‘¤)â‹…(ğ‘˜1 +1)

âˆ£ğ‘ƒ âˆ£

0
ğ‘¡ğ‘“ğ‘ƒ0 (ğ‘¤)+ğ‘˜1 â‹…(1âˆ’ğ‘+ğ‘â‹… ğ‘ğ‘£ğ‘”ğ‘‘ğ‘™

â‹… ğ‘–ğ‘‘ğ‘“ (ğ‘¤),

Data and Methodology

We use two publicly available large TREC web collections (GOV2 and ClueWeb09-T09B). GOV2 is a standard TREC web collection [4] crawled from government web
sites during early 2004. The ClueWeb09 collection is a much
larger and more recent web crawl, which contains over 1 billion pages. ClueWeb09-T09B is a subset of ClueWeb09 and
contains about 50 million English web pages. Compared
with GOV2 crawled only from the gov domain, ClueWeb09T09B is crawled from the general web thus is a less biased web sample; in another aspect, GOV2 contains relatively high quality government web pages thus having less
noise than ClueWeb09-T09B. Thus we use both GOV2 and
ClueWeb09-T09B in our experiments to show how diï¬€erent
approaches perform in web collections that have diï¬€erent
characteristics. The Indri Search Engine4 was used to index both collections by removing a standard list of 418 INQUERY [2] stopwords and applying Krovetz stemmer. In a
separate process, we run Indri Search Engineâ€™s harvestlinks
utility on the two collections to collect web page inlinks and
raw anchor text information where we do not perform stopping or stemming.
To evaluate the quality of discovered anchor text for a web
page ğ‘ƒ0 , we utilize the original anchor text ğ´ğ‘œğ‘Ÿğ‘–ğ‘” associated
with all inlinks of ğ‘ƒ0 . Speciï¬cally, we ï¬rst hide the page
ğ‘ƒ0 â€™s ğ´ğ‘œğ‘Ÿğ‘–ğ‘” , apply diï¬€erent anchor text discovery approaches
on ğ‘ƒ0 , then compare the discovered anchor text with ğ‘ƒ0 â€™s
ğ´ğ‘œğ‘Ÿğ‘–ğ‘” . This procedure can be run automatically so that
we can leverage large volumes of web pages to evaluate the
performance of diï¬€erent approaches with no human labeling
eï¬€ort. More speciï¬cally, we consider each anchor term in
a page ğ‘ƒ0 â€™s ğ´ğ‘œğ‘Ÿğ‘–ğ‘” as a good description term, or a relevant
term, for ğ‘ƒ0 while terms not in ğ´ğ‘œğ‘Ÿğ‘–ğ‘” as non-relevant ones; in
this way, we can generate term relevance judgments for ğ‘ƒ0 .
Then we employ each diï¬€erent approach to discover a ranked
list of plausible missing anchor terms for ğ‘ƒ0 and then use
the relevant judgments to evaluate the ranked anchor term
list. Note that for fair comparison ğ‘ƒ0 â€™s ğ´ğ‘œğ‘Ÿğ‘–ğ‘” is not used in
Equation 2 for calculating RALM in our approach. In the
experiments, we perform slight stopping on the raw anchor
text by removing a short list of 39 stopwords, which includes
25 common stopwords[12, pp.26] and 14 additional anchor
terms5 that are either common navigational purposed words
or part of URLs â€“ it is common that anchor text contains
some URL.
We calculate some typical TREC style evaluation measurements including Mean Average Precision (MAP), Mean
Reciprocal Rank(MRR), Precision at the number of relevant terms(R-Prec), Precision at ğ¾ (P@ğ‘˜) and also normalized discounted cumulative gain (NDCG) [7]. In the
experiments, we are speciï¬cally interested in the quality of
top ranked discovered anchor terms; thus, we only use the

which is the likelihood of generating ğ´0 â€™s context ğ‘ƒ0 from
ğ´ğ‘– â€™s context ğ‘ƒğ‘– â€™s smoothed language model and being normalized by ğ´0 â€™s context length. This likelihood can be easily obtained by issuing ğ‘ƒ0 as a long query to any language
model based search engine. In addition, we use the observed
incomplete original anchor text language model ğ‘ğ‘œğ‘ğ‘  (ğ‘¤âˆ£ğ´ğ‘– )
associated with ğ‘ƒğ‘– to approximate ğ‘(ğ‘¤âˆ£ğ´ğ‘– ) in Equation 5,
and let ğ‘ğ‘œğ‘ğ‘  (ğ‘¤âˆ£ğ´ğ‘– ) = 0 if ğ‘ƒğ‘– has no ğ´ğ‘œğ‘Ÿğ‘–ğ‘” . In this way, the
RALM ğ‘(ğ‘¤âˆ£ğ´0 ) can be computed.
In practice, for eï¬ƒciency the RALM of the target page
ğ‘ƒ0 is computed from ğ‘ƒ0 â€™s top-ğ‘˜ most similar pagesâ€™ ğ´ğ‘œğ‘Ÿğ‘–ğ‘”
(original anchor text) because ğ‘¡(ğ´ğ‘– âˆ£ğ´0 ) in Equation 4 is very
small for the other pages. Due to the anchor text sparsity,
we set ğ‘˜ = 2000 in our experiments. Because some of these
similar pages do not have associated ğ´ğ‘œğ‘Ÿğ‘–ğ‘” , we use another
parameter ğ‘š to denote the number of most similar pages
whose associated original anchor text is not missing and
contributes information in the RALM, and we tune ğ‘š in
the experiments. Intuitively, increasing ğ‘š can increase the
number of anchor text samples to better estimate RALM
but may also introduce more noise when the sample size is
large.
The probability ğ‘(ğ‘¤âˆ£ğ´0 ) of an anchor term ğ‘¤ in the RALM
directly reï¬‚ects the goodness of the term ğ‘¤ used as original
anchor text for the page ğ‘ƒ0 , thus we use the anchor terms
that have the largest probabilities ğ‘(ğ‘¤âˆ£ğ´0 ) in the RALM
to evaluate the eï¬€ectiveness of our content based approach.
Theoretically our approach can associate any web page with
some anchor term distribution information if there is some
anchor text in the corpus, thus it can further reduce the
anchor text sparsity.

ğµğ‘€ 25(ğ‘¤) =

EVALUATING DISCOVERY

(7)

where ğ‘ğ‘£ğ‘”ğ‘‘ğ‘™ is the average document length of the pages in
the collection. We use the typical setting ğ‘˜1 = 2, ğ‘ = 0.75 in
Equation 7 in our experiments.
The top ranked terms in a page ğ‘ƒ0 by three methods are
used as the possible missing original anchor terms for ğ‘ƒ0 .
We will use three keyword based methods as baselines in Â§4.

4

http://www.lemurproject.org/indri/
The additional terms are: http, https, www, gov, com, org,
edu, net, html, htm, click, here, next, home.
5

430

DOC-TF
DOC-TFIDF
DOC-OKAPI
AUX-TF
AUX-TFIDF
RALM

MAP
0.3162
0.2936
0.2936
0.1969
0.1716
0.3183

NDCG
0.4585
0.4348
0.4348
0.2598
0.2423
0.4275

MRR
0.5441
0.5400
0.5400
0.3707
0.3442
0.5050

P@2
0.3833
0.3700
0.3700
0.2833
0.2433
0.3467

P@5
0.2800
0.2613
0.2613
0.1773
0.1720
0.2840

P@10
0.2060
0.1827
0.1827
0.1153
0.1140
0.1860

P@20
0.1333
0.1240
0.1240
0.0643
0.0647
0.1140

R-Prec
0.2716
0.2530
0.2530
0.1643
0.1428
0.3051

Discovered Rel.
400
372
372
193
194
342

Table 2: Performances on the GOV2 collection. There are 708 relevant anchor terms overall. Column 10
shows overall relevant anchor terms discovered by each diï¬€erent approach. RALM performs statistically
signiï¬cantly better than AUX-TF and AUX-TFIDF by each measurement in columns 2â€“9 according to the
one-sided t-test (ğ‘ < 0.005). There exists no statistically signiï¬cant diï¬€erence between each pair of RALM,
DOC-TF, DOC-TFIDF and DOC-OKAPI by each measurement according to the one-sided t-test (ğ‘ < 0.05).
DOC-TF
DOC-TFIDF
DOC-OKAPI
AUX-TF
AUX-TFIDF
RALM

MAP
0.3517
0.3107
0.3107
0.1840
0.1634
0.2612

NDCG
0.4891
0.4388
0.4388
0.2507
0.2347
0.3615

MRR
0.5588
0.5145
0.5145
0.3309
0.3116
0.4630

P@2
0.3467
0.3133
0.3133
0.2248
0.2047
0.2833

P@5
0.2373
0.2213
0.2213
0.1463
0.1383
0.1733

P@10
0.1360
0.1173
0.1173
0.0729
0.0676
0.0911

P@20
0.1090
0.0983
0.0983
0.0577
0.0560
0.0770

R-Prec
0.2990
0.2608
0.2608
0.1675
0.1402
0.2398

Discovered Rel.
327
295
295
172
167
231

Table 3: Performances on the ClueWeb09-T09B collection. There are 582 relevant anchor terms overall.
Column 10 shows overall relevant anchor terms discovered by each diï¬€erent approach. DOC-TF performs
statistically signiï¬cantly better than both RALM and AUX-TF by each measurement in columns 2â€“9 according to the one-sided t-test (ğ‘ < 0.05). RALM performs statistically signiï¬cantly better than AUX-TF and
AUX-TFIDF by each measurement in columns 2â€“9 according to the one-sided t-test (ğ‘ < 0.05).
top-20 terms in the discovered term rank lists by diï¬€erent
approaches to calculate the measurements.
Note that web pages that can be used in our evaluation procedure need to satisfy two requirements: (1) they
need to have some associated ğ´ğ‘œğ‘Ÿğ‘–ğ‘” and (2) they can collect
some auxiliary anchor text from the web graph as described
in Â§3.1. Thus, for each of two collections, we randomly sample 150 pages satisfying the two requirements for training
and another 150 pages for testing. On both training sets,
RALMâ€™s parameter ğ‘š = 15 described in Â§3.2 achieves the
highest MAPs.

ğ‘ğ‘ğ‘¡(AUX-TF, DOC-TF)
ğ‘ğ‘ğ‘¡(AUX-TF, RALM)
ğ‘ğ‘ğ‘¡(RALM, DOC-TF)

GOV2
30.5%
47.6%
26.0%

ClueWeb09-T09B
26.0%
46.3%
22.3%

Table 4: The average percentage ğ‘ğ‘ğ‘¡(ğ‘‹, ğ‘Œ ) of the
terms discovered by the ğ‘‹ approach appearing in
the ones discovered by the ğ‘Œ approach.
tion words from the web content. One plausible reason that
RALM performs relatively poorly on ClueWeb09-T09B is
that, compared with the high quality GOV2 pages, ClueWeb
pages are crawled from the general web, where the inlinks
and anchor text may be generated in a more noisy way (e.g.
spam), degrading RALMâ€™s performance. To better understand the performance of diï¬€erent approaches, in Table 5
and Table 6 we show the top-10 words of the anchor term
rank lists discovered by diï¬€erent approaches for one evaluation web page in GOV2 and ClueWeb09-T09B, respectively.
Although using keyword information can discover some
good anchor terms, the content-generated anchor terms do
not help bridging the lexical gap between a web page and
varied queries that attempt to search the page. Indeed, human generated anchor text is highly useful for reducing the
word mismatch problem because the lexical gap between
anchor text and queries is relatively small[14]. Here, we do
some lexical gap analysis to show that our approach can also
discover anchor terms similar in nature to human-generated
ones but diï¬€erent from content-generated ones.
For each web page ğ‘– in the testing set, we calculate the
percentage ğ‘ğ‘ğ‘¡ğ‘– (ğ‘‹, ğ‘Œ ) of the terms discovered by the ğ‘‹ approach also appearing in the ones discovered by the ğ‘Œ approach, then compute the average percent ğ‘ğ‘ğ‘¡(ğ‘‹, ğ‘Œ ) with
all the pages. We use the outputs from the keyword based
DOC-TF, the link based AUX-TF, and the RALM in this
analysis. Table 4 shows three average percentages ğ‘ğ‘ğ‘¡(ğ‘‹, ğ‘Œ )

4.2 Results and Analysis
The performance of discovering original anchor text by different approaches on the testing set of GOV2 and ClueWeb09-T09B are shown in Table 2 and Table 3, respectively.
The results show that our approach (RALM) can eï¬€ectively
discover missing original anchor terms. On both collections
RALM performs statistically signiï¬cantly better than two
link based approaches (AUX-TF and AUX-TFIDF). This
indicates that, for discovering a pageâ€™s missing anchor text,
the anchor text associated with the similar pages provides
more useful information than that associated with the linked
web neighbors. The numbers of discovered relevant anchor
terms by diï¬€erent approaches, shown in the last column of
two tables, also indicate that only using auxiliary anchor
text misses more original anchor text information than our
content based approach.
Another observation is that RALM performs worse on
ClueWeb09-T09B and not statistically signiï¬cantly better
on GOV2 than the keyword based approaches. This indicates that words having high IR utility like ğ‘¡ğ‘“ or ğ‘¡ğ‘“ â‹… ğ‘–ğ‘‘ğ‘“
scores are often good description terms for the page and
used by human being as the anchor text. Removing a long
list of stopwords from web page content has also helped the
keyword based approaches to eï¬€ectively select good descrip-

431

which we have speciï¬c interest in. We observe that AUXTFâ€™s discovered terms have much higher average per query
overlap ratio with RALMâ€™s than with DOC-TFâ€™s. Moreover, RALMâ€™s discovered anchor terms have small overlap
with DOC-TFâ€™s.

QL
M-ORG
M-AUX
M-ORG-AUX
M-RALM
M-ORG-RALM

5. USING DISCOVERED ANCHOR TEXT
FOR WEB SEARCH

MRR
0.3132
0.3696
0.3187
0.3711
0.3388â–³
0.3975â˜…â–³

%Top10
49.7
57.5
50.8
57.5
53.6
59.7

Opt. Param.
ğ›¼ = 0.95
ğ›¼ = 0.99
ğ›¼ = 0.95, ğ›½ = 0.99
ğ‘š = 20, ğ›¼ = 0.95
ğ›¼, ğ›½ = 0.95, ğ‘š = 20

We now describe how we use the discovered anchor text
by diï¬€erent approaches for retrieval in a language modeling approach [17]. We point out that our focus here is not
to evaluate diï¬€erent schemes to aggregate or combine anchor text [14]; instead, we focus on comparing the utility of
RALM and auxiliary anchor text for helping retrieval.

Table 7: Retrieval performance of diï¬€erent approaches with TREC 2006 NP queries. The star
indicates statistically signiï¬cant improvement over
MRRs of M-ORG and M-ORG-AUX by one-sided
t-test (ğ‘ < 0.05). The triangle indicates statistically
signiï¬cant improvement over MRRs of QL and MAUX by one-sided t-test (ğ‘ < 0.05).

We follow the typical language modeling based retrieval
approach[17] and score each web page ğ‘ƒ for a query ğ‘„ by the
likelihood of the page ğ‘ƒ â€™s document language model ğ‘(ğ‘¤âˆ£ğ‘ƒ )
generating the query ğ‘„:

4. M-RALM, which only uses the RALM ğ‘(ğ‘¤âˆ£ğ´0 ) in Equation 2. The original anchor text of ğ‘ƒ0 is not used in Equation
2 for calculating RALM.
5. M-ORG-RALM, which uses both ğ‘(ğ‘¤âˆ£ğ´ğ‘œğ‘Ÿğ‘–ğ‘” ) and the
RALM ğ‘(ğ‘¤âˆ£ğ´0 ) in Equation 2 by:

5.1 Retrieval Models

ğ‘(ğ‘„âˆ£ğ‘ƒ ) =

âˆ

ğ‘(ğ‘¤âˆ£ğ‘ƒ ).

(8)

ğ‘¤âˆˆğ‘„

ğ‘Ëœ(ğ‘¤âˆ£ğ‘ƒ ) = ğ›½(ğ›¼ğ‘(ğ‘¤âˆ£ğ‘ƒ ) + (1 âˆ’ ğ›¼)ğ‘(ğ‘¤âˆ£ğ´ğ‘œğ‘Ÿğ‘–ğ‘” ))
+(1 âˆ’ ğ›½)ğ‘(ğ‘¤âˆ£ğ´0 ).

When using Dirichlet smoothing, the document language
model ğ‘(ğ‘¤âˆ£ğ‘ƒ ) can be calculated by Equation 3 and then
used in Equation 8 for retrieval. We call this baseline QL.
We only ï¬x ğœ‡ = 2500 in Equation 3 for the document models
used to calculate RALM, but tune the ğœ‡ for QL to achieve
the best retrieval performance in our experiments in Â§5.2.
We follow the mixture model approach [15, 16] to use the
discovered anchor text information for helping retrieval. In
this approach, a web page ğ‘ƒ â€™s document language model is
assumed to be a mixture of multiple component distributions
where each component is associated with a prior probability,
or a mixture weight. Therefore, we can estimate a language
model ğ‘(ğ‘¤âˆ£ğ´) from anchor text discovered by each diï¬€erent
approach for the page ğ‘ƒ and use ğ‘(ğ‘¤âˆ£ğ´) as a component
of ğ‘ƒ â€™s document model thus obtaining a better document
language model ğ‘Ëœ(ğ‘¤âˆ£ğ‘ƒ ):
ğ‘Ëœ(ğ‘¤âˆ£ğ‘ƒ ) = ğ›¼ğ‘(ğ‘¤âˆ£ğ‘ƒ ) + (1 âˆ’ ğ›¼)ğ‘(ğ‘¤âˆ£ğ´),

The original anchor text of ğ‘ƒ0 is not used in Equation 2 for
calculating RALM.
Note that we can update each pageâ€™s document model
oï¬„ine, thus this computationally expensive procedure has
little impact on the online query processing time. Moreover,
diï¬€erent from experiments in Â§4.1, we use all anchor terms
instead of the top-20 most important terms discovered by
diï¬€erent approaches.

5.2

(9)

where ğ‘(ğ‘¤âˆ£ğ‘ƒ ) is the original smoothed document model in
the QL baseline. Then we can plug ğ‘Ëœ(ğ‘¤âˆ£ğ‘ƒ ) into equation
8 for retrieval. We compare the retrieval performance of
document language models updated by diï¬€erent discovered
anchor text information.
We consider three diï¬€erent anchor text sources to update
a web page ğ‘ƒ â€™s document model: (1) the observed original anchor text ğ´ğ‘œğ‘Ÿğ‘–ğ‘” associated with ğ‘ƒ , (2) the auxiliary
anchor text ğ´ğ‘ğ‘¢ğ‘¥ of ğ‘ƒ , and (3) the RALM computed by
our approach for ğ‘ƒ . We estimate the anchor text language
model ğ‘(ğ‘¤âˆ£ğ´ğ‘œğ‘Ÿğ‘–ğ‘” ) and ğ‘(ğ‘¤âˆ£ğ´ğ‘ğ‘¢ğ‘¥ ) by using the ML estimate
of observing each word ğ‘¤ in ğ´ğ‘œğ‘Ÿğ‘–ğ‘” and ğ´ğ‘ğ‘¢ğ‘¥ , respectively.
Here, we design the following ï¬ve retrieval methods that use
the above three anchor text sources:
1. M-ORG, which only uses the observed original anchor
text language ğ‘(ğ‘¤âˆ£ğ´ğ‘œğ‘Ÿğ‘–ğ‘” ).
2. M-AUX, which only uses the auxiliary anchor text language ğ‘(ğ‘¤âˆ£ğ´ğ‘ğ‘¢ğ‘¥ ).
3. M-ORG-AUX, which uses both ğ‘(ğ‘¤âˆ£ğ´ğ‘œğ‘Ÿğ‘–ğ‘” ) and ğ‘(ğ‘¤âˆ£ğ´ğ‘ğ‘¢ğ‘¥ )
to update the document model ğ‘(ğ‘¤âˆ£ğ‘ƒ ) by:
ğ‘Ëœ(ğ‘¤âˆ£ğ‘ƒ ) = ğ›½(ğ›¼ğ‘(ğ‘¤âˆ£ğ‘ƒ ) + (1 âˆ’ ğ›¼)ğ‘(ğ‘¤âˆ£ğ´ğ‘œğ‘Ÿğ‘–ğ‘” ))
+(1 âˆ’ ğ›½)ğ‘(ğ‘¤âˆ£ğ´ğ‘ğ‘¢ğ‘¥ ).

(11)

(10)

432

Experiments

We use the TREC web named page ï¬nding tasks in Terabyte Track[4, 5] to evaluate the performance of diï¬€erent
retrieval methods described in Â§5.1. The objective of the
named page (NP) ï¬nding task is to ï¬nd a particular page in
the GOV2 collection, given a topic that describes it. We use
the NP topics and their relevance judgments for our experiments. In this experiment, we used Porter stemmer and did
not remove stopwords when indexing the GOV2 collection.
For each NP query, we ï¬rst run it against the GOV2
collection to obtain the QL baseline; then we use ï¬ve retrieval methods described in Â§5.1 to rerank the top-100 web
pages returned by QL. The reranked lists are evaluated by
two TREC measurements previously used for the task [5]:
MRR which is the mean reciprocal rank of the ï¬rst correct
answer and the %Top10 which is the proportion of queries
for which a correct answer was found in the ï¬rst 10 search
results. We use the TREC 2005 NP topics (NP601-872)
for training and the TREC 2006 NP topics (NP901-1081)
for testing. We ï¬rst tune the Dirichlet parameter ğœ‡ = 500
for QL to achieve the highest MRR on the training set and
obtain QLâ€™s top-100 web pages for reranking. We then ï¬x
ğœ‡ = 500 to calculate the smoothed document model component ğ‘(ğ‘¤âˆ£ğ‘ƒ ) in the ï¬ve retrieval methods but tune the
mixture parameters ğ›¼ and ğ›½ for them to achieve the highest
MRRs with the training queries. For the two approaches
that use RALM, the parameter ğ‘š of RALM is also tuned.
After that, we run diï¬€erent methods on the testing set.
Table 7 shows the retrieval performance of diï¬€erent methods and the tuned parameters in each method. We observe:
(1) M-ORG-RALM performs statistically signiï¬cantly bet-

â€œOptima National Wildlife Refugeâ€, â€œOptima NWRâ€, â€œWashita Optima National Wildlife Refuge near Butler OKâ€
DOC-TF
ğ‘¡ğ‘“ğ‘ƒ0 (ğ‘¤)
DOC-TFIDF ğ‘¡ğ‘“ğ‘ƒ0 ğ‘–ğ‘‘ğ‘“ (ğ‘¤) DOC-OKAPI ğµğ‘€ 25ğ‘ƒ0 (ğ‘¤) AUX-TF ğ‘¡ğ‘“ğ‘ğ‘¢ğ‘¥ (ğ‘¤)
refuge
15
refuge
79.69
refuge
153.76
oklahoma
6
wildlife
10
optima
74.30
optima
143.37
wildlife
2
oklahoma
10
hardesty
47.48
hardesty
91.63
refuge
2
optima
8
hawk
36.20
hawk
69.86
website
1
species
6
oklahoma
36.03
oklahoma
69.53
u
1
hawk
6
wildlife
31.98
wildlife
61.71
service
1
habitat
6
guymon
29.35
guymon
56.63
s
1
area
6
habitat
26.42
habitat
50.98
oï¬ƒce
1
prairie
5
species
23.70
species
45.73
national
1
national
5
quail
21.74
quail
41.95
ï¬sh
1
AUX-TFIDF ğ‘¡ğ‘“ğ‘ğ‘¢ğ‘¥ ğ‘–ğ‘‘ğ‘“ (ğ‘¤)
RALM
ğ‘ƒ (ğ‘¤âˆ£ğ´0 )
Rel.
oklahoma
21.62
nwr
0.1164
butler
refuge
10.62
wildlife
0.0834
national
wildlife
6.40
refuge
0.0834
near
ï¬sh
3.11
national
0.0834
nwr
u
3.03
general
0.0657
optima
website
2.36
brochure
0.0657
refuge
oï¬ƒce
1.54
kansas
0.0601
washita
s
1.29
lake
0.0522
wildlife
national
1.22
tear
0.0308
service
1.09
sheet
0.0308
Table 5: Discovered missing anchor terms and their term weights by applying diï¬€erent approaches on one
GOV2 web page (TREC DocID in GOV2: GX010-01-9459902) . The ï¬rst row shows the original three pieces
of anchor text associated with the page. The Rel column in bold font shows the term relevance judgments
extracted from the ï¬rst row. RALM can discover some term like â€œNWRâ€, which may not appear in both the
page and the auxiliary anchor text, thus may help to bridge the lexical gap between pages and web queries
as using the original anchor text does.

â€œWeight Loss Resolutionsâ€, â€œWeight Loss New Yearâ€™s Resolution to Lose Weightâ€,â€œResolve to Lose Weightâ€
DOC-TF
ğ‘¡ğ‘“ğ‘ƒ0 (ğ‘¤)
DOC-TFIDF ğ‘¡ğ‘“ğ‘ƒ0 ğ‘–ğ‘‘ğ‘“ (ğ‘¤) DOC-OKAPI ğµğ‘€ 25ğ‘ƒ0 (ğ‘¤) AUX-TF ğ‘¡ğ‘“ğ‘ğ‘¢ğ‘¥ (ğ‘¤)
weight
46
weight
96.38
weight
112.53
weight
709
loss
26
loss
78.65
loss
91.83
loss
705
lose
20
lose
64.47
lose
75.28
diet
32
new
17
resolution
46.57
resolution
54.38
weightloss
21
year
15
diet
34.27
diet
40.02
guide
20
resolution
13
goal
26.01
goal
30.37
scott
8
time
12
eat
25.61
eat
29.90
jennifer
8
make
10
year
23.90
year
27.90
contact
8
goal
9
calorie
15.73
calorie
18.36
site
6
diet
9
pound
15.34
pound
17.91
s
4
AUX-TFIDF ğ‘¡ğ‘“ğ‘ğ‘¢ğ‘¥ ğ‘–ğ‘‘ğ‘“ (ğ‘¤)
RALM
ğ‘ƒ (ğ‘¤âˆ£ğ´0 )
Rel.
loss
2132.63
weight
0.2245
lose
weight
1485.49
loss
0.1737
loss
weightloss
157.70
diet
0.0550
new
diet
121.86
easy
0.0436
resolution
guide
37.26
lose
0.0422
resolve
jennifer
33.96
way
0.0412
s
scott
28.52
myth
0.0396
weight
guidesite
22.04
warn
0.0232
em
13.15
ppa
0.0232
mlibrary
11.37
fda
0.0232
Table 6: Discovered missing anchor terms and their term weights by applying diï¬€erent approaches on one
ClueWeb09 web page (ClueWeb09 RecordID: clueweb09-en0004-60-01628). The ï¬rst row shows the original
three pieces of anchor text associated with the page. The Rel column in bold font shows the term relevance
judgments extracted from the ï¬rst row. The keyword approaches discovered â€œnew year resolutionâ€, which
may be hard to be discovered by using the pageâ€™s web-graph neighbor pagesâ€™ anchor text or using the pageâ€™s
similar pagesâ€™ anchor text.

433

ter than M-ORG. This indicates that missing anchor text
discovered by RALM provides additional information not in
the original anchor text so that combining them can further
improve the retrieval performance. (2) M-ORG-RALM and
M-RALM performs statistically signiï¬cantly better than MORG-AUX and M-AUX, respectively. This indicates that
in GOV2 missing anchor text information discovered by our
content based approach helps retrieval more eï¬€ectively than
the auxiliary anchor text.6
In Table 7, we observe that the auxiliary anchor text helps
the performance very little in this task. There are two plausible reasons: ï¬rst, TREC NP queries are short queries and
Metzler et al. observed that auxiliary anchor text does not
help or even hurts the performance of short navigational web
queries[14]; second, the anchor text sparsity problem is serious on the GOV2, thus very small percentage of pages can
collect some auxiliary anchor text as shown in Table 1 to
beneï¬t the search task. However, even when serious anchor
text sparsity exists and queries are short, our content based
approach still helps improving retrieval eï¬€ectiveness.
We expect our technique can enhance the retrieval performance of general web search engines where there are large
portion of short navigational queries. As is well known, in
the general web search environment there are many lowquality web pages and spam; thus, we need to address issues
about web page quality and noise ï¬ltering for better beneï¬tting general web search. We leave this as future work.

There are several interesting directions of future work.
Metzler et al. found that auxiliary anchor text can eï¬€ectively help longer, informational queries [14]; we will explore
how well RALM can help long informational queries. We
also want to explore using RALMâ€™s discovered missing anchor text information beyond the language modeling based
retrieval framework, e.g. using it to extract useful features
for learning-to-rank retrieval approaches [3].

7.

ACKNOWLEDGMENTS

8.

REFERENCES

This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF IIS-0910884.
Any opinions, ï¬ndings and conclusions or recommendations
expressed in this material are the authorsâ€™ and do not necessarily reï¬‚ect those of the sponsor.
[1] A. Broder et al. Graph structure in the web. Comput.
Netw., 33(1-6):309â€“320, 2000.
[2] J. Broglio, J. P. Callan, and W. B. Croft. An overview of
the INQUERY system as used for the TIPSTER project.
Technical report, Amherst, MA, USA, 1993.
[3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,
N. Hamilton, and G. Hullender. Learning to rank using
gradient descent. In Proc. of ICML, pp. 89â€“96, 2005.
[4] S. BuÌˆttcher, C. L. A. Clarke, and I. Soboroï¬€. The TREC
2006 Terabyte Track. In TREC, 2006.
[5] C. L. A. Clarke, F. Scholer, and I. Soboroï¬€. The TREC
2005 Terabyte Track. In TREC, 2005.
[6] A. Fujii. Modeling anchor text and classifying queries to
enhance web document retrieval. In Proc. of WWW, pp.
337â€“346, 2008.
[7] K. JaÌˆrvelin and J. KekaÌˆlaÌˆinen. Cumulated gain-based
evaluation of IR techniques. ACM Trans. Inf. Syst.,
20(4):422â€“446, 2002.
[8] O. Kurland and L. Lee. Corpus structure, language models,
and ad hoc information retrieval. In SIGIR, pp. 194â€“201,
2004.
[9] O. Kurland and L. Lee. Respect my authority!: Hits
without hyperlinks, utilizing cluster-based language
models. In SIGIR, pp. 83â€“90, 2006.
[10] V. Lavrenko and W. B. Croft. Relevance based language
models. In SIGIR, pp. 120â€“127, 2001.
[11] X. Liu and W. B. Croft. Cluster-based retrieval using
language models. In SIGIR, pp. 186â€“193, 2004.
[12] C. D. Manning, P. Raghavan, and H. SchuÌˆtze. Introduction
to Information Retrieval. Cambridge Univ. Press. 2008.
[13] Q. Mei, D. Zhang, and C. Zhai. A general optimization
framework for smoothing language models on graph
structures. In SIGIR, pp. 611â€“618, 2008.
[14] D. Metzler, J. Novak, H. Cui, and S. Reddy. Building
enriched document representations using aggregated anchor
text. In SIGIR, pp. 219â€“226, 2009.
[15] R. Nallapati, B. Croft, and J. Allan. Relevant query
feedback in statistical language modeling. In Proc. of
CIKM, pp. 560â€“563, 2003.
[16] P. Ogilvie and J. Callan. Combining document
representations for known-item search. In SIGIR, pp.
143â€“150, 2003.
[17] J. M. Ponte and W. B. Croft. A language modeling
approach to information retrieval. In SIGIR, pp. 275â€“281,
1998.
[18] T. Tao, X. Wang, Q. Mei, and C. Zhai. Language model
information retrieval with document expansion. In Proc. of
NAACL-HLT, pp. 407â€“414, 2006.
[19] X. Wang and C. Zhai. Mining term association patterns
from search logs for eï¬€ective query reformulation. In Proc.
of CIKM, pp. 479â€“488, 2008.
[20] T. Westerveld, W. Kraaij, and D. Hiemstra. Retrieving
web pages using content, links, urls and anchors. In Proc.
of TREC, pp. 663â€“672, 2001.

6. CONCLUSIONS AND FUTURE WORK

In this paper, we proposed a language modeling based
technique to overcome the anchor text sparsity problem by
using web content similarity. Our approach computes a
relevant anchor text language model, called RALM, from
its similar web pagesâ€™ associated anchor text to discover its
plausible missing anchor text. Compared with a link based
approach [14], our content based approach has no speciï¬c
link structure requirements on the web page of interest and
thus can further reduce anchor text sparsity.
We designed experiments with two TREC web corpora
to evaluate the eï¬€ectiveness of discovering missing anchor
terms by three diï¬€erent approaches: the link based approach,
the RALM approach, and the keyword based approach. Experimental results show that the RALM approach can effectively discover missing original anchor text and performs
statistically signiï¬cantly better than the two link based approaches on both collections. Moreover, RALMâ€™s discovered
anchor text is similar in nature to auxiliary anchor text while
diï¬€erent from the keywords in the web page.
By using the mixture model[15, 16], we used diï¬€erent discovered anchor text information within the language modeling framework for retrieval. We evaluated using diï¬€erent approaches for improving retrieval eï¬€ectiveness with the
TREC named page ï¬nding task. The results show that (1)
RALM helps retrieval more than using the auxiliary anchor
text collected over the web graph and (2) combining RALM
and the original anchor text can statistically signiï¬cantly improve the retrieval performance of only using the original anchor text. Furthermore, RALM can help improving retrieval
eï¬€ectiveness for short navigational queries even when serious
anchor text sparsity exists. This makes RALM a promising
technique for improving general web search engines.
6

Our goal is not to compare ranking schemes, but to show the
utility of the discovered anchor text. However, we note that these
scores match or beat top-performing approaches [4].

434

