A Content based Approach for Discovering Missing
Anchor Text for Web Search
Xing Yi and James Allan
Center for Intelligent Information Retrieval
Computer Science Department
University of Massachusetts, Amherst, MA, USA

{yixing,allan}@cs.umass.edu
ABSTRACT

# of web pages
# of pages having inlinks
# of pages having original
or enriched inlinks[14]

Although anchor text provides very useful information for
web search, a large portion of web pages have few or no
incoming hyperlinks (anchors), which is known as the anchor text sparsity problem. In this paper, we propose a language modeling based technique for overcoming anchor text
sparsity by discovering a web page’s plausible missing anchor text from its similar web pages’ in-link anchor text.
We design experiments with two publicly available TREC
web corpora (GOV2 and ClueWeb09) to evaluate diﬀerent
approaches for discovering missing anchor text. Experimental results show that our approach can eﬀectively discover
plausible missing anchor terms. We then use the web named
page ﬁnding task in the TREC Terabyte track to explore the
utility of missing anchor text information discovered by our
approach for helping retrieval. Experimental results show
that our approach can statistically signiﬁcantly improve retrieval performance, compared with several approaches that
only use anchor text aggregated over the web graph.

GOV2
25,205,179
376,121 (1.5%)
977,538 (3.9%)

ClueWeb09-T09B
50,220,423
7,640,585 (15.2%)
19,096,359 (38.0%)

Table 1: Summary of in-link statistics on two TREC
web corpora used in our study.
here” and “next”), many times anchor text provides succinct description of the destination URL’s content, e.g. “SIGIR 2010(Geneva, Switzerland)” is from an anchor linked
to http://www.sigir2010.org/. Anchor text instances are
usually reasonable queries that web users may issue to search
for the associated URL and have been used to simulate plausible web queries relevant to the associated web pages in
some web search research [15]. Therefore, anchor text is
highly useful for bridging the lexical gap between user issued web queries and the relevant web pages. It is arguably
the most important piece of evidence used in web ranking
functions[14].
However, previous research has shown that the distribution of the number of inlinks on the web follows a power law
[1], where a small portion of web pages have a large number
of inlinks while most have few or no inlinks. Thus, most web
pages do not have in-link associated anchor text, a problem
originally referred to as the anchor text sparsity problem by
Metzler et al. [14]. This problem presents a major obstacle for any web search algorithms that want to use anchor
text to improve retrieval eﬀectiveness. Table 1 shows the anchor text sparsity problem in two large TREC1 web corpora
(GOV22 and ClueWeb09-T09B3 ). To address this problem,
Metzler et al. [14] proposed aggregating, or propagating, anchor text across the web hyperlink graph so that web pages’
lack of anchor text can be enriched with their linked web
pages’ associated anchor text. Table 1 shows that the number of URLs associated with some anchor text (original or
propagated) in the two TREC web corpora is signiﬁcantly
increased by using their linked-based anchor text enrichment
approach. Nevertheless, in Table 1 we notice that large portion of web pages still do not have any associated anchor
text after having been enriched. This observation motivated
us to consider another possible approach, which utilizes the
content similarity between web pages, to alleviate anchor
text sparsity.

Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and
Retrieval–Search process,Retrieval models;H.3.5 [Information Storage and Retrieval]:Online Information Services–
Web-based services
General Terms: Algorithms, Experimentation
Keywords: anchor text, anchor text sparsity, language
models, relevance models, content similarity, web search

1. INTRODUCTION
There are rich dynamic human generated hyperlink structures on the web. Most web pages contain some hyperlinks,
referred to as anchors, that point to other pages. Each anchor consists of a destination URL and a short piece of text,
which is called anchor text. Anchors play an important role
in helping web users conveniently navigate to their interested web information. Although some anchor text only
functions as a navigational shortcut which does not have direct semantic relation to the destination URL (e.g.,“click

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

1

http://trec.nist.gov/
http://ir.dcs.gla.ac.uk/test_collections/
gov2-summary.htm
3
http://boston.lti.cs.cmu.edu/Data/clueweb09/
2

427

Speciﬁcally, we hypothesize that the anchor text associated with a web page’s inlinks typically has close semantic
relations to the web page so that web pages that are similar in content may be pointed to by anchors having similar
anchor text. Under this assumption, in this paper we propose a language modeling based technique for discovering a
web page’s plausible missing in-link anchor text by using its
most similar web pages’ in-link anchor text. We then test
the eﬀectiveness of our approach by using the discovered
missing anchor text information for some TREC web search
tasks. We ﬁnd that even on the GOV2 data where a serious anchor text sparsity problem exists as shown in Table
1, our approach can signiﬁcantly improve retrieval performance. Our content based approach can be combined with
the hyperlink based approach to further reduce anchor text
sparsity and beneﬁt web search. Our enriched document
and anchor text representations can also be used for many
other tasks beyond web search, including estimating better
document models and extracting advanced textual features
for content match and document classiﬁcation.
Our work has four chief contributions: 1) although content similarity has been used widely in other applications,
we are the ﬁrst to propose using web content similarity to
address the anchor text sparsity problem. 2) We develop a
language modeling based technique, which stems from ideas
in one eﬀective retrieval technique – relevance based language models [10], to eﬀectively discover plausible missing
anchor text information and use it for retrieval. 3) We empirically show that our approach performs better than Metzler et al.’s linked-based approach [14] in terms of discovering
plausible missing anchor terms in two standard large TREC
web corpora. 4) We show that our approach statistically
signiﬁcantly improves retrieval eﬀectiveness, compared with
several approaches that only use aggregated anchor text over
the web graph, in the web named page ﬁnding task of the
TREC Terabyte track [4].
We begin by reviewing related work in §2. Next, we
describe diﬀerent approaches of discovering missing anchor
text to enrich document representations in §3. Then we
describe the experimental setup and results of evaluating
diﬀerent approaches for anchor text discovery in §4. After
that, we present how to use discovered anchor text information for retrieval in a language modeling approach and
report the experimental results in §5. We conclude in §6.

but focus on discovering a plausible associated anchor language model for web pages with no or few inlinks. Our approach can be easily used together with any language model
based retrieval model (e.g., Ogilvie and Callan’s model [16])
that takes document structure into account.
Our approach of overcoming anchor text sparsity stems
from ideas in the relevance based language models(RMs),
proposed by Lavrenko and Croft [10]. Their original work
introduces the RMs to ﬁnd plausible useful terms missing
in the original query for query expansion. Here we adapt
the RMs to compute a web content dependent associated
anchor language model for discovering missing anchor terms
and using anchor text for retrieval. Thus, our approach, although similar in spirit to, diﬀers from document expansion
[18] and graph-based document smoothing[13].

3.

DISCOVERING MISSING ANCHOR TEXT

We now describe three diﬀerent approaches for discovering plausible missing anchor text for web pages with few or
no inlinks. The goal of each is to produce a ranked list of
plausible anchor text terms for a page.

3.1

Aggregating Anchor Text

To overcome anchor text sparsity, Metzler et al.[14] originally proposed to augment web pages with auxiliary anchor
text (denoted as 𝐴𝑎𝑢𝑥 ) that is derived by aggregating anchor
text over the web graph. We ﬁrst brieﬂy review the procedure they have used to build 𝐴𝑎𝑢𝑥 , which is very important
for our discussions and comparisons in this research. Given
a web page 𝑃0 ’s URL 𝑢𝑃0 , the procedure ﬁrst collects all
pages 𝑃𝐼𝑛 (𝑃0 ), within the same site (domain), that link to
𝑢𝑃0 . These links are known as 𝑢𝑃0 ’s internal inlinks. Then
the procedure collects all anchor text 𝐴 from pages (denoted
as 𝑃𝐴𝑢𝑥 (𝑃0 )) that are linked to any page in 𝑃𝐼𝑛 (𝑃0 ) from
outside the site. The anchor text set 𝐴 is known as external
anchor text and is used as 𝐴𝑎𝑢𝑥 for 𝑢𝑃0 .
Figure 1 illustrates the procedure by using a real-world
example from the TREC GOV2 collection. We collect the
auxiliary anchor text 𝐴𝑎𝑢𝑥 for the page 𝑃0 . 𝑃0 ’s original
anchor text (denoted as 𝐴𝑜𝑟𝑖𝑔 ), which comes from all pages
(denoted as 𝑃𝑂𝑟𝑖𝑔 (𝑃0 )) that are directly linked to 𝑃0 from
outside the site, consists of lines including “Optima National
Wildlife Refuge” and “Optima NWR”. 𝑃0 ’s 𝐴𝑎𝑢𝑥 consists of
lines including “Oklahoma Refuge Websites” and “Oklahoma
National Wildlife Refuges”.
Note that the above procedure does not use any anchor
text associated with internal inlinks, because internal inlinks
are typically generated by the owner of the site for navigational purposes and their associating anchor text tends to
be navigational in nature (e.g., “home”,“next page”, etc.; refer to [14] for more discussions on this issue). We emphasize
that in the remainder part of this paper we follow Metzler et
al. and do not use the anchor text associated with internal
inlinks in any way.
In this paper we are speciﬁcally interested in the eﬀectiveness of using 𝐴𝑎𝑢𝑥 to serve as a surrogate for possibly missing
original anchor text. In other words, we consider how eﬀectively we may use 𝐴𝑎𝑢𝑥 to discover plausibly missing original
anchor text of the URL of the interest so that anchor text
sparsity can be eﬀectively reduced. Therefore, we focus on
the discovered anchor terms themselves in the 𝐴𝑎𝑢𝑥 . We use
two typical methods to rank the relative importance of each
anchor term 𝑤. The ﬁrst method, denoted as AUX-TF, is
to use each term 𝑤’s term frequency 𝑡𝑓𝑎𝑢𝑥 (𝑤) in the 𝐴𝑎𝑢𝑥 .

2. RELATED WORK
Metzler et al. [14] ﬁrst directly addressed the anchor text
sparsity problem by using the web hyperlink graph and propagating anchor text over the web graph. Our work also addresses the same problem but using a diﬀerent approach,
which is based on the content similarity between web pages.
Our approach is similar in nature to other similarity based
techniques, such as cluster-based smoothing from the language modeling framework[8, 9, 11], except we focus on enriching web documents’ anchor text representation by using
their similar documents’ associated anchor text.
Anchor text can be modeled in many diﬀerent ways. Westerveld et al. [20] and Nallapati et al. [15] model anchor
text in the language modeling approach [17] and calculate
an associated anchor language model to update the original
document model for retrieval. Fujii [6] further considers differently weighting each line of anchor text associated with
the same page thus obtaining a more robust anchor language
model. Here, we also adopt the language modeling approach

428

http://saltplains.fws.gov/just4kid s.html

http://saltplains.fws.gov/index.html

auxiliary anchortext
(aggregated)

Oklahoma
Refuge
Websites
…

P5

Pages within
the same site

Oklahoma
National
Wildlife
Refuges

P6

P2

Similar Pages:

P3

P0

P8

Buffalo
Lake
NWR

P7

P1

Optima
National
Wildlife
Refuge

http://ifw2irm2.irm1.r2.fws.gov/texas.html

original anchortext

…..
Optima
NWR
…..

P9

P4

PIn (P0)={P1, P2 }
POrig (P0)={P8 , P 9}
PAux (P0 )={P5, P6 }
POrig (P4 )={P7 }

http://ifw2es.fws.gov/Oklahoma/refuges.htm l http://ifw2irm2.irm1.r2.fws.gov/toklahoma.html
P 0 : http://southwest.fws.gov/refuges/oklahoma/optima.html. P 1 :http://southwest.fws.gov/oklahoma.html .
P 2 : http://southwest.fws.gov/refuges/okrefuges.html .
P 4 : http://southwest.fws.gov/refuges/texas/buffalo.html .

Figure 1: Illustration of how to aggregate anchor text over the web graph or use similar web pages’ anchor
text for discovering more anchor text for a web page (𝑃0 in this example). The page 𝑃0 is a GOV2 web page,
whose DocID is GX010-01-9459902 and URL is http://southwest.fws.gov/refuges/oklahoma/optima.html.
The second method, denoted as AUX-TFIDF, is to use
each term 𝑤’s 𝑡𝑓𝑎𝑢𝑥 ⋅ 𝑖𝑑𝑓 (𝑤) score, computed by multiplying
𝑡𝑓𝑎𝑢𝑥 (𝑤) with 𝑤’s 𝑖𝑑𝑓 score in the web collection. The quality of the discovered anchor term rank lists produced from
these two link based approaches implies the eﬀectiveness of
using auxiliary anchor text as a surrogate of missing original
anchor text. We will compare these two approaches with our
content based approach in §4.

where 𝑤 is a word from the vocabulary 𝒱 of 𝒞. Similarly,
given an target page 𝑃0 , our approach aims to calculate a
relevant anchor text language model (RALM) 𝑝(𝑤∣𝐴0 ) by:
𝑝(𝑤∣𝐴0 ) =

Note that in the link based approach, a web page 𝑃0 still
cannot obtain the auxiliary anchor text if it has no internal
inlinks or if all pages in its 𝑃𝐼𝑛 (𝑃0 ) have no external anchor
text. Indeed, Metzler et al. reported only 38% anchor text
sparsity reduction on a web sample with the link based approach[14]. Therefore, we propose a content based approach,
which does not have speciﬁc link structure requirements on
the target web page, to discover its plausible missing anchor text. Intuitively, our approach assumes that web pages
that are similar in content may be described by similar associated anchor text. For example, in Figure 1, the target
page 𝑃0 , which is about Optima national wildlife refuge, is
similar in content with the page 𝑃4 , which is about Buﬀalo
Lake national wildlife refuge. We observe that the anchor
term “NWR”, which appears in 𝑃0 ’s and 𝑃4 ’s 𝐴𝑜𝑟𝑖𝑔 but not
in 𝑃0 ’s 𝐴𝑎𝑢𝑥 , can be used to partially describe both 𝑃0 and
𝑃4 although two pages are concerned about diﬀerent places.
We consider a language modeling approach to better use
document similarity and anchor text information, based on
ideas from the relevance-based language models (RM)[10].
In brief, given a query 𝑞, RM ﬁrst calculates the posterior
𝑝(𝐷𝑖 ∣𝑞) of each document 𝐷𝑖 in the collection 𝒞 generating
the query 𝑞, then calculates a query dependent language
model 𝑝(𝑤∣𝑞):
∑

𝑝(𝑤∣𝐷𝑖 ) × 𝑝(𝐷𝑖 ∣𝑞),

𝑝(𝑤∣𝐴𝑖 ) × 𝑝(𝐴𝑖 ∣𝐴0 ),

(2)

where 𝐴𝑖 denotes the complete original anchor text that
should be associated with 𝑃𝑖 but may be missing, 𝒜 denotes the complete original anchor text space for all pages,
𝑝(𝑤∣𝐴𝑖 ) is a multinomial distribution over the anchor text
vocabulary 𝒱𝒜 . To compute 𝑝(𝐴𝑖 ∣𝐴0 ) in Equation 2 where
𝐴0 and 𝐴𝑖 information may be missing, we view each page
𝑃𝑖 ’s content as its anchor text 𝐴𝑖 ’s context and use 𝑃𝑖 ’s document language model 𝑝𝑖 = {𝑝(𝑤∣𝑃𝑖 )} as 𝐴𝑖 ’s contextual
model. Then we can calculate a translation model 𝑡(𝐴𝑖 ∣𝐴0 )
by using 𝐴0 and 𝐴𝑖 ’s contextual models and use 𝑡(𝐴𝑖 ∣𝐴0 )
to approximate 𝑝(𝐴𝑖 ∣𝐴0 ). This contextual translation approach is also used in Wang and Zhai’s work [19].
When calculating a page 𝑃𝑖 ’s document language model
{𝑝(𝑤∣𝑃𝑖 )}, we employ Dirichlet smoothing on the maximum
likelihood (ML) estimate of observing a word 𝑤 in the page
(𝑝𝑀 𝐿 (𝑤∣𝑃𝑖 )) with the word’s collection probability 𝑝(𝑤∣𝒞):

3.2 Discovering Anchor Text through Finding
Similar Web Pages

𝑝(𝑤∣𝑞) =

∑
𝐴𝑖 ∈𝒜

𝑝(𝑤∣𝑃𝑖 ) =

𝑁𝑃𝑖
𝜇
𝑝𝑀 𝐿 (𝑤∣𝑃𝑖 ) +
𝑝(𝑤∣𝒞),
𝑁𝑃𝑖 + 𝜇
𝑁𝑃𝑖 + 𝜇

(3)

where 𝑁𝑃𝑖 is the length of 𝑃𝑖 ’s content and 𝜇 is the Dirichlet
smoothing parameter (𝜇 = 2500 in our experiments). Then
given two pages 𝑃0 and 𝑃𝑖 , we use the Kullback-Leibler
divergence (KL) 𝐷𝑖𝑣(⋅∣∣⋅) between their document models
𝑝0 and 𝑝𝑖 to measure their similarity and view that as the
contextual similarity between the associated anchor text 𝐴0
and 𝐴𝑖 . Then the contextually based translation probability
𝑡(𝐴𝑖 ∣𝐴0 ) is calculated by:

(1)

𝑡(𝐴𝑖 ∣𝐴0 ) =

𝐷𝑖 ∈𝒞

429

∑exp(−𝐷𝑖𝑣(𝑝0 ∣∣𝑝𝑖 )) .
𝑖 exp(−𝐷𝑖𝑣(𝑝0 ∣∣𝑝𝑖 ))

(4)

This 𝑡(𝐴𝑖 ∣𝐴0 ) is then used to approximate 𝑝(𝐴𝑖 ∣𝐴0 ) in Equation 2 to get:
𝑝(𝑤∣𝐴0 ) ≈

∑

𝑝(𝑤∣𝐴𝑖 ) × 𝑡(𝐴𝑖 ∣𝐴0 ).

4.

(5)

We now compare the capability of discovering missing anchor text by diﬀerent approaches described in §3, including
two link based approaches (AUX-TF and AUX-TFIDF), our
content based approach (RALM), and three keyword based
approaches (DOC-TF, DOC-TFIDF and DOC-OKAPI).

(6)

4.1

𝐴𝑖 ∈𝒜

A few transformations of Equation 4 can obtain:
∏
𝑡(𝐴𝑖 ∣𝐴0 ) ∝ 𝑝(𝑤∣𝑃𝑖 )𝑝(𝑤∣𝑃0 ) ,
𝑤

3.3 Using Keywords as Anchor Text
The keyword based approaches come from the intuition
that important keywords in a web page may be good description terms for the page, thus may be arguably used as
anchor text. We use three typical term weighting schemes
to identify the keywords and rank the words in a web page’s
content. The ﬁrst method, denoted as DOC-TF, uses each
word 𝑤’s term frequency 𝑡𝑓𝑃0 (𝑤) in the page 𝑃0 for term
weighting. The second method, denoted as DOC-TFIDF,
uses each word 𝑤’s 𝑡𝑓𝑃0 ⋅ 𝑖𝑑𝑓 (𝑤) score, computed by multiplying 𝑡𝑓𝑃0 (𝑤) with 𝑤’s 𝑖𝑑𝑓 score in the web collection. The
third method, denoted as DOC-OKAPI, uses each word
𝑤’s Okapi BM25 score 𝐵𝑀 25𝑃0 (𝑤), computed by:
𝑡𝑓𝑃0 (𝑤)⋅(𝑘1 +1)

∣𝑃 ∣

0
𝑡𝑓𝑃0 (𝑤)+𝑘1 ⋅(1−𝑏+𝑏⋅ 𝑎𝑣𝑔𝑑𝑙

⋅ 𝑖𝑑𝑓 (𝑤),

Data and Methodology

We use two publicly available large TREC web collections (GOV2 and ClueWeb09-T09B). GOV2 is a standard TREC web collection [4] crawled from government web
sites during early 2004. The ClueWeb09 collection is a much
larger and more recent web crawl, which contains over 1 billion pages. ClueWeb09-T09B is a subset of ClueWeb09 and
contains about 50 million English web pages. Compared
with GOV2 crawled only from the gov domain, ClueWeb09T09B is crawled from the general web thus is a less biased web sample; in another aspect, GOV2 contains relatively high quality government web pages thus having less
noise than ClueWeb09-T09B. Thus we use both GOV2 and
ClueWeb09-T09B in our experiments to show how diﬀerent
approaches perform in web collections that have diﬀerent
characteristics. The Indri Search Engine4 was used to index both collections by removing a standard list of 418 INQUERY [2] stopwords and applying Krovetz stemmer. In a
separate process, we run Indri Search Engine’s harvestlinks
utility on the two collections to collect web page inlinks and
raw anchor text information where we do not perform stopping or stemming.
To evaluate the quality of discovered anchor text for a web
page 𝑃0 , we utilize the original anchor text 𝐴𝑜𝑟𝑖𝑔 associated
with all inlinks of 𝑃0 . Speciﬁcally, we ﬁrst hide the page
𝑃0 ’s 𝐴𝑜𝑟𝑖𝑔 , apply diﬀerent anchor text discovery approaches
on 𝑃0 , then compare the discovered anchor text with 𝑃0 ’s
𝐴𝑜𝑟𝑖𝑔 . This procedure can be run automatically so that
we can leverage large volumes of web pages to evaluate the
performance of diﬀerent approaches with no human labeling
eﬀort. More speciﬁcally, we consider each anchor term in
a page 𝑃0 ’s 𝐴𝑜𝑟𝑖𝑔 as a good description term, or a relevant
term, for 𝑃0 while terms not in 𝐴𝑜𝑟𝑖𝑔 as non-relevant ones; in
this way, we can generate term relevance judgments for 𝑃0 .
Then we employ each diﬀerent approach to discover a ranked
list of plausible missing anchor terms for 𝑃0 and then use
the relevant judgments to evaluate the ranked anchor term
list. Note that for fair comparison 𝑃0 ’s 𝐴𝑜𝑟𝑖𝑔 is not used in
Equation 2 for calculating RALM in our approach. In the
experiments, we perform slight stopping on the raw anchor
text by removing a short list of 39 stopwords, which includes
25 common stopwords[12, pp.26] and 14 additional anchor
terms5 that are either common navigational purposed words
or part of URLs – it is common that anchor text contains
some URL.
We calculate some typical TREC style evaluation measurements including Mean Average Precision (MAP), Mean
Reciprocal Rank(MRR), Precision at the number of relevant terms(R-Prec), Precision at 𝐾 (P@𝑘) and also normalized discounted cumulative gain (NDCG) [7]. In the
experiments, we are speciﬁcally interested in the quality of
top ranked discovered anchor terms; thus, we only use the

which is the likelihood of generating 𝐴0 ’s context 𝑃0 from
𝐴𝑖 ’s context 𝑃𝑖 ’s smoothed language model and being normalized by 𝐴0 ’s context length. This likelihood can be easily obtained by issuing 𝑃0 as a long query to any language
model based search engine. In addition, we use the observed
incomplete original anchor text language model 𝑝𝑜𝑏𝑠 (𝑤∣𝐴𝑖 )
associated with 𝑃𝑖 to approximate 𝑝(𝑤∣𝐴𝑖 ) in Equation 5,
and let 𝑝𝑜𝑏𝑠 (𝑤∣𝐴𝑖 ) = 0 if 𝑃𝑖 has no 𝐴𝑜𝑟𝑖𝑔 . In this way, the
RALM 𝑝(𝑤∣𝐴0 ) can be computed.
In practice, for eﬃciency the RALM of the target page
𝑃0 is computed from 𝑃0 ’s top-𝑘 most similar pages’ 𝐴𝑜𝑟𝑖𝑔
(original anchor text) because 𝑡(𝐴𝑖 ∣𝐴0 ) in Equation 4 is very
small for the other pages. Due to the anchor text sparsity,
we set 𝑘 = 2000 in our experiments. Because some of these
similar pages do not have associated 𝐴𝑜𝑟𝑖𝑔 , we use another
parameter 𝑚 to denote the number of most similar pages
whose associated original anchor text is not missing and
contributes information in the RALM, and we tune 𝑚 in
the experiments. Intuitively, increasing 𝑚 can increase the
number of anchor text samples to better estimate RALM
but may also introduce more noise when the sample size is
large.
The probability 𝑝(𝑤∣𝐴0 ) of an anchor term 𝑤 in the RALM
directly reﬂects the goodness of the term 𝑤 used as original
anchor text for the page 𝑃0 , thus we use the anchor terms
that have the largest probabilities 𝑝(𝑤∣𝐴0 ) in the RALM
to evaluate the eﬀectiveness of our content based approach.
Theoretically our approach can associate any web page with
some anchor term distribution information if there is some
anchor text in the corpus, thus it can further reduce the
anchor text sparsity.

𝐵𝑀 25(𝑤) =

EVALUATING DISCOVERY

(7)

where 𝑎𝑣𝑔𝑑𝑙 is the average document length of the pages in
the collection. We use the typical setting 𝑘1 = 2, 𝑏 = 0.75 in
Equation 7 in our experiments.
The top ranked terms in a page 𝑃0 by three methods are
used as the possible missing original anchor terms for 𝑃0 .
We will use three keyword based methods as baselines in §4.

4

http://www.lemurproject.org/indri/
The additional terms are: http, https, www, gov, com, org,
edu, net, html, htm, click, here, next, home.
5

430

DOC-TF
DOC-TFIDF
DOC-OKAPI
AUX-TF
AUX-TFIDF
RALM

MAP
0.3162
0.2936
0.2936
0.1969
0.1716
0.3183

NDCG
0.4585
0.4348
0.4348
0.2598
0.2423
0.4275

MRR
0.5441
0.5400
0.5400
0.3707
0.3442
0.5050

P@2
0.3833
0.3700
0.3700
0.2833
0.2433
0.3467

P@5
0.2800
0.2613
0.2613
0.1773
0.1720
0.2840

P@10
0.2060
0.1827
0.1827
0.1153
0.1140
0.1860

P@20
0.1333
0.1240
0.1240
0.0643
0.0647
0.1140

R-Prec
0.2716
0.2530
0.2530
0.1643
0.1428
0.3051

Discovered Rel.
400
372
372
193
194
342

Table 2: Performances on the GOV2 collection. There are 708 relevant anchor terms overall. Column 10
shows overall relevant anchor terms discovered by each diﬀerent approach. RALM performs statistically
signiﬁcantly better than AUX-TF and AUX-TFIDF by each measurement in columns 2–9 according to the
one-sided t-test (𝑝 < 0.005). There exists no statistically signiﬁcant diﬀerence between each pair of RALM,
DOC-TF, DOC-TFIDF and DOC-OKAPI by each measurement according to the one-sided t-test (𝑝 < 0.05).
DOC-TF
DOC-TFIDF
DOC-OKAPI
AUX-TF
AUX-TFIDF
RALM

MAP
0.3517
0.3107
0.3107
0.1840
0.1634
0.2612

NDCG
0.4891
0.4388
0.4388
0.2507
0.2347
0.3615

MRR
0.5588
0.5145
0.5145
0.3309
0.3116
0.4630

P@2
0.3467
0.3133
0.3133
0.2248
0.2047
0.2833

P@5
0.2373
0.2213
0.2213
0.1463
0.1383
0.1733

P@10
0.1360
0.1173
0.1173
0.0729
0.0676
0.0911

P@20
0.1090
0.0983
0.0983
0.0577
0.0560
0.0770

R-Prec
0.2990
0.2608
0.2608
0.1675
0.1402
0.2398

Discovered Rel.
327
295
295
172
167
231

Table 3: Performances on the ClueWeb09-T09B collection. There are 582 relevant anchor terms overall.
Column 10 shows overall relevant anchor terms discovered by each diﬀerent approach. DOC-TF performs
statistically signiﬁcantly better than both RALM and AUX-TF by each measurement in columns 2–9 according to the one-sided t-test (𝑝 < 0.05). RALM performs statistically signiﬁcantly better than AUX-TF and
AUX-TFIDF by each measurement in columns 2–9 according to the one-sided t-test (𝑝 < 0.05).
top-20 terms in the discovered term rank lists by diﬀerent
approaches to calculate the measurements.
Note that web pages that can be used in our evaluation procedure need to satisfy two requirements: (1) they
need to have some associated 𝐴𝑜𝑟𝑖𝑔 and (2) they can collect
some auxiliary anchor text from the web graph as described
in §3.1. Thus, for each of two collections, we randomly sample 150 pages satisfying the two requirements for training
and another 150 pages for testing. On both training sets,
RALM’s parameter 𝑚 = 15 described in §3.2 achieves the
highest MAPs.

𝑝𝑐𝑡(AUX-TF, DOC-TF)
𝑝𝑐𝑡(AUX-TF, RALM)
𝑝𝑐𝑡(RALM, DOC-TF)

GOV2
30.5%
47.6%
26.0%

ClueWeb09-T09B
26.0%
46.3%
22.3%

Table 4: The average percentage 𝑝𝑐𝑡(𝑋, 𝑌 ) of the
terms discovered by the 𝑋 approach appearing in
the ones discovered by the 𝑌 approach.
tion words from the web content. One plausible reason that
RALM performs relatively poorly on ClueWeb09-T09B is
that, compared with the high quality GOV2 pages, ClueWeb
pages are crawled from the general web, where the inlinks
and anchor text may be generated in a more noisy way (e.g.
spam), degrading RALM’s performance. To better understand the performance of diﬀerent approaches, in Table 5
and Table 6 we show the top-10 words of the anchor term
rank lists discovered by diﬀerent approaches for one evaluation web page in GOV2 and ClueWeb09-T09B, respectively.
Although using keyword information can discover some
good anchor terms, the content-generated anchor terms do
not help bridging the lexical gap between a web page and
varied queries that attempt to search the page. Indeed, human generated anchor text is highly useful for reducing the
word mismatch problem because the lexical gap between
anchor text and queries is relatively small[14]. Here, we do
some lexical gap analysis to show that our approach can also
discover anchor terms similar in nature to human-generated
ones but diﬀerent from content-generated ones.
For each web page 𝑖 in the testing set, we calculate the
percentage 𝑝𝑐𝑡𝑖 (𝑋, 𝑌 ) of the terms discovered by the 𝑋 approach also appearing in the ones discovered by the 𝑌 approach, then compute the average percent 𝑝𝑐𝑡(𝑋, 𝑌 ) with
all the pages. We use the outputs from the keyword based
DOC-TF, the link based AUX-TF, and the RALM in this
analysis. Table 4 shows three average percentages 𝑝𝑐𝑡(𝑋, 𝑌 )

4.2 Results and Analysis
The performance of discovering original anchor text by different approaches on the testing set of GOV2 and ClueWeb09-T09B are shown in Table 2 and Table 3, respectively.
The results show that our approach (RALM) can eﬀectively
discover missing original anchor terms. On both collections
RALM performs statistically signiﬁcantly better than two
link based approaches (AUX-TF and AUX-TFIDF). This
indicates that, for discovering a page’s missing anchor text,
the anchor text associated with the similar pages provides
more useful information than that associated with the linked
web neighbors. The numbers of discovered relevant anchor
terms by diﬀerent approaches, shown in the last column of
two tables, also indicate that only using auxiliary anchor
text misses more original anchor text information than our
content based approach.
Another observation is that RALM performs worse on
ClueWeb09-T09B and not statistically signiﬁcantly better
on GOV2 than the keyword based approaches. This indicates that words having high IR utility like 𝑡𝑓 or 𝑡𝑓 ⋅ 𝑖𝑑𝑓
scores are often good description terms for the page and
used by human being as the anchor text. Removing a long
list of stopwords from web page content has also helped the
keyword based approaches to eﬀectively select good descrip-

431

which we have speciﬁc interest in. We observe that AUXTF’s discovered terms have much higher average per query
overlap ratio with RALM’s than with DOC-TF’s. Moreover, RALM’s discovered anchor terms have small overlap
with DOC-TF’s.

QL
M-ORG
M-AUX
M-ORG-AUX
M-RALM
M-ORG-RALM

5. USING DISCOVERED ANCHOR TEXT
FOR WEB SEARCH

MRR
0.3132
0.3696
0.3187
0.3711
0.3388△
0.3975★△

%Top10
49.7
57.5
50.8
57.5
53.6
59.7

Opt. Param.
𝛼 = 0.95
𝛼 = 0.99
𝛼 = 0.95, 𝛽 = 0.99
𝑚 = 20, 𝛼 = 0.95
𝛼, 𝛽 = 0.95, 𝑚 = 20

We now describe how we use the discovered anchor text
by diﬀerent approaches for retrieval in a language modeling approach [17]. We point out that our focus here is not
to evaluate diﬀerent schemes to aggregate or combine anchor text [14]; instead, we focus on comparing the utility of
RALM and auxiliary anchor text for helping retrieval.

Table 7: Retrieval performance of diﬀerent approaches with TREC 2006 NP queries. The star
indicates statistically signiﬁcant improvement over
MRRs of M-ORG and M-ORG-AUX by one-sided
t-test (𝑝 < 0.05). The triangle indicates statistically
signiﬁcant improvement over MRRs of QL and MAUX by one-sided t-test (𝑝 < 0.05).

We follow the typical language modeling based retrieval
approach[17] and score each web page 𝑃 for a query 𝑄 by the
likelihood of the page 𝑃 ’s document language model 𝑝(𝑤∣𝑃 )
generating the query 𝑄:

4. M-RALM, which only uses the RALM 𝑝(𝑤∣𝐴0 ) in Equation 2. The original anchor text of 𝑃0 is not used in Equation
2 for calculating RALM.
5. M-ORG-RALM, which uses both 𝑝(𝑤∣𝐴𝑜𝑟𝑖𝑔 ) and the
RALM 𝑝(𝑤∣𝐴0 ) in Equation 2 by:

5.1 Retrieval Models

𝑝(𝑄∣𝑃 ) =

∏

𝑝(𝑤∣𝑃 ).

(8)

𝑤∈𝑄

𝑝˜(𝑤∣𝑃 ) = 𝛽(𝛼𝑝(𝑤∣𝑃 ) + (1 − 𝛼)𝑝(𝑤∣𝐴𝑜𝑟𝑖𝑔 ))
+(1 − 𝛽)𝑝(𝑤∣𝐴0 ).

When using Dirichlet smoothing, the document language
model 𝑝(𝑤∣𝑃 ) can be calculated by Equation 3 and then
used in Equation 8 for retrieval. We call this baseline QL.
We only ﬁx 𝜇 = 2500 in Equation 3 for the document models
used to calculate RALM, but tune the 𝜇 for QL to achieve
the best retrieval performance in our experiments in §5.2.
We follow the mixture model approach [15, 16] to use the
discovered anchor text information for helping retrieval. In
this approach, a web page 𝑃 ’s document language model is
assumed to be a mixture of multiple component distributions
where each component is associated with a prior probability,
or a mixture weight. Therefore, we can estimate a language
model 𝑝(𝑤∣𝐴) from anchor text discovered by each diﬀerent
approach for the page 𝑃 and use 𝑝(𝑤∣𝐴) as a component
of 𝑃 ’s document model thus obtaining a better document
language model 𝑝˜(𝑤∣𝑃 ):
𝑝˜(𝑤∣𝑃 ) = 𝛼𝑝(𝑤∣𝑃 ) + (1 − 𝛼)𝑝(𝑤∣𝐴),

The original anchor text of 𝑃0 is not used in Equation 2 for
calculating RALM.
Note that we can update each page’s document model
oﬄine, thus this computationally expensive procedure has
little impact on the online query processing time. Moreover,
diﬀerent from experiments in §4.1, we use all anchor terms
instead of the top-20 most important terms discovered by
diﬀerent approaches.

5.2

(9)

where 𝑝(𝑤∣𝑃 ) is the original smoothed document model in
the QL baseline. Then we can plug 𝑝˜(𝑤∣𝑃 ) into equation
8 for retrieval. We compare the retrieval performance of
document language models updated by diﬀerent discovered
anchor text information.
We consider three diﬀerent anchor text sources to update
a web page 𝑃 ’s document model: (1) the observed original anchor text 𝐴𝑜𝑟𝑖𝑔 associated with 𝑃 , (2) the auxiliary
anchor text 𝐴𝑎𝑢𝑥 of 𝑃 , and (3) the RALM computed by
our approach for 𝑃 . We estimate the anchor text language
model 𝑝(𝑤∣𝐴𝑜𝑟𝑖𝑔 ) and 𝑝(𝑤∣𝐴𝑎𝑢𝑥 ) by using the ML estimate
of observing each word 𝑤 in 𝐴𝑜𝑟𝑖𝑔 and 𝐴𝑎𝑢𝑥 , respectively.
Here, we design the following ﬁve retrieval methods that use
the above three anchor text sources:
1. M-ORG, which only uses the observed original anchor
text language 𝑝(𝑤∣𝐴𝑜𝑟𝑖𝑔 ).
2. M-AUX, which only uses the auxiliary anchor text language 𝑝(𝑤∣𝐴𝑎𝑢𝑥 ).
3. M-ORG-AUX, which uses both 𝑝(𝑤∣𝐴𝑜𝑟𝑖𝑔 ) and 𝑝(𝑤∣𝐴𝑎𝑢𝑥 )
to update the document model 𝑝(𝑤∣𝑃 ) by:
𝑝˜(𝑤∣𝑃 ) = 𝛽(𝛼𝑝(𝑤∣𝑃 ) + (1 − 𝛼)𝑝(𝑤∣𝐴𝑜𝑟𝑖𝑔 ))
+(1 − 𝛽)𝑝(𝑤∣𝐴𝑎𝑢𝑥 ).

(11)

(10)

432

Experiments

We use the TREC web named page ﬁnding tasks in Terabyte Track[4, 5] to evaluate the performance of diﬀerent
retrieval methods described in §5.1. The objective of the
named page (NP) ﬁnding task is to ﬁnd a particular page in
the GOV2 collection, given a topic that describes it. We use
the NP topics and their relevance judgments for our experiments. In this experiment, we used Porter stemmer and did
not remove stopwords when indexing the GOV2 collection.
For each NP query, we ﬁrst run it against the GOV2
collection to obtain the QL baseline; then we use ﬁve retrieval methods described in §5.1 to rerank the top-100 web
pages returned by QL. The reranked lists are evaluated by
two TREC measurements previously used for the task [5]:
MRR which is the mean reciprocal rank of the ﬁrst correct
answer and the %Top10 which is the proportion of queries
for which a correct answer was found in the ﬁrst 10 search
results. We use the TREC 2005 NP topics (NP601-872)
for training and the TREC 2006 NP topics (NP901-1081)
for testing. We ﬁrst tune the Dirichlet parameter 𝜇 = 500
for QL to achieve the highest MRR on the training set and
obtain QL’s top-100 web pages for reranking. We then ﬁx
𝜇 = 500 to calculate the smoothed document model component 𝑝(𝑤∣𝑃 ) in the ﬁve retrieval methods but tune the
mixture parameters 𝛼 and 𝛽 for them to achieve the highest
MRRs with the training queries. For the two approaches
that use RALM, the parameter 𝑚 of RALM is also tuned.
After that, we run diﬀerent methods on the testing set.
Table 7 shows the retrieval performance of diﬀerent methods and the tuned parameters in each method. We observe:
(1) M-ORG-RALM performs statistically signiﬁcantly bet-

“Optima National Wildlife Refuge”, “Optima NWR”, “Washita Optima National Wildlife Refuge near Butler OK”
DOC-TF
𝑡𝑓𝑃0 (𝑤)
DOC-TFIDF 𝑡𝑓𝑃0 𝑖𝑑𝑓 (𝑤) DOC-OKAPI 𝐵𝑀 25𝑃0 (𝑤) AUX-TF 𝑡𝑓𝑎𝑢𝑥 (𝑤)
refuge
15
refuge
79.69
refuge
153.76
oklahoma
6
wildlife
10
optima
74.30
optima
143.37
wildlife
2
oklahoma
10
hardesty
47.48
hardesty
91.63
refuge
2
optima
8
hawk
36.20
hawk
69.86
website
1
species
6
oklahoma
36.03
oklahoma
69.53
u
1
hawk
6
wildlife
31.98
wildlife
61.71
service
1
habitat
6
guymon
29.35
guymon
56.63
s
1
area
6
habitat
26.42
habitat
50.98
oﬃce
1
prairie
5
species
23.70
species
45.73
national
1
national
5
quail
21.74
quail
41.95
ﬁsh
1
AUX-TFIDF 𝑡𝑓𝑎𝑢𝑥 𝑖𝑑𝑓 (𝑤)
RALM
𝑃 (𝑤∣𝐴0 )
Rel.
oklahoma
21.62
nwr
0.1164
butler
refuge
10.62
wildlife
0.0834
national
wildlife
6.40
refuge
0.0834
near
ﬁsh
3.11
national
0.0834
nwr
u
3.03
general
0.0657
optima
website
2.36
brochure
0.0657
refuge
oﬃce
1.54
kansas
0.0601
washita
s
1.29
lake
0.0522
wildlife
national
1.22
tear
0.0308
service
1.09
sheet
0.0308
Table 5: Discovered missing anchor terms and their term weights by applying diﬀerent approaches on one
GOV2 web page (TREC DocID in GOV2: GX010-01-9459902) . The ﬁrst row shows the original three pieces
of anchor text associated with the page. The Rel column in bold font shows the term relevance judgments
extracted from the ﬁrst row. RALM can discover some term like “NWR”, which may not appear in both the
page and the auxiliary anchor text, thus may help to bridge the lexical gap between pages and web queries
as using the original anchor text does.

“Weight Loss Resolutions”, “Weight Loss New Year’s Resolution to Lose Weight”,“Resolve to Lose Weight”
DOC-TF
𝑡𝑓𝑃0 (𝑤)
DOC-TFIDF 𝑡𝑓𝑃0 𝑖𝑑𝑓 (𝑤) DOC-OKAPI 𝐵𝑀 25𝑃0 (𝑤) AUX-TF 𝑡𝑓𝑎𝑢𝑥 (𝑤)
weight
46
weight
96.38
weight
112.53
weight
709
loss
26
loss
78.65
loss
91.83
loss
705
lose
20
lose
64.47
lose
75.28
diet
32
new
17
resolution
46.57
resolution
54.38
weightloss
21
year
15
diet
34.27
diet
40.02
guide
20
resolution
13
goal
26.01
goal
30.37
scott
8
time
12
eat
25.61
eat
29.90
jennifer
8
make
10
year
23.90
year
27.90
contact
8
goal
9
calorie
15.73
calorie
18.36
site
6
diet
9
pound
15.34
pound
17.91
s
4
AUX-TFIDF 𝑡𝑓𝑎𝑢𝑥 𝑖𝑑𝑓 (𝑤)
RALM
𝑃 (𝑤∣𝐴0 )
Rel.
loss
2132.63
weight
0.2245
lose
weight
1485.49
loss
0.1737
loss
weightloss
157.70
diet
0.0550
new
diet
121.86
easy
0.0436
resolution
guide
37.26
lose
0.0422
resolve
jennifer
33.96
way
0.0412
s
scott
28.52
myth
0.0396
weight
guidesite
22.04
warn
0.0232
em
13.15
ppa
0.0232
mlibrary
11.37
fda
0.0232
Table 6: Discovered missing anchor terms and their term weights by applying diﬀerent approaches on one
ClueWeb09 web page (ClueWeb09 RecordID: clueweb09-en0004-60-01628). The ﬁrst row shows the original
three pieces of anchor text associated with the page. The Rel column in bold font shows the term relevance
judgments extracted from the ﬁrst row. The keyword approaches discovered “new year resolution”, which
may be hard to be discovered by using the page’s web-graph neighbor pages’ anchor text or using the page’s
similar pages’ anchor text.

433

ter than M-ORG. This indicates that missing anchor text
discovered by RALM provides additional information not in
the original anchor text so that combining them can further
improve the retrieval performance. (2) M-ORG-RALM and
M-RALM performs statistically signiﬁcantly better than MORG-AUX and M-AUX, respectively. This indicates that
in GOV2 missing anchor text information discovered by our
content based approach helps retrieval more eﬀectively than
the auxiliary anchor text.6
In Table 7, we observe that the auxiliary anchor text helps
the performance very little in this task. There are two plausible reasons: ﬁrst, TREC NP queries are short queries and
Metzler et al. observed that auxiliary anchor text does not
help or even hurts the performance of short navigational web
queries[14]; second, the anchor text sparsity problem is serious on the GOV2, thus very small percentage of pages can
collect some auxiliary anchor text as shown in Table 1 to
beneﬁt the search task. However, even when serious anchor
text sparsity exists and queries are short, our content based
approach still helps improving retrieval eﬀectiveness.
We expect our technique can enhance the retrieval performance of general web search engines where there are large
portion of short navigational queries. As is well known, in
the general web search environment there are many lowquality web pages and spam; thus, we need to address issues
about web page quality and noise ﬁltering for better beneﬁtting general web search. We leave this as future work.

There are several interesting directions of future work.
Metzler et al. found that auxiliary anchor text can eﬀectively help longer, informational queries [14]; we will explore
how well RALM can help long informational queries. We
also want to explore using RALM’s discovered missing anchor text information beyond the language modeling based
retrieval framework, e.g. using it to extract useful features
for learning-to-rank retrieval approaches [3].

7.

ACKNOWLEDGMENTS

8.

REFERENCES

This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF IIS-0910884.
Any opinions, ﬁndings and conclusions or recommendations
expressed in this material are the authors’ and do not necessarily reﬂect those of the sponsor.
[1] A. Broder et al. Graph structure in the web. Comput.
Netw., 33(1-6):309–320, 2000.
[2] J. Broglio, J. P. Callan, and W. B. Croft. An overview of
the INQUERY system as used for the TIPSTER project.
Technical report, Amherst, MA, USA, 1993.
[3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds,
N. Hamilton, and G. Hullender. Learning to rank using
gradient descent. In Proc. of ICML, pp. 89–96, 2005.
[4] S. Büttcher, C. L. A. Clarke, and I. Soboroﬀ. The TREC
2006 Terabyte Track. In TREC, 2006.
[5] C. L. A. Clarke, F. Scholer, and I. Soboroﬀ. The TREC
2005 Terabyte Track. In TREC, 2005.
[6] A. Fujii. Modeling anchor text and classifying queries to
enhance web document retrieval. In Proc. of WWW, pp.
337–346, 2008.
[7] K. Järvelin and J. Kekäläinen. Cumulated gain-based
evaluation of IR techniques. ACM Trans. Inf. Syst.,
20(4):422–446, 2002.
[8] O. Kurland and L. Lee. Corpus structure, language models,
and ad hoc information retrieval. In SIGIR, pp. 194–201,
2004.
[9] O. Kurland and L. Lee. Respect my authority!: Hits
without hyperlinks, utilizing cluster-based language
models. In SIGIR, pp. 83–90, 2006.
[10] V. Lavrenko and W. B. Croft. Relevance based language
models. In SIGIR, pp. 120–127, 2001.
[11] X. Liu and W. B. Croft. Cluster-based retrieval using
language models. In SIGIR, pp. 186–193, 2004.
[12] C. D. Manning, P. Raghavan, and H. Schütze. Introduction
to Information Retrieval. Cambridge Univ. Press. 2008.
[13] Q. Mei, D. Zhang, and C. Zhai. A general optimization
framework for smoothing language models on graph
structures. In SIGIR, pp. 611–618, 2008.
[14] D. Metzler, J. Novak, H. Cui, and S. Reddy. Building
enriched document representations using aggregated anchor
text. In SIGIR, pp. 219–226, 2009.
[15] R. Nallapati, B. Croft, and J. Allan. Relevant query
feedback in statistical language modeling. In Proc. of
CIKM, pp. 560–563, 2003.
[16] P. Ogilvie and J. Callan. Combining document
representations for known-item search. In SIGIR, pp.
143–150, 2003.
[17] J. M. Ponte and W. B. Croft. A language modeling
approach to information retrieval. In SIGIR, pp. 275–281,
1998.
[18] T. Tao, X. Wang, Q. Mei, and C. Zhai. Language model
information retrieval with document expansion. In Proc. of
NAACL-HLT, pp. 407–414, 2006.
[19] X. Wang and C. Zhai. Mining term association patterns
from search logs for eﬀective query reformulation. In Proc.
of CIKM, pp. 479–488, 2008.
[20] T. Westerveld, W. Kraaij, and D. Hiemstra. Retrieving
web pages using content, links, urls and anchors. In Proc.
of TREC, pp. 663–672, 2001.

6. CONCLUSIONS AND FUTURE WORK

In this paper, we proposed a language modeling based
technique to overcome the anchor text sparsity problem by
using web content similarity. Our approach computes a
relevant anchor text language model, called RALM, from
its similar web pages’ associated anchor text to discover its
plausible missing anchor text. Compared with a link based
approach [14], our content based approach has no speciﬁc
link structure requirements on the web page of interest and
thus can further reduce anchor text sparsity.
We designed experiments with two TREC web corpora
to evaluate the eﬀectiveness of discovering missing anchor
terms by three diﬀerent approaches: the link based approach,
the RALM approach, and the keyword based approach. Experimental results show that the RALM approach can effectively discover missing original anchor text and performs
statistically signiﬁcantly better than the two link based approaches on both collections. Moreover, RALM’s discovered
anchor text is similar in nature to auxiliary anchor text while
diﬀerent from the keywords in the web page.
By using the mixture model[15, 16], we used diﬀerent discovered anchor text information within the language modeling framework for retrieval. We evaluated using diﬀerent approaches for improving retrieval eﬀectiveness with the
TREC named page ﬁnding task. The results show that (1)
RALM helps retrieval more than using the auxiliary anchor
text collected over the web graph and (2) combining RALM
and the original anchor text can statistically signiﬁcantly improve the retrieval performance of only using the original anchor text. Furthermore, RALM can help improving retrieval
eﬀectiveness for short navigational queries even when serious
anchor text sparsity exists. This makes RALM a promising
technique for improving general web search engines.
6

Our goal is not to compare ranking schemes, but to show the
utility of the discovered anchor text. However, we note that these
scores match or beat top-performing approaches [4].

434

