Learning to Rank Query Reformulations
Van Dang, Michael Bendersky and W. Bruce Croft
Center for Intelligent Information Retrieval
Department of Computer Science
University of Massachusetts
Amherst, MA 01003

{vdang, bemike, croft}@cs.umass.edu
ABSTRACT

(Similarity Collection Query) [8] and query clarity [1], we
can substantially improve the ranking of reformulated queries
in terms of the quality of the reformulations in the top two
ranks (measured by NDCG@2 ), which then leads to signiï¬cant improvements in retrieval eï¬€ectiveness.

Query reformulation techniques based on query logs have recently proven to be eï¬€ective for web queries. However, when
initial queries have reasonably good quality, these techniques
are often not reliable enough to identify the helpful reformulations among the suggested queries. In this paper, we show
that we can use as few as two features to rerank a list of reformulated queries, or expanded queries to be speciï¬c, generated by a log-based query reformulation technique. Our
results across ï¬ve TREC collections suggest that there are
consistently more useful reformulations in the ï¬rst ğ‘¡ğ‘¤ğ‘œ positions in the new ranked list than there were initially, which
leads to statistically signiï¬cant improvements in retrieval effectiveness.

2.
2.1

Log-based Query Expansion

The log-based query expansion method [2] (referred to
as LQE) is a slight modiï¬cation of the query substitution
method proposed by Wang and Zhai [6]. It ï¬rst estimates
a context distribution for terms occuring in a query log. It
then constructs a translation model that can suggest similar
words based on their distributional similarity. Given any
query, the expansion model will try to expand it with candidates suggested by the translation model for each query
term. The model decides whether to expand the query based
on how similar the candidate is to the query term and how
appropriate it is to the context of the query. For more details, see [2].

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Query Formulation

General Terms
Algorithms, Measurement, Performance, Experimentation.

Keywords

2.2

Query reformulation, query expansion, query log, query performance predictor, learning to rank.

1.

METHOD

The Reranking Approach

Query quality predictors aim to predict a queryâ€™s quality
without explicit relevance judgements. Thus, given a ranked
list of reformulated queries, it is intuitive to think about
reorganizing this list based on the â€œqualityâ€ score given by
some predictor.
We tried some of the top-performing predictors that Kumaran and Carvalho [4] used in a similar task and found that
ğ‘†ğ¶ğ‘„ [8] and clarity score [1] are the most eï¬€ective for our
problem. Therefore, we rerank the list of expanded queries
by

INTRODUCTION

Query logs have become an important resource for many
tasks including query reformulation [3, 6]. Most log-based
reformulation techniques, however, are evaluated using nonstandard approaches and proprietary query logs, making it
hard to compare one to another. A more recent study [2]
compares diï¬€erent techniques using TREC collections and
ï¬nds that when intial queries have relatively high quality,
query expansion is much more reliable than substitution.
Although the log-based expansion technique [2] can generate some good reformulations for high-quality TREC queries,
it also produces many bad reformulations and it does not
generate a reliable ranking of the reformulations by quality.
In this paper, we show that we can eï¬€ectively rerank the
list of reformulated queries obtained with this log-based expansion approach. By using as few as two features, SCQ

ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’(ğ‘) = ğœ†1 Ã— ğ‘†ğ¶ğ‘„(ğ‘) + ğœ†2 Ã— ğ‘ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦(ğ‘)
where ğœ†1 and ğœ†2 are weight of the two predictors.

Table 1: Statistics of queries used for reformulation
AP WSJ Robust-04 WT10G Gov-2
Title Q.
133
133
200
66
119
Desc. Q. 150
150
246
94
134

Copyright is held by the author/owner(s).
SIGIRâ€™10, July 19â€“23, 2010, Geneva, Switzerland.
ACM 978-1-60558-896-4/10/07.

807

Table 3: Evaluation of retrieval eï¬€ectiveness in terms of MAP. âˆ— and â€  indicate signiï¬cant diï¬€erence to the
original query and LQEâ€™s ranked list respectively. Best result in each column is marked in bold.
Orig-Q
LQE
Rerank

AP
0.1694
0.1741
0.1749âˆ—

WSJ
0.2594
0.2563
0.2663âˆ—â€ 

Title Query
RBT-04
WT10G
0.2247
0.1904
0.2297
0.1911
0.2382âˆ—â€  0.1962âˆ—

Gov-2
0.2829
0.2559âˆ—
0.2901âˆ—â€ 

AP
WSJ
Robust-04
WT10G
Gov-2

3.
3.1

Title Query
LQE
Rerank
0.2434 0.4805
0.2318 0.5040
0.2905 0.5559
0.2673 0.5499
0.1933 0.5830

3.4

Desc. Query
LQE
Rerank
0.2307 0.3728
0.2250 0.3296
0.2138 0.3687
0.1680 0.3847
0.2059 0.4093

EVALUATION
Experiment Settings

4.

Retrieval Effectiveness

CONCLUSIONS

In this paper, we have shown that by reranking the list of
reformulations generated by the log-based query expansion
technique [2] with only two features, we can push more good
reformulations into the ï¬rst two positions in the list. This is
reï¬‚ected in the huge gain of NDCG@2 and statistically signiï¬cant improvement in retrieval eï¬€ectiveness. In the future,
we will investigate more features. We hope this will lead to
greater improvement in NDCG@1, helping retrieval systems
to reformulate queries implicitly without user involvement.

5.

ACKNOWLEDGMENTS

This work was supported in part by the Center for Intelligent Information Retrieval, in part by NSF grant #IIS0711348, and in part by ARRA NSF IIS-9014442. Any opinions, ï¬ndings and conclusions or recommendations expressed
in this material are the authorsâ€™ and do not necessarily reï¬‚ect those of the sponsor.

Training Data

We run LQE with the MSN log to obtain a list of reformulations for each original query. We use all these queries
to do retrieval and record their MAP and use them to create our dataset. Training and testing are done using 5-fold
cross validation on this dataset. ğœ†1 and ğœ†2 are learned using AdaRank [7] to maximize the average NDCG@2. The
algorithm ends up choosing either (ğœ†1 = 1, ğœ†2 = 0) or
(ğœ†1 = 0, ğœ†2 = 1) depending on the collection.

3.3

Gov-2
0.2518
0.2497
0.2579âˆ—â€ 

We deï¬ne the MAP of a ranked list of reformulations as
the best MAP observed among its top ğ‘¡ğ‘¤ğ‘œ queries. In this
section, we compare the MAP obtained by (i) the original
query, (ii) the list of reformulations generated by LQE, and
(iii) the list reranked by our method.
As can be seen in Table 3, the best of the top two reformulated queries ranked by our approach is almost always
signiï¬cantly better than the original query. This is not the
case in LQE. In many cases, our method also provides significant improvements over LQE. This result suggests that the
reranking can push better reformulations to the ï¬rst two
positions in the ranked list.

In this section, we evaluate the performance of our reranking technique. Evaluation is done on ï¬ve TREC collections:
AP, WSJ, Robust-04, WT10G and Gov-2, with both title
and description queries. We use the language modeling
framework and remove all stop words at indexing time. We
adopt the parameter settings for LQE from the authors [2].
Due to the limited coverage of the available query log [5],
we use only a subset of TREC queries where the LQE can
generate at least one reformulation. Information about these
subsets is given in Table 1.
On each collection, we ï¬rst use LQE to generate a list of
ğ¾ expanded queries (ğ¾ = 30) for each original query. We
append to this list the original query - in the case when all
generated reformulations are bad, the reranking approach
has a chance to choose not to reformulate. We then use our
approach to rerank this list and compare its performance
with that of the intial list as well as original query.

3.2

Description Query
WSJ
RBT-04
WT10G
0.2358
0.2519
0.1770
0.2391
0.2538
0.1775
0.2374
0.2584âˆ—â€  0.1836âˆ—

than the initial list. All improvements are statistically signiï¬cant at ğ‘ < 0.05 using a two-tailed t-test.

Table 2: Our approach (â€œRerankâ€) consistently outperforms LQE in NDCG@2. All diï¬€erences are significant at ğ‘ < 0.05
Collection

AP
0.1660
0.1694âˆ—
0.1820âˆ—â€ 

6.

REFERENCES

[1] S. Cronen-Townsend, Y. Zhou, and W.B. Croft. Predicting
Query Performance. In Proc. of SIGIR, pages 299-306, 2002.
[2] V. Dang and W.B. Croft. Query Reformulation Using Anchor
Text. In Proc. of WSDM, pages 41-50, 2010.
[3] R. Jones, B. Rey and O. Madani. Generating Query
Substitutions. In Proc. of WWW, pages 387-396, 2006.
[4] G. Kumaran and V.R. Carvalho. Reducing Long Queries Using
Query Quality Predictors. In Proc. of SIGIR, pages 564-571,
2009.
[5] Proc. of the 2009 workshop on Web Search Click Data,
Barcelona, Spain. ACM New York, NY, USA, 2009.
[6] X. Wang and C. Zhai. Mining Term Association Patterns from
Search Logs for Eï¬€ective Query Reformulation. In Proc. of
CIKM, pages 479-488, 2008.
[7] J. Xu and H. Li. AdaRank: A Boosting Algorithm for
Information Retrieval. In Proc. of SIGIR, pages 391-398, 2007.
[8] Y. Zhao, F. Scholer, and Y. Tsegay. Eï¬€ective Pre-retrieval
Query Performance Prediction Using Similarity and Variability
Evidence. In Proc. of ECIR, pages 52-64, 2008.

Reranking Effectiveness

We use NDCG@2 to measure the quality of the ranked
list of reformulations given by our approach. Reformulations
are graded on a scale from zero to four with respect to the
improvement ğ‘š they provide over the original query. In
particular, improvement larger than 0.03 corresponds to a
4, or (ğ‘š > 0.03) â†’ 4. Similarly, (0.01 < ğ‘š â‰¤ 0.03) â†’ 3,
(0 < ğ‘š â‰¤ 0.01) â†’ 2, (ğ‘š = 0) â†’ 1 and (ğ‘š < 0) â†’ 0.
Table 2 summarizes the result: the list of reformulations
ranked by our approach has a much higher average NDCG@2

808

