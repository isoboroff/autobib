Probabilistic Latent Maximal Marginal Relevance
Shengbo Guo

Scott Sanner

ANU & NICTA
Canberra, Australia

NICTA & ANU
Canberra, Australia

shengbo.guo@nicta.com.au

scott.sanner@nicta.com.au

ABSTRACT

manner by selecting s∗j given Sj−1 = {s∗1 , . . . , s∗j−1 } (where
Sj = Sj−1 ∪ {s∗j }) according to the following criteria

Diversity has been heavily motivated in the information retrieval literature as an objective criterion for result sets in
search and recommender systems. Perhaps one of the most
well-known and most used algorithms for result set diversification is that of Maximal Marginal Relevance (MMR).
In this paper, we show that while MMR is somewhat adhoc and motivated from a purely pragmatic perspective, we
can derive a more principled variant via probabilistic inference in a latent variable graphical model. This novel derivation presents a formal probabilistic latent view of MMR
(PLMMR) that (a) removes the need to manually balance
relevance and diversity parameters, (b) shows that specific
definitions of relevance and diversity metrics appropriate to
MMR emerge naturally, and (c) formally derives variants
of latent semantic indexing (LSI) similarity metrics for use
in PLMMR. Empirically, PLMMR outperforms MMR with
standard term frequency based similarity and diversity metrics since PLMMR maximizes latent diversity in the results.

s∗j = arg max [λ(Sim1 (sj , q)) − (1 − λ) max Sim2 (sj , si )]
sj ∈D\Sj−1

si ∈Sj−1

(1)
where Sim1 (·, ·) measures the relevance between an item
and a query, Sim2 (·, ·) measures the similarity between two
items, and the manually tuned λ ∈ [0, 1] trades off relevance
and similarity. In the case of s∗1 , the second term disappears.
While MMR is a popular algorithm, it was specified in a
rather ad-hoc manner and good performance typically relies
on careful tuning of the λ parameter. Furthermore, MMR is
agnostic to the specific similarity metrics used, which indeed
allows for flexibility, but makes no indication as to the choice
of similarity metrics for Sim1 and Sim2 that are compatible
with each other and also appropriate for good performance.
In the next section, we address these concerns by taking a
more principled approach to set-based information retrieval
via maximum a posteriori probabilistic inference in a latent
variable graphical model of marginal relevance (PLMMR).
As an elegant and novel contribution, we note that natural
relevance and diversity metrics emerge from this derivation
(with no analogous manually tuned λ parameter) and that
these metrics also formally motivate variants of similarity
metrics used in latent semantic indexing (LSI) [3].

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval
Models

General Terms

2. PROBABILISTIC LATENT MMR

Algorithms

Keywords
diversity, graphical models, maximal marginal relevance

1.

INTRODUCTION

Maximal marginal relevance (MMR) [2] is perhaps one of
the most popular methods for balancing relevance and diversity in set-based information retrieval and has been cited
over 530 times1 since its publication in 1998.
The basic idea of MMR is straightforward: suppose we
have a set of items D and we want to recommend a small
subset Sk ⊂ D (where |Sk | = k and k ≪ |D|) relevant to
a given query q. MMR proposes to build Sk in a greedy
1

Figure 1: Graphical model used in PLMMR.
We begin our discussion of PLMMR by introducing a
graphical model of (marginal) relevance in Figure 1. Shaded
nodes represent observed variables while unshaded nodes are
latent; we do not distinguish between variables and their assignments. The observed variables are the vector of query
terms q and the selected items s1 ∈ D and s2 ∈ D. For
the latent variables, let T be a discrete topic set; variables
t1 ∈ T and t2 ∈ T respectively represent topics for s1 and

According to Google Scholar.

Copyright is held by the author/owner(s).
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
ACM 978-1-60558-896-4/10/07.

833

s2 and t′ ∈ T represents a topic for query q. r1 ∈ {0, 1} and
r2 ∈ {0, 1} are variables that indicate whether the respective
selected items s1 and s2 are relevant (1) or not (0).
The conditional probability tables (CPTs) in this discrete
directed graphical model are defined as follows. P (t1 |s1 ) and
P (t2 |s2 ) represent topic models of the items and P (t′ |q) represents a topic model of the query. There are a variety of
ways to learn these topic CPTs based on the nature of the
items and query; for an item set D consisting of text documents and a query that can be treated as a text document,
a natural probabilistic model for P (ti |si ) and P (t′ |q) can be
derived from Latent Dirichlet Allocation (LDA) [1]. Finally,
the CPTs for relevance ri have a very natural definition:

Method
MMR-TF
MMR-TFIDF
PLMMR-LDA

1
0

if t1 = t′
if t1 =
6 t′

P (r2 |t′ , r1 = 0, t1 , t2 ) =

(

1
0

if (t2 6= t1 ) ∧ (t2 = t′ )
if (t2 = t1 ) ∨ (t2 6= t′ )

We report experiments on a subset of TREC 6-8 data focusing on diversity. We follow the same experimental setup
as [6] who measure the weighted subtopic loss (WSL) of
recommended item sets where in brief, WSL gives higher
penalty for not covering popular subtopics. We do not compare directly to [6] as their method was supervised while
MMR and PLMMR are inherently unsupervised.
Standard query and item similarity metrics used in MMR
applied to text data include the cosine of the term frequency
(TF) and TF inverse document frequency (TFIDF) vector
space models [5]. We denote these variants of MMR as
MMR-TF and MMR-TFIDF. PLMMR specifically suggests
the use of LSI-based similarity metrics defined in the last
section; thus, we use LDA to derive these models, referring to the resulting algorithm as PLMMR-LDA. LDA was
trained with α = 2.0, β = 0.5, |T | = 15; we note the results
were not highly sensitive to these parameter choices.
Average WSL scores are shown in Table 1 on the 17
queries examined by [6]. We use both full documents and
also just the first 10 words of each document. For both
MMR algorithms, the best performing λ = 0.5 is shown.
We note that due to the power of the latent topic model and
derived similarity metrics, PLMMR-LDA is able to perform
better than MMR with standard TF and TFIDF metrics and
without a λ parameter to be tuned. In addition, PLMMRLDA works very well with short documents since intrinsic
document and query similarities are automatically derived
from the latent PLMMR relevance and diversity metrics.

s∗2 = arg max MR(S1 , s2 , q) = arg max P (r2 |s∗1 , s2 , q)
s2 ∈D\{s∗
1}

X

= arg max

s2 ∈D\{s∗
1 } t1 ,t2 ,t′

= arg max
s2 ∈D\{s∗
1}

X
t′

|

P (r2 |r1 = 0, t1 , t2 , t′ )P (t1 |s∗1 )
P (r1 = 0|t1 , t′ )P (t2 |s2 )P (t′ |q)
!

P (t′ |q)P (t2 = t′ |s2 )
{z

relevance

X

|

−

}

!

P (t′ |q)P (t1 = t′ |s∗1 )P (t2 = t′ |s2 )

t′

{z

diversity

(2)

}

The basic insight leading to this fascinating result is the
exploitation of the indicator structure of the relevance variables r1 and r2 to make convenient variable substitutions.
We note that in this special case for MR(S1 , s2 , q), a very
natural mapping to the MMR algorithm in (1) when λ = 0.5
has emerged automatically from the derivation that maximized MR. This derivation automatically balances relevance
and diversity without an analogous λ and it suggests very
specific (and different) relevance and diversity metrics, both
effectively variants of similarity metrics used in latent semantic indexing (LSI) [3]. To make this clear, we examine
the relevance metric SimPLMMR
given by PLMMR where we
1
let T′ and T2 be respective topic probability vectors for
query q and item s2 with vector elements T′ i = P (t′ = i|q)
and T2i = P (t2 = i|s2 ) and using h·, ·i for the inner product:
SimPLMMR
(q, s2 ) =
1

X

WSL (all words)
0.534
0.493
0.468 ± 0.0019

query-relevant! Given these definitions of SimPLMMR
and
1
SimPLMMR
, we can now substitute these into the MMR al2
gorithm defined in (1) to arrive at a definition of PLMMR.

Simply, s1 is relevant if its topic t1 = t (the query topic).
s2 is relevant with the same condition and the addition that
if s1 was irrelevant (r1 = 0), then topic t2 for s2 should also
not match t1 . Following the click-chain model [4], we assume
the user only examines s2 if s1 was irrelevant (r1 = 0).
Let us assume that like MMR we use a greedy item set selection algorithm and we have already selected s1 = s∗1 . Now
given S1 = {s∗1 }, we want to select s2 in order to maximize
its marginal relevance w.r.t. q given S1 , formally defined as
MR(S1 , s2 , q) and derived as a query in the graphical model:
s2 ∈D\S1

WSL (first 10 words)
0.555
0.549
0.458 ± 0.0058

3. EXPERIMENTAL COMPARISON

P (r1 |t , t1 ) =

(

′

Table 1: Weighted subtopic loss (WSL) of three
methods using all words and first 10 words. Standard error estimates are shown for PLMMR-LDA.

4. REFERENCES
[1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent
Dirichlet allocation. JMLR, 3:993–1022, 2003.
[2] J. Carbonell and J. Goldstein. The use of MMR,
diversity-based reranking for reordering documents and
producing summaries. In SIGIR, 335–336. 1998.
[3] S. Deerwester, S. T. Dumaisand, G. W. Furnas, T. K.
Landauer, and R. Harshman. Indexing by latent
semantic analysis. JASIS, 41:391–407, 1990.
[4] F. Guo, C. Liu, A. Kannan, T. Minka, M. Taylor,
Y.-M. Wang, and C. Faloutsos. Click chain model in
web search. In WWW-09, Madrid, Spain, 2009. ACM.
[5] G. Salton and M. McGill. Introduction to modern
information retrieval. McGraw-Hill, 1983.
[6] Y. Yue and T. Joachims. Predicting diverse subsets
using structural SVMs. In ICML, 1224–1231, 2008.

P (t′ |q)P (t2 = t′ |s2 ) = hT′ , T2 i.

t′

A similar analysis gives diversity metric SimPLMMR
(s1 , s2 ),
2
yielding a variant LSI similarity metric reweighted by the
query topic probability P (t′ |q). This points out the important correction to MMR that item set diversity should be

834

