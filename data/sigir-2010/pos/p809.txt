Many are Better Than One: Improving Multi-Document
Summarization via Weighted Consensus
Dingding Wang

Tao Li

School of Computer Science
Florida International University
Miami, FL 33199

{dwang003,taoli}@cs.fiu.edu
ABSTRACT

individual summarization methods, in which, the relative
contribution of an individual summarizer to the consensus
is determined by its agreement with other members of the
summarization systems. Note that usually a high degree
of agreements does not automatically imply the correctness
since the systems could agree on a faulty answer. However,
each of the summarization systems has shown its eﬀectiveness individually, so the agreement measure can be used in
the consensus summarization.

Given a collection of documents, various multi-document
summarization methods have been proposed to generate a
short summary. However, few studies have been reported
on aggregating diﬀerent summarization methods to possibly generate better summarization results. We propose a
weighted consensus summarization method to combine the
results from single summarization systems. Experimental
results on DUC2004 data sets demonstrate the performance
improvement by aggregating multiple summarization systems, and our proposed weighted consensus summarization
method outperforms other combination methods.

2. WEIGHTED CONSENSUS SUMMARIZATION (WCS)

Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval.

2.1 Notations
Suppose there are K single summarization methods, each
of which produces a ranking for the sentences containing
in the document collection. Then we have K ranking lists
{r1 , r2 , · · · , rK } and ri ∈ RN , i = 1, · · · , K, where N is the
total number of sentences in the documents. The task is to
ﬁnd a weighted consensus ranking of the sentences r∗ with
a set of weights {w1 , w2 , · · · , wK } assigning to each of the
individual summarization methods.

General Terms: Algorithms, Experimentation.
Keywords: Weighted consensus, summarization.

1.

INTRODUCTION

Various multi-document summarization methods base on
diﬀerent strategies and usually produce diverse outputs. A
natural question arises: can we perform ensemble or consensus summarization by combining diﬀerent summarization
methods to improve summarization performance? In general, the terms of “consensus methods” or “ensemble methods” are commonly reserved for the aggregation of a number
of diﬀerent (input) systems. Previous research has shown
that ensemble methods, by combining multiple input systems, are a popular way to overcome instability and increase
performance in many machine learning tasks, such as classiﬁcation, clustering and ranking. The success of ensemble
methods in other learning tasks provides the main motivation for applying ensemble methods in summarization. To
the best of our knowledge, so far there are only limited attempts on using ensemble methods in multi-document summarization.
As a good ensemble requires the diversity of the individual
members, here we study several widely used multi-document
summarization systems based on a variety of strategies, and
evaluate diﬀerent baseline combination methods for obtaining a consensus summarizer to improve the summarization
performance. Motivated from [5], we also propose a novel
weighted consensus scheme to aggregate the results from

2.2 Formulation
Our goal is to minimize the weighted distance between
r∗ and all the ri . Let w = [w1 , w2 , . . . , wK ]T ∈ RK . The
problem can be formulated as follows.
arg min
w

s.t.

(1 − λ)

K


wi r∗ − ri 2 + λw2

i=1
K


wi = 1;

wi ≥ 0

∀i,

i=1

where 0 ≤ λ ≤ 1 is the regularization parameter which speciﬁes the tradeoﬀ between the minimization of the weighted
distance and the smoothness enforced by w. In experiments,
λ is set to 0.3 empirically. For simplicity, we use Euclidean
distance to measure the discordance of the consensus ranking r∗ and each of individual sentence rankings ri .
1
, and this optimization problem can
We initialize wi = K
be solved by iterating the following two steps:
Step 1: Solve for r∗ while ﬁxing
w. The optimal solution

is the weighted average r∗ = i wi ri .
Step 2: Solve for w while ﬁxing r∗ . Let

Copyright is held by the author/owner(s).
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
ACM 978-1-60558-896-4/10/07.

d = [r∗ − r1 2 , r∗ − r2 2 , · · · , r∗ − rK 2 ] ∈ RK .

809

Systems
DUCBest
Centroid
LexPageRank
LSA
NMF
Ave Score
Ave Rank
Med Rank
RR
BC
CW
ULTRA
Graph
WCS

Note that
(1 − λ)

K


wi r∗ − ri 2 + λw2 = (1 − λ)d w + λw w

i=1

λ − 1 2 (λ − 1)2
d −
d2
2λ
4λ
For ﬁxing r∗ , the optimization problem becomes
= λw −

arg min
w

w −

λ−1 2
d s.t.
2λ

K


wi = 1;

wi ≥ 0,

∀i

i=1

This is a quadratic function optimization problem with
linear constraints with K variables. This is a problem of
just about tens of variables (i.e., weights for each input summarization system) and thus can be computed quickly. It
can also be solved by simply projecting vector λ−1
d onto
2λ
(K − 1)-simplex. With step 1 and 2, we iteratively update
w and r∗ until convergence. Then we sort r∗ in ascending
order to get the consensus ranking.

3.

R-1
0.382
0.367
0.378
0.341
0.367
0.388
0.385
0.385
0.364
0.378
0.378
0.392
0.379
0.398

R-2
0.092
0.073
0.085
0.065
0.072
0.089
0.087
0.087
0.072
0.085
0.085
0.090
0.086
0.096

R-SU
0.132
0.125
0.130
0.119
0.129
0.132
0.131
0.131
0.126
0.129
0.131
0.133
0.132
0.135

Table 1:
Overall performance comparison on
DUC2004. Remark: DUCBest shows the best results from DUC 2004 competition.

3.2 Diversity of Individual Summarizers
In this set of experiments, we further examine if the four
individual summarization methods are complementary to
each other. We use our WCS method to aggregate any three
of the four summarization methods and compare the results
with the aggregation utilizing all the four methods. Table 2
shows the comparison results. From the results, we observe
that adding any of the four individual methods improves
the summarization performance. This is because these individual summarization methods are diverse and their performance is data dependant.

EXPERIMENTS

In the experiments, we use four typical multi-document
summarization methods as individual summarizers and compare our WCS method with other eight aggregation methods. The four individual summarization methods are: (a)
Centroid [7], (b) LexPageRank [1], (c) LSA [2], and (d)
NMF [4]. And the baseline aggregation methods are: (1)
average score (Ave Score), which normalizes and averages
the raw scores from diﬀerent summarization systems; (2)
average rank (Ave Rank), which averages individual rankings; (3) median aggregation (Med Rank); (4) Round Robin
(RR); (5) Borda Count (BC); (6) correlation-based weighting (CW), which weights individual systems by their average
Kendall’s Tau correlation between the ranking list they generated and all the other lists; (7) ULTRA [3], which aims to
ﬁnd a consensus ranking with the minimum average Spearman’s distance [8] to all the individual ranking lists; (8)
graph-based combination (Graph), the basic idea of which
is similar to the work proposed in [9], however, we use cosine similarity so that we can compare this method with
other combination methods fairly. We conduct experiments
on DUC benchmark data for generic multi-document summarization and use ROUGE [6] toolkit (version 1.5.5) to
measure the summarization performance.

Systems
Centroid+LexPageRank+LSA
Centroid+LexPageRank+NMF
Centroid+LSA+NMF
LexPageRank+LSA+NMF
All

R-1
0.383
0.385
0.376
0.382
0.398

R-2
0.088
0.090
0.082
0.087
0.096

R-SU
0.132
0.133
0.131
0.132
0.135

Table 2: WCS results on DUC2004.
Acknowledgements: The work is partially supported
by an FIU Dissertation Year Fellowship and NSF grants IIS0546280 and DMS-0915110.

4. REFERENCES
[1] G. Erkan and D. Radev. Lexpagerank: Prestige in
multi-document text summarization. In EMNLP, 2004.
[2] Y. Gong and X. Liu. Generic text summarization using
relevance measure and latent semantic analysis. In SIGIR,
2001.
[3] A. Klementiev, D. Roth, and K. Small. An unsupervised
learning algorithm for rank aggregation. In ECML, 2007.
[4] D. Lee and H. Seung. Learning the parts of objects by
non-negative matrix factorization. Nature, pages 788–791,
1999.
[5] T. Li and C. Ding. Weighted consensus clustering. SIAM Data
Mining, 2008.
[6] C.-Y. Lin and E.Hovy. Automatic evaluation of summaries
using n-gram co-occurrence statistics. In NLT-NAACL, 2003.
[7] D. Radev, H. Jing, M. Stys, and D. Tam. Centroid-based
summarization of multiple documents. Information Processing
and Management, pages 919–938, 2004.
[8] C. Spearman. The proof and measurement of association
between two things. Amer. J. Psychol., 1904.
[9] V. Thapar, A. A. Mohamed, and S. Rajasekaran. A consensus
text summarizer based on meta-search algorithms. In
Proceedings of 2006 IEEE International Symposium on
Signal Processing and Information Technology, 2006.

3.1 Overall Summarization Performance
Table 1 show Rouge-1, Rouge-2, and Rouge-SU scores of
diﬀerent individual and combination methods using DUC2004
data sets (intuitively, the higher the scores, the better the
performance). From the results, we observe that (1) Most
of the combination summarization systems outperform all
the individual systems except the round robin combination.
The results demonstrate that in general consensus methods
can improve the summarization performance. (2) Weighted
combinations (e.g., CW, ULTRA, and WCS) outperform
average combination methods which treat each individual
system equally. (3) Our WCS method outperforms other
weighted combination methods because WCS optimizes the
weighted distance between the consensus sentence ranking
to individual rankings and updates the weights and consensus ranking iteratively, which is closer to the nature of
consensus summarization than other approximation based
weighted methods.

810

