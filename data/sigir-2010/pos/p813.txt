Investigating the Suboptimality and Instability of
Pseudo-Relevance Feedback
Abhijit Bhole

Raghavendra Udupa

Microsoft Research India,
Bangalore 560080, India

Microsoft Research India,
Bangalore 560080, India

raghavu@microsoft.com

v-abhibh@microsoft.com

ABSTRACT

techniques by studying more than 800 topics from several
test collections including the TREC Robust Track.
We develop DEX, an oracle method for extracting a set of
expansion terms from the pseudo-relevant documents using
discriminative learning. Being an oracle method, DEX can
be viewed as a good approximation to the ideal PRF technique that we can hope to design. As state-of-the-art PRF
techniques and DEX extract expansion terms from the same
set of pseudo-relevant documents, the gap in their retrieval
performance indicates the future potential for improvement
in retrieval performance of PRF techniques.

Although Pseudo-Relevance Feedback (PRF) techniques improve average retrieval performance at the price of high variance, not much is known about their optimality1 and the reasons for their instability. In this work, we study more than
800 topics from several test collections including the TREC
Robust Track and show that PRF techniques are highly
suboptimal, i.e. they do not make the fullest utilization
of pseudo-relevant documents and under-perform. A careful selection of expansion terms from the pseudo-relevant
document with the help of an oracle can actually improve
retrieval performance dramatically (by > 60%). Further,
we show that instability in PRF techniques is mainly due to
wrong selection of expansion terms from the pseudo-relevant
documents. Our findings emphasize the need to revisit the
problem of term selection to make a break through in PRF.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Experimentation

1.

2.

THE DEX ORACLE

DEX is an oracle for extracting a set of useful expansion terms from the pseudo-relevant documents by using the
knowledge of relevant documents. DEX first extracts a set
of candidate expansion terms2 t1 , ..., tN from the pseudorelevant documents and then partitions this set into a set of
useful terms and a set of non-useful terms using statistical
learning [6]. It treats relevant documents for the topic as
+ve instances and top scoring non-relevant documents as ve instances3 . It learns a linear discriminant function w to
discriminate the +ve instances from the −ve instances. The
linear discriminant function classifies a vector x as +ve if
wT x > 0 and as −ve if wT x ≤ 0. Therefore, DEX treats
terms ti : wi > 0 as useful terms and the rest as non-useful.
Finally, DEX picks the largest weighted k > 0 terms for
expansion.

INTRODUCTION

Pseudo-relevance feedback (PRF) uses terms from the top
ranking documents of the initial unexpanded retrieval for
expanding the query [5]. Although PRF improves average
retrieval performance, improvements have been incremental
despite years of research [1, 3]. It is not known whether PRF
techniques are under-performing or have already given their
best by making the fullest use of pseudo-relevant documents
in Query Expansion. Further, there is no satisfactory explanation of instability of PRF techniques. It is commonly believed that PRF is unstable because the initial unexpanded
retrieval brings many non-relevant documents in the top for
some topics and therefore, query expansion produces topic
drift. However, for a good number of queries, retrieval performance does not change significantly. It is not known why
PRF techniques fail to make a difference to such queries.
It is important that the twin issues of optimality and instability be addressed to decide whether to continue investment on new research in PRF and to devise effective ways of
combating instability. In this work, we take the first steps towards understanding the optimality and instability of PRF

3.

EXPERIMENTAL STUDY

We employed a KL-divergence based retrieval system with
two stage Dirichlet smoothing as our baseline [4]. We used
model-based feedback technique (Mixture Model) as a representative PRF technique [3]. For expanded retrieval, we interpolated the feedback model with the original query model
with α set to 0.5. For estimating the feedback model, we
used the top 10 documents fetched by the initial retrieval.
Topics as well as documents were stemmed using the well
known Porter stemmer and stop-words were removed. We
compared model-based feedback with DEX-based PRF. We
used the DEX algorithm (Section 2) to extract k = 5 expansion terms from the top 10 documents of the unexpanded
retrieval. We formed a feedback model from the expansion

1

2

By optimality of PRF techniques, we mean their use of
pseudo-relevant documents in such a way as to maximize
retrieval performance.

Candidate expansion terms are those terms from the
pseudo-relevant documents whose idf > ln10 and collection
frequency ≥ 5 [2].
3
Each labeled instance D has an associated feature vector x
whose dimensions i = 1, ..., N correspond to the candidate
expansion terms t1 , ..., tN respectively.

Copyright is held by the author/owner(s).
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
ACM 978-1-60558-896-4/10/07.

813

Table 1: Comparitive Retrieval Performance
Collection
CLEF 00-02
CLEF 03,05,06
AP
WSJ
SJM
Robust03
Robust04

LM
MAP P@5
0.43
0.49
0.38
0.42
0.28
0.47
0.27
0.48
0.21
0.34
0.11
0.32
0.25
0.49

MF
MAP P@5
0.44
0.50
0.41
0.43
0.33
0.50
0.30
0.52
0.24
0.36
0.13
0.34
0.28
0.49

Table 2: Effect of MF and DEX on individual topics

DEX
MAP P@5
0.74* 0.74
0.66* 0.72
0.48* 0.73
0.43* 0.72
0.43* 0.63
0.29* 0.64
0.40* 0.71

Collection
Name
CLEF 00-02
CLEF 03,05,06
AP
WSJ
SJM
Robust03(Hard)
Robust04

% Topics
improved
MF DEX
54
91
56
89
68
97
61
93
56
98
52
90
55
85

% Topics
indifferent
MF DEX
23
8
25
8
24
1
27
2
29
1
34
4
28
4

% Topics
hurt
MF DEX
23
1
19
3
8
2
11
5
15
1
14
6
17
11

terms by assigning equal probability mass to the DEX terms.
As with model-based feedback, we interpolated our feedback
model with the query model with α set to 0.5. We call unexpanded retrieval, model-based feedback, and DEX-based
PRF as LM, MF and DEX respectively.

of DEX gives hope for PRF techniques to achieve a higher
degree of robustness while not sacrificing the gain in average retrieval performance. Wrong selection of terms is at the
root of instability and PRF techniques will need to relook
term selection strategies [1, 2].

3.1

3.3

Test Collections

We used the CLEF (LATimes 94, Glasgow Herald 95) and
TREC (Associated Press 88-89, Wall Street Journal, San
Jose Mercury, Disks 4&5 minus the Congressional Record)
document collections in our experiments. We studied retrieval performance on the following sets of topics: CLEF
Topics 1 - 140 (CLEF 2000-2002), Topics 141-200 (CLEF
2003), Topics 251-350 (CLEF 2005-2006), TREC Topics 51
- 200 (TREC Adhoc Tasks 1, 2, 3), Assorted Topics (TREC
Robust 2003, Hard), Topics 301-450, 601-700 (TREC Robust 2004). There were totally 821 unique topics. Some
topics were used for retrieval on multiple document collections.

3.2

Discussion

To understand why MF is suboptimal and unstable, we
computed the average rank of the DEX terms in the rank
list of the MF expansion terms (ranked according to p(.|θF )).
We suspected that DEX terms would not be at the top of the
rank list. Because otherwise MF would not be comparatively
so worse. Our suspicion turned out to be true. For a large
majority of the topics, DEX terms were deep down in the
MF rank list. For instance, the average rank of DEX terms
for Topic 193 from TREC 3 was 114 and for TREC Topics
350 and 190 it was 229 and 735 respectively. It is clear that
MF fails to recognize the importance of DEX terms and
ranks them poorly. As a consequence of not choosing the
right terms, expanded retrieval fails to improve the retrieval
performance of these topics.

Retrieval Performance

We used MAP and P@5 as the average performance measures to compare the three retrieval models. We say that
a topic is hurt by expanded retrieval if the average precision decreases by 0.01 or more relative to the unexpanded
retrieval. Similarly, we say that a topic is improved (benefitted) by a retrieval model if the average precision increases
by 0.01 or more. We compare the performance of LM, MF,
and DEX on all topics in Table 1. We see that MF fares
better than LM overall but the improvement in retrieval
performance is modest. In contrast, DEX gives dramatic
improvement in retrieval performance relative to both LM
and MF on all test sets despite using the same set of documents for estimating the feedback model. DEX improves
MAP by > 60% over LM and by > 42% over MF in general
and on Robust03, DEX improves MAP by 123% over MF.
Not only the MAP has improved dramatically P@5 has also
improved. The huge gap between the average retrieval performance of MF and DEX highlights a very important fact:
PRF techniques are highly suboptimal. Their average
retrieval performance is much lower than what can be potentially achieved using the same set of feedback documents.
Table 2 shows the percentage of topics which benefitted
from MF and DEX, topics which got hurt by MF and DEX
and topics which remained indifferent to MF and DEX. We
observe that nearly 25% of topics in all collections are indifferent to MF whereas a smaller percentage of topics are hurt
by MF. DEX reduces the percentage of topics in these two
categories substantially. In the Robust03 track (hard), we
see that the percentage of topics which got hurt reduced to
6% from 14% and the percentage of topics which remained
indifferent from 28% to 4%. The relatively high robustness

4.

CONCLUSION

Our study shows that current PRF techniques are highly
suboptimal and also that wrong selection of expansion terms
is at the root of instability of current PRF techniques. A
careful selection of expansion terms from the pseudo-relevant
documents with the help of an oracle can actually improve
retrieval performance dramatically (by > 60%). We believe
our findings will motivate PRF researchers to revisit the issue of term seleaction in PRF. It might be worthwhile to
selectively extract expansion terms from the feedback documents. Further, term interactions may prove crucial in
addressing the problems of suboptimality and instability[2].

5.

REFERENCES

[1] G. Cao, J.-Y. Nie, J. Gao, and S. Robertson. Selecting
good expansion terms for pseudo-relevance feedback. In
Proceedings of SIGIR ’08.
[2] R. Udupa, A. Bhole, and P. Bhattacharyya. On
selecting a good expansion set in pseudo-relevance
feedback. In Proceedings of ICTIR ’09.
[3] C. Zhai and J. Lafferty. Model-based feedback in the
language modeling approach to information retrieval.In
Proceedings of CIKM ’01.
[4] C. Zhai and J. Lafferty. A study of smoothing methods
for language models applied to information retrieval.
ACM Trans. Inf. Syst.(2)
[5] E. N. Efthimiadis. Query expansion. Annual Review of
Information Systems and Technology.
[6] T. Hastie, R. Tibshirani, and J. Friedman. The
Elements of Statistical Learning.

814

