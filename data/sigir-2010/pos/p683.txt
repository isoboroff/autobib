Discriminative Models of Integrating Document Evidence
and Document-Candidate Associations for Expert Search
Yi Fang

Luo Si

Aditya P. Mathur

Department of Computer Science
Purdue University
West Lafayette, IN 47907, USA

Department of Computer Science
Purdue University
West Lafayette, IN 47907, USA

Department of Computer Science
Purdue University
West Lafayette, IN 47907, USA

fangy@cs.purdue.edu

lsi@cs.purdue.edu

apm@cs.purdue.edu

vertical search engines known as expert ﬁnder have emerged
for enterprise organizations.
As an important IR application, expert search (also known
as expert ﬁnding) has received substantial attention in the
IR research community. Rapid progress has been made in
modeling and evaluation since the launch of TREC Enterprise Track in 2005 [12]. A notable observation is that probabilistic generative models have dominated the literature of
expert search. In particular, many statistical language modeling techniques were proposed to model the relationship between a candidate expert and a query. These models usually
characterize a generative process of how a query is generated
from supporting documents of an expert. The key ingredient
in these methods is to determine associations between people and documents because the associations are ambiguous
in the TREC scenarios as well as in many realistic settings.
Previous works have investigated diﬀerent metrics or a combination of them to measure the associations, but the way
of choosing or combining them is rather often heuristic and
lacks of a clear justiﬁcation. Furthermore, document evidence such as document or expert authority information,
internal and external document structures, global evidence
and so on is shown to be able to signiﬁcantly improve expert retrieval performance, but to incorporate these features
often requires many modeling assumptions and is often unwieldy.
On the other hand, discriminative models, another important class of probabilistic models with solid statistical foundation, are nearly absent in the research of expert search,
especially on the TREC evaluations. In fact, discriminative
models have been preferred over generative models in the
recent past in many machine learning applications, partly
because of their attractive theoretical properties. In the domain of IR, various discriminative models have also been
applied to many retrieval problems (e.g., [23]). However,
very limited research has been conducted to design discriminative models for expert search.
In this work, we present a relevance-based discriminative learning framework for expert search and derive speciﬁc discriminative models from the framework. Similar to
some prominent language models, the proposed models aggregate document evidence and document-candidate associations through supporting documents. Unlike the language
models, we directly model the conditional probability of relevance given a query and an expert. As a result, heterogeneous or even arbitrary features can be naturally included
into a single model. The parameters associated with the
features are automatically learned from training data. We

ABSTRACT
Generative models such as statistical language modeling have
been widely studied in the task of expert search to model
the relationship between experts and their expertise indicated in supporting documents. On the other hand, discriminative models have received little attention in expert
search research, although they have been shown to outperform generative models in many other information retrieval
and machine learning applications. In this paper, we propose
a principled relevance-based discriminative learning framework for expert search and derive speciﬁc discriminative
models from the framework. Compared with the state-ofthe-art language models for expert search, the proposed research can naturally integrate various document evidence
and document-candidate associations into a single model
without extra modeling assumptions or eﬀort. An extensive
set of experiments have been conducted on two TREC Enterprise track corpora (i.e., W3C and CERC) to demonstrate
the eﬀectiveness and robustness of the proposed framework.

Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: H.3.3 Information Search and Retrieval; H.3.4 Systems and Software

General Terms
Algorithms, Design, Experimentation

Keywords
Expert search, enterprise search, discriminative models

1. INTRODUCTION
With vast amount of information available within large organizations, the key challenge is to harness existing knowledge and expertise in a timely and eﬀective manner. In
consequence, enterprise information retrieval systems are increasingly demanded to return people with speciﬁc knowledge and skills in response to a user’s query. A class of

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

683

report an extensive set of experiments on two TREC corpora
to evaluate the eﬀectiveness and robustness of the proposed
discriminative framework.
The next section discusses related work. Section 3 introduces the state-of-the-art generative language models for expert search. Section 4 presents our proposed approaches. In
section 5, we discuss the advantages of discriminative models in the context of expert search. Section 6 explains our
experimental methodology and Section 7 presents the experimental results. Section 8 concludes and points out some
future work.

Besides the models, some researchers have shown that
suitable features can help signiﬁcantly boost the performance
of expert ﬁnding. These features include document authority information such as the PageRank, indegree, and URL
length [38], graph-based expert authority [10], internal document structures that indicate the experts’ associations with
the content of documents [6], non-local evidence [2], and the
evidence that can be acquired outside of an enterprise [29].
Additional evidence can be integrated by identifying home
pages of candidate experts and clustering relevant documents [20]. Proximity features that characterize the cooccurrence of query and expert mentions in the document
are also shown indicative by the top runs in the TREC evaluations [16]. This led to several window-based approaches
including [25, 4, 20].
On the other hand, the early work of applying discriminative models in IR can date back to the early 1980s in
which the maximum entropy approach was investigated to
get around term independence assumptions in probabilistic generative models [11]. More recently, Nallapati [23]
compared the performance of the maximum entropy model
and support vector machines with that of language modeling
in ad hoc retrieval and homepage ﬁnding, and argued that
SVMs are preferred over language models because of their
ability to learn arbitrary features automatically. Furthermore, it has been shown that feature-based discriminative
models can consistently and signiﬁcantly outperform current state of the art retrieval models with the correct choice
of features [22]. Discriminative models have received increasing attention in IR, as another related area, learning to
rank for IR, sparked genuine interest among researchers in
the community [18]. Most of the learning to rank models are
discriminative in nature and they have been shown improvements over their generative counterparts in ad hoc retrieval.
Benchmark data sets such as LETOR [19] are also available
for research on learning to rank. Although valuable work has
been done on discriminative models for ad hoc retrieval and
other IR domains, very limited research has been conducted
to design discriminative models for expert search. The only
relevant work that we are aware of is [15], which addressed
the issue of diﬀerentiating heterogeneous sources according
to speciﬁc queries and experts by learning associated weights
from data, but the work did not model document-candidate
relationship nor address how to incorporate new document
evidence, which are two key issues in expert search.

2. RELATED WORK
The early work on expert ﬁnding systems was initiated in
the Knowledge Management community, usually in the form
of yellow pages [9]. These systems relied on experts to judge
and input their skills by themselves against a predeﬁned set
of keywords, and thus the task was time-consuming. More
recent techniques locate experts in an automatic fashion. An
overview of early automatic expert ﬁnding systems is provided in [36]. The task of expert search has received a significant amount of attention as the task had been included in
the TREC Enterprise track from 2005 to 2008 [12, 32, 1, 7].
The TREC Enterprise tracks provided a common platform
for researchers to empirically evaluate methods for expert
search. They demonstrated the feasibility of expert search
on heterogeneous data collections. In the TREC corpora,
the relationship between documents and experts is ambiguous and thus to model the document-candidate associations
is a key issue in expert search research.
Most of the recent work on expert search generally falls
into two categories: proﬁle-centric and document-centric approaches. Balog et al. [3] formalizes the two methods by
proposing two generative language models. Their Model 1
directly models the knowledge of an expert from associated
documents, which is equivalent to a proﬁle-centric approach,
and their Model 2 ﬁrst locates documents on the topic and
then ﬁnds the associated experts, which is a documentcentric approach. It has been shown in [3] that Model 2 is
generally more eﬀective than Model 1 and since then it becomes one of the most prominent language models for expert
search. In [8], a two-stage language model combining a document relevance and co-occurrence model is proposed, which
is essentially equivalent to Model 2. An attempt to further
improve their models is made by proposing a proximitybased document representation for incorporating sequential
information in text [25]. There are many other generative
probabilistic models proposed for expert ﬁnding. For example, Serdyukov and Hiemstra [30] propose an expert-centric
language model. Fang and Zhai [14] derive two families of
generative models by applying probability ranking principle.
Probabilistic topic models are also proposed to simultaneously model the topical distribution of expertise evidence
and experts [34].
Some alternative approaches to expert search exist beyond
language modeling. One eﬀective approach is to treat the
problem of ranking experts as a voting problem based on
data fusion techniques [21]. Eleven diﬀerent voting strategies were proposed to aggregate over the documents associated to an expert. Another approach is to model the process
of expert ﬁnding by probabilistic random walks on so-called
expertise graphs [31]. Many other expert ﬁnding methods
were proposed during TREC Enterprise tracks.

3.

GENERATIVE MODELS

To predict a class 𝐶 given an observation 𝑥, the desired
choice of 𝐶 is given by the conditional class probabilities
𝑃 (𝐶∣𝑥). Depending on how to compute 𝑃 (𝐶∣𝑥), the existing classiﬁcation techniques can be broadly classiﬁed into
two major categories: generative models and discriminative
models. In a discriminative approach, a parametric model
is introduced for 𝑃 (𝐶∣𝑥), and the values of the parameters
are inferred from a set of labeled training data. In contrast,
the generative approach attempts to capture the manner in
which an observation 𝑥 is generated from given classes 𝐶
by specifying a prior distribution 𝑃 (𝐶) over classes and a
class-conditional distribution 𝑃 (𝑥∣𝐶) over the observation.
The posterior 𝑃 (𝐶∣𝑥) is obtained from Bayes’ Theorem as
𝑃 (𝐶∣𝑥) ∝ 𝑃 (𝑥∣𝐶)𝑃 (𝐶)

(1)

In the context of expert search, the task is to ﬁnd out what

684

is the probability of a candidate 𝑒 being an expert given a
query topic 𝑞. In other words, we want to know 𝑃 (𝑒∣𝑞)
in order to rank candidate 𝑒 according to this probability.
Similarly, by invoking Bayes’ Theorem, we have:
𝑃 (𝑒∣𝑞) ∝ 𝑃 (𝑞∣𝑒)𝑃 (𝑒)

parametric probability function. We cast expert search into
a binary classiﬁcation problem that treats the relevant queryexpert pairs as positive data and irrelevant pairs as negative
data. Formally, we use a relevance variable 𝑟 ∈ {1, 0} to
denote whether two entities are relevant or not and thus
the conditional probability of relevance 𝑃𝜃 (𝑟∣𝑒, 𝑞) represents
the extent to which the expert 𝑒 is relevant to the query
𝑞. In our framework, 𝑃𝜃 (𝑟∣𝑒, 𝑞) can take any function form
with parameter 𝜃 that needs to estimate from training data.
Based on diﬀerent forms of 𝑃𝜃 , the resulting discriminative
models are diﬀerent. Given the relevance judgment 𝑟𝑚𝑘 for
the training expert-query pair (𝑒𝑘 , 𝑞𝑚 ) which is assumed independently generated, the conditional likelihood 𝐿 of the
training data is as follows

(2)

where 𝑃 (𝑒) is the prior probability of a candidate, which is
generally assumed uniform. Thus, the key quantity to estimate in the generative models is the probability of a query
given the candidate, 𝑃 (𝑞∣𝑒). Many language modeling techniques are proposed to estimate this quantity. One of the
most prominent and eﬀective one was called document models (often referred as Model 2) [3] where documents act as a
hidden variable in the process which accumulates expertise
evidence. Formally, it is expressed as
𝑃 (𝑞∣𝑒) =

𝑛
∑

𝑃 (𝑞∣𝑑𝑡 )𝑃 (𝑑𝑡 ∣𝑒)

𝐿=

(3)

𝑀 ∏
𝐾
∏
𝑚

𝑡=1

𝑃𝜃 (𝑟 = 1∣𝑒𝑘 , 𝑞𝑚 )𝑟𝑚𝑘 𝑃𝜃 (𝑟 = 0∣𝑒𝑘 , 𝑞𝑚 )1−𝑟𝑚𝑘 (4)

𝑘

where 𝑀 is the number of queries and 𝐾 is the number of
experts. The parameters can then be estimated by maximizing the following log likelihood function

where 𝑃 (𝑞∣𝑑𝑡 ) is the probability of the document 𝑑𝑡 to generate the query 𝑞 and can be calculated using a standard
language model. 𝑃 (𝑑𝑡 ∣𝑒) is the probability of association
between the document 𝑑𝑡 and the candidate 𝑒. 𝑛 is the
number of documents in the collection. Model 2 mimics the
process one might use to ﬁnd experts using a document retrieval system. Here, relevant documents are retrieved for
the expertise requested, and they are used as evidence to
indicate whether the associated candidates are experts. After aggregating all such evidence, the experts can be identiﬁed. As 𝑃 (𝑞∣𝑑𝑡 ) is relatively easy to determine in language models, the key ingredient in this model (and also in
many other language models for expert search) is to estimate
the document-candidate associations: 𝑃 (𝑑𝑡 ∣𝑒), or 𝑃 (𝑒∣𝑑𝑡 ) if
𝑃 (𝑑𝑡 ) is assumed to be uniform. 𝑃 (𝑒∣𝑑𝑡 ) can be estimated
by various methods. The simplest form is the boolean model
where associations are binary decisions: 𝑃 (𝑒∣𝑑𝑡 ) = 1 if the
candidate appears in the document; otherwise, 𝑃 (𝑒∣𝑑𝑡 ) = 0.
More sophisticated methods are frequency based which consider the number of times that a candidate appears in the
document. A set of heuristic combinations of all these metrics are also compared and investigated in [6].

𝜃∗ = arg max
𝜃

𝑀 ∑
𝐾 (
∑
𝑚

𝑟𝑚𝑘 log 𝑃𝜃 (𝑟 = 1∣𝑒𝑘 , 𝑞𝑚 )

(5)

𝑘

(
)
+ (1 − 𝑟𝑚𝑘 ) log 1 − 𝑃𝜃 (𝑟 = 1∣𝑒𝑘 , 𝑞𝑚 )

)

The estimated parameters can then be plugged back in
𝑃𝜃 (𝑟 = 1∣𝑒𝑘 , 𝑞𝑚 ). According to the probability ranking principle, the experts are presented to users in the descending
order of 𝑃𝜃 (𝑟 = 1∣𝑒𝑘 , 𝑞𝑚 ). In the next section, we propose a speciﬁc discriminative model by deﬁning the form
of 𝑃𝜃 (𝑟 = 1∣𝑒𝑘 , 𝑞𝑚 ).

4.2

A Discriminative Model

According to the previous work, Model 2 turned out to
be one of the most eﬀective formal models for expert search.
The success of the model lies in its eﬀective process to collect expertise evidence from documents. Our discriminative
model builds on the same process in which the supporting
document 𝑑 serves as a bridge to connect expert 𝑒 and query
𝑞. Given a document 𝑑, whether 𝑒 and 𝑞 are relevant depends
on two factors: document evidence and document-candidate
associations. More speciﬁcally, we consider: 1) whether the
document 𝑑 is relevant to the query 𝑞; 2) whether the expert
𝑒 is relevant to the document 𝑑. The ﬁnal relevance decision for (𝑒, 𝑞) is made by averaging over all the documents.
Formally, this can be expressed as

4. DISCRIMINATIVE MODELS FOR
EXPERT SEARCH
4.1 Discriminative Learning Framework for
Expert Search
For the text-based retrieval, conventional relevance-based
probabilistic models rank documents by sorting the conditional probability that each document would be judged
relevant to the given query [17]. The underlying principle
using probabilistic models for information retrieval is called
probability ranking principle [26]. The Binary Independence
Model (BIM) [27] is a realization of this principle. In the
domain of expert search, the similar principle can be used
where experts are ranked according to the descending order
of the conditional probability of relevance given an expert
and a query. Fang and Zhai [14] applied this principle in
studying expert search problem. Both BIM and [14]’s models are generative and they use Bayes’ theorem to reverse
the original conditional probability.
We propose a discriminative learning framework to directly model the conditional probability of relevance by a

𝑃𝜃 (𝑟 = 1∣𝑒, 𝑞) =

𝑛
∑
𝑡=1

𝑃 (𝑟1 = 1∣𝑞, 𝑑𝑡 )𝑃 (𝑟2 = 1∣𝑒, 𝑑𝑡 )𝑃 (𝑑𝑡 )

(6)
where 𝑃 (𝑟1 = 1∣𝑞, 𝑑𝑡 ) allows us to model the probability that
a document 𝑑𝑡 matches a topic 𝑞, which indicates the document evidence. 𝑃 (𝑟2 = 1∣𝑒, 𝑑𝑡 ) allows us to model the probability that a supporting document 𝑑𝑡 mentions a candidate
𝑒, which indicates the document-candidate associations. A
document 𝑑𝑡 with higher values on both probabilities would
contribute more to the value of 𝑃 (𝑟 = 1∣𝑒, 𝑞). The prior
probability of a document, 𝑃 (𝑑𝑡 ), is generally assumed uniform (i.e., 𝑃 (𝑑𝑡 ) = 𝑛1 ). We model both 𝑃 (𝑟1 = 1∣𝑞, 𝑑𝑡 ) and
𝑃 (𝑟2 = 1∣𝑒, 𝑑𝑡 ) by logistic functions on a linear combination

685

of features. Formally, they are parameterized as follows:
𝑁𝑓

𝑃 (𝑟1 = 1∣𝑞, 𝑑𝑡 ) = 𝜎

(∑

)
𝛼𝑖 𝑓𝑖 (𝑞, 𝑑𝑡 )

(7)

)
𝛽𝑗 𝑔𝑗 (𝑒, 𝑑𝑡 )

(8)

tivates an alternative discriminative model which we refer
as the geometric mean discriminative (GMD) model where
𝑃𝜃 (𝑟 = 1∣𝑒, 𝑞) is modeled by the geometric mean as follows:
)1
𝑛 (
𝑛
1 ∏
𝑃 (𝑟1 = 1∣𝑞, 𝑑𝑡 )𝑃 (𝑟2 = 1∣𝑒, 𝑑𝑡 )
𝑃 (𝑟 = 1∣𝑒, 𝑞) =
𝑍 𝑡=1
(9)
where 𝑍 is the normalization factor that scales the geometric
mean to be a proper probability distribution as follows
)1
𝑛 (
∑
∏
𝑛
𝑍=
𝑃 (𝑟1 ∣𝑞, 𝑑𝑡 )𝑃 (𝑟2 ∣𝑒, 𝑑𝑡 )
(10)

𝑖=1
𝑁

𝑃 (𝑟2 = 1∣𝑒, 𝑑𝑡 ) = 𝜎

𝑔
(∑

𝑗=1

where 𝜎(𝑥) = 1/(1 + exp (−𝑥)) is the standard logistic function. 𝛼𝑖 is the weight for the 𝑖𝑡ℎ query-document feature
𝑓𝑖 (𝑞, 𝑑𝑡 ) and 𝛽𝑗 is the weight for the 𝑗 𝑡ℎ document-candidate
feature 𝑔𝑗 (𝑒, 𝑑𝑡 ). Speciﬁcally, 𝑓𝑖 (𝑞, 𝑑𝑡 ) is the document evidence such as document retrieval scores that indicates how
relevant the document is to the query. 𝑔𝑗 (𝑒, 𝑑𝑡 ) is the feature
such as the boolean associations that describe the strength
of associations between a document and a candidate. 𝑁𝑓
denotes the number of document evidence features and 𝑁𝑔
denotes the number of document-candidate association features. The weight parameters can be learned by maximizing the conditional log-likelihood of the data (i.e., Eqn. 5).
Because there is no analytical solution, we use the BFGS
Quasi-Newton for the optimization [13]. The method requires the objective function and its gradients. The partial
derivatives of the log-likelihood 𝐿 with respect to 𝛼𝑖 and 𝛽𝑗
are given as
)
𝑛
𝑀 ∑
𝐾 (
∑
𝑟𝑚𝑘 − 𝑃𝑟 ∑
∂𝐿
𝜎𝛼 (1 − 𝜎𝛼 )𝜎𝛽 𝑓𝑖 (𝑞𝑘 , 𝑑𝑡 )
=
∂𝛼𝑖
𝑃𝑟 (1 − 𝑃𝑟 ) 𝑡=1
𝑚
𝑘
)
𝑛
𝑀 ∑
𝐾 (
∑
∂𝐿
𝑟𝑚𝑘 − 𝑃𝑟 ∑
𝜎𝛽 (1 − 𝜎𝛽 )𝜎𝛼 𝑔𝑗 (𝑒𝑚 , 𝑑𝑡 )
=
∂𝛽𝑗
𝑃𝑟 (1 − 𝑃𝑟 ) 𝑡=1
𝑚

𝑟1 ∈{0,1},𝑟2 ∈{0,1} 𝑡=1

Both 𝑃 (𝑟1 = 1∣𝑞, 𝑑𝑡 ) and 𝑃 (𝑟2 = 1∣𝑒, 𝑑𝑡 ) here take the
same form with Eqn. 7 and Eqn. 8. By plugging them and
Eqn. 10 into Eqn. 9, we can get
𝑃 (𝑟 = 1∣𝑒, 𝑞) =

1
(11)
1 + exp(−𝐸) + exp(−𝐹 ) + exp(−𝐺)

where
𝐸=

𝑁𝑓
∑

𝛼𝑖

𝑁𝑔
𝑛
𝑛
∑
(1 ∑
)
(1 ∑
)
𝑓𝑖 (𝑞, 𝑑𝑡 ) , 𝐹 =
𝛽𝑗
𝑔𝑗 (𝑒, 𝑑𝑡 )
𝑛 𝑡=1
𝑛 𝑡=1
𝑗=1

𝛼𝑖

𝑁𝑔
𝑛
𝑛
(1 ∑
) ∑
(1 ∑
)
𝑓𝑖 (𝑞, 𝑑𝑡 ) +
𝛽𝑗
𝑔𝑗 (𝑒, 𝑑𝑡 )
𝑛 𝑡=1
𝑛
𝑡=1
𝑗=1

𝑖=1

𝐺=

𝑁𝑓
∑
𝑖=1

𝑘

where 𝑃𝑟 , 𝜎𝛼 and 𝜎𝛽 denote the probabilities of Eqn. 6,
Eqn. 7, and Eqn. 8, respectively. The main computation of
the gradient method is evaluating the log likelihood function
and its gradients against parameters.
Both of )them have
(
computational complexity of 𝑂 𝑀 𝐾𝑛(𝑁𝑓 + 𝑁𝑔 ) . In practice, we only have a small number of relevance judgments
for training and thus 𝐾 is relatively small. In addition, the
number of documents associated with each expert and the
number of features used are also usually relatively small.
Therefore, the training procedure can be eﬃcient.
We can see that both Model 2 and this discriminative
model try to aggregate document evidence and documentcandidate associations through the bridge of documents, but
they are diﬀerent in how to estimate these two probabilities. In Model 2, the document evidence (i.e., 𝑃 (𝑞∣𝑑𝑡 )) is
calculated by standard language models and the documentcandidate associations (i.e., 𝑃 (𝑑𝑡 ∣𝑒)) are estimated by a heuristic combination of document-candidate association features.
In our proposed discriminative model, both quantities are
modeled by logistic functions with arbitrary features and
the parameters are automatically determined from training
data. From Eqn. 6, we can see that 𝑃𝜃 (𝑟 = 1∣𝑒, 𝑞) is essentially the arithmetic mean of 𝑃 (𝑟 = 1∣𝑞, 𝑑, 𝑒) with respect to
𝑑. Thus we refer the model as the arithmetic mean discriminative (AMD) model.

4.3 An Alternative Discriminative Model with
Geometric Mean
It has been shown that in certain cases geometric mean
(the product rule) is better than arithmetic mean (the sum
rule) in combining evidences [35]. This observation mo-

686

We can notice that in Eqn. 11 there are three exponential
terms in the denominator, which means that either querydocument features 𝑓𝑖 (𝑞, 𝑑𝑡 ) or document-candidate features
𝑔𝑗 (𝑒, 𝑑𝑡 ) alone cannot dominate the ﬁnal relevance 𝑃 (𝑟 =
1∣𝑒, 𝑞). The parameters of the model can also be estimated
by maximizing the conditional log-likelihood function using
BFGS. The GMD model has the same computational complexity with AMD.

4.4

Advantages of Discriminative Models for
Expert Search

Some theoretical results show that discriminative models tend to have a lower asymptotic error [24]. Besides the
theoretical considerations, we believe there are speciﬁc reasons for the domain of expert search that make discriminative models a suitable choice. First of all, the proposed
discriminative models can eﬀortlessly incorporate features.
As shown in Section 2 and prior research, expert search can
beneﬁt from including various types of features. Language
modeling approaches often require many modeling assumptions and extra modeling eﬀort to include new features especially when the heterogeneous features are present. Secondly, discriminative models typically make fewer model
assumptions than their generative counterparts. For example, many state-of-the-art generative models, including
Model 2, the candidate-generation model [14] and the twostage language model approach [8], assume that the query
𝑞 and candidate 𝑒 are independent given the document 𝑑,
i.e., 𝑝(𝑒∣𝑞, 𝑑) = 𝑝(𝑒∣𝑑). It requires extra modeling eﬀort for
these models to overcome the assumption [4]. In contrast,
our proposed discriminative models can easily get around
it. For example, 𝑃 (𝑟2 = 1∣𝑒, 𝑑𝑡 ) in Eqn. 6 can be replaced
by 𝑃 (𝑟2 = 1∣𝑒, 𝑞, 𝑑𝑡 ) where no independence assumption
is made on 𝑃 (𝑟2 = 1∣𝑒, 𝑞, 𝑑𝑡 ). Thirdly, the discriminative
models directly and naturally characterize the notion of relevance. In Model 2 and many other language models, there

is no explicit reference to the class variable that denotes
whether an expert is relevant or not. We use 𝑃 (𝑟 = 1∣𝑒, 𝑞)
instead of 𝑃 (𝑒∣𝑞) to make it explicit that the relevance of
an expert is measured with respect to a query. This explicit
notion of relevance can help quantify the extent to which a
user’s information need is satisﬁed.

Table 1: Statistics of the W3C and CERC testbeds
# Documents
# People
Avg. Doc Length in Token
Avg. # Rel Experts/Topic
(TREC Year)
Training Queries

5. EXPERIMENTS

Testing Queries

W3C
331,037
1,092
983.4
51.5 (2006)
30.2 (2005)
2006 (49)
2005 (10)
2005 (50)

CERC
370,715
3,482
354.8
10.4 (2008)
3.0 (2007)
2008 (77)
2007 (50)

5.1 Data Collections
Our experiments are carried out in the setting of the Expert Search task of the TREC Enterprise tracks from 2005
to 2008. For TREC 2005 and 2006, the document collection
was a crawl of the World Wide Web Consortium (W3C) [12,
32]. For TREC 2007 and 2008, a diﬀerent and more realistic corpus was introduced, which is a crawl of the website
of Commonwealth Scientiﬁc and Industrial Research Organization (CSIRO). The corpus is known as the CSIRO Enterprise Research Collection (CERC) [1, 7]. Table 1 gives
detailed statistics of the collections and query sets. The
W3C data is supplemented with a list of 1092 candidate experts represented by their full names and email addresses
while the CERC data do not contain a predeﬁned list of
candidates. Based on the observation that most CSIRO employees have a CSIRO email address following the pattern
“ﬁrstname.lastname@csiro.au”, we extract a list of candidates with email addresses matching this pattern from text.
We also use heuristic rules to ﬁlter non-personal addresses
(e.g. education.act@csiro.au). The total number of candidates extracted is 3,482. In 2005, 50 queries were created
based on the working groups in W3C (there were 10 training topics also available in 2005). In 2006, 49 queries were
developed by the track participants collectively using the
provided list of supporting documents for each candidate.
The 50 queries used in 2007 were created with the help of
CSIRO’s Science Communicators, while the judgments of 77
queries in 2008 were made by participants.
To evaluate the proposed models on W3C, we use the
TREC 2006 topics plus the 10 available TREC 2005 training
topics for training and test the models on the TREC 2005
topics. Similarly on CERC, we use TREC 2008 topics for
training and TREC 2007 topics for testing. Although diﬀerent years have diﬀerent ways of topic assessments, we will
see in the experiments that the discriminative models can
still gain signiﬁcant improvements from the training data.
Our decision of choosing the training and testing conﬁgurations is mainly based on the number of relevance judgments
available. We need a reasonable amount of training data for
the discriminative models and there are relatively more relevance judgments in 2006 for W3C and in 2008 for CERC.
Because the two test collections have very diﬀerent characteristics, we do not evaluate the models across the corpora.
To obtain a balanced training set, we randomly select the
same number of negative instances with the number of positive instances for each training query, by following the undersampling method in [23]. To acquire negative instances for
the queries without non-relevance judgments (i.e., 10 TREC
2005 training topics), we use the Base method introduced in
Section 6.1 to identify a list of unjudged/irrelevant experts
for each query. Evaluation measures are mean average precision (MAP), R-precision (R-Prec), mean reciprocal rank
(MRR), and precision@5 (p@5) and precision@10 (p@10).

5.2

Research Questions

An extensive set of experiments were designed to address
the following questions of the proposed research:
∙ Can the discriminative trained model perform better
than its generative counterpart when the same set of
features are available for use? (Section 6.1)
∙ Can integration of additional features into the discriminative model improve the performance? (Section 6.1)
∙ What features are likely more important in terms of
the relative values of the learned weights in the discriminative model? (Section 6.1)
∙ What is the eﬀect of only retrieving a subset of documents on the proposed model? (Section 6.2)
∙ How robust is the proposed discriminative model with
respect to the underlying document retrieval methods?
(Section 6.3)
∙ How robust is the proposed discriminative learning
framework with respect to speciﬁc discriminative models? (Section 6.4)
In all the sections except Section 6.4, we only use the
arithmetic mean discriminative (AMD) model to assess the
discriminative learning approach, since we care less about
the diﬀerence between discriminative models than about the
diﬀerence between generative and discriminative models.

5.3

Experimental Setup

In all our experiments, we have done minimal preprocessing in which both queries and documents are stemmed using
Krovetz stemmer. We only use the “title” or “query” ﬁelds
in the topics without using extra information (e.g., “narrative”). No query expansion nor external resource is utilized.
As shown in Section 4, each query-expert pair is characterized by two feature vectors, i.e., document evidence 𝑓𝑖 (𝑞, 𝑑𝑡 )
and document-candidate associations 𝑔𝑗 (𝑒, 𝑑𝑡 ). Table 2 summarizes the features used in the discriminative models.
These features include the score from the standard document language model (𝑓1 ), document features (𝑓2 − 𝑓5 ),
external document structure features (𝑓6 − 𝑓9 ), basic association features (𝑔1 −𝑔5 ), internal document structure features
(𝑔6 − 𝑔9 ), and proximity features (𝑔10 − 𝑔13 ). Here the external document structure features are the boolean variables
to represent whether a document (in W3C) comes from speciﬁc types of documents (e.g., 𝑓8 = 1 means the document
is either from “www” or “esw”). The evaluations on W3C
use all the features, while the features 𝑓6 − 𝑓9 and 𝑔6 − 𝑔9
are not applied to CERC, as the CERC dataset does not

687

Table 2: Features used in the discriminative models.
“B” denotes the feature takes boolean values and
“N” represents numerical values
Feature
𝑓1
𝑓2
𝑓3
𝑓4
𝑓5
𝑓6
𝑓7
𝑓8
𝑓9

Description
LM
PageRank
URL length
Anchor text
Title
From lists
From people
From www+esw
From other+dev

𝑔1
𝑔2
𝑔3
𝑔4
𝑔5
𝑔6
𝑔7
𝑔8
𝑔9
𝑔10 ∼ 𝑔13

Exact name match
Name match
Last name match
Email match
LM score
EMAIL FROM
EMAIL TO
EMAIL CC
EMAIL CONTENT
Proximity

Type
N
N
N
N
N
B
B
B
B

References
[37]
[38]
[38]
[38]
[38]
[12]
[12]
[12]
[12]

B
B
B
B
N
B
B
B
B
B

[3]
[3]
[3]
[3]
[6]
[5]
[5]
[5]
[5]
-

Base
R1
R2
R3
R4

Table 3: Experimental conﬁgurations

Balog et al’s Model 2 (candidate-centric) with 4
association features (i.e., 𝑔1 − 𝑔4 ) [3]
Discriminative model with 4 association features
(𝑔1 − 𝑔4 ) and LM document evidence feature (𝑓1 )
Discriminative model with full document evidence features and 4 association features (𝑔1 −𝑔4 )
Discriminative model with full association features and one document evidence feature (𝑓1 )
Discriminative model with full document evidence features and full association features

Table 4: Comparison of the discriminative model
(AMD) with the Base mehod on W3C and CERC.
Best results on each collection are highlighted. The
†symbol indicates statistical signiﬁcance at 0.95 conﬁdence interval against Base
MAP

R-Prec

MRR

P5

P10

0.1909
0.2001
0.2282†
0.2412†
0.2598†

0.2445
0.2552
0.2764
0.2904†
0.3035†

0.5081
0.5300
0.5624†
0.6232†
0.6196†

0.3760
0.3820
0.3960
0.4020
0.4130†

0.3120
0.3310
0.3370
0.3560†
0.3680†

0.4039
0.4123
0.4453†
0.4569†
0.4604†

0.3514
0.3569
0.3854†
0.3879†
0.3938†

0.5389
0.5593
0.5924†
0.5886†
0.6143†

0.2240
0.2280
0.2390
0.2610†
0.2520†

0.1540
0.1540
0.1650
0.1660
0.1770†

W3C
Base
R1
R2
R3
R4

contain explicit document types nor many emails with internal structure information useful for expert search [38].
The 𝑓1 feature is the document retrieval score by LM using the topic as the query. The smoothing method of LM is
Jelinek-Mercer with the parameter 𝜆 = 0.5 (we use the same
smoothing for other LMs). The 𝑔5 feature is the retrieval
score by LM using the candidate identiﬁer as the query [6].
The “Proximity” features (𝑔6 − 𝑔9 ) are the boolean variables
indicating whether the candidate identiﬁer co-occurs with
the query term in a window with various sizes. We use 20,
50, 100 and 250 as the window sizes (in number of words),
approximated to the sizes of sentence, passage, paragraph
and section, respectively. The details about these features
can be found in the corresponding reference. To normalize the features, we use query-based normalization for each
feature as suggested in [19].
Many of these features have been shown useful for expert
search. Because of the generative nature of language models, it is diﬃcult for them to incorporate such heterogeneous
features in a uniﬁed modeling framework, but discriminative models can eﬀortless include all the features and many
more. Since the focus of this study is on the probabilistic
models rather than feature engineering, we do not intend to
choose a complete set of features, but they are one of the
most comprehensive and diverse feature sets in a single work
among the existing expert search research.

CERC
Base
R1
R2
R3
R4

available for R1 and Base to use. The weights in Base are
set by following the choice of the best run in [3]. R4 is the
conﬁguration with full applicable features for the discriminative model (the R4 conﬁguration is the default setting in
all the experiments except explicitly noted). Table 4 contains the evaluation results on the two test collections. We
can see that the discriminative model consistently performs
better than Base across all the feature conﬁgurations on all
measures. With the full set of features (i.e., R4 vs Base), all
the diﬀerences are statistically signiﬁcant by two-tailed Student’s t-test at 0.95 conﬁdence level. In R1 vs Base, although
their diﬀerences are not signiﬁcant, the discriminative model
outperforms the Base method on all the evaluation metrics.
Since all the features are normalized, the weight associated
with each feature can reﬂect the importance of the feature
in some degree. Table 5 reports the top 3 features with the
largest weights in 𝑓𝑖 and 𝑔𝑖 respectively in the learned AMD
model. These features are ordered alphabetically in the table since their weights are not very distinct from each other.
We ﬁnd that the features listed for the two testbeds are generally diﬀerent with the exception of 𝑓1 and 𝑓2 , showing the
importance of these two features across the corpora. An
interesting observation is that the 𝑔8 feature when used on
W3C has a large weight among all the document-candidate
association features. This is intuitive in the sense that the
person who is in the email cc ﬁeld is likely an authoritative
of the topics of the email, which is also consistent with what
was reported in [5]. Another observation is that the “Proximity” features have large weights for both testbeds (i.e., 𝑔13

6. RESULTS
6.1 Discriminative Model vs. Model 2
In this section, we compare the proposed discriminative
model with its generative counterpart: Model 2. The proposed model is evaluated on four diﬀerent feature conﬁgurations, which are presented in Table 3. The Base method
is the implementation of Model 2 by following [3], which
includes 4 types of document-candidate associations. The
R1 conﬁguration uses these 4 association features plus 𝑓1
as document evidence. Thus, the identical information is

688

Table 5: The top 3 features with the largest weights
in AMD (R4) learned from training data
Doc evidence
𝑓 1 , 𝑓2 , 𝑓 6
𝑓 1 , 𝑓2 , 𝑓 5

W3C
CERC

Table 6: Evaluation of AMD with diﬀerent document retrieval methods on W3C and CERC

Doc-candidate associations
𝑔1 , 𝑔8 , 𝑔13
𝑔4 , 𝑔5 , 𝑔11

LM
BM25
Indri

MAP

MRR

P5

P10

0.2598
0.2658
0.2562

0.3035
0.3141
0.3066

0.6196
0.6238
0.6149

0.4130
0.4060
0.4090

0.3680
0.3700
0.3640

0.4604
0.4551
0.4667

0.3938
0.3895
0.4086

0.6143
0.5877
0.6000

0.2520
0.2470
0.2550

0.1770
0.1740
0.1780

CERC

0.25

LM
BM25
Indri

0.2
Base
AMD

0.15
0.1

2

10

0.5

MAP

R-Prec

W3C

W3C

3

4

10
Number of Documents Retrieved
CERC

10

Table 7: Comparison of the geometric mean discriminative model with Base and AMD (R4) on W3C
and CERC. The †symbol indicates statistical significance at 0.95 conﬁdence interval for GMD against
Base

0.4
Base
AMD

0.3
0.2

MAP

1

10

2

3

10
10
Number of Documents Retrieved

4

MAP

R-Prec

MRR

P5

P10

0.1909
0.2598
0.2512†

0.2445
0.3035
0.3010†

0.5081
0.6196
0.6266†

0.3760
0.4130
0.4110†

0.3120
0.3680
0.3640†

0.4039
0.4604
0.4669†

0.3514
0.3938
0.4030†

0.5389
0.6143
0.6274†

0.2240
0.2520
0.2500†

0.1540
0.1770
0.1790†

W3C

10

Base
AMD
GMD

Figure 1: Impact of varying the number of documents retrieved (𝑀 ) on the discriminative model.
Top: impact on W3C; Bottom: impact on CERC.

CERC
Base
AMD
GMD

for W3C and 𝑔11 for CERC), but with diﬀerent window sizes:
i.e., larger size on W3C. This may come from the fact that
these two collections have very diﬀerent average document
lengths.

ument retrieval score 𝑓1 is an important feature to show
document evidence for expert search. In this experiment,
we assess the extent to which the performance of the discriminative model is aﬀected by the choice of the underlying
document retrieval model. Besides LM, another two diﬀerent document retrieval methods are used (i.e., BM25 [28]
and Indri [33]). Speciﬁcally, the 𝑓1 feature is replaced by
these two retrieval scores respectively in the R4 conﬁguration. Table 6 shows the MAP results of the proposed model
across the three retrieval models. From the table, we can
see that the results are quite similar and they are all signiﬁcantly better than the baseline. This indicates that the
discriminative model is robust to the underlying document
retrieval method.

6.2 The Effect of the Size of Retrieved Documents
Similar to Model 2, the learned discriminative model can
be eﬃciently used on top of an existing document search
engine as follows: 1) Perform a standard document retrieval
run using the topic as a query and retrieve the top 𝑚 documents; 2) For each candidate associated with the relevant documents, calculate the probability of relevance using
Eqn. 6 on these 𝑚 documents. In this section, we aim to investigate the eﬀect of the size of documents retrieved on the
performance of the discriminative model. We use LM as the
document retrieval run. Figure 1 shows the MAP results by
varying 𝑀 on the two test collections. Note that the scales
on the x-axis and y-axis diﬀer per plot. From the ﬁgure, we
can see that as 𝑀 increases, the discriminative model has a
similar trend with the baseline: increasing, achieving a maximum, and then ﬂattening. On W3C, the MAP value tops
after 300 documents retrieved, fewer than what the baseline
needs (i.e., 400). For CERC, both models need around 50
documents for best performance. Therefore, using a subset
of documents could speed up the process of expert search as
the best performers use much less documents than the whole
set of relevant documents. At the same time, the retrieval
performance can be improved although their diﬀerences are
not found statistically signiﬁcant.

6.4

The Alternative Discriminative Model vs.
Base and AMD

In this section, we conduct the experiment to evaluate
the alternative discriminative model (GMD). The aim is
to investigate the robustness of the proposed discriminative
framework with respect to the choice of speciﬁc discriminative models derived from the framework. Table 7 contains the results. From the table, we can see that all the
results achieved by GMD signiﬁcantly outperform the baseline. Furthermore, these results are quite similar with those
achieved by the AMD (R4) model. In particular, the GMD
model is generally better than AMD on CERC and worse on
W3C, but the diﬀerences between GMD and AMD are not
statistically signiﬁcant. These results demonstrate that the
proposed discriminative framework generates accurate and
robust results with both types of discriminative models.

6.3 Experiments by Using Different Document
Retrieval Methods
As shown in Section 6.1 as well as in prior work, the doc-

689

7. CONCLUSIONS AND FUTURE WORK

[12] N. Craswell, A. de Vries, and I. Soboroﬀ. Overview of the
trec-2005 enterprise track. In TREC-13, 2005.
[13] J. Dennis and R. Schnabel. Numerical Methods for
Unconstrained Optimization and Nonlinear Equations.
Society for Industrial Mathematics, 1996.
[14] H. Fang and C. Zhai. Probabilistic models for expert
ﬁnding. In ECIR, 2007.
[15] Y. Fang, L. Si, and A. Mathur. Ranking experts with
discriminative probabilistic models. In SIGIR Workshop on
Learning to Rank for Information Retrieval, 2009.
[16] Y. Fu, W. Yu, Y. Li, Y. Liu, M. Zhang, and S. Ma. THUIR
at TREC 2005: Enterprise track. In TREC-14, 2006.
[17] N. Fuhr. Probabilistic models in information retrieval. The
Computer Journal, 35(3):243, 1992.
[18] T. Liu. Learning to rank for information retrieval.
Foundations and Trends in Information Retrieval,
3(3):225–331, 2009.
[19] T. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor:
Benchmark dataset for research on learning to rank for
information retrieval. In SIGIR Workshop on Learning to
Rank for Information Retrieval, 2007.
[20] C. Macdonald, D. Hannah, and I. Ounis. High quality
expertise evidence for expert search. In ECIR, 2008.
[21] C. Macdonald and I. Ounis. Voting for candidates:
adapting data fusion techniques for an expert search task.
In CIKM, 2006.
[22] D. Metzler and W. Bruce Croft. Linear feature-based
models for information retrieval. Information Retrieval,
10(3):257–274, 2007.
[23] R. Nallapati. Discriminative models for information
retrieval. In SIGIR, 2004.
[24] A. Ng and M. Jordan. On discriminative vs. generative
classiﬁers: a comparison of logistic regression and naive
bayes. NIPS, 2002.
[25] D. Petkova and W. Croft. Proximity-based document
representation for named entity retrieval. In CIKM, 2007.
[26] S. Robertson. The probability ranking principle in IR.
Journal of documentation, 33(4):294–304, 1977.
[27] S. Robertson and K. Jones. Relevance weighting of search
terms. JASIST, 27(3):129–146, 1976.
[28] S. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu,
and M. Gatford. Okapi at TREC-4. In TREC-4, 1996.
[29] P. Serdyukov and D. Hiemstra. Being omnipresent to be
almighty: The importance of the global web evidence for
organizational expert ﬁnding. In SIGIR Workshop on
Future Challenges in Expertise Retrieval, 2008.
[30] P. Serdyukov and D. Hiemstra. Modeling documents as
mixtures of persons for expert ﬁnding. In ECIR, 2008.
[31] P. Serdyukov, H. Rode, and D. Hiemstra. Modeling
multi-step relevance propagation for expert ﬁnding. In
CIKM, 2008.
[32] I. Soboroﬀ, A. de Vries, and N. Craswell. Overview of the
trec-2006 enterprise track. In TREC-14, 2006.
[33] T. Strohman, D. Metzler, H. Turtle, and W. Croft. Indri: A
language model-based search engine for complex queries. In
International Conference on Intelligence Analysis, 2004.
[34] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su.
Arnetminer: Extraction and mining of academic social
networks. In SIGKDD, 2008.
[35] D. Tax, M. Van Breukelen, R. Duin, and J. Kittler.
Combining multiple classiﬁers by averaging or by
multiplying? Pattern recognition, 33(9):1475–1485, 2000.
[36] D. Yimam-Seid and A. Kobsa. Expert ﬁnding systems for
organizations. Sharing Expertise: Beyond Knowledge
Management, 2003.
[37] C. Zhai and J. Laﬀerty. A study of smoothing methods for
language models applied to information retrieval. TOIS,
22(2):214, 2004.
[38] J. Zhu, X. Huang, D. Song, and S. Ruger. Integrating
multiple document features in language models for expert
ﬁnding. Knowledge and Information Systems, pages 1–26.

In this work, we propose a discriminative learning framework and derive speciﬁc models for expert search. The
main advantage of the proposed approaches is their ability
to integrate a variety of document evidence and documentcandidate association features. The evaluations on two TREC
Enterprise track testbeds have shown the eﬀectiveness and
robustness of the proposed framework.
There are several possibilities to extend the research in
this paper. We chose “out-of-order” training in the experiments because more training data are available in 2006 and
2008. It would be interesting to perform the “in-order” experiments (i.e., training on 2005 or 2007), which would allow
fair comparisons with the TREC submitted runs. The relevance judgments in 2005 and 2007 seem also more likely to
be obtained in a real enterprise. In fact, lack of training data
hinders the applicability of many discriminative models. On
the other hand, generative models may be able to eﬀectively
utilize abundant unlabeled data. It is desirable to develop
a hybrid of discriminative and generative models to obtain
the best of both for expert search. In addition, in certain
scenarios, pairwise comparisons between experts might be
more easily collectible than the pointwise judgment for each
expert. We will explore to extend the proposed discriminative learning framework to handle this type of training
data.

8. ACKNOWLEDGMENTS
We thank the anonymous reviewers for many valuable
comments. This research was partially supported by a grant
from the Indiana Economic Development Company, the NSF
research grant IIS-0749462, and a grant from Purdue University. Any opinions, ﬁndings, conclusions, or recommendations expressed in this paper are the authors’, and do not
necessarily reﬂect those of the sponsor.

9. REFERENCES
[1] P. Bailey, N. Craswell, A. De Vries, and I. Soboroﬀ.
Overview of the trec-2007 enterprise track. In TREC-15,
2007.
[2] K. Balog. Non-local evidence for expert ﬁnding. In CIKM,
2008.
[3] K. Balog, L. Azzopardi, and M. de Rijke. Formal models
for expert ﬁnding in enterprise corpora. In SIGIR, 2006.
[4] K. Balog, L. Azzopardi, and M. de Rijke. A language
modeling framework for expert ﬁnding. Information
Processing & Management, 45(1):1–19, 2009.
[5] K. Balog and M. de Rijke. Finding experts and their details
in e-mail corpora. In WWW, page 1036. ACM, 2006.
[6] K. Balog and M. De Rijke. Associating people and
documents. In ECIR, 2008.
[7] K. Balog, I. Soboroﬀ, P. Thomas, N. Craswell, A. de Vries,
and P. Bailey. Overview of the trec-2008 enterprise track.
In TREC-16, 2008.
[8] Y. Cao, J. Liu, S. Bao, and H. Li. Research on expert search
at enterprise track of TREC 2005. In TREC-13, 2005.
[9] P. Carlile. Working knowledge: how organizations manage
what they know. Human Resource Planning, 21(4):58–60,
1998.
[10] H. Chen, H. Shen, J. Xiong, S. Tan, and X. Cheng. Social
network structure behind the mailing lists: Ict-iiis at trec
2006 expert ﬁnding track. In TREC-14, 2006.
[11] W. Cooper. Exploiting the maximum entropy principle to
increase retrieval eﬀectiveness. JASIST, 34(1):31–39.

690

