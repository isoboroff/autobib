Visual Concept-based Selection of Query Expansions
for Spoken Content Retrieval
Stevan Rudinac, Martha Larson, Alan Hanjalic
Multimedia Information Retrieval Lab, Delft University of Technology, Delft, The Netherlands

{s.rudinac, m.a.larson, a.hanjalic}@tudelft.nl

visual concepts are potentially powerful indicators of the similarity of videos with respect to their semantic themes. In this initial
realization, we focus on exploiting the effects of concept presence
and frequency.
In order to combine speech-based retrieval with visual concepts, we select a coherence-based query prediction framework
[4]. This technique automatically chooses between results lists
produced by a range of different query expansions by making use
of a coherence-based indicator calculated over top documents of
the results list returned by each expansion. Our specific innovation is use of concept-based representations of videos for calculation of the query prediction indicator. In the following, we first
introduce our concept-based video representations and our query
prediction method. Then we report on results of experiments that
confirm the viability of our approach for effectively exploiting
visual concepts to refine spoken content retrieval of videos at the
semantic theme level.

ABSTRACT
In this paper we present a novel approach to semantic-themebased video retrieval that considers entire videos as retrieval units
and exploits automatically detected visual concepts to improve the
results of retrieval based on spoken content. We deploy a query
prediction method that makes use of a coherence indicator calculated on top returned documents and taking into account the information about visual concepts presence in videos to make a
choice between query expansion methods. The main contribution
of our approach is in its ability to exploit noisy shot-level concept
detection to improve semantic-theme-based video retrieval. Strikingly, improvement is possible using an extremely limited set of
concepts. In the experiments performed on TRECVID 2007 and
2008 datasets our approach shows an interesting performance
improvement compared to the best performing baseline.
Categories and Subject Descriptors: H.3.3 [Information Storage
and Retrieval]: Information Search and Retrieval

2. APPROACH

General Terms: Algorithms, Performance, Experimentation

The input for creating video-level representations is a vector in
which each component represents a single concept. In order to
weight each concept, we make use of term frequency (TF) and
inverse document frequency (IDF) analogously to their use in text
retrieval. However, concept detectors do not output binary concept occurrences, but rather shot-level lists of confidence scores.
Each score represents the probability that a particular concept
occurred in each shot. Our concept weights are created by accumulating the confidence scores for each concept over all shots in
the video and then normalizing with the total shot count.
Following the previous feature generation step, we select in
the next (feature selection) step the concepts that are potentially
the most helpful in discriminating between videos with respect to
their semantic themes. Because the output of concept detectors is
notoriously noisy (in TRECVID 2009 the best performance failed
to exceed 0.25 in the terms of MAP [5]), feature selection is a key
aspect of our approach since it introduces a noise control effect.
We developed our feature selection method by performing exploratory experimentation. Our experiments make use of TRECVID 2007 dataset and 46 semantic theme labels introduced by the
VideoCLEF 2009 (www.multimediaeval.org) benchmark. The
labels are manually assigned by professional archivists from the
Netherlands Institute for Sound and Vision. We use a set of publicly available concept detection scores, generated for a set of 374
concepts selected from the LSCOM (www.lscom.org) ontology.
In order to determine the most representative concepts, we trained
classifiers that can identify videos related to each semantic theme
and ranked the concepts according to their usefulness to the classifier. A simple voting approach was then applied to merge the lists
into a single list that ranked the concept from most to least valuable for semantic discrimination. Our investigation revealed that it

Keywords: Semantic-theme-based video retrieval, video-level
retrieval, query expansion, concept-based video indexing, query
performance prediction

1. INTRODUCTION
The semantic theme (or subject matter) of a video is encoded in
both its speech track and visual channel. This paper presents a
novel multimodal retrieval approach aiming at videos covering the
semantic theme expressed by the query. The approach improves
the results of a speech-based retrieval system by exploiting concepts (e.g. human, car, house, female, children, indoor) detected
in the visual channel. In contrast with most previous works on
video retrieval, which focus on shot-level retrieval, e.g., [2, 5], our
approach is designed to retrieve entire videos. These larger retrieval units are more appropriate than shots in cases where the
searcher is looking for informational material or for entertainment, typical for the semantic-theme-based retrieval scenario
(e.g., find a video about archaeology or psychology). Recent
work on retrieval beyond the shot level includes [1]. Moving from
the shot to the video level requires the combination of multiple
shot-level concepts into an effective video-level representation.
The novel contribution of our work is the successful use of such a
video-level representation to combine the output of automatic
speech recognition and visual-concept detection, both known to
be noisy, and achieve an overall improvement in retrieval of videos on the basis of semantic theme. The key insight motivating
our approach is that the presence, frequency and co-occurrence of
Copyright is held by the author/owner(s).
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
ACM 978-1-60558-896-4/10/07.

891

is the most frequently occurring concepts in the videos that best
support discrimination between videos in terms of semantic class.
We use this result to select features in the coherence-based query
prediction step. The fact that the most frequent concepts are most
discriminative suggests that it is not so much the occurrence of a
particular visual concept in a video that distinguishes that video's
semantic class, but rather its relative frequency.
Our approach compares results of multiple query expansions
and returns, as the final output, the results list with the highest
coherence score over the Top N videos. The coherence score is
calculated as:
∑ i ≠ j∈{1,..., N} δ ( vi , v j )
Co(TopN ) =
(1)
1
N ( N − 1)
2
Here δ is a function defined on the pair of videos (vi,vj) that is
equal to 1 if their similarity is higher than the similarity of particularly close video pairs from the collection (i.e., closer than TP% of
pair-wise similarities, where TP is the threshold defined below).
The sum is taken over all video pairs in the set of Top N videos.
As a similarity measure between vectors of concept frequencies
we used the cosine similarity. Use of alternative similarity/distance measures such as Kullback-Leibler divergence and
Euclidean distance yielded similar results.

Table 2. MAPs After Query Expansion Selection (QES)
QES
Best Baseline

5. CONCLUSIONS AND OUTLOOK
We have proposed a multimodal approach to semantic-themebased retrieval of entire videos that exploits frequencies of (semantic) concepts detected in a video to enhance the initial retrieval result obtained at the spoken-content level. We have demonstrated that our approach can be effectively used to decide
whether the query should be expanded and which of several query
expansions to use. Further, we are making use of only a fraction
(5-10) of the set of available concepts (374). This result suggests
that concept detectors that focus on a very small number of concepts have large potential to be useful for improving the results of
semantic-theme-based video retrieval. In our future work we will
further study the characteristics of concept detector output that
contribute to effective performance of our approach, investigating,
for example, whether the relatively larger performance improvement achieved on the TRECVID 2008 set (cf. Table 2) can be
attributed to better performing concept detectors. We will also
work to take into account concept co-occurrences and to combine
proposed concept-based and text-based indicators to further improve query prediction.

We test our approach on TRECVID 2007 and TRECVID 2008
datasets, using the 46 VideoCLEF 2009 semantic labels as queries. We index the Dutch speech recognition transcripts and the
English machine translation and carry out retrieval using the Lemur toolkit. The initial results lists are produced using the original
query and three query expansions: 1) Conventional PRF, where
the number of feedback documents and terms used for expansion
are selected for the optimal performance, 2) WordNet
(http://wordnet.princeton.edu/) expansion and 3) Google Sets
(http://labs.google.com/sets) expansion, where each query is expanded with a set of up to 15 related items (words or multi-word
phrases). For each query, the results list yielding the highest coherence indicator is selected. In the (rare) cases of the same indicator values the priority is given to the baseline or the expansions
following the ordering as above. We use the concept detection
output provided by [3], as mentioned above. In the experiments on
both collections we swept the parameter space for the following
parameters: number of most frequent (discriminative) semantic
concepts (NC), number of documents from the top of results list
(N) and the threshold TP used to calculate the coherence score.
We reported the results for the optimal parameter setting.

6. ACKNOWLEDGMENTS
The research leading to these results has received funding from
the European Commission's 7th Framework Programme (FP7)
under grant agreement n° 216444 (NoE PetaMedia).

7. REFERENCES
[1] Aly, R., Doherty, A., Hiemstra, D., and Smeaton, A. 2010.
Beyond shot retrieval: searching for broadcast news items using language models of concepts. In ECIR, Milton Keynes,
UK, 2010.
[2] Hsu, W. H., Kennedy, L. S., and Chang, S. 2006. Video
search reranking via information bottleneck principle. In
ACM MM, Santa Barbara, CA, USA, 2006.

4. EXPERIMENTAL RESULTS
The quality of the initial results list for the original query and
three expansion methods is reported in terms of Mean Average
Precision (MAP) in Table 1.
Table 1. MAPs of the Baseline and Expansion Methods
Baseline
0.326
0.245

PRF
0.332
0.265

WordNet
0.260
0.268

TRECVID 2008
0.296
0.268

The results confirm the viability of exploiting visual concepts
for refining the output of spoken-content-based video retrieval at
the level of a semantic theme. Recall that our feature selection
approach was optimized using TRECVID 2007 as a development
set. The fact that TRECVID 2008 yielded similar performance
demonstrates the ability of our feature selection method to generalize to new data. The optimal parameter settings for TRECVID
2007 and TRECVID 2008 are not the same for both datasets, but
are in the similar range: N=5-10, TP=80-90%, NC = 5-10.

3. EXPERIMENTAL SETUP

TRECVID 2007
TRECVID 2008

TRECVID 2007
0.355^
0.332

[3] Jiang, Y-G., Yanagawa, A., Chang, S-F., and Ngo, C-W.
2008. CU-VIREO374: Fusing Columbia374 and VIREO374
for Large Scale Semantic Concept Detection. Columbia University ADVENT Technical Report #223-2008-1.

Google Sets
0.120
0.142

[4] Rudinac, S., Larson, M., and Hanjalic, A. 2010. Exploiting
result consistency to select query expansions for spoken content retrieval. In ECIR, Milton Keynes, UK, 2010.

Table 2 contains the MAPs of our concept-based selection of
query expansion for TRECVID 2007 and TRECVID 2008 datasets. Statistical significance w.r.t. Wilcoxon Signed Rank Test, p
< 0.02 is indicated with ‘^’.

[5] Snoek, C. G. M., van de Sande, K. E. A., de Rooij, O., et al.
2009. The MediaMill TRECVID 2009 semantic video search
engine. In TRECVID Workshop, Gaithersburg, USA, 2009.

892

