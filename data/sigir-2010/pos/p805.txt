Feature Subset Non-Negative Matrix Factorization and its
Applications to Document Understanding
Dingding Wang

Chris Ding

Tao Li

School of Computer Science
Florida International University
Miami, FL 33199

CSE Department
University of Texas at Arlington
Arlington, TX 76019

School of Computer Science
Florida International University
Miami, FL 33199

dwang003@cs.fiu.edu

taoli@cs.fiu.edu

chqding@uta.edu

ABSTRACT

2.

In this paper, we propose feature subset non-negative matrix
factorization (NMF), which is an unsupervised approach to
simultaneously cluster data points and select important features. We apply our proposed approach to various document
understanding tasks including document clustering, summarization, and visualization. Experimental results demonstrate the eﬀectiveness of our approach for these tasks.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval.
General Terms: Algorithms, Experimentation.
Keywords: Feature subset selection, NMF.

FEATURE SUBSET NON-NEGATIVE MATRIX FACTORIZATION (FS-NMF)

Let X = {x1 , · · · , xn ) contains n documents with m keywords (features). In general, NMF factorizes the input nonnegative data matrix X into two nonnegative matrices,
X ≈ F GT ,

where G ∈ Rn×k
is the cluster indicator matrix for cluster+
m×k
ing columns of X and F = (f1 , · · · , fk ) ∈ R+
contains k
cluster centroids. In this paper, we propose a new objective
to simultaneously factorize X and rank the features in X as
follows:
min

W ≥0,F ≥0,G≥0

||X − F GT ||2W , s.t.

∑

Wjα = 1

(1)

j

where W ∈ Rm×m
which is a diagonal matrix indicating the
+
weights of the rows (keywords or features) in X, and α is a
parameter (set to 0.7 empirically).

1. INTRODUCTION
Keyword (Feature) selection enhances and improves many
IR tasks such as document categorization, automatic topic
discovery, etc. Many supervised keyword selection techniques have been developed for selecting keywords for classiﬁcation problems. In this paper, we propose an unsupervised approach that combines keyword selection and document clustering (topic discovery) together.
The proposed approach extends non-negative matrix factorization (NMF) by incorporating a weight matrix to indicate the importance of the keywords. This work considers
both theoretically and empirically feature subset selection
for NMF and draws the connection between unsupervised
feature selection and data clustering.
The selected keywords are discriminant for diﬀerent topics
in a global perspective, unlike those obtained in co-clustering,
which typically associate with one cluster strongly and are
absent from other clusters. Also, the selected keywords are
not linear combinations of words like those obtained in Latent Semantic Indexing (LSI): our selected words provide
clear semantic meanings of the key features while LSI features combine diﬀerent words together and are not easy to
interpret. Experiments on various document understanding
applications demonstrate the eﬀectiveness of our proposed
approach.

2.1

Optimization

We will optimize the objective with respect to one variable
while ﬁxing the other variables. This procedure repeats until
convergence.

2.1.1

Computation of W
Optimizing Eq.(1) with respect to W is equivalent to optimizing
J1 =

∑
i

∑
∑
Wi bi − λ(
Wiα − 1), bi =
(X − F GT )2ij .
i

j

∂J1
Now, setting ∂W
= bi − λαWiα−1 = 0, we obtain the foli
lowing updating formula

1


Wi =  ∑

1
α
α−1

α



1

biα−1

(2)

i bi

2.1.2

Computation of G
Optimizing Eq.(1) with respect to G is equivalent to optimizing
J2 = T r(X T W T X − 2GF T W T X + F T W T F GT G).
2
= −2X T W F + 2GF T W T F = 0, we obtain the
Setting ∂J
∂G
following updating formula

Gik ← Gik
Copyright is held by the author/owner(s).
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
ACM 978-1-60558-896-4/10/07.

(X T W F )ik
(GF T W F )ik

(3)

The correctness and convergence of this updating rule can
be rigorously proved. Details are skipped here.

805

2.1.3 Computation of F
Optimizing Eq.(1) with respect to F is equivalent to optimizing
J3 = T r[W XX T − 2W XGF T + W F GT GF ].
3
Setting ∂J
= −2W XG + W F GT G + (GT GF T W )T = 0, we
∂F
obtain the following updating formula

Fik ← Fik

(W XG)ik
(W F GT G)ik

(4)

First of all, we examine the clustering performance of
FS-NMF using four text datasets as described in Table 1,
and compare the results of FS-NMF with seven widely used
document clustering methods: (1) K-means; (2) PCA-Km:
PCA is ﬁrstly applied to reduce the data dimension followed by the K-means clustering; (3) LDA-Km [2]: an adaptive subspace clustering algorithm by integrating linear discriminant analysis (LDA) and K-means; (4)Euclidean coclustering (ECC) [1]; (5) minimum squared residueco-clustering
(MSRC) [1]; (6) Non-negative matrix factorization (NMF) [5];
(7) Spectral Clustering with Normalized Cuts (Ncut) [9].
More description of the datasets can be found in [6]. The
accuracy evaluation results are presented in Figure 2.
# Dimensions
1000
200
1000
1000

WebACE

Log

Reuters

CSTR

0.4081
0.4432
0.4774
0.4081
0.4432
0.4774
0.4513
0.5577

0.6979
0.6562
0.7198
0.7228
0.5655
0.7608
0.7574
0.7715

0.4360
0.3925
0.5142
0.4968
0.4516
0.4047
0.4890
0.4697

0.5210
0.5630
0.5630
0.5210
0.5630
0.5630
0.5435
0.6996

R-L

R-W

R-SU

0.386
0.345
0.361
0.375
0.349
0.367
0.381

0.133
0.117
0.124
0.131
0.120
0.129
0.139

0.132
0.117
0.125
0.130
0.119
0.129
0.134

3.3

Visualization

In this set of experiments, we calculate the pairwise document similarity using the top 20 word features selected
by diﬀerent methods, and Figure 1 demonstrates the document similarity matrix visually. Note that in the document
dataset (CSTR dataset), we order the documents based on
their class labels.
50

100

150

50

50

100

100

150

150

200

200

250

250

300

300

200

250

300
350

350

350
400

400

400
450
50

100

150

200

250

300

350

400

(a) FS-NMF

# Class
4
8
10
20

450
50

100

150

200

250

300

350

(b) NMF

400

450

50

100

150

200

250

300

350

400

450

(c) LSI

Figure 1:
Visualization results on the CSTR
Dataset. Note that CSTR has 4 clusters.
From Figure 1, we have the following observations. (1)
Word features selected by FS-NMF can eﬀectively reﬂet the
document distribution. (2) NMF tends to select some irrelevant or redundant words thus Figure 1(b) shows no obvious
patterns at all. (3) LSI can also ﬁnd meaningful words,
however, the ﬁrst two clusters are not clearly discovered in
Figure 1(c).
Acknowledgements: The work is partially supported
by an FIU Dissertation Year Fellowship and NSF grants
DMS-0915110 and DMS-0915228.

Table 1: Dataset descriptions.
K-means
PCA-Km
LDA-Km
ECC
MSRC
NMF
Ncut
FS-NMF

R-2
0.092
0.063
0.073
0.085
0.065
0.072
0.101

From the results, we observe that the summary generated
by FS-NMF outperforms those created by other methods,
and the scores are even higher than the best results in DUC
competition. The good results beneﬁt from good sentence
feature selection in FS-NMF.

3.1 Document Clustering

# Samples
475
1367
2900
2340

R-1
0.382
0.318
0.367
0.378
0.341
0.367
0.388

Table 3:
Overall performance comparison on
DUC2004 data using ROUGE evaluation methods.

3. EXPERIMENTS

Datasets
CSTR
Log
Reuters
WebACE

Systems
DUC Best
Random
Centroid [8]
LexPageRank [3]
LSA [4]
NMF [5]
FS-NMF

4.

Table 2: Clustering accuracy on text datasets.

REFERENCES

[1] H. Cho, I. Dhillon, Y. Guan, and S. Sra. Minimum sum
squared residue co-clustering of gene expression data. In
Proceedings of SDM 2004.
[2] C. Ding and T. Li. Adaptive dimension reduction using
discriminant analysis and k-means c lustering. In ICML, 2007.
[3] G. Erkan and D. Radev. Lexpagerank: Prestige in
multi-document text summarization. In EMNLP, 2004.
[4] Y. Gong and X. Liu. Generic text summarization using
relevance measure and latent semantic analysis. In SIGIR,
2001.
[5] D. D. Lee and H. S. Seung. Algorithms for non-negative matrix
factorization. In NIPS, 2000.
[6] T. Li and C. Ding. The relationships among various
nonnegative matrix factorization methods for clustering. In
ICDM, 2006.
[7] C.-Y. Lin and E.Hovy. Automatic evaluation of summaries
using n-gram co-occurrence statistics. In NLT-NAACL, 2003.
[8] D. Radev, H. Jing, M. Stys, and D. Tam. Centroid-based
summarization of multiple documents. Information Processing
and Management, pages 919–938, 2004.
[9] S. X. Yu and J. Shi. Multiclass spectral clustering. In ICCV,
2003.

From the results, we clearly observe that FS-NMF outperforms other document clustering algorithms in most of
the cases, and the eﬀectiveness of FS-NMF for document
clustering is demonstrated.

3.2 Document Summarization
In this set of experiments, we apply our FS-NMF algorithm on document summarization. Let X be the documentsentence matrix, which can be generated from the documentterm and sentence-term matrices, and now each feature (column) in X represents a sentence. Then the sentences can
be ranked based on the weights in W . Top-ranked sentences are included into the ﬁnal summary. We use the
DUC benchmark dataset (DUC2004) for generic document
summarization to compare our method with other state-ofart document summarization methods using ROUGE evaluation toolkit [7]. The results are demonstrated in Table 3.

806

