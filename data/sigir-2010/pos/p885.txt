Achieving High Accuracy Retrieval using
Intra-Document Term Ranking
Hyun-Wook Woo†
hwwoo@nlp.korea.ac.kr

Jung-Tae Lee†
jtlee@nlp.korea.ac.kr

Hae-Chang Rim†
rim@nlp.korea.ac.kr

Young-In Song‡
yosong@microsoft.com
†

Dept. of Computer & Radio Comms. Engineering, Korea University, Seoul, South Korea
‡
Microsoft Research Asia, Beijing, China

ABSTRACT

provided at the top part of the ranked list. Despite its simplicity, such a scoring method has been eﬀective in many
previous document ranking experiments.
However, traditional models virtually do not consider the
relative importance of query terms compared to other individual non-query terms within a document in ranking. For
example, consider the query word “q” and the following two
equal-length documents at top ranks in which “q” occurs
twice: “q q b c d e f ” and “q q b b b b c”. Traditional ranking
models would assume that the weight of “q” is roughly the
same for both documents. However, this is indeed not true if
we examine the relative importance of “q” within individual
documents. In the ﬁrst document, “q” occurs relatively more
than any of the other non-query terms, which implies that
the document may be mainly about “q”. In contrast, in the
second document, “q” occurs relatively less than the word
“b”, which implies that “q” may be a subtopic of that document. Intuitively, given two documents that both contain
the same number of query terms, we would like to rank the
document in which the query is dedicated as a main topic
above the one where the query is regarded as a minor topic.
The relative importance of query terms with regard to
other terms in a document can be observed by ranking all
the terms in the document with some weighting scheme,
such as the tf-idf method. We refer to this technique as
intra-document term ranking. In this paper, we present two
heuristic document ranking methods based on the rank positions of given query terms in intra-document term ranking
lists of individual documents. Our hypothesis is two-fold:

Most traditional ranking models roughly score the relevance
of a given document by observing simple term statistics,
such as the occurrence of query terms within the document
or within the collection. Intuitively, the relative importance
of query terms with regard to other individual non-query
terms in a document can also be exploited to promote the
ranks of documents in which the query is dedicated as the
main topic. In this paper, we introduce a simple technique
named intra-document term ranking, which involves ranking
all the terms in a document according to their relative importance within that particular document. We demonstrate
that the information regarding the rank positions of given
query terms within the intra-document term ranking can be
useful for enhancing the precision of top-retrieved results by
traditional ranking models. Experiments are conducted on
three standard TREC test collections.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval—Retrieval models

General Terms
Algorithms, Experimentation

Keywords
inter-document term ranking, precision at top ranks

1.

Seung-Wook Lee†
swlee@nlp.korea.ac.kr

• First, when query terms are ranked in relatively higher
term rank positions than other terms in a given document, it implies that the query is dedicated as the
central topic of the document. Thus, there is a higher
chance that the document is relevant to the query.

INTRODUCTION

With the rapid growth of Web document collection sizes
in recent years, achieving high precision at the top of the
retrieved result has become a major issue for search engine
users [1]. In traditional IR ranking models [2, 3], the relevance of a document for a given query is scored primarily
based on simple term statistics, such as the number of occurrence of query terms within the document or within the
whole document collection. Basically, the more query terms
occur in a document, the higher the chance that the document would be relevant to the query and thus would be

• Second, individual query terms having similar term
rank positions to each other in a document reﬂect that
they are treated equally. Thus, there is a chance that
the query terms have close relationships to each other
within the document, which implies higher relevance.
The succeeding section will elaborate on how we rank
documents based on the two hypotheses. We validate our
method on three standard TREC test collections.

Copyright is held by the author/owner(s).
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
ACM 978-1-60558-896-4/10/07.

885

2.

PROPOSED METHOD

Table 1: Retrieval performance.
Method
MRR
P@1
P@5
LM
0.4598
0.3100
0.2840
0.4433
0.3000
0.2740
+R1
(-3.59%) (-3.23%) (-3.52%)
AP88-90
0.4671
0.3200
0.2900
+R2
(1.59%)
(3.23%)
(2.11%)
0.4743
0.3200
0.3020
+R1 +R2
(3.15%)
(3.23%)
(6.34%)
LM
0.6342
0.6000
0.4880
0.6874
0.5800
0.4640
+R1
(8.39%)
(-3.33%) (-4.92%)
WT2g
0.7151
0.6200
0.4640
+R2
(12.76%) (3.33%)
(-4.92%)
0.7097
0.6200
0.5000
+R1 +R2
(11.90%) (3.33%)
(2.64%)
LM
0.6634
0.5333
0.5747
0.7090
0.6000
0.5987
+R1
(6.87%) (12.51%) (4.18%)
Blogs06
0.7404
0.6467
0.6067
+R2
(11.61%) (21.26%) (5.57%)
0.7384
0.6400
0.6453
+R1 +R2
(11.31%) (20.01%) (12.28%)

To analyze the relative importance of individual terms
that occurred in a given document, we rank the terms by
their standard tf-idf weights. We use the term frequency
normalized with the document length as the tf component,
and the logarithm of the inverse document frequency as the
idf component. We normalize the ranks in range 0 to 1 so
that the term with highest tf-idf weight will be at rank 0
and the term with the lowest weight at rank 1.
Given this ranked list of terms within a document, we
derive two document ranking heuristics based on the rank
positions of query terms within the list. The ﬁrst heuristic,
which corresponds to our ﬁrst hypothesis, ranks documents
according to the average rank position of query terms. Here
we basically assume that the higher the rank of the query
term within a document, the more likely that the document
is relevant. Given a query Q and the term ranking result of
a document D, the average rank position of query terms is
calculated as follows:

qi ∈Q {1 − rank(qi , D)}
R1 (Q, D) =
(1)
|Q|

Corpus

where qi represents ith query term, |Q| is the size of the
query in words, and rank(qi , D) is a function that returns
the normalized term rank of qi in D.
The second heuristic, which corresponds to our second
hypothesis, assumes that the more cohesive the rank positions of query terms (i.e. have similar rank positions to
each other) in a document, the higher the probability that
the document is relevant. There are a number of ways to
measure the cohesiveness of the term ranks. In this paper,
we calculate such measure by the maximum rank diﬀerence
of all pairs of query terms as follows:
R2 (Q, D) = 1 − {

max

qi ,qj ∈Q∩D,qi =qj

{dif f (qi , qj , D)}}

ranked list consisting of top n documents from a test collection using LM, which represents a state-of-the-art baseline.
Then, we create a new ranked list of the n documents using
each heuristic function. We ﬁnally re-rank the documents
in the initial list in the ascending order of the mean rank of
documents among individual ranked lists. If a tie occurs, a
document that has a higher rank in the initial ranked list
is promoted. We compare the top results of the re-ranked
result with the baseline result using Mean Reciprocal Rank
(MRR) and Precision at k ranks (P@1 and P@5). The Indri
implementation2 is used for indexing and retrieval; stemming and stopword removal are performed.
The retrieval performances of the baseline and the proposed method are presented in Table 1. We observe that
when the baseline ranking is aggregated with either one
of the heuristic rankings, the improvement is not consistent across diﬀerent data collections. However, when the
baseline ranking is merged with all two heuristic rankings,
the improvement over the baseline is consistent across all
collections for all evaluation measures. This demonstrates
that the analysis of rank positions of query terms in interdocument term ranking is eﬀective in improving the precision at top ranks. For future work, we plan to explore other
features that capture various aspects of inter-document term
ranking. We also plan to investigate on designing a uniﬁed
ranking model that combines the proposed heuristics with
the features of the traditional retrieval models.

(2)

where dif f (qi , qj , D) is a function that returns the absolute
value of the diﬀerence in rank positions of qi and qj . For
example, if three query terms were ranked at 0, 0.1, and 0.4,
the maximum diﬀerence would be 0.3.

3.

EXPERIMENTS

For test collections, we use the following standard TREC
collections: AP88-90 (Associated Press news 1988-90), WT2g
(2 gigabyte Web data), and Blogs06 (Web data used in
TREC Blog Track 2006-08). For queries, we use the title
ﬁeld of the TREC topics 50-150 for AP88-90, 401-450 for
WT2g, and 851-950 and 1001-1050 for Blogs06.
To investigate whether the two new heuristics can infer the
true relevance for documents at top ranks of the retrieved list
by traditional ranking models, we retrieved the top 20 documents from the collections using query-likelihood language
models [2] with dirichlet prior1 (LM) and computed the correlation between the output value of either of the heuristics
and the relevance. We observe that both have positive correlations with document relevance (0.21 for R1 and 0.31 for
R2 ). This result is consistent with our two hypotheses.
We validate the eﬀectiveness of the two ranking heuristics
in a re-ranking scheme. First, given a query, we generate a

4. REFERENCES
[1] I. Matveeva, C. Burges, T. Burkard, A. Laucius, and L. Wong.
High accuracy retrieval with multiple nested ranker. In SIGIR
’06, 2006.
[2] J. M. Ponte and W. B. Croft. A language modeling approach
to information retrieval. In SIGIR ’98, 1998.
[3] G. Salton, A. Wong, and C. S. Yang. A vector space model for
automatic indexing. Commun. ACM, 18(11):613–620, 1975.

1
We tuned µ to be optimal for each collection (1000, 3000,
and 5000 for AP88-90, WT2g, and Blogs06, respectively).

2

886

http://www.lemurproject.org/indri/

