High Precision Opinion Retrieval using
Sentiment-Relevance Flows
Seung-Wook Lee†
swlee@nlp.korea.ac.kr

Jung-Tae Lee†
jtlee@nlp.korea.ac.kr

Young-In Song‡
yosong@microsoft.com

Hae-Chang Rim†
rim@nlp.korea.ac.kr
†

Dept. of Computer & Radio Comms. Engineering, Korea University, Seoul, South Korea
‡
Microsoft Research Asia, Beijing, China

ABSTRACT
Opinion retrieval involves the measuring of opinion score of a
document about the given topic. We propose a new method,
namely sentiment-relevance flow, that naturally unifies the
topic relevance and the opinionated nature of a document.
Experiments conducted over a large-scaled Web corpus show
that the proposed approach improves performance of opinion
retrieval in terms of precision at top ranks.

Categories and Subject Descriptors

Figure 1: Illustration of various flows.

H.3.3 [Information Search and Retrieval]: Retrieval
Models

In this paper, we present a new opinion retrieval method
that adopts a very recently proposed technique involving
relevance flow graphs [2]. A relevance flow graph of a document is a graphical plot of the topic relevance degree of
individual sentences (with regard to a query) versus their
positions in the document. The line graph labeled “Relevance” in Figure 1 illustrates an example; it visually shows
the fluctuation of topic relevance levels within a document in
regard to the query. [2] demonstrates that topically relevant
documents have distinguishable flows from non-relevant ones
in terms of the variance of relevance levels or the positions
of high relevance levels (namely peaks), by designing a regression model based on such information and applying it to
re-rank the retrieved results of conventional document ranking. Such method has shown promising results in improving
the accuracy at top ranks.
Motivated by this achievement, we hypothesize that a
truly relevant document in opinion retrieval not only has
opinion sentences regarding the topic but does have them
in predictable patterns within the document. Our idea is
to create a new flow graph, namely sentiment flow, that
plots the opinionated nature of individual sentences using
some opinion scoring method, and then merge it with the
topic relevance flow to generate a whole new flow graph,
called sentiment-relevance flow. The dotted line graph labeled “Sent-Rel” in Figure 1 illustrates an example. The
following section will elaborate on such technique.

General Terms
Algorithms, Measurement, Experimentation

Keywords
opinion retrieval, sentiment analysis, sentiment-relevance flow

1.

INTRODUCTION

Opinion retrieval is a new retrieval task which involves
locating documents that express opinions about a topic of
interest. With the rapid growth of user-centric media such as
blogs and forums, opinion retrieval has been gaining considerable attention in recent years from academia and industry
motivated by the huge business opportunities.
A key to success in opinion retrieval is to leverage both
the topical relevance and the opinionated nature of documents simultaneously in ranking. However, most opinion retrieval systems separate the two components independently
by adopting a re-ranking approach [1]. This approach involves finding as many relevant documents with regard to
a given topic as possible regardless of their opinionated nature at first and then re-ranking them by combining the
topical relevance scores with the opinion scores computed
using some opinion detection techniques. Although a few
approaches have shown attempts to unify the two components, for example, by using the proximity between topic
words and opinion words within a document [3], the results
are not yet conclusive and require more investigation.

2. SENTIMENT-RELEVANCE FLOW
The sentiment-relevance flow of a document (SRF) is a
sequence of scores that reflect both the topic relevance with
regard to the query and the opinionated nature of individual

Copyright is held by the author/owner(s).
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
ACM 978-1-60558-896-4/10/07.

817

sentences, ordered by the sentence positions. Given a query
Q, we calculate the score of a sentence si at position i as
follows:
score(Q, si ) = topic(Q, si ) · opinion(si )

Table 1: Performance of topical retrieval
Method
λ
P@1
P@3
P@5
BM25
N/A 0.5800 0.6334 0.6041
BM25+RF
0.15 0.6333 0.6422 0.6427
BM25+SRF 0.00 0.7200 0.7089 0.6907
QL
N/A 0.5933 0.6200 0.6160
QL+RF
0.02 0.6333 0.6711 0.6587
QL+SRF
0.01 0.7600 0.6956 0.6933

(1)

where topic(Q, si ) refers to the topic relevance score of si
for Q calculated using some conventional relevance scoring
function, such as the BM25 function. opinion(si ) represents
the opinion score of si computed as follows:
i+W
X

opinion(si ) =

|O|
X

f req(sj , owk )

Table 2: Performance of opinion retrieval
Method
λ
P@1
P@3
P@5
BM25
N/A 0.4400 0.4556 0.4320
BM25+SRF 0.01 0.5800 0.5800 0.5573
QL
N/A 0.4800 0.4756 0.4640
QL+FULL
N/A 0.5600 0.5222 0.5373
QL+PROX N/A 0.5800 0.5556 0.5547
QL+SRF
0.01 0.6333 0.5689 0.5520

(2)

j=i−W k=1

where W is the half size of the context window (empirically set to 15), O is a set of opinion word lexicon, and
f req(sj , owk ) is a function that returns the frequency of
opinion word owk in sentence sj . When computing this
score, we look at not only the sentence at hand but also
its context, because we have observed that opinions often
appear not in the same sentence where topic words occur
but in preceding or succeeding sentences.
We normalize the individual sentence scores and the sentence positions in range 0 to 1 as in [2]. The graphical plot
of such SRF, as shown in Figure 1, indicates the fluctuation
of both topic relevance and opinionated nature within the
document in a comprehensive way. As in [2], we refer to
sentences that have scores higher than 0.5 as peaks.
In order to infer document relevance from SRFs, we use
maximum entropy modeling to train a regression model that
is able to predict the relevance of a document based on its
SRF. The main features extracted from the SRFs for regression are the ones found useful in [2], which include the
following: the variance of sentence scores, the fraction of
peaks, and the first peak position.
For opinion retrieval, we rank documents by linearly combining the topic relevance scores with the new relevance
scores inferred from their SRFs as follows:
score(Q, D) = λ · topic(Q, D) + (1 − λ) · srf (Q, D)

relevance. Thus, we report the topic P@Ns and the opinion
P@Ns of the system separately. For sentence boundary detection, we use a public sentence splitter software2 . We use
the sentiment word list in the General Inquirer3 , which is
a public opinion dictionary in the linguistics field. We only
collect adjectives, adverbs, and verbs from the opinion and
emotion categories; as a result, the lexicon is made up of
1,496 entries.
Table 1 shows the performance of topical retrieval for
the two initial results, relevance flow based re-ranking (RF)
and our sentiment-relevance flow based re-ranking (SRF).
As mentioned from previous study, RF successfully re-ranks
high position documents. It is notable that our SRF also improves the performance of traditional topical retrieval since
it basically aims to capture the pattern of relevant sentences.
In aspect of opinion retrieval, as shown in Table 2, SRF
shows significant and consistant improvement. We compare our method with two previous re-ranking approaches
for opinion retrieval with QL setting (since it outperforms
BM25). Opinion score measured by QL+FULL is dominated by a number of opinion words that appeared in a
whole document, while QL+PROX only considers opinion
words located within W sentences from the query terms.
We can observe that proximity is a helpful feature for opinion retrieval. It is remarkable that our sentiment-relevance
flow based re-ranking scheme achieves better improvement.
Note that the maximum performance are acheived on low λ
values which implies that the trained regression model based
on SRF features are very accurate and reliable.

(3)

where topic(Q, D) is the topic relevance of document D with
regard to Q, and srf (Q, D) is the prediction score of the
classifier for D. λ is a weight parameter where 0 ≤ λ ≤ 1.

3.

EXPERIMENT

We conduct our experiments over Blogs06, a large-scaled
Web blog collection. We use the title queries of 150 topics
used in 2006, 2007, and 2008 TREC Blog Tracks. We validate our method in a re-ranking scheme. In other words,
we initially retrieve the top n (=15) documents using two
popular ranking models, the BM25 model and the query
likelihood (QL) language model with dirichlet smoothing1 ,
and then re-rank the results using Equation 3. When evaluating one query set (50 topics), other two query sets (100
topics) are used to train a regression model. 15 top ranked
documents for every training query are divided into positive
and negative training instance groups based on their relevance judgments. The main evaluation measures are P@1,
P@3, and P@5 since our method aims to achieve high accuracy at top ranks via re-ranking. The relevance judgement
set consists of two distict aspects, relevance and opinionated

4. REFERENCES
[1] I. Ounis, C. Macdonald, and I. Soboroff. Overview of
the TREC-2008 blog track. In Proc. of TREC 2008.
[2] J. Seo and J. Jeon. High precision retrieval using
relevance-flow graph. In Proc. of SIGIR 2009.
[3] M. Zhang and X. Ye. A generation model to unify topic
relevance and lexicon-based sentiment for opinion
retrieval. In Proc. of SIGIR 2008.
2

1

We empirically tuned k1 =1.2, b=0.1 for BM25 and µ=5000
for QL smoothing parameter.

3

818

http://l2r.cs.uiuc.edu/˜ecogcomp/tools.php
http://www.wjh.harvard.edu/˜inquirer/

