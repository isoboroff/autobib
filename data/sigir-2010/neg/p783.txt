Text Document Clustering with Metric Learning
Jinlong Wang, Shunyao Wu

Huy Quan Vu, Gang Li

School of Computer Engineering
Qingdao Technological University
Qingdao, 266033, China

School of Information Technology
Deakin University
Victoria 3125, Australia

{hqv, gang.li}@deakin.edu.au

{wangjinlong, shunyaowu}@gmail.com

Moreover, when the priori knowledge contains some inconsistent constraints, existing methods might even not be able
to generate a feasible result because no solution can satisfy
all constraints.
In order to alleviate this problem so that the document
clustering can work more eﬀectively with inadequate priori
constraints, we propose a novel soft-constraint algorithm for
document clustering: instead of satisfying ALL constraints,
the method aims to satisfy constraints as much as possible. The proportion of satisﬁed constraints is adopted as a
heuristic to inform the search of the optimal solution. Experimental results show the eﬀectiveness of the proposed
method, especially when the number of priori constraints
are limited.

ABSTRACT
One reason for semi-supervised clustering fail to deliver satisfactory performance in document clustering is that the
transformed optimization problem could have many candidate solutions, but existing methods provide no mechanism
to select a suitable one from all those candidates. This paper alleviates this problem by posing the same task as a
soft-constrained optimization problem, and introduces the
salient degree measure as an information guide to control
the searching of an optimal solution. Experimental results
show the eﬀectiveness of the proposed method in the improvement of the performance, especially when the amount
of priori domain knowledge is limited.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Clustering

2. CLUSTERING DOCUMENT WITH METRIC LEARNING

General Terms

Let 𝒳 ∈ ℝ𝑑 stands for the data space which contains 𝑛
data points {x1 , ⋅ ⋅ ⋅ , xn }, x𝑖 = [𝑥𝑖1 , ⋅ ⋅ ⋅ , 𝑥𝑖𝑑 ]𝑇 . Let w =
[𝑤1 , ⋅ ⋅ ⋅ , 𝑤𝑑 ]𝑇 , and 𝑤𝑗 are the weights for attributes (𝑗 =
1, ..., 𝑑). For document clustering, we use the weighted 𝑐𝑜𝑠𝑖𝑛𝑒
⟨x,y⟩w
similarity and set 𝑑w (x, y) = 1− ∥x∥
, where ⟨x, y⟩w =
w ∥y∥w
√
∑𝑑
𝑤
𝑥
𝑦
and
∥x∥
=
⟨x,
x⟩
.
With
a must-link pairw
w
𝑗=1 𝑗 𝑗 𝑗
wise constraints set 𝒮 and a cannot-link pair-wise constraints
set 𝒟, the document clustering problem can be transformed
into an optimization problem with the objective as [5]:

Algorithm, Experimentation

Keywords
Document Clustering, Metric Learning

1.

INTRODUCTION

As one of the most fundamental data mining tasks, clustering is a subjective process in nature: diﬀerent users may
want diﬀerent clusterings when exploring the same data set.
However, specifying an appropriate similarity measure in advance is usually diﬃcult for general users. Recently, semisupervised clustering which can utilize priori pair-wise constraints has attracted a lot of research interest [5].
For the topic of document clustering, there have been
some pioneer work in applying semi-supervised clustering to
increase clustering quality [3]. However, their performance
is still not as good as expected, especially when the number
of priori constraints is limited [2, 3]. One of the possible reasons is that: existing work transforms the clustering into an
optimization problem, with priori constraints as hard constraints. When the number of constraints is inadequate,
there could be many candidate solutions to this optimization problem, and existing methods fail to provide a mechanism to select a suitable one from these possible solutions.

∑

𝑚𝑖𝑛w

𝑑w (x, y)

(x,y)∈𝒮

∑
subject to (x,y)∈𝒟 𝑑w (x, y) ≥ 𝐶 and w ≥ 0.
Considering the fact
∑ that any 𝑐𝑜𝑠𝑖𝑛𝑒 similarity is within
the range [0, 1], set
(x,y)∈𝒟 𝑑w (x, y) ≥ ∣𝐷∣ (∣𝐷∣ stands
for the number of elements in the cannot-link set 𝒟) in the
algorithm which enforces that every pair in the cannot-link
set 𝒟 is exactly 1, with an aim to make sure distances of
instances in the cannot-link set 𝒟 as large as possible.
During the search for an optimal set of weights, we introduce a new measure, the salient degree, to evaluate how
the current clustering result respects the priori constraints.
We utilize k -means method with metric 𝑑w (., .) to partition data, and get salient degree of clustering result. The
centroids in clustering
∑ process of k -means are estimated as

{𝜇ℎ }𝑘ℎ=1 [1]. 𝜇ℎ =

Copyright is held by the author/owner(s).
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
ACM 978-1-60558-896-4/10/07.

∥

∑

x𝑖 ∈𝜒ℎ

x𝑖 ∈𝜒ℎ

x𝑖

x𝑖 ∥w

, where, 𝜒ℎ represents points

assigned to the cluster ℎ. The salient degree is deﬁned as the
proportion of satisifed constraints by the clustering result

783

with parameter w, namely 𝑠𝑎𝑙𝑖𝑒𝑛𝑡(w) = ∣𝑠𝑎𝑡(𝒟)∣+∣𝑠𝑎𝑡(𝒮)∣
,
∣𝒮∣+∣𝒟∣
here 𝑠𝑎𝑡(∗) means the satisﬁed constraints in the set ∗.
Accordingly, the document clustering can be carried out
as an optimization problem, but with the salient degree as
a heuristic to decide whether it is necessary to carry out
further gradient descending search or not. The pseudo-code
of this algorithm is provided in Algorithm 1.
Algorithm 1: Document clustering with metric learning
Input: Dataset 𝒳 , number of output clusters 𝑘, mustlink constraints 𝒮, cannot-link constraints 𝒟.
Output: Clusters obtained with metric learning.
𝑡 = 1; w𝑡 = [ 1𝑑 , ..., 1𝑑 ];
while not convergent do
𝑡𝑡 = 1;
w𝑡𝑡 = w𝑡 ;
Step 1: Select initial cluster centroids;
Randomly select cluster centroids, and make sure
𝑠𝑎𝑙𝑖𝑒𝑛𝑡(w𝑡𝑡+1 ) − 𝑠𝑎𝑙𝑖𝑒𝑛𝑡(w𝑡𝑡 ) > 0,
where, w𝑡𝑡+1 = w𝑡𝑡 + 𝜂 ∗𝑔𝑟𝑎𝑑𝑖𝑒𝑛𝑡(w𝑡𝑡 ).
Step 2: Iteration for optimization;
while the value of objective function decreases and
𝑠𝑎𝑙𝑖𝑒𝑛𝑡(w𝑡𝑡+1 ) − 𝑠𝑎𝑙𝑖𝑒𝑛𝑡(w𝑡𝑡 ) > 0 do
w𝑡𝑡+1 = w𝑡𝑡 + 𝜂 ∗ 𝑔𝑟𝑎𝑑𝑖𝑒𝑛𝑡(w𝑡𝑡 );
𝑡𝑡 = 𝑡𝑡 + 1;
end
𝑡 = 𝑡 + 1; w𝑡 = w𝑡𝑡 ;
end

3.

(a) NMI

(b) Purity

Figure 1: Clustering result on 𝑁 𝑒𝑤𝑠 𝐷𝑖𝑓 𝑓 3

(a) NMI

(b) Purity

Figure 2: Clustering result on 𝑁 𝑒𝑤𝑠 𝑆𝑖𝑚𝑖 3

4. CONCLUSIONS

EXPERIMENTS AND RESULTS

Here we compare the performance of the proposed method
with k -means and the hard-constraint algorithm COP-Kmeans
implemented according to [4]. The 20Newsgroup dataset is
used in our experiment.
From original 20Newsgroup dataset, we randomly select
100 documents for each category, and create 2 datasets:
the 𝑁 𝑒𝑤𝑠 𝐷𝑖𝑓 𝑓 3 data set (alt.athei-sm, rec.sport.baseball,
sci.space) consisting of 3 clusters on 3 distinct topics, and the
𝑁 𝑒𝑤𝑠 𝑆𝑖𝑚𝑖 3 data set (comp.graphics, comp.os.ms-windows,
comp.windows.x) contains 3 clusters with large overlaps between them. All the datasets have been pre-processed by
removing stop-words, and words with too high or too low
frequency, and each document is then represented by TFIDF.
We run 10 trials of 2-fold cross-validation for each dataset:
50% of the dataset is used as training set to obtain pairwise constraints, and the other half is used as input of compared algorithms after peering oﬀ its class/clustering information. The clustering results are then compared with the
“ground truth” clustering using Normalized Mutual Information (𝑁 𝑀 𝐼) and 𝑃 𝑢𝑟𝑖𝑡𝑦 measures.
The results are shown as Figure 1 and Figure 2. On both
data sets, we can see that the proposed method outperforms
the other methods in both the 𝑁 𝑀 𝐼 and the 𝑃 𝑢𝑟𝑖𝑡𝑦 measures. Additionally, the results in our method are more
stable. Another important observation is that: When the
amount of priori knowledge is adequate, our method performs similarly with the compared methods; but when the
amount of priori knowledge is limited, our method can still
achieve satisfactory clustering results, while the performance
of other methods deterioriate signiﬁcantly.

784

This paper proposes an eﬃcient soft-constraint algorithm
by obtaining a satisfactory clustering result so that the constraints will be respected as many as possible. Experiments
show the advantage of the proposed algorithm especially
when provided with little priori domain knowledge, the proposed method is more robust and accurate than the existing
methods.

5. ACKNOWLEDGMENTS
This paper was partially supported by the National Natural Science Foundation of P.R.China (No.60802066), the Excellent Young Scientist Foundation of Shandong Province of
China under Grant (No.2008BS01009) and the Science and
Technology Planning Project of Shandong Provincial Education Department (No.J08LJ22).

6. REFERENCES
[1] S. Basu, M. Bilenko, and R. J. Mooney. A probabilistic
framework for semi-supervised clustering. In KDD ’04,
pages 59–68, 2004.
[2] I. Davidson, K. Wagstaﬀ, and S. Basu. Measuring
constraint-set utility for partitional clustering
algorithms. In PKDD ’06, pages 115–126, 2006.
[3] A. Huang, D. Milne, E. Frank, and I. H. Witten.
Clustering documents with active learning using
wikipedia. In ICDM ’08, pages 839–844, 2008.
[4] K. Wagstaﬀ, C. Cardie, S. Rogers, and S. Schroedl.
Constrained k-means clustering with background
knowledge. In ICML ’01, pages 577–584, 2001.
[5] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. J. Russell.
Distance metric learning with application to clustering
with side-information. In NIPS ’02, pages 505–512,
2002.

