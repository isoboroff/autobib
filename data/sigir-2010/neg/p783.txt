Text Document Clustering with Metric Learning
Jinlong Wang, Shunyao Wu

Huy Quan Vu, Gang Li

School of Computer Engineering
Qingdao Technological University
Qingdao, 266033, China

School of Information Technology
Deakin University
Victoria 3125, Australia

{hqv, gang.li}@deakin.edu.au

{wangjinlong, shunyaowu}@gmail.com

Moreover, when the priori knowledge contains some inconsistent constraints, existing methods might even not be able
to generate a feasible result because no solution can satisfy
all constraints.
In order to alleviate this problem so that the document
clustering can work more eï¬€ectively with inadequate priori
constraints, we propose a novel soft-constraint algorithm for
document clustering: instead of satisfying ALL constraints,
the method aims to satisfy constraints as much as possible. The proportion of satisï¬ed constraints is adopted as a
heuristic to inform the search of the optimal solution. Experimental results show the eï¬€ectiveness of the proposed
method, especially when the number of priori constraints
are limited.

ABSTRACT
One reason for semi-supervised clustering fail to deliver satisfactory performance in document clustering is that the
transformed optimization problem could have many candidate solutions, but existing methods provide no mechanism
to select a suitable one from all those candidates. This paper alleviates this problem by posing the same task as a
soft-constrained optimization problem, and introduces the
salient degree measure as an information guide to control
the searching of an optimal solution. Experimental results
show the eï¬€ectiveness of the proposed method in the improvement of the performance, especially when the amount
of priori domain knowledge is limited.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Clustering

2. CLUSTERING DOCUMENT WITH METRIC LEARNING

General Terms

Let ğ’³ âˆˆ â„ğ‘‘ stands for the data space which contains ğ‘›
data points {x1 , â‹… â‹… â‹… , xn }, xğ‘– = [ğ‘¥ğ‘–1 , â‹… â‹… â‹… , ğ‘¥ğ‘–ğ‘‘ ]ğ‘‡ . Let w =
[ğ‘¤1 , â‹… â‹… â‹… , ğ‘¤ğ‘‘ ]ğ‘‡ , and ğ‘¤ğ‘— are the weights for attributes (ğ‘— =
1, ..., ğ‘‘). For document clustering, we use the weighted ğ‘ğ‘œğ‘ ğ‘–ğ‘›ğ‘’
âŸ¨x,yâŸ©w
similarity and set ğ‘‘w (x, y) = 1âˆ’ âˆ¥xâˆ¥
, where âŸ¨x, yâŸ©w =
w âˆ¥yâˆ¥w
âˆš
âˆ‘ğ‘‘
ğ‘¤
ğ‘¥
ğ‘¦
and
âˆ¥xâˆ¥
=
âŸ¨x,
xâŸ©
.
With
a must-link pairw
w
ğ‘—=1 ğ‘— ğ‘— ğ‘—
wise constraints set ğ’® and a cannot-link pair-wise constraints
set ğ’Ÿ, the document clustering problem can be transformed
into an optimization problem with the objective as [5]:

Algorithm, Experimentation

Keywords
Document Clustering, Metric Learning

1.

INTRODUCTION

As one of the most fundamental data mining tasks, clustering is a subjective process in nature: diï¬€erent users may
want diï¬€erent clusterings when exploring the same data set.
However, specifying an appropriate similarity measure in advance is usually diï¬ƒcult for general users. Recently, semisupervised clustering which can utilize priori pair-wise constraints has attracted a lot of research interest [5].
For the topic of document clustering, there have been
some pioneer work in applying semi-supervised clustering to
increase clustering quality [3]. However, their performance
is still not as good as expected, especially when the number
of priori constraints is limited [2, 3]. One of the possible reasons is that: existing work transforms the clustering into an
optimization problem, with priori constraints as hard constraints. When the number of constraints is inadequate,
there could be many candidate solutions to this optimization problem, and existing methods fail to provide a mechanism to select a suitable one from these possible solutions.

âˆ‘

ğ‘šğ‘–ğ‘›w

ğ‘‘w (x, y)

(x,y)âˆˆğ’®

âˆ‘
subject to (x,y)âˆˆğ’Ÿ ğ‘‘w (x, y) â‰¥ ğ¶ and w â‰¥ 0.
Considering the fact
âˆ‘ that any ğ‘ğ‘œğ‘ ğ‘–ğ‘›ğ‘’ similarity is within
the range [0, 1], set
(x,y)âˆˆğ’Ÿ ğ‘‘w (x, y) â‰¥ âˆ£ğ·âˆ£ (âˆ£ğ·âˆ£ stands
for the number of elements in the cannot-link set ğ’Ÿ) in the
algorithm which enforces that every pair in the cannot-link
set ğ’Ÿ is exactly 1, with an aim to make sure distances of
instances in the cannot-link set ğ’Ÿ as large as possible.
During the search for an optimal set of weights, we introduce a new measure, the salient degree, to evaluate how
the current clustering result respects the priori constraints.
We utilize k -means method with metric ğ‘‘w (., .) to partition data, and get salient degree of clustering result. The
centroids in clustering
âˆ‘ process of k -means are estimated as

{ğœ‡â„ }ğ‘˜â„=1 [1]. ğœ‡â„ =

Copyright is held by the author/owner(s).
SIGIRâ€™10, July 19â€“23, 2010, Geneva, Switzerland.
ACM 978-1-60558-896-4/10/07.

âˆ¥

âˆ‘

xğ‘– âˆˆğœ’â„

xğ‘– âˆˆğœ’â„

xğ‘–

xğ‘– âˆ¥w

, where, ğœ’â„ represents points

assigned to the cluster â„. The salient degree is deï¬ned as the
proportion of satisifed constraints by the clustering result

783

with parameter w, namely ğ‘ ğ‘ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡(w) = âˆ£ğ‘ ğ‘ğ‘¡(ğ’Ÿ)âˆ£+âˆ£ğ‘ ğ‘ğ‘¡(ğ’®)âˆ£
,
âˆ£ğ’®âˆ£+âˆ£ğ’Ÿâˆ£
here ğ‘ ğ‘ğ‘¡(âˆ—) means the satisï¬ed constraints in the set âˆ—.
Accordingly, the document clustering can be carried out
as an optimization problem, but with the salient degree as
a heuristic to decide whether it is necessary to carry out
further gradient descending search or not. The pseudo-code
of this algorithm is provided in Algorithm 1.
Algorithm 1: Document clustering with metric learning
Input: Dataset ğ’³ , number of output clusters ğ‘˜, mustlink constraints ğ’®, cannot-link constraints ğ’Ÿ.
Output: Clusters obtained with metric learning.
ğ‘¡ = 1; wğ‘¡ = [ 1ğ‘‘ , ..., 1ğ‘‘ ];
while not convergent do
ğ‘¡ğ‘¡ = 1;
wğ‘¡ğ‘¡ = wğ‘¡ ;
Step 1: Select initial cluster centroids;
Randomly select cluster centroids, and make sure
ğ‘ ğ‘ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡(wğ‘¡ğ‘¡+1 ) âˆ’ ğ‘ ğ‘ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡(wğ‘¡ğ‘¡ ) > 0,
where, wğ‘¡ğ‘¡+1 = wğ‘¡ğ‘¡ + ğœ‚ âˆ—ğ‘”ğ‘Ÿğ‘ğ‘‘ğ‘–ğ‘’ğ‘›ğ‘¡(wğ‘¡ğ‘¡ ).
Step 2: Iteration for optimization;
while the value of objective function decreases and
ğ‘ ğ‘ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡(wğ‘¡ğ‘¡+1 ) âˆ’ ğ‘ ğ‘ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡(wğ‘¡ğ‘¡ ) > 0 do
wğ‘¡ğ‘¡+1 = wğ‘¡ğ‘¡ + ğœ‚ âˆ— ğ‘”ğ‘Ÿğ‘ğ‘‘ğ‘–ğ‘’ğ‘›ğ‘¡(wğ‘¡ğ‘¡ );
ğ‘¡ğ‘¡ = ğ‘¡ğ‘¡ + 1;
end
ğ‘¡ = ğ‘¡ + 1; wğ‘¡ = wğ‘¡ğ‘¡ ;
end

3.

(a) NMI

(b) Purity

Figure 1: Clustering result on ğ‘ ğ‘’ğ‘¤ğ‘  ğ·ğ‘–ğ‘“ ğ‘“ 3

(a) NMI

(b) Purity

Figure 2: Clustering result on ğ‘ ğ‘’ğ‘¤ğ‘  ğ‘†ğ‘–ğ‘šğ‘– 3

4. CONCLUSIONS

EXPERIMENTS AND RESULTS

Here we compare the performance of the proposed method
with k -means and the hard-constraint algorithm COP-Kmeans
implemented according to [4]. The 20Newsgroup dataset is
used in our experiment.
From original 20Newsgroup dataset, we randomly select
100 documents for each category, and create 2 datasets:
the ğ‘ ğ‘’ğ‘¤ğ‘  ğ·ğ‘–ğ‘“ ğ‘“ 3 data set (alt.athei-sm, rec.sport.baseball,
sci.space) consisting of 3 clusters on 3 distinct topics, and the
ğ‘ ğ‘’ğ‘¤ğ‘  ğ‘†ğ‘–ğ‘šğ‘– 3 data set (comp.graphics, comp.os.ms-windows,
comp.windows.x) contains 3 clusters with large overlaps between them. All the datasets have been pre-processed by
removing stop-words, and words with too high or too low
frequency, and each document is then represented by TFIDF.
We run 10 trials of 2-fold cross-validation for each dataset:
50% of the dataset is used as training set to obtain pairwise constraints, and the other half is used as input of compared algorithms after peering oï¬€ its class/clustering information. The clustering results are then compared with the
â€œground truthâ€ clustering using Normalized Mutual Information (ğ‘ ğ‘€ ğ¼) and ğ‘ƒ ğ‘¢ğ‘Ÿğ‘–ğ‘¡ğ‘¦ measures.
The results are shown as Figure 1 and Figure 2. On both
data sets, we can see that the proposed method outperforms
the other methods in both the ğ‘ ğ‘€ ğ¼ and the ğ‘ƒ ğ‘¢ğ‘Ÿğ‘–ğ‘¡ğ‘¦ measures. Additionally, the results in our method are more
stable. Another important observation is that: When the
amount of priori knowledge is adequate, our method performs similarly with the compared methods; but when the
amount of priori knowledge is limited, our method can still
achieve satisfactory clustering results, while the performance
of other methods deterioriate signiï¬cantly.

784

This paper proposes an eï¬ƒcient soft-constraint algorithm
by obtaining a satisfactory clustering result so that the constraints will be respected as many as possible. Experiments
show the advantage of the proposed algorithm especially
when provided with little priori domain knowledge, the proposed method is more robust and accurate than the existing
methods.

5. ACKNOWLEDGMENTS
This paper was partially supported by the National Natural Science Foundation of P.R.China (No.60802066), the Excellent Young Scientist Foundation of Shandong Province of
China under Grant (No.2008BS01009) and the Science and
Technology Planning Project of Shandong Provincial Education Department (No.J08LJ22).

6. REFERENCES
[1] S. Basu, M. Bilenko, and R. J. Mooney. A probabilistic
framework for semi-supervised clustering. In KDD â€™04,
pages 59â€“68, 2004.
[2] I. Davidson, K. Wagstaï¬€, and S. Basu. Measuring
constraint-set utility for partitional clustering
algorithms. In PKDD â€™06, pages 115â€“126, 2006.
[3] A. Huang, D. Milne, E. Frank, and I. H. Witten.
Clustering documents with active learning using
wikipedia. In ICDM â€™08, pages 839â€“844, 2008.
[4] K. Wagstaï¬€, C. Cardie, S. Rogers, and S. Schroedl.
Constrained k-means clustering with background
knowledge. In ICML â€™01, pages 577â€“584, 2001.
[5] E. P. Xing, A. Y. Ng, M. I. Jordan, and S. J. Russell.
Distance metric learning with application to clustering
with side-information. In NIPS â€™02, pages 505â€“512,
2002.

