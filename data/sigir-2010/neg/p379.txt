Understanding Web Browsing Behaviors through
Weibull Analysis of Dwell Time
Chao Liu

Ryen W. White

Susan Dumais

Microsoft Research
One Microsoft Way
Redmond, WA 98052

Microsoft Research
One Microsoft Way
Redmond, WA 98052

Microsoft Research
One Microsoft Way
Redmond, WA 98052

chaoliu@microsoft.com

ryenw@microsoft.com

ABSTRACT

forms such as document clickthrough, viewing, scrolling,
and bookmarking. Many researchers have studied the correlations between implicit feedback and document relevance
(e.g., [6, 21, 5, 10]), and revealed that document dwell time
(i.e., the length of time a user spends on a document), is generally the most signiï¬cant indicator of document relevance
besides clickthrough, although the extent of the relationship
may vary depending on the information seeking task [14, 15].
Because of the correlation between dwell time and document
relevance, dwell time has been successfully used in various
applications, such as learning to rank [2, 3], query expansion [5], and inferring query-independent page importance
[19]. Speciï¬cally, Agichtein et al. [2, 3] demonstrate that
user browsing features, a major component of which is Web
page dwell time, signiï¬cantly improve the retrieval performance of a competitive search engine, even with the presence
of other important features such as BM25 and search-result
clickthrough. Although post-query browsing is intuitively
more relevant to IR than general browsing, general browsing
is still an important component in information seeking [20,
25]. Indeed, some of aforementioned studies (e.g., [19]) and
real-world search engines also leverage the general browsing
activities for improved eï¬ƒcacy and coverage.
Although dwell time has been extensively studied, some
important questions have not been suï¬ƒciently addressed.
For example, what distribution is appropriate to model the
dwell time ğ‘¡ on a Web page1 ğ‘‘ across all visits, i.e., what does
ğ‘ƒ ğ‘Ÿ(ğ‘¡âˆ£ğ‘‘) look like? Furthermore, how does the distribution
depend on the features of ğ‘‘? And ï¬nally, what does the
distribution tell us about usersâ€™ general browsing behaviors?
These questions are not only interesting in themselves, but
are also useful for various IR applications, as we now explain.
First, accurately modeling ğ‘ƒ ğ‘Ÿ(ğ‘¡âˆ£ğ‘‘) would enable the construction of generative models involving dwell time for Web
text analysis. For example, when dwell time is properly
modeled, topic discovery can be guided by considering both
ğ‘ƒ ğ‘Ÿ(ğ‘¡âˆ£ğ‘‘) and content. Second, ğ‘ƒ ğ‘Ÿ(ğ‘¡âˆ£ğ‘‘) can readily answer
questions such as â€œwhat is the probability that a user will
stay longer than ğ‘¡1 on the page?â€ (answer: ğ‘ƒ ğ‘Ÿ(ğ‘¡ â‰¥ ğ‘¡1 âˆ£ğ‘‘))
or â€œwhat is the expected remaining time that a user will
spend on a page that he has dwelled on for ğ‘¡1 ?â€ (answer:
ğ¸(ğ‘¡âˆ£ğ‘¡ â‰¥ ğ‘¡1 , ğ‘‘)). Answers to such questions could help publishers optimize advertising and content placement. Third,
understanding ğ‘ƒ ğ‘Ÿ(ğ‘¡âˆ£ğ‘‘) would help us gain insights into user
browsing behaviors that can help inform the design of search

Dwell time on Web pages has been extensively used for various information retrieval tasks. However, some basic yet important questions have not been suï¬ƒciently addressed, e.g.,
what distribution is appropriate to model the distribution of
dwell times on a Web page, and furthermore, what the distribution tells us about the underlying browsing behaviors.
In this paper, we draw an analogy between abandoning a
page during Web browsing and a system failure in reliability analysis, and propose to model the dwell time using the
Weibull distribution. Using this distribution provides better goodness-of-ï¬t to real world data, and it uncovers some
interesting patterns of user browsing behaviors not previously reported. For example, our analysis reveals that Web
browsing in general exhibits a signiï¬cant â€œnegative agingâ€
phenomenon, which means that some initial screening has
to be passed before a page is examined in detail, giving rise
to the browsing behavior that we call â€œscreen-and-glean.â€ In
addition, we demonstrate that dwell time distributions can
be reasonably predicted purely based on low-level page features, which broadens the possible applications of this study
to situations where log data may be unavailable.

Categories and Subject Descriptors
H.1.2 [Information Systems]: Models and Principles â€“
User/Machine Systems

General Terms
Algorithms, Measurement, Human Factors

Keywords
Weibull analysis, User behaviors, Web browsing, Dwell time

1.

sdumais@microsoft.com

INTRODUCTION

Real-world information retrieval (IR) heavily relies on effective usage of implicit feedback, which comes in various

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGIRâ€™10, July 19â€“23, 2010, Geneva, Switzerland.
Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

1
We use page, Web page, URL and document interchangeably in this paper

379

and advertising technologies, as we will explain later in the
paper.
Precise modeling of dwell time is not straightforward since
duration on a Web page depends on many factors, some of
which may not even be fully captured by log data (e.g., the
mood of the user). In addition, the distribution family may
vary with diï¬€erent information seeking tasks in diï¬€erent settings (e.g., time of day). As the ï¬rst step towards precise
modeling of dwell time, we choose to model the overall distribution of user dwells on each Web page, which we believe
will help us better understand the dwell time distributions
in general across all users.
In this paper, we draw an analogy between abandoning a
page during Web browsing and a system failure in reliability analysis, and use Weibull analysis techniques which are
commonly used in reliability engineering [1] to characterize
general browsing behaviors. Furthermore, we demonstrate
that it is possible to predict the dwell time distribution based
on page-level features. We make the following contributions
in this study:

measures include document retention (e.g., printing, saving, bookmarking) and document interaction (e.g., viewing, scrolling, dwell time) [21, 6, 14]. Morita and Shinoda
[21] measured the relationship between dwell time, saving,
following-up and copying of a document and usersâ€™ explicit
ratings, and showed that there was a relationship between
dwell time and interest, but no relationship between interest and any other measures. Claypool et al. [6] examined
mouse clicks, scrolling, dwell time, and requested explicit
ratings, and found that dwell time and the amount of mouse
scrolling had a strong positive correlation with explicit ratings. Studies by Kelly and Belkin [14, 15] further found that
special attention is needed to interpret dwell time as relevance because of the implications of diï¬€erent tasks. On the
application side, besides being incorporated into learning to
rank for Web IR [2, 3, 19], dwell time is also widely used in
other information seeking tasks (e.g., [16, 5]). In this paper,
we do not focus on particular search and retrieval tasks as
done in most previous work, but instead try to model the
dwell time distribution across all users engaged in general
Web browsing.
This work is also related to online user behavior modeling, which has been attracting signiï¬cant attention in recent
years (e.g., [24, 25, 9]). There are two main complementary
approaches to uncovering user behavior models: one based
on controlled user studies (e.g., [12, 13, 24]), and the other
based on large-scale log analysis (e.g., [25, 9, 26, 4, 19]). This
work falls into the second category, and is mostly related to
BrowseRank [19], which tries to infer a query-independent
score for each page from page dwell time in general browsing.
In particular, BrowseRank assumes (mainly for tractability)
that the dwell time for a given page follows an exponential
distribution. In this paper, we show that the Weibull distribution is more versatile than the exponential distribution
used in [19]; it better ï¬ts the real-world dwell time data and
provides insights on browsing behaviors.
This work also relates to research on Weibull analysis,
which has been extensively and successfully applied in nearly
all scientiï¬c disciplines, such as biological, demographical,
reliability sciences (c.f. [1, 23]). This paper therefore adds a
new application area to the rich literature of Weibull analysis, and meanwhile introduces a disciplined method for analyzing temporal data on the Web, e.g., time-to-ï¬rst-click
on search result pages and the session length in time, in
addition to the page dwell time as studied here.

âˆ™ Weibull analysis of Web dwell time data: To the
best of our knowledge, this is the ï¬rst time an analogy
has been drawn between abandoning a Webpage and
a system failure, which leads to a principled way of
analyzing dwell time data. The same or similar analogies can be made for other kinds of temporal data on
the Web (e.g., time-to-ï¬rst-click on search result pages
and session length).
âˆ™ Discoveries about user browsing behaviors: Our
analysis leads to some interesting new insights regarding usersâ€™ Web browsing behaviors. Speciï¬cally, we
ï¬nd that Web browsing exhibits a signiï¬cant â€œnegative
agingâ€ phenomenon (i.e., the rate of Web page abandonment decreases over time), and that this eï¬€ect is
stronger for less entertaining pages. These discoveries, together with the application of Weibull analysis
to a new domain, enhance our understanding of user
browsing behaviors.
âˆ™ Predicting dwell time distribution: We demonstrate that the dwell time distribution (in Weibull parametric form) can be eï¬€ectively predicted from low-level
page features. Not only does this broaden the applicability of dwell time data, but it also reveals what page
features correlate with the dwell time distribution.
The remainder of this paper is organized as follows. We
ï¬rst discuss the related work in Section 2, and then examine
the goodness-of-ï¬t of the Weibull distribution in Section 3.
We present the Weibull analysis results in Section 4, and
elaborate on the predictive model in Section 5. With extensions and future work discussed in Section 6, Section 7
concludes this study.

2.

3. MODEL FITTING AND COMPARISON
In this section, we ï¬t the dwell time data with exponential and Weibull distributions (Section 3.2), and compare
their goodness-of-ï¬t in Section 3.3. The data used for this
comparison and throughout the paper are discussed in Section 3.1.

3.1 Experimental Data

RELATED WORK

We collected two-weeks of log data from a popular Web
browser plug-in operating in the English (US) market, which
records the searches and browsed pages for opted-in users.
The log data is organized in sessions, each of which is deï¬ned
as a series of Web page visits that extends until either the
browser is closed or a period of 30-minutes of inactivity.
Based on the visit time of consecutive page visits within
sessions, the dwell time of each page visit is calculated. We
do this for all pages apart from the last page in the session,

This paper is related to work on implicit feedback within
IR. Research on implicit feedback has sought to address the
high cost of soliciting explicit feedback from users by unobtrusively observing their natural interactions and building models for activities such as query expansion and user
proï¬ling [17]. Although implicit feedback may be less accurate than explicit feedback [22], it is available in significantly greater quantity than explicit feedback. Implicit

380

2

Î»=
Î»=
Î»=
Î»=

PDF

1.5

1.0, k =
1.0, k =
1.0, k =
2.0, k =

Eqn 4 gives

0.5
1.0
3.0
3.0

ğœ†ğ‘˜ =

0.5

1

2

3

ğ‘˜
ğ‘–=1 ğ‘¡ğ‘–

,
ğ‘›
which, once plugged into Eqn 3, renders
âˆ‘
ğ‘›
ğ‘˜
ğ‘› ğ‘›
ğ‘› âˆ‘
ğ‘–=1 ğ‘¡ğ‘– ğ‘™ğ‘›(ğ‘¡ğ‘– )
âˆ‘
= 0,
+
ğ‘™ğ‘›(ğ‘¡ğ‘– ) âˆ’
ğ‘›
ğ‘˜
ğ‘˜
ğ‘–=1 ğ‘¡ğ‘–
ğ‘–=1

1

0
0

âˆ‘ğ‘›

(5)

(6)

which only involves ğ‘˜ and can be solved through NewtonRaphson iterations. Speciï¬cally, let
âˆ‘ğ‘› ğ‘˜ âˆ‘ğ‘›
ğ‘›
ğ‘›
âˆ‘
âˆ‘
ğ‘¡ğ‘– ğ‘–=1 ğ‘™ğ‘›(ğ‘¡ğ‘– )
âˆ’ğ‘˜
ğ‘¡ğ‘˜ğ‘– ğ‘™ğ‘›(ğ‘¡ğ‘– ),
ğ‘”(ğ‘˜) =
ğ‘¡ğ‘˜ğ‘– + ğ‘˜ ğ‘–=1
ğ‘›
ğ‘–=1
ğ‘–=1

4

t

Figure 1: Example Weibull Distributions

then
which is then discarded from the analysis because we do
not have a succeeding page visit from which to calculate its
dwell time. For accurate parameter estimation, only pages
with 10,000 or more visits are used. This results in a set
of 205,873 URLs, each of which is accompanied by at least
10,000 dwell time observations.

â€²

ğ‘” (ğ‘˜) =

ğ‘›
ğ‘›
ğ‘›
âˆ‘
âˆ‘
ğ‘™ğ‘›(ğ‘¡ğ‘– ) âˆ‘ ğ‘˜
(
ğ‘¡ğ‘– + ğ‘˜
ğ‘¡ğ‘˜ğ‘– ğ‘™ğ‘›(ğ‘¡ğ‘– )) âˆ’ ğ‘˜
ğ‘¡ğ‘˜ğ‘– ğ‘™ğ‘›2 (ğ‘¡ğ‘– ).
ğ‘›
ğ‘–=1
ğ‘–=1
ğ‘–=1

ğ‘–=1

Ë† is obtained by
Then the MLE of ğ‘˜, denoted by ğ‘˜,
(ğ‘š)
Ë†(ğ‘š+1) â† ğ‘˜
Ë† (ğ‘š) âˆ’ ğ‘”(ğ‘˜ )
ğ‘˜
â€²
ğ‘” (ğ‘˜(ğ‘š) )

3.2 Model Fitting with Maximum Likelihood
The probability density function (PDF) of Weibull distribution is given by
{ (ğ‘¡) }
ğ‘˜ ( ğ‘¡ )ğ‘˜âˆ’1
ğ‘˜
ğ‘’ğ‘¥ğ‘ âˆ’
ğ‘¡ â‰¥ 0,
(1)
ğ‘“ (ğ‘¡âˆ£ğ‘˜, ğœ†) =
ğœ† ğœ†
ğœ†

ğ‘š = 1, 2, â‹… â‹… â‹…

We terminate the iterations when the change of ğ‘˜ is less
Ë† is obtained, ğœ†
Ë† immediately follows from
than 10âˆ’6 . Once ğ‘˜
Eqn 5. We try initial value ğ‘˜(1) = 0.1, 1, 10, and choose the
Ë† ğ‘˜)
Ë† with the largest likelihood. Readers interested
ï¬nal (ğœ†,
in a thorough treatment of parameter estimations (besides
MLE) for Weibull distributions are referred to [23]. The
above estimation can be trivially parallelized across URLs
for distributed computing, which aï¬€ords Web-scale dwell
time data analysis.

with ğ¸(ğ‘¡âˆ£ğœ†, ğ‘˜) = ğœ†Î“(1+1/ğ‘˜). ğœ† and ğ‘˜ are the scale and shape
parameters, respectively. Figure 1 plots the PDFs of some
typical parameterizations, which exemplify the versatility of
the Weibull distribution, and how parameters ğœ† and ğ‘˜ aï¬€ect
the scale and shape of the distribution, respectively.
When ğ‘˜ = 1, the Weibull distribution reduces to the exponential distribution with PDF
{ ğ‘¡}
1
ğ‘¡â‰¥0
ğ‘“ (ğ‘¡âˆ£ğœ†) = ğ‘’ğ‘¥ğ‘ âˆ’
ğœ†
ğœ†

3.3 Goodness-of-fit Comparison
We use the log-likelihood (LL) and the Kolmogorov-Smirnov
distance (KS-distance) [8] to evaluate the goodness-of-ï¬t of
ğ‘€ğ‘Š and ğ‘€ğ¸ . In general, a better ï¬t corresponds to a bigger
LL and/or a smaller KS-distance.
The KS-distance as deï¬ned below

and ğ¸(ğ‘¡âˆ£ğœ†) = ğœ†.
Given a sample of ğ‘› observed dwell time for a page, {ğ‘¡ğ‘– }ğ‘›
ğ‘–=1 ,
we choose to ï¬t the model through maximum likelihood estimation (MLE), and denote the ï¬tted model with Weibull
by ğ‘€ğ‘Š and that with exponential by ğ‘€ğ¸ . While ï¬tting ğ‘€ğ¸
is as simple as
âˆ‘ğ‘›
ğ‘–=1 ğ‘¡ğ‘–
Ë†=
ğœ†
,
(2)
ğ‘›

ğ¾ğ‘†(ğ¹ âˆ— , ğ‘†) = ğ‘ ğ‘¢ğ‘ğ‘¥ âˆ£ğ¹ âˆ— (ğ‘¥) âˆ’ ğ‘†(ğ‘¥)âˆ£

(7)

is the test statistic for Kolmogorov goodness-of-ï¬t test, which
tests whether a random sample ğ‘‹1 , ğ‘‹2 , â‹… â‹… â‹… , ğ‘‹ğ‘› , whose empirical cumulative distribution function (eCDF) is described
by ğ‘†(ğ‘¥), comes from a completely speciï¬ed hypothesis distribution whose cumulative distribution function (CDF) is
given by ğ¹ âˆ— (ğ‘¥).
Because the exponential distribution is a special case of
the Weibull distribution, ğ‘€ğ‘Š is guaranteed to be no worse
than ğ‘€ğ¸ if ï¬tted and evaluated on the same dataset. To be
fair, the dwell time observations for each page are randomly
split into training and testing portions with a ratio of 4:1.
In other words, the data are split within the dwell time
observations for each page rather than across pages. ğ‘€ğ‘Š
and ğ‘€ğ¸ are ï¬t using the training portion and evaluated on
the testing portion for each page. LL and KS-distance on
the testing portion are used to determine which model wins.
The number of pages on which a model wins are listed in
Table 1, which clearly shows the superiority of ğ‘€ğ‘Š to ğ‘€ğ¸ .
Speciï¬cally, ğ‘€ğ‘Š outperforms ğ‘€ğ¸ on more than 85% of the
pages in terms of both metrics, and a sign test for each result
gives a p-value that is very close to zero.

ï¬tting ğ‘€ğ‘Š is nontrivial because the MLE of (ğœ†, ğ‘˜) has no
closed form. Instead, we need to use an iterative approach
proposed in [7]. For completeness, we brieï¬‚y outline the
estimation.
Given the likelihood function as
ğ‘›
âˆ
ğ‘˜ ğ‘¡ğ‘– ğ‘˜âˆ’1 âˆ’( ğ‘¡ğœ†ğ‘– )ğ‘˜
( )
ğ‘’
ğ¿(ğ‘¡1 , ğ‘¡2 , â‹… â‹… â‹… , ğ‘¡ğ‘› âˆ£ğ‘˜, ğœ†) =
,
ğœ† ğœ†
ğ‘–=1
we set the partial derivative w.r.t. ğœ† and ğ‘˜ to 0, i.e.,
ğ‘›
ğ‘›
âˆ‘
âˆ‘
ğ‘›
ğ‘¡ğ‘–
ğ‘¡ğ‘–
âˆ‚ğ‘™ğ‘›(ğ¿)
= âˆ’ğ‘›ğ‘™ğ‘›(ğœ†)+
ğ‘™ğ‘›(ğ‘¡ğ‘– )âˆ’
( )ğ‘˜ ğ‘™ğ‘›( ) = 0, (3)
âˆ‚ğ‘˜
ğ‘˜
ğœ†
ğœ†
ğ‘–=1
ğ‘–=1
ğ‘›

âˆ‚ğ‘™ğ‘›(ğ¿)
ğ‘˜ğ‘› âˆ‘
ğ‘¡ğ‘˜ğ‘–
=âˆ’
+
ğ‘˜ ğ‘˜+1
= 0.
âˆ‚ğœ†
ğœ†
ğœ†
ğ‘–=1

âˆ‘ğ‘›

(4)

381

ğ‘€ğ‘Š Wins

176,242

178,892

ğ‘€ğ¸ Wins

29,631

26,981

2.5

2

Table 1: Comparison of Goodness-of-Fit

4.

WEIBULL ANALYSIS OF DWELL TIME

In this section, we discuss the implications of a ï¬tted
Weibull distribution for understanding user browsing behaviors (Section 4.2). We ï¬rst provide a brief introduction to
Weibull analysis in Section 4.1.

ğ‘˜ ğ‘˜ âˆ’ 1 ( ğ‘¥ )ğ‘˜âˆ’2
.
ğœ† ğœ†
ğœ†

2

3

4

5

1

Cumulative Probability (CDF)

1

0.8

â†(70, 0.8)

0.6

0.4

0.2

0
0

200

400

600

Î»

800

(a) eCDF of ğœ†

1000

1200

â†‘
(1, 0.985)
0.8

0.6

0.4

0.2

0
0

0.5

1

1.5

2

k

(b) eCDF of ğ‘˜

Figure 3: Distributions of the Fitted ğœ† and ğ‘˜ Values
in a constant hazard function, indicating a constant failure
rate, which is the physical model of the exponential distribution.
The hazard functions of some example Weibull distributions are plotted in Figure 2, which illustrates diï¬€erent types
of aging. Note that when ğ‘˜ âˆˆ (0, 1), we see negative aging,
or a decrease in the failure rate over time. In the context
of Web browsing this would mean a decrease in Web page
abandonment rate over time. Conversely, when ğ‘˜ > 0, we
see positive aging, or an increase in the failure rate over
time.

4.2 Weibull Analysis on Dwell Time
Using the data set as described in Section 3.1, we now examine the ï¬tted ğœ† and ğ‘˜ values on the training portion for
each page. Figure 3 plots the empirical cumulative distribution function (eCDF) for the ï¬tted ğœ† and ğ‘˜ values. Figure 3(a) shows the eCDF for the scale parameter ğœ† of the estimated dwell time distribution. We see that the dwell time
is no more than 70 seconds on 80% of the 205,873 pages,
which gives us an overall estimate of the dwell time scales
across pages. Figure 3(b) shows the eCDF for the shape
parameter, ğ‘˜. We see that ğ‘˜ is less than 1 on 98.5% pages.
Recalling that ğ‘˜ < 1 indicates a negative aging eï¬€ect. Thus,
Figure 3(b) suggests that Web browsing exhibits a strong
â€œnegative agingâ€ phenomenon, that is, some â€œscreeningâ€ is
carried out at the early stage of browsing a page, and the
rate of subsequent abandonment decreases over time.
This discovery agrees well with the intuition about how a
user browses a page: upon landing on a Web page, the user
would ï¬rst skim through the page, assessing the potential
beneï¬t of further reading, before delving into it and gleaning
needed information. During the screening, the probability
of abandoning the page is high (i.e., a high hazard rate),
but once the page survives the screening (e.g., is regarded

(8)

whose ï¬rst-order derivative is
â„â€²ğ‘¡ (ğ‘¥) =

1

Figure 2: Example Weibull Hazard Functions

If an item that has survived time ğ‘¥ is called an ğ‘¥-survivor,
the hazard function gives the probability that an ğ‘¥-survivor
fails immediately at time ğ‘¥, and it is also known as the
instantaneous failure rate or the hazard rate. Usually, the
hazard rate is interpreted as the amount of risk associated
with an ğ‘¥-survivor at time ğ‘¥ in reliability study and as the
force of mortality in demography and actuarial science.
The hazard function of a Weibull distribution is given by
ğ‘˜ ğ‘˜âˆ’1
ğ‘¥
,
ğœ†ğ‘˜

No Aging
â†“

1

t

ğ‘ƒ ğ‘Ÿ(ğ‘¥ â‰¤ ğ‘¡ < ğ‘¥ + ğ›¿âˆ£ğ‘¡ â‰¥ ğ‘¥)
.
ğ›¿

â„ğ‘¡ (ğ‘¥) =

0.5
0.8
1.0
1.5

â† Positive Aging

1.5

0
0

Weibull analysis dates back to 1937 when Waloddi Weibull
invented the Weibull distribution. It has been successfully
applied to nearly all scientiï¬c disciplines, such as biological,
environmental, health, physical and social sciences, but, to
the best of our knowledge, not in the Web data analysis
domain. By ï¬tting time-to-failure data to Weibull distributions, Weibull analysis enables principled failure interpretation, risk assessment, failure forecasting, and planning of
corrective actions. Since a full introduction to Weibull analysis is neither realistic nor necessary here, we will highlight
those aspects that pertain to our analysis and referring interested readers to [23] and [1] for a thorough treatment of
Weibull analysis and applications.
The most popular characteristic function of a Weibull distribution is the Hazard function, which is deï¬ned as
ğ›¿â†’0

1.0, k =
1.0, k =
1.0, k =
1.0, k =

0.5

4.1 A Primer on Weibull Analysis

â„ğ‘¡ (ğ‘¥) = lim

Î»=
Î»=
Î»=
Î»=

â† Negative Aging

Cumulative Probability (CDF)

KS-distance

Hazard Rate

Log-Likelihood

(9)

When ğ‘˜ âˆˆ (0, 1), the ï¬rst-order derivative, â„â€²ğ‘¡ (ğ‘¥), is less
than 0, so the hazard rate monotonically decreases w.r.t. ğ‘¥.
This phenomenon is often termed â€œnegative aging,â€ which
means that the longer one survives, the less likely it would
fail instantaneously. Since the hazard rate is high at the
onset, it is also called the â€œinfant mortalityâ€ phenomenon.
In abstract terms, negative aging means that a screening is
taken place at the early stage so that weak items with hidden defects are sorted out while leaving robust and healthy
ones in the population, or as Lehman [18] suggests â€œSo once
the obstacle of early youth have been hurdled, life can continue almost indeï¬nitely.â€ We will reveal the implication of
negative aging for Web browsing in Section 4.2.
In contrast, ğ‘˜ > 1 corresponds to the â€œpositive agingâ€ phenomenon, which means that the longer one survives, the
more likely it fails instantaneously. Finally, ğ‘˜ = 1 results

382

0.35

1.5

Pr(Category | k < 1)
Pr(Category | k > 1)

0.3
0.25

1

k

0.2
0.15
0.5

0.1

(a) ğ‘ƒ ğ‘Ÿ(ğ¶ğ‘ğ‘¡ğ‘’ğ‘”ğ‘œğ‘Ÿğ‘¦ âˆ£ ğ‘˜ < 1) vs. ğ‘ƒ ğ‘Ÿ(ğ¶ğ‘ğ‘¡ğ‘’ğ‘”ğ‘œğ‘Ÿğ‘¦ âˆ£ ğ‘˜ > 1)

Sc
ie
nc
C
e
om
pu
te
rs
So
ci
et
R
y
ec
re
En
at
io
te
n
rta
in
m
en
t
T
R
r
av
el
el
at
io
ns
hi
ps
Ve
hi
cl
es

n

nc
ia
l

Fi
na

Ed

uc
a

tio

Tr
av
el
So
ci
et
y
Sc
ie
nc
e
Ed
uc
at
io
n
Ve
hi
cl
es

ps

nc
ia
l

na

sh
i
io
n

el
at

Fi

s

at
io

0

R

R

ec
re

te
r
pu

om
C

En
t

er
ta
in

m

en

t

0

n

0.05

(b) ğ‘ƒ ğ‘Ÿ(ğ‘˜âˆ£ğ¶ğ‘ğ‘¡ğ‘’ğ‘”ğ‘œğ‘Ÿğ‘¦)

Figure 4: Relationship between Categories and Aging Eï¬€ect as Characterized by ğ‘˜
of the data, and the lower and upper lines of the box represent the 25ğ‘¡â„ and 75ğ‘¡â„ percentiles of the data, respectively.
The categories are ordered in ascending order of the median values from left to right, which median value of 0.6506
for Education and a median value of 0.7979 for Vehicles.
Again, we observe that less-entertaining categories appear
on the left of the ï¬gure, supporting the hypothesis that lessentertaining pages may be more harshly screened.

as useful by the user), the abandonment rate decreases. We
therefore suspect that users do in general adopt a â€œscreenand-gleanâ€ type browsing behavior, which gives rise to the
dwell time distribution showing the observed negative aging
eï¬€ect.
We now examine whether and how the â€œnegative agingâ€
phenomenon relates to the topic of the page, as deï¬ned by
category membership, i.e., do people impose equal screening on pages of diï¬€erent categories? For this purpose, we
employed a proprietary document classiï¬er that assigned
each page into one of 23 top-level categories in a taxonomy
similar to dmoz2 . The categorization succeeded on 136,395
pages. We then analyzed how category information relates
to the aging eï¬€ect from two complementary aspects: ï¬rst,
we compare ğ‘ƒ ğ‘Ÿ(ğ¶ğ‘ğ‘¡ğ‘’ğ‘”ğ‘œğ‘Ÿğ‘¦âˆ£ğ‘˜ < 1) with ğ‘ƒ ğ‘Ÿ(ğ¶ğ‘ğ‘¡ğ‘’ğ‘”ğ‘œğ‘Ÿğ‘¦âˆ£ğ‘˜ > 1),
and second, we examine ğ‘ƒ ğ‘Ÿ(ğ‘˜âˆ£ğ¶ğ‘ğ‘¡ğ‘’ğ‘”ğ‘œğ‘Ÿğ‘¦) for diï¬€erent categories. In order to have suï¬ƒcient data in each category, only
the top-10 categories were retained, which included 106,169
(77.8%) pages.
Figure 4(a) compares the category distributions for Web
pages with ğ‘˜ < 1 and ğ‘˜ > 1. We show the category distributions for each of these two types of pages. We see that
Entertainment, Recreation, Relationships, Travel and Vehicles have a proportionally greater presence when ğ‘˜ > 1 than
when ğ‘˜ < 1 (recall the sets of pages with ğ‘˜ > 1 and ğ‘˜ < 1
are highly imbalanced). Thus pages exhibiting positive aging (ğ‘˜ > 1) are more likely to fall into these categories,
which we can characterize as more entertaining, than those
showing negative aging eï¬€ect. Conversely, we see that the
presence of Computers and Education is stronger in ğ‘˜ < 1
than ğ‘˜ > 1, indicating that people are more likely to screen
pages in these two categories before examining them in more
details. This observation leads to a hypothesis that negative aging is more common on less-entertaining pages than
on fun pages, which in turn suggests that people tend to
screen less-entertaining pages more harshly.
Figure 4(b) shows boxplots of ğ‘ƒ ğ‘Ÿ(ğ‘˜âˆ£ğ¶ğ‘ğ‘¡ğ‘’ğ‘”ğ‘œğ‘Ÿğ‘¦) for the 10
categories. The line in the middle of each box is the median
2

5. PREDICTING DWELL TIME
DISTRIBUTION
In this section, we investigate the feasibility of predicting
dwell time distribution from page-level features. A successful prediction from page features will not only enable thirdparties without access to browsing logs to use dwell time
information, but will also provide us with an opportunity to
identify page features that are most related to dwell time
distributions. We describe the experimental setup and page
features in Section 5.1, report on the prediction results in
Section 5.2, and inspect the learned model in Section 5.3.

5.1 Experimental Setup
We randomly sampled 5000 pages from the set of pages
whose test KS-distance is less than 0.05 from the experiments in Section 3.3.
By choosing pages with a
high goodness-of-ï¬t to the Weibull distribution (small KSdistance), we can provide good training examples to the classiï¬er. For each sampled page, the ğœ† and ğ‘˜ values ï¬tted on
the training portion of data are taken as the learning labels. In order to extract page features, we crawled these
pages using a dynamic crawler, which employs an Internet
Explorer object to execute all dynamic components (e.g.,
ï¬‚ash, javascript, etc.) and download the ï¬nal rendered page.
Pages containing the term â€œloginâ€ are excluded because these
login pages are usually automatically loaded through a timeout redirection. This, together with failed crawling, gave us
a set of 4,771 pages, which are randomly partitioned into
training, validation and testing sets with a ratio of 7:1:2.
Since we want to inspect the learned model we use Mul-

http://www.dmoz.org/

383

Feature
PageSize
PageHeight
PageWidth
DownloadCount
DownloadTime
SecDownloadCount
SecDownloadTime
ParseTime
RenderTime

Description
Size (in bytes) of the rendered page
Height of the rendered page
Width of the rendered page
Number of total downloaded URLs
Time to download all URLs
Number of secondary URLs
Time to download secondary URLs
Time to parse all URLs
Time to layout and render the page

5.2 Prediction Results
In order to determine how diï¬€erent sets of features interact, we tested seven feature conï¬gurations as listed in
Table 3. Also listed in the table are the optimal parameters
determined by the validation set. In particular, the MART
parameters for predicting ğœ† and ğ‘˜ are denoted by ğœ†(â‹…) and
ğ‘˜(â‹…), respectively.
Log-likelihood (LL) and KS-distance are again used as the
evaluation metrics. For each test page, we evaluate the LL
and KS-distance on the test portion data with the predicted
ğœ† and ğ‘˜ values, and compare it with the baseline model,
Â¯ ğ‘˜)
Â¯ across all training pages,
which returns the mean value (ğœ†,
which resembles, and is stronger than, the exponential model
(c.f. Eq. 2).
Results are presented in Table 3, in which â€œPredict Winâ€
means that the predictive model achieves a higher LL (or a
smaller KS-distance) than the baseline model. As the two
metrics give very similar results, we will focus on the result
based on LL in the following.
First, we see that the prediction model outperforms the
baseline method on all seven conï¬gurations with statistical signiï¬cance. Sign tests for â„‹0 : ğ‘ğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ â‰¤ ğ‘ğ‘ğ‘ ğ‘’ğ‘™ğ‘–ğ‘›ğ‘’ all
return ğ‘-values that are very close to zero for the seven conï¬gurations. This result shows that low-level page features
do carry some prediction power that can be leveraged for
eï¬€ective dwell time prediction.
Second, HtmlTag is as eï¬€ective as Dynamic when used individually, and when combined, they bring further improvement. This observation indicates that the nine Dynamic features are as predictive as the 93 HtmlTag features, and their
prediction power is complementary. Conversely, Content in
itself outperforms Content+Dynamic, and adding HtmlTag
to Content only provides some marginal improvement.
Finally, the best performance is actually achieved by
Content+Dynamic. This is reasonable in that Dynamic represents what users would experience immediately after clicking
through to a page while Content corresponds to what content users would see once the page is loaded. Note that
adding HtmlTag does not provide much beneï¬t. Given the
promising results from Content+Dynamic and the fact that
only the top-1,000 frequent words are used in Content, we expect further improvements from better feature engineering,
for example, by choosing words with high inverse document
frequencies rather than simply the most frequent ones, or by
including the number of graphics or tables in the page. We
will explore how to fully utilize the content together with
other kinds of features in future work. For now, let us inspect the learned models to understand what page features
are the most useful for dwell time prediction.

Table 2: Details of Dynamic Features
tiple Additive Regression Trees (MART) [11], which provide good interpretability and high accuracy. We used the
validation set to locate the optimal parameters, which include the maximum number (ğ¿) of leaf nodes of the base
learner tree and the shrinkage parameter (ğ‘£). We varied
ğ¿ âˆˆ {2, 3, 4, 6, 11, 21, 25}, ğ‘£ âˆˆ {1, 2âˆ’1 , 2âˆ’2 , â‹… â‹… â‹… , 2âˆ’6 }, and
recorded the number (ğ‘š) of iterations that achieved the
minimum error. The tuple (ğ¿, ğ‘£, ğ‘š) that achieved the lowest error on the validation set was used in the ï¬nal testing
phase.
We constructed the following three sets of features, ranked
in ascending order based on their closeness to what users
would actually experience when viewing that page in a Web
browser:
âˆ™ HtmlTag: The frequency of each of the 93 HTML
tags (obtained from http://www.quackit.com/html/
tags/) is taken as an independent feature, comprising
the ï¬rst set. These features represent the underlying
elements that are determinants of page formatting and
layout but are not visible to users.
âˆ™ Content: We leverage the â€œ6of12 listâ€ of the English
words, which contains 32,153 most commonly used English words that â€œapproximates the common core of the
vocabulary of American English.â€3 The top-1000 most
frequent terms that appear in the training set of pages,
together with one more dimension about the document
length, are taken as the second set of features. They
correspond to the most frequent words users would see.
The value of each feature, except the document length,
is the word frequency in each page.
âˆ™ Dynamic: We also recorded nine measures during the
dynamic crawling of each page. The dynamic crawler
ï¬rst downloads the backbone page, parses it, downloads any secondary URLs (e.g., javascript, ï¬‚ash, image, etc.) if any, calculates the page layout, and ï¬nally
renders the page. The nine features based on these
measures are listed in Table 2. Because the crawler executes the scripts and renders the page, these features
are meant to closely estimate usersâ€™ browsing experience with the page.
We intentionally chose not to include advanced features
such as PageRank, number of inlinks and any log-based features in the feature set, because these features are generally
not available to researchers outside search engine companies.
By restricting to page-level features, anyone can crawl the
page, construct the features, reproduce the result, and more
importantly, utilize the predictive model to asses dwell time
for any pages that can be downloaded.
3

5.3 Feature Importance
By virtue of the interpretability of MART, we could estimate the importance of each feature and sort them in
descending order of the estimated importance. Figure 5
depicts the six most important features for predicting ğœ†
and ğ‘˜ respectively under each conï¬guration. The ï¬gure for
â€œHtmlTag+Contentâ€ is dropped due to space constraints.
Figure 5(a) shows that Html tags about scripts and links
are the most important (â€œ<!--â€ is for comments in Html).
In Figure 5(b), it is unsurprising to see that the document
length is the most relevant feature, followed by words related
to pornography, games and news. This looks reasonable as
the dwell times for those topics are likely very diï¬€erent, since

http://wordlist.sourceforge.net/

384

Training & Validation
ğœ†(ğ‘£) ğœ†(ğ‘š) ğ‘˜(ğ¿) ğ‘˜(ğ‘£)

Test by Log-Likelihood
Predict Win Baseline Win

Test by KS-Distance
Predict Win Baseline Win

Features

ğœ†(ğ¿)

HtmlTag

25

2âˆ’6

113

25

2âˆ’4

244

654

301

684

271

Content

25

2âˆ’5

67

25

2âˆ’3

159

702

253

727

228

Dynamic

25

2âˆ’6

124

6

2âˆ’2

186

653

302

685

270

HtmlTag+Content

25

2âˆ’6

126

21

2âˆ’3

199

706

249

724

231

HtmlTag+Dynamic

25

2âˆ’5

65

25

2âˆ’3

120

669

286

701

254

Content+Dynamic

25

2âˆ’6

123

25

2âˆ’4

195

724

231

727

228

HtmlTag+Content+Dynamic

25

2âˆ’6

133

21

2âˆ’4

198

717

238

725

230

ğ‘˜(ğ‘š)

Table 3: Prediction Eï¬ƒcacy with Diï¬€erent Feature Conï¬gurations
Feature Relative Weight

Feature Relative Weight
1.5

0

1

<!âˆ’âˆ’
<a

1

2

<s
<script

2

3

<script
<s

4

<meta
<li

5

<link
<meta

6

<div
<div

0.5

3

"game"
"download"

4

"porn"
"game"

5

"latest"
"photo"

0

1

PageSize
PageSize

1

2

PageWidth
ParseTime

2

3

PageHeight
PageHeight

4

<!âˆ’âˆ’
PageWidth

5

ParseTime
DownloadTime

6

2

PageSize
PageSize
PageWidth
PageHeight

3

ParseTime
DownloadCount

4

DownloadCount
PageWidth
DownloadTime
DownloadTime

6

Predicting Î»
Predicting k

<meta
<s

Predicting Î»
Predicting k

(d) HtmlTag+Dynamic

0.5

1

Feature Relative Weight
1.5

0

PageSize
PageHeight
PageWidth
PageWidth

4

DocLength
DocLength

5

"sex"
ParseTime

6

"game"
DownloadCount

(e) Content+Dynamic

Predicting Î»
Predicting k

(c) Dynamic

PageHeight
PageSize

3

1.5

PageHeight
ParseTime

Feature Relative Weight
1.5

1

1

(b) Content

Feature Rank

Feature Rank

1

0.5

5

"follow"
"news"

Feature Relative Weight
0.5

0

"sex"
"online"

(a) HtmlTag

0

Feature Relative Weight
1.5

DocLength
DocLength

6

Predicting Î»
Predicting k

1

Feature Rank

1

Feature Rank

0.5

Feature Rank

Feature Rank

0

0.5

1.5

PageHeight
PageSize

2

PageSize
PageHeight

3

"porn"
PageWidth
"game"
DocLength

4

PageWidth
ParseTime

5
Predicting Î»
Predicting k

1

1

DocLength
DownloadCount

6

Predicting Î»
Predicting k

(f) HtmlTag+Content+Dynamic

Figure 5: Feature Importance in Diï¬€erent Feature Conï¬gurations
users may interact with pages on these topics in diï¬€erent
ways. Similarly, in Figure 5(c) we see that the height of the
rendered page is the top feature for ğœ†, followed by the page
size and width. Interestingly, the time to parse the page
is the most relevant feature for predicting ğ‘˜ in Figure 5(c).
This may suggest when parsing takes a comparably long
time, the page will have a lower chance to survive usersâ€™
screening. Finally, for the remaining three ï¬gures involving
feature combinations, we see that Dynamic, although only
comprising nine features, always appears near the top. This
conï¬rms our belief that the nine dynamic features are strong
predictors; but because of the limited number of features,
complementary support from Content is necessary for the
best performance.

6.

breadth and depth. In breadth, there are many characteristic functions/quantities for Weibull analysis besides the hazard function, e.g., the cumulative hazard rate function and
the mean residual life, each of which has a natural correspondence to interesting aspects of Web browsing. In depth, it
is interesting to investigate how sophisticated models (e.g.,
mixture of Weibulls) would bring better goodness-of-ï¬t and
more insights into understanding user browsing behaviors.
The predictive models as presented here demonstrate the
possibility of predicting Web page dwell time distributions.
While better feature engineering and algorithm improvements would likely further improve performance, the current
approach has an inherent shortcoming: it predicts ğœ† and ğ‘˜
separately whereas it is their combination that determines
the goodness-of-ï¬t of the predicted model. So instead of
predicting ğœ† and ğ‘˜ separately, a more principled approach
could be to optimize the likelihood directly, which would
Likely provide a much better goodness-of-ï¬t.

DISCUSSION AND FUTURE WORK

This paper presents the ï¬rst step in Weibull analysis of
Web page dwell time data, which can be extended in both

385

The Weibull analysis in this paper reveals some implications for understanding the browsing behaviors of all users.
Alternatively, the user population can be partitioned along
explicit dimensions such as time-of-day and geographical
locations or implicit dimensions such as user intent and
domain expertise estimates. For the latter, we can partition the dwell time based on how users reach the page,
e.g., through a search clickthrough, an advertisement clickthrough, or a link from a general Web page. In this way, we
would gain more detailed understanding of user browsing
dwell time in diï¬€erent scenarios.

7.

[9] D. Downey, S. Dumais, D. Liebling, and E. Horvitz.
Understanding the relationship between searchersâ€™
queries and information goals. In CIKM, pages
449â€“458, 2008.
[10] S. Fox, K. Karnawat, M. Mydland, S. Dumais, and
T. White. Evaluating implicit measures to improve
web search. ACM Trans. Inf. Syst., 23(2):147â€“168,
2005.
[11] J. H. Friedman. Greedy function approximation: A
gradient boosting machine. Annals of Statistics,
29:579â€“588, 1999.
[12] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and
G. Gay. Accurately interpreting clickthrough data as
implicit feedback. In SIGIR, pages 154â€“161, 2005.
[13] T. Joachims, L. Granka, B. Pan, H. Hembrooke,
F. Radlinski, and G. Gay. Evaluating the accuracy of
implicit feedback from clicks and query reformulations
in web search. ACM Transaction on Information
System, 25(2):7, 2007.
[14] D. Kelly and N. J. Belkin. Reading time, scrolling and
interaction: exploring implicit sources of user
preferences for relevance feedback. In SIGIR, pages
408â€“409, 2001.
[15] D. Kelly and N. J. Belkin. Display time as implicit
feedback: understanding task eï¬€ects. In SIGIRâ€™04,
pages 377â€“384, 2004.
[16] D. Kelly and C. Cool. The eï¬€ects of topic familiarity
on information search behavior. In JCDL, pages
74â€“75, 2002.
[17] D. Kelly and J. Teevan. Implicit feedback for inferring
user preference: a bibliography. SIGIR Forum,
37(2):18â€“28, 2003.
[18] E. Lehman. Shapes, moments and estimators of the
weibull distribution. IEEE Transactions on Reliability,
12:32â€“38, 1963.
[19] Y. Liu, B. Gao, T.-Y. Liu, Y. Zhang, Z. Ma, S. He,
and H. Li. BrowseRank: letting web users vote for
page importance. In SIGIR, pages 451â€“458, 2008.
[20] G. Marchionini and B. Shneiderman. Finding facts vs.
browsing knowledge in hypertext systems. Computer,
21(1):70â€“80, 1988.
[21] M. Morita and Y. Shinoda. Information ï¬ltering based
on user behavior analysis and best match text
retrieval. In SIGIR, pages 272â€“281, 1994.
[22] D. Nichols. Implicit ratings and ï¬ltering. In
Proceedings of the 5th DELOS Workshop on Filtering
and Collaborative Filtering, pages 31â€“36, 1997.
[23] H. Rinne. The Weibull Distribution: A Handbook.
Chapman & Hall, ï¬rst edition, 2008.
[24] J. Teevan, C. Alvarado, M. S. Ackerman, and D. R.
Karger. The perfect search engine is not enough: a
study of orienteering behavior in directed search. In
CHI, pages 415â€“422, 2004.
[25] R. W. White and S. M. Drucker. Investigating
behavioral variability in web search. In WWW, pages
21â€“30, 2007.
[26] R. W. White and S. T. Dumais. Characterizing and
predicting search engine switching behavior. In CIKM,
pages 87â€“96, 2009.

CONCLUSION

This paper has drawn an analogy between abandoning a
browsed page and the failure of a system, and presented
the ï¬rst Weibull analysis on Web page dwell time data.
We found that general Web surï¬ng exhibits a signiï¬cant
â€œnegative agingâ€ phenomenon, suggesting that users adopt
a â€œscreen-and-gleanâ€ browsing behavior where they vet the
page prior to more detailed examination. This study brings
a new approach to analyzing implicit feedback involving
dwell time, complementing previously conducted user studies in that area. We have proposed some directions for
building more sophisticated dwell time models and presented
some implications for understanding user browsing behavior.
Future work will build on our application of Weibull analysis,
as well as the numerous successes of it in other application
domains, to improve search and advertising.

8.

ACKNOWLEDGEMENTS

The authors would like to thank Yutaka Suzue for the dynamic crawler, Krysta Svore and Qiang Wu for the MART
implementation, Wen-tau Yih and Alice Zheng for the discussion on analyzing dwell time, and Xiaoxin Yin for the
proofreading and helpful comments. The authors also appreciate the anonymous reviewers who not only oï¬€ered us
detailed review comments but also insightful suggestions on
future work.

9.

REFERENCES

[1] R. Abernethy. The New Weibull Handbook. ï¬fth
edition, 2006.
[2] E. Agichtein, E. Brill, and S. Dumais. Improving web
search ranking by incorporating user behavior
information. In SIGIR, pages 19â€“26, 2006.
[3] E. Agichtein, E. Brill, S. Dumais, and R. Ragno.
Learning user interaction models for predicting web
search result preferences. In SIGIR, pages 3â€“10, 2006.
[4] J. Attenberg, S. Pandey, and T. Suel. Modeling and
predicting user behavior in sponsored search. In KDD,
pages 1067â€“1076, 2009.
[5] G. Buscher, L. van Elst, and A. Dengel. Segment-level
display time as implicit feedback: a comparison to eye
tracking. In SIGIR, pages 67â€“74, 2009.
[6] M. Claypool, P. Le, M. Wased, and D. Brown. Implicit
interest indicators. In IUI, pages 33â€“40, 2001.
[7] A. C. Cohen. Maximum likelihood estimation in the
weibull distribution based on complete and on
censored samples. Technometrics, 7(4):579â€“588, 1965.
[8] W. J. Conover. Practical Nonparametric Statistics.
Wiley, third edition, 1998.

386

