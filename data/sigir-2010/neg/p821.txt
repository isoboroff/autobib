Multi-view Clustering of Multilingual Documents
Young-Min Kim†

Massih-Reza Amini‡
‡

†
Université Pierre et Marie Curie
Laboratoire d’Informatique de Paris 6
104, avenue du Président Kennedy
75016 Paris, France

First.Last@lip6.fr

Patrick Gallinari†

National Research Council Canada
Institute for Information Technology
283, boulevard Alexandre-Taché
Gatineau, J8X 3X7, Canada

First.Last@nrc-cnrc.gc.ca

ABSTRACT

over the multiple views deﬁnes cluster signatures for each
document. We use those to prime a second-stage clustering
process over all the views. Experiments carried out on a
large ﬁve language corpus of Reuters documents show that
we consistently improve over competing techniques.

We propose a new multi-view clustering method which uses
clustering results obtained on each view as a voting pattern
in order to construct a new set of multi-view clusters. Our
experiments on a multilingual corpus of documents show
that performance increases signiﬁcantly over simple concatenation and another multi-view clustering technique.

2. MUTLI-VIEW CLUSTERING
We consider a multilingual document as d def
= (d1 , ..., dV )
where each version or view dv , v ∈ {1, ..., V } provides a representation of document d in a diﬀerent language, with feature space Xv . Our algorithm operates in two steps.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Clustering;
I.5.3 [Clustering]: Algorithms

2.1 Stage I - Single-view clustering

General Terms

At the ﬁrst stage of our multilingual clustering, we apply Probabilistic Latent Semantic Analysis (PLSA) [7] independently over each of the V languages, constraining each
model to have the same number of unobserved topics. For
every view v, the probability that document dv arises from
topic z ∈ Z is given by p(z|dv ), estimated by PLSA. Documents are then assigned to each topic using the maximum
posterior probability. We hence obtain a set of V estimated
topics (zd1 , ..., zdV ) for each document d, which we call the
voting pattern in the following. Each zdv indicates the estimated topic index of d on the v th view according to the
view-speciﬁc PLSA model.

Algorithms, Experimentation

Keywords
Multilingual document clustering, Multi-view learning, PLSA

1.

Cyril Goutte‡

INTRODUCTION

Much data is now available in multiple representations, or
views, for example multimedia content or web pages translated into several languages. Multi-view learning is a principled approach to handle these kinds of documents using the
relations between the multiple views. The key is to leverage each view’s characteristics in order to do better than
simply concatenating views. Recently, multi-view clustering methods have been proposed that address this situation
and have been shown to improve over traditional single-view
clustering. [2] proposed an extension of k-means and EM
for a dataset with two views, and [5] presented a late fusion
approach which re-estimates the relationship between documents from single-view clustering results. [3] and [6] show
that dimensionality reduction via canonical correlation between views gives better results for document clustering than
via principal components analysis or random projections.
This paper introduces a novel multi-view clustering method.
Our approach consists of two steps. First we ﬁnd robust topics for each view using the PLSA approach. The topic pattern

2.2 Stage II - Voting & Multi-view clustering
Once a voting pattern is obtained for each multilingual
document, we attempt to group documents such that in
each group, documents share similar voting patterns. As
documents belonging to each of these groups received by
deﬁnition similar votes from the view-speciﬁc PLSA models,
the voting pattern representing each of these groups is called
the cluster signature. We keep the C largest groups with the
most documents as initial clusters. Documents that have
voting patterns with at least V − 1 in common with a cluster signature are pre-assigned to that cluster. The remaining
documents have voting patterns diﬀerent from any of the selected cluster signatures. They are matched to one of these
C groups by applying a PLSA model on the concatenated
document features.
The parameters of the ﬁnal PLSA model are ﬁrst initialized
using the documents that have been pre-assigned to the selected cluster signatures. For these documents p(c | d) has
a binary value equal to 1 if d belongs to cluster c and 0
otherwise. For the remaining documents, posteriors are estimated at each iteration as in the traditional E-step. In the
M-step, after updating model parameters, we keep the val-

Copyright 2010 Crown in Right of Canada.
This article was authored by employees of the National Research Council
of Canada. As such, the Canadian Government retains all interest in the
copyright to this work and grants to ACM a nonexclusive, royalty-free right
to publish or reproduce this article, or to allow others to do so, provided that
clear attribution is given both to the NRC and the authors.
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
ACM 978-1-60558-896-4/10/07.

821

ues of p(c | d) ﬁxed for the pre-assigned documents. After
convergence, documents are assigned to the clusters using
the posteriors p(c | d). Note that any generative model giving p(c | d) may be employed instead of PLSA, such as Latent
Dirichlet Allocation [4].

3.

Table 2: micro-AvgPre and NMI of diﬀerent clustering
techniques averaged over 10 initialization sets and 5
languages.
Strategy
conc-PLSA
Fusion-LM
voted-PLSA

EXPERIMENTS

We perform experiments on a publicly available multilingual multi-view text categorization corpus extracted from
the Reuters RCV1/RCV2 corpus [1].1 This corpus contains more than 110K documents from 5 diﬀerent languages,
(English, German, French, Italian, Spanish), distributed
over 6 classes. The multilingual collection is originally a
comparable corpus as it covers the same subset of topics in
all languages. In order to produce multiple views for documents, each original document extracted from the Reuters
corpus was translated in all other languages using a phrasebased statistical machine translation system. The indexed
translations are part of the corpus distribution.
Experiments are repeated 10 times on the whole dataset,
using diﬀerent random initializations of the PLSA models.
The number of topics in each single-view PLSA model as
well as the number of clusters C are ﬁxed to 6, the number of classes in the collection. We used the micro-averaged
precision (micro-AvgPre) as well as the Normalized Mutual
Information (NMI) to measure clustering results [8]. In order to use these evaluation measures, the predicted label for
each cluster is the label of the most dominant class in that
cluster. The reported performance is averaged over the 10
diﬀerent runs. To validate our approach we compare our
algorithm (denoted by voted-PLSA in the following) with
a PLSA model operating over the concatenated feature representations of documents (conc-PLSA) and the late fusion
approach (Fusion-LM) for multi-view clustering [5].
First, we are interested in the clustering results after the
ﬁrst step of our algorithm on the C = 6 largest clusters containing each the same voting pattern documents. Table 1
shows the micro-AvgPre performance of the clustering results per language as well as the percentage of documents
being grouped with our voting strategy. We observe that
partitions formed using the votes of single-view models contain more than half of the documents in the collection and
that these groups are highly homogeneous with an average
precision of 0.76.

% of documents
51.18
63.85
67.44
58.03
73.73
62.84

4. CONCLUSIONS
We presented a multi-view clustering approach for multilingual document clustering. The proposed approach is an
incremental algorithm which ﬁrst groups documents having the same voting patterns assigned by view-speciﬁc PLSA
models. Working in the concatenated feature spaces the
remaining unclustered documents are then assigned to the
groups using a constrained PLSA model. Our results have
brought to light the positive impact of the ﬁrst stage of
our approach which can be viewed as a voting mechanism
over diﬀerent views. The eﬀect of the length of these voting
patterns and the number of latent variables in view-speciﬁc
PLSA models are interesting avenues for future research.

Acknowledgments
This work was supported in part by the IST Program of the
EC, under the PASCAL2 Network of Excellence.

References
[1] M.-R. Amini, N. Usunier, and C. Goutte. Learning from multiple partially observed views - an application to multilingual
text categorization. In NIPS 22, pages 28–36, 2009.
[2] S. Bickel and T. Scheﬀer. Multi-view clustering. In ICDM-04,
pages 19–26, 2004.
[3] M. B. Blaschko and C. H. Lampert. Correlational spectral
clustering. In CVPR-08, 2008.
[4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet
allocation. JMLR, pages 993–1022, 2003.
[5] E. Bruno and S. Marchand-Maillet. Multiview clustering: A
late fusion approach using latent models. In SIGIR-09, pages
736–737, 2009.
[6] K. Chaudhuri, S. M. Kakade, K. Livescu, and K. Sridharan.
Multi-view clustering via canonical correlation analysis. In
ICML-09, pages 129–136, 2009.
[7] T. Hofmann. Unsupervised learning by probabilistic latent semantic analysis. Machine Learning, 42(1-2):177–196, January
2001.
[8] N. Slonim, N. Friedman, and N. Tishby. Unsupervised document classiﬁcation using sequential information maximization. In SIGIR-02, pages 129–136, 2002.

micro-AvgPre
0.79
0.78
0.80
0.60
0.81
0.76

Table 2 summarizes results obtained by conc-PLSA, FusionLM and voted-PLSA averaged over ﬁve languages and 10 dif1

NMI
0.41↓
0.41↓
0.44

ferent initializations. We use bold face to indicate the highest performance rates, and the symbol ↓ indicates that performance is signiﬁcantly worse than the best result, according to a Wilcoxon rank sum test used at a p-value threshold
of 0.05. Note that in our approach, the second stage multiview clustering model relies on a PLSA on the concatenated
views, just as in conc-PLSA. This suggests that the diﬀerence
of 2 to 3 points in micro-AvgPre and NMI (respectively) between voted-PLSA and conc-PLSA shows the real impact of
the ﬁrst stage voting process. In addition, both voted-PLSA
and conc-PLSA perform at least as well as Fusion-LM.

Table 1: Proportion of pre-assigned documents and
average precision on those, obtained from the ﬁrst
stage single-view PLSA models.
Language
English
French
German
Italian
Spanish
Average

micro-AvgPre
0.63↓
0.61↓
0.65

http://multilingreuters.iit.nrc.ca/

822

