Using Local Precision to Compare Search Engines in
Consumer Health Information Retrieval
Carla Teixeira Lopes

Cristina Ribeiro

Departamento de Engenharia Informática
Faculdade de Engenharia da Universidade do
Porto
Rua Dr. Roberto Frias s/n
4200-465 Porto, Portugal

Departamento de Engenharia Informática
Faculdade de Engenharia da Universidade do
Porto/INESC-Porto
Rua Dr. Roberto Frias s/n
4200-465 Porto, Portugal

ctl@fe.up.pt

mcr@fe.up.pt

ABSTRACT

2. METHOD

We have conducted a user study to evaluate several generalist and health-speciﬁc search engines on health information
retrieval. Users evaluated the relevance of the top 30 documents of 4 search engines in two diﬀerent health information
needs. We introduce the concepts of local and global precision and analyze how they aﬀect the evaluation. Results
show that Google surpasses the precision of all other engines,
including the health-speciﬁc ones, and that precision diﬀers
with the type of clinical question and its medical specialty.

We conducted a user study with 5 work tasks that were
deﬁned based on popular questions submitted to web health
support groups. Each work task acts as the context of 4
information needs (IN) that are linked to it. The deﬁned
work tasks are available at http://www.carlalopes.com/
research/userstudy2.html and are associated with the following medical specialties: gynecology, dermatology, psychiatry and urology. Moreover, each IN is associated with
one of the following types of clinical questions: overview,
diagnosis/symptoms, treatment, prevention/screening, disease management, prognosis/outcome.
Each user chose 2 IN that, after being transformed into
queries, were submitted to 4 SE selected by the user. Following the pooling approach, each user assessed the relevance
of the top 30 documents returned by each SE. Totally there
were 82 sets of judged documents, one for each pair of user
and IN, from which were excluded duplicates. Forty-one undergraduate users participated in this study (27 females; 14
males) with a mean age of 27.2 years (SD=9.8). These users
evaluated 9,572 documents, less than 41 × 2 × 4 × 30 because
some queries returned less than 30 documents.
To evaluate and compare the SE we used the 11-point
interpolated average precision and the Mean Average Precision (MAP). In these measures we used two types of precision, a local precision that is calculated based on the actual relevance judgments of the users and a global precision,
based on the set of relevant documents assembled for the
IN in the pool. As human relevance judgments are idiosyncratic, these two types of precision will allow us to compare
the real precision - as judged by the users - and the estimated
precision - calculated in a test-collection fashion way.
The ﬁrst type of precision is calculated using a local set of
relevant documents that is deﬁned by LRel(u, in) = {doc :
doc ∈ P (u, in) ∧ RJ(doc) = 1}. The second uses a global
set of relevant
documents that is deﬁned by GRel(in) =
S
unique( u∈U LRel(u, in)). In these formulas, u is an user;
U represents the set of all users, in is an information need;
doc is a document; P (u, in) is the pool of judged documents
for user u and information need in and RJ(doc) is the relevance judgment for document doc that can be 0 or 1.

Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; J.3 [Computer Applications]: Life and Medical Sciences
General Terms: Experimentation, Performance, Human
Factors.
Keywords: Health information retrieval, evaluation, precision, user study.

1.

INTRODUCTION

Patients, their family and friends are increasingly using
the Web to search for health information [2]. The last Pew
Internet report on health information [4] reveals that 61% of
the american adults look online for health information. In
the Internet users, this proportion rises to 83%. A previous
study reported that 66% of health information sessions start
at generalist search engines (SE) and 27% start at healthspeciﬁc websites [3]. Large companies in information retrieval have been developing eﬀorts in the health area (e.g.
Google Health and Bing Health) and several health-speciﬁc
services are also appearing.
This study evaluates the performance of 4 generalist SE
(Google, Bing, Yahoo! and Sapo) and 3 speciﬁc SE (MedlinePlus, WebMD and Sapo Saúde) in health information
retrieval. The evaluation is based on the data collected in
a user study with undergraduate students and work tasks
deﬁned according to the framework proposed by Borlund
[1]. Besides an overall comparison, the SE are also compared according to the type of clinical question and medical
specialty.

3. RESULTS
With the exception of one IN, all information needs were
associated with at least one user. All users chose Google as
one of the four SE. The other SE with more selections were

Copyright is held by the author/owner(s).
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
ACM 978-1-60558-896-4/10/07.

835

at an engine perspective, show us that only MedlinePlus and
Yahoo have signiﬁcant diﬀerences in MAP at query types
at α=0.1 (p=0.08 and p=0.07, respectively). In MedlinePlus, the Overview and Treatment query types have signiﬁcant larger MAP at diﬀerent α (p=0.01 and p=0.07, respectively) than the Prevention/Screening ones. In Yahoo, the
Overview category is signiﬁcantly better in MAP than Diagnosis/Symptoms and Treatment (p=0.07 and p=0.05, respectively). In the comparative analysis of the engine’s precision in each query type, in the Prevention/Screening query
type, the engines have signiﬁcant diﬀerent MAP (p=0.06).
In a deeper analysis, we found that in this type of question,
Google is signiﬁcantly better (p<0.05) than MedlinePlus,
Sapo and WebMD and also that WebMD is signiﬁcantly
worse than Bing (p= 0.03) and MedlinePlus (p= 0.03).
In the overall analysis on the medical specialty, we found
signiﬁcant diﬀerences at α=0.1 (p=0.06). At diﬀerent significance levels, dermatology has lower MAP than gynecology
(p=0.01), psychiatry (p=0.00) and urology (p=0.05). In
an engine analysis, only Sapo Saude has signiﬁcant diﬀerences between specialties (p=0.09). A deeper analysis show
us that in this engine, the psychiatry questions have higher
MAP than urology (p=0.09), gynecology (p=0.05) and dermatology (p=0.01). In an specialty analysis, we could verify
that there are signiﬁcant diﬀerences between engines in the
psychiatry (p=0.04) and dermatology (p=0.08) specialties.
In psychiatry, we found that Google has a signiﬁcant better MAP than Bing (p=0.00), MedlinePlus (p<0.05), Sapo
(p=0.04) and Yahoo (p=0.02). On the other hand, Bing has
also a worse MAP than MedlinePlus (p=0.02), Sapo Saude
(p=0.00) and WebMD (p=0.04). In dermatology, Google
has a signiﬁcant better MAP than Bing (p=0.05), MedlinePlus (p=0.05) and Sapo Saude (p=0.05).

Table 1: Local MAP, global MAP and maximum
recall in GRel(in) by search engine
Engine
Local MAP Global MAP Recall
Bing
0.56
0.65
0.25
Google
0.74
0.82
0.56
MedlinePlus
0.61
0.69
0.27
Sapo
0.58
0.67
0.22
Sapo Saúde
0.59
0.71
0.21
WebMD
0.56
0.63
0.29
Yahoo!
0.59
0.63
0.18

the Sapo Saúde (27 users), Bing (25 users) and MedlinePlus
(23 users).
Table 1 presents the Local MAP, calculated with local
precision, the Global MAP, calculated with global precision,
and the maximum recall of each SE. As global map is more
comprehensive, it was expected to have a Local MAP lower
than Global MAP, and the diﬀerence is in fact signiﬁcative
(p<0.01). From this table we can see that Google is the
engine with larger MAP and maximum recall. Although the
literature mentions that the variation of individual assessors’
judgements have little impact on the the relative eﬀectiveness ranking of diﬀerent systems [5], we found that the use
of these two types of precision slightly changes this ranking.
MedlinePlus and Sapo Saúde, two health-speciﬁc SE, appear
in both rankings but in reverse order, after Google. Yahoo!
is the SE that rises more positions, from 6th to 3rd, in the
ranking with local MAP.
Recall is not an uniform measure because each SE has its
own collection. Moreover, while generalist SE index contents
from several sources, the 3 analyzed health-speciﬁc SE only
index their own contents. Therefore, the maximum recall
in GRel(in), presented in Table 1, is not supposed to be
used as a measure to rank the 7 SE. In the generalist SE,
we can see that in terms of recall, Bing, Sapo and Yahoo,
in this speciﬁc order, follow Google. Considering the unfair
situation of health-speciﬁc SE, an individual analysis of their
recall make us predict that, in a pool composed by only their
documents, they would have a good recall. The recall issues
described above and space constraints made us decide to
omit the 11-point interpolated average precision analysis.
We found signiﬁcative diﬀerences in local and global MAP
between SE, both with p<0.01. As local MAP is more
strict on users’ relevance judgements, it is more realistic and
therefore will be used in the following analysis. We further
studied the diﬀerences between every pair of SE and found
that Google has a signiﬁcant larger MAP than the other SE
(p<0.05 with Sapo and p<0.01 in the other comparisons).
The diﬀerences between other SE are not signiﬁcant.
Our analysis has also focused on the type of clinical question and on the medical specialty of the query. In both
cases, we have compared the MAP of each category of the
query type and query specialty, the MAP of each SE in each
category and the MAP of each category in all SE.
Each IN is associated with one type of clinical question.
The small number of submitted queries (< 5) of the Disease Management and Prognosis/Outcome categories, made
us omit them from our analysis. In the overall analysis,
we found signiﬁcant diﬀerences at α=0.1 (p=0.06) between
types of queries. The Overview query type has the largest
MAP median and the smallest dispersion. A similar analysis

4. CONCLUSION
We have introduced two types of precision, a local and a
global one. The former is closer to each user relevance judgments and the latter is similar to the notion of precision
used in TREC evaluations. We found that there are signiﬁcant diﬀerences between them and that diﬀerent ranking of
search engines appear with these two type of precisions. Future work will be done on the inﬂuence of context features
in relevance evaluations by human assessors and on ways to
deal with diﬀerent judgements. We also found that generalist search engines don’t have lower precision than healthspeciﬁc ones. In fact, Google has shown a precision significantly higher than all the others search engines. It would
be interesting to complement this study with an evaluation
of the documents’ contents by health experts and to analyze
its correlation with user’s judgements.

5. REFERENCES
[1] P. Borlund. The IIR evaluation model: a framework for
evaluation of interactive information retrieval systems.
Information Research, 8(3), 2003.
[2] R. J. W. Cline and K. M. Haynes. Consumer health information
seeking on the Internet: the state of the art. Health Educ. Res.,
16(6):671–692, December 2001.
[3] S. Fox. Online health search 2006. Technical report, Pew
Internet & American Life Project, 2006.
[4] S. Fox and S. Jones. The social life of health information.
Technical report, Pew Internet & American Life Project, June
2009.
[5] C. D. Manning, P. Raghavan, and H. Schütze. Introduction to
Information Retrieval. Cambridge University Press, first
edition, July 2008.

836

