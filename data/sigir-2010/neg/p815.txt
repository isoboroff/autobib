From Fusion to Re-ranking: a Semantic Approach
Annalina Caputo
acaputo@di.uniba.it

Pierpaolo Basile
basilepp@di.uniba.it

Giovanni Semeraro
semeraro@di.uniba.it

Department of Computer Science
University of Bari “Aldo Moro”
70125 Bari, Italy

ABSTRACT

representation levels: keyword and synset. The ranked list
of documents retrieved using the synset-based representation (synset list) is exploited to re-rank the list of documents
retrieved using the keyword-based one (keyword list). The
insight of this method is that documents in the keyword
list with the highest number of similar documents in the
synset list should climb in the result set. To compute interdocument similarities we use Semantic Vectors [13] which
rely on the WordSpace model. In that model, documents
and words are represented by points in a vector space in
such a way that words/documents related to each other are
close in that space.

A number of works have shown that the aggregation of several Information Retrieval (IR) systems works better than
each system working individually. Nevertheless, early investigation in the context of CLEF Robust-WSD task, in
which semantics is involved, showed that aggregation strategies achieve only slight improvements. This paper proposes
a re-ranking approach which relies on inter-document similarities. The novelty of our idea is twofold: the output of
a semantic based IR system is exploited to re-weigh documents and a new strategy based on Semantic Vectors is used
to compute inter-document similarities.
Categories and Subject Descriptors: H.3.3 [Information Search
and Retrieval]: Retrieval models
General Terms: Algorithms, Experimentation
Keywords: Re-ranking, Semantics, WordNet

1.

BACKGROUND AND MOTIVATION

To date, many semantic approaches to IR have been developed. These approaches tackle the word ambiguity problem by shifting from a lexical space towards a semantic
one. Among the most investigated techniques are those that
rely on WordNet1 synsets through which groups of synonym
words are uniquely identified and linked to each other by semantic relations. Ranging from query expansion to concept
representation of documents by synset indexing, these attempts have not shown a turning point with respect to classical techniques. Further investigations were performed in the
context of Robust-WSD task at Cross Language Evaluation
Forum (CLEF). Systems which achieved the best performance in the last two campaigns [6, 3, 14] adopted strategies
based on ranking aggregation. Ranking aggregation methods [7, 9] are founded on the idea that different retrieval
methods find different sets of documents with small overlap
in both relevant and non-relevant documents sets. Thus,
fusing all these sets in a single list of ranked documents
should result in the best performance. Although the usage
of this kind of strategy showed at times slight improvements,
in most cases they are not significant. This paper presents a
different approach to document aggregation based on a variation of the “inter-document similarities” [8, 10] idea. We
combine two retrieval strategies that work at two different
1

2. METHODOLOGY
Following the cluster hypothesis [12] in which “closely associated documents tend to be relevant to the same requests”,
our approach tries to re-weigh documents in response to a
query promoting those documents with the highest number
of supporters. In this context, a supporter is a document
with content similar to the target one.
Let us denote by Lk and Ls the ranked lists of documents
retrieved using keywords and synsets representation, respectively. The idea behind our method is to give more evidence
to the documents in Lk that are widely supported by similar
documents occurring in both lists.
The method requires the following steps:
1. For each document di ∈ Lk we compute the supporters
(di , α), which is the set of α documents {d1 , ...dα } ⊂
Lk with the highest inter-document similarity to di .
2. We get the overlap supporters = {dj ∈ Ls : dj ∈
supporters(di, α)} which is the set of documents occurring in both Ls and supporters.
3. We assign to di a new score S(di ) taking into account
supporting documents computed in the step 2. Formally:
S(di ) = θ ∗ Ssupporters + (1 − θ) ∗ Sk (di )

(1)

where
Ssupporters =

X

Sk (dj ) ∗ Ss (dj ) (2)

dj ∈overlap supporters

A semantic lexicon for the English language.
and Sk (dj ) is the score of dj in Lk , while Ss (dj ) is the
score of dj in Ls , and θ is a free parameter used to
smooth Ssupporters , which denotes the scores combination of supporting documents.

Copyright is held by the author/owner(s).
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
ACM 978-1-60558-896-4/10/07.

815

To compute the inter-document similarities we build a
vector space (DocSpace) where similar documents are represented by close vectors by means of the Semantic Vectors
package [13]. To build the DocSpace, Semantic Vectors rely
on a technique called Random Indexing [4], which performs
a matrix reduction of the term-document matrix. Hence,
in the DocSpace the similarity between documents is computed by the traditional cosine similarity.

3.

Table 1: Experimental Results (figures in boldface
are statistically significant)
Exp
MAP GMAP
Keyword
0.4205
0.1900
Synset
0.3201
0.1242
CombSU M
0.4252
0.1972
CombM N Z 0.4238
0.1969
ReRank
0.4332 0.1989

EVALUATION AND REMARKS

Our evaluation aims to establish if a synset-based retrieval
system brings to significative improvements in IR when it is
exploited by a re-ranking approach based on inter-document
similarities.
The evaluation is carried out on the CLEF 2009 Ad-Hoc
Robust WSD dataset [2]. The document collection is made
up of LA Times 94 and Glasgow Herald 95 newspaper documents. Each user’s information need is expressed by a topic,
a structured sentence consisting of a title, a description and
a narrative field. The benchmark supplies 150 topics for
the training step and 160 for the test step. Documents and
topics are automatically annotated with WordNet synsets
using two state-of-the art systems [1, 5]. We built a retrieval system based on the Okapi BM25 model for both
levels of representation: keyword and synset. Stemming
and stop word removal were applied to keyword-based representation of documents and topics, but a different list of
stop words was used for topics in order to remove frequent
words which are poorly discriminating. Queries were built
exploiting all topic fields and using different boosting factors
to adjust their impacts on the result set (title=8, narrative=2, description=2). Hence, we retrieved the Lk and Ls
lists of ranked documents. The evaluation was performed
using the MAP and GMAP measures. Table 1 summarizes the main results. Foremost, we evaluated each system
alone (Keyword and Synset). Keyword was used as baseline of the evaluation. Then, we evaluated two aggregation
strategies, CombM N Z and CombSU M [7]. In particular
we adopted a modified version of those strategies to assign
different weights to each list during the aggregation. Finally, the result of the proposed method has been denoted
by ReRank. After a tuning step, we set the weights for Lk
and Ls to 0.8 and 0.2, respectively. Moreover, we tested
several values of θ ∈ {0.2, 0.3, . . . , 0.9} and α ∈ {10, 20, 30}.
The number of supporters α was set to 20 and the smoothing parameter θ was set to 0.3. Tuning was performed using
training topics provided by the CLEF organizers.
The ReRank method achieves the best results in term of
MAP and GMAP. These improvements are significant with
respect to both baseline Keyword and aggregation methods.
We validated our experiments using both the parametric
Student paired t-test and the non parametric Randomization test as suggested in [11] (α = 5%). Results confirm
our hypothesis: the ranking provided by synsets (Ls ) contributes significantly to the final document score. In fact,
to point up our result, we proposed another experiment in
which only the keyword list was exploited. In this experiment the supporters score was computed using only Lk list
and we obtained a MAP of 0.3677.

4.

[2]

[3]
[4]

[5]

[6]
[7]

[8]

[9]

[10]

[11]

[12]
[13]

[14]

REFERENCES

[1] E. Agirre and O. L. de Lacalle. UBC-ALM: combining
k-NN with SVD for WSD. In SemEval ’07: Proc. of the 4th

816

Int. Workshop on Semantic Evaluations, pages 342–345.
ACL, 2007.
E. Agirre, G. M. D. Nunzio, N. Ferro, T. Mandl, and
C. Peters. CLEF 2008: Ad Hoc Track Overview. In
Evaluating Systems for Multilingual and Multimodal
Information Access, 9th Workshop of the Cross-Language
Evaluation Forum, CLEF 2008, Aarhus, Denmark,
September 17-19, 2008, Revised Selected Papers, volume
5706 of LNCS, pages 15–37. Springer, 2009.
P. Basile, A. Caputo, and G. Semeraro. UNIBA-SENSE @
CLEF 2009: Robust WSD task. In Working Notes for the
CLEF 2009 Workshop, 2009.
E. Bingham and H. Mannila. Random projection in
dimensionality reduction: Applications to image and text
data. In KDD ’01: Proc. of the 7th ACM SIGKDD Int.
Conf. on Knowledge Discovery and Data mining, pages
245–250. ACM, 2001.
Y. S. Chan, H. T. Ng, and Z. Zhong. NUS-PT: exploiting
parallel texts for word sense disambiguation in the English
all-words tasks. In SemEval ’07: Proc. of the 4th Int.
Workshop on Semantic Evaluations, pages 253–256. ACL,
2007.
L. Dolamic, C. Fautsch, and J. Savoy. UniNE at CLEF
2008: TEL, Persian and Robust IR. In Working Notes for
the CLEF 2008 Workshop, 2008.
E. A. Fox and J. A. Shaw. Combination of multiple
searches. In Proc. of the 2nd Text REtrieval Conference
(TREC-2), pages 243–252, 1994.
A. K. Kozorovitzky and O. Kurland. From ”identical” to
”similar”: Fusing retrieved lists based on inter-document
similarities. In Advances in Information Retrieval Theory,
2nd Int. Conf. on the Theory of Information Retrieval,
ICTIR 2009, Cambridge, UK, September 10-12, 2009,
Proceedings, volume 5766 of LNCS, pages 212–223.
Springer, 2009.
J. H. Lee. Analyses of multiple evidence combination. In
SIGIR ’97: Proc. of the 20th annual Int. ACM SIGIR
Conf. on Research and Development in Information
Retrieval, pages 267–276. ACM, 1997.
L. Meister, O. Kurland, and I. Kalmanovich. Two are
better than one! Re-ranking search results using an
additional retrieved list. Technical report IE/IS-2009-01,
Technion, 2009.
M. D. Smucker, J. Allan, and B. Carterette. A comparison
of statistical significance tests for information retrieval
evaluation. In CIKM ’07: Proc. of the 16th ACM Conf. on
Information and Knowledge Management, pages 623–632.
ACM, 2007.
C. J. van Rijsbergen. Information Retrieval. Butterworth,
1979.
D. Widdows and K. Ferraro. Semantic Vectors: a scalable
open source package and online technology management
application. In Proc. of the 6th Int. Language Resources
and Evaluation (LREC’08). ELRA, 2008.
E. Wolf, D. Bernhard, and I. Gurevych. Combining
probabilistic and translation-based models for information
retrieval based on word sense annotations. In Working
Notes for the CLEF 2009 Workshop, 2009.

