Transitive History-based Query Disambiguation for Query
Reformulation
Karim Filali
filali@yahoo-inc.com

Anish Nair
anishn@yahoo-inc.com

Chris Leggetter
cjl@yahoo-inc.com

Yahoo! Labs
Santa Clara, CA 95054

ABSTRACT

2. MODEL AND ALGORITHMS
2.1 History-reformulation Model

We present a probabilistic model of a user’s search history and a
target query reformulation. We derive a simple transitive similarity
algorithm for disambiguating queries and improving history-based
query reformulation accuracy. We compare the merits of this approach to other methods and present results on both examples assessed by human editors and on automatically-labeled click data.

Let q be a search query, A the set of alternative reformulations
of q, and H the vector of queries preceding q in the search history. As an example (from Yahoo’s search logs), a user searches
for q="bec" and we map q to alternative rewrites, A = {a1 =
"bose einstein condensates", a2 = "bahamas electric company", a3 =
"basic ecclesial community"}. All three reformulations are reasonable in the absence of other information. Knowing the previous queries H = {h1 = "bahamas", h2 = "facebook", h3 =
"bahamas electric"}, however, strongly suggests a2 might the more
appropriate alternative of the three. In this section, we wish to build
a model that encodes the intuition that the subset of the history relevant to the current query should affect query-to-rewrite similarity
through history-to-reformulation similarity.
We introduce a latent relevance indicator set, G (of size m),
whose setting determines the subset of the history related to the
current query, q (search history can consist of many unrelated intents).For a given a ∈ A, we write the joint probability, P (q, a, H)
as
P
P (q, a, H) = PG.m P (q, a, H, G, m)
(1)
= G,m P (q, a, H|G, m)P (G|m)P (m)

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval

General Terms: Algorithms, Experimentation
Keywords: Personalization, Reformulation, Graphical Models
1. INTRODUCTION
The importance of using web search history to personalize and
improve search results has long been recognized and several approaches have been proposed in the literature to leverage short or
long-term user events to better predict relevant results ([1, 2, 3, 5,
6]). In this paper, we focus on improving query reformulation (a
query rewriting technique for better search result selection) by using search history for disambiguating a given user query.
Specifically, we address the problem of adjusting query similarity scores in the presence of additional history context. Given candidate query reformulations, we build a query-history-reformulation
model and update reformulation scores that can be used to rerank
the candidate set or, as we do, to remove reformulations which become less relevant in the context of the search history.
Our model is general and can be used to derive various scoring algorithms depending on modeling choices. In this work, we
describe a simple algorithm whose main feature is the use of the
transitive nature of query similarity: given two candidate reformulations (possibly with very different senses) of a query q, the better
rewrite is likely the one most related to past search queries relevant
to q. By using rewrite-to-history similarity and history-to-currentquery similarity, our model strives to account for the search intent
that can be implicit in relevant search history. The algorithm builds
on (but is not tied to) an independently-trained general discriminative pairwise query similarity model which extends the machine
learned model in [4]. The pairwise similarity model can be treated
as a black box and is not the focus of this paper. We just note
that we use several types of features (e.g., lexical, semantic, search
log-derived) and train the model on both web click log data and
examples judged by human editors.

where P (m) represents a prior on the size of the relevant history
and P (G|m), a prior on the distribution of relevant history (e.g.,
more recent history tends to be more relevant than distant history).
Let RG,m (R when unambiguous) be the subset of H relevant
to q given a setting of (G, m), and R̄ = H \ R.
P (q, a, H|G, m) = P (q, a|R, R̄, G, m)P (H)
= P (q, a|R)P (R, R̄) = P (q, a, R)P (R̄|R)
where we assume (q, a) is conditionally independent from (G, m, R̄)
given R. P (R̄|R) represents the degree to which it is likely to observe the remaining history R̄ given the history relevant to q.
Finally, we decompose P (q, a, R) as
P (q, a, R) = Q
P (R|q, a)P (q|a)P (a)
(3)
= r∈R P (r|q, a)P (q|a)P (a)
where we assume independence of queries in the history conditioned on q and a. Putting together eqn. 1, 2, and 3, we obtain
P (q, a, H) =

X
G,m

φG,m

Y

P (r|q, a)P (q|a)P (a)

(4)

r∈RG,m

where φG,m is the prior P (G|m)P (m)P (R̄G,m |RG,m ).
A graphical model (GM) corresponding to the history-reformulation
model is shown in Fig. 1. The idea of reformulation transitivity is
embodied in the dependence of q and a through R, even if we
removed the direct edge from a to q.

Copyright is held by the author/owner(s).
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
ACM 978-1-60558-896-4/10/07.

849

(2)

a

0.65

m

0.63

G

r1

...

0.62

rm

accuracy

q

Figure 1: GM representation of the history-reformulation model.

2.2

0.57
0.56

if max s(q, ri ) < α
otherwise

0.2

0.4

0.6

0.8

1

Figure 2: Results on hand-labeled example set with varying history
weight (x-axis) and different α values. Somewhat surprising, putting
all the weight on history (wh = 1) achieves better performance than
using direct query-reformulation similarity (wh = 0). This validates
the idea of transitive query-history-reformulation similarity in that the
a − r − q path in fig 1 is at least as important as the direct a − q link.

Fig. 2 shows improvements on editorial data when history is used
to varying degrees (the no-history baseline, wh = 0, has accuracy
56%). The cosine similarity (§2.3) method has 58% accuracy on
the same data, lower than the all-history extreme (wh = 1) but
higher than the no-history baseline. This reinforces the importance
of indirect query-reformulation similarity. Similar results were observed using the Markov chain method (results did not improve
with increasing random walk iterations).
Testing on the much larger automatically labeled data, the disambiguation algorithm shows similar statistically significant gains
(60% to 64%). An early version of the disambiguation algorithm
was also run on live traffic over a week resulting in a 1% CTR
lift—a significant result for a heavily optimized system.

Require: Query q; prev relevant queries R = {r1 , . . . , rm }; rewrites A =
{a1 , . . . , an }; history weight, wh ; history contribution cutoff, α
Ensure: Output rewrites {ai1 , . . . , ai ′ } with new similarity scores, where
n
{i1 , . . . , in′ } ⊂ {1, . . . , n}
1: Calculate similarity scores, s(r, q), for each r ∈ R
2: For each rewrite a in A
3:
Compute similarity scores s(r, a) for each r ∈ R
4:
Calculate new score snew (q, a)
m
X
s(q, ri )
new
s
(q, a) = w0 s(q, a) + (1 − w0 )
s(ri , a) P
j s(q, rj )
i=1
0.0

0

w_h

Algorithm 1 Transitive disambiguation algorithm

max s(q, ri )

0.6

0.58

To implement eq. 4 into an algorithm that takes advantage of
an optimized pairwise similarity model, we approximate P (r|q, a)
by P (r|a)P (r|q) . This also addresses the data sparsity problem in
estimating P (r|q, a). Finally, we use a uniform prior P (a) and
approximate the summation in eq.4 by a maximization (we pick a
minimum query-history similarity threshold, t, so that R = {h ∈
H|s(q, h) > t}, where s(q, h) is the pairwise similarity score for
(q, h)) and leave the implementation of the full Bayesian formulation for future work. With a log-space transformation and introduction of a history weight, wh , and history cutoff, α (if no history
query passes the α bar of relevance to q, we drop the history contribution altogether), we obtain alg. 1.

(

0.61

0.59

Transitive Disambiguation Algorithm

where w0 = 1 − wh β and β =

0.0
0.25
0.75

0.64

4.

i

CONCLUSIONS AND RELATED WORK

Another simple approach we propose to leverage history for query
reformulation is based on cosine similarity in the vector space whose
axes are the degrees of similarity to each query in the search history: a pair (q, a) has high cosine similarity if q and a tend to
match the same history H similarly.1
For comparison, we have also implemented the Markov chain
random walk method proposed in [1]. A random walk is performed
on a query similarity graph, where the nodes are the current query,
the user’s search history, and candidate rewrites. (q, a) pairs strongly
connected to the same history queries have their similarity scores
boosted and vice-versa for pairs with weak connections.

We introduce a general history-reformulation probabilistic model
and build a simple disambiguation approach on top of an optimized
query-pair similarity function. Compared to the recent vlHMM
model in [3], our approach presents the advantages that (i) reformulations are not restricted to history-based rewrites but can be
derived from arbitrary sources (in our case, from search sessions,
bipartite query-url graphs, and query segment substitutions); (ii)
the model can handle long mixed-goal sessions and put a prior
on its subsets importance; no history segmentation is imposed (iii)
the algorithm is less prone to sparsity problems an applies to rare
queries/sessions.
Our algorithm does not require retraining. This is useful to easily
apply it to a different setting, such as query-document matching,
but can be a limitation in terms of obtaining the best accuracy.

3.

5.

2.3

i

Vector Space Similarity and Random Walks

EVALUATION

We evaluate our algorithms on a sample of 600 history-reformulation
examples assessed by editors as good or bad, and on 2M automaticallylabeled examples. The examples are randomly sampled across all
Yahoo search users over a period of one month. Automatic labeling is done by calculating reformulation click-through rates (CTR)
on a separate large commercial search engine log, and labeling examples with higher than average (normalized by position on the results page) CTR as good reformulations. We summarize the history
into a most relevant bag of words to obtain reliable CTR estimates.
Search history can go back as far as one month.
1

We also extend this approach to the history-rewrite vector space to
encourage rewrite set cohesiveness by favoring rewrites with high
similarity to each other.

850

6.

ACKNOWLEDGMENTS

We thank the reviewers for their extensive feedback.

REFERENCES

[1] P. Boldi et al. The query-flow graph: model and applications. In CIKM
’08, pages 609–618, New York, NY, USA, 2008.
[2] H. Cao et al. Context-aware query suggestion by mining click-through
and session data. In KDD ’08, pages 875–883.
[3] H. Cao et al. Towards context-aware search by learning a very large
variable length hidden markov model from search logs. In WWW ’09.
[4] R. Jones et al. Generating query substitutions. In WWW’06.
[5] X. Shen, B. Tan, and C. Zhai. Implicit user modeling for personalized
search. In CIKM’05, pages 824–831.
[6] B. Tan, X. Shen, and C. Zhai. Mining long-term search history to
improve search accuracy. In KDD’06, pages 718–723.

