Optimal Meta Search Results Clustering
Claudio Carpineto

Giovanni Romano

Fondazione Ugo Bordoni
Rome, Italy

Fondazione Ugo Bordoni
Rome, Italy

carpinet@fub.it

romano@fub.it

ABSTRACT

of information disorganization and redundancy in lists of
search results returned by current search engines in response
to multi-topic queries. If the items that relate to the same
topic have been correctly placed within the same cluster and
if the user is able to choose the right cluster from the cluster labels, such items can be accessed in logarithmic rather
than linear time. Because there are so many available SRC
systems employing very diﬀerent techniques and algorithms
(see [4] for a review), it is tempting to combine their outputs,
just as the results of several search engines can be merged
into a meta search engine. To the best of our knowledge,
this problem has not been addressed so far.
As every clustering algorithm implicitly or explicitly assumes a certain data model, the main rationale for combining multiple clusterings is to try to produce a clustering
with improved accuracy and robustness when such assumptions are not satisﬁed by the sample data. Meta SRC is
clearly related to the the general ﬁeld of meta clustering,
also known as consensus clustering or clustering ensembles
(e.g., [6], [15], [7]), but it poses unique challenges that cannot be easily addressed by available techniques:

By analogy with merging documents rankings, the outputs
from multiple search results clustering algorithms can be
combined into a single output. In this paper we study the
feasibility of meta search results clustering, which has unique
features compared to the general meta clustering problem.
After showing that the combination of multiple search results clusterings is empirically justiﬁed, we cast meta clustering as an optimization problem of an objective function measuring the probabilistic concordance between the clustering
combination and the single clusterings. We then show, using an easily computable upper bound on such a function,
that a simple stochastic optimization algorithm delivers reasonable approximations of the optimal value very eﬃciently,
and we also provide a method for labeling the generated clusters with the most agreed upon cluster labels. Optimal meta
clustering with meta labeling is applied to three descriptioncentric, state-of-the-art search results clustering algorithms.
The performance improvement is demonstrated through a
range of evaluation techniques (i.e., internal, classiﬁcationoriented, and information retrieval-oriented), using suitable
test collections of search results with document-level relevance judgments per subtopic.

• the features or algorithms that determined the clusterings are not accessible (characterized as the hard
ensemble clustering in [14];

Categories and Subject Descriptors

• the need to label the generated clusters with linguistic
descriptions of high quality is very important for SRC
applications, whereas this aspect is usually ignored in
the meta clustering ﬁeld;

H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval—Clustering

General Terms

• the clusterings may have been formed from non-identical
sets of objects;

Algorithms, Experimentation, Measurement, Performance

Keywords

• high computational eﬃciency of the meta clustering
algorithm is required to support real-time applications.

Meta clustering, search results clustering, optimization

1.

In this paper we ﬁrst show that the characteristics of the
outputs returned by multiple SRC algorithms suggest the
adoption of a meta clustering approach. Based on this observation, we introduce a novel criterion for measuring the
concordance of two partitions of n objects into m disjoint
clusters, using the information content associated with the
series of decisions made by the partitions on single pairs of
objects.1 We then cast meta clustering as an optimization
problem of the concordance between the clustering combination and the given set of clusterings. The optimization
framework is the ﬁrst main contribution of the paper.

INTRODUCTION

Search results clustering (hereafter referred to as SRC)
has become a popular means of approaching the problem

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

1
The terms ‘partition’ and ‘clustering’ are often used interchangeably throughout this paper.

170

To solve this problem, we test several metaheuristic methods: through an easily computable upper bound on the
objective function, we show that a simple stochastic optimization method delivers fast approximations of the optimal
value. The clusters of the meta clustering are then labeled
with the most agreed upon cluster labels of the input clusterings. The procedure for building the meta clustering and
labeling its clusters is our second main contribution.
The superior performance of meta SRC over individual
clusterings is demonstrated through a range of evaluation
techniques; i.e., using internal, classiﬁcation-oriented, and
information retrieval-oriented measures. The set of experiments are carried out on two test collections explicitly designed to evaluate the performance of clustering algorithms
that post-process search results, including a newly created
test collection made available for reuse. The extensive evaluation part is our third contribution. Overall, the proposed approach is empirically motivated, theoretically wellfounded, computationally eﬃcient, and highly eﬀective.
The remainder of the paper has the following organization. We ﬁrst compare the results produced by some well
known SRC algorithms, making use of methods and concepts on which we build in the following sections. Then
we introduce the theoretical framework of optimal probabilistic meta clustering and study approximate solutions to
the problem. After describing the procedure for meta labeling the generated clusters, we present the evaluation experiments. We ﬁnally discuss related work on meta clustering
and oﬀer some conclusions.

2.

documents; i.e., documents not grouped under the ‘other
topics’ cluster. We found that the ﬁrst ten clusters of each
method covered, on average, 58% of the 100 input documents. The overlap of truly classiﬁed documents across pairs
of methods was 45%, the documents uniquely classiﬁed by
either method were 31%, and the remaining 24% of documents were unclassiﬁed in both methods.
The next step was to measure the similarity of the clusterings produced by the diﬀerent methods. In the following
set of experiments we considered all 100 search results, thus
including those that were grouped under the ‘other topics’
cluster. While this cluster may be not so useful for information retrieval, it is relevant from the point of view of comparing the two partitions because it contains documents that
cannot be grouped with similar documents. In the limit, if
all the documents were placed into the ‘other topics’ cluster
by both systems, the two systems would be useless but very
similar. To evaluate clustering similarity, we used the Rand
index [13], explained below.
Given a set O of n objects and two partitions of O to
compare, Π1 and Π2 , the Rand index (R) is deﬁned as:
R=

a+d
a+d

= 
a+b+c+d
n
2

(1)

where:
• a: number of pairs of objects in O that are in the same
cluster in Π1 and in the same cluster in Π2 .
• b: number of pairs of objects in O that are in the same
cluster in Π1 and in diﬀerent clusters in Π2 .
• c: number of pairs of objects in O that are in diﬀerent
clusters in Π1 and in the same cluster in Π2 .
• d: number of pairs of objects in O that are in diﬀerent
clusters in Π1 and in diﬀerent clusters in Π2 .

COMPARING SEARCH RESULTS CLUSTERINGS

Method combination usually works well when the results
of the individual methods are diﬀerent and of good quality. In this section we experimentally analyze whether SRC
complies with these requirements. We use four state-of-theart SRC algorithms: Clusty, KeySRC, Lingo, and Lingo3G.
KeySRC [1] and Lingo [11] are research systems2 , while
Clusty and Lingo3G are commercial web clustering engines.
These systems are characterized by highly descriptive phrases
as cluster labels, and are known to perform well on browsing
retrieval tasks [4].
The clusterings to be compared were generated in the following way. We used the 100 most popular web queries
provided by Google Trends as of February 2009. The queries
were submitted to Clusty (with the clustering being restricted
to the ﬁrst 100 documents retrieved for each query), and the
resulting set of snippets was collected and given as input to
other SRC algorithms. In this way we have been able to
include Clusty in the evaluation while ensuring that all the
algorithms operate on the same set of snippets, for full comparability of the results.
In SRC, although the algorithms may produce a variable
number of clusters, only those with the highest coverage are
usually displayed on the ﬁrst results page. Considering the
ﬁrst ten clusters is a typical choice, and is what we do in this
paper. We group all the documents that are not covered by
the ﬁrst ten clusters in a dummy cluster ‘other topics’.
We ﬁrst measured coverage and overlap of truly classiﬁed

Intuitively, one can think of a + d as the number of agreements between Π1 and Π2 , and b + c as the number of disagreements between Π1 and Π2 .3 The Rand index has a
value between 0 (i.e., no agreement on any pair of objects)
and 1 (i.e., when the two partitions coincide).4
In Table 1, in the triangle above the main diagonal of the
matrix, we report the mean Rand index value for each pair
of methods, averaged over the set of queries. From these
ﬁgures, the similarity among clusterings seems consistently
high across method pairs. However, we must consider that
a great contribution to the value of R comes from the pairs
of objects that belong to diﬀerent clusters both in Π1 and
Π2 ; i.e., term d in Equation 1. As term a and term d do not
have equal information in the SRC domain, a more reliable
3
The Rand index assumes that clusters do not overlap. The
clusters generated by the SRC systems were not strictly disjoint, but their overlap was very small: we found that on
average less than one document in ten was assigned to more
than one cluster. When multiple clusters contained the same
document, we simulated a true partition by considering only
the most highly ranked one in the list displayed by the system.
4
As the expected value of the Rand index for random partitions does not take a constant value, the adjusted Rand index
[8] is sometimes preferred, but we did not use it because its
assumptions (e.g., a ﬁxed number of objects in each cluster)
do not ﬁt the SRC data.

2

KeySRC and Lingo and can be tested, respectively,
at http://keysrc.fub.it/Keysrc and http://search.
carrot2.org/stable/search

171

Clusty
KeySRC
Lingo
Lingo3G

Clusty
1
J = 0.28
J = 0.26
J = 0.25

KeySRC
R = 0.67
1
J = 0.27
J = 0.30

Lingo
R = 0.66
R = 0.63
1
J = 0.26

Method

Lingo3G
R = 0.63
R = 0.64
R = 0.60
1

KeySRC
Lingo
Lingo3G
Best combination
Search engine

Table 1: Pairwise similarity of four SRC methods on
100 popular web queries. We show the Rand index
values in the upper triangle of the matrix and the
Jaccard coeﬃcient values in the lower triangle.

kSSL

kSSL

kSSL

kSSL

(k=1)

(k=2)

(k=3)

(k=4)

24.07
24.40
24.00
21.65
21.60

32.39
30.64
32.37
29.28
35.47

38.19
36.57
39.55
33.15
41.96

42.13
40,69
42.97
37.29
47.55

Table 2: Retrieval performance on the Ambient test
collection measured as mean kSSL over the set of
queries, for several values of k.

measure than the Rand index should probably underweigh
the contribution of term d to the similarity.
A drastic solution [2] is to argue that pairs of type d are
not clearly indicative either of similarity or of dissimilarity,
as opposed to counts of “good pairs” (term a) and “bad pairs”
(terms b and c), thus ending up with a formula conceptually
similar to Jaccard’s coeﬃcient:
J=

a
a+b+c

(2)

In the lower triangle of Table 1 we report pairwise Jaccard’s coeﬃcient values. The results clearly show that the
inter-clustering similarity becomes dramatically lower than
that measured with the Rand index, according to such a
strong interpretation. In the next section we will deﬁne a
more balanced way of assessing the importance of each term
in Equation 1, depending on the number of clusters.
As the primary objective of meta SRC is to improve retrieval performance, it is useful to evaluate the eﬀectiveness of the individual methods and analyse whether there
is scope for improvement using a method combination. To
this aim, we used AMBIENT, a test collection introduced in
[3] and downloadable from http://credo.fub.it/ambient.
AMBIENT is explicitly designed for evaluating the subtopic
retrieval eﬀectiveness of systems that post-process search
results. It consists of 44 topics extracted from the ambiguous Wikipedia entries, each with a set of subtopics and a
list of 100 ranked search results collected from a plain web
search engine and manually annotated with subtopic relevance judgments.
As an evaluation measure, we used the Subtopic Search
Length under k document suﬃciency (kSSL), introduced in
[1]. It is deﬁned as the average number of items (cluster
labels or search results) that must be examined before ﬁnding a suﬃcient number (k) of documents relevant to any
of the query’s subtopics, assuming that both cluster labels
and search results are read sequentially from top to bottom,
and that only cluster with labels relevant to the subtopic at
hand are opened. The main features of kSSL are that: (i)
it allows evaluation of full-subtopic retrieval (i.e., retrieval
of multiple documents relevant to any subtopic) rather than
focusing on subtopic coverage (i.e., retrieving at least one
relevant document for some subtopics, as e.g. with subtopic
recall at n); (ii) the modelization of the user search behavior is realistic because the role played by cluster labels is
taken into account, whereas most earlier clustering evaluation studies assume that the user can choose the best cluster
regardless of its label. The kSSL measure can be applied
not only to clustered results but also to ranked lists, thus
allowing clustering performance to be compared to the per-

Figure 1: Retrieval performance variation of SRC
methods on individual Ambient queries using kSSL
with k=2 (search engine list is the baseline).

formance of the original ranked list of search results given
as input to the clustering algorithms (used as a baseline).
The systems being tested were ran on the 100 search results associated with each AMBIENT query and the performance of the corresponding output was evaluated using
kSSL, with k = 1, 2, 3, 4. Again we considered the systems introduced above, except for Clusty, whose data were
not available to us. The results, averaged over the set of
queries, are reported in Table 2. Note that, for k = 1, SRC
did not improve over using the plain list of ranked results,
whereas its superiority becomes clear as the value of k increases, with a comparable mean performance improvement
across diﬀerent methods.
As we were interested in testing the hypothesis that individual methods behave diﬀerently on single queries, we also
performed a query-by-query analysis. In Figure 1, we show
the range of performance variations exhibited by the three
clustering methods over the baseline on each query, using
kSSL with k = 2 as evaluation measure. The diﬀerences
were ample, with a lot of scope for performance improvement: if we were able to select the best method for each
query, we would get the kSSL values reported in the penultimate row in Figure 1.
To summarize the results reported in this section, the individual clusterings were diﬀerent and presented considerable
variations of retrieval performance on single queries, while
demonstrating comparable mean retrieval performance.
Taken together, these ﬁndings indicate the use of a method
combination strategy with the goal of maximizing the agreement with the individual clusterings, much in the same spirit

172

as multiple classiﬁers are combined through a majority vote,
in the hope that the single classiﬁers make uncorrelated errors. This issue is dealt with in the next section.

3.

P C(Π1 , Π2 ) = 

OPTIMAL PROBABILISTIC META
CLUSTERING: PROBLEM DEFINITION

Πopt = arg max
Π∗

2
m−1
,
(4)
m
and the probability that two objects are in the same cluster in one partition and in diﬀerent clusters in the other
partition is

q

(6)

Π

Π2

= cj 2 )

if

=

= cj 2 )

if
if

=
=

Π
cj 1 )
Π
cj 1 )
Π
cj 1 )

∩
∩
∩

Π
(ci 2
Π
(ci 2
Π
(ci 2

Π1
Π2
Π3

q


M P C(Π∗ , Πr )

(10)

r=1

1

n
2


i, j
i<j

arg max
V
V
cΠ
= cΠ
i
j
ΠV
ΠV
ci = cj

q


OCΠV ,Πr (i, j)

r=1

=
=
=

(1, 1, 2, 2, 3, 4, 3, 3, 4, 4)
(1, 1, 3, 4, 2, 2, 3, 4, 3, 4)
(3, 4, 1, 1, 2, 2, 3, 4, 4, 3)

The partitions were chosen in such a way that the same
number of object pairs are grouped together in any pair of
partitions, i.e., the ﬁrst two objects in Π1 and Π2 , the third
and fourth in Π1 and Π3 , the ﬁfth and sixth in Π2 and Π3 .
With such small partitions, the optimal meta clustering
can be found by brute force search. The number N of distinct partitions of n objects into m non-empty clusters is
[5]:

Π

= cj 1 ) ∩ (ci

Π
(ci 1
Π
(ci 1
Π
(ci 1

(9)

(11)
In other words, it suﬃces to consider the two possible cases
(either oi and oj in the same cluster, or oi and oj in diﬀerent
clusters) and compute the corresponding concordance values
at the object-pair level with each given partition. Note that,
in general, there is no guarantee that that there will be an
admissible partition ΠV that fulﬁlls the decisions made when
ΠV
V
V
V
computing M P C V ; e.g., think of cΠ
= cΠ
= cΠ
i
j , cj
k ,
ΠV
ΠV
and ci = ck . The upper bound corresponds to an actual
value if the input partitions coincide: the optimal partition
in this case is the given partition.
As an illustration, consider the following three partitions
of ten objects into four clusters (each position of the vector
corresponds to an object and the value is the cluster to which
the object has been assigned):

Such weights (taken with positive sign for agreements and
negative sign for disagreements) can be used to deﬁne a
measure of probabilistic concordance of two partitions, as
detailed below.
Given a set of n objects O = {o1 , o2 , ....oi , ...., on }, consider two partitions Π1 , Π2 of O into m clusters, with cor1
2
responding object-cluster assignments cΠ
and cΠ
i
i . The
concordance of the two partitions at the object-pair level,
denoted OC (object-pair concordance), is:
Π1



M P CV =

(m − 1)
pb = pc =
,
(5)
m2

with h ph = 1. The smaller the probability, the larger
the information content associated with the observation that
the event indeed occurred. We estimate the weights associated with each of the four possible types of agreements or
disagreements with the self information, i.e:

if (ci

q
1
P C(Π∗ , Πr )
q r=1

As the optimization of M P C is computationally expensive, it is useful to derive an upper bound that can be easily
computed. An upper bound M P C V on M P C can be deﬁned by considering for each pair of objects a contribution
to M P C equal to its maximum theoretical contribution according to the given partitions:



⎧
−log2 m12
⎪
⎪


⎪
⎪
⎪
⎨ +log2 m−1
m2 

OCΠ1 ,Π2 (i, j) =
⎪
+log2 m−1
⎪
2
⎪
⎪
 m 2
⎪
⎩
−log2 m−1
m

(8)

This is our objective function. The optimal partition Πopt
is the one that has maximal concordance with the given
partitions:

pd =



OCΠ1 ,Π2 (i, j)

i, j
i<j

M P C(Π∗ , Π1 , Π2 , ..., Πq ) =

1
pa = 2 ,
(3)
m
the probability that two objects are in diﬀerent clusters
in both partitions is





Based on this pairwise measure of partition concordance,
we deﬁne the meta partition concordance (M P C) between a
single (meta) partition Π∗ and a set of q partitions
Π1 , Π2 , ..., Πq as:

The Rand index and the Jaccard coeﬃcient can be used
even when considering partitions with a diﬀerent number of
clusters. However, when the partitions to be compared have
a ﬁxed number m of clusters, it may be more convenient
to weigh the diﬀerent types of agreements/disagreements on
the basis of their probability of occurring by chance, rather
than just counting them. As m grows, the chance for a pair
of objects to be placed in the same cluster decreases, while
at the same time the chance to be placed in diﬀerent clusters
increases. We can estimate the relative importance of terms
a, b, c, d as a function of m.
Assuming that each object is randomly assigned to one
cluster, the probability that two objects are in a same cluster
in both partitions is

wh = −log2 (ph )

1

n
2

Π
Π

= cj 2 )
Π

= cj 2 )
(7)

where i, j are two objects, with i = j.
The concordance between the two partitions, denoted P C
(partition concordance), is deﬁned as the average OC value
of all pairs of objects:

N (n, m) =

173



m
1 
m
in
(−1)m−i
i
m! i=0

(12)

Method

For n = 10, m = 4, we get: N = 34, 105, wa = 4.0,
wb = wc = 1.41, wd = 0.83. We generated all possible
partitions and computed the M P C score associated with
each, seen as a meta clustering of Π1 , Π2 , Π3 . The optimal
partition is:
Πopt

=

SAHC
SHC

2.08
2.07

Imprv of meta
over best Πi
12.3%
12.1%

Upper
bound
2.38
2.38

Num. of
candidates
14,950
4,967

Table 3: Performance of stochastic optimization
methods on the Ambient collection.

(1, 1, 2, 2, 3, 3, 4, 4, 4, 4),
Method

with M P C = 0.66, while the value of the upper bound
M P C V is 0.77. Note that if we chose one of the original
clusterings as meta clustering, we would get the same MPC
value for each ( i.e., M P C = 0.59), due to their regularities.
Intuitively, the optimal meta clustering retained the three
clusters shared by pairs of individual methods and placed
the remaining objects in the fourth cluster. In this example,
the improvement attainable by the optimal M P C value is
limited, because with few clusters and few objects choosing
one of the original partitions as meta clustering ensures a
fair concordance with the other partitions (including itself).
We will see in Section 4.6 that in practical situations the
increase of the M P C value is larger.
Clearly, brute force methods cannot be applied to ﬁnd
Πopt for values of interest in the SRC domain. For instance,
with 100 objects and 10 clusters, we get ≈ 1039 distinct partitions. In the next section we study approximate solutions
to this problem.

4.

MP C

SAHC
SHC

MP C
1.29
1.29

Imprv of meta
over best Πi
21.9%
21.4%

Upper
bound
1.70
1.70

Num. of
candidates
23,147
5,891

Table 4: Performance of stochastic optimization
methods on the ODP-239 collection.

4.3 Random restart hill climbing
Stochastic hill climbing with random restart iteratively
does hill climbing for a random amount of time, each time
with a new random partition. The best partition is kept:
if a new run of hill climbing produces a better solution, it
replaces the stored one.

4.4 Simulated annealing
In simulated annealing, the current state may be replaced
by a successor with a lower quality. That is, the algorithm
sometimes goes down hills. If the objective function value of
the successor (M P C  ) is lower than that of the current best
partition (M P C), we move to the successor with a prob-

METAHEURISTIC OPTIMIZATION OF
CLUSTERING CONCORDANCE

M P C  −M P C

T
ability equal to e
, where T is a parameter that
decreases slowly, eventually to 0, at which point the algo0
rithm is doing plain hill climbing. We used T = log2T(i+1)
,
with T0 = 10 and i set to the iteration index.

The impracticability of examining every possible partition
naturally leads to the adoption of a hill climbing strategy,
which essentially consists of iteratively rearranging existing
partitions by moving individual objects to diﬀerent clusters,
and keeping the new partition only if it provides an improvement of the objective function. Metaheuristic algorithms [9] are elaborate combinations of hill climbing and
random search to deal with local maxima. In this section
we study the applicability of some well known metaheuristic algorithms to the clustering concordance optimization
problem. Any of the following algorithms starts with a random (potentially poor) solution, because we observed that
choosing more valuable starting points, such as using one
of the given partitions, did not have a clear impact on the
algorithm performance.

4.5 Quantum annealing
Unlike previous algorithms, in quantum annealing the successors of the current partition are not generated by moving
a single object. Ideally, the neighborhood of states explored
by the method should initially extend over the whole search
space, and then should shrink through the computation to
the nearest states. We mimic this behavior by allowing
moves involving both pairs of objects and single objects.
Note that in this way the number of successors grows from
nm to n2 m2 .

4.6 Testing metaheuristic optimization methods

4.1 Steepest ascent hill climbing
In steepest ascent hill climbing all successors of a current
partition are evaluated and the partition with the highest
M P C is chosen. The computation halts when the movement
of single objects no longer causes the objective function to
improve.

The last three algorithms introduced above favor global
optimization, at the cost of exploring a larger portion of
the search space. However, preliminary tests suggested that
they converged to acceptably good solutions more slowly.
Based on this observation, we focused on the ﬁrst two algorithms, namely steepest ascent hill climbing (SAHC) and
stochastic hill climbing (SHC), and made a systematic evaluation of their performance.
For each query and each method, we computed the M P C
value of the meta partition, the improvement over the initial partition with the best M P C score, the upper bound
value on Πopt , and the number of candidate partitions generated during the search. In Table 3 we show the results for
Ambient, averaged over the query set. In Table 4 we show
the analogous results on a diﬀerent test collection, termed
ODP-239, that will be discussed in Section 6.1.

4.2 Stochastic hill climbing
Stochastic hill climbing does not examine all successors
before deciding how to move. Rather, it selects a successor
at random, and moves to that successor provided that there
is an improvement of M P C. The computation usually halts
when we have not been able to choose a better successor
after a ﬁxed number of attempts. In our case, consistently
with the termination criterion used for the steepest ascent
hill climbing algorithm, we test all possible successors before
halting the search.

174

The meta partition was always much better than the best
initial partition across both data sets. Furthermore, the
results show that the meta clusterings generated by the
two methods were reasonable approximations of the optimal meta clusterings because the gap between the heuristic
values and the upper bound of the optimal value was not
large. This was especially true for the Ambient collection,
where the heuristic M P C value was indeed very close to the
upper bound value for several queries. The results also show
that the two methods built meta clusterings with very similar M P C values, but SHC explored a much smaller portion
of the search space than SAHC. In practice, the processing times were in the order of hundreds of milliseconds on
a computer of medium power (2.8 Ghz CPU, 4GB RAM),
with SHC being about four times faster than SAHC.

5.

OPTIMSRC
Imprv over KeySRC
Imprv over Lingo
Imprv over Lingo3G
Imprv over search eng
Imprv over best comb

kSSL

kSSL

kSSL

(k=2)

(k=3)

(k=4)

20.56
14.5%*
15.7%*
14.3%*
4.8%
5.0%

28.93
10.6%*
5.5%
10.6%*
18.4%*
1.1%

34.05
10.8%*
6.8%
13.9%*
18.8%*
-2.7%

38.94
7.5%*
4.3%
9.4%*
18.1%*
-4.4%

Table 6: Retrieval performance improvement of OPTIMSRC over individual clustering methods, baseline, and best combined method.

least one of the subtopics of ‘Bronx’ deﬁned in the Ambient
collection.

6. EVALUATION

META LABELING

In this section we describe the test collections used in the
experiments and three complementary techniques to validate
the results of OPTIMSRC compared to those of its input
algorithms.

After ﬁnding the optimal partition, we need to label its
clusters. This is a very important key to the success of meta
SRC as a browsing retrieval system, because a cluster with
a poor label is very likely to be entirely omitted by the user
even if it points to a group of strongly related and relevant
documents. We do not generate cluster labels on our own,
because this is a diﬃcult task and it would require accessing
the input documents. We take advantage of the fact that
the individual SRC algorithms return phrase labels of high
quality, and devise a procedure to select the most agreed
upon labels from those given as input.
The procedure takes into account the characteristics of
the set of labels provided by the individual SRC algorithms.
We observed that, on average, the number of labels that
are shared by a pair of SRC algorithms is only 9% of the
total labels generated. By contrast, if we consider the single
distinct non-stop words contained in the set of cluster labels
generated for a given query, the similarity between pairs of
methods is much higher, with a mean Jaccard index value
of 0.24.
The meta labeling algorithm consists of three steps:
1. We associate with each cluster of the meta partition
a set of candidate labels, formed by all labels under which
each document in the cluster has been classiﬁed in at least
one individual method.
2. We assign a score to each candidate label based on both
its extensional coverage of the set of objects and intensional
coverage of the set of labels. The purpose of the intensional
factor is to promote syntactically diﬀerent labels that refer
to the same concept. The exact formula is the following:
Score(l) = count(obj) ·

kSSL
(k=1)



count(w),

6.1 Test collections
There is no standard test collection for evaluating SRC
algorithms. In addition to using Ambient, introduced in
Section 2, we created a new, larger test collection, termed
ODP-239. ODP-239 combines the features of search results
data with those of classiﬁcation benchmarks. It consists
of 239 topics, each with about 10 subtopics and 100 documents. Each document is represented by a title and a
very short snippet. The topics, subtopics, and their associated documents were selected from the top levels of the
Open Directory Project (http://www.dmoz.org), in such a
way that the distribution of documents across subtopics reﬂects the relative importance of subtopics. Unlike Ambient,
all documents are relevant to at least one subtopic and the
document-subtopic assignment comes for free. ODP-239 and
Ambient have complementary aspects: the former collection
deals with ambiguous queries and is suitable for information
retrieval, the latter is about truly multi-topic queries and is
aimed at classiﬁcation. ODP-239 is available for download
at http://credo.fub.it/odp239.

6.2 Subtopic retrieval
In this section we evaluate the subtopic retrieval eﬀectiveness of OPTIMSRC. We use the same experimental setting
as previous experiments with individual methods reported
in Table 2. For each topic we found the meta partition associated with the clusterings produced by KeySRC, Lingo,
and Lingo3G, and computed the corresponding kSSL values. This experiment was limited to the Ambient collection.5 . The results are shown in Table 6. Asterisks are used
to denote that the diﬀerence is statistically signiﬁcant, using
a two-tailed paired t test with a conﬁdence level in excess of
95%.
OPTIMSRC obtained better results than any individual
method for all evaluation measures, with most diﬀerences
being statistically signiﬁcant. Unlike the individual meth-

(13)

w∈l

where count(obj) is the number of search results in the
cluster that are labeled by l, w is a non-stop word contained
in l, and count(w) is the number of distinct labels in the
cluster that contain word w.
3. We select the label with the highest score.
Hereafter, the full clustering method consisting of generation of the meta partition with stochastic hill climbing
followed by meta labeling will be referred to as OPTIMSRC
(OPTImal Meta Search Results Clustering). As an illustration, consider the query ‘Bronx’. We show in Table 5 the set
of cluster labels generated by each clustering algorithm (including OPTIMSRC) that were judged to be relevant to at

5
The computation of kSSL requires that we know which
cluster labels are relevant to each query’s subtopic. Such
relevance judgments were available to us for Ambient, but
not for ODP-239, which is why this set of experiments could
not be replicated on the former test collection.

175

Bronx query
Subtop1
Subtop2
Subtop3
Subtop4

OPTIMSRC
Borough of New York city
/ Bronx County
Bronx River
Bronx Music
Bronx Zoo

KeySRC
city’s borough / Bronx NY
/ Bronx district
Bronx River
Bronx Zoo

Lingo
Borough of New York city /
Bronx County / Bronx district
Bronx River
Music
Bronx Zoo

Lingo3G
New York /
Bronx County
Bronx River
Bronx Music
Bronx Zoo

Table 5: Cluster labels relevant to the Ambient subtopics for the query ‘Bronx’. The subtopic deﬁnitions are
the following. Subtop1: ‘The Bronx, one of the ﬁve boroughs of New York City’; Subtop2: ‘Bronx River,
a river that ﬂows south through The Bronx’; Subtop3: ‘The Bronx(band), an American punk rock band’;
Subtop4: ‘Bronx Zoo’.
Collection
Ambient
ODP-239

OPTIMSRC
0.27
0.25

KeySRC
0.21
0.20

Lingo
0.22
0.19

Lingo3G
0.14
0.15

ter), thus leading to more recognizable cluster structures in
the feature space.
On the other hand, the relatively low mean absolute values of S for all SRC systems, including OPTIMSRC, indicate that the separation between clusters was not always
clear. This is not surprising, given that neither the individual clusterings nor OPTIMSRC were explicitly optimized for
cohesion or separation. Also, perhaps more importantly, we
must consider that such algorithms perform sophisticated
forms of feature construction to detect inter-document similarities that go beyond the bag-of-words approach. Thus,
the feature space in which we deﬁned the distance function
used to compute S is not the same as that employed to do
the clustering. The former is the space of the original single terms describing the documents, the latter is a space of
phrases that do not necessarily occur in exactly the same
form in the documents to which they are assigned by the
algorithms.

Table 7: Mean silhouette coeﬃcient value of SRC
systems on the Ambient and ODP-239 collections.

ods, OPTIMSRC improved on the baseline not only for
k ≥ 2 but also for k = 1. Note that for k = 1, k = 2, the
performance of OPTIMSRC was even better than the performance that we would obtain by selecting the best individual
method for each query. Although somewhat surprising, this
is perfectly consistent with the approach proposed here, because the meta strategy does not combine the performance
results of individual methods but truly integrates their performance components.

6.3 Internal measures: the silhouette coefficient

6.4 Ground-truth validation

Internal indices of cluster validity use only information
present in the data set. Most of them (see [12] for a comprehensive summary) are based on the notions of cluster cohesion, i.e., how close the objects in a cluster are, and cluster
separation, i.e., how distinct a cluster is from other clusters.
The popular method of silhouette coeﬃcients combines both
cohesion and separation. The silhouette coeﬃcient si for an
individual object i is deﬁned as:
si =

bi − a i
,
max (ai , bi )

Ground truth validation is aimed at assessing how good a
clustering method is at recovering known clusters (referred
to as classes) from a gold standard partition. For this experiment we use only the ODP-239 collection, because many
documents in Ambient are not assigned to any category (i.e.,
those search results that were not relevant to any of the
query’s subtopics listed in Wikipedia).
Several evaluation measures are available for this task (see
[10] for a detailed summary), including the well known Fβ
measure [16] that combines precision P and recall R:

(14)

where ai is the average distance of object i from all other
objects in its cluster, and bi is the minimum average distance to objects in another cluster. The value of the silhouette coeﬃcient can vary between −1 and 1; a negative value
is undesirable because this corresponds to ai > bi , meaning
that the distance of data objects to the center of their cluster is greater than the distance to the next nearest cluster.
The silhouette coeﬃcient S of a clustering is deﬁned as the
average silhouette coeﬃcient of all objects.
We computed the S value of the individual clusterings and
of OPTIMSRC for each query, representing objects as tf-idf
weighted term vectors (up to text normalization) and using
the formula 1 − cosine similarity as distance function. In
Table 7, we show the results for each collection averaged
over the corresponding query set.
As OPTIMSRC achieved much better results than the individual methods, it may be conjectured that the use of meta
clustering helped to remove noisy object-cluster assignments
(i.e., objects that could not be clearly assigned to one clus-

Fβ =

(β 2 + 1)P R
,
β2P + R

(15)

with
P =

TP
TP
, R=
TP + FP
TP + FN

(16)

where T P , T N , F P , and F N are respectively, the number of true-positives (i.e., two documents of the same class
assigned to the same cluster), true-negatives (i.e., two documents of diﬀerent classes assigned to diﬀerent clusters),
false-positives (i.e., two documents of diﬀerent classes assigned to the same cluster), and false-negatives (i.e., two
documents of the same class assigned to diﬀerent clusters).
The parameter β is a weighting factor for the importance of
the recall (or precision).
Because separating documents of a same class usually has
a worse eﬀect than placing pairs of documents of diﬀerent
classes in the same cluster, at least in the SRC domain,

176

Method
OPTIMSRC
KeySRC
Lingo
Lingo3G

F1
0.313
0,295
0.273
0.311

F2
0.341
0.318
0.283
0.292

F5
0.380
0.341
0.294
0.285

include an experimental comparison with other meta clustering methods such as ﬁnding the median partition.

9. ACKNOWLEDGMENTS
We would like to thank Stanislaw Osiński and Dawid Weiss
for providing us with the results of Lingo and Lingo3G, and
four anonymous reviewers for their comments.

Table 8: Classiﬁcation performance on the ODP239 collection measured as mean (micro-averaged)
Fβ over the set of queries, for several values of β.

10. REFERENCES
[1] A. Bernardini, C. Carpineto, and M. D’Amico.
Full-Subtopic Retrieval with Keyphrase-Based Search
Results Clustering. In Proc. Web Intelligence 2009,
Milan, Italy, pages 206–213. IEEE Computer Society,
2009.
[2] R. J. G. B. Campello. A fuzzy extension of the Rand
index and other related indexes for clustering and
classiﬁcation assessment. Pattern Recognition Letters,
28(7):833–841, 2007.
[3] C. Carpineto, S. Mizzaro, G. Romano, and
M. Snidero. Mobile Information Retrieval with Search
Results Clustering: Prototypes and Evaluations.
JASIST, 60(5):877–895, 2009.
[4] C. Carpineto, S. Osiński, G. Romano, and D. Weiss. A
survey of Web clustering engines. ACM Computing
Survey, 41(3), 2009.
[5] G. L. Liu. Introduction to Combinatorial Mathematics.
McGraw Hill, 1968.
[6] A. Fred and A. Jain. Data clustering using evidence
accumulation. In ICPR, pages 276–280, 2002.
[7] A. Goder and V. Filkov. Consensus Clustering
Algorithms: Comparison and Reﬁnement. In Proc.
ALENEX 2008, San Francisco, CA, USA, pages
109–117, 2008.
[8] L. Hubert and P. Arabie. Comparing partitions.
Journal of Classiﬁcation, 2(1):193–218, 1985.
[9] S. Luke. Essentials of Metaheuristics. 2009. available
at http://cs.gmu.edu/∼sean/book/metaheuristics/.
[10] C. D. Manning, P. Raghavan, and H. Schütze.
Introduction to Information Retrieval. Cambridge
University Press, 2008.
[11] S. Osiński and D. Weiss. A Concept-Driven Algorithm
for Clustering Search Results. IEEE Intelligent
Systems, 20(3):48–54, 2005.
[12] M. S. P.-N. Tan and V. Kumar. Introduction to Data
Mining, chapter 8: Cluster analysis: basic concepts
and algorithms, pages 487–568. Addison Wesley, 2005.
[13] W. M. Rand. Objective criteria for the evaluation of
clustering methods. Journal of American Statistical
Association, 66:846–850, 1971.
[14] A. Strehl and J. Ghosh. Cluster Ensembles – A
Knowledge Reuse Framework for Combining Multiple
Partitions. JMLR, (3):583–617, 2002.
[15] A. Topchy, A. K. Jain, and W. Punch. Clustering
ensembles: models of consensus and weak partitions.
IEEE Trans. PAMI, 27(12):1866–1881, 2005.
[16] K. van Rijsbergen. Information Retrieval.
Butterworth-Heinemann, 1979.
[17] Y. Wakabayashi. The Complexity of Computing
Medians of Relations. Resenhas, 3(3):323–349, 1998.
[18] H. Wang, H. Shan, and A. Banerjee. Bayesian Cluster
Ensembles. In Proc. SDM 2009, pages 209–220, 2009.

we give more weight to recall. In Table 8 we show the mean
(micro-averaged) Fβ values for each clustering method, with
β = 1, 2, 5.
OPTIMSRC clearly outperformed the other methods for
all evaluation measures, with higher values of β leading to
greater performance improvements, up to 11.76 % over the
best individual method for β = 5. Note that although OPTIMSRC was a clear improvement over individual methods,
its performance is, on an absolute scale, still relatively low.
This is probably due to the intrinsic diﬃculty of the classiﬁcation task on the ODP-239 collection, where documents are
very short and subtopics do not always have very distinct
meanings.

7.

RELATED WORK

While there is no earlier work on meta SRC, the general problem of ﬁnding a meta (or consensus) clustering
from multiple partitions has been approached from various
perspectives (e.g., graph-based, statistical, and combinatorial), using, among others: hypergraph partitioning [14], coassociation matrix [6], mixture model [15], and Bayesian approach [18].
Most relevant to us is the work on ﬁnding the median
partition, that is the partition that minimizes the distance
to the given partitions. Similar to our paper, the key to
ﬁnding the most important commonalities and diﬀerences
among partitions are the decisions made on single pairs of
objects. Under the hypothesis that the distance between
partitions
by the number of disagreements, i.e.,

 is measured
n
− (a + d), the median partition problem is
b+c =
2
known to be NP-complete [17] and various heuristics can
be used for approximating it [7]. In contrast to our work,
the objective function is strictly modeled after the notion of
binary agreements/disagreements, in a Rand index style.

8.

CONCLUSIONS AND FUTURE WORK

In this paper we studied the problem of meta search results clustering. We introduced a novel probabilistic criterion for combining the results of a given set of partitions of n
objects into m clusters, and showed that a simple stochastic
optimization algorithm delivers fast approximations of the
optimal value. Through a range of evaluation techniques,
we showed that optimal meta SRC, enriched with meta labeling, is more eﬀective than the individual SRC algorithms,
and it is also eﬃcient for real-time applications.
Two natural research directions are the extension of the
proposed framework to partitions with a variable number of
clusters and to partitions of diﬀerent but overlapping sets of
objects (e.g., web clustering engines that fetch their search
results from distinct search engines). Future work will also

177

