Effective Music Tagging through Advanced Statistical
Modeling
Jialie Shen†
†

Meng Wang‡

Shuicheng Yan

HweeHwa Pang†

Xiansheng Hua‡

School of Information Systems, Singapore Management University, Singapore
{jlshen, hhpang}@smu.edu.sg
‡

Microsoft Research Asia, Beijing, China
{mengwang, xhua}@microsoft.com



Department of ECE, National University of Singapore, Singapore
eleyans@nus.edu.sg

ABSTRACT

1. INTRODUCTION

Music information retrieval (MIR) holds great promise as a
technology for managing large music archives. One of the
key components of MIR that has been actively researched
into is music tagging. While signiﬁcant progress has been
achieved, most of the existing systems still adopt a simple
classiﬁcation approach, and apply machine learning classiﬁers directly on low level acoustic features. Consequently,
they suﬀer the shortcomings of (1) poor accuracy, (2) lack of
comprehensive evaluation results and the associated analysis based on large scale datasets, and (3) incomplete content
representation, arising from the lack of multimodal and temporal information integration.
In this paper, we introduce a novel system called MMTagger that eﬀectively integrates both multimodal and temporal information in the representation of music signal. The
carefully designed multilayer architecture of the proposed
classiﬁcation framework seamlessly combines Multiple Gaussian Mixture Models (GMMs) and Support Vector Machine
(SVM) into a single framework. The structure preserves
more discriminative information, leading to more accurate
and robust tagging. Experiment results obtained with two
large music collections highlight the various advantages of
our multilayer framework over state of the art techniques.

Music is an unique art form created by human to represent emotion, cultural background, social context and time.
To facilitate music information retrieval (MIR) from large
music collections, it is necessary to annotate the music documents with comprehensive textual information [2, 16, 24].
Existing music tagging approaches generally perform musical feature extraction, followed by applying machine learning
methods to model the relationship between text labels and
music features. The approaches hinge on two interrelated
issues: (1) the extraction of high quality acoustic features
to represent multiple music characteristics, and (2) the judicious application of statistical model(s) for classiﬁcation
and tagging. The eﬀectiveness of the approaches should be
demonstrated through a proper evaluation process involving
large test collections and appropriate benchmarking metrics.
There has been a long history of using low level acoustic
features extracted from audio objects as content descriptors [13, 3, 10, 9]. Unfortunately, how to eﬀectively derive high-level semantic concepts (such as genre and mood)
from the physical features still remains an extremely diﬃcult problem. There are several reasons for this. First, there
is a gulf between high level concepts and low level acoustic
characteristics, as evident by the mismatch in semantic similarity in the search results produced by systems that rely
solely on low level features [5]. Second, the content of music
is rich and complex, spanning a wide range of features like
timbral texture, harmony, rhythm structure and pitch [21,
27, 12, 19, 20, 30]. It is thus imperative to employ a content
representation that captures these features comprehensively,
and to determine which features to use for what purpose. In
view of the challenges, it is not surprising that existing music tagging systems that adopt a simple approach of applying machine learning classiﬁers directly on low level acoustic
features do not deliver good performance.
In this work, we propose a framework called MMTagger
(Multifeature based Music Tagger) that combines advanced
feature extraction techniques and high level semantic concept modeling for eﬀective annotation of music documents.
The basic idea for the proposed scheme is to model music
information (text based description) with hierarchical structure and relationship between tags and concepts. It tries to
map sound documents to a representation in the so-called
latent musical concept space, where relevance between documents and tags can be more accurately modeled than in the

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval,
Search process; I.2.m [Computing Methodologies]: Artiﬁcial Intelligence; H.5.5 [Sound and Music Computing]:
Systems

General Terms
Algorithms, Design, Experimentation, Human Factors

Keywords
Music Information Retrieval, Tagging, Browsing, Search

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

635

Symbols
C
s
f
F
t
T
Gc
wk
μk
Σk
K
V
|V |
A
M
mss
tss
tes
pt
λ
r

acoustic feature space. The MMTagger’s architecture comprises three interconnected functionality layers. The technical design of the ﬁrst layer aims at not only providing high
quality feature combination but also to incorporate temporal information. The latter is motivated by the observation
that music documents belonging to the same category generally share certain temporal patterns. The second layer of the
proposed system is for discriminative musical concept modeling, and is intended to bridge the ’semantic gap’ between
the low level music features in the ﬁrst layer, and the music
tags in the third layer. Here, we utilize multiple Gaussian
Mixture Models (GMMs) to represent diﬀerent concepts [15,
7]. Since a semantic concept could be relevant to many different keywords, the third layer contains multiple support
vector machines (SVM), each trained to derive the likelihood
score of a tag from its association strength with the various
music concepts. We have conducted a comprehensive experiment study with two large test collections. The results
indicate that our solution achieves substantial performance
improvement in accuracy and robustness in annotating music documents.
The rest of the article is structured as below: Section 2
gives a brief overview of related work in the area of music
tagging, including their assumptions and limitations. In Section 3, we provide details on our proposed architecture and
introduce the structure of each system component module
and its learning algorithms. Section 4 reports on our experiment conﬁguration while Sections 5 presents empirical
evaluation results. Finally, our conclusions and directions
for future research are summarized in Section 6.

2.

Table 1: Summary of symbols and definitions
for the purpose of ranking search results. The acoustic feature considered in this system is MFCC. Using the CAL500
dataset, they achieved a nice performance improvement in
retrieval and annotation accuracy. More recently, Duan et
al. designed an interesting approach for collective annotation of music data [6]. It assumes that there are certain
levels of correlation between diﬀerent tags. Studying the relationship is useful for improving annotation performance.
They employed two diﬀerent statistical models - GMMs and
Conditional Random Field to exploit the label correlation.
Experiment results demonstrate a small but consistent performance gain. In addition, Bertin-Mahieux et al. proposed Autotagger system using advanced ensemble learning
schemes to combine discriminative power of diﬀerent classiﬁers [8, 4]. Those schemes include AdaBoost and FilterBoost. Acoustic features considered by the scheme include
20 MFCC Coeﬃcients, 176 autocorrelation coeﬃcients, and
85 spectrogram coeﬃcients. Experiment results based on the
CAL500 dataset and another large test collection demonstrate that Autotagger performs better than MSML. It is
currently the most advanced technique for music tagging.

RELATED WORK

Automated music tagging is an important research problem with numerous applications such as music search and
music recommendation. This area has received considerable
attention and many related techniques have been developed
in recent years. Among the earliest of such systems, Whitman and Rifkin [29, 28] proposed a novel Regularized LeastSquares Classiﬁcation (RLSC) based approach. The goal is
to derive non-linear relationship between text captions and
acoustic features in an eﬃcient way. For performance evaluation, 255 songs from 51 performers are separated into training and testing sets with roughly equal size. Using the SVM
classiﬁer, accuracy achieved ranges from 0.0% to 38.9% depending on the terms used for evaluation process. In [23],
Turnbull et al. applied a supervised multiclass naı̈ve Bayes
model to estimate relationship between musical sound and
words. The features considered by this system can be classiﬁed into two categories - textual features and audio features
(e.g., dMFCC and auditory ﬁlter-bank temporal envelope
features). The test collection contains totally 2,131 songs
and their song reviews. Using dMFCC feature, precision
and recall rates achieved by the system is 0.072 and 0.119
for the annotation task. To facilitate eﬀective music retrieval
with semantic description, Turnbull et al. developed a music labeling scheme based on the supervised multi-class labeling model (SML) [26, 25]1 . In this approach, sound documents are modeled as a GMM distribution over a set of
predeﬁned terms (corpus). The distance between the multinomial distributions of keyword query and a music feature
can be estimated with the Kullback-Leibler (KL) divergence
1

Deﬁnitions
Total number of high level music concepts
Notation of music segment s
Notation of feature f
Total number of acoustic features extracted
Notation of tag t
Total number of tags
GMMs for music concept c
Weight of the kth Gaussian component
Mean of the kth Gaussian component
Covariance matrix of the kth Gaussian component
Number of mixture components in GMMs
Vocabulary of test collection
Size of vocabulary
Annotation length
Transformation matrix
Music segment s
Starting time of music segment s
End time of music segment s
Probability for music tag t
Likelihood vector generated by DCML
Tag relevance vector generated by TRL

3. A TAGGING FRAMEWORK WITH MULTILAYER STRUCTURE
This section presents a novel scheme to facilitate eﬀective automated tagging over large music collections. As illustrated in Figure 1, the architecture of our system consists of three functionality layers: music preprocessing layer
(MPL) for music sequence segmentation and acoustic feature
extraction, discriminative concept modeling layer (DCML)
and SVM based tag reﬁnement layer (TRL). Similar to an
Artiﬁcial Neural Network [7], each layer is fully connected
with each other. The ﬁrst layer MPL aims to extract four
diﬀerent features including timbral feature, spectral feature,
rhythm feature and melody feature. Using those features,
a set of statistical models based on GMMs are constructed
to characterize high level musical concepts in database, one
GMMs per concept (e.g., genre, singer, mood). Those concepts can be treated as the most fundamental component of

In this paper, we use MSML to denote this system.

636

Music Document 2

GMMs for
Concept 2

Spectral
Feature
Exraction

Music
Segmentation

Relevance Score
for Tag 1

SVM for Tag 2

Relevance Score
for Tag 2

SVM for Tag 3

Relevance Score
for Tag 3

SVM for Tag 4

Relevance Score
for Tag 4

SVM for Tag T

Relevance Score
for Tag T

GMMs for
Concept 1

Timbral
Feature
Extraction

Music Document 1

SVM for Tag 1

Rhythmic
Feature
Extraction

GMMs for
Concept 3

Music Document i
Melody
Feature
Extraction
GMMs for
Concept C

Raw Music
Signal Input

Music Concept
Modeling with Multiple GMMs

Music Feature
Extraction

Music Tag
Modeling with SVM

Figure 1: Architecture of MMTagger scheme.
computed include Mel-Frequency Cepstral Coeﬃcients
(MFCCs) [13], Spectral Centroid, Rolloﬀ, Flux, LowEnergy feature [27], and Spectral Contrast [14]. The
total dimensionality of timbral features calculated is
20.

latent musical concept space. The output of this layer is a
set of likelihood scores, which serve as input to TRL. The
TRL contains a collection of SVM based tag classiﬁer, each
is trained to generate relevance score for a tag. Based on
the relevance scores, we can ﬁnally rank each tag and pick
the top N tags to annotate the input music. The following sections elaborate on each of the layers and give a full
description of the associated algorithms.

• Spectral features (SF) characterize the spectral composition of music signal. In our implementation. each
spectral feature vector contains Auto-regressive (AR)
features; Spectral Asymmetry, Kurtosis, Flatness, Crest
Factors, Slope, Decrease, Variation; Frequency Derivative of Constant-Q Coeﬃcients; and Octave Band Signal Intensities [14]. The total dimensionality of these
feature vectors is 20.

3.1 Music Preprocessing Layer
The function of this layer is to preprocess raw audio signal and compute music features. The related process comprises two steps: music segmentation and feature extraction.
When an audio signal is received, it is ﬁrst partitioned into
several short ﬁxed length time-frames. For this study, we
set the length of each frame to be 0.5 second. Distinguished
from previous tagging schemes, we introduce a temporal
descriptor in the music content representation. Its main
advantage is better content description capability through
combining both acoustic information and temporal information. For a segment with starting time tss and end time tes ,
the corresponding temporal musical descriptor is deﬁned as,
tdf (mi , mss ) = extraf (mi , mss )

• Rhythmic features (RF) summarize the patterns of
a music object over a certain duration. The rhythmic
features calculated in this study include: Beat Histogram [27]; Rhythm Strength, Regularity and Average
Tempo [14]. The total dimensionality is 12.
• Melody features (MF) describe the pitch content
and its duration in a music document. The Pitch Histogram proposed in [27] is used as melody features in
our proposed framework. The total dimensionality of
this group of features is 48.

(1)

where tdf (mi , mss ) denotes the feature f calculated from
the segment mss = (tss , tes ) of music ﬁle mi and extraf is
an extraction function for feature f . Each music document
is treated as a composite of diﬀerent kinds of acoustic feature vectors with temporal information. The motivation derives from the observation that discriminative characteristics
are often embodied within local temporal acoustic features.
Thus the proposed temporal based feature enjoys greatest
potential to provide more comprehensive summarization for
the purpose of classiﬁcation. The MMTagger system considers four diﬀerent kinds of music features:

Accordingly, the ﬁnal content representation includes four
diﬀerent groups of musical features (local content information) and time information (temporal information). Total
dimensionality of the feature set considered in this study is
100.

3.2 Discriminative Concept Modeling Layer
For the second layer of the proposed MMTagger system,
multiple GMMs are trained to statistically model the relationship between each high level concept and various acoustic features. Those high level concepts constitute the latent
musical concept space. Each high level music concept corresponds to one GMMs. GMMs is among the most widely
applied statistical analysis methods due to its ﬂexibility of

• Timbral features (TF) characterize the timbral property of music objects. We apply short time Fourier
transform in the calculation. The timbral features

637

representing diﬀerent kinds of distributions. However, gaining an accurate estimation of the distribution of the music
features associated with to a high level concept goes beyond
a straightforward application of the GMM method. While
the distance between two music concepts can be estimated
via the KL divergence between their GMMs, accurate result
cannot be expected in general. There are two main reasons:

When EM iteration stops, the resulting GMMs is the Universal Background Model (UBM).

3.4 Discriminative Concept Adaptation
The purpose of discriminative concept adaptation is to estimate the GMMs parameter belonging to a certain music
concept from a UBM. After it, a series of GMMs {G1 , ..., GC }
= {P 1 (x|θ1 ), P 2 (x|θ2 ), ..., P C (x|θC )} are constructed with
each GMMs approximating distribution over the audio feature space of a high level concept. A special transformation
matrix M on the mean vectors of the GMMs is designed
to enhance classiﬁcation capability further. Since each high
level music concept is represented by a GMMs, the distance
between two concepts can be measured using the KL divergence between their GMMs. However, since the KL divergence of GMMs is not analytically tractable, we use the
upper bound of the divergence for calculating distance. It
can be proved that

• Due to the limited number of learning examples (music clips) for a certain music concept, it is very hard
to estimate the parameters of a GMM robustly and
accurately.
• The KL divergence between GMMs does not take the
concept label information into account and consequently
can result in poor discriminative ability.
To solve those problems, we develop a two-step adaptation
approach to construct the GMMs based on adaptive learning [1]. It includes generative adaptation and discriminative
music concept adaptation. Generative adaptation has been
widely explored in many tasks such as speaker identiﬁcation
and image categorization [11, 18]. It tries to use all learning
samples in the training process and then obtain the Universal Background Model (UBM), which is the GMMs optimized by the principle of Maximum a Posteriori (MAP).
In the second step, discriminative music concept adaptation
is designed to adjust the mean vectors of each GMMs to
achieve the targets of 1) keeping the music documents belonging the same semantic concept closer, and 2) separating
out those with diﬀerent labels. In this way, the obtained
GMMs exhibit better classiﬁcation capability.

D(Ga ||Gb )

≈

wk N (x; μk , Σk )

(2)

k=1

where wk , μk and Σk are the weight, mean and covariance
matrix of the kth Gaussian component, respectively. x is
input feature vector. K is the total number of Gaussian
components and the probabilistic density is calculated as a
weighted combination of K Gaussian densities,
1

e− 2 (z−μk )

N (x; μk , Σk ) =

T

Σ−1
(z−μk )
k

d
2

(2π) |Σk |

1
2

.

wk N (x; μk , Σk )
,
K
P
wk N (x; μk , Σk )

(3)

where nk =
vectors via

(6)

(7)

where Dij = ||M xi − M xj ||2 . The objective is to maximize
the expected number of samples correctly classiﬁed. Thus,
we have,
f (M ) =

(4)

X X exp(−Dij )
P
k=j Djk
i j∈C

(8)

i

where Ci denotes the set of samples in the same class as i-th
sample. After carrying out diﬀerentiation with respect to
the transformation matrix M , we can obtain,

k=1
K
P

K
1X
a
b
wk (μak − μbk )T Σ−1
k (μk − μk )
2 k=1

e(−Dij )
pij = P
k=j Djk

The parameters of UBM are estimated using the traditional EM algorithm. In the E-step, the posterior probability is calculated via
P r(k|xi ) =

wk D(N (x; μak , Σk )||N (x; μbk , Σk ))

where μak and μbk respectively denote the mean of the kth
component from music concept a and b. The current model
cannot yet achieve optimal classiﬁcation performance because inter-class and intra-class distances are not taken into
account. To improve its eﬀectiveness, Neighborhood Component Analysis (NCA) is performed to derive transformation matrix M and apply it on D(Ga ||Gb ). NCA is a learning method, which constructs a distance metric optimizing
leave-one-out (LOO) performance based on training data.
An inﬁnitesimal change in A may change the neighbor graph
and thus lift LOO discrimination power by a ﬁnite amount.
NCA adopts a more well behaved measure of nearest neighbor performance by introducing a diﬀerentiable cost function
based on stochastic neighbor assignment in the transformed
space. Each point i in multidimensional feature space selects
another point j as its neighbor with certain probability pij ,
and inherits its class label from the selected point. pij is
deﬁned as

The UBM obtained in the initial phase of training the
GMMs is denoted as,
K
X

K
X
k=1

3.3 Generative Adaptation

G = P (x|θ) =

≤

P r(k|xi ) ,and the M-step updates the mean

k=1

μ̂k =

1
nk + r

n
X
i=1

P r(k|xi )xi +

r
μk
nk + r

XX
X
∂f
=−
pij (qij −
pik qik )
∂M
i j∈C
i

(5)

where

638

k

(9)

qik =

K
P
k=1

a
b
a
b
wk Σ−1
k M (μk − μk )(μk − μk )

Since the size of the CAL500 data set is relatively small,
we developed the second test collection called TS2. It contains 4000 popular music items downloaded from Youtube.
They are performed by 110 diﬀerent singers including 55 females and 55 males. The music documents are converted
to 22050Hz, 16-bit, mono audio ﬁles. 12 amateur musicians,
who are familiar with various music taxonomy and concepts,
were hired to create ground truth about this collection. The
ground truth information was generated by attaching a tag
to a music item if at least three people assigned the tag to
the song. In the case that an agreement on tag assignment
between diﬀerent respondents can not be reached, a similar resolution used in generating CAL500 is applied. At the
end of the process, we obtain totally 250 tags, belonging to
8 diﬀerent categories. They are instrumentation, emotions,
country, time, genre, vocal characteristics, solo and usage
terms. Consequently, our evaluation with TS2, involves 8
high level concepts. The size of the vocabulary |V | is 174.

(10)

The optimization problem above can be easily solved with
a gradient descent process. After the training process, the
GMMs for each high level music concept estimates the likelihood score λc and this score is used to quantify the distance
between raw feature input and concept label. At the same
time, the output of DCML is a vector λ that models probabilistic relationship between diﬀerent music concepts and
audio input, where λ = [λ1 , λ2 , ...., λC ]. It serves as input
for tag reﬁnement layer, the last layer in the MMTagger
framework.

3.5 Tag Refinement Layer
The SVM based computational nodes constitute the Tag
Reﬁnement Layer (TRL) of the MMTagger framework. Each
of them is designed to estimate the probability of a particular tag based on input from DCML. In the current implementation of MMTagger system, SVM is selected as a tag
classiﬁer due to its eﬀectiveness [22].
Since traditional SVM can be used only for binary classiﬁcation, the method proposed by Hastie and Tibshirani [17]
is employed to derive numeric value for the probability that
an unknown sample belongs to a certain tag. Its key idea
is to adopt Gaussian distribution to model tag-conditional
densities pt (λ|y = 1) and pt (λ|y = −1). Using Bayes’ rule,
the relevance score (posterior probability) rt for each given
tag t can be computed via:
pt (λ|y = 1)Pt (y = 1)
i=−1,1 pt (λ|y = i)Pt (y = i)

rt = Pt (y = 1|λ) = P

4.2 Evaluation Metrics and Methodology
Textual information generated by music tagging systems
can be used for many diﬀerent MIR applications. To validate
the performance of diﬀerent tagging schemes, we select two
MIR tasks:
• Task 1 - music annotation: for a given song track issued
by the user, determine a proper set of tags. In this
study, the size of tag set is 10.
• Task 2 - music search: for a given tag selected from
the vocabulary, search for relevant song tracks in the
test collection.

(11)

Here, three diﬀerent evaluation metrics are used for music
annotation. They are mean per-tag precision and recall, and
the F-score. Based on the methodology used by Turnbull et
al. [26], the top 10 tags generated by the models are used
for comparison and thus annotation length A is 10. Per-tag
recall and per-tag precision is formally deﬁned as

where λ = [λ1 , λ2 , ...., λC ] is a vector generated by DCML. It
contains a set of likelihood values describing the probability
that an input music belongs to music concept c, where c =
1, ..., C. Using equation 11, a set of relevance scores r =
[r1 , r2 , ..., rT ] are obtained for ranking tags. Eventually the
top k tags are selected as annotation for input music, with
value of k being predeﬁned by the user.

4.

P recision =

EXPERIMENTAL CONFIGURATION

|tT P |
|tGT |

Recall =

|tT P |
|tA |

(12)

where |tGT | is the number of songs annotated with the tags
in the human-generated ”ground truth” annotation and tT P
is the number of songs annotated correctly with the tags.
Based on per-tag precision and per-tag recall, the F-score is
deﬁned as

This section introduces the experiment conﬁguration for
our performance evaluation. We report details on two test
datasets, performance metrics, competitors and evaluation
methodology. All tagging methods evaluated in this study
have been fully implemented and tested on a Pentium (R)
D, 3.20GHz, 1.98 GB RAM PC running the Windows XP
operating system.

F − score

4.1 Test Collections

= 2×

P recision × Recall
P recision + Recall

(13)

To measure the performance of diﬀerent approaches for
music search, the mean average precision (MeanAP) and
the area under the receiver operating characteristic curve
(AROC) are adopted as assessment metrics. Given a query
tag, the focus of MeanAP is on ﬁnding the most relevant
songs, while AROC emphasizes whether relevant songs are
ranked higher than irrelevant ones. We also apply α-fold
cross validation to ensure the stability and robustness of the
empirical results. α is predeﬁned to be 5.
In this study, we compare the performance of our system
against two state-of-the-art approaches including Autotagger [8, 4] and MSML [26, 25]. Acoustic feature considered

Test collections play a very important role in empirical
study. Two test collections are used in this evaluation.
The ﬁrst one (TS1) is the Computer Audition Lab 500-Song
(CAL 500) data set developed by the CAL group [25, 26].
This collection contains 500 modern western music documents performed by 500 diﬀerent artists. Altogether, there
are 174 tags categorized into six diﬀerent semantic groups
including instrumentation, vocal characteristics, genre, emotions, solo and usage terms. For this dataset, we use those
six groups as high level musical concepts to train our statistical model.

639

Model
Precision
by MSML is Mel-frequency cepstral coeﬃcient (MFCC). AuMSML
0.121
totagger is evaluated based on three feature sets including
Autotagger(MFCC delta)
0.257
MFCC delta, afeats and bfeats 2 For MMTagger, we consider
Autotagger(afeats)
0.239
ﬁve low level feature conﬁgurations (timber features denoted
Autotagger(bfeats)
0.268
by TF, rhythm features denoted by RF, spectral features deMMTagger(ALL)
0.327
noted by SF, melody features denoted by MF and timber feaMMTagger(TF)
0.231
tures+rhythm features+spectral features+melody features
MMTagger(SF)
0.220
denoted by ALL.). Autotagger(MFCC delta), Autotagger(afeats)
MMTagger(MF)
0.207
and Autotagger(bfeats) denote Autotagger with MFCC delta,
MMTagger(RF)
0.262
afeats and bfeats respectively. MMTagger(TF), MMTagger(SF), MMTagger(MF), MMTagger(RF), MMTagger(ALL)
denote our proposed model with timbral features, spectral
Table 3: Tagging accuracy on test
features, rhythmic features, melody features and the combination of all four musical features.

Recall
0.043
0.102
0.073
0.139
0.241
0.117
0.116
0.103
0.125

F-Score
0.072
0.101
0.117
0.186
0.284
0.144
0.139
0.125
0.156

collection TS2.

5.2 Result Analysis for Music Retrieval

5.

EXPERIMENT RESULTS

With the wide availability of large music collections, accurate music search is mandatory to achieve usability. This
section presents empirical results to compare the accuracy of
music retrieval facilitated by our proposed scheme and the
two competitors. Experimental methodology is that given
a keyword query kwq in vocabulary V , a test set of songs
are ranked. The metrics MeanAP and MeanAROC of each
ranking are calculated for performance comparison. Tables 4
and 5 summarize the experiment results with CAL500(TS1)
and TS2. Clearly, the proposed MMTagger(ALL) signiﬁcantly outperforms the other approaches.
In particular, the results shows that relative to Autotagger, MMTagger enjoys at least 10% MeanAP increase on
both test collections. Although a nice improvement over
Autotagger can be observed, we ﬁnd that there is more signiﬁcant gain over MSML. Averagely around 21% lift in term
of accuracy can be found for the two datasets. In addition,
we can summarize that for our proposed system, a proper
integration of multiple music features can bring substantial
improvement for search and annotation eﬀectiveness. This
observation corroborates other researchers’ ﬁnding that accurate MIR can not be achieved with a single type of music
feature and development of eﬀective acoustic feature combination scheme plays important role for tagging system’s
performance enhancement.

This section presents an experiment study to evaluate the
competing techniques on the task of music annotation, retrieval as well as music annotation in noisy environment.

5.1 Result Analysis for Music Annotation
We report a comparative study of the various tagging systems on music annotation processing. Table 2 and 3 summarize the empirical results for three systems with various
conﬁgurations on the two test collections. The size of tag set
is set to 10. Here, MMTagger is tested with ﬁve diﬀerent feature settings is tested. The bottom four rows of both tables
present the accuracy of our proposed system with just one
acoustic feature. In comparison to MMTagger(ALL), they
suﬀer from lower accuracy. In fact, the multiple feature
combination achieves signiﬁcant eﬀectiveness gain ranging
from 5% to 15%. The empirical results points clearly to the
importance of combining features intelligently to tagging effectiveness. The experimental results also demonstrate that
the MMTagger signiﬁcantly outperforms the existing approaches. For example, in Table 2, comparing to Autotagger(bfeats), MMTagger(ALL) improves the precision from
0.291 to 0.351 for the CAL500 dataset, and from 0.268 to
0.327 for the TS2 collection. Similar observations can be
made on the other two evaluation metrics over diﬀerent test
collections. We thus conclude that MMTagger emerges as
the most eﬀective music tagging scheme.
Model
MSML
Autotagger(MFCC delta)
Autotagger(afeats)
Autotagger(bfeats)
MMTagger(ALL)
MMTagger(TF)
MMTagger(SF)
MMTagger(MF)
MMTagger(RF)

Precision
0.144
0.281
0.266
0.291
0.351
0.256
0.241
0.226
0.289

Recall
0.064
0.131
0.094
0.153
0.291
0.141
0.137
0.131
0.150

Model
MSML
Autotagger(MFCC delta)
Autotagger(afeats)
Autotagger(bfeats)
MMTagger(ALL)
MMTagger(TF)
MMTagger(SF)
MMTagger(MF)
MMTagger(RF)

F-Score
0.089
0.179
0.139
0.205
0.314
0.176
0.165
0.149
0.171

MeanAP
0.231
0.305
0.323
0.340
0.410
0.282
0.275
0.286
0.288

MeanAROC
0.503
0.678
0.622
0.662
0.782
0.496
0.489
0.501
0.508

Table 4: Music retrieval accuracy on test collection
CAL500(TS1).

Table 2:
Tagging accuracy on test collection
CAL500(TS1).

2
Detail information about those feature sets can be found
in [26].

640

Model
MSML
Autotagger(MFCC delta)
Autotagger(afeats)
Autotagger(bfeats)
MMTagger(ALL)
MMTagger(TF)
MMTagger(SF)
MMTagger(MF)
MMTagger(RF)

MAP
0.204
0.267
0.289
0.301
0.385
0.251
0.257
0.249
0.242

MeanAROC
0.461
0.613
0.597
0.626
0.753
0.449
0.452
0.467
0.472

Model
MSML
Autotagger(MFCC delta)
Autotagger(afeats)
Autotagger(bfeats)
MMTagger(ALL)
MMTagger(TF)
MMTagger(SF)
MMTagger(MF)
MMTagger(RF)

Precision
0.121
0.241
0.226
0.252
0.325
0.206
0.201
0.216
0.253

Recall
0.053
0.112
0.084
0.130
0.271
0.121
0.107
0.101
0.117

F-Score
0.075
0.154
0.113
0.167
0.291
0.133
0.127
0.119
0.143

Table 6: Tagging accuracy in noisy environment on
test collection CAL500(TS1). Noise type - 50% volume amplification.

Table 5: Music retrieval accuracy on test collection
TS2.

5.3 Result Analysis for Noise Robustness
Model
MSML
Autotagger(MFCC delta)
Autotagger(afeats)
Autotagger(bfeats)
MMTagger(ALL)
MMTagger(TF)
MMTagger(SF)
MMTagger(MF)
MMTagger(RF)

Modern MIR systems often need to work robustly in presence of ambient noise (e.g. raw music signal recorded from
live concerts or other outdoor environments). However, existing schemes might not perform eﬀectively when handling
noisy audio input. Thus, it is important to evaluate the robustness of diﬀerent music annotation schemes against music
sources containing diﬀerent kinds of audio distortion. In this
work, we study how diﬀerent types of noise in the query music aﬀect annotation accuracy. We use the TS1 (CAL500)
as evaluation data and apply the same set of test music as
those used in the music annotation experiment. Before the
test, various kinds of audio distortion are injected into each
query music item. The distortion cases include 50% volume ampliﬁcation, 50% volume deampliﬁcation, 10 second
cropping, 35dB SNR mean background noise and 35db SNR
white background noise3 .
Tables 6-10 illustrate the noise robustness performance of
MMTagger and its competitors for the diﬀerent distortion
cases. In general, the results show that when the music input
is polluted by a certain kind of noise, annotation accuracy of
all the systems suﬀers. Comparing to the other approaches,
MMTagger demonstrates high resilience and stable performance. Speciﬁcally, MMTagger using single kind of acoustic
feature suﬀers greater performance degradation than MMTagger with all four acoustic features. Moreover, crosschecking the results from this section and Section 5.1 reveals that
MMTagger demonstrates more stable performance and enjoys less accuracy degradation than Autotagger and MSML
under noisy circumstances. For example, MMTagger’s precision decreases about 7% when annotating inputs with 50%
volume ampliﬁcation. Whereas, Autotagger and MSML suffer about 15% and 17% drop on average. We thus conclude
that MMTagger is robust to diﬀerent kinds of noise.

6.

Precision
0.132
0.239
0.231
0.248
0.321
0.212
0.209
0.214
0.249

Recall
0.059
0.117
0.088
0.125
0.281
0.128
0.111
0.108
0.118

F-Score
0.071
0.167
0.119
0.171
0.295
0.131
0.119
0.121
0.145

Table 7: Tagging accuracy in noisy environment on
test collection CAL500(TS1). Noise type - 50% volume deamplification.
Model
MSML
Autotagger(MFCC delta)
Autotagger(afeats)
Autotagger(bfeats)
MMTagger(ALL)
MMTagger(TF)
MMTagger(SF)
MMTagger(MF)
MMTagger(RF)

Precision
0.127
0.237
0.230
0.251
0.330
0.211
0.211
0.209
0.249

Recall
0.051
0.120
0.085
0.131
0.281
0.128
0.116
0.111
0.113

F-Score
0.080
0.150
0.121
0.162
0.211
0.135
0.120
0.121
0.145

Table 8: Tagging accuracy in noisy environment on
test collection CAL500(TS1). Noise type - 10 second
cropping.
Model
MSML
Autotagger(MFCC delta)
Autotagger(afeats)
Autotagger(bfeats)
MMTagger(ALL)
MMTagger(TF)
MMTagger(SF)
MMTagger(MF)
MMTagger(RF)

CONCLUSION

As a key enabling technology for music information retrieval, tagging has received a lot of research attentions in
recent years. However, the performance of existing systems
is far from satisfactory. In this paper, we describe a new music annotation scheme based on advanced feature extraction
and multilayer structure. We have applied our method to
two large test collections. Theoretical analysis and empirical results indicate that our approach achieves substantially

Precision
0.117
0.240
0.221
0.259
0.332
0.201
0.208
0.212
0.252

Recall
0.064
0.110
0.081
0.133
0.271
0.124
0.106
0.105
0.112

F-Score
0.081
0.160
0.109
0.171
0.291
0.130
0.129
0.111
0.146

Table 9: Tagging accuracy in noisy environment on
test collection CAL500(TS1). Noise type - 35dB
SNR mean background noise

3

S
SN RdB = 10log10 N
is the equation used to calculate the
signal-to-noise ratio, where S denotes the signal power, and
N denotes the noise power in dB

641

Model
MSML
Autotagger(MFCC delta)
Autotagger(afeats)
Autotagger(bfeats)
MMTagger(ALL)
MMTagger(TF)
MMTagger(SF)
MMTagger(MF)
MMTagger(RF)

Precision
0.115
0.242
0.219
0.256
0.328
0.204
0.207
0.209
0.245

Recall
0.061
0.108
0.082
0.130
0.268
0.127
0.111
0.112
0.109

F-Score
0.083
0.156
0.107
0.176
0.287
0.127
0.125
0.113
0.140

[11] C. Lee, C. Lin, and B. Juang. A study on speaker
adaptation of the parameters of continuous density hidden
markov models. IEEE Transactions on Signal Processing,
39(4), 1991.
[12] T. Li, M. Ogihara, and Q. Li. A comparative study on
content-based music genre classiﬁcation. In Proc. of ACM
SIGIR Conference, 2003.
[13] B. Logan. Mel frequency cepstral coeﬃcients for music
modeling. In Proc. of the ISMIR, 2000.
[14] L. Lu, D. Liu, and H. Zhang. Automatic mood detection
and tracking of music audio signals. IEEE Trans. Acoust.,
Speech, Signal, 2006.
[15] G. McLachlan and D. Peel. Finite Mixture Models. John
Wiley & Sons, 2000.
[16] N. Orio. Music retrieval: A tutorial and review.
Foundations and Trends in Information Retrieval, 1(1),
2006.
[17] J. Platt. Probabilistic outputs for support vector machines
and comparisons to regularized likelihood methods.
Advances in Large Margin Classifiers, 2000.
[18] G.-J. Qi, X.-S. Hua, Y. Rui, J. Tang, Z.-J. Zha, and H.-J.
Zhang. A joint appearance-spatial distance for kernel-based
image categorization. In Proc. of CVPR, 2008.
[19] J. Shen, B. Cui, J. Shepherd, and K.-L. Tan. Towards
eﬃcient automated singer identiﬁcation in large music
databases. In Proc. of ACM SIGIR Conference, pages
59–66, 2006.
[20] J. Shen, J. Shepherd, B. Cui, and K.-L. Tan. A novel
framework for eﬃcient automated singer identiﬁcation in
large music databases. ACM Trans. Inf. Syst., 27(3), 2009.
[21] J. Shen, J. Shepherd, and A. H. H. Ngu. Towards eﬀective
content-based music retrieval with multiple acoustic feature
combination. IEEE Transactions on Multimedia,
8(6):1179–1189, 2006.
[22] S. Shwartz and N. Srebro. SVM optimization: inverse
dependence on training set size. In Proc. of ICML, 2008.
[23] D. Turnbull, L. Barrington, and G. Lanckriet. Modeling
music and words using a multi-class naı̈ve bayes approach.
In Proc. of ISMIR, 2006.
[24] D. Turnbull, L. Barrington, G. R. G. Lanckriet, and
M. Yazdani. Combining audio content and social context
for semantic music discovery. In Proc. of ACM SIGIR
Conference, pages 387–394, 2009.
[25] D. Turnbull, L. Barrington, D. Torres, and G. Lanckriet.
Towards musical query-by-semantic-description using the
CAL500 data set. In Proc. of ACM SIGIR Conference,
2007.
[26] D. Turnbull, L. Barrington, D. Torres, and G. R. G.
Lanckriet. Semantic annotation and retrieval of music and
sound eﬀects. IEEE Transactions on Audio, Speech &
Language Processing, 16(2), 2008.
[27] G. Tzanetakis and P. Cook. Musical genre classiﬁcation of
audio signals. IEEE Trans. on Speech and Audio
Processing, 2002.
[28] B. Whitman. Learning the meaning of music. PhD thesis,
Massachusetts Institute of Technology, 2005.
[29] B. Whitman and R. M. Rifkin. Musical
query-by-description as a multiclass learning problem. In
Proc. of IEEE Workshop on Multimedia Signal Processing,
2002.
[30] B. Zhang, J. Shen, Q. Xiang, and Y. Wang. Compositemap:
a novel framework for music similarity measure. In Proc. of
ACM SIGIR, pages 403–410, 2009.

Table 10: Tagging accuracy in noisy environment
on test collection CAL500(TS1). Noise type - 35dB
SNR white background noise
higher accuracy in tagging music documents comparing to
existing techniques. Moreover, our method demonstrates superior robustness against diﬀerent kinds of audio distortion.
This work can be extended in several directions: At this
stage, we have only tested our method on audio data. It
would be very interesting to apply the method to data from
other application domains (e.g. image and video retrieval)
and investigate its performance characteristics. In addition,
we plan to integrate more acoustic features into our framework. A natural question arises as to what kinds of feature
combination is best in terms of eﬀectiveness and robustness enhancement. Finally, designing a robust and eﬀective
evaluation methodology is also very important for further
investigation and fair performance comparison.

7.

ACKNOWLEDGEMENTS

Shuichang Yan is partially supported by AcRF Tier-1
Grant of R-263-000-464-112, Singapore.

8.

REFERENCES

[1] E. Alpaydin. Introduction to Machine Learning (Adaptive
Computation and Machine Learning). The MIT Press,
2004.
[2] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information
Retrieval. Addison Wesley, 1999.
[3] M. Bartsch and G. Wakeﬁeld. To catch a chorus: Using
chroma-based representations for audio thumbnailing. In
Proc. of IEEE Workshop on Applications of Signal
Processing to Audio and Acoustics, 2001.
[4] T. Bertin-Mahieux, D. Eck, F. Maillet, and P. Lamere.
Autotagger: A model for predicting social tags from
acoustic features on large music databases. Journal of New
Music Research, 37(2), 2008.
[5] C. Dorai and S. Venkatesh. Bridging the semantic gap with
computational media aesthetics. IEEE Multimedia, 10(2),
2003.
[6] Z. Duan, L. Lu, and C. Zhang. Collective annotation of
music from multiple semantic categories. In Proc. of
ISMIR, 2008.
[7] R. Duda, P. Hart, and D. Stork. Pattern Classification.
John Wiley and Sons, 2001.
[8] D. Eck, P. Lamere, T. Bertin-Mahieux, and S. Green.
Automatic generation of social tags for music
recommendation. In Proc. of NIPS, 2007.
[9] H. Hermansky and N. Morgan. Rasta processing of speech.
IEEE Transaction on Speech and Audio Processing,
2:578–589, 1994.
[10] N. Hu, R. Dannenberg, and G. Tzanetakis. Polyphonic
audio matching and alignment for music retrieval. In Proc.
of IEEE Workshop on Applications of Signal Processing to
Audio and Acoustics, pages 185–188, 2003.

642

