HCC: A Hierarchical Co-Clustering Algorithm
Jingxuan Li

∗

Tao Li

School of Computer Science
Florida International University
Miami, FL, 33199

{jli003,taoli}@cs.fiu.edu

ABSTRACT

ŽŽŬƐ
ϭ

In this poster, we develop a novel method, called HCC, for
hierarchical co-clustering. HCC brings together two interrelated but distinct themes from clustering: hierarchical clustering and co-clustering. The goal of the former theme is
to organize clusters into a hierarchy that facilitates browsing and navigation, while the goal of the latter theme is
to cluster diﬀerent types of data simultaneously by making
use of the relationship information. Our initial empirical
results are promising and they demonstrate that simultaneously attempting both these goals in a single model leads to
improvements over models that focus on a single goal.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering
General Terms: Algorithms, Experimentation
Keywords: Hierarchical, Co-Clustering

1.

dƌĂĚŝƚŝŽŶĂů
,ŝĞƌĂƌĐŚǇ

&ŝƚŶĞƐƐ ĨŽƌůŝĨĞ

Ϯ

&ŝƚŶĞƐƐŝƐƌĞŐŝŽŶ

ϯ

ŐƵŝĚĞƚŽĂ
ŚĞĂůƚŚǇůŝĨĞƐƚǇůĞ

ϰ

ƌƵŝƐĞdƌĂǀĞů

ϱ

ƚƌĂǀĞůŐƵŝĚĞƚŽ
:ĞǁŝƐŚƵƌŽƉĞ

ϲ

/ŶƚĞƌŶĂƚŝŽŶĂů
dƌĂǀĞůĂŶĚ,ĞĂůƚŚ

ϭ

Ϯ

ϯ

ϰ

ϭϮ
ϯϰ
ϱϲ

ϭϮ
ϯϰ
ϱ

ϰ
ϲ

ϯ
ϰ
ϱ
ϰ
ϱ

<ĞǇtŽƌĚƐ
<ϭ

&ŝƚŶĞƐƐ

<Ϯ

,ĞĂůƚŚ

<ϯ

'ƵŝĚĞ

<ϰ

dƌĂǀĞů

<ϱ

/ŶƚĞƌŶĂƚŝŽŶĂů

ϲ

,
ϰ
ϱ
ϲ

ϭ
Ϯ

ϭ
Ϯ
ϯ

ϱ

ϰ
ϱ

ϲ
ϯ
ϭ
Ϯ

Ϯ

ϭ
ϱ
ϭ

ϭ

Ϯ

ϯ

ϯ

ϭ

ϲ
ϰ

ϱ

ϲ

<ϭ

<Ϯ

<ϯ

ϱ
<ϰ

<ϱ

Figure 1: An Illustrative Example. Blue rectangles denote books and red rectangles denote keywords. The nodes containing both blue and red
rectangles denote the clusters having some books
and keywords.

INTRODUCTION

clustering algorithms, HCC applies on the union of diﬀerent types of data instead of on the points of a single data
type. At each step of the merging process, HCC can merge
a subset of one type of data objects with a subset of another
type of data objects based on a measure of cluster internal
heterogeneity. Figure 1 shows the hierarchy generated by
traditional single-link hierarchical clustering algorithm and
the hierarchy generated by HCC on an example dataset of
book titles with two data types (books and keywords). It
can be easily observed that HCC builds two coupled hierarchies (book hierarchy and keyword hierarchy) simultaneously. Books in a cluster can be well explained using the
keywords in the same cluster. In addition, the relationships
between book clusters can be described using their shared
keywords.
Our initial empirical results are promising and they demonstrate that simultaneously attempting both these goals in a
single model leads to improvements over models that focus
on a single goal. In the rest of the poster, we introduce our
HCC algorithm and also present some experimental results.

Hierarchical clustering generates tree-like structures (e.g.,
dendrograms and hierarchies) which can be utilized to facilitate data navigation and browsing and has many applications in Information Retrieval(IR) [1]. Most existing hierarchical clustering algorithms aim at clustering homogeneous
single type of data.
In many real world applications, however, a typical task
often involves more than one type of data points. For example, in document analysis, there are terms and documents.
Recently, many co-clustering algorithms have been proposed
to simultaneously clustering diﬀerent types of data [2]. However, few algorithms have been developed on simultaneously
building hierarchies for diﬀerent types of data.
In this poster, we develop a novel method, called HCC, for
hierarchical co-clustering, which aims at generating dendrograms for diﬀerent types of data simultaneously by making
use of their relationship information. In this regard, HCC
brings together two interrelated but distinct themes from
clustering: hierarchical clustering and co-clustering. HCC
utilizes the agglomerative hierarchical clustering algorithms
as the frame, starting with singleton clusters, successively
merges the two nearest clusters until only one cluster remains. Diﬀerent from traditional agglomerative hierarchical

2. HCC METHOD
In the typical setting of a dyadic co-clustering problem,
there are two types of data objects, A={a1 , a2 , a3 . . . am }
and B={b1 , b2 , b3 . . . bn } with size m and n, and we are given
a relationship matrix X = (xij ) ∈ Rm×n , such that xij
represents the relationship between i-th point in A and the
j-th point in B. The goal of HCC is to generate hierarchical
clustering solutions for A and B simultaneously by making

∗

The work is partially supported by NSF grants IIS-0546280
and CCF-0939179.
Copyright is held by the author/owner(s).
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
ACM 978-1-60558-896-4/10/07.

861

use of X. In the example of Figure 1, A is the set of book
titles, B is the set of keywords, and X is the (normalized)
book-term matrix indicating the relationship between the
book titles and keywords.
HCC has its base in the approach developed in [4]. Similar to agglomerative hierarchical clustering algorithms, HCC
starts with singleton clusters and then successively merges
the two nearest clusters until only one cluster remains. Different from traditional agglomerative hierarchical clustering
algorithms, HCC applies on the union of diﬀerent types of
data instead of on the points of a single data type. The output of HCC is a tree: each leaf of the tree is a data point of
A or B and each internal node is a cluster of data points in
A and B. The root of this tree is the largest cluster which
contains all data points of A and B. The general procedure
of HCC is described in Algorithm 1.

ƵƚŚŽƌƐ
Ϯ

ůŝƐĂĞƌƚŝŶŽ

ϯ

:ŝĂǁĞŝ ,ĂŶ

dϭ

ĂƚĂ

dϮ

'ƌĂƉŚ

dϯ

DĂƚĐŚŝŶŐ

dϰ

DŝŶŝŶŐ

dϱ

KďũĞĐƚ

dϲ

ϭ
ϰ

ϯ

Ϯ

ϱ
ϭ

^ŚĂƉĞ

Ϯ
ϲ

ϭ
ϭ
ϯ

ϭ

Ϯ

ϯ

Ϯ
ϯ
ϲ

ϲ

ϰ

dϭ

dϮ

dϯ

dϰ

dϱ

dϲ

Figure 2: The dendrogram generated by HCC on
the DBLP dataset
Clustering NMI CPCC
HCC
0.19 0.67
SLHC
0.06 0.61
K-means
0.10 N/A
TNMF
0.17 N/A
Table 1: Clustering performance comparison
Note that authors’ research interests can be clearly described
by the associated terms in a hierarchical manner. The more
representative are the words for certain authors, the larger
possibility for them to be clustered together. For example,
at the sixth layer, ‘data’ and ‘mining’ have been merged
with ‘Jiawei Han’ who is renowned computer scientist in
data mining research area.
We also evaluate the clustering performance of our HCC
method. Normalized mutual information(NMI) and CoPhenetic Correlation Coeﬃcient(CPCC) are used as evaluation
measures. NMI is the normalized version of mutual information which measures how much information the two clusterings shares. In general, larger NMI indicates better clustering results. CPCC measures how faithfully a dendrogram
preserves the pairwise distances between the original data
points. Since DBLP dataset contains 9 classes, we make a
cut on certain layer of the generated dendrogram, such that
we can generate 9 clusters from the tree to compare with the
original classes. We compare HCC with K-means, Tri-Factor
Nonnegative Matrix Factorization(TNMF) [3], and Single
Linkage Hierarchical Clustering(SLHC). Note that TNMF is
an eﬀective matrix-based framework for co-clustering. The
experimental results presented in Table 1 clearly show that
HCC outperforms other rivals. In summary, HCC combines
the strengths of hierarchical clustering and co-clustering, simultaneously builds hierarchies for diﬀerent types of data
utilizing their relationships, and thus leads to better clustering performance.

for i = 0 to N − 1 do
p, q = PickUpTwoNodes(List)
o = Merge(p, q)
Remove p, q from List
Add o to List
Add List to H at a higher layer
end for
The core issue in Algorithm 1 is the process of PickUpTwoNodes: picking up two nodes (corresponding to two clusters) from the current layer to merge. Diﬀerent from traditional hierarchical clustering where a node only contains
objects of a single data type, a node here can have objects
from diﬀerent data types. Given a cluster C with m1 objects
from A (denoted as A1 ) and n1 objects from B (denoted as
B1 ), the cluster heterogeneity CH(C) can be deﬁned as

1
(xij − µ)2
(1)
CH(C) =
m1 n1 s∈A ,t∈B
1

where µ = Avgs,t xst is the average value of the corresponding entries in X. Once the cluster heterogeneity is deﬁned,
we can select two nodes which would result in the least increase in cluster heterogeneity to merge [4]. Therefore, at
each step of the merging process, HCC can merge two subsets of diﬀerent types of data objects.

3.

ĚǁŝŶZ͘,ĂŶĐŽĐŬ

dĞƌŵƐ

Algorithm 1 HCC Algorithm Description
Create an empty hierarchy H
List ← Objects in A + Objects in B
N ← size[A] + size[B]
Add List to H as the bottom layer

1

ϭ

4. REFERENCES
[1] D.R. Cutting, D.R. Karger, J.O. Pedersen, and J.W. Tukey.
Scatter/gather: a cluster-based approach to browsing large
document collections. In SIGIR ’92, 1992.
[2] M. Deodhar and J. Ghosh. A framework for simultaneous
co-clustering and learning from complex data. In KDD ’07,
2007.
[3] C. Ding, T. Li, W. Peng, and H. Park. Orthogonal nonnegative
matrix tri-factorizations for clustering. In KDD ’06, 2006.
[4] T. Eckes and P. Orlik. An error variance approach to
two-mode hierarchical clustering. Journal of Classification,
10(1):51–74, January 1993.
[5] T. Li, C. Ding, Y. Zhang, and B. Shao. Knowledge
transformation from word space to document space. In SIGIR
’08, 2008.

EXPERIMENTAL RESULTS

We perform experiments on DBLP dataset [5] to evaluate
our HCC method. DBLP dataset contains the paper titles
published by 552 relatively productive researchers over the
last 20 years (from 1988 to 2007, inclusive) from 9 categories.
The input to our HCC method is a normalized authorterm matrix where each entry indicating the relationship
between the corresponding author’s paper titles and the corresponding keyword. As we discussed before, HCC is able to
generate a coupled dendrogram for both authors and terms.
Figure 2 shows the dendrogram generated by HCC. In the
dendrogram, each leaf represents one author or one term
and each internal node contain subsets of authors and terms.

862

