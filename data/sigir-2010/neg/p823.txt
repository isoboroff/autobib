A Stack Decoder Approach to Approximate
String Matching
Juan M. Huerta
IBM T. J. Watson Research Center
1101 Kitchawan Road,
Yorktown Heights, NY, 10598
(914) 945 3378
huerta@us.ibm.com
we do not partition the query pattern and we search terms in decreasing rarity order.

ABSTRACT
We present a new efficient algorithm for top-N match retrieval of
sequential patterns. Our approach is based on an incremental approximation of the string edit distance using index information
and a stack based search. Our approach produces hypotheses with
average edit error of about 0.29 edits from the optimal SED result
while using only about 5% of the CPU computation.

2.INCREMENTAL APPROXIMATION
The SED between sentences A and B consists of the sum of the

cost of the edit operations given the optimal alignment γ (where
the costs of the kth insertion, deletion or substitution operations are

ψk

ξ k , respectively):

Categories and Subject Descriptors

denoted by φ k ,

H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval – retrieval models, search process.

(
φk + ∑ ψ k + ∑ ξ k )
γ ∑
k∈γ
k ∈γ
k ∈γ
Using dynamic programming (DP) one can simultaneously obtain
the optimal alignment and the associated minimum total distance
between A and B. The computational cost of naïve DP is O(nm),
where n is the length of string A and m is the length of string B
(cfr. [3]). When the comparison is performed between string A
SED( A , B ) =

General Terms
Algorithms, Performance, Experimentation

Keywords

and

min

and each sentence in the database Ψ = {B1 , B 2 ...B k } the cost of
exhaustively computing SED using naïve DP is O( Ψ nm) . In this

A* search, Stack decoder, String Edit Distance, String Matching

1.INTRODUCTION

case m is the average sentence length in Ψ , and | Ψ | is the set
cardinality. In translation memory domains | Ψ | is typically large

We address the problem of efficiently identifying the top scoring
set of sentences from a large database of sentences using an incremental approximation of the string edit distance (SED) given a
query sentence. Our motivation is the translation memory domain
(e.g. [6]) where the goal is to identify the closest matching sentence from a large database of translation pairs given a query
sentence in a source language in order to reduce the load of the
translation engine, if the match is close enough. Sentences in this
domain are typically short (consisting of less than 20 words).

(millions of sentences) while n and m are relatively small.
In order to approximate the SED using index-derived counts we
manipulate the SED from a distance into a similitude score. For
this we define the non-negative complemented SED (denoted as
SED*) between A and the jth sentence B j in Ψ :
SED * ( A, B j ) = 2n − SED( A, B j )
When A=Bj then the SED* = 2n . This is two times the total number of observed evidence counts that are obtainable using the
index (if we increment the score by one per observation). We can
express the SED* in terms of two components: one provided by
the counts directly identified using the index (which are equal to n
minus deletions). We call this the observable evidence. The other
term arises from the total number of spurious terms (n minus insertions). This is the unobservable evidence (the index returns no
counts for these word-sentence combinations). Then:

The key idea in our approach is to approximate the SED which
consists of the sum of edits costs (insertions, deletions and substitutions) under an optimal alignment using instead position adjusted similitude counts coming from an index. We also propose a
way to carry out the search using a stack but without computing
string alignments explicitly. This is desirable because, in general,
index-based approaches are computationally advantageous when
the search is carried out over large databases.
Our search strategy is similar to Jelinek [1] and Paul [4] with
these differences: (1) we rely on an inverted index with position
information rather than HMM state observations. (2) We approximate the SED instead of the observation likelihoods. (3) Our
approach approximates string alignment scores. (4) The evidence
is considered in order of decreasing rarity. Compared to Navarro
and Baeza-Yates [2] our approach is based on a simpler structure,

SED * ( A, Bj ) = (n − ∑ ψ k ) + ( n − ∑ ξk − ∑ φ k )
k ∈γi

k∈γi

k ∈γi

When estimating SED* for a collection of strings and a query we
keep track and update the SED* for each sentence in Ψ as the
sum of the observable and unobservable evidence as each of the
terms a i in the query {a1 , a 2 ...a n } is incrementally considered.
We denote the observable evidence (n minus deletions) up to term
ai as g(i). While h(i) is the estimate up to term ai of unobservable

Copyright is held by the author/owner(s).
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
ACM 978-1-60558-896-4/10/07.

823

terms (amortized deletions). The SED*i up to the ith term is:

are introduced in increasing frequency order (decreasing rarity).
(2) We consider only the top n terms in terms of rarity. (3) The
lowest scoring hypotheses are pruned in each iteration.

SEDi * ( A, B j ) = g (i ) + h(i )
We want g(n) to approximate the actual insertion related score:
g ( n) ≈ n − ∑ψ k

4.EXPERIMENTS

k ∈γi

We performed experiments based on a translation memory database. The task consisted of identifying the closest match from a
database consisting of 1,000,000 sentences given a query. The test
set consisted of 5,745 query sentences. The baseline computation
of the SED using DP (without backtrack step) for the test set consumed a median of 0.56 (x10^6) CPU cycles/sentence. Table 1
shows the average edit error distance between the closest sentence
and the top 1 stack hypothesis (and in parenthesis, the median
CPU cycles per sentence) for several stack and rarity cutoff configurations. With a stack with maximum depth of 400 and considering only the top 4 terms per query, we speed up the computation approximately twenty-fold while still obtaining results that
have edit scores that are 0.29 edit points higher per sentence on
average from the top match.

As the evidence is incrementally introduced we update g(i):

g (i ) = g (i − 1) + Δ gi
The incremental contribution of term a i to g(i) is computed
based on the linear distances between the position of that term and
the previously considered term

a i −1 in the query and hypothesis:

Δ gi = 1 − dist A (a i , a i −1 ) − dist Bj (a i , a i −1 )
The incremental contribution of a term should only be introduced
if its location in the sentence is consistent with the hypothesis in
question and the previously observed evidence. For example, if
A=”fast run” and B=”run fast”; terms a1=“fast” and a2=“run”
should not both contribute towards the observable score of B.
In the case of h(i) the increment depends on the amortized expected deletions is:
m − Bj ∩ A
Δ hi = 1 −
Bj ∩ A

Table 1. Avg. edit error and median CPU /sentence (in parenthesis) for rarity cutoff vs. stack depth

Where B j ∩ A denotes the cardinality of the set of terms occurring both in the hypothesis and in the query, and m is B j .

3.A* STACK DECODER ALGORITHM
To understand the process of searching for the top match from a
large corpus using SED* estimates, we depart from a conceptual
formulation in which alignments between the words in the query
A and words in each of the sentences Bj are represented as unique
paths in a tree with edges corresponding to alignment associations
between ai and bj. Each root-to-leaf path represents an implicit
alignment between A and Bj. Partial paths with common prefixes
represent hypotheses with common observable evidence sets. Our
goal is to find the root-to-leaf path that maximizes the total SED*
score as evidence is incrementally introduced. This conceptualization is useful to motivate the use of A* search [5] to conduct bestfirst tree search using our incremental SED* approximation as
described below:

250

3
0.36(0.01)

4
0.30(0.03)

5
0.34(0.07)

6
0.43(0.18)

400

0.36(0.01)

0.29(0.03)

0.34(0.08)

0.43(0.19)

600

0.36(0.01)

0.30(0.04)

0.36(0.09)

0.47(0.19)

900

0.36(0.01)

0.32(0.04)

0.38(0.10)

0.51(0.19)

5.CONCLUSION
We introduced a new approach for efficient approximate string
match using an inverted index using order information. Through a
stack search and heuristics our approach reduces the search computation while producing near-optimal hypotheses. It can be
shown that the average complexity of our approach (as implemented) is approximately O (log S nl ) , where l is the average
number of sentence occurrences for the rarest terms in the query
and |S| is the size of the stack.

6.REFERENCES

Algorithm: StackSED*
Input: query string A= {a1 , a 2 ...a n } and inverted index Y
Output: ranked list of hypotheses R
for each word ai in {a1 , a 2 ...a n }
obtain the set of sentences X recalled by ai from
for each sentence Bj in set X
if Bj exists in S

[1] F. Jelinek (1969), Fast sequential decoding algorithm using a
stack, IBM Journal of Res.and Devel., vol. 13, Nov. 1969.
[2] G. Navarro and R. Baeza-Yates (2000). A hybrid indexing
method for approximate string matching. Journal of Discrete
Algorithms, 1(1):205–239, 2000.
[3] G. Navarro (2001), A guided tour to approximate string
matching, ACM Computing Surveys v.33 No. 1 2001.

compute Δ hi , Δ gi and update hypoth. score and positions
else
append Bj sentence id to S with score=1 and initial positions,
eliminate hypotheses with score smaller than P% of top score
sort stack S by score
return R list of top-n sentence id’s.

[4] D. Paul, (1992), An efficient A* stack decoder algorithm for
continuous speech recognition with a stochastic language
model. Proc. of the Workshop on Speech and N. L. 1992.
[5] S. Russel and P. Norvig, (1995) Artificial Intelligence: A
Modern Approach. Prentice Hall

Figure 1. StackSED* Algorithm

[6] E. Sumita, (2001). Example-based machine translation using
DP-matching between word sequences. Proc. of the Workshop on Data-Driven Methods in M.T. Vol. 14

During search we do not explicitly build the whole search tree,
but rather dynamically extend hypotheses representing top partial
paths in a hypothesis stack (similar to [1, 2]) and prune low performing ones. Our approach uses these heuristics: (1) Terms in A

824

