MEMOSE – Search Engine for Emotions
in Multimedia Documents
Kathrin Knautz

Tobias Siebenlist

Wolfgang G. Stock

Heinrich-Heine-University

Heinrich-Heine-University

Heinrich-Heine-University

Düsseldorf
Universitätsstrasse 1
D-40225 Düsseldorf
+49 (0)211-81-12334

Düsseldorf
Universitätsstrasse 1
D-40225 Düsseldorf
+49 (0)211-81-12913

Düsseldorf
Universitätsstrasse 1
D-40225 Düsseldorf
+49 (0)211-81-12913

kathrin.knautz@uniduesseldorf.de

tsiebenlist@acm.org

stock@phil-fak.uniduesseldorf.de

The MEMOSE (Media Emotion Search) system is a specialized
search engine for fundamental emotions in all kinds of emotionalladen documents. We apply a controlled vocabulary for basic
emotions, a slide control to adjust the intensities of the emotions
and the approach of broad folksonomies. The paper describes the
indexing and the retrieval tool of MEMOSE and results from its
evaluation.

We found two forms of distribution: the power law and the
inverse logistic distribution [6]. In the first one users mostly
assign only one emotion with a high intensity to a resource. An
inverse logistic distribution is available if several emotions with a
high average value are assigned to the multimedia documents.
Knautz et al. [1] showed that a relative stability of the
distributions' shapes can be reached after some dozens of tagging
users.

Categories and Subject Descriptors

2. METHOD

ABSTRACT

Our next-generation retrieval system for emotions in multimedia
documents is designed as a web application and consists of the
following components: a tool for emotional tagging for adding
weighted emotional tags to the resources, some processing scripts
that interact with the APIs of Web 2.0 services and our API and
finally a retrieval tool needed to access the indexed data.

H.3.1 [Information storage and retrieval]: Content Analysis
and Indexing - indexing methods
H.3.3 [Information storage and retrieval]: Information Search
and Retrieval – Search process

General Terms

2.1 Dataset & Technology

Design, Human Factors

For our experimental evaluation we selected about 500 pictures
with animals on it from Flickr. The pictures were collected by a
group of students studying information science. We reduced the
scope of this broad field by limiting the available animals to pets
and farm animals in order to get several results for every type of
animal. There were no restrictions regarding the selection of
images, the students were encouraged to choose pictures by
random and not by means of felt emotions. The only demand was
that these pictures were taken from Flickr. After the links were
collected, they were checked for validity. Additional data (e.g,
information about the author, associated tags, license etc.) were
collected by using the Flickr API. All the tools that will be
described here were developed using PHP. The data were held in
a MySQL database with an InnoDB engine. In order to get fast
response times the emotional intensity was saved in a materialized
view. The user interface was designed using strict XHTML and
the jQuery library for dynamic elements like the slide controls.

Keywords
Emotion, Multimedia Resources, Collaborative Indexing, Slide
control tagging, Emotional Information Retrieval (EmIR)

1. INTRODUCTION
Some content in multimedia resources is able to display feelings
or to provoke certain emotions in the users. The aim of our
research is to identify these emotions and to make them
searchable so that they can be used for information retrieval.
Emotions are hidden in different document types. We can find
emotional-laden documents in textual objects like lyrics, poems or
novels, music [3], images [5], videos [1] and blogs. Lee and Neal
[3], Schmidt and Stock [5] and Knautz et al. [1] pointed out that
users are able to index fundamental emotions consistently by
means of tagging (in the context of a broad folksonomy). In our
studies we worked with a controlled vocabulary for basic
emotions (love, happiness, fun, surprise, desire, sadness, anger,
disgust and fear), a slide control to adjust the emotions'
intensities, and the approach of broad folksonomies [4].

2.2 Emotional tagging
For the purpose of emotional tagging we created a user interface
consisting of one picture and 18 slide controls on one web page
(figure 1). The 18 slide controls where split into two groups of 9
slide controls each.

Copyright is held by the author/owner(s).
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
ACM 978-1-60558-896-4/10/07.

791

usability test. In order to get a first idea of what people think of a
search engine for emotions we took a number of people who were
not involved in any part of development. They had to do several
tasks (e.g., to search for pictures with dogs and happiness)
including filling out questionnaires and doing task-based
thinking-aloud tests that were recorded. We concluded from the
results of the evaluation that the search for emotions in
multimedia documents is an exciting new task that people need to
adapt to. Especially the separated display of shown and felt
emotions in a two-column raster was at first hard to cope with.
And – not unimportant for Web 2.0 services – our test persons
found MEMOSE an enjoyable system.

Figure 1. MEMOSE's tagging interface

One group contained the emotions shown, i.e. the emotions
that are displayed on the picture. The other group covered the
emotions felt by the viewer when looking at the picture. The
range of every scroll bar contains values from 0 to 10, so the
viewers could differentiate the intensity of every emotion. A zero
value means that the emotion is not shown on the image or not
felt by the observer. As we had this large number of pictures for
every user to tag it was not practicable to force the students to tag
all the pictures in one pass one after the other. We applied a user
management by which the already tagged pictures were logged
and with its help every user could make breaks whenever needed
and go on later. Another benefit of this user management was that
no user could tag a picture twice.
Figure 2. MEMOSE's search results interface

2.3 Emotional retrieval
The retrieval tool was designed to access the tagged pictures in a
manner of a search engine. For this purpose we designed a clean
user interface consisting of one input field and nine checkboxes
where the emotions could be selected. The user first had to choose
the emotions to search for and then got suggestions by typing in
the search terms through an auto completion feature based on the
tags we got from Flickr according to the pictures. After sending
the request the results are shown in two distinct columns,
differencing between shown and felt emotions (figure 2). Both
columns are sorted in descending order regarding the selected
emotions. For every hit a thumbnail of the corresponding picture
and bars that show the intensity of the chosen emotions are
displayed. The retrieval status value (RSV) of the documents
(which satisfy both topic and emotion) was calculated through the
arithmetic mean of the intensity of the emotion. If there were
more than one emotional search argument, we took the sum of the
means as RSV. By clicking on the thumbnail the picture is opened
in an overlay window filled with further information like
associated tags, information about the author etc.

4. REFERENCES
[1] Knautz, K. et al. 2010. Indexieren von Emotionen bei
Videos. Information – Wissenschaft und Praxis 61(4), in
press
[2] Knautz, K., Soubusta, S., and Stock, W.G. 2010. Tag
Clusters as Information Retrieval Interfaces. In Proceedings
of the 43th Annual Hawaii International Conference on
System Sciences (HICSS-43), January 5-8, 2010. IEEE
Computer Society Press, 10 pages.
[3] Lee, H.J. and Neal, D. 2007. Toward Web 2.0 Music
Information Retrieval. Utilizing Emotion-based, Userassigned Descriptors. In Proceedings of the 70th ASIS&T
Annual Meeting. Joining Research and Practice: Social
Computing and Information Science, Milwaukee, October
19-24, 2007.
[4] Peters, I. 2009. Folksonomies. Indexing and Retrieval in
Web 2.0. Berlin, Germany: De Gruyter Saur.
[5] Schmidt, S. and Stock, W.G. 2009. Collective Indexing of
Emotions in Images. A Study in Emotional Information
Retrieval. J. Am. Soc. Inf. Sci. Tec, 60(5), 863-876.

3. Evaluation
From the tool box of evaluation methods [2] we took the
SERVQUAL, the customer value discovery and the critical
incident approach to evaluate the IT service dimension. To
evaluate the IT system dimension we worked with success factors
such as ease of use, perceived usefulness, trust and fun, and with a

[6] Stock, W.G. 2006. On Relevance Distributions. J. Am. Soc.
Inf. Sci. Tec., 57(8),

792

