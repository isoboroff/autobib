Combining Coregularization and Consensus-based
Self-Training for Multilingual Text Categorization
Massih-Reza Amini†

Cyril Goutte†

Nicolas Usunier‡

‡
Université Pierre et Marie Curie
Laboratoire d’Informatique de Paris 6
104, avenue du Président Kennedy
75016 Paris, France
Nicolas.Usunier@lip6.fr

†

National Research Council Canada
Institute for Information Technology
283, boulevard Alexandre-Taché
Gatineau, J8X 3X7, Canada
First.Last@nrc-cnrc.gc.ca

ABSTRACT

1. INTRODUCTION

We investigate the problem of learning document classiﬁers
in a multilingual setting, from collections where labels are
only partially available. We address this problem in the
framework of multiview learning, where diﬀerent languages
correspond to diﬀerent views of the same document, combined with semi-supervised learning in order to beneﬁt from
unlabeled documents. We rely on two techniques, coregularization and consensus-based self-training, that combine multiview and semi-supervised learning in diﬀerent ways. Our
approach trains diﬀerent monolingual classiﬁers on each of
the views, such that the classiﬁers’ decisions over a set of
unlabeled examples are in agreement as much as possible,
and iteratively labels new examples from another unlabeled
training set based on a consensus across language-speciﬁc
classiﬁers. We derive a boosting-based training algorithm
for this task, and analyze the impact of the number of views
on the semi-supervised learning results on a multilingual extension of the Reuters RCV1/RCV2 corpus using ﬁve different languages. Our experiments show that coregularization and consensus-based self-training are complementary
and that their combination is especially eﬀective in the interesting and very common situation where there are few
views (languages) and few labeled documents available.

In this paper, we address the problem of semi-supervised
learning of document classiﬁers in a multilingual setting
where documents are available as a parallel corpus with two
or more languages for which labels are only partially available.
Our motivation is that multilingual collections are becoming more and more common in national and supranational
contexts. However, the bulk of document classiﬁcation and
organization techniques and research is developed in the
monolingual setting, most often for English. In addition, labeling text documents may require cost- and time-intensive
human annotation, hence the widespread interest for semisupervised text classiﬁcation approaches that leverage unlabeled documents to speed-up the learning process.
Our work addresses the two issues of limited annotation
and multilingual setting. Using the diﬀerent languages as
diﬀerent views on a document, we develop a multiview, semisupervised approach that learns from collection of multilingual documents.
We formalize the problem as follows. Given a collection
of partially-labeled documents written in diﬀerent languages
and belonging to a set of classes that is ﬁxed across languages, we wish to learn a number of monolingual classiﬁers
for this common set of classes. Note that this problem is
diﬀerent from cross-language text categorization [5], where
a document written in one language must be classiﬁed in a
category system learned in another language.
In our setting, we assume that each document is available in several languages and we are interested in learning
improved monolingual classiﬁers. We also emphasize that
we wish to develop inter-dependent monolingual classiﬁers,
rather than a single multilingual classiﬁer, as we wish to be
able to classify an incoming document in whatever language
it is made available, without having to translate it beforehand.
There have been at least two approaches to multiview
semi-supervised learning. One can use coregularization [19]
to improve the view-speciﬁc classiﬁers by constraining them
to agree on some unlabeled data, leveraging unlabeled data
in a multiview learning framework. A more recent proposal
[3], by contrast, leverages the multiple views in a semisupervised learning framework by using the consensus between the diﬀerent views in a self-training framework. Our
solution is to combine those two components into a single boosting-based algorithm. View-speciﬁc classiﬁers are
trained using coregularization, and a consensus-based self-

Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Information
Storage - Record classiﬁcation; I.2 [Artiﬁcial Intelligence]:
Learning

General Terms
Algorithms, Experimentation, Theory

Keywords
Multilingual Document Classiﬁcation, Learning from Multiple Views, Semi-supervised Learning
Copyright 2010 Crown in Right of Canada.
This article was authored by employees of the National Research Council
of Canada. As such, the Canadian Government retains all interest in the
copyright to this work and grants to ACM a nonexclusive, royalty-free right
to publish or reproduce this article, or to allow others to do so, provided that
clear attribution is given both to the NRC and the authors.
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
ACM 978-1-60558-896-4/10/07.

475

training process iteratively labels unlabeled examples on
which the view-speciﬁc classiﬁers agree.
Using a large publicly available corpus of multilingual documents extracted from the Reuters RCV1 and RCV2 corpora, we show that our approach consistently improves over
both coregularization and self-training taken in isolation.
We also analyze the conditions in which the combination
is most proﬁtable. It turns out that adding coregularization to consensus-based self-training helps most when there
are few languages and few documents available. This is a
particularly interesting setting when resources are limited,
and corresponds in particular to the common situation of
bilingual data.
In the next section, we position our work with respect to
the state of the art. In Section 3, we then present the problem of multiview semi-supervised learning for multilingual
text classiﬁcation. Section 4 describes the boosting-based
algorithm we developed to obtain the language-speciﬁc classiﬁers. In Section 5, we present experimental results obtained with our approach on a subcollection of the Reuters
RCV1/RCV2 corpus. Finally, in Section 6 we discuss the
outcomes of this study and give some pointers to further
research.

We combine this coregularized learning with a consensusbased self-training framework similar to [3] where unlabeled
documents are iteratively labeled using the consensus prediction across the multiple views.
As both coregularization and consensus-based self-training
use multiview information and unlabeled data for training,
the key question we address is to see whether the two techniques can be complementary and improve on each other,
as opposed to being completely redundant. We also investigate in which conditions such a complementarity may be
exploited. We are particularly interested in the eﬀects of
coregularization in the common situation where the number
of views is small (eg bilingual documents) and few labeled
data are available.

2.

where each view xv provides a representation of the same
document in a diﬀerent vector space Xv . In the seminal
work on co-training [7], web pages are represented by either
their textual content (ﬁrst view) or anchor text pointing to
them (second view). In our setting of multilingual classiﬁcation, each view is the textual representation in a diﬀerent
language. Although typically one of the views is the original
version of the document and the others are its translations,
we never rely on this information and treat all views equally.
Note that in this framework all views of each document are
present simultaneously, hence we deal with multilingual text
classiﬁcation in a parallel corpus.
We further assume that we have a labeled training set
Z = {(xi , yi )|i ∈ {1, .., l}} and a possibly much larger
set of unlabeled training data that we split into two parts
denoted respectively by XU1 = {xl+i |i ∈ {1, .., m1 }} and
XU2 = {xl+m1 +i |i ∈ {1, .., m2 }}. Our goal is to obtain V
binary classiﬁers {hv : Xv → {−1, 1}|v ∈ {1, .., V }}, working each on one view, such that the predictive performance
as estimated for example from a test set is optimized. Note
that by construction, the label for a given document is the
same for all views.

3. FRAMEWORK
We consider V input spaces Xv ⊂ Rdv ; ∀v ∈ {1, .., V }, and
an output space Y. We take Y = {−1, +1} since we restrict
our presentation to binary classiﬁcation. Each multiview
document x ∈ X1 × ... × XV is a sequence
1
V
x def
= (x , ..., x )

RELATION TO STATE-OF-THE-ART

Document classiﬁcation has been a very popular application domain for Machine Learning algorithms, and in particular for multiview [7] and semi-supervised learning [16, 12].
The setting of multilingual document classiﬁcation, however,
has been much less studied so far [1, 2].
Interestingly, the original work on co-training [7] introduced both multiview and semi-supervised learning on a
document classiﬁcation task. Since then, both ﬁelds have developed greatly but mostly independently. Semi-supervised
learning approaches include generative approaches, densitybased or graph-based approaches (cf. [9] for an overview).
Multiview learning techniques include multiple kernel learning [4] and techniques relying on kernel Canonical Correlation Analysis [11].
Some recent work more in line with the original co-training
approach have introduced coregularization [19, 8], where
classiﬁers are learnt in each view using a multiview regularizer that constrains predictions made in each view to be
as similar as possible.
When this multiview regularizer is computed on unlabeled
data, this provides a way to perform semi-supervised learning in a multiview setting. More recently, a semi-supervised
multiview approach has been developed [3] where classiﬁers
are learned on each view using standard single view training, but unlabeled examples are iteratively labeled in a selftraining manner using the consensus across the views. The
multiview consensus ensures higher conﬁdence in the labeling, which yields improved semi-supervised learning rates.
Our work analyses and illustrates the combination of these
two techniques. We use a coregularization component similar to [19, 8], with the key diﬀerence that instead of the
coregularized least squares, we penalize disagreement using
a Kullback-Leibler divergence which has a more natural interpretation in the context of probabilistic classiﬁer outputs.
In addition, it allows us to develop a novel boosting-based
algorithm for solving the coregularized multilingual classiﬁcation problem.

4. MODEL
We iteratively learn each classiﬁer hv , ∀v ∈ V, while keeping ﬁxed the classiﬁers for the other views, hu , u ∈ V ∧u = v,
by optimizing the loss
L(hv , Z , XU1 , λ) = C(hv , Z ) +

λ
V −1

V
X

d(hv , hu , XU1 ),

u=1,u=v

(1)
where C(hv , Z ) is the (monolingual) cost of hv on the labeled training set Z , d(hv , hu , XU1 ) measures the divergence
between the two classiﬁers hv and hu on the unlabelled documents in XU1 , and λ is a discount factor which modulates
the inﬂuence of the disagreement cost on the optimization.

476

q0 ∈ Ω with respect to BF , under the set of linear constraints
{p ∈ Ω|pt Mv = p̃t Mv }, where p̃ ∈ Ω is a speciﬁed vector and
Mv is a n × d matrix, with n the number of examples in the
training set and d the dimension of the problem.1
Deﬁning the Legendre transform as

For the monolingual cost, we consider the standard misclassiﬁcation error:
C(hv , Z ) =

l
1X
[[yi hv (xvi ) ≤ 0]],
l i=1

where [[π]] is equal to 1 if the predicate π is true, and 0 otherwise. As this cost is non-continuous and non-diﬀerentiable,
it is typically replaced by an appropriate convex and differentiable proxy. Following standard practice in Machine
Learning algorithms, we replace [[z ≤ 0]] by the upper bound
a log(1+e−z ), with a = (log 2)−1 . The monolingual misclassiﬁcation cost becomes:
C(hv , Z ) =

Lf (q, Mv βv ) def
= argmin(BF (p||q) + Mv βv , p ),
p∈Ω

the dual optimization problem can be stated as ﬁnding a
vector q in the closure Q̄ of the set Q = {LF (q, Mv βv )|β ∈
Rp }, for which BF (p̃||q) is the lowest, under the set of linear
constraints {q ∈ Ω|q t Mv = p̃t Mv }.
It has been shown that both of these optimization problems have the same unique solution [14]. Moreover, [10] have
proposed a single parallel-update optimization algorithm to
ﬁnd this solution in the dual form.
They have further shown that their algorithm is a general
procedure for solving problems which aim to minimize the
exponential loss, like in Adaboost, or a log-likelihood loss,
like in logistic regression. Indeed, they showed the equivalence of these two loss minimization problems in terms of
Bregman distance optimization.
In order to apply the boosting algorithm proposed by [10],
we have to deﬁne a continuously diﬀerentiable function F
such that by properly setting Ω, p̃, q0 and Mv , the Bregman
distance BF (0||LF (q0 , Mv βv )) is equal to Eq. (2). Following
[10], we choose:

l
1X
a log(1 + exp(−yi hv (xvi ))),
l i=1

Assuming that each classiﬁer output may be turned into
a posterior class probability, we measure the disagreement
between the output distributions for each view using the
Kullback-Leibler (KL) divergence. Using the sigmoid function σ(z) = (1 + e−z )−1 to map the real-valued outputs of
the functions hv and hu into a probability, and assuming
that the reference distribution is the output of the classiﬁer
learned on the other views, hu , u ∈ {1, ..., V } ∧ u = v, the
disagreement d(hv , hu , XU1 ) becomes
d(hv , hu , XU1 ) =

m1
1 X
kl(σ(hu (xul+i ))||σ(hv (xvl+i ))),
m1 i=1

where for two binary probabilities p and q, the KL divergence is deﬁned as:
„
«
„ «
1−p
p
+ (1 − p) log
kl(p||q) = p log
q
1−q

∀p ∈ Ω = [0, 1]n , F (p) =

where αvi are non-negative real-valued weights associated to
examples xvi .
This deﬁnition yields that ∀p, q ∈ Ω × Ω:

BF (p||q) =

4.1 A view-specific boosting-like algorithm

1
=
l

a log(1 +

αvi

„
««
„ «
„
pi
1−pi
pi log
+ (1−pi ) log
qi
1−qi
(3)
z

and, ∀i, LF (q, z)i =

exp(−yi hv (xvi )))

i=1

m1
V
X
X
λ
+
kl(σ(hu (xul+i ))||σ(hv (xvl+i )))
(V − 1)m1 i=1 u=1,u=v

n
X
i=1

In order to learn the classiﬁer hv for view v, we need to
minimize
L(hv , Z , XU1 , λ)

αvi (pi log pi + (1−pi ) log(1−pi )) ,

i=1

There are two reasons for choosing the KL divergence:
ﬁrst, it is the natural equivalent in the classiﬁcation context of the l2 norm used for regression in previous work on
coregularization [19, 8, 18]; second, it allows the derivation
of a boosting approach for minimizing the local objective
function (1), as described in the following section.

l
X

n
X

qi e

i
− αv
i

1 − qi + qi e

(4)

z

i
− αv
i

Using Equations (3) and (4), and setting q0 = 12 1, the
vector with all components set to 12 , and Mv the matrix
such that ∀i, j, (Mv )ij = αvi yi xvij ,2 the Bregman distance in
Equation (3) writes:

(2)

We show how the loss-minimization of (2) is equivalent to
the minimization of a Bregman distance. This equivalence
will allow us to employ the boosting-like parallel-update optimization algorithm proposed by [10] to learn a linear classiﬁer hv : xv → βv , xv minimizing (2).
A Bregman distance BF of a convex, continuously diﬀerentiable function F : Ω → R on a set of closed convex set Ω
is deﬁned as

BF (0||LF (q0 , Mv βv )) =

n
X

αvi log(1 + e−yi

βv ,xv
i

).

(5)

i=1

1
We have deliberately set the number of examples to n as
in our equivalent rewriting of the minimization problem the
latter is not exactly m1 .
2
All vectors ∀i ∈ {1, .., n}, αi yi xvi should be normalized in
order to respect the constraint Mv ∈ [−1, 1]n×d .

∀p, q ∈ Ω, BF (p||q) = F (p) − F (q) − ∇F (q), (p − q) .
def

One optimization problem arising from a Bregman distance is to ﬁnd a vector p∗ ∈ Ω, closest to a given vector

477

Algorithm 1: Parallel-update optimization algorithm

Algorithm 2: Coregularized semi-supervised Learning
Input
: A set of labeled training examples Z ;
Two sets of unlabeled training data XU1 and XU2 ;
Initialize: Set ZU\ ← ∅;
(0)
∀v, hv def
= argminh C(h, Z );
repeat
t ← 1;
repeat
for v = 1, .., V do
(t)
Learn hv = argminh L(h, Z ∪ ZU\ , XU1 , λ);
end
t ← t + 1;
(t)
until Convergence of Δ(⊗Vv=1 hv , Z ∪ ZU\ , λ) ;
− Let XU\ be the set of unlabeled examples in XU2
on which all classiﬁers agree over the class label of
examples ;
−XU2 ← XU2 XU\ ;
−ZU\ ← ZU\ ∪ XU\ ;
until XU2 = ∅ or XU\ = ∅ ;
Output : Classiﬁers hv , ∀v ∈ {1, ..., V }

Input
: Matrix ∀v, Mv ∈ [−1, 1]
.
Initialize: Let ∀v, βv ← 0
for v = 1, ..., V do
for t = 1, 2, ... do
(t)
Let q (t) be the solution of LF (q0 , Mv βv );
for j = 1, ..., d do
P
(t)+
(t)
Wv,j ← i:sign((Mv )ij )=+1 qi |(Mv )ij |;
P
(t)−
(t)
Wv,j ← i:sign((Mv )ij )=−1 qi |(Mv )ij |;
!
+
n×d

(t)

δv,j ←

1
2

(t)

log

Wv,j

(t)−
Wv,j

;

end
(t+1)
(t)
(t)
← βv + δv ;
βv
end
end
(1)
(2)
Output : ∀v, the sequence βv , βv , ... verifying
lim BF (0||LF (q0 , Mv βv(t) )) = inf BF (0||LF (q0 , Mv βv ))
βv ∈Rd

t→∞

optimize each of the hv classiﬁers while keeping the classiﬁers for the other views ﬁxed, until the global objective
By developing Eq. (2), we get:
L(hv , Z , XU1 , λ)

1
=K+
l

l
X

a log(1 +

exp(−yi hv (xvi )))

Δ(⊗Vv=1 hv , Z ∪ ZU\ , λ) =
+

V
X

L(hv , Z ∪ ZU\ , XU1 , λ)

(8)

v=1

i=1

m1
V
X
X
v
λ
σ(hu (xul+i )) log(1 + e−hv (xl+i ) ) +
(V − 1)m1 i=1 u=1,u=v
m1
V
X
X
v
λ
(1 − σ(hu (xul+i ))) log(1 + ehv (xl+i ) ) (6)
(V − 1)m1 i=1 u=1,u=v

where K is a constant which does not depend on hv .
In order to make Eq. (6) identical to Eq. (5) (up to a
constant), we create, for each unlabeled document xvi ∈ XU1 ,
two examples (xvi , +1) and (xvi , −1) (which makes n = l +
2m1 ), and set the weights as follows:
8a
>
if xi ∈ Z ,
>
<l
v
V
X
αi =
λ
>
[[yi = −1]]+ yi σ(hu (xui )) else.
>
: (V − 1)m1
u=1,u=v

(7)
As a consequence, minimizing Eq. (2) is equivalent to
minimizing BF (0||q) over q ∈ Q̄, where
Q = {q ∈ [0, 1]l+2m1 | qi = σ(yi βv , xvi ), βv ∈ Rdv }.
This equivalence allows us to adapt the parallel-update
optimization algorithm described in [10] to learn each speciﬁcview classiﬁer, as described in Algorithm 1.

has reached a (possibly local) minimum.
This alternating optimization of partial cost functions bears
similarity with the block-coordinate descent technique [6].
At each iteration, block coordinate descent splits variables
into diﬀerent subsets, the set of the active variables and the
sets of inactive ones, then minimizes the objective function
along active dimensions while inactive variables are ﬁxed at
current values.
Once all language-speciﬁc classiﬁers have been trained we
assign class labels to unlabeled examples in XU2 for which all
mono-lingual classiﬁers predict the same class label. These
newly labeled examples are added to the labeled training
set. We then go back to the boosting-based coregularized
classiﬁer training using the combined labeled data, and so on
until either no remaining unlabeled example can be labeled
by consensus, or all unlabeled examples have been labeled.
As shown by [3], focusing on functions which agree across
several views reduces the complexity of the function class
and therefore improves the prediction ability of the resulting
model.
Algorithm 2 summarizes this coregularized self-training
strategy.

5. EXPERIMENTS
We conducted a number of experiments aimed at evaluating how the combination of coregularization and consensusbased self-training can help to take advantage of multilingual
unlabeled documents in order to learn eﬃcient classiﬁcation
functions.

4.2 Coregularized semi-supervised algorithm
We embed the boosting-based coregularized classiﬁer learning inside a self-training framework (cf. [22], Section 3)
which relies on consensus across views in order to automatically label documents from an unlabeled document pool XU2 .
Each monolingual classiﬁer hv , v ∈ V is ﬁrst initialized on
the supervised monolingual cost alone, then we iteratively

5.1 Data set
We perform experiments on a publicly available multilingual multiview text categorization corpus extracted from

478

Language
English
French
German
Italian
Spanish
Total

# docs
18,758
26,648
29,953
24,039
12,342
111,740

Class
C15
CCAT
E21
ECAT
GCAT
M11

# docs
18,816
21,426
13,701
19,198
19,178
19,421

combination, over diﬀerent subsets of the unlabeled training
documents.
- Baseline method [Boost]: This baseline corresponds
to a supervised monolingual boosting model optimizing Eq. 2 for λ = 0.
- Coregularized boosting [reg-Boost]: Boosting using coregularization on XU1 , optimizing Eq. 2 for λ = 0.
This constrains the supervised monolingual boosting
models to achieve high agreement among their predictions on XU1 .

Table 1: Number of documents per language (left)
and per class (right) in Reuters RCV1/RCV2 subcollection used in our experiments.

- Boosting with self-training [Boost-cst]: Boosting using consensus-based self-training, but no coregularization. This is similar in spirit to the iterative
co-training algorithm [7]. Given the language-speciﬁc
classiﬁers trained on an initial set of labeled examples,
we iteratively assign pseudo-labels to the unlabeled examples in XU2 for which all classiﬁer predictions agree.

the Reuters RCV1/RCV2 corpus [3].3 This corpus contains more than 110K documents from 5 diﬀerent languages,
(English, German, French, Italian, Spanish), distributed
over 6 classes (Table 1). Documents that originally had more
than one of these 6 labels were assigned to the smallest class.
We reserved a test split containing 25% of the documents,
respecting class and language proportions. Within the training set containing the remaining 75% of documents, we randomly sampled labeled documents (Z ), and split the remaining unlabeled data into two subsets: one for evaluating
the coregularization term (XU1 ), and one for the self-training
process (XU2 ). The motivation for that split is to avoid bias:
as coregularization enforces agreement between classiﬁers, it
may yield artiﬁcially high consensus for the examples used
in the coregularization term.
This corpus of multilingual documents is originally a comparable corpus as it covers the same subset of topics in all
languages. In order to produce multiple views for each documents, each original document extracted from the Reuters
corpus was translated in all other languages using a phrasebased statistical machine translation system [20]. The indexed translations are part of the corpus distribution.
More precisely, each document is indexed by the text appearing in its title (headline tag) and body (body tag). As
preprocessing, all text is lowercased, digits are mapped to a
single digit token, and tokens containing non-alphanumeric
characters are removed. For each language, words in a stoplist as well as tokens occurring in less than 5 documents
were also ﬁltered out. Documents were then represented as
a bag of words, using a TFIDF weighting scheme based on
BM25 [17].
Results are evaluated over the test set using the accuracy and the standard F1 measure [21], which is the harmonic average of precision and recall. The reported performance is averaged over the resulting ﬁve language-speciﬁc
classiﬁers. In addition, we also averaged over 10 random
(train/unlabeled/test) sets of the initial collection.

- SVM with self-training [SVM-cst]: This is similar to the previous method except that we use the
SVM-Perf package [13] to learn each language-speciﬁc
classiﬁers instead of boosting. For tuning the hyperparameter C, we ﬁrst tried the leave-one-out crossvalidation strategy. However, withP
small training sets
we found out that the default ( 1l li=1 ||xi ||)−1 gave
similar, and in some cases, better results. We therefore used that default C in all of our experiments.
- Coregularization+self-training [reg-Boost-cst]:
Coregularized boosting using the consensus-based selftraining: The coregularization term is computed over
XU1 and self-training iteratively labels documents from
XU2 .
- Boosting with full self-training [Boost-cst∗ ]: In
order to determine when the combination of coregularization and self-training is the most useful, we also
trained algorithm Boost-cst using all the unlabeled
training examples XU = XU1 ∪ XU2 rather than just
those in XU2 .
Our aim is to show the gradual eﬀect of each of the multiview and semi-supervised learning approaches on the boosting algorithm, progressing from Boost to reg-Boost and
Boost-cst, to reg-Boost-cst. Note that the reg-Boost
and Boost-cst algorithms use the two separate unlabeled
training subsets in diﬀerent manners. SVM-cst is the same
as Boost-cst using a SVM algorithm instead of Boosting.
This will allow us to benchmark the boosting-based algorithm against the state of the art SVM model in a similar
framework. Note that adding co-regularization in a SVM implementation requires some signiﬁcant changes to the underlying code, which is why we do not provide reg-SVM variants. Finally, using all the unlabeled training examples in
Boost-cst∗ and comparing the results to reg-Boost-cst
will allow us to uncover the situations in which it is beneﬁcial to combine coregularization and self-training rather
than use the latter alone on the combined unlabeled data.
This gives an idea of the true beneﬁt brought by coregularization.

5.2 Experimental setup
To validate the coregularized consensus-based self-training
approach described in the previous section, we test the following six classiﬁcation methods. The ﬁrst method is a
purely supervised technique which does not make use of
any unlabeled examples in the training stage. The following methods make use of the multiview and semi-supervised
learning approaches in diﬀerent ways, using coregularization and/or consensus-based self-training separately or in
3

http://multilingreuters.iit.nrc.ca/

479

Table 2: Test classiﬁcation accuracy and F1 of diﬀerent learning algorithms on the six classes, averaged over
10 random sets of 50 labeled examples per training set. For each class, the best result is in bold, and a ↓
indicates a result that is statistically signiﬁcantly worse than the best, according to a Wilcoxon rank sum test
with p < .01.

Strategy
Boost
reg-Boost
Boost-cst
SVM-cst
reg-Boost-cst

C15
Acc.
F1
0.771↓ 0.506↓
0.793↓ 0.532↓
0.804↓ 0.572↓
0.815 0.583
0.823 0.595

CCAT
Acc.
F1
0.662↓ 0.398↓
0.689↓ 0.419↓
0.708↓ 0.421↓
0.720↓ 0.438
0.748 0.449

E21
Acc.
F1
0.765↓ 0.323↓
0.783↓ 0.342↓
0.794↓ 0.365↓
0.800↓ 0.378↓
0.815 0.394

5.3 Experimental Results

ECAT
Acc.
F1
0.505↓ 0.347↓
0.513↓ 0.372↓
0.511↓ 0.384↓
0.522↓ 0.395↓
0.542 0.408

GCAT
Acc.
F1
0.781↓ 0.587↓
0.803↓ 0.608↓
0.866↓ 0.655↓
0.873↓ 0.662↓
0.895 0.687

M11
Acc.
F1
0.793↓ 0.586↓
0.815↓ 0.611↓
0.848↓ 0.668↓
0.861↓ 0.676↓
0.883 0.693

rely in some way on the consensus between classiﬁers trained
on the diﬀerent views. The question therefore arises as to
how redundant these two techniques are? Our experimental
results suggest that these techniques are in fact complementary.
The gains provided by adding coregularization to the selftraining boosting-based model is in fact similar to the gain
provided by coregularization in the supervised setting, which
suggest that the two eﬀects are essentially independent and
additive. In order to analyze more ﬁnely the situations in
which the combination of coregularization and consensusbased self-training is more advantageous, we compared all
the algorithms, including Boost-cst∗ , for diﬀerent numbers
of languages and diﬀerent amounts of labeled documents.
These results are reported in Section 5.3.2 and 5.3.3, right
after we address the issue of the discount factor λ.

We start our evaluation by analyzing the gains provided
by coregularization, the consensus-based self-training and
the combination of both, over the baseline boosting algorithm. We measure the classiﬁcation accuracy and F1 for a
ﬁxed number of labeled and unlabeled examples in the training set. In order to study the role of unlabeled data on the
learning behavior we begin our experiments with very few
labeled training examples. The size of the labeled training
sets in these ﬁrst experiments is ﬁxed to 50 (an average of
10 per language), with an equal sampling of 25 positive and
25 negative examples in Z . For coregularization, results
are reported for the best discount factor λ = 1, although as
illustrated in Section 5.3.1, results are fairly stable across a
wide range of values. We will later investigate the impact
on the test performance of the number of labeled examples
and the number of views (cf Sections 5.3.3 and 5.3.2).
Table 2 summarizes results obtained by Boost, reg-Boost,
Boost-cst, SVM-cst and reg-Boost-cst averaged over ﬁve
languages and 10 random splits of tests sets for our six main
categories. We use bold face to indicate the highest performance rates, and the symbol ↓ indicates that performance
is signiﬁcantly worse than the best result, according to a
Wilcoxon rank sum test used at a p-value threshold of 0.01
[15]. From these results it becomes clear that:

5.3.1 The effect of the coregularization factor λ
We analyze the inﬂuence of the discount factor λ on the
performance of reg-Boost-cst for varying amounts of labeled training data.4 The results obtained on class E21 are
presented in Figure 1. Note that λ controls the relative importance of the unlabeled data in the coregularization (with
λ = 0 corresponding to no regularization). Figure 1 shows
that unlabeled examples become relatively less important
as more labeled data is available: as the amount of labeled
training data increases from 50 to 300, the optimal discount
factor λ moves away from 1.
We recall that for λ = 1, unlabeled data plays the same
role in the training procedure as labeled data.
Note also that in all cases, the performance of the resulting
classiﬁers seems relatively stable for a wide range of values
of λ. This suggests that the results are not overly sensitive
to a precise choice of discount factor λ.

1. Using the ﬁrst part of the unlabeled training examples
(XU1 ) to coregularize the boosting algorithm, algorithm
reg-Boost always improves over Boost by an average
of 2-3 points in F1 .
2. The consensus-based self-training framework implemented in Boost-cst and SVM-cst also improves over the
baseline. In addition, it always seems to outperform
coregularization (reg-Boost) alone. In this self-training
framework, the SVM classiﬁers SVM-cst tend to outperform the boosting-based classiﬁers Boost-cst.

5.3.2 The value of labeled data
We also analyze the behavior of the various algorithms
for growing initial amounts of labeled data in the training
set. Figure 2, illustrates this by showing the F1 measures
on classes CCAT and ECAT with respect to the number of labeled documents in the initial labeled training set Z . For
all labeled data sizes, the proportion of negative/positive examples is maintained at 50%. As expected, all performance
curves increase monotonically with respect to the additional

3. Finally, the combination of coregularization and selftraining (reg-Boost-cst) produces a further improvement of around 1-2 points in F1 over the best semisupervised result (SVM-cst). The improvement is statistically signiﬁcant in four classes out of six.
Our analysis of these results is that both coregularization and the consensus-based self-training provide consistent
improvements over training independent monolingual classiﬁers. Both are instances of multiview learning, and both

4
We always maintain the proportion of positive/negative
documents in the labeled training set to 50%/50%.

480

ECAT
0.55

0.5

0.5

0.45

0.45

0.4

0.4

F1

F1

CCAT
0.55

0.35

0.35
*

Boost-cst
reg-Boost-cst
SVM-cst
reg-Boost
Boost

0.3
0.25
10

50

100

200

400

Boost-cst*
reg-Boost-cst
SVM-cst
reg-Boost
Boost

0.3
0.25
1000

10

# of labeled documents in the training set

50

100

200

400

1000

# of labeled documents in the training set

Figure 2: F1 on classes CCAT and ECAT with respect to the number of labeled documents in the initial labeled
training set Z .

forming coregularization and self-training on the same unlabeled data. The previous results suggest that the performance gain is higher when unlabeled examples are iteratively labeled in the self-training framework than when
they are used in coregularization to enforce agreement between the language-speciﬁc classiﬁers. The question therefore arises as to what the performance would be if all the unlabeled examples were used in consensus-based self-training
rather than being split between coregularization and selftraining? In addition, the consensus is expected to be more
reliable when there are many views than when there are few,
in which case the language-speciﬁc classiﬁers could agree
by chance but erroneously. We therefore investigate the
eﬀect of the number of views on the performance of the
reg-Boost-cst and Boost-cst∗ algorithms. Figure 3 depicts these results by comparing both algorithms for varying
numbers of languages on two classes, E21 and C15. All re-

labeled data. When there are suﬃcient labeled examples,
all algorithms actually converge to the same F1 value, suggesting that the labeled data carries suﬃcient information
and that no additional information could be extracted from
unlabeled examples. For a low number of labeled training
data, the contribution of each of the algorithms that use
unlabeled data is clearly shown. Note that these curves are
obtained using ﬁve languages, such that the highest performance is achieved by Boost-cst∗ , which is consistent with
the ﬁndings of the previous section. When fewer views are
available, the relative positions of the top algorithms are different, but the eﬀect is similar in that the gains are more
important when fewer initial labeled documents are available.

5.3.3 The effect of the number of languages
In our experiments, the unlabeled training set was split in
two parts, one for coregularization and one for self-training.
Our motivation was to examine the eﬀect of each of the
techniques individually without introducing any bias by per-

0.6
0.55

E21

0.5
F1

0.46
0.44

0.45
0.4

F1

0.42
0.35
0.4

Boost-cst*
reg-Boost-cst
C15
E21

0.3

0.38

|Zl|=50
|Zl|=150
|Zl|=300
Maxima

0.36
0

0.2

0.4

0.6

0.8

2

3

4

5

# of languages

Figure 3: F1 with respect to the number of languages
used for coregularization and self-training on classes
E21 (solid) and C15 (dash). Comparisons involve
reg-Boost-cst () and the boosting algorithm using
the unlabeled examples (XU1 ∪ XU2 ) for self-training
Boost-cst∗ ().

1

λ

Figure 1: F1 with respect to the coregularization
factor λ for diﬀerent labeled training sizes on class
E21.

481

[6] D. P. Bertsekas. Nonlinear Programming. Athena
Scientiﬁc, 1999.
[7] A. Blum and T. M. Mitchell. Combining Labeled and
Unlabeled Data with Co-Training. In Proc. 11th
Annual Conference on Learning Theory (COLT 1998),
pages 92–100, 1998.
[8] U. Brefeld, T. Gärtner, T. Scheﬀer, and S. Wrobel.
Eﬃcient Co-regularised Least Squares Regression. In
Proc. 23rd International Conference on Machine
Learning (ICML 2006), pages 137–144, 2006.
[9] O. Chapelle, B. Schölkopf, and A. Zien, editors.
Semi-Supervised Learning. MIT Press, 2006.
[10] M. Collins, R. E. Schapire, and Y. Singer. Logistic
regression, Adaboost and Bregman Distances.
Machine Learning, 48(1-3):253–285, 2002.
[11] J. D. Farquhar, D. R. Hardoon, H. Meng,
J. Shawe-Taylor, and S. Szedmak. Two View
Learning: SVM-2k, Theory and Practice. In Advances
in Neural Information Processing 18 (NIPS 2005),
pages 355–362, 2005.
[12] T. Joachims. Transductive Inference for Text
Classiﬁcation using Support Vector Machines. In Proc.
of the Sixteenth International Conference on Machine
Learning (ICML 1999), pages 200–209, 1999.
[13] T. Joachims. Training Linear SVMs in Linear Time.
In Proceedings of the ACM Conference on Knowledge
Discovery and Data Mining (KDD), pages 217–226,
2006.
[14] J. D. Laﬀerty, S. D. Pietra, and V. D. Pietra.
Statistical Learning Algorithms Based on Bregman
Distances. In Canadian Workshop on Information
Theory, 1997.
[15] E. Lehmann. Nonparametric Statistical Methods Based
on Ranks. McGraw-Hill, New York, 1975.
[16] K. Nigam, A. McCallum, S. Thrun, and T. M.
Mitchell. Learning to Classify Text from Labeled and
Unlabeled Documents. In Proc. of the 15th National
Conference on Artificial intelligence (AAAI/IAAI
1998, pages 792–799, 1998.
[17] S. E. Robertson, S. Walker, S. Jones,
M. Hancock-Beaulieu, and M. Gatford. Okapi at
TREC-3. In Proc. 3rd Text Retrieval Conference
(TREC), pages 109–126, 1994.
[18] D. S. Rosenberg and P. L. Bartlett. The Rademacher
Complexity of Co-regularized Kernel Classes. In Proc.
of the 11th International Conference on Artificial
Intelligence and Statistics (AISTATS 2007), pages
396–403, 2007.
[19] V. Sindhwani, P. Niyogi, and M. Belkin. A
Co-regularization Approach to Semi-supervised
Learning with Multiple Views. In Proceedings of the
ICML-05 Workshop on Learning with Multiple Views,
pages 74–79, 2005.
[20] N. Ueﬃng, M. Simard, S. Larkin, and J. H. Johnson.
NRC’s PORTAGE system for WMT 2007. In
ACL-2007 Second Workshop on SMT, 2007.
[21] C. J. Van Rijsbergen. Information Retrieval.
Butterworth-Heinemann, Newton, MA, 1979.
[22] X. Zhu. Semi-supervised Learning Literature Survey.
Technical report, University of Wisconsin Madison,
2008.

sults obtained for less than ﬁve languages are averaged over
all possible such combinations of languages.
These results show that for ﬁve languages, using all the
unlabeled data for self-training is slightly more eﬃcient than
reserving part of it for coregularization. However, when the
number of views is smaller, the combination of both coregularization and consensus-based self-training is more advantageous. Note that this is a common situations, for example
when only bilingual documents are available.
This result suggests that in the situation where we have
few views, reducing the disagreement between language speciﬁc classiﬁers through coregularization may lead to a more
eﬀective use of consensus-based labeling, decreasing the number of noisy examples added to the training set during selftraining. On the other hand, when the number of views is
large, the consensus is usually reliable enough without the
need for coregularization.

6.

CONCLUSION

In this paper we proposed a multiview semi-supervised
boosting algorithm for multilingual document classiﬁcation.
We have shown how to embed a disagreement-based coregularization term into a classiﬁcation objective function using
a Bregman distance. This embedding allowed us to adapt an
existing boosting algorithm to learn language-speciﬁc classiﬁers while enforcing consistency in prediction across languages. We then proposed a self-training algorithm which
assigns class labels to unlabeled data based on the consensus
of the classiﬁer predictions across the diﬀerent views.
Our results show clearly that the consensus based selftraining allows to reach high performance in the situation
where few initial labeled training documents are available.
We also showed that when there are fewer languages, combining coregularization with the consensus-based self-training
approach provides a better leverage of the unlabeled data by
improving the quality of the consensus.

Acknowlegdements
This work was supported in part by the IST Program of
the European Community, under the PASCAL2 Network of
Excellence, IST-2002-506778.

7.

REFERENCES

[1] J. J. G. Adeva, R. A. Calvo, and D. L. de Ipiña.
Multilingual Approaches to Text Categorisation.
UPGRADE: The European Journal for the
Informatics Professional, VI(3):43–51, 2005.
[2] M.-R. Amini and C. Goutte. A Co-classiﬁcation
Approach to Learning from Multilingual Corpora.
Machine Learning, 79(1-2):105–121, 2010.
[3] M.-R. Amini, N. Usunier, and C. Goutte. Learning
from Multiple Partially Observed Views - an
Application to Multilingual Text Categorization. In
Advances in Neural Information Processing Systems
22 (NIPS 2009), pages 28–36, 2009.
[4] F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan.
Multiple Kernel Learning, Conic Duality, and the
SMO Algorithm. In Proc. 21st International
Conference on Machine Learning (ICML 2004), 2004.
[5] N. Bel, C. H. Koster, and M. Villegas. Cross-lingual
Text Categorization. In ECDL-2003, pages 126–139,
2003.

482

