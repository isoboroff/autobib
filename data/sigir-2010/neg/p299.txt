SED: Supervised Experimental Design and Its Application
to Text Classification
Yi Zhen and Dit-Yan Yeung
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
Hong Kong, China

{yzhen, dyyeung}@cse.ust.hk
ABSTRACT

number of unlabeled documents, such as web pages, newspapers and journal articles. In recent years, a new approach
called active learning [1, 3, 5, 6, 9, 11, 13, 14, 15, 16, 18, 20,
25] has been developed in the machine learning community
with the goal of reducing the labeling cost by identifying
and presenting the most informative examples from the unlabeled examples for the human experts to label.
Although a lot of work has been done in active learning research, most of the existing active learning methods are still
far from satisfactory with apparent shortcomings. In particular, many methods only take into consideration partial
information to determine the informativeness of examples.
Some methods consider information conveyed by the class
boundaries, some consider information conveyed by the distribution of unlabeled data, and some consider the disagreement between learners when multiple learners are involved.
Unfortunately, none of these methods is consistently better
than others in all situations. Another drawback is that most
active learning algorithms select only one example at a time
for labeling. Compared with a batch approach [7, 9, 10]
which selects multiple examples in one iteration, this greedy
incremental approach is at best suboptimal and is not suitable for large-scale and parallel computing applications.
Experimental design [2, 20, 21], which is one of the stateof-the-art active learning approaches for text classiï¬cation,
can eï¬€ectively exploit the distribution of unlabeled data
while supporting batch selection at the same time. Despite
their appealing properties, existing methods based on experimental design cannot make use of label information even
when labeled data are available. Thus, these methods are
intrinsically unsupervised in nature.
In this paper, we propose a novel batch mode active learning algorithm, called supervised experimental design (SED),
which incorporates label information into the experimental
design procedure. SED is a supervised extension of experimental design with a new regularization term that incorporates label information added to the objective function.
To the best of our knowledge, no work has been done so far
to utilize label information in the experimental design procedure. Some favorable properties of SED are highlighted
here:

In recent years, active learning methods based on experimental design achieve state-of-the-art performance in text
classiï¬cation applications. Although these methods can exploit the distribution of unlabeled data and support batch
selection, they cannot make use of labeled data which often
carry useful information for active learning. In this paper,
we propose a novel active learning method for text classiï¬cation, called supervised experimental design (SED), which
seamlessly incorporates label information into experimental
design. Experimental results show that SED outperforms
its counterparts which either discard the label information
even when it is available or fail to exploit the distribution of
unlabeled data.

Categories and Subject Descriptors
G.3 [Mathematics of Computing]: Probability and Statisticsâ€”Experimental Design; H.3 [Information Storage and
Retrieval]: Information Search and Retrievalâ€”Clustering

General Terms
Algorithms, Theory

Keywords
Active Learning, Supervised Experimental Design, Text Classiï¬cation, Convex Optimization

1.

INTRODUCTION

There has been a long tradition of research on text classiï¬cation in both the information retrieval and machine learning communities. In order to learn a good text classiï¬er, a
large number of labeled documents are often needed for classiï¬er training. However, labeling documents always needs
domain knowledge and thus is diï¬ƒcult, time consuming and
costly. On the other hand, it is much easier to obtain a large

âˆ™ To the best of our knowledge, SED is the ï¬rst work
that incorporates label information into experimental
design.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGIRâ€™10, July 19â€“23, 2010, Geneva, Switzerland.
Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

âˆ™ SED outperforms (unsupervised) experimental design,
which discards the label information even when it is
available. This shows that label information does pro-

299

Given a set of labeled data {(xğ‘– , ğ‘¦ğ‘– )}ğ‘€
ğ‘–=1 , the maximum
likelihood estimate (MLE) of the model parameter vector w
can be obtained by minimizing the residual sum of squares:

vide useful information for active document selection
and SED can utilize the information very eï¬€ectively.
âˆ™ SED outperforms margin-based active learning under
highly unbalanced data distributions which are often
encountered in practice.

ğ‘€
{
}
âˆ‘
wÌ‚ = argmin ğ½(w; X, y) =
(wğ‘‡ xğ‘– âˆ’ ğ‘¦ğ‘– )2
w
ğ‘‡

= (X X)

âˆ™ SED is convex and thus global optimality can be guaranteed.

ğ‘‡

X y,

(1)

ğ‘‡

where X = [x1 , . . . , xğ‘€ ] is a matrix of the labeled data and
y is a vector of the corresponding target outputs.2
If we put a spherical Gaussian prior on the noise ğœ–, i.e.,
ğœ– âˆ¼ ğ’© (0, ğœ 2 ), it can be proved easily that wÌ‚ is an unbiased
estimate of w with covariance:
[
]
cov[wÌ‚] = cov (Xğ‘‡ X)âˆ’1 Xğ‘‡ y

The remainder of this paper is organized as follows. In
Section 2, we will introduce the notations and some related
work. In Section 3, we ï¬rst introduce transductive experimental design and then present our SED model and algorithm in detail. Extensive empirical studies conducted on
two real-world text corpora are presented in Section 4. Section 5 concludes our paper.

2.

ğ‘–=1

âˆ’1

= (Xğ‘‡ X)âˆ’1 Xğ‘‡ cov[y] X(Xğ‘‡ X)âˆ’1
= ğœ 2 (Xğ‘‡ X)âˆ’1 .

Traditional experimental design aims at minimizing the
covariance of wÌ‚, which characterizes the model uncertainty
in some sense. Three criteria have been commonly used in
the literature:

NOTATIONS AND RELATED WORK

Throughout this paper, we use boldface uppercase letters
(e.g. X) to denote matrices and boldface lowercase letters
(e.g. x) to denote vectors. We use tr(X) to denote the trace
of X and Xğ‘‡ to denote its transpose. Moreover, we use
calligraphic letters (e.g. ğ’œ) to denote sets and âˆ£ğ’œâˆ£ to denote
the size of ğ’œ.
Given the whole data set represented as Xğ’« âˆˆ â„ğ‘€ Ã—ğ· or
ğ’« = {x1 , . . . , xğ‘€ }, in which each data point xğ‘– is a ğ· Ã—
1 vector, a generic active learning problem [4, 11] can be
deï¬ned as selecting a subset of unlabeled data points from
a candidate set Xğ’ âˆˆ â„ğ‘ Ã—ğ· or ğ’ = {x1 , . . . , xğ‘ }, such
that if the selected data points are labeled and added to the
training set for re-training the classiï¬er, the improvement
of the classiï¬er will be maximized. We call the subset of
selected data the active set and denote it as Xğ’œ âˆˆ â„ğ¾Ã—ğ· or
ğ’œ = {x1 , . . . , xğ¾ }.1
The promise of active learning is appealing because it
can help to alleviate the labeled data deï¬ciency problem
commonly encountered in many supervised learning applications. Existing active learning algorithms for text classiï¬cation either select the most uncertain data given the current
classiï¬er [11], select the data with the smallest margin [18],
select the data on which multiple classiï¬ers disagree most
with each other [5, 14, 17], or select the data that optimize
some information gain [6, 13, 16, 25].
Closely related to active learning is experimental design
in statistics [2]. Conventionally, experimental design considers the problem of learning a predictive function ğ‘“ (x) from
experiment-measurement pairs (xğ‘– , ğ‘¦ğ‘– ). Given that conducting an experiment is expensive, experimental design seeks
to select the most informative experiments to conduct such
that the number of measurements needed can be reduced.
Traditional experimental design considers the following
linear regression model:

âˆ™ D-optimal design: minimizing the determinant of cov[wÌ‚];
âˆ™ A-optimal design: minimizing the trace of cov[wÌ‚];
âˆ™ E-optimal design: minimizing the maximum eigenvalue
of cov[wÌ‚].
Recently, Yu et al. [20] proposed a method, called transductive experimental design (TED), which selects the most
informative examples by reducing the model uncertainty on
all of the unlabeled data and thus eï¬€ectively exploits the
distribution of the unlabeled data. He et al. [8] applied similar ideas to content-based image retrieval (CBIR), where a
Laplacian regularization term is added and then the model
uncertainty, represented by a new covariance matrix, considers the smoothness among data points.
Despite the appealing properties which include clear mathematical formulation and the ability of batch selection, algorithms based on experimental design often have to deal with
combinatorial complexity and are NP-hard. Since the optimization problems involved are non-convex, the solutions
obtained may correspond to poor local minima. To address
this problem, some approximation methods based on convex
relaxation have been developed [21, 23].

3. SUPERVISED EXPERIMENTAL DESIGN
Existing active learning methods based on experimental
design, such as TED, are formulated under the setting that
all available data are unlabeled. As such, they cannot make
use of the label information even when it is available.
Since label information has been found very useful to example (or document) selection [5, 11, 14, 16, 18], incorporating label information into the example selection procedure
of experimental design is a very worthwhile direction to explore.
In this section, we ï¬rst brieï¬‚y review TED in Section 3.1
and then present our method, SED, in Section 3.2. The
algorithm for SED will be summarized in Section 3.3 and its
complexity analysis will be presented in Section 3.4.

ğ‘¦ = wğ‘‡ x + ğœ–,
where ğ‘¦ is the measurement, x is the ğ· Ã— 1 feature vector
of the experiment, w is the ğ· Ã— 1 model parameter vector
and ğœ– is the noise term.
1
The reader should note that points in diï¬€erent sets with
the same index are not necessarily the same point, although
we require that the points in ğ’ should appear in ğ’« and the
points in ğ’œ should appear in ğ’.

2
In the sequel, we will also refer to them as labels even
though the term â€˜labelâ€™ is more appropriately used for classiï¬cation problems.

300

3.1 Transductive Experimental Design

where fÌƒğ’œ is similar to fÌƒ but is deï¬ned only on the active
set Xğ’œ and ğ›¾ is a user-deï¬ned parameter controlling the
contribution of model uncertainty due to the current labeled
data. In other words, ğ›¾ controls the contribution of label
information.
Since the optimization problem in Equation (4) is NPhard, non-convex and can easily get trapped in local minima,
we borrow ideas from [21] to reformulate it in a convex form
and deï¬ne our SED problem as follows.

TED [20] seeks to choose Xğ’œ from Xğ’ such that a function ğ‘“ learned from Xğ’œ has the smallest predictive variance
on Xğ’« . The goal can be achieved by solving the following
optimization problem:
[
)âˆ’1 ğ‘‡ ]
(
min tr Xğ’« Xğ‘‡ğ’œ Xğ’œ + ğœ‡I
Xğ’«
Xğ’œ

s.t.

Xğ’œ âŠ‚ Xğ’ , âˆ£ğ’œâˆ£ = ğ¾,

(2)

where I is an identity matrix whose dimensionality is determined by the problem and ğ¾ is the number of examples
selected. The objective function may also be considered as
model uncertainty over Xğ’« . We note that it only depends on
the input features of the training examples and thus is independent of the labels. This is because in the error function
ğ½(w; X, y) of the linear regression model in Equation (1),
the model parameter vector w is only coupled with the labels ğ‘¦ğ‘– linearly and hence a second derivative with respect
to w makes all the ğ‘¦ğ‘– terms disappear.
Since the TED optimization problem is non-convex and
can easily get stuck in local optima, Yu et al. [21] proposed a
convex relaxation of TED (Convex TED). The optimization
problem of Convex TED is deï¬ned as follows:
(
)
âˆ‘ğ‘€
âˆ‘ğ‘ ğ›¼2ğ‘–ğ‘—
ğ‘‡
2
min
+ ğ›¾âˆ£âˆ£ğœ·âˆ£âˆ£1
ğ‘–=1 âˆ£âˆ£xğ‘– âˆ’ Xğ’ ğœ¶ğ‘– âˆ£âˆ£ +
ğ‘—=1 ğ›½ğ‘—
ğœ· ,ğœ¶ğ‘–
xğ‘– âˆˆ Xğ’« , ğœ¶ğ‘– âˆˆ â„ğ‘ , ğœ· âˆˆ â„ğ‘ Ã—1 , ğœ· â‰¥ 0,

s.t.

Deï¬nition 1. Supervised Experimental Design (SED)
(
)
âˆ‘ğ‘ ğ›¼2ğ‘–ğ‘—
âˆ‘ğ‘€
ğ‘‡
2
+ ğ›¾1 âˆ£âˆ£ğœ·âˆ£âˆ£1 + ğ›¾2 ğœ· ğ‘‡ fÌƒ
âˆ’
X
ğœ¶
âˆ£âˆ£
+
min
âˆ£âˆ£x
ğ‘–
ğ’ ğ‘–
ğ‘–=1
ğ‘—=1 ğ›½ğ‘—
ğœ· ,ğœ¶ğ‘–

Theorem 1. SED is convex w.r.t. ğœ· and {ğœ¶ğ‘– }.
Proof. Let the objective
function of SED be ğ¹) = ğ¹1 +
(
âˆ‘ğ‘€
âˆ‘
ğ›¼2
ğ‘–ğ‘—
ğ¹2 , where ğ¹1 = ğ‘–=1 âˆ£âˆ£xğ‘– âˆ’ Xğ‘‡ğ’ ğœ¶ğ‘– âˆ£âˆ£2 + ğ‘
+ğ›¾1 âˆ£âˆ£ğœ·âˆ£âˆ£1
ğ‘—=1 ğ›½ğ‘—
and ğ¹2 = ğ›¾2 ğœ· ğ‘‡ fÌƒ . Because fÌƒ is constant, ğ¹2 is linear in ğœ·.
Thus ğ¹2 is convex with respect to ğœ·. Since ğ¹1 is also convex
with respect to ğœ· and {ğœ¶ğ‘– }3 and ğ¹1 + ğ¹2 is a convex combination of two convex functions ğ¹1 and ğ¹2 , ğ¹ is thus convex
with respect to ğœ· and {ğœ¶ğ‘– }. This completes the proof.

(3)

3.3

Algorithm

It is convenient to ï¬nd the local optimum of Problem (5),
which is also the global optimum, by updating ğœ· and {ğœ¶ğ‘– }
iteratively. More speciï¬cally, we can ï¬nd the analytical solution for updating one variable while ï¬xing the other as
follows:
v
u
u
ğ›½ğ‘— = â·

ğ‘€
âˆ‘
1
2
ğ›¼ğ‘–ğ‘—
, ğ‘— = 1, . . . , ğ‘,
ğ›¾1 + ğ›¾2 ğ‘“Ëœğ‘— ğ‘–=1

ğœ¶ğ‘– = (diag(ğœ·)âˆ’1 + Xğ’ Xğ‘‡ğ’ )âˆ’1 Xğ’ xğ‘– , ğ‘– = 1, . . . , ğ‘€.

(6)
(7)

The proposed algorithm is summarized in Algorithm 1.

3.2 Supervised Experimental Design
Given a set of labeled data points (training set), we can
learn a classiï¬er ğ‘“ from the data. In a typical active learning setting in which labeled data are scarce, ğ‘“ may not be
accurate enough and hence it is desirable to select some unlabeled data points for labeling to enlarge the training set.
However, although ğ‘“ is not accurate enough, it still carries
some useful information about the data points. Let f be a
vector of decision values on the candidate set Xğ’ and fÌƒ be
the vector after taking the absolute value of each element
of f . For example, in support vector machine (SVM), fÌƒ indicates the uncertainty of the current classiï¬er about the
labels of the examples. The smaller the ğ‘—th element ğ‘“Ëœğ‘— of fÌƒ
is, the less certain is the classiï¬er about the example. Intuitively speaking, the most informative examples should be
those with the smallest ğ‘“Ëœğ‘— values.
Based on the above intuition and the formulation of TED,
we can choose the most informative Xğ’œ by solving the following optimization problem,
[
(
)âˆ’1 ğ‘‡ ]
min tr Xğ’« Xğ‘‡ğ’œ Xğ’œ + ğœ‡I
Xğ’« + ğ›¾ fÌƒğ’œ

3.4

Complexity Analysis

The main computation of SED is to update ğœ· and {ğœ¶ğ‘– }.
The time complexity of updating ğœ· (Equation (6)) is ğ‘‚(ğ‘€ ğ‘ )
and that of updating {ğœ¶ğ‘– } (Equation (7)) is ğ‘‚(ğ‘ 3 ). Hence,
the time complexity of one iteration is ğ‘‚(ğ‘ 3 +ğ‘€ ğ‘ ). Though
our algorithm converges very quickly in practice, it is interesting and worthwhile to explore more eï¬ƒcient techniques
to solve the problem, and we leave it as future work.

4. EMPIRICAL ANALYSIS
We conduct several experiments to compare SED with
some other related methods. We have the following questions in mind while designing and conducting the experiments:
1. How does SED perform when compared with other
state-of-the-art active learning methods?
2. How eï¬€ective is label information for experimental design?

Xğ’œ

Xğ’œ âŠ‚ Xğ’ , âˆ£ğ’œâˆ£ = ğ¾,

(5)

We can further prove that SED is a convex problem.

where the variables ğ›½ğ‘— , ğ‘— = 1, . . . , ğ‘ , control the inclusion of
examples in Xğ’ into the training set Xğ’œ , the â„“1 -norm âˆ£âˆ£ğœ·âˆ£âˆ£1
enforces the sparsity of ğœ·, and ğ›¼ğ‘–ğ‘— denotes the ğ‘—th element
of ğœ¶ğ‘– . According to [20, 21], TED and Convex TED tend to
select examples representative of all the unlabeled data and
hence exploit the distribution of the whole data space.
Since experimental design based methods do not use label information, we call them unsupervised active learning
methods here. In the next subsection, we will present our
supervised extension, SED, which can eï¬€ectively utilize the
available label information to select the most informative
examples.

s.t.

xğ‘– âˆˆ Xğ’« , ğœ¶ğ‘– âˆˆ â„ğ‘ , ğœ· âˆˆ â„ğ‘ Ã—1 , ğœ· â‰¥ 0.

s.t.

3

(4)

301

The proof can be found in [21].

Algorithm 1 Algorithm for SED
1: INPUT:
â„’0 â€“ set of labeled data points
ğ’° 0 â€“ set of unlabeled data points
ğ‘‡ â€“ number of active learning iterations
ğ¾ â€“ number of examples selected in each iteration
2: for ğ‘¡ = 1 to ğ‘‡ do
3:
Train classiï¬er ğ‘“ based on â„’ğ‘¡âˆ’1 .
4:
Compute absolute decision values fÌƒ .
5:
Initialize {ğœ¶ğ‘– }.
6:
repeat
7:
Fix {ğœ¶ğ‘– }, update ğœ· using Equation (6).
8:
Fix ğœ·, update {ğœ¶ğ‘– } using Equation (7).
9:
until converge w.r.t. objective value of Problem (5).
10:
Choose ğ¾ examples with the largest ğœ· values into
and request their labels.
Xğ‘¡âˆ’1
ğ’œ
11:
Update â„’ğ‘¡ â† â„’ğ‘¡âˆ’1 âˆª Xğ‘¡âˆ’1
and ğ’° ğ‘¡ â† ğ’° ğ‘¡âˆ’1 âˆ– Xğ‘¡âˆ’1
ğ’œ
ğ’œ .
12: end for
13: Train classiï¬er ğ‘“ based on â„’ğ‘‡ .
14: return ğ‘“

performance, because in our setting, each binary classiï¬cation task is unbalanced (only about 25% of the documents in
each Newsgroups data set and about 30% of the documents
in each Reuters data set are positive). Note that a larger
value of AUC indicates a better performance.
At each iteration of our experiments, an active learning
method selects a set of ğ¾ = 5 unlabeled examples from the
candidate set. The selected examples are then labeled and
added to the training set â„’. The classiï¬er is then trained
on the expanded training set and used to predict the class
labels of all documents. An AUC score is then computed
based on the predictions. In order to randomize the experiments as well as to reduce the computational cost, we
restrict the candidate set to randomly cover only a fraction
of all the unlabeled documents. Ten diï¬€erent candidate sets
are generated for each experiment and the average AUC
value, together with the standard deviation, is reported.
We compare SED with four popular active learning methods for text classiï¬cation:

3. How does varying the size of the candidate set aï¬€ect
the performance of SED?

âˆ™ Sequential TED [20], which sequentially selects examples using TED.

âˆ™ Convex TED [21], which is a convex relaxation of
TED.

These questions are answered in separate subsections: question 1 in Section 4.3, question 2 in Section 4.4.1, and question 3 in Section 4.4.2.

âˆ™ Margin, which chooses the examples closest to the
class boundary. This method implements the basic
idea of [18] but uses the squared loss instead of the
hinge loss. We use this method because it performs
much better than [18] in practice.

4.1 Data Sets
We conduct experiments on two public benchmark data
sets. The ï¬rst one is a subset of the Newsgroups corpus [21],
which consists of 3, 970 documents with TFIDF features of
8, 014 dimensions. Each document belongs to exactly one of
the four categories: autos, motorcycles, baseball and hockey.
The other one is the Reuters data set, which is a subset of
the RCV1-v2 data set [12]. We randomly choose from the
original data set 5, 000 documents with TFIDF features of
6, 451 dimensions. Each document belongs to at least one
of the four categories: CCAT, ECAT, GCAT and MCAT.
Some characteristics of the two data sets are summarized in
Table 1 respectively.

âˆ™ Random Sampling, which randomly selects examples from the candidate set.
We note that all the methods use kernel ridge regression, which is essentially equivalent to least squares SVM
(LS-SVM), as the base classiï¬er. LS-SVM has been reported to give state-of-the-art performance in text classiï¬cation tasks [22, 24]. Since no labeled data exists in the
beginning of each experiment, we use Convex TED to select
the ï¬rst ğ¾ = 5 examples for SED and Margin.

4.3

Table 1: Characteristics of Data Sets
Data Sets
Newsgroups

Reuters

Category
Autos
Motorcycles
Baseball
Hockey
CCAT
ECAT
GCAT
MCAT

# of Documents
988
993
992
997
907
1, 259
1, 524
2, 337

4.3.1

Performance Evaluation
Comparison of Methods on Newsgroups Data

We ï¬rst compare the ï¬ve methods on the Newsgroups
data set. For each method, we restrict the candidate set to
cover 50% of the unlabeled data and set the parameters as
ğœ‡ = 0.01, ğ›¾1 = 0.1ğ›¾max , ğ›¾2 = 1.4
The AUC values averaged over four binary classiï¬cation
tasks are reported in Table 2, where each row corresponds to
one iteration. We use boldface numbers to indicate the best
results among the ï¬ve methods. It is obvious that SED consistently outperforms the other methods. To evaluate how
signiï¬cant SED outperforms other methods, we have conducted paired t-tests [19] on the results of SED and the second best method, Convex TED. The p-value is 2.37 Ã— 10âˆ’5 ,
indicating that SED achieves a signiï¬cantly better result.
It is not surprising that Random Sampling performs the
worst because the randomly selected examples may not provide much useful information to the classiï¬er. We also note
âˆ‘
4
ğ›¾1 â‰¤ ğ›¾max = maxğ‘—âˆˆğ’ ğ‘–âˆˆğ’« (xğ‘‡ğ‘– xğ‘— )2 is a necessary condition for the cardinality constraint âˆ¥ğœ·âˆ¥0 â‰¥ 1. The reader is
referred to [21] for details.

# of Features
8, 014

6, 451

4.2 Experimental Settings and Metrics
In the experiments, we simply treat the multi-class/label
classiï¬cation problem as a set of binary classiï¬cation problems by using the one-versus-all scheme, i.e., documents
from the target category are labeled as positive examples
and those from the other categories are labeled as negative
examples. We use area under the ROC curve (AUC) as the
performance measure to measure the overall classiï¬cation

302

Table 2: Comparison of Methods (in Average AUC) on Newsgroups Data
SED
Convex TED
Sequential TED
Margin
Random Sampling
0.8854Â±0.0256 0.8854Â±0.0256
0.8195Â±0.0299
0.8854Â±0.0256
0.7138Â±0.0295
0.9179Â±0.0133 0.9057Â±0.0134
0.8501Â±0.0244
0.8800Â±0.0274
0.7703Â±0.0354
0.9327Â±0.0115 0.9186Â±0.0120
0.9023Â±0.0120
0.8914Â±0.0270
0.8006Â±0.0234
0.9456Â±0.0067 0.9244Â±0.0103
0.9219Â±0.0122
0.9027Â±0.0163
0.8261Â±0.0231
0.9512Â±0.0061 0.9361Â±0.0076
0.9304Â±0.0105
0.9115Â±0.0103
0.8460Â±0.0169
0.9546Â±0.0042 0.9407Â±0.0058
0.9362Â±0.0095
0.9171Â±0.0093
0.8639Â±0.0159
0.9573Â±0.0055 0.9446Â±0.0049
0.9406Â±0.0081
0.9212Â±0.0090
0.8782Â±0.0164
0.9609Â±0.0041 0.9471Â±0.0054
0.9434Â±0.0073
0.9262Â±0.0098
0.8904Â±0.0138
0.9631Â±0.0051 0.9493Â±0.0051
0.9460Â±0.0062
0.9321Â±0.0081
0.9009Â±0.0132
0.9655Â±0.0043 0.9514Â±0.0046
0.9486Â±0.0068
0.9363Â±0.0074
0.9076Â±0.0126

âˆ£â„’âˆ£
5
10
15
20
25
30
35
40
45
50

4.3.2

that the methods based on experimental design, i.e., Convex TED and Sequential TED, perform better than Margin.
This indicates that exploiting the distribution of unlabeled
data can provide more useful information than selecting only
examples near the class boundary. We also observe that in
the beginning of the learning procedure, examples selected
by Margin actually degrade the performance. This is because the labeled data are scarce at that time and hence the
class boundary learned by training on the labeled data is not
accurate enough and hence may be misleading for document
selection.

We now compare the ï¬ve methods on the Reuters data set.
Each candidate set covers 20% of the unlabeled documents.
The parameters are set as ğœ‡ = 0.01, ğ›¾1 = 0.1ğ›¾max , ğ›¾2 = 10.
The AUC values averaged over the four tasks are reported
in Table 3. Again the best results are shown in bold. As
in the Newsgroups data set, SED signiï¬cantly outperforms
Convex TED (the p-value of paired t-test is 2.26 Ã— 10âˆ’5 ),
validating the eï¬€ectiveness of label information. It is interesting to ï¬nd that Margin performs better than Convex
TED. This can be attributed to two reasons. First, the
data are very balanced in this data set and Margin selects
the most discriminative examples without querying the outliers. Second, the representative examples selected by TED
might not be as helpful as those discriminative ones. However, SED can take advantage of both criteria and always
performs the best, especially in the early stage.

0.98
0.96
0.94
0.92
0.9

0.96

0.88

0.92

SED
Convex TED
Margin

0.86
0.84

5

10

15
20
25
30
35
40
Number of Training Examples

45

0.88
0.84

50

AUC

AUC

Comparison of Methods on Reuters Data

0.8

Figure 1: Learning Curves on Newsgroups Data
0.76

We plot the learning curves of SED, Convex TED and
Margin in Figure 1. As we can see, SED performs better
than its two counterparts by a large margin. This observation validates that considering label information and the
distribution of unlabeled data together can provide more
useful information for active selection than only considering
either of them.
To further understand the properties of SED, we plot the
learning curves of SED, Convex TED and Margin for four
binary classiï¬cation tasks in Figure 2. For three categories,
i.e., autos, baseball and hockey, SED consistently outperforms the second best, Convex TED, by a large margin. For
the motorcycles category, SED and Convex TED perform
similarly. We note that Margin is consistently the worst
with the largest variance for all tasks. We conjecture that
Margin always selects the outliers, which stay close to the
class boundary but are not useful to the learner. On the
other hand, SED and TED can exploit the distribution of
unlabeled data and hence have a smaller chance to select the
outliers.

SED
Convex TED
Margin

0.72
0.68

5

10

15

20
25
30
35
40
Number of Training Examples

45

50

Figure 3: Learning Curves on Reuters Data
The learning curves of SED, Convex TED and Margin
are plotted in Figure 3. From the ï¬gure, SED outperforms
Convex TED and Margin especially in the early stage. This
observation again validates the contribution of label information to experimental design.
We also plot the learning curves of SED, Convex TED
and Margin for the four tasks in Figure 4. SED again outperforms its counterpart, Convex TED, for all tasks. It is
interesting to observe that when the data are rather balanced, such as in MCAT, Margin performs better than Convex TED. This is actually possible, because when the data
are balanced, discriminative examples near the class boundary will provide the most useful information to the learner.
Note that the eï¬€ectiveness of SED can be further improved
if we put more weight on the label information for this task.
Nevertheless, we leave the issue of automatically learning
the weight of label information to our future research.

303

0.98

0.98
0.96

0.96

0.94
0.92

0.94

AUC

AUC

0.9
0.92

0.88
0.86

0.9

0.84
0.82

0.88

0.86

SED
Convex TED
Margin
5

10

15

20
25
30
35
40
Number of Training Examples

45

SED
Convex TED
Margin

0.8
0.78

50

5

10

(a) Autos

15

20
25
30
35
40
Number of Training Examples

45

50

(b) Motorcycles

0.98
0.98

0.96
0.94

0.96

0.92

AUC

AUC

0.9
0.88
0.86

0.94

0.92

0.84
0.82

0.78

0.9

SED
Convex TED
Margin

0.8
5

10

15

20
25
30
35
40
Number of Training Examples

45

0.88

50

(c) Baseball

SED
Convex TED
Margin
5

10

15

20
25
30
35
40
Number of Training Examples

45

50

(d) Hockey

Figure 2: Learning Curves for Four Binary Classiï¬cation Tasks on Newsgroups Data
âˆ£â„’âˆ£
5
10
15
20
25
30
35
40
45
50

Table 3: Comparison of Methods (in Average AUC) on Reuters Data
SED
Convex TED
Sequential TED
Margin
Random Sampling
0.7347Â±0.0416 0.7347Â±0.0416 0.7910Â±0.0250
0.7347Â±0.0401
0.6932Â±0.0669
0.8215Â±0.0201 0.8039Â±0.0267
0.8137Â±0.0232
0.7942Â±0.0264
0.7579Â±0.0434
0.8565Â±0.0216 0.8327Â±0.0190
0.8397Â±0.0135
0.8341Â±0.0203
0.7945Â±0.0246
0.8720Â±0.0117 0.8416Â±0.0154
0.8551Â±0.0117
0.8538Â±0.0215
0.8235Â±0.0227
0.8842Â±0.0066 0.8550Â±0.0126
0.8668Â±0.0131
0.8646Â±0.0233
0.8383Â±0.0190
0.8902Â±0.0078 0.8645Â±0.0106
0.8725Â±0.0137
0.8775Â±0.0168
0.8536Â±0.0226
0.8963Â±0.0069 0.8709Â±0.0091
0.8807Â±0.0139
0.8897Â±0.0114
0.8684Â±0.0162
0.8992Â±0.0088 0.8774Â±0.0085
0.8850Â±0.0118
0.8993Â±0.0129
0.8749Â±0.0130
0.9024Â±0.0085 0.8789Â±0.0080
0.8898Â±0.0126
0.9042Â±0.0117
0.8864Â±0.0108
0.9048Â±0.0087 0.8856Â±0.0068
0.8932Â±0.0120
0.9093Â±0.0101
0.8948Â±0.0096

4.4 Discussions
4.4.1

ğ›¾2 is too large, e.g. ğ›¾2 = 10 or 100, the learning rate will be
slower than that with moderate ğ›¾2 values in the early stage
of learning.
This is because the training set is too small in this stage
and the class boundary learned is not very accurate, so
adopting too large ğ›¾2 values will mislead example selection
by querying the outliers. The risk can be mitigated as the
size of the training set increases. We also note that choosing ğ›¾2 = 1 will achieve the best performance not only in the
early stage but also in the later stage.
Similar experiments are conducted for the MCAT task of
the Reuters data set. As before, the random candidate sets
cover 20% of the unlabeled documents and the parameters
are set to be ğœ‡ = 0.01, ğ›¾1 = 0.1ğ›¾max . The learning curves of
SED with diï¬€erent ğ›¾2 values are plotted in Figure 6.
From Figure 6, it is interesting to observe that, diï¬€erent from what we have found in Figure 5, adopting a larger
value of ğ›¾2 will always improve the active learning procedure. This is because in the MCAT task, about 50% of the
documents are positive, but in the autos task, only 25% of

Effectiveness of Label Information

As we have discussed in Section 3, the contribution of label information is controlled by the parameter ğ›¾2 . If ğ›¾2 = 0,
we do not use the label information at all and hence our
method degenerates to Convex TED; as ğ›¾2 increases, we
put larger weight on the label information. To evaluate the
contribution of label information, we carry out a set of experiments by varying the value of ğ›¾2 in the autos task of the
Newsgroups data set. As before, each candidate set covers
50% of the unlabeled documents and the parameters are set
to be ğœ‡ = 0.01, ğ›¾1 = 0.1ğ›¾max .
The learning curves of SED with diï¬€erent ğ›¾2 values are
plotted in Figure 5. As we can see, using a large enough
ğ›¾2 value, e.g. ğ›¾2 = 1, can greatly speed up the learning
procedure, while using small values, e.g. ğ›¾2 = [0, 0.1], will
not improve much.
This observation validates the eï¬€ectiveness of label information for experimental design. It should be noted that if

304

0.95

0.9

0.9

0.85

0.85

0.8

0.8

AUC

AUC

0.95

0.75

0.75

0.7

0.7
SED
Convex TED
Margin

0.65
5

10

15

20
25
30
35
40
Number of Training Examples

45

SED
Convex TED
Margin

0.65

50

5

10

15

(a) CCAT

20
25
30
35
40
Number of Training Examples

45

50

(b) ECAT
0.9

0.95
0.85
0.9

AUC

AUC

0.8
0.85

0.8

0.75

0.7
SED
Convex TED
Margin

0.75

5

10

15

20
25
30
35
40
Number of Training Examples

45

SED
Convex TED
Margin

0.65

50

5

10

15

(c) GCAT

20
25
30
35
40
Number of Training Examples

45

50

(d) MCAT

Figure 4: Learning Curves for Four Binary Classiï¬cation Tasks on Reuters Data
0.95

0.98
0.97

0.9

0.96

0.85

0.95

AUC

AUC

0.94
0.93

0.8
0.75

0.92

0.9
0.89
0.88

5

10

15
20
25
30
35
40
Number of Training Examples

45

0.65

5

50

10

15
20
25
30
35
40
Number of Training Examples

45

50

Figure 6: Eï¬€ect of Varying ğ›¾2 on MCAT Task

Figure 5: Eï¬€ect of Varying ğ›¾2 on Autos Task

ever, in Figure 8, the learning curves are not so sensitive to
the candidate set size as those in Figure 7. This can again
be explained by the distribution of data. The more balanced
the data are, the less sensitive is the method to the candidate set size. We should note that the candidate set size
has great impact on the optimization problem. Speciï¬cally,
the larger the candidate set is, the longer time we need to
solve the problem. Thus, in practice, we should maintain a
tradeoï¬€ between performance and the time cost and use a
candidate set of a reasonable size.

the documents are positive. Since the data distribution is
more balanced in the MCAT task, adopting a larger value of
ğ›¾2 will always choose those discriminative examples without
taking the risk of querying the outliers.

4.4.2

0
0.01
0.1
1
10
100

0.7

0
0.01
0.1
1
10
100

0.91

Effect of Candidate Set Size

In this section, we conduct several experiments to investigate the eï¬€ect of the candidate set size by randomly choosing 20%, 40%, 60% and 80% of the unlabeled documents to
form diï¬€erent candidate sets. For the autos task, the parameters are set to be ğœ‡ = 0.01, ğ›¾1 = 0.1ğ›¾max , ğ›¾2 = 1, and the
learning curves for diï¬€erent candidate set sizes are plotted
in Figure 7. For the MCAT task, the parameters are set to
be ğœ‡ = 0.01, ğ›¾1 = 0.1ğ›¾max , ğ›¾2 = 10, and the learning curves
are plotted in Figure 8.
As we can see from Figure 7, using a larger candidate set
will greatly speed up the learning procedure. We note that
as the size of the candidate set increases, the performance
gap between the learned classiï¬ers becomes smaller. How-

5. CONCLUSION
In this paper, we have proposed a novel active learning
method, SED, to seamlessly incorporate label information
into the document selection procedure of experimental design. To the best of our knowledge, SED is the ï¬rst work
that uses label information to improve experimental design.
One promising property of SED is that it can eï¬€ectively use
label information and the distribution of unlabeled data in a

305

[4] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active
learning with statistical models. J. Artif. Intell. Res.,
4:129â€“145, 1995.
[5] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An
eï¬ƒcient boosting algorithm for combining preferences.
J. Mach. Learn. Res., 4:933â€“969, 2003.
[6] Y. Guo and R. Greiner. Optimistic active learning
using mutual information. In IJCAI, 2007.
[7] Y. Guo and D. Schuurmans. Discriminative batch
mode active learning. In NIPS, 2007.
[8] X. He, W. Min, D. Cai, and K. Zhou. Laplacian
optimal design for image retrieval. In SIGIR, 2007.
[9] S. C. Hoi, R. Jin, and M. R. Lyu. Large-scale text
categorization by batch mode active learning. In
WWW, 2006.
[10] S. C. Hoi, R. Jin, J. Zhu, and M. R. Lyu. Batch mode
active learning and its application to medical image
classiï¬cation. In ICML, 2006.
[11] D. D. Lewis and W. A. Gale. A sequential algorithm
for training text classiï¬ers. In SIGIR, 1994.
[12] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. RCV1:
A new benchmark collection for text categorization
research. J. Mach. Learn. Res., 5:361â€“397, 2004.
[13] D. MacKay. Information-based objective functions for
active data selection. Neural Comput., 4(4):590â€“604,
1992.
[14] A. McCallum and K. Nigam. Employing EM and
pool-based active learning for text classiï¬cation. In
ICML, 1998.
[15] H. T. Nguyen and A. Smeulders. Active learning using
pre-clustering. In ICML, 2004.
[16] N. Roy and A. McCallum. Toward optimal active
learning through sampling estimation of error
reduction. In ICML, 2001.
[17] H. S. Seung, M. Opper, and H. Sompolinsky. Query by
committee. In COLT, 1992.
[18] S. Tong and D. Koller. Support vector machine active
learning with applications to text classiï¬cation. J.
Mach. Learn. Res., 2:45â€“66, 2002.
[19] Y. Yang and X. Liu. A re-examination of text
categorization methods. In SIGIR, 1999.
[20] K. Yu, J. Bi, and V. Tresp. Active learning via
transductive experimental design. In ICML, 2006.
[21] K. Yu, S. Zhu, W. Xu, and Y. Gong. Non-greedy
active learning for text categorization using convex
transductive experimental design. In SIGIR, 2008.
[22] J. Zhang and Y. Yang. Robustness of regularized
linear classiï¬cation methods in text categorization. In
SIGIR, 2003.
[23] L. Zhang, C. Chen, W. Chen, J. Bu, D. Cai, and
X. He. Convex experimental design using manifold
structure for image retrieval. In ACM MM, 2009.
[24] T. Zhang and F. J. Oles. Text categorization based on
regularized linear classiï¬cation methods. Inform.
Retrieval, 4(1):5â€“31, 2001.
[25] X. Zhu, J. Laï¬€erty, and Z. Ghahramani. Combining
active learning and semi-supervised learning using
gaussian ï¬elds and harmonic functions. In ICML
Workshop, 2003.

0.98

0.96

AUC

0.94

0.92

0.9
20%
40%
60%
80%

0.88

0.86

5

10

15
20
25
30
35
40
Number of Training Examples

45

50

Figure 7: Eï¬€ect of Candidate Set Size on Autos Task
0.95
0.9

AUC

0.85
0.8
0.75
0.7
20%
40%
60%
80%

0.65

5

10

15
20
25
30
35
40
Number of Training Examples

45

50

Figure 8: Eï¬€ect of Candidate Set Size on MCAT
Task
uniï¬ed framework. In particular, SED can greatly speed up
the learning procedure when the distribution of unlabeled
data is balanced, while existing methods based on experimental design always perform badly in this case. Moreover,
SED can greatly outperform margin-based active learning
when the distribution of unlabeled data is unbalanced. As
another promising property, SED is convex and thus global
optimality is guaranteed. Experiments conducted on two
text corpora demonstrate that SED outperforms state-ofthe-art active learning algorithms, such as TED and marginbased methods, which take into consideration only partial
information.
One of our future research directions is to automatically
learn from data the contribution of label information, i.e.
ğ›¾2 . Another possible research direction is to apply SED to
other information retrieval applications.

6.

ACKNOWLEDGMENTS

The authors thank Wu-Jun Li and Ning Zhu for some
helpful discussions. This research has been supported by
General Research Fund 622209 from the Research Grants
Council of Hong Kong.

7.

REFERENCES

[1] D. Angluin. Queries and concept learning. Mach.
Learn., 2(4):319â€“342, 1988.
[2] A. Atkinson and A. Donev. Optimum Experimental
Designs. Oxford University Press, USA, 1992.
[3] D. Cohn, L. Atlas, and R. Ladner. Improving
generalization with active learning. Mach. Learn.,
15(2):201â€“221, 1994.

306

