SED: Supervised Experimental Design and Its Application
to Text Classification
Yi Zhen and Dit-Yan Yeung
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
Hong Kong, China

{yzhen, dyyeung}@cse.ust.hk
ABSTRACT

number of unlabeled documents, such as web pages, newspapers and journal articles. In recent years, a new approach
called active learning [1, 3, 5, 6, 9, 11, 13, 14, 15, 16, 18, 20,
25] has been developed in the machine learning community
with the goal of reducing the labeling cost by identifying
and presenting the most informative examples from the unlabeled examples for the human experts to label.
Although a lot of work has been done in active learning research, most of the existing active learning methods are still
far from satisfactory with apparent shortcomings. In particular, many methods only take into consideration partial
information to determine the informativeness of examples.
Some methods consider information conveyed by the class
boundaries, some consider information conveyed by the distribution of unlabeled data, and some consider the disagreement between learners when multiple learners are involved.
Unfortunately, none of these methods is consistently better
than others in all situations. Another drawback is that most
active learning algorithms select only one example at a time
for labeling. Compared with a batch approach [7, 9, 10]
which selects multiple examples in one iteration, this greedy
incremental approach is at best suboptimal and is not suitable for large-scale and parallel computing applications.
Experimental design [2, 20, 21], which is one of the stateof-the-art active learning approaches for text classiﬁcation,
can eﬀectively exploit the distribution of unlabeled data
while supporting batch selection at the same time. Despite
their appealing properties, existing methods based on experimental design cannot make use of label information even
when labeled data are available. Thus, these methods are
intrinsically unsupervised in nature.
In this paper, we propose a novel batch mode active learning algorithm, called supervised experimental design (SED),
which incorporates label information into the experimental
design procedure. SED is a supervised extension of experimental design with a new regularization term that incorporates label information added to the objective function.
To the best of our knowledge, no work has been done so far
to utilize label information in the experimental design procedure. Some favorable properties of SED are highlighted
here:

In recent years, active learning methods based on experimental design achieve state-of-the-art performance in text
classiﬁcation applications. Although these methods can exploit the distribution of unlabeled data and support batch
selection, they cannot make use of labeled data which often
carry useful information for active learning. In this paper,
we propose a novel active learning method for text classiﬁcation, called supervised experimental design (SED), which
seamlessly incorporates label information into experimental
design. Experimental results show that SED outperforms
its counterparts which either discard the label information
even when it is available or fail to exploit the distribution of
unlabeled data.

Categories and Subject Descriptors
G.3 [Mathematics of Computing]: Probability and Statistics—Experimental Design; H.3 [Information Storage and
Retrieval]: Information Search and Retrieval—Clustering

General Terms
Algorithms, Theory

Keywords
Active Learning, Supervised Experimental Design, Text Classiﬁcation, Convex Optimization

1.

INTRODUCTION

There has been a long tradition of research on text classiﬁcation in both the information retrieval and machine learning communities. In order to learn a good text classiﬁer, a
large number of labeled documents are often needed for classiﬁer training. However, labeling documents always needs
domain knowledge and thus is diﬃcult, time consuming and
costly. On the other hand, it is much easier to obtain a large

∙ To the best of our knowledge, SED is the ﬁrst work
that incorporates label information into experimental
design.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

∙ SED outperforms (unsupervised) experimental design,
which discards the label information even when it is
available. This shows that label information does pro-

299

Given a set of labeled data {(x𝑖 , 𝑦𝑖 )}𝑀
𝑖=1 , the maximum
likelihood estimate (MLE) of the model parameter vector w
can be obtained by minimizing the residual sum of squares:

vide useful information for active document selection
and SED can utilize the information very eﬀectively.
∙ SED outperforms margin-based active learning under
highly unbalanced data distributions which are often
encountered in practice.

𝑀
{
}
∑
ŵ = argmin 𝐽(w; X, y) =
(w𝑇 x𝑖 − 𝑦𝑖 )2
w
𝑇

= (X X)

∙ SED is convex and thus global optimality can be guaranteed.

𝑇

X y,

(1)

𝑇

where X = [x1 , . . . , x𝑀 ] is a matrix of the labeled data and
y is a vector of the corresponding target outputs.2
If we put a spherical Gaussian prior on the noise 𝜖, i.e.,
𝜖 ∼ 𝒩 (0, 𝜎 2 ), it can be proved easily that ŵ is an unbiased
estimate of w with covariance:
[
]
cov[ŵ] = cov (X𝑇 X)−1 X𝑇 y

The remainder of this paper is organized as follows. In
Section 2, we will introduce the notations and some related
work. In Section 3, we ﬁrst introduce transductive experimental design and then present our SED model and algorithm in detail. Extensive empirical studies conducted on
two real-world text corpora are presented in Section 4. Section 5 concludes our paper.

2.

𝑖=1

−1

= (X𝑇 X)−1 X𝑇 cov[y] X(X𝑇 X)−1
= 𝜎 2 (X𝑇 X)−1 .

Traditional experimental design aims at minimizing the
covariance of ŵ, which characterizes the model uncertainty
in some sense. Three criteria have been commonly used in
the literature:

NOTATIONS AND RELATED WORK

Throughout this paper, we use boldface uppercase letters
(e.g. X) to denote matrices and boldface lowercase letters
(e.g. x) to denote vectors. We use tr(X) to denote the trace
of X and X𝑇 to denote its transpose. Moreover, we use
calligraphic letters (e.g. 𝒜) to denote sets and ∣𝒜∣ to denote
the size of 𝒜.
Given the whole data set represented as X𝒫 ∈ ℝ𝑀 ×𝐷 or
𝒫 = {x1 , . . . , x𝑀 }, in which each data point x𝑖 is a 𝐷 ×
1 vector, a generic active learning problem [4, 11] can be
deﬁned as selecting a subset of unlabeled data points from
a candidate set X𝒞 ∈ ℝ𝑁 ×𝐷 or 𝒞 = {x1 , . . . , x𝑁 }, such
that if the selected data points are labeled and added to the
training set for re-training the classiﬁer, the improvement
of the classiﬁer will be maximized. We call the subset of
selected data the active set and denote it as X𝒜 ∈ ℝ𝐾×𝐷 or
𝒜 = {x1 , . . . , x𝐾 }.1
The promise of active learning is appealing because it
can help to alleviate the labeled data deﬁciency problem
commonly encountered in many supervised learning applications. Existing active learning algorithms for text classiﬁcation either select the most uncertain data given the current
classiﬁer [11], select the data with the smallest margin [18],
select the data on which multiple classiﬁers disagree most
with each other [5, 14, 17], or select the data that optimize
some information gain [6, 13, 16, 25].
Closely related to active learning is experimental design
in statistics [2]. Conventionally, experimental design considers the problem of learning a predictive function 𝑓 (x) from
experiment-measurement pairs (x𝑖 , 𝑦𝑖 ). Given that conducting an experiment is expensive, experimental design seeks
to select the most informative experiments to conduct such
that the number of measurements needed can be reduced.
Traditional experimental design considers the following
linear regression model:

∙ D-optimal design: minimizing the determinant of cov[ŵ];
∙ A-optimal design: minimizing the trace of cov[ŵ];
∙ E-optimal design: minimizing the maximum eigenvalue
of cov[ŵ].
Recently, Yu et al. [20] proposed a method, called transductive experimental design (TED), which selects the most
informative examples by reducing the model uncertainty on
all of the unlabeled data and thus eﬀectively exploits the
distribution of the unlabeled data. He et al. [8] applied similar ideas to content-based image retrieval (CBIR), where a
Laplacian regularization term is added and then the model
uncertainty, represented by a new covariance matrix, considers the smoothness among data points.
Despite the appealing properties which include clear mathematical formulation and the ability of batch selection, algorithms based on experimental design often have to deal with
combinatorial complexity and are NP-hard. Since the optimization problems involved are non-convex, the solutions
obtained may correspond to poor local minima. To address
this problem, some approximation methods based on convex
relaxation have been developed [21, 23].

3. SUPERVISED EXPERIMENTAL DESIGN
Existing active learning methods based on experimental
design, such as TED, are formulated under the setting that
all available data are unlabeled. As such, they cannot make
use of the label information even when it is available.
Since label information has been found very useful to example (or document) selection [5, 11, 14, 16, 18], incorporating label information into the example selection procedure
of experimental design is a very worthwhile direction to explore.
In this section, we ﬁrst brieﬂy review TED in Section 3.1
and then present our method, SED, in Section 3.2. The
algorithm for SED will be summarized in Section 3.3 and its
complexity analysis will be presented in Section 3.4.

𝑦 = w𝑇 x + 𝜖,
where 𝑦 is the measurement, x is the 𝐷 × 1 feature vector
of the experiment, w is the 𝐷 × 1 model parameter vector
and 𝜖 is the noise term.
1
The reader should note that points in diﬀerent sets with
the same index are not necessarily the same point, although
we require that the points in 𝒞 should appear in 𝒫 and the
points in 𝒜 should appear in 𝒞.

2
In the sequel, we will also refer to them as labels even
though the term ‘label’ is more appropriately used for classiﬁcation problems.

300

3.1 Transductive Experimental Design

where f̃𝒜 is similar to f̃ but is deﬁned only on the active
set X𝒜 and 𝛾 is a user-deﬁned parameter controlling the
contribution of model uncertainty due to the current labeled
data. In other words, 𝛾 controls the contribution of label
information.
Since the optimization problem in Equation (4) is NPhard, non-convex and can easily get trapped in local minima,
we borrow ideas from [21] to reformulate it in a convex form
and deﬁne our SED problem as follows.

TED [20] seeks to choose X𝒜 from X𝒞 such that a function 𝑓 learned from X𝒜 has the smallest predictive variance
on X𝒫 . The goal can be achieved by solving the following
optimization problem:
[
)−1 𝑇 ]
(
min tr X𝒫 X𝑇𝒜 X𝒜 + 𝜇I
X𝒫
X𝒜

s.t.

X𝒜 ⊂ X𝒞 , ∣𝒜∣ = 𝐾,

(2)

where I is an identity matrix whose dimensionality is determined by the problem and 𝐾 is the number of examples
selected. The objective function may also be considered as
model uncertainty over X𝒫 . We note that it only depends on
the input features of the training examples and thus is independent of the labels. This is because in the error function
𝐽(w; X, y) of the linear regression model in Equation (1),
the model parameter vector w is only coupled with the labels 𝑦𝑖 linearly and hence a second derivative with respect
to w makes all the 𝑦𝑖 terms disappear.
Since the TED optimization problem is non-convex and
can easily get stuck in local optima, Yu et al. [21] proposed a
convex relaxation of TED (Convex TED). The optimization
problem of Convex TED is deﬁned as follows:
(
)
∑𝑀
∑𝑁 𝛼2𝑖𝑗
𝑇
2
min
+ 𝛾∣∣𝜷∣∣1
𝑖=1 ∣∣x𝑖 − X𝒞 𝜶𝑖 ∣∣ +
𝑗=1 𝛽𝑗
𝜷 ,𝜶𝑖
x𝑖 ∈ X𝒫 , 𝜶𝑖 ∈ ℝ𝑁 , 𝜷 ∈ ℝ𝑁 ×1 , 𝜷 ≥ 0,

s.t.

Deﬁnition 1. Supervised Experimental Design (SED)
(
)
∑𝑁 𝛼2𝑖𝑗
∑𝑀
𝑇
2
+ 𝛾1 ∣∣𝜷∣∣1 + 𝛾2 𝜷 𝑇 f̃
−
X
𝜶
∣∣
+
min
∣∣x
𝑖
𝒞 𝑖
𝑖=1
𝑗=1 𝛽𝑗
𝜷 ,𝜶𝑖

Theorem 1. SED is convex w.r.t. 𝜷 and {𝜶𝑖 }.
Proof. Let the objective
function of SED be 𝐹) = 𝐹1 +
(
∑𝑀
∑
𝛼2
𝑖𝑗
𝐹2 , where 𝐹1 = 𝑖=1 ∣∣x𝑖 − X𝑇𝒞 𝜶𝑖 ∣∣2 + 𝑁
+𝛾1 ∣∣𝜷∣∣1
𝑗=1 𝛽𝑗
and 𝐹2 = 𝛾2 𝜷 𝑇 f̃ . Because f̃ is constant, 𝐹2 is linear in 𝜷.
Thus 𝐹2 is convex with respect to 𝜷. Since 𝐹1 is also convex
with respect to 𝜷 and {𝜶𝑖 }3 and 𝐹1 + 𝐹2 is a convex combination of two convex functions 𝐹1 and 𝐹2 , 𝐹 is thus convex
with respect to 𝜷 and {𝜶𝑖 }. This completes the proof.

(3)

3.3

Algorithm

It is convenient to ﬁnd the local optimum of Problem (5),
which is also the global optimum, by updating 𝜷 and {𝜶𝑖 }
iteratively. More speciﬁcally, we can ﬁnd the analytical solution for updating one variable while ﬁxing the other as
follows:
v
u
u
𝛽𝑗 = ⎷

𝑀
∑
1
2
𝛼𝑖𝑗
, 𝑗 = 1, . . . , 𝑁,
𝛾1 + 𝛾2 𝑓˜𝑗 𝑖=1

𝜶𝑖 = (diag(𝜷)−1 + X𝒞 X𝑇𝒞 )−1 X𝒞 x𝑖 , 𝑖 = 1, . . . , 𝑀.

(6)
(7)

The proposed algorithm is summarized in Algorithm 1.

3.2 Supervised Experimental Design
Given a set of labeled data points (training set), we can
learn a classiﬁer 𝑓 from the data. In a typical active learning setting in which labeled data are scarce, 𝑓 may not be
accurate enough and hence it is desirable to select some unlabeled data points for labeling to enlarge the training set.
However, although 𝑓 is not accurate enough, it still carries
some useful information about the data points. Let f be a
vector of decision values on the candidate set X𝒞 and f̃ be
the vector after taking the absolute value of each element
of f . For example, in support vector machine (SVM), f̃ indicates the uncertainty of the current classiﬁer about the
labels of the examples. The smaller the 𝑗th element 𝑓˜𝑗 of f̃
is, the less certain is the classiﬁer about the example. Intuitively speaking, the most informative examples should be
those with the smallest 𝑓˜𝑗 values.
Based on the above intuition and the formulation of TED,
we can choose the most informative X𝒜 by solving the following optimization problem,
[
(
)−1 𝑇 ]
min tr X𝒫 X𝑇𝒜 X𝒜 + 𝜇I
X𝒫 + 𝛾 f̃𝒜

3.4

Complexity Analysis

The main computation of SED is to update 𝜷 and {𝜶𝑖 }.
The time complexity of updating 𝜷 (Equation (6)) is 𝑂(𝑀 𝑁 )
and that of updating {𝜶𝑖 } (Equation (7)) is 𝑂(𝑁 3 ). Hence,
the time complexity of one iteration is 𝑂(𝑁 3 +𝑀 𝑁 ). Though
our algorithm converges very quickly in practice, it is interesting and worthwhile to explore more eﬃcient techniques
to solve the problem, and we leave it as future work.

4. EMPIRICAL ANALYSIS
We conduct several experiments to compare SED with
some other related methods. We have the following questions in mind while designing and conducting the experiments:
1. How does SED perform when compared with other
state-of-the-art active learning methods?
2. How eﬀective is label information for experimental design?

X𝒜

X𝒜 ⊂ X𝒞 , ∣𝒜∣ = 𝐾,

(5)

We can further prove that SED is a convex problem.

where the variables 𝛽𝑗 , 𝑗 = 1, . . . , 𝑁 , control the inclusion of
examples in X𝒞 into the training set X𝒜 , the ℓ1 -norm ∣∣𝜷∣∣1
enforces the sparsity of 𝜷, and 𝛼𝑖𝑗 denotes the 𝑗th element
of 𝜶𝑖 . According to [20, 21], TED and Convex TED tend to
select examples representative of all the unlabeled data and
hence exploit the distribution of the whole data space.
Since experimental design based methods do not use label information, we call them unsupervised active learning
methods here. In the next subsection, we will present our
supervised extension, SED, which can eﬀectively utilize the
available label information to select the most informative
examples.

s.t.

x𝑖 ∈ X𝒫 , 𝜶𝑖 ∈ ℝ𝑁 , 𝜷 ∈ ℝ𝑁 ×1 , 𝜷 ≥ 0.

s.t.

3

(4)

301

The proof can be found in [21].

Algorithm 1 Algorithm for SED
1: INPUT:
ℒ0 – set of labeled data points
𝒰 0 – set of unlabeled data points
𝑇 – number of active learning iterations
𝐾 – number of examples selected in each iteration
2: for 𝑡 = 1 to 𝑇 do
3:
Train classiﬁer 𝑓 based on ℒ𝑡−1 .
4:
Compute absolute decision values f̃ .
5:
Initialize {𝜶𝑖 }.
6:
repeat
7:
Fix {𝜶𝑖 }, update 𝜷 using Equation (6).
8:
Fix 𝜷, update {𝜶𝑖 } using Equation (7).
9:
until converge w.r.t. objective value of Problem (5).
10:
Choose 𝐾 examples with the largest 𝜷 values into
and request their labels.
X𝑡−1
𝒜
11:
Update ℒ𝑡 ← ℒ𝑡−1 ∪ X𝑡−1
and 𝒰 𝑡 ← 𝒰 𝑡−1 ∖ X𝑡−1
𝒜
𝒜 .
12: end for
13: Train classiﬁer 𝑓 based on ℒ𝑇 .
14: return 𝑓

performance, because in our setting, each binary classiﬁcation task is unbalanced (only about 25% of the documents in
each Newsgroups data set and about 30% of the documents
in each Reuters data set are positive). Note that a larger
value of AUC indicates a better performance.
At each iteration of our experiments, an active learning
method selects a set of 𝐾 = 5 unlabeled examples from the
candidate set. The selected examples are then labeled and
added to the training set ℒ. The classiﬁer is then trained
on the expanded training set and used to predict the class
labels of all documents. An AUC score is then computed
based on the predictions. In order to randomize the experiments as well as to reduce the computational cost, we
restrict the candidate set to randomly cover only a fraction
of all the unlabeled documents. Ten diﬀerent candidate sets
are generated for each experiment and the average AUC
value, together with the standard deviation, is reported.
We compare SED with four popular active learning methods for text classiﬁcation:

3. How does varying the size of the candidate set aﬀect
the performance of SED?

∙ Sequential TED [20], which sequentially selects examples using TED.

∙ Convex TED [21], which is a convex relaxation of
TED.

These questions are answered in separate subsections: question 1 in Section 4.3, question 2 in Section 4.4.1, and question 3 in Section 4.4.2.

∙ Margin, which chooses the examples closest to the
class boundary. This method implements the basic
idea of [18] but uses the squared loss instead of the
hinge loss. We use this method because it performs
much better than [18] in practice.

4.1 Data Sets
We conduct experiments on two public benchmark data
sets. The ﬁrst one is a subset of the Newsgroups corpus [21],
which consists of 3, 970 documents with TFIDF features of
8, 014 dimensions. Each document belongs to exactly one of
the four categories: autos, motorcycles, baseball and hockey.
The other one is the Reuters data set, which is a subset of
the RCV1-v2 data set [12]. We randomly choose from the
original data set 5, 000 documents with TFIDF features of
6, 451 dimensions. Each document belongs to at least one
of the four categories: CCAT, ECAT, GCAT and MCAT.
Some characteristics of the two data sets are summarized in
Table 1 respectively.

∙ Random Sampling, which randomly selects examples from the candidate set.
We note that all the methods use kernel ridge regression, which is essentially equivalent to least squares SVM
(LS-SVM), as the base classiﬁer. LS-SVM has been reported to give state-of-the-art performance in text classiﬁcation tasks [22, 24]. Since no labeled data exists in the
beginning of each experiment, we use Convex TED to select
the ﬁrst 𝐾 = 5 examples for SED and Margin.

4.3

Table 1: Characteristics of Data Sets
Data Sets
Newsgroups

Reuters

Category
Autos
Motorcycles
Baseball
Hockey
CCAT
ECAT
GCAT
MCAT

# of Documents
988
993
992
997
907
1, 259
1, 524
2, 337

4.3.1

Performance Evaluation
Comparison of Methods on Newsgroups Data

We ﬁrst compare the ﬁve methods on the Newsgroups
data set. For each method, we restrict the candidate set to
cover 50% of the unlabeled data and set the parameters as
𝜇 = 0.01, 𝛾1 = 0.1𝛾max , 𝛾2 = 1.4
The AUC values averaged over four binary classiﬁcation
tasks are reported in Table 2, where each row corresponds to
one iteration. We use boldface numbers to indicate the best
results among the ﬁve methods. It is obvious that SED consistently outperforms the other methods. To evaluate how
signiﬁcant SED outperforms other methods, we have conducted paired t-tests [19] on the results of SED and the second best method, Convex TED. The p-value is 2.37 × 10−5 ,
indicating that SED achieves a signiﬁcantly better result.
It is not surprising that Random Sampling performs the
worst because the randomly selected examples may not provide much useful information to the classiﬁer. We also note
∑
4
𝛾1 ≤ 𝛾max = max𝑗∈𝒞 𝑖∈𝒫 (x𝑇𝑖 x𝑗 )2 is a necessary condition for the cardinality constraint ∥𝜷∥0 ≥ 1. The reader is
referred to [21] for details.

# of Features
8, 014

6, 451

4.2 Experimental Settings and Metrics
In the experiments, we simply treat the multi-class/label
classiﬁcation problem as a set of binary classiﬁcation problems by using the one-versus-all scheme, i.e., documents
from the target category are labeled as positive examples
and those from the other categories are labeled as negative
examples. We use area under the ROC curve (AUC) as the
performance measure to measure the overall classiﬁcation

302

Table 2: Comparison of Methods (in Average AUC) on Newsgroups Data
SED
Convex TED
Sequential TED
Margin
Random Sampling
0.8854±0.0256 0.8854±0.0256
0.8195±0.0299
0.8854±0.0256
0.7138±0.0295
0.9179±0.0133 0.9057±0.0134
0.8501±0.0244
0.8800±0.0274
0.7703±0.0354
0.9327±0.0115 0.9186±0.0120
0.9023±0.0120
0.8914±0.0270
0.8006±0.0234
0.9456±0.0067 0.9244±0.0103
0.9219±0.0122
0.9027±0.0163
0.8261±0.0231
0.9512±0.0061 0.9361±0.0076
0.9304±0.0105
0.9115±0.0103
0.8460±0.0169
0.9546±0.0042 0.9407±0.0058
0.9362±0.0095
0.9171±0.0093
0.8639±0.0159
0.9573±0.0055 0.9446±0.0049
0.9406±0.0081
0.9212±0.0090
0.8782±0.0164
0.9609±0.0041 0.9471±0.0054
0.9434±0.0073
0.9262±0.0098
0.8904±0.0138
0.9631±0.0051 0.9493±0.0051
0.9460±0.0062
0.9321±0.0081
0.9009±0.0132
0.9655±0.0043 0.9514±0.0046
0.9486±0.0068
0.9363±0.0074
0.9076±0.0126

∣ℒ∣
5
10
15
20
25
30
35
40
45
50

4.3.2

that the methods based on experimental design, i.e., Convex TED and Sequential TED, perform better than Margin.
This indicates that exploiting the distribution of unlabeled
data can provide more useful information than selecting only
examples near the class boundary. We also observe that in
the beginning of the learning procedure, examples selected
by Margin actually degrade the performance. This is because the labeled data are scarce at that time and hence the
class boundary learned by training on the labeled data is not
accurate enough and hence may be misleading for document
selection.

We now compare the ﬁve methods on the Reuters data set.
Each candidate set covers 20% of the unlabeled documents.
The parameters are set as 𝜇 = 0.01, 𝛾1 = 0.1𝛾max , 𝛾2 = 10.
The AUC values averaged over the four tasks are reported
in Table 3. Again the best results are shown in bold. As
in the Newsgroups data set, SED signiﬁcantly outperforms
Convex TED (the p-value of paired t-test is 2.26 × 10−5 ),
validating the eﬀectiveness of label information. It is interesting to ﬁnd that Margin performs better than Convex
TED. This can be attributed to two reasons. First, the
data are very balanced in this data set and Margin selects
the most discriminative examples without querying the outliers. Second, the representative examples selected by TED
might not be as helpful as those discriminative ones. However, SED can take advantage of both criteria and always
performs the best, especially in the early stage.

0.98
0.96
0.94
0.92
0.9

0.96

0.88

0.92

SED
Convex TED
Margin

0.86
0.84

5

10

15
20
25
30
35
40
Number of Training Examples

45

0.88
0.84

50

AUC

AUC

Comparison of Methods on Reuters Data

0.8

Figure 1: Learning Curves on Newsgroups Data
0.76

We plot the learning curves of SED, Convex TED and
Margin in Figure 1. As we can see, SED performs better
than its two counterparts by a large margin. This observation validates that considering label information and the
distribution of unlabeled data together can provide more
useful information for active selection than only considering
either of them.
To further understand the properties of SED, we plot the
learning curves of SED, Convex TED and Margin for four
binary classiﬁcation tasks in Figure 2. For three categories,
i.e., autos, baseball and hockey, SED consistently outperforms the second best, Convex TED, by a large margin. For
the motorcycles category, SED and Convex TED perform
similarly. We note that Margin is consistently the worst
with the largest variance for all tasks. We conjecture that
Margin always selects the outliers, which stay close to the
class boundary but are not useful to the learner. On the
other hand, SED and TED can exploit the distribution of
unlabeled data and hence have a smaller chance to select the
outliers.

SED
Convex TED
Margin

0.72
0.68

5

10

15

20
25
30
35
40
Number of Training Examples

45

50

Figure 3: Learning Curves on Reuters Data
The learning curves of SED, Convex TED and Margin
are plotted in Figure 3. From the ﬁgure, SED outperforms
Convex TED and Margin especially in the early stage. This
observation again validates the contribution of label information to experimental design.
We also plot the learning curves of SED, Convex TED
and Margin for the four tasks in Figure 4. SED again outperforms its counterpart, Convex TED, for all tasks. It is
interesting to observe that when the data are rather balanced, such as in MCAT, Margin performs better than Convex TED. This is actually possible, because when the data
are balanced, discriminative examples near the class boundary will provide the most useful information to the learner.
Note that the eﬀectiveness of SED can be further improved
if we put more weight on the label information for this task.
Nevertheless, we leave the issue of automatically learning
the weight of label information to our future research.

303

0.98

0.98
0.96

0.96

0.94
0.92

0.94

AUC

AUC

0.9
0.92

0.88
0.86

0.9

0.84
0.82

0.88

0.86

SED
Convex TED
Margin
5

10

15

20
25
30
35
40
Number of Training Examples

45

SED
Convex TED
Margin

0.8
0.78

50

5

10

(a) Autos

15

20
25
30
35
40
Number of Training Examples

45

50

(b) Motorcycles

0.98
0.98

0.96
0.94

0.96

0.92

AUC

AUC

0.9
0.88
0.86

0.94

0.92

0.84
0.82

0.78

0.9

SED
Convex TED
Margin

0.8
5

10

15

20
25
30
35
40
Number of Training Examples

45

0.88

50

(c) Baseball

SED
Convex TED
Margin
5

10

15

20
25
30
35
40
Number of Training Examples

45

50

(d) Hockey

Figure 2: Learning Curves for Four Binary Classiﬁcation Tasks on Newsgroups Data
∣ℒ∣
5
10
15
20
25
30
35
40
45
50

Table 3: Comparison of Methods (in Average AUC) on Reuters Data
SED
Convex TED
Sequential TED
Margin
Random Sampling
0.7347±0.0416 0.7347±0.0416 0.7910±0.0250
0.7347±0.0401
0.6932±0.0669
0.8215±0.0201 0.8039±0.0267
0.8137±0.0232
0.7942±0.0264
0.7579±0.0434
0.8565±0.0216 0.8327±0.0190
0.8397±0.0135
0.8341±0.0203
0.7945±0.0246
0.8720±0.0117 0.8416±0.0154
0.8551±0.0117
0.8538±0.0215
0.8235±0.0227
0.8842±0.0066 0.8550±0.0126
0.8668±0.0131
0.8646±0.0233
0.8383±0.0190
0.8902±0.0078 0.8645±0.0106
0.8725±0.0137
0.8775±0.0168
0.8536±0.0226
0.8963±0.0069 0.8709±0.0091
0.8807±0.0139
0.8897±0.0114
0.8684±0.0162
0.8992±0.0088 0.8774±0.0085
0.8850±0.0118
0.8993±0.0129
0.8749±0.0130
0.9024±0.0085 0.8789±0.0080
0.8898±0.0126
0.9042±0.0117
0.8864±0.0108
0.9048±0.0087 0.8856±0.0068
0.8932±0.0120
0.9093±0.0101
0.8948±0.0096

4.4 Discussions
4.4.1

𝛾2 is too large, e.g. 𝛾2 = 10 or 100, the learning rate will be
slower than that with moderate 𝛾2 values in the early stage
of learning.
This is because the training set is too small in this stage
and the class boundary learned is not very accurate, so
adopting too large 𝛾2 values will mislead example selection
by querying the outliers. The risk can be mitigated as the
size of the training set increases. We also note that choosing 𝛾2 = 1 will achieve the best performance not only in the
early stage but also in the later stage.
Similar experiments are conducted for the MCAT task of
the Reuters data set. As before, the random candidate sets
cover 20% of the unlabeled documents and the parameters
are set to be 𝜇 = 0.01, 𝛾1 = 0.1𝛾max . The learning curves of
SED with diﬀerent 𝛾2 values are plotted in Figure 6.
From Figure 6, it is interesting to observe that, diﬀerent from what we have found in Figure 5, adopting a larger
value of 𝛾2 will always improve the active learning procedure. This is because in the MCAT task, about 50% of the
documents are positive, but in the autos task, only 25% of

Effectiveness of Label Information

As we have discussed in Section 3, the contribution of label information is controlled by the parameter 𝛾2 . If 𝛾2 = 0,
we do not use the label information at all and hence our
method degenerates to Convex TED; as 𝛾2 increases, we
put larger weight on the label information. To evaluate the
contribution of label information, we carry out a set of experiments by varying the value of 𝛾2 in the autos task of the
Newsgroups data set. As before, each candidate set covers
50% of the unlabeled documents and the parameters are set
to be 𝜇 = 0.01, 𝛾1 = 0.1𝛾max .
The learning curves of SED with diﬀerent 𝛾2 values are
plotted in Figure 5. As we can see, using a large enough
𝛾2 value, e.g. 𝛾2 = 1, can greatly speed up the learning
procedure, while using small values, e.g. 𝛾2 = [0, 0.1], will
not improve much.
This observation validates the eﬀectiveness of label information for experimental design. It should be noted that if

304

0.95

0.9

0.9

0.85

0.85

0.8

0.8

AUC

AUC

0.95

0.75

0.75

0.7

0.7
SED
Convex TED
Margin

0.65
5

10

15

20
25
30
35
40
Number of Training Examples

45

SED
Convex TED
Margin

0.65

50

5

10

15

(a) CCAT

20
25
30
35
40
Number of Training Examples

45

50

(b) ECAT
0.9

0.95
0.85
0.9

AUC

AUC

0.8
0.85

0.8

0.75

0.7
SED
Convex TED
Margin

0.75

5

10

15

20
25
30
35
40
Number of Training Examples

45

SED
Convex TED
Margin

0.65

50

5

10

15

(c) GCAT

20
25
30
35
40
Number of Training Examples

45

50

(d) MCAT

Figure 4: Learning Curves for Four Binary Classiﬁcation Tasks on Reuters Data
0.95

0.98
0.97

0.9

0.96

0.85

0.95

AUC

AUC

0.94
0.93

0.8
0.75

0.92

0.9
0.89
0.88

5

10

15
20
25
30
35
40
Number of Training Examples

45

0.65

5

50

10

15
20
25
30
35
40
Number of Training Examples

45

50

Figure 6: Eﬀect of Varying 𝛾2 on MCAT Task

Figure 5: Eﬀect of Varying 𝛾2 on Autos Task

ever, in Figure 8, the learning curves are not so sensitive to
the candidate set size as those in Figure 7. This can again
be explained by the distribution of data. The more balanced
the data are, the less sensitive is the method to the candidate set size. We should note that the candidate set size
has great impact on the optimization problem. Speciﬁcally,
the larger the candidate set is, the longer time we need to
solve the problem. Thus, in practice, we should maintain a
tradeoﬀ between performance and the time cost and use a
candidate set of a reasonable size.

the documents are positive. Since the data distribution is
more balanced in the MCAT task, adopting a larger value of
𝛾2 will always choose those discriminative examples without
taking the risk of querying the outliers.

4.4.2

0
0.01
0.1
1
10
100

0.7

0
0.01
0.1
1
10
100

0.91

Effect of Candidate Set Size

In this section, we conduct several experiments to investigate the eﬀect of the candidate set size by randomly choosing 20%, 40%, 60% and 80% of the unlabeled documents to
form diﬀerent candidate sets. For the autos task, the parameters are set to be 𝜇 = 0.01, 𝛾1 = 0.1𝛾max , 𝛾2 = 1, and the
learning curves for diﬀerent candidate set sizes are plotted
in Figure 7. For the MCAT task, the parameters are set to
be 𝜇 = 0.01, 𝛾1 = 0.1𝛾max , 𝛾2 = 10, and the learning curves
are plotted in Figure 8.
As we can see from Figure 7, using a larger candidate set
will greatly speed up the learning procedure. We note that
as the size of the candidate set increases, the performance
gap between the learned classiﬁers becomes smaller. How-

5. CONCLUSION
In this paper, we have proposed a novel active learning
method, SED, to seamlessly incorporate label information
into the document selection procedure of experimental design. To the best of our knowledge, SED is the ﬁrst work
that uses label information to improve experimental design.
One promising property of SED is that it can eﬀectively use
label information and the distribution of unlabeled data in a

305

[4] D. A. Cohn, Z. Ghahramani, and M. I. Jordan. Active
learning with statistical models. J. Artif. Intell. Res.,
4:129–145, 1995.
[5] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An
eﬃcient boosting algorithm for combining preferences.
J. Mach. Learn. Res., 4:933–969, 2003.
[6] Y. Guo and R. Greiner. Optimistic active learning
using mutual information. In IJCAI, 2007.
[7] Y. Guo and D. Schuurmans. Discriminative batch
mode active learning. In NIPS, 2007.
[8] X. He, W. Min, D. Cai, and K. Zhou. Laplacian
optimal design for image retrieval. In SIGIR, 2007.
[9] S. C. Hoi, R. Jin, and M. R. Lyu. Large-scale text
categorization by batch mode active learning. In
WWW, 2006.
[10] S. C. Hoi, R. Jin, J. Zhu, and M. R. Lyu. Batch mode
active learning and its application to medical image
classiﬁcation. In ICML, 2006.
[11] D. D. Lewis and W. A. Gale. A sequential algorithm
for training text classiﬁers. In SIGIR, 1994.
[12] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. RCV1:
A new benchmark collection for text categorization
research. J. Mach. Learn. Res., 5:361–397, 2004.
[13] D. MacKay. Information-based objective functions for
active data selection. Neural Comput., 4(4):590–604,
1992.
[14] A. McCallum and K. Nigam. Employing EM and
pool-based active learning for text classiﬁcation. In
ICML, 1998.
[15] H. T. Nguyen and A. Smeulders. Active learning using
pre-clustering. In ICML, 2004.
[16] N. Roy and A. McCallum. Toward optimal active
learning through sampling estimation of error
reduction. In ICML, 2001.
[17] H. S. Seung, M. Opper, and H. Sompolinsky. Query by
committee. In COLT, 1992.
[18] S. Tong and D. Koller. Support vector machine active
learning with applications to text classiﬁcation. J.
Mach. Learn. Res., 2:45–66, 2002.
[19] Y. Yang and X. Liu. A re-examination of text
categorization methods. In SIGIR, 1999.
[20] K. Yu, J. Bi, and V. Tresp. Active learning via
transductive experimental design. In ICML, 2006.
[21] K. Yu, S. Zhu, W. Xu, and Y. Gong. Non-greedy
active learning for text categorization using convex
transductive experimental design. In SIGIR, 2008.
[22] J. Zhang and Y. Yang. Robustness of regularized
linear classiﬁcation methods in text categorization. In
SIGIR, 2003.
[23] L. Zhang, C. Chen, W. Chen, J. Bu, D. Cai, and
X. He. Convex experimental design using manifold
structure for image retrieval. In ACM MM, 2009.
[24] T. Zhang and F. J. Oles. Text categorization based on
regularized linear classiﬁcation methods. Inform.
Retrieval, 4(1):5–31, 2001.
[25] X. Zhu, J. Laﬀerty, and Z. Ghahramani. Combining
active learning and semi-supervised learning using
gaussian ﬁelds and harmonic functions. In ICML
Workshop, 2003.

0.98

0.96

AUC

0.94

0.92

0.9
20%
40%
60%
80%

0.88

0.86

5

10

15
20
25
30
35
40
Number of Training Examples

45

50

Figure 7: Eﬀect of Candidate Set Size on Autos Task
0.95
0.9

AUC

0.85
0.8
0.75
0.7
20%
40%
60%
80%

0.65

5

10

15
20
25
30
35
40
Number of Training Examples

45

50

Figure 8: Eﬀect of Candidate Set Size on MCAT
Task
uniﬁed framework. In particular, SED can greatly speed up
the learning procedure when the distribution of unlabeled
data is balanced, while existing methods based on experimental design always perform badly in this case. Moreover,
SED can greatly outperform margin-based active learning
when the distribution of unlabeled data is unbalanced. As
another promising property, SED is convex and thus global
optimality is guaranteed. Experiments conducted on two
text corpora demonstrate that SED outperforms state-ofthe-art active learning algorithms, such as TED and marginbased methods, which take into consideration only partial
information.
One of our future research directions is to automatically
learn from data the contribution of label information, i.e.
𝛾2 . Another possible research direction is to apply SED to
other information retrieval applications.

6.

ACKNOWLEDGMENTS

The authors thank Wu-Jun Li and Ning Zhu for some
helpful discussions. This research has been supported by
General Research Fund 622209 from the Research Grants
Council of Hong Kong.

7.

REFERENCES

[1] D. Angluin. Queries and concept learning. Mach.
Learn., 2(4):319–342, 1988.
[2] A. Atkinson and A. Donev. Optimum Experimental
Designs. Oxford University Press, USA, 1992.
[3] D. Cohn, L. Atlas, and R. Ladner. Improving
generalization with active learning. Mach. Learn.,
15(2):201–221, 1994.

306

