Contextual Video Advertising System Using
Scene Information Inferred from Video Scripts
Bong-Jun Yi, Jung-Tae Lee, Hyun-Wook Woo, and Hae-Chang Rim
Dept. of Computer & Radio Comms. Engineering, Korea University
Seoul, 136-713, South Korea

{bjyi,jtlee,hwwoo,rim}@nlp.korea.ac.kr

ABSTRACT

Table 1: Situation categories (in no specific order).
Work, Shopping, Meal, Interaction, Leisure,
Travel, Accident, Health, Infant care, Beauty
care, Finance, Love, Marriage, Conflict, Religious activity, Home living, Telephone conversation, Transportation, Study, Miscellaneous

With the rise of digital video consumptions, contextual video
advertising demands have been increasing in recent years.
This paper presents a novel video advertising system that
selects relevant text ads for a given video scene by automatically identifying the situation of the scene. The situation
information of video scenes is inferred from available video
scripts. Experimental results show that the use of the situation information enhances the accuracy of ad retrieval for
video scenes. The proposed system represents one of the pioneer video advertising systems using contextual information
obtained from video scripts.

contextual video advertising, scene, script, situation

use of their scripts. A main diﬀerence of our system against
a typical contextual advertising system is that, instead of
scanning a given text (a scene script in this case) for bid advertising keywords right away, the system examines the situation of individual scenes using descriptions and dialogues
in scripts to ensure contextually relevant advertising. For
example, if the user is viewing a sports game scene, the
user may expect to see ads for sports related topics, such as
sportswear or sports equipments. We adopt a learning-based
approach to classify individual scenes into (pre-deﬁned) situation categories. The ads are also automatically located in
given categories in order to ensure that they would only appear in the appropriate situation context. To the best of our
knowledge, this work is one of the ﬁrst attempts to match
video scenes to text ads based on the situation information
of the scenes inferred from video scripts.

1.

2. SITUATION CATEGORIES

Categories and Subject Descriptors
H.3.5 [Information Storage and Retrieval]: Online Information Services—Commercial services

General Terms
Algorithms, Experimentation

Keywords
INTRODUCTION

Over the past years, demands to serve ads on digital videos
based on the content displayed to the user have been consistently increasing with the growing number of various digital
devices and services. However, it is diﬃcult to get direct
access to the content of the video with existing visual analysis and speech recognition techniques, because of not only
the enormous computational cost they require but their low
performance. Other video metadata, such as title, summary,
or genre, provides very limited information for dynamically
analyzing individual scenes within the video. A reasonable
approach would be to utilize available video scripts that
contain descriptions of scenes as well as dialogues. This
approach is also practically applicable today, because it is
easy to obtain such data on the Web especially for popular
TV shows or movie contents.
In this paper, we demonstrate the implementation of an
intelligent contextual video advertising system that is able
to select ads for individual scenes in video contents with the

As in [1], we have pre-deﬁned a set of situation categories
as follows. Category names of products and services were
collected from various sources, such as shopping websites.
For each category name, four volunteers were asked to suggest general scenes that the category reminds of. For example, a category name “Health” may infer exercise scenes and
hospital scenes. All suggested situations were manually ﬂat
clustered into 20 diﬀerent clusters (categories) as shown in
Table 1. Note that these categories do not cover all possible situations that could happen in the real world, but they
represent the ones that may often provoke advertising.

3. SYSTEM IMPLEMENTATION
Figure 1 illustrates the architecture of the proposed system. A video is given along with its script as the input. The
video is ﬁrst decomposed into a series of individual scenes
according to the video script. For each scene, the system
extracts advertising keywords and analyzes the situation information from the corresponding scene text in the script.
A candidate list of ads is retrieved from the ad index with
regard to the extracted keywords using a standard informa-

Copyright is held by the author/owner(s).
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
ACM 978-1-60558-896-4/10/07.

771

Table 2: Retrieval performance.
NDCG Baseline
Proposed
at 1
0.4467
0.4733 (5.95%)
at 3
0.4562
0.4787 (4.93%)
at 5
0.4566
0.5002 (9.55%)

Initial Retriever
Given a set of extracted keywords for each scene, the initial
retriever module ﬁnds a list of top n candidate ads from the
ad index using the Indri retrieval model [2] as the basis of
textual relevance.

Situation based Reranker

Figure 1: System architecture.

Given a ranked list of candidate ads and the situation information for each scene, this module re-ranks the initial list
so that ads relevant to the situation context of the scene
are ranked higher. Because ads are generally not located in
pre-deﬁned situation categories, we built a multi-class logistic regression model for ads as for scenes. The regression
model uses the same PMI features but is trained with different training data, consisting of ad-situation pair samples.
Ad candidates are ranked according to their probability of
being classiﬁed as the situation category of the given scene.

tion retrieval model. Given the candidate list of ads and the
situation information inferred from the script, the system
ﬁnally re-ranks the list and outputs the top n ads that are
determined to be most appropriate for each scene.

Preprocessor
The duty of the preprocessor module includes decomposing
a given script input as a series of individual scenes and converting each scene text into machine readable format. For
the scene decomposition, it uses a collection of hand-crafted
rules with several cue expressions that reﬂect the beginning
of a new scene. It also distinguishes dialogues from instructions using pre-deﬁned regular expressions. Some shallow
linguistic analyses, such as part-of-speech (POS) tagging,
are applied to the text for later use.

4. EXPERIMENTS
We compare the quality of the ﬁnal re-ranked list of the
proposed system with the initial retrieval list, which represents the result of conventional advertising that do not
consider the situation context of individual scenes. We indexed 414,419 ad texts collected from various web search
engines. For scene data, we randomly chose 75 scene texts
from the data set used in [1], which consists of 3406 scene
texts from the scripts of popular TV drama shows broadcasted in Korea. From each scene text, the system extracted
average 3.25 keywords, which were used as queries for that
particular scene. The relevance of the top n retrieved ads
were manually judged by undergraduate students in 3-scale
(2=relevant, 1=somewhat relevant, 0=non-relevant). Since
video advertising systems can display only a few ads due
to the limited screen size, we measure the Normalized Discounted Cumulative Gain (NDCG) at top k ranks. Table 2
shows the results. It is observed that the proposed system
achieves better retrieval performance, which implies a better
user experience than conventional advertising.

Keyword Extractor
The role of this module is to automatically extract advertising keywords from each scene. Given a scene text, the module detects noun words and phrases as keyword candidates.
For each keyword candidate, several features suggested in
earlier advertising keyword extraction studies [1, 4], such as
its frequency features, position features, and external advertising keyword log features, are extracted. The module computes the score of each candidate using a logistic regression
model trained in advance with a set of correct and incorrect
keyword samples. It ﬁnally outputs candidates that receive
probability scores higher than a pre-deﬁned threshold value
as keyword queries for ad retrieval.

5. ACKNOWLEDGMENTS
This work was supported by Samsung Electronics.

Scene Situation Analyzer
Given a preprocessed scene text, this module takes a learning based text categorization approach to classify the scene
into one or more pre-deﬁned situation categories. Because
multiple situations can occur in a scene in general, we built
a multi-class logistic regression model in advance with a
training set of scene-situation pair samples. All situation
categories in which the regression model outputs probability scores higher than a manually-tuned threshold value are
selected. For features, we use the association measures (calculated using Pointwise Mutual Information) between all
words in a scene text and individual situation categories
instead of the traditional tf-idf features. This approach is
similar to the one used in [3].

6. REFERENCES
[1] J.-T. Lee, H. Lee, H.-S. Park, Y.-I. Song, and H.-C.
Rim. Finding advertising keywords on video scripts. In
Proc. SIGIR ’09, pages 686–687, 2009.
[2] D. Metzler and W. B. Croft. Combining the language
model and inference network approaches to retrieval. Inf.
Process. Manage., 40(5):735–750, 2004.
[3] G. Mishne. Experiments with mood classiﬁcation in
blog posts. In Proc. SIGIR ’05 Style Workshop, 2005.
[4] W.-t. Yih, J. Goodman, and V. R. Carvalho. Finding
advertising keywords on web pages. In Proc. WWW ’06,
pages 213–222, 2006.

772

