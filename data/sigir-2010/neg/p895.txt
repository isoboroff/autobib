A Co-learning Framework for Learning User Search Intents
from Rule-Generated Training Data
Jun Yan1
1

Zeyu Zheng1

Li Jiang2

Microsoft Research Asia
Sigma Center, No.49,
Zhichun Road
Beijing, 100190, China

Yan Li2

Shuicheng Yan3

2

Microsoft Corporation
One Microsoft Way
Redmond, WA 98004

{lij, roli}@microsoft.com

{junyan, v-zeyu,
zhengc}@microsoft.com

Zheng Chen1

3

Department of Electrical and
Computer Engineering
National University of Singapore
117576, Singapore

eleyans@nus.edu.sg

H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval – search process.

Though various popular machine learning techniques could be
applied to learn the underlying search intents of the users, it is
generally laborious or even impossible to collect sufficient and
label high quality training data for such learning task [1]. Despite
of the laborious human labeling efforts, many intuitive insights,
which could be formulated as rules, can help generate small scale
possibly biased and noisy training data. For example, to identify
whether the users have intents to compare different products,
several assumptions may help make the judgment. Generally, we
may assume that if a user submits a query with explicit intent
expression, such as “Canon 5d compare with Nikon D300”,
he/she may want to compare products. Though the rules satisfy
the human common sense, there are two major limitations if we
directly use them to infer ground truth. First, the coverage of each
rule is often small and thus the training data may be seriously
biased and insufficient. Second, the training data are usually not
clean since no matter which rule we use, there may exist
exceptions. In this paper, we propose a co-learning framework
(CLF) for learning user intent from the rule-generated training
data, which are possibly biased and noisy. The problem is,
Without laborious human labeling work, is it possible to train
user search intent classifier using the rule-generated training data,
which are generally noisy and biased? Given
sets of rulegenerated training datasets ,
1,2, … , how to train the
classifier :
on top of these biased and noisy training data
sets with good performance?

General Terms

2. THE CO-LEARNING FRAMEWORK

ABSTRACT
Learning to understand user search intents from their online
behaviors is crucial for both Web search and online advertising.
However, it is a challenging task to collect and label a sufficient
amount of high quality training data for various user intents such
as “compare products”, “plan a travel”, etc. Motivated by this
bottleneck, we start with some user common sense, i.e. a set of
rules, to generate training data for learning to predict user intents.
The rule-generated training data are however hard to be used since
these data are generally imperfect due to the serious data bias and
possible data noises. In this paper, we introduce a Co-learning
Framework (CLF) to tackle the problem of learning from biased
and noisy rule-generated training data. CLF firstly generates
multiple sets of possibly biased and noisy training data using
different rules, and then trains the individual user search intent
classifiers over different training datasets independently. The
intermediate classifiers are then used to categorize the training
data themselves as well as the unlabeled data. The confidently
classified data by one classifier are added to other training
datasets and the incorrectly classified ones are instead filtered out
from the training datasets. The algorithmic performance of this
iterative learning procedure is theoretically guaranteed.

Categories and Subject Descriptors

Algorithms, Experimentation

Suppose we have sets of rule-generated training data ,
1,2, … , which are possibly noisy and biased, and a set of
unlabeled user behavioral data
. Each data sample in the
training datasets is represented by a triple
,
,
1 ,
1,2, … | |, where
stands for the feature vector of the
data
sample in the training data ,
is its class label and | | is the
total number of training data in . On the other hand, each
unlabeled data sample, i.e. the user search session that could not
be covered by our rules, is represented as
,
,
0 ,
1,2, … | |. Suppose for any
, all the features constituting
|
1,2, … .
the feature space are represented as a set
Suppose among all the features F, some have direct correlation to
the rules, that is they are used to generate the training dataset .
These features are denoted by
, which constitute a subset
be the subset of features having no direct
of F. Let =
correlation to the rules used for generating training dataset .
, where
is any subset of F, we
Given a classifier :
use
to represent an untrained classifier and use
to represent
|
the classifier trained by the training data . Suppose

Keywords
User intent, search engine, classification.

1. INTRODUCTION
The classical relevance based search strategies may fail in
satisfying the end users due to the lack of consideration on the real
search intents of users. For example, when different users search
with the same query “Canon 5D” under different contexts, they
may have distinct intents such as to buy Canon 5D, to repair
Canon 5D, etc. The search results about Canon 5D repairing
obviously cannot satisfy the users who want to buy a Canon 5D
camera. Learning to understand the true user intents behind the
users’ search queries is becoming a crucial problem for both Web
search and behavior targeted online advertising.

Copyright is held by the author/owner(s).
SIGIR'10, July 19-23, 2010, Geneva, Switzerland.
ACM 978-1-60558-896-4/10/07.

895

means to train the classifier
by training dataset
using the
features
, we have
| ,
1,2, … . For the
| stands for classifying
trained classifier
, let
using features F. We assume for each output result of trained
classifier , it can output a confidence score. Let
|
,
where
is the class label of
assigned by
and the
is
the corresponding confidence score.
After generating a set of training data ,
1,2, … , based on
rules, we first train the classifier
by
,
1,2, … ,
independently. Then we can get a set of K classifiers,
| ,
1,2, … .
to train classifier on top of
Note that the reason why we use
instead of using the full set of features F is that
is generated
from some rules correlated to , which may overfit the classifier
if we do not exclude them. After each classifier
is trained
by , we use
to classify the training dataset
itself. A basic
assumption of CLF is that the confidently classified instances by
1,2, … , have high probability to be correctly
classifier ,
, if the
classified. Based on this assumption, for any
confidence score of the classification is larger than a threshold, i.e.
>θ and the class label assigned by the classifier is different
from the class label assigned by the rule, i.e.
, then
is considered as noise in the training data . Note that here
is the label of
assigned by classifier,
is its observed
class label in training data, and
is the true class label, which is
not observed. We exclude it from
and put it into the unlabeled
dataset . Thus we update the training data by
,
.
,
1,2, … , to classify the
Then we use the classifier
independently. Based on the same assumption
unlabeled data
that the confidently classified instances by classifier have high
probability to be correctly classified, for any data belonging to ,
if the confidence score of the classification is larger than a
|
threshold, i.e.
>θ , where
, we
include
into the training dataset. In other words,
,
,
1,2 … ,
.
Through this way, we can gradually reduce the bias of the rulegenerated training data.
On the other hand, some unlabeled data are added into the training

3. EXPERIMENTS
In this short paper, we utilize the real user search behavioral
dataset, which comes from the search click-through log of a
commonly used commercial search engine. It contains 3,420 user
search sessions, in each of which, the user queries and clicked
Web pages are all logged. Six labelers are asked to label the user
intents according to the user behaviors as ground truth for results
validation. We name this dataset as the “Real User Behavioral
Data”. The n-gram features are used for classification in the Bag
of Words (BOW) model. One of the most classical evaluation
metrics for classification problems, F1, which is a tradeoff
between Precision (Pre) and recall (Rec) is used as the evaluation
metric. For comparison purpose, we utilize several baselines to
show the effectiveness of the proposed CLF. Firstly, since we can
use different rules to initialize several sets of training data,
directly utilizing one training dataset or the combination of all
rule-generated training datasets to train the same classification
model can give us a set of classifiers. Among them, we take the
classifier with the best performance as the first baseline, referred
to as “Baseline” in the remaining parts of this section. The second
baseline is the DL-CoTrain algorithm, which is a variant of cotraining algorithm. It also starts from the rule-generated training
data for classification and thus has the same experiments
configuration as CLF. The classification method selected in CLF
is the classical Support Vector Machine (SVM). In Table 3, we
show the experimental results of CLF after 25 rounds of iterations
compared with the baseline algorithms. From the results we can
see that, in terms of F1, the CLF can improve the classification
performance as high as 47% compared with the baseline.
Table 3. Results of CLF after 25 iterations
Pre
Rec
F1

1|

1

DL-CoTrain
0.78
0.12
0.21

CLF
0.81
0.39
0.53

4. CONCLUSION
One bottleneck of user search intent learning for Web search and
online advertising is the laborious training data collection. In this
paper, we proposed a co-learning framework (CLF), which aims
to classify the users’ search intents without laborious human
labeling efforts. We firstly utilize a set of rules coming from the
common sense of humans to automatically generate some initial
training datasets. Since the rule-generated training data are
generally noisy and biased, we propose to iteratively reduce the
bias of the training data and control the noises in the training data.
Experimental results on both real user search click data and public
dataset show the good performance of the co-learning framework.

datasets. Suppose the ,
1|
is the probability of a
data sample to be involved in the training data
at the iteration n
conditioned on this data sample is represented as a feature vector
and
1 is the probability of any data sample in D is
considered as a training data sample. It can be proved that after n
iterations using CLF, for each training dataset, we have
,

Baseline
0.78
0.24
0.36

5. REFERENCE

.

[1] Russell, D.M., Tang, D., Kellar, M. and Jeffries, R. 2009.
Task behaviors during web search: the difficulty of assigning
labels. Proceedings of the 42nd Hawaii International
Conference on System Sciences (Hawaii, United States,
January 05 - 08, 2009). HICSS '09. IEEE Press, 1-5. DOI=
10.1109/HICSS.2009.417.

The remaining questions are when to stop the iteration and how to
train the classifier after iteration stops. In this work, we define the
iteration stopping criteria as “if |{ |
,
}| < n
or the number of iterations reaches N, then we stop the iteration”.
After the iterations stop, we obtain K updated training datasets
with both noise and bias reduced. Finally, we merge all these
training datasets into one. Thus we can train the final classifier as
.

896

