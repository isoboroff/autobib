Temporal Click Model for Sponsored Search

∗

Wanhong Xu

Eren Manavoglu

Erick Cantú-Paz

Carnegie Mellon University
5000 Forbes Ave
Pittsburgh, PA 15213

Yahoo! Labs
4401 Great America Parkway
Santa Clara, CA 95054

Yahoo! Labs
4401 Great America Parkway
Santa Clara, CA 95054

wanhong@cmu.edu

erenm@yahoo-inc.com

erick@yahoo-inc.com

ABSTRACT

1. INTRODUCTION

Previous studies on search engine click modeling have identified two presentation factors that affect users’ behavior:
(1) position bias: the same result will get a different number of clicks when displayed in different positions and (2)
externalities: the same result might get more clicks when
displayed with results of relatively lower quality than when
shown with higher quality results. In this paper we focus on
analyzing the sequence of user actions to model users’ click
behavior on sponsored listings shown on the search results
page. We first show that temporal click sequences are good
indicators of externalities in the advertising domain. We
then describe the positional rationality hypothesis to explain
both the position bias and the externalities, and based on
this hypothesis we further propose the temporal click model
(T CM), a Bayesian framework that is scalable and computationally efficient. To the best of our knowledge, this is the
first attempt in the literature to estimate positional bias, externalities and unbiased user-perceived ad quality from user
click logs in a combined model. We finally evaluate the proposed model on two real datasets, each containing over 100
million ad impressions obtained from a commercial search
engine. The experimental results show that T CM outperforms two other competitive methods at click prediction.

Commercial web search engines typically generate revenue
by presenting sponsored results as well as organic web results
to satisfy a user query. The most commonly employed payment model is “pay-per-click”, where an advertiser pays the
search engine when a user clicks on their ad [4]. The cost of
a click depends on the quality and bid of competing ads, and
is usually determined by a second price auction [10]. Sponsored search enables the advertisers (1) to target their campaigns to very specific markets by bidding only on the search
terms interesting to them, (2) to explore different markets
with minimal risk (there is no cost unless there is a click),
and (3) to iterate and improve quickly their campaigns by
using immediate feedback from performance. The combination of these features makes sponsored search one of the
most attractive and profitable advertising approaches [11].
The objective of the search engine in the sponsored search
model is to maximize its revenue over the long term [2]. This
involves a delicate balance of possibly conflicting objectives:
(1) maximize the revenue per search (RPS), (2) minimize
the negative impact of ads on the user experience, and (3)
maximize the return on investment for advertisers. Estimating the probability that users click on ads displayed in
response to their queries is essential to sponsored search,
because accurate predictions are necessary to address the
objectives above. In particular, the click probability is a
factor in ranking, placement, filtering, and pricing of ads.
Several studies have been published recently analyzing
user click behavior in organic web search [1, 15, 16, 5, 9, 6,
13] and in sponsored search advertising [17, 19, 20]. Joachims
et al. [15, 16] conducted an eye-tracking experiment to understand the decision making process of users when browsing
search engine results. An important finding of this study,
now well known as position bias, is that users tend to click
less on documents that are shown in lower positions, even
when the results were presented in reverse order. To explain the position bias phenomenon, Richardson et al. [19]
proposed the examination hypothesis, which assumes that a
result must be examined before being clicked, and the probability of being clicked after being examined depends on its
user-perceived quality. Craswell et al. [7] later proposed
the cascade hypothesis, which assumes that users always examine results in order from top to bottom. Under this assumption, results displayed at the top are more likely to
be examined than results shown at the bottom, regardless
of their quality. The cascade model proposed in [7] makes
another strong assumption: the user session ends after the
first click on a result. This assumption clearly fails to ex-

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]:
Learning; G.3 [Probability and Statistics]

General Terms
Algorithms, Experimentation

Keywords
Sponsored search, advertising, externalities, Bayesian model,
click log analysis, click-through rate
∗This work was done when the first author was on a summer

internship with Yahoo! Labs.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGIR’10, July 19–23, 2010, Geneva, Switzerland.
Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.

106

Figure 1: Statistical analyses on two real datasets SAD and NAD (see Sec. 2 for data description): The left four graphs are
statistical analyses of NAD: The top two graphs show that the CTR of ads at one position does not always increase as that
at the other position increases, which proves the existence of ad externalities; The bottom two graphs reveal that when the
CTR at one position increases, the average percentage of the first click being at the other position decreases, which suggests
that temporal click information could be useful to learn externalities. The right four graphs are statistical analyses for SAD.
nalities are indeed present in the sponsored search domain
and then investigate how to explain both position bias and
externalities in a combined model. We exploit the temporal order of user’s actions for this analysis. Our contributions can be summarized in three points. First, we present
a statistical analysis demonstrating that temporal user click
sequences are good indicators of ad externalities. Second,
we propose the positional rationality hypothesis which explains both position bias and externalities. Third, based on
this hypothesis we develop the temporal click model (T CM),
which has the properties: (1) Foundation: To the best
of our knowledge, this is the first attempt in the literature
to estimate positional bias, externalities and unbiased userperceived ad quality from user click logs in a combined model;
(2) Framework: T CM is based on a strict Bayesian framework. Closed-form representations of the ad quality and user
behavior posteriors can be derived using this framework,
making it scalable and computationally efficient to handle
the challenges imposed by the voluminous click logs; (3)
Effectiveness: T CM consistently outperforms two stateof-the art models in a number of metrics at click prediction.

plain sessions with multiple clicks. Most of the subsequent
research on click modeling for search results have focused
on relaxing this assumption. The Click Chain Model [13]
and the Bayesian Browsing Model (BBM) [18] both allow
the user session to continue with a probability that is dependent on the relevance of the clicked ad. The dynamic
Bayesian network model proposed by Chapelle and Zhang
[6] also lets the user session continue after a click, but in their
model the probability of continuing the session depends on
the quality of the landing page. Their model is the first to
separate the perceived relevance of the ads from the actual
quality of the landing page.
The click models discussed so far do not account for the
attractiveness or relevance of the results below when considering the probability of click on a particular ad. In Guo’s
[13] and Chapelle’s [6] models, the probability of click for the
results might be affected by the results shown immediately
above them, since the examination probability depends on
the previous result’s (or landing page’s) quality. However,
the click probabilities cannot be influenced by the results
presented below. This can be a significant limitation, especially in sponsored search, where the advertisers compete
and pay for the user’s attention. In advertising, the effect of
the set of ads on the user behavior is widely accepted and
referred to as externalities. Ghosh et al. [12] proposed the
rationality hypothesis to explain this behavior. Under their
hypothesis users are assumed to be rational. Given a set
of ads displayed, a user first compares the qualities of the
ads and clicks on the best one. Kempe and Mahdian [17]
later proposed a variation of the cascade model based on
this hypothesis. Instead of using the top to bottom order
of scanning the ads, their model allows each user to have a
different ordering over the positions. Unfortunately both of
these models were used only in the context of analyzing the
auction mechanisms, therefore there are no results on the
accuracy of the click models proposed, or even analysis to
show that the externalities exist in the data.
In this paper, our objective is to first show that exter-

2. DATA ANALYSIS
In the online advertising literature [12], externalities are
often considered a primary factor influencing user click behavior. In this section we present a click log analysis indicating this effect exists in real data sets.
Our goal is to analyze how the click behavior of users
on a particular ad changes depending on the quality of the
rest of the ads. We focus on ad impression sequences with
exactly two ads. Showing that externalities exist for ad sets
with only two ads proves the existence of externalities for
longer sequences as well. We collected two real data sets
in March 2009 from a commercial search engine. One of
the data sets consists of queries that have exactly two ads
shown on top (north) of the organic web search results, and
the other contains ad impressions that were shown at the

107

bottom (south). We will refer to these data set as North Ad
Data(NAD) and South Ad Data(SAD), respectively.
Since there is no ground truth for ad quality, we use empirical click-through rates (CTR) as a proxy. For each ad
impression sequence we calculate the CTR of the ad in Position 1 and Position 2 separately, and compare the CTRs at
these two positions using the following approach: We group
all impressions with similar CTRs at Position 1 in one bin
and plot the average CTR at Position 2. This way we can
use the CTRs at Position 1 as quality measure (since each
bin contains ads with similar CTRs) and the average CTR
at Position 2 as the click metric. Results are shown in the
top row of Fig. 1. We see that as the quality in one position
increases (x-axis) from 0 to about 0.2, CTR of the other position (y-axis) also increases. However, when the quality of
one position keeps increasing further, the CTR of the other
position starts dropping. This observation suggests that the
high quality ads (CTR > 0.2 in these graphs) could affect
the click rates of the other ads adversely. Note that the
graphs for SAD dataset doesn’t show the decrease, but that
the absolute value of the max CTRs in these positions are
much lower than the ones in North. Still, we see that around
0.1 the curve starts to saturate.
We also examine if the quality of ads in one position influences which ads user clicks first. For this, we group all
impressions with similar CTR at Position 1 in one bin and
plot the percentage of events where the first click occurred
at Position 2 and vice versa. Results are shown in the bottom row of Fig. 1. We observe that when CTR in Position
1 increases users prefer to click first on Position 1. This observation verifies that users are rational: they compare the
qualities of both ads and choose the better one to click first.
Hence, the temporal click sequences of users can be used to
learn ad externalities.
Analyzing the first click graphs in more detail shows that
users aren’t completely rational, and that they are biased
by the position of the ads. For example, in the left bottom
two plots of Fig. 1, when CTR at Position 1 is 0.2, the percentage of first click at Position 2 is around 0.2 too. In other
words, the probability of clicking Position 1 first is around
0.8. But, when CTR at Position 2 is 0.2, the probability of
clicking Position 2 first is around 0.6. The reason for this
behavior could be that users may be trusting the ranking of
the search engine more if they don’t think the ad at Position
2 is much better than the one at Position 1 and don’t use
their ranking to take over the ranking of search engine.
Based on the above observations, we propose the positional rationality hypothesis: Users examine both ads together and assess their qualities, and then users compare
their qualities. If the ad at Position 2 is much better than
ad at Position 1, users would consider clicking the ad at
Position 2 first instead of the ad at Position 1.

3.

ad presented in position i and D is the total number of slots.
An ad ai in this sequence is shown at a higher position (i.e.
ranked higher) than ad aj if i < j . The clicked ads can be
represented as a sequence as well, a sequence of click events
ordered by their time of click: C =<c1 . . . cT >, where T ≤ D
and ci corresponds to the ad in position i. We call this
sequence the temporal click sequence.
To study the impact of ad externalities, we focus on ad impression sequences with two ads only. For an ad impression
sequence A =<a1 a2>, there are five possible click sequences:
<>, < a1 >, < a1 a2 >, < a2 > and < a2 a1 >. Multiple clicks
on the same ad in the same position are discarded in this
model. To simplify notation and have sequences of equal
lengths, we will use the symbol 2 to indicate a no-ad-clicked
action. We can then rewrite the five possible click sequences
as: <22>, <a1 2>, <a1 a2 >, <a2 2> and <a2 a1 >. We will
refer to these sequences as click sequences type 1..5.

3.1 Model Specification
The temporal click model can be described as a generative
process as illustrated in Fig. 2(Left). User submits a query
and sees an ad impression sequence A =<a1 a2 >. She can
either examine or ignore the ads . We treat this user behavior as a probabilistic event, represented by a binary random
variable E . The probability of examining ads, denoted as
P (E = 1|A), is set to a global model parameter γ , i.e.,
P (E = 1|A) = γ.
(1)
If the user decides not to ignore the ads, she examines
both of them as the position rationality hypothesis states,
and picks one. We denote the perceived quality of ads a1
and a2 by Ra1 and Ra2 , respectively, where Rai ∈ [0..1]. Our
model says that the quality of a2 has to be greater than that
of a1 ’s by a margin of Ua1 for the user to ignore the search
engine’s ranking and pick a2 . Otherwise she picks a1 . Ua1
can be viewed as the advantage of being ranked in position
1. We treat this user behavior as a probabilistic event, too.
Let F be the random variable to represent the first ad the
user picks, then the probability of picking a1 or a2 would be:
P (F = a2 |E = 1, A) = 1I[Ra2 − Ra1 > Ua1 ]
(2)
P (F = a1 |E = 1, A) = 1I[Ra2 − Ra1 ≤ Ua1 ],
(3)
where 1I is the indicator function.
Once the user picks an ad ai , whether she clicks it or not
depends solely on its quality, Rai . Thus, the probability of
ai being the first clicked ad c1 or not having a click at all is:
P (c1 = ai |F = ai ) = Rai
(4)
P (c1 = 2|F = ai ) = 1 − Rai , i ∈ {1, 2}.
(5)
If the user does not click on the ad she picked our session
model terminates. If, however, she clicks on that ad she can
re-consider the ad that was skipped. Our model assumes
that the probability of re-considering the other ad depends
on the difference of the perceived qualities of the two ads.
The higher the relative perceived quality of the clicked ad,
the smaller the probability of re-considering the skipped ad.
In T CM, we model this probability as a linear function of
the difference in qualities with a scale factor α and a bias
term β . We let S be the random variable representing the
skipped ad, and derive the following probabilities:
P (S = 2|c1 = 2) = 1
(6)
P (S = a3−i |c1 = ai , A) = β − α(Rai − Ra3−i ), i ∈ {1, 2}. (7)
Again, once the user decides that she will consider the

TEMPORAL CLICK MODEL

We first introduce definitions and notations, and next
specify and discuss the temporal click model.
A user starts a query session by submitting a query to
the search engine. The search engine retrieves the ads that
match the user query and finds the ranking that optimizes
the objective function and presents them to the user in different slots on the results page, alongside organic web results.
The set of ads presented to the user can be represented as
an ad impression sequence A =<a1 . . . aD >, where ai is the

108

31

$%&'()"*&+*,*&-

:;(<

!"#

!"#

45(67*&+8

31
./"0"/*&12"/*&+

31

Ua1

Ra1

Ra2

E

F

S

C1

C2

!"#

45(67*&-8

31
91)"

!"#

31

31
45(67*&-8

!"#

45(67*&+8

!"#

Figure 2: (Left) The user model of T CM; (Right) The graphical model representation of T CM for an ad impression
A=<a1 a2 >: Ra1 and Ra2 are user-perceived quality variables of Ad a1 and a2 separately; Ua1 is the position advantage
variable of Ad a1 ; and observed click sequence <c1 c2 > is shaded.
skipped ad aj , whether she clicks or not depends on its perceived quality only. Thus the probability of having a second
click c2 on aj or not having a second click at all has the
form:
P (c2 = aj |S = aj ) = Raj
(8)

gives us the flexibility to learn both of them from click data
by finding its posterior probability.
To generalize our model to more than two ads we can
assume that each ad except for the last one, has a corresponding position advantage; and that an ad would only be
picked first if its quality beats all the other ads. After the
first ad is picked the same process can be used to select the
second ad and so on and so forth.
In reality, most search engines, for example Google, Yahoo! and Bing, receive a large fraction of ad clicks from the
ads displayed on the top of organic search results. Improving the accuracy of click prediction in this premium slot will
have a profound effect on the overall click yield and revenue.
Those engines show less than five ads in the top. Therefore,
we believe that the proposed model is potentially useable
since it can be easily generalized and applied to the limited
number of ads that can be displayed in the top. In fact,
the proposal model on two ads is practicable too and it can
be applied to the bottom slot of Bing and Yahoo! where at
most two ads could be shown.

P (c2 = 2|S = aj ) = 1 − Raj , j ∈ {1, 2}.
(9)
Since we ignore multiple clicks on the same ad at the same
position our session model ends after this step. However, we
still need to specify the probabilities for the case where the
user ignores the ads in the first step. If the user ignores both
of the ads, she considers neither, and if she doesn’t consider
either one of the ads, she clicks none:
P (F = 2|E = 0, A) = 1
(10)
P (c1 = 2|F = 2) = 1
(11)
P (c2 = 2|S = 2) = 1
(12)
We complete the specifications of our Bayesian framework
by introducing priors on Rai s and Uai s. Although any form
priors are possible, we follow [13, 18] in choosing the iid
non-informative uniform priors within [0, 1]. We show the
graphical model representation of T CM in Fig. 2(Right).
To guarantee that we have a well-defined model, we have
to put constraints on the parameters of the model such that
the probability of any click sequence generated by this model
is between 0 and 1. We describe these constraints in the
following lemma.

4. ALGORITHMS
In this section, we propose algorithms for the T CM. We
first present the closed form posterior distributions of userperceived ad quality and ad position advantage; then explain
how to estimate the model parameters θ = {α, β, γ}; finally
we show how to use them to predict the click-through rate
of any given ad impression sequence.

Lemma 3.1. The temporal click model is well-defined if
its parameters satisfy the constraints: −1 ≤ α ≤ 1, 0 ≤ β ≤ 1,
0 ≤ γ ≤ 1, α ≤ β and 0 ≤ α + β ≤ 1.

4.1 Posterior Distribution
For a given query with N sessions, we let A = {A1 , A2 , . . . ,
AN } and C = {C 1 , C 2 , . . . , C N } be their corresponding ad

3.2 Discussion
Now that the T CM model is formally presented we illustrate how it explains the position bias and ad externalities.
Let’s consider the two extreme cases of the T CM. If we set
the position advantage variable Ua1 to be 1, then the quality
difference Ra2 − Ra1 can not be greater than Ua1 because
Ra2 −Ra1 ≤ 1. In this case, user never considers clicking a2
first, regardless of its quality. Only after a1 is clicked could
a2 have a chance. In other words, T CM with Ua1 = 1 would
roughly correspond to the cascade model described in [7].
On the other hand, If we set the user variable Ua1 to
be 0, the user would always be fully rational. She would
always pick the ad with better quality first, regardless of its
position, like the rationality hypothesis proposed in [12] that
explains ad externalities.
In a nutshell, the position advantage variable Ua1 is a
tradeoff between position bias and ad externalities, and it

impression and click sequences respectively. We assume that
in A there are M distinct ads indexed from 1 to M , and let
R = {R1 , R2 , . . . , RM } be the corresponding user-perceived
ad quality variable and U = {U1 , U2 , . . . , UM } be the position
advantage variable.
We assume that the ad impression and click sequences in
different sessions are independent of each other, given Rm
and Um . We can then compute the posteriors of Rm and
Um using the Bayes principle:
Q
n
n
P (Rm |C, A) ∝ P (Rm ) N
n=1 P (C |Rm , A ),
QN
P (Um |C, A) ∝ P (Um ) n=1 P (C n |Um , An ).

Since P (Rm ) and P (Um ) are already known, we only need
to figure out P (C n |Rm , An ) and P (C n |Um , An ). To calculate
them, we have to integrate out all hidden random variables
other than Rm and Um . We show that P (C n |Rm , An ) and

109

P (C n |Um , An ) have closed forms by demonstrating integration on a specific case of P (C n |Rm , An ). In this particular
case, the ad impression is An =< a1 a2 >, click sequence is
C n =<a2 a1> and m = a2 . After applying the Bayes rule and
uniform priors on Ra1 and Ua1 we get Eq.(13):
P (C n =<ma1> |Rm , An =<a1 m>)
R
P
n
n
= Ra Ua
E,F,S P (C , E, F, S, Ra1 , Ua1 |Rm , A )
1
1
R
R
P
n
n
= Ra Ua
E,F,S P (C , E, F, S, |Rm , Ra1 , Ua1 , A ).
R

1

1

model parameters θ = {α, β, γ}. We want to maximize the
Q
n
n
likelihood of C, i.e., P (C|θ, A) = N
n=1 P (C |θ, A ). The
n
n
likelihood of P (C |θ, A ) can be computed by integrating
out the Rai s and Uai s as described in the previous section.
The exact derivation is omitted to save space.
By summing log likelihood of all click sequences, we get
the following log-likelihood function:
11
13β
α
7γ
) + N2 log[(
−
+
)γ]
12
24
60
72
13β
α
1
β
α
+ N3 log[(
−
)γ] + N4 log[( −
+
)γ]
60
72
8
30
72
β
α
+ N5 log[(
−
)γ],
30
72

L(θ) = N1 log(1 −

(13)

We first integrate out hidden variables E , F and S in
Eq.(13) by applying model specification. To generate the
click sequence <ma1> given ad impression <a1 m>, E , F , S
must be 1, m, a1 . Applying Eq.(1,2,4,7,8), we get Eq.(14).

where Ni represents the number of instances of click sequence type i in C .
By setting the derivative of log likelihood function L(θ)
on γ to zero and also considering model constraints listed in
Lemma 3.1, we get a closed form function to estimate γ :

n
n
S P (C =<ma1>,E,F,S,|Rm,Ra1 ,Ua1 ,A =<a1 m>)
n
=P (E = 1)P (F = m|E = 1,Rm ,Ra1 ,Ua1 )P (C1 = m|F = m,Rm )
P (S = a1 |C1n = m,Rm ,Ra1 )P (C2n = a1 |S = a1 ,Ra1 )

P P P
E

F

(14)
Next, plugging Eq.(14) back into Eq.(13) and integrating
out random variables Ra1 and Ua1 , gives us the closed-form
of P (C n =<ma1> |Rm , An =<a1 m>):
=γRm Ra1 (β −α(Rm−Ra1 ))1I[Rm −Ra1 > Ua1 ].

γ = min

0 0 γRm Ra1 (β −α(Rm−Ra1 ))1I[Rm −Ra1 > Ua1 ]dRa1dUa1
R RmR Rm −Ua1
=0
γRm Ra1 (β −α(Rm−Ra1 ))dRa1dUa1
0

(15)

P (C n |Rm , An )

We can see that the closed form of
depends
only on the click sequence type and the position of Ad m
in An , independent of the other ads. There are five click
sequence types (as described in 3) and two possible positions.
We let φk,i (·) be the closed form of P (C n |Rm , An ) for k -th
click sequence type and i-th position. The functional form
of φk,i (·) for all k and i is listed in Table.1. Finally, the
posterior of Rm has the unnormalized closed form:
P (Rm |C, A) ∝
m
Nk,i

Q5

k=1

Given the model parameters and posteriors, we can use
the mid-point rule to estimate the posterior probability of
any click sequence. For example the probability of click
sequence <a2 a1> is:

m
Nk,i
,
i=1 (φk,i (Rm ))

P (C =<a2 a1> |A)

Q2

m
Nk,1
.
k=1 (ψk (Um ))

Q5

Because of the potential dimensionality curse introduced
m , we follow [13] and utilize the midpoint rule
by large Nk,i
to numerically normalize P (Rm |C, A) and P (Um |C, A). For
example, the normalizing constant of P (Rm |C, A) can be
approximated by
PB

b=1

Q5

k=1

Q2

i=1 (φk,i (

12 (N + N + N + N ) )
2
3
4
5
7
.
(N1 + N2 + N3 + N4 + N5 )

4.3 Click Through Rate Prediction

is the number of sessions that the click sequence
where
is k -th type and Ad m appears at i-th position in the ad
impression sequence.
We can get the unnormalized closed form of posterior of
Um , similarly. We let ψk (·) be the closed form of P (C n |Um , An )
for k -th click sequence type and m at Position 1 of An .
P (C n |Um , An ) will be a constant if m is at Position 2 of
An because ad has position advantage only in Position 1 for
sequences of length 2. The functional form of ψk (·) is also
given in Table. 1. The closed form of the posterior of Um is
P (Um |C, A) ∝

1,

Unfortunately, there are no closed form solutions for α
and β . But, it is not hard to verify that the log likelihood
function L(θ) is concave in α and β under the parameter constraints listed in Lemma 3.1. Therefore, we can utilize convex optimization techniques to find approximate solutions.
In our implementation we used a barrier method combined
with the Newton’s method [3]. Since there are only two parameters to estimate, this is a simple optimization problem.

R 1R 1

1
1
= βγR4m −
αγR5m .
6
12

(

=E(γRa2 Ra1 (β −α(Ra2−Ra1 ))1I[Ra2 −Ra1 > Ua1 ])
PB
1 P
(b2 − 0.5) (b1 − 0.5)
b − b1
≈ 2 B
[β − α( 2
)]
b2 =1 b1 =1 γ
B
B
B
B
b − 0.5
b − 0.5
b − b1
Pa2 ( 2
)Pa1 ( 1
)Fa1 ( 2
),
B
B
B
where Pa1 (·) and Pa2 (·) are normalized density functions of
posteriors of Ra1 and Ra2 , and Fa1 (·) is cumulative distribution of posterior of Ua1 .

Once we have the probabilities of individual click sequences,
we can compute the click through rate of any ad, CT R(ai ),
in an ad impression sequence A =<a1 a2> by taking the sum
of probabilities of all click sequences that include a click on
that particular ad ai :
CT R(a1 ) = P (C =<a1 2>∨C =<a1 a2>∨C =<a2 a1> |A)
CT R(a2 ) = P (C =<a2 2>∨C =<a1 a2>∨C =<a2 a1> |A).

4.4 MapReduce Implementation

m
b − 0.5 Nk,i
))
B

To implement the inference algorithm proposed above, we
m}
first need to collect sufficient statistics {Nk,i
k=1..5,i=1..2
for computing posteriors of Rm and Um , and {Nk }k=1..5
for estimating model parameters. Obviously, a sequential
scan of data could provide those statistics. However, to be
able to apply the T CM to petabyte-scale data requires parallelization. In this section we describe how to leverage the
MapReduce paradigm [8] for this purpose.
MapReduce is a programming framework to parallelize

with B equal-size bins on [0..1].
Once normalized with an appropriate B , both density
function and cumulative distribution function of posteriors
of Rm and Um could be evaluated at desired precision.

4.2 Parameter Estimation
We use the maximum likelihood principle to estimate the

110

Algorithm 1 Map(A, C) - Mapping a query session
Input: A: ad impressions of a query, C: click log.
Output (a, v): a is the ad index; v is sufficient statistics of
a for one session.
1: for each ad impression sequencen = 1, . . . , N do
2:
Set k be the click sequence type of C n
3:
for each position i = 1, 2 do
4:
v = 0;
5:
v[k ∗ 2 + i − 3] = 1;
6:
Emit(an
i , v);
7:
end for
8: end for

Figure 3: Summary of Datasets: N AD (left) and SAD
(right).
pare the performance of T CM on this task to any other
method.

Algorithm 2 Reduce(a, vectList)
Input: a is the ad index; vectList is a list of vectors associated with a.
Output (a, N a ): N a is sufficient statistics of a for all sessions.
1: N a = 0;
2: for each v in vectList do
3:
N a + = v;
4: end for
5: return (a, N a );

5.1 Experiment Setup
We evaluate our model on two data sets: N AD and SAD,
which are introduced in Section 2. N AD and SAD have
around 0.3 and 1.1 million distinct queries with 0.1 and
0.65 billion sessions each, respectively. For the very frequent queries, direct statistics can do well on CTR prediction because there are many historical observations for these
queries. In order to prevent the model evaluation from being dominated by these samples, only queries with less than
103.1 sessions are kept for evaluation. This filtering removes
3% and 5% queries in N AD and SAD respectively. A summary of the query frequency distributions in the two data
sets is provided in Fig. 3.
Because our model aims at learning externalities between
ads, we evaluate our model on the estimating the CTR of ad
impression sequences instead of an individual ad. Therefore,
we design and use the following leave-one-out experimental
protocol:

computational tasks by expressing them as pairs of Map
and Reduce functions. The implementation of MapReduce
infrastructure frees programmers from the practical issues of
running algorithms over a cluster of computers, such as loadbalancing, fault tolerance and data distribution problems.
Algorithm 1 and 2 are the implementation of a pair of Map
m}
and Reduce functions to collect {Nk,i
k=1..5,i=1..2 . Specifically, the Map function reads each session from input and
emits a set of intermediate (key, value) pairs, where the key
is the ad index and the value is this ad’s sufficient statistics of that session. The MapReduce infrastructure then
groups together all values with the same intermediate key
and passes them to the Reduce function. The Reduce function accepts an intermediate key and a list of values for the
key. It summarizes those statistics and outputs the final result. The idea behind the MapReduce framework is that a
large scale dataset could be broken down into small chunks
and each chunk is fed to a Map function that will be run
on an individual computer. The Reduce function is used
for summarizing results from all chunks to get the complete
result on the whole dataset.

5.

1. Retrieve all the sessions related to a given query;
2. Consider each distinct ad impression sequence in those
sessions;
3. Hold out as test sessions all the sessions with this ad
impression sequence;
4. Train our model and baselines on the remaining sessions and the predict the CTR of this ad impression
sequence;
5. Compute the true CTRs from test sessions;
6. Compute the Mean-Square-Error (MSE) between the
true CTRs and the predicted CTRs;
7. Average the error on all ad impression sequences, weighted
by the number of test sessions.

5.2 Baselines

EXPERIMENTS

We considered two baseline models for model comparisons. One is naive CTR statistics (NS), and the other
is Bayesian browsing model (BBM) [18], a state-of-the-art
model in click modeling literature.
Given training data A and C and an ad impression sequence A =<a1 a2>, the baseline NS simply uses the empirical CTR of ai in training data, i.e., the percentage of clicks
on ai at position i.
The BBM model follows the examination hypothesis as
T CM does. But, T CM differentiates itself from BBM in
several ways. For example, BBM examines one ad at a time
and T CM examines both at the same time, in order to model
externalities. Moreover, BBM ignores temporal information,
and therefore it treats two click sequences <a1 a2 > and <

We conduct experiments to evaluate the performance of
the T CM. The evaluation criterion in both cases was the
accuracy of the predicted click-through rates. The ranking
produced by ordering the results based on estimated CTRs
are often used in literature [6] for model comparison purposes. However, in sponsored search, the final ranking presented to the user is a product of an auction mechanism.
For our end goal, the accuracy of the click prediction matters most. One of the differentiating properties of T CM
compared to other state of the art models [13, 18, 6] is that
T CM considers temporal sequences, and predicts the probabilities of sequences of clicks. Since the existing methods
completely ignore the time of clicks, we are unable to com-

111

Figure 4: Accuracy in predicting the CTR of ad impressions
from queries with different frequencies (Left: N AD dataset,
Right: SAD dataset). Both T CM and BBM are significantly
better than NS for all query frequencies. T CM is noticeably
better than BBM on less-frequent queries but shows similar
performance on frequent ones.

Figure 5: Accuracy in predicting the CTR at Position 2
for varying query frequencies (Left: N AD dataset, Right:
SAD dataset). Both T CM and BBM are significantly better
than NS for all query frequencies on both datasets. T CM is
noticeably better than BBM on all queries of N AD, and on
less-frequent queries of SAD.

Figure 6: Average percentage of first click at Position 1 for
varying query frequencies (Left: N AD dataset, Right: SAD
dataset). The probability that users click Position 1 first for
frequent queries tends to be higher than less frequent queries.

Figure 7: Percentage of queries where T CM performs as
good or better than BBM for varying query frequencies (Left:
N AD dataset, Right: SAD dataset)

a2 a1> as the same. Given an ad impression sequence A and
the click sequence C =< c1 c2 >, the BBM specification is as

in Position 1 first is higher for the frequent queries. This
graph could suggest that the users consider ads in Position
1 more frequently in these very frequent queries. It’s quite
possible that search engine users are trained to consider top
results more for the frequent queries via positive reinforcement: the more user feedback there is, the better the search
engine ranking will be; as the search engine rankings improve users will trust search engine’s ranking more. If this
is the case, the assumption that the users will examine both
of the ads before clicking would not hold for the frequent
queries. Since T CM uses this positional rationality assumption, degradation in performance for the frequent queries
would not be surprising. Fig. 7 further verifies this claim.
It presents the percentage of queries that T CM performs as
good or better than BBM for varying query frequencies on
the two data sets. The percentage goes down as the query
frequency increases.

follows:
P (F = a1 ) = γ1 ,
P (c1 = a1 |F1 = 2) = 0,
P (S = a2 |c1 = a1 ) = γ2 ,
P (c2 = a2 |S = 2, Ra2 ) = 0,

P (c1 = a1 |F = a1 ) = Ra1 ,
P (S = a2 |c1 = 2) = γ3 ,
P (c2 = a2 |S = a2 , Ra2 ) = Ra2 ,

where γ1 , γ2 and γ3 are model parameters. Please refer to
[18] for the details of BBM inference algorithms.

5.3 Predicting CTR
We compare T CM to the two baselines on predicting the
CTR of ad impressions. As described in the experimental
protocol, mean square error is used as the evaluation metric
[14]. Fig. 4 reports the prediction accuracy of three methods
on both datasets N AD and SAD for varying query frequencies. Both T CM and BBM are significantly better than NS
across all query frequencies. Our model T CM is noticeably
better than BBM on less-frequent queries but shows similar
performance on frequent ones.
One obvious reason for the similar performance of T CM
and BBM on frequent queries is that frequent queries have
more sessions available for training and have more uniform
click patterns than less-frequent ones. Even the naive method
NS tends to close the performance gap with T CM and BBM
on frequent queries.
We present more detailed analysis of the data in Fig. 6
to better understand this behavior. This figure shows the
percentage of first clicks at Position 1 for different query
frequencies. We see that the probability of clicking the ad

5.4 Discussion
T CM performs noticeably better than BBM on less-frequent
queries. But, what contributes to this improvement?
We compare T CM, BBM and NS on predicting the CTR
at Position 2 of ad impressions. Fig. 5 presents the accuracy
results for varying query frequencies as measured by mean
square error. Both T CM and BBM are significantly better
than NS for all query frequencies on both datasets N AD and
SAD . T CM is noticeably better than BBM on all queries of
N AD , and on less-frequent queries of SAD as before, but the
magnitude of improvement is much bigger. Therefore, T CM
performs better than BBM mainly because it predicts clicks
at Position 2 better.

112

P (C|Rm , A =<a1 a2>)

Case

Function Expression

C = 22, m = a1
C = 22, m = a2

1 γ − γR2 + 1 γR3
m
m
3
3
1 γ − 1 γR3
m
2
3
1 α)γR + (1 − 1 β − 1 α)γR2 + (− 1 + 1 α)γR3 + 1 βR4 − 1 αγR5
( 12 − 61 β − 12
m
m
m
m
m
2
6
2
2
6
12
1 γ − ( 1 β − 1 α)γR − 1 αγR2 − 1 γR3 + 1 βγR4 + 1 αγR5
m
m
m
m
m
2
2
3
2
6
6
12
1 α)γR + ( 1 β + 1 α)γR2 − 1 αγR3 − 1 βγR4 + 1 αγR5
( 16 β + 12
m
m
m
m
m
2
6
2
6
12
1 αγR5
( 12 β − 13 α)γRm + 21 αγR2m − 16 βγR4m − 12
m
1 γ + (− 1 − 1 β + 1 α)γR + ( 1 β − 2 α)γR2 + ( 1 + 1 α)γR3 − 1 βγR4 − 1 αγR5
m
m
m
m
m
3
2
3
4
2
3
6
2
6
12
1 γR3 − 1 βγR4 + 1 αγR5
m
m
m
2
6
12
1 αγR5
( 13 β − 14 α)γRm + (− 12 β + 23 α)γR2m − 12 αγR3m + 61 βγR4m + 12
m
1 βγR4 − 1 αγR5
m
m
6
12

1−
1−

C = a1 2, m = a1
C = a1 2, m = a2
C = a1 a2 , m = a1
C = a1 a2 , m = a2
C = a2 2, m = a1
C = a2 2, m = a2
C = a2 a1 , m = a1
C = a2 a1 , m = a2

P (C|Um , A =<a1 a2>)

Case
C = 22, m = a1
C = a1 2, m = a1
C = a1 a2 , m = a1
C = a2 2, m = a1
C = a2 a1 , m = a1

Function Expression
2 − 1 γU 3
1 − 23 γ + 12 γUm
m
3
1
1
1
2 + ( 1 + 1 α)γU 3 − 1 βγU 4 − 1 αγU 5
( 3 − 8 β + 30 α)γ + ( 12 − 13 β)γUm + (− 12 + 14 β − 16 α)γUm
m
m
m
6
6
24
30
1
1
1
1
1
1
1 βγU 4 + 1 αγU 5
2
3
( 8 β − 30 α)γ + 3 βγUm + (− 4 β + 6 α)γUm − 6 αγUm + 24
m
m
30
1 α)γ + (− 1 + 1 β)γU − ( 1 β + 1 α)γU 2 + ( 1 + 1 α)γU 3 + 1 βγU 4 − 1 αγU 5
( 13 − 18 β + 10
m
m
m
m
m
2
3
4
6
6
6
24
30
1
1
2 − 1 αγU 3 − 1 βγU 4 + 1 αγU 5
( 8 β − 10 α)γ − 13 βγUm + ( 41 β + 16 α)γUm
m
m
m
6
24
30

Table 1: Probabilities conditional on Rm and Um for different cases

6.

CONCLUSIONS AND FUTURE WORK

Predicting user click behavior is crucial to the sponsored
search business model. In this paper, we first presented
statistical analyses demonstrating that externalities exist in
this domain and that temporal click sequences are good indicators of externalities. We proposed the positional rationality hypothesis to explain both position bias and externalities. Based on this hypothesis, we further developed the
temporal click model.
We evaluated the proposed model on two real data sets.
We presented results showing that our model outperforms
one of the state of the art click models, BBM [18], on mid
to lower decile queries. We believe the results presented in
this paper are convincing evidence that the time and order
of user actions contain useful information about the user’s
click behavior.
We would like to extend our model to incorporate time
users spend on the landing pages, which could provide us
a direct measure of user satisfaction, and therefore improve
our estimate of ad quality. Another extension is to model
repeated clicks, which are not rare in click logs. Repeated
clicks could be viewed as the re-judgement of users on ad
quality. Being able to model them could be helpful to improve the accuracy of our click prediction.

[7]

[8]
[9]

[10]

[11]

[12]
[13]

[14]
[15]

[16]

7.

REFERENCES

[1] Eugene Agichtein, Eric Brill, and Susan Dumais. Improving
web search ranking by incorporating user behavior
information. SIGIR, 2006.
[2] H. K. Bhargava and Juan Feng. Paid placement strategies
for internet search engines. WWW, 2002.
[3] S. Boyd and L. Vandenberghe. Convex optimization. 2004.
[4] R. Briggs and N. Hollis. Advertising on the web: Is there
response before click-through. Journal of Advertising
Research, 37, 1997.
[5] B. Carterette and R. Jones. Evaluating search engines by
modeling the relationship between relevance and clicks.
NIPS, 2007.
[6] O. Chapelle and Y. Zhang. A dynamic bayesian network

[17]
[18]
[19]
[20]

113

click model for web search ranking. WWW, pages 1–10,
2009.
N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An
experimental comparison of click position-bias models.
WSDM, 2008.
J. Dean and S. Ghemawat. Mapreduce: Simplified data
processing on large clusters. OSDI, 2004.
Georges E. Dupret and Benjamin Piwowarski. A user
browsing model to predict search engine click data from
past observations. SIGIR, pages 331–338, 2008.
B. Edelman, M. Ostrovsky, and M. Schwarz. Internet
advertising and the generalized second price auction:
Selling billions of dollars worth of keywords. American
Economic Review, 97(1), 2007.
D. C. Fain and J. O. Pedersen. Sponsored search: A brief
history. Bulletin of the American Society for Information
Science and Technology, 32, 1997.
A. Ghosh and M. Mahdian. Externalities in online
advertising. WWW, 2008.
F. Guo, C. Liu, A. Kannan, T. Minka, M. Taylor, Y. Wang,
and C. Faloutsos. Click chain model in web search. WWW,
2009.
F. Guo, C. Liu, and Y. Wang. Efficient multiple-click
models in web search. WSDM, 2009.
T. Joachims, L. Granka, B. Pan, H. Hembrooke, and
G. Gay. Accurately interpreting clickthrough data as
implicit feedback. SIGIR, pages 154–161, 2005.
T. Joachims, L. Granka, B. Pan, H. Hembrooke,
F. Radlinski, and G. Gay. Evaluating the accuracy of
implicit feedback from clicks and query reformulations in
web search. ACM Transaction on Information System,
25(2), 2007.
D. Kempe and M. Mahdian. A cascade model for
externalities in sponsored search. Workshop on Ad
Auctions, 2008.
C. Liu, F. Guo, and C. Faloutsos. Bbm: Bayesian browsing
model from petabyte-scale data. KDD, 2009.
M. Richardson, E. Dominowska, and R. Ragno. Predicting
clicks: estimating the click-through rate for new ads.
WWW, pages 521–530, 2007.
Benyah Shaparenko, Özgür Çetin, and Rukmini Iyer.
Data-driven text features for sponsored search click
prediction. ADKDD, 2009.

