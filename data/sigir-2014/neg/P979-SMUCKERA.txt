Mouse Movement During Relevance Judging:
Implications for Determining User Attention
Mark D. Smucker, Xiaoyu Sunny Guo, and Andrew Toulis
Department of Management Sciences
University of Waterloo

mark.smucker | xiaoyu.guo | aptoulis @uwaterloo.ca
ABSTRACT

dication of how much or what parts of the summaries and
documents the users had focused their attention on.
Disappointed by the seemingly useless nature of the mouse
movements, we filed away this aspect of the user study as an
uninteresting negative result. In the subsequent years, researchers have continued to find that mouse position and eye
gaze are correlated in search tasks [1, 4, 5, 6, 7]. Given the
considerable success others have reported with using mousemovements, we decided to take a more careful look at the
data from our experiment and quantify the degree to which
the mouse movements gave some indication of user attention.
For our investigation, we first made a classification scheme
for the observed mouse movements, and then we annotated
6722 images of mouse movements according to our classification scheme. We found that:

Several researchers have found that a user’s mouse position
gives an indication of the user’s gaze during web search and
other tasks. As part of a user study that involved relevance judging of document summaries and full documents,
we recorded users’ mouse movements. We found that in
a large number of cases, the users did nothing more with
their mouse than move it to the buttons used for recording
the relevance decision. In addition, we found that different
search topics can result in large differences in the amount
of mouse movement that is indicative of user attention. For
simple reading tasks, such as short document summaries,
mouse-tracking does not appear to be an effective means of
discerning user attention. While more complex tasks may
allow mouse movements to provide information regarding
user attention, on average, indications of user attention existed in only 59% of the relevance judgments made for full
documents.

• The amount and nature of mouse movement changes
based on the amount of material presented to the user.
In particular, for short document summaries, only 24%
of the time was the mouse observed to move in a manner that might indicate what the user’s attention was
focused on. In contrast, for documents, on average
59% of the time the mouse gave some indication of
user attention.

Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]
Keywords: mouse tracking; relevance judging

1.

INTRODUCTION

Encouraged by research that showed a correlation between
the mouse and a user’s gaze [8], in late 2009 and early 2010,
we collected mouse positions as part of a user study [9] that
required users to judge the relevance of documents and document summaries. Our hope was that by collecting the position of the mouse, we could gain insight into user attention.
In effect, we hoped the mouse position could be a “poor
man’s eye-tracker” [2]. On inspection of the mouse movements, we discovered that for a large number of users, the
mouse told us nothing more than that the user moved the
mouse to click the relevant and not relevant buttons in the
user interface. We knew the users had read an appropriate
amount of the material to make reasonably good relevance
judgments, but for many of the judgments, we had no in-

• The amount of mouse movements that give some indication of user attention can vary greatly between
search topics. For example, a potentially easy to judge
search topic only produced mouse movements showing
some indication of user attention 40% of the time when
judging documents while a potentially difficult search
topic had movements indicating attention 82% of the
time.
Reflecting on these results, we are not surprised that the
document summaries show little mouse movement. A document summary consists of a document title and approximately two sentences of material. We cannot know about
how long the user truly attends to the document summary,
or how they read the document summary, but there is little
else on the page to distract them from reading the summary.
On the other hand, for full documents, we had hoped that
mouse tracking would work as a poor man’s eye-tracker, but
instead, 41% of the time all we know is that the user made
a relevance judgment.
Our study is unique in that we are not comparing mouse
movement to eye gaze, but instead, we are looking at mouse
movements as practitioners without eye-trackers would. In
addition, rather than reporting on user behavior with a

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2257-7/14/07 ...$15.00.
http://dx.doi.org/10.1145/2600428.2609489.

979

Number
310
336
362
367
383
426
427
436

Topic Title
Radio Waves and Brain Cancer
Black Bear Attacks
Human Smuggling
Piracy
Mental Illness Drugs
Law Enforcement, Dogs
UV Damage, Eyes
Railway Accidents

Table 1: Topics used in the study.

search engine results page (SERP), we are considering mouse
movement on pages consisting of text to read and buttons
to enter a relevance judgment. Our results give support to
the work of Cox and Silva [3] who showed that as web menu
search tasks included more distractors, users increasingly
used the mouse to “tag” potentially good selections. For the
mouse to be a good poor man’s eye-tracker, we apparently
need to present the user with consistently complex tasks.

2.

Figure 1:
An example of decision-only
re-scoping mouse movement (see Table 2).

and

Decision-Only: Mouse movement is classified as decision-only
when a user is seen to only click the “Relevant” or “Not Relevant”
buttons. Commonly, movement is from one button to another
or from the same button. As well, the user may move to one
of the buttons from a random point on the page, i.e. the user’s
only goal was to use the mouse to click a button. Figure 1 shows
an example of decision-only behavior.
Horizontal: This behavior occurs when the mouse moves back
and forth in the horizontal direction and appears to show the
user moving the mouse while reading lines of text. Behavior
should occur in more than one line of text in a row and movements should at least cover 25% of the text on that line. Figure 2
shows an example of horizontal behavior.
Vertical: This behavior occurs when the mouse moves in a continuous movement that does not have to occur on text but might.
It is identified by back-and-forth movement and/or stalling of
the mouse in the vertical direction. The movement of the mouse
gives the appearance that it is following the material that the
user is reading. Figure 3 shows an example of vertical behavior.
Highlighting: This behavior occurs when a user moves the
mouse from one position to a new one where some sort of information must be available. The mouse remains inactive in this
new area for some amount of time or rebounds off of it after a
short, but noticeable, period of time. The mouse may also move
back and forth very tightly in the new zone.
Re-scoping: When a new page loads, the old position of the
mouse may not be of use to the user, or the user may be seen
moving away from the judging buttons to possibly avoid accidental clicks. Moving the mouse to the side or to a new zone of
interest after loading a new page is known as re-scoping. Figure 1 shows an example of re-scoping behavior.
Scrolling: When a user scrolls the page, the mouse movement
shows up as long vertical or diagonal lines with no clear signs of
back and forth movement or staggering as can be shown in the
vertical behavior.
Random: Random movement can be treated simply as none
of the above mouse behaviors. Quite commonly, behaviors may
appear to be some form of interaction such as reading. However, if there is no clear definition for the behavior and it does
not resemble any of the classifications above, then it is random
movement. This category is reserved for larger, noticeable movements of the mouse that have no clear definition but given some
indication of the user’s attention.
No-Movement: It is possible for a user to make a judgment
without moving the mouse if the mouse is positioned above the
correct button for judging the current document.

METHODS AND MATERIALS

We used mouse movement data from a previous study [9].
In this study, the mouse movements of 48 participants were
recorded as they judged the relevance of documents and document summaries. Each participant judged 4 search topics
selected from the 8 topics shown in Table 1. The documents came from the AQUAINT news collection, and the
document summaries were in the style of search result snippets with the article’s title displayed as a link and up to
two sentences displayed as a query-biased summary. The
participants judged alternating summaries and documents,
which were displayed in orders with a uniform precision of
0.3 and 0.6, i.e. 3 or 6 of every 10 documents were relevant
as per the NIST relevance judgments. Topics and result list
precision were balanced across blocks of participants such
that each topic was judged by 24 participants, of which half
experienced a precision of 0.3, and the other half, 0.6. Participants worked on each topic for 10 minutes.
Mouse movements were captured using javascript in the
browser and sent in batched updates to the web server for
logging. At 0.1 second intervals, we recorded the position of
the mouse. We had one issue with our mouse-tracking code
whereby rather than poll the mouse position, we updated
the known position on every mouse move event. As a result,
if the user did not move the mouse, the variables holding
the mouse position had undefined as their values. Thus, the
position of the mouse was only known to us once the user
began to move the mouse. In many cases, the user never
moved the mouse to judge a document, for the user left the
mouse over the button used to judge the previous document
and the user’s judgment for this document was the same.
The participants used LCD monitors with a resolution of
1680 pixels wide by 1050 pixels high. At all times, the user
interface was maximized. The effective viewport for the web
page was 1680 by 915 pixels. We captured screenshots of the
full web page, which for long documents extended beyond
the bottom of the screen. The recorded mouse movements
were plotted on top of the screenshot with each recorded
location drawn as a small circle and subsequent locations
connected with a straight line. The starting position was

Table 2:
ments.

980

Classification scheme for mouse move-

Behavior
decision-only
horizontal
vertical
highlighting
scrolling
random
re-scope
no-indication
no-indication (doc)
no-indication (sum)

drawn with a green or yellow circle and the end position
with a red circle. A yellow circle was used to represent that
some positions had been recorded as undefined at the start
of this web page as described above. If the mouse stayed in
one position for an extended time, the corresponding circle
was increased in size and the time spent in that location
was output as well. For example, Figure 1 shows the mouse
moving from the “Relevant” button to a location below the
document summary and resting there for 14.59 seconds before the mouse was moved to the “Not Relevant” button,
where it stayed for 5.5 seconds before the page exited.
The second and third authors examined these images of
mouse movements for many different users and each separately developed a classification scheme for the types of
mouse movements they saw. The independent schemes were
discussed and then a final classification was jointly developed, which is shown in Table 2.
In addition the categories in Table 2, we define the mouse
as giving no indication of user attention when no mouse
movement was seen or when the category is decision-only
and none of the following categories is true: horizontal, vertical, highlighting, scrolling, and random.
The user study produced 6722 images of mouse movements. Each image represented a single study participant’s
interaction with the relevance judging interface for a single document summary or full document. The second and
third authors acted as annotators. We randomly selected
500 images to be judged by both the annotators and then
randomly divided the remaining images between the annotators. The jointly judged images allowed the annotators
to improve their consistency as well as allow us to measure
the degree to which they agreed in their judgments. The
annotators first judged 100 of their 500 shared images. After judging the first 100 images, the annotators discussed
differences in their judgments and slightly refined the classification scheme. The remaining 400 shared images occurred
at random amongst the remaining randomly ordered images.
For each of the categories in Table 2, we measured agreement between the annotators on the 500 images as overlap.
We did not use Cohen’s κ, for κ requires mutually exclusive
categories. Let A be the set of documents judged to be of a
given category by annotator A, and B the set of documents
judged to be of the same category by annotator B. The overlap for the two annotators is then equal to: |A ∩ B|/|A ∪ B|.
Table 3 shows the overlap for each of the categories in Table 2 as well as agreement for when the mouse gave no indication of user attention. Good agreement was obtained
for the decision-only category and likewise for no-indication
of user attention, which incorporates the decision-only category. Reasonable agreement existed for horizontal, vertical,
and scrolling behaviors, but identification of highlighting,
random, and re-scoping had low agreement.
For our final analysis, we randomly selected between the
two annotators’ judgments on their 500 shared images.

3.

Agreement
0.84
0.65
0.54
0.08
0.61
0.31
0.15
0.87
0.84
0.89

Table 3: Annotator agreement measured using overlap.

Figure 2: An example of horizontal mouse movement (see Table 2).

RESULTS AND DISCUSSION

Given the annotated images of mouse movements, we computed the average percent of images in which each of the
movement categories were seen. To compute these averages,
we first compute a study participant’s average for each topic,
and then average these averages. This method of averaging
is important because participants worked at their own rate
and judged different numbers of documents and summaries.

Figure 3: An example of vertical mouse movement
(see Table 2).

981

Topic
310
336
362
367
383
426
427
436
Average

Pct. Mouse Indication of Attention
Summaries
Documents
32%
77%
27%
63%
30%
59%
20%
37%
24%
82%
22%
58%
22%
59%
18%
40%
24%
59%

Table 5: Percent of judgments made with mouse
movements giving some indication of user attention.
Figure 4: An example of random mouse movement
(see Table 2).

Behavior
decision-only
horizontal
vertical
highlight
scrolling
random
re-scope
no-movement
no-indication

of the images judged to contain horizontal movement, and
only 21 participants produced any horizontal movement.

Pct. Mouse Behavior Observed
Summaries
Documents
67%
34%
6.5%
7.7%
0.3%
29%
4.0%
7.5%
2.0%
36%
12%
20%
14%
11%
10%
7.1%
76%
41%

4.

CONCLUSION

We tracked the mouse position of 48 study participants as
they judged the relevance of document summaries and full
documents. A significant fraction of the time, mouse movements gave no indication of user attention. Researchers hoping to use mouse movements as a “poor man’s eye-tracker”
should be certain to pilot the use of mouse-tracking and
confirm its correlation with gaze before use.

5.

ACKNOWLEDGMENTS

Chad Xu, Kevin Burt, and Michael Tatham wrote the
software to draw the mouse-tracking images.
This work was supported in part by the Natural Sciences
and Engineering Research Council of Canada (NSERC), in
part by an Amazon Web Services in Education Research
Grant, and in part by the University of Waterloo.

Table 4: Average percent of relevance judgments
made with different mouse behaviors. The category
“no-indication” represents when the mouse gives no
indication of the user’s attention.

6.

Table 4 shows the average percent of relevance judgments
made with the exhibited category of mouse movement. The
average study participant judges 76% of the document summaries without making any indication of what their attention is, and 41% of the full documents are also judged with
no indication of user attention. Evidence of horizontal movement is nearly the same for both summaries and documents,
and is likely because a subset of users frequently read using
their mouse as one would use a finger to trace under the
words as read, and this behavior occurs whether the user is
reading a short summary or a longer document. The other
behaviors increase for documents. The increase is likely a
result of the added length of documents and complexity of
finding relevant information in documents.
Table 5 shows that different topics have different amounts
of mouse movement that give some indication of user attention. In general, the topics with more indication of user
attention, e.g. 383, contain longer documents and/or require
careful searching of the document for relevant information.
While the percentages in Table 4 reflect the average user
on an average topic, they do not mean that for a given user
we should expect a certain percentage of judgments to show
a given mouse movement behavior. For example, horizontal
mouse movement is largely restricted to a subset of study
participants. Eleven of the 48 participants produced 90%

REFERENCES

[1] M. Ageev, D. Lagun, and E. Agichtein. Improving search
result summaries by using searcher behavior data. In SIGIR,
pages 13–22, 2013. ACM.
[2] L. Cooke. Is the mouse a “poor man’s eye tracker”? In 53rd
International STC Conference in Las Vegas, Nevada, pages
252–255, 2006.
[3] A. L. Cox and M. M. Silva. The role of mouse movements in
interactive search. In Proceedings of the 28th Annual
Meeting of the Cognitive Science Society, pages 1156–1161,
2006.
[4] Q. Guo and E. Agichtein. Towards predicting web searcher
gaze position from mouse movements. In SIGCHI Extended
Abstracts, pages 3601–3606, 2010. ACM.
[5] D. Hauger, A. Paramythis, and S. Weibelzahl. Using
browser interaction data to determine page reading
behavior. In UMAP, pages 147–158, 2011. Springer-Verlag.
[6] J. Huang, R. White, and G. Buscher. User see, user point:
Gaze and cursor alignment in web search. In SIGCHI, pages
1341–1350, 2012. ACM.
[7] J. Huang, R. W. White, and S. Dumais. No clicks, no
problem: Using cursor movements to understand and
improve search. In SIGCHI, pages 1225–1234, 2011. ACM.
[8] K. Rodden, X. Fu, A. Aula, and I. Spiro. Eye-mouse
coordination patterns on web search results pages. In
SIGCHI Extended Abstracts, pages 2997–3002, 2008. ACM.
[9] M. D. Smucker and C. Jethani. Human performance and
retrieval precision revisited. In SIGIR, pp. 595–602, 2010.

982

