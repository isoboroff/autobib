Hashing with List-wise Learning to Rank
Zhou Yu, Fei Wu, Yin Zhang, Siliang Tang, Jian Shao, Yueting Zhuang
College of Computer Science, Zhejiang University, China
{yuz, wufei, zhangyin98}@zju.edu.cn,
siliang.tang@gmail.com, {jshao, yzhuang}@zju.edu.cn

ABSTRACT

into compact hash codes so that similar data have the same
or similar hash codes.
One of the well-known hashing methods is the Locality
Sensitive Hashing (LSH) [1], which uses random projections
to obtain the hash functions. However, due to the limitation
of random projection, LSH usually needs a quite long hash
code to guarantee good retrieval performance. To alleviate
this weakness, several data-dependent learning based methods are proposed. Spectral Hashing (SH) [11] exploits the
distribution of the training data and uses eigenfunction to
obtain the hash functions. Compared with LSH, SH achieves
better performance since the learned hash functions better
disentangle the manifold structure of the data set.
Since semantic similarity is usually given in terms of labeled information (e.g., annotated tags or categories), semisupervised or supervised learning algorithms have been adapted to devise hash functions in order to make the learned hash
functions preserve the semantic similarity of the data set [9].
From the learning to rank (LTR) point of view, these aforementioned methods can be regarded as the point-wise supervised hashing since the supervision is on the instance-level
labels. However, it is observed that the point-wise hashing
methods are limited by the data distribution and the outlier data points may deteriorate the retrieval performance.
To overcome this limitation, Norouzi et al. introduced a
triplet supervision into hashing [6]. In this method, each
triplet {xi , xj , xk } is conducted to indicate the data point
xi is more relevant to the data point xj than the data point
xk . This kind of relative similarity in terms of triplets less
rely on the data distribution and thus generates more robust hash functions. Their method can be classified into
the category of pair-wise supervised hashing. Although the
point-wise and pair-wise hashing methods achieve promising performance, their objectives are sub-optimal for the
retrieval task since the ranking loss of the whole list-wise
permutation is disregarded. Therefore, we attempt to introduce the list-wise ranking supervision (i.e., given queries
and their corresponding true ranking lists) into hash function learning, which corresponds to the list-wise supervised
hashing. Since the main target of hashing is to retrieve
relevant results, the list-wise supervised hashing method is
more straightforward when comparing with point-wise and
pair-wise hashing approaches. To the best of our knowledge,
there are only several approaches focus on list-wise hashing.
Wang et al. proposed a hashing method supervised with
the list-wise ranking information when learning hash functions [10]. However, in their optimization, the total ranking
lists are transformed into various triplets, and the hashing

Hashing techniques have been extensively investigated to
boost similarity search for large-scale high-dimensional data.
Most of the existing approaches formulate the their objective as a pair-wise similarity-preserving problem. In this
paper, we consider the hashing problem from the perspective of optimizing a list-wise learning to rank problem and
propose an approach called List-Wise supervised Hashing
(LWH). In LWH, the hash functions are optimized by employing structural SVM in order to explicitly minimize the
ranking loss of the whole list-wise permutations instead of
merely the point-wise or pair-wise supervision. We evaluate the performance of LWH on two real-world data sets.
Experimental results demonstrate that our method obtains
a significant improvement over the state-of-the-art hashing
approaches due to both structural large margin and list-wise
ranking pursuing in a supervised manner.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information
Search and Retrieval

Keywords
Hashing; Learning to rank; Structural SVM

1.

INTRODUCTION

With the rapid development of the Internet and social
networks, an increasing number of multimedia data are produced at every moment, e.g., images and videos. Given a
query example, retrieving relevant samples from large-scale
database has become an emergent need in many practical
applications. An effective way to speed up the similarity
search is the hashing technique. The hashing techniques
often make a tradeoff between accuracy and efficiency and
therefore relax the nearest neighbor search to approximate
nearest neighbor (ANN) search. The underlying motivation
of the hashing techniques is to map high-dimensional data
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00.
http://dx.doi.org/10.1145/2600428.2609494.

999

function is in fact learned in a pair-wise supervised hashing
manner. As a result,this transformation potentially weakens
the ranking information. Furthermore, when the size of the
ranking list is large, the tremendous combinations of triplets
will make the problem intractable.
In this paper, we propose a list-wise supervised hashing
approach named LWH according to the given total ranking
list. Unlike [10] which transform the total ranking list into
triplets, we directly learn the hash functions to optimize
the ranking evaluation measures of the total ranking list
(e.g., MAP [12], NDCG[2]). In LWH, the hash functions are
learned by structural SVM [8] in order to explicitly minimize
the ranking loss of the whole list-wise permutation.

one based the Hamming distance [10].
φ(q, x) =

H(q)T H(x)
1
= H(q)T H(x)
kH(q)k · kH(x)k
k

By adapting the partial order feature representation, we
define a scoring function F parameterized by the hash functions H(·) to measure how compatible the ranking based on
hash codes are with the current predicted ranking y:
X X
φ(q, xi ) − φ(q, xj )
F (q, X, y) =
yij
|Xq+ | · |Xq− |
+
−
i∈Xq j∈Xq

=

X X
+

H(q)T [H(xi ) − H(xj )]
yij
k|Xq+ | · |Xq− |
−

(1)

i∈Xq j∈Xq

2.

THE MODEL OF LWH

Here, the scoring function F (q, X, y) is a summation over
the differences of all pairs of relevant-irrelevant results. Moreover, the scoring function F has a property that if the hash
functions H(·) (i.e., W ) is fixed, the ranking y that maximizes F (q, X, y) is simply obtained in descending order of
φ(H(q), H(xi )), (i ∈ {1, 2, ..., N }). This simple prediction
rule exactly satisfies the hash functions we expect to learn
since the descending order of φ(H(q), H(xi )) is equivalent to
the ascending order of the Hamming distance for each hash
code H(xi ) from the query hash code H(q).
However, due to the binary constraint of the hash functions, Eq.(1) is non-differentiable and hard to optimize. Following the strategy commonly used in the hashing approaches such as [11, 5], we relax the binarization function. In this
way, Eq.(1) can be rewritten as:

In this section, we first define notations that will be used
in this paper and briefly describe the problem to be solved.
Then we propose a list-wise supervised framework to learn
hash functions. Finally, we overview the entire framework
and clarify some implementation details.

2.1 Preliminary
Assume the training set X = {xi }N
i=1 has N data points
with each xi ∈ Rd corresponds to a d-dimensional feature
vector. The training set we used is in the query vs. retrieved
ranking list format. We have a query set Q = {qm }M
m=1
consisting of M queries. For each query q ∈ Q, we can derive
its true ranking y∗ ∈ Y over X, where Y denotes the set of
all possible rankings. We formulate a ranking as a matrix
of pair orderings as [12] does and Y ⊂ {+1, 0, −1}N×N . For
any y ∈ Y, yij = +1 if xi is ranked before xj , and yij = −1
if xj is ranked before xi , and yij = 0 if xi and xj has equal
rank. The true ranking y∗ for each query q is assumed to
be a weak ranking in this paper, which indicates two levels
of relevancy (relevant or irrelevant) w.r.t. q. Therefore, for
any query q, its true ranking list consists of two disjoint sets
Xq+ and Xq− representing the relevant and irrelevant results,
respectively.
After given the true ranking y∗ (in terms of semantic similarity) of each query q in training set, we attempt to learn
hash functions H : Rd → {−1, 1}k , where k is the dimensionality of Hamming space with k < d, to make the ranking
generated by the hash codes consistent with y∗ .

F (q, X, y) =

X X

yij

i∈Xq+ j∈Xq−

q T W W T (xi − xj )
k|Xq+ | · |Xq− |

=< W W T , Ψ(q, X, y) >F
where < A, B >F = tr(AT B) and
Ψ(q, X, y) = q

X X
i∈Xq+ j∈Xq−

2.2 Hash Function Learning with List-wise Supervision
Similar with most of the hashing approaches, we adopt
a linear formulation for designing hash functions. Assume
that the data have been zero-centered, the hash functions
H(·) are defined as follows:

yij

(xi − xj )T
k|Xq+ | · |Xq− |

By representing the scoring function F as a Frobenius
inner product of W T W and Ψ(·), we find that it is straightforward to conduct structural SVM [8] to learn F as well as
the projection matrix W of the hash functions.
For the purpose of listwise learning to rank, the structural
SVM takes a set of vector-valued features which characterize the relationship between the input query and a set of
retrieved data as the input, and predicts a ranking y ∈ Y of
the retrieved data. The structural SVM is applied to maximize the margins between the true ranking y∗ and all the
other possible rankings y.
For each q ∈ Q, we have:
∀y ∈ Y : δF (q, X, y∗ , y) ≥ ∆(y∗ , y) − ξ

H(x) = [h1 (x), ..., hk (x)] = B(W T x)

where δF (q, X, y∗ , y) = F (qi , X, yi∗ )−F (qi , X, yi ) is defined
for simplicity. ∆(y∗ , y) ∈ [0, 1] is the non-negative loss function to reflect the similarity of two ranking y∗ and y. Naturally, we can use many criteria to define the loss function
∆, such as ∆map [12], ∆ndcg [2], etc. Empirically, we choose
∆map due to its superior effectiveness and robustness over
the other criteria observed by [2].
Following the structural SVM framework, our LWH learns the optimal W which maximizes the margins between the

d×k

where W = [w1 , ..., wk ] ∈ R
is a linear projection matrix
and B(·) is a binarization function that maps real values into
binary values. Following the strategy which is common used
by [11][9], we simply use the sgn function with threshold at
0, i.e., B(x) = sgn(W T x).
To measure the similarity of two hash codes H(q) and
H(x), we use the following cosine-similarity based function
which has been proved to generate the identical ranking with

1000

true ranking and all other possible rankings. Specifically, we
replaced the traditional quadratic regularization term λ2 |w|22
in structural SVM with λ2 kW k2F to obtain a better generalization performance of the learned hash functions.
The overall objective function of the proposed LWH is
formulated as follows:
λ
kW k2F + ξ
min
W,ξ
2
s.t. ξ > 0
(2)
∀i ∈ {1, 2, ..., M }, ∀y ∈ Y :
δF (qi , X, yi∗ , yi ) ≥ ∆(yi∗ , yi ) − ξ

constraints in step 5. For the problem in step 5, different
loss functions ∆(y∗ , y) lead to different solutions. Recall
that we use ∆map in this paper, the work of [12] can be
easily applied to solve our problem of the step 5.
For the minimization problem of step 3, we implement a
sub-gradient descent solver adapted from Pegasos algorithm
[7], which is very efficient to solve the primal SVM problem.
The experiments show that the optimization yields a fast
convergence rate. For a fixed tolerance ǫ = 0.01, Algorithm
1 always terminates within 50 iterations in our experiments.
Since our LWH is derived from structural SVM, we do not
further analyze the complexity in this paper. The detailed
complexity analysis can be referred to [8].

where ξ is a slack variable over all the constrains. Compared
with the traditional SVM with n-slacks, the 1-slack method
shares a single slack variable ξ across all constraint batches,
which leads to less computation time.

3. EXPERIMENTS
In this section, we conduct experiments and comparison
analysis over two real-world data sets, i.e., CIFAR-10 1 and
NUS-WIDE 2 . To demonstrate of effectiveness of our LWH, we compare LWH with the state-of-the-art hashing
approaches.

2.3 Optimization and Implementation details
For each triplet (qi , X, yi∗ ) in the training set, there are
|Y| possible permutations which is super-exponential w.r.t.
N , thus can not be exactly optimized. To make this problem
tractable, we adopt the cutting-plane algorithm [3] to solve
the objective problem in Eq.(2).
The objective function in Eq.(2) is optimized by an iterative mechanism between the following two steps alternatively: 1) optimizing W with the current constraints set consisting of batches of rankings (y1 , ..., yM ) which most violate
the current constraints. 2) updating the constraints set and
adding a new batch which is mostly violated by current W .
The iterative procedure terminates once the empirical loss
on the new constraint batch is within a pre-defined tolerance
ǫ on the current constraints set. The overall optimization of
LWH is listed in Algorithm 1.

3.1 Data Sets
The CIFAR-10 data set contains 60,000 tiny images that
have been manually grouped into 10 concepts (e.g., airplane,
bird, cat, deer). The images are 32 × 32 pixels and we
represent them with 512-D GIST descriptors. Given a query
image, the images sharing the same concept with the query
image are regarded as the relevant ones.
The NUS-WIDE data set contains 269,648 labeled images
and is manually annotated with 81 concepts. The images are
represented with 500-D Bag of Visual Words (BOVW). Given a query image, the images sharing at least one common
concept with the query image are regarded as the relevant
ones.

Algorithm 1 The Optimization of LWH
Input: ranking triplets (qi , X, yi∗ ), i = 1, ..., M , trade-off
parameter λ, tolerance threshold ǫ
1: Initialize the constraints set W ← ∅
2: repeat
3:
λ
min
kW k2F + ξ
W,ξ
2
s.t. ξ > 0
∀{y1 , ..., yM } ∈ W :

3.2 Experimental Setup
We compare the proposed LWH with six stat-of-the-art
hashing methods including four unsupervised methods LSH
[1], SH [11], AGH [5], KLSH [4], one supervised(semi) method
SSH [9], and one list-wise supervised method RSH [10].
Except for the LSH and KLSH method which do not need
training samples, for the unsupervised methods (i.e., SH and
AGH), we randomly sample 3000 data points as the training
set; for the point-wise supervised method SSH, we additionally sample 1000 data points with their concept labels; for
the list-wise supervised methods (i.e., RSH and LWH), we
randomly sample 300 query samples from the 1000 labeled
samples to compute the true ranking list. In the test stage,
we use 2000 random samples as queries and the rest samples
as the database set to evaluate the retrieval performance.
To evaluate the performance, we adopt four criteria, i.e.,
Mean Average Precision (MAP), Precision within Hamming
distance 2 (PH2), Precision vs. Recall (PR), Recall vs. top
retrieved examples (Recall), respectively.
The parameters of all the comparing methods are tuned
on the validation set to achieve the best performance. For
LWH, the only parameter is the trade-off parameter λ in
Eq.(2). We test the settings of λ = {10−3 , 10−2 , 10−1 , 1} on
the validation set and find that λ = 10−2 is the best setting
for both data sets in our experiments.

M
M
1 X
1 X
δF (qi , X, yi∗ , yi ) ≥
∆(yi∗ , yi ) − ξ
M i=1
M i=1

4:
5:

for i = 1 to M do
ŷi ← argmax ∆(yi∗ , y) + F (qi , X, y)
y∈Y

6:
end for
7:
W ← W ∪ {(ŷ1 , ..., ŷM )}
8: until
M
1 X
(∆ (yi∗ , ŷi ) − δF (qi , X, yi∗ , ŷi )) ≤ ξ + ǫ
M i=1

Output: The optimized projection matrix W

1

The main parts of the optimization in Algorithm 1 is the
minimization in step 3 and the searching of the most violated

2

1001

http://www.cs.toronto.edu/~kriz/cifar.html
http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm

0.16

0.2
0.15
0.1

0.4

1

SH
KLSH
AGH
LSH
SSH
RSH
LWH

0.3
0.2

0.8
Recall

MAP

0.2
0.18

SH
KLSH
AGH
LSH
SSH
RSH
LWH

0.25

Precision

SH
KLSH
AGH
LSH
SSH
RSH
LWH

PH2

0.22

0.1

0.14

32

48

64

96

0
16

128

32

48

# of hash bits

64

96

0
0

128

# of hash bits

(a) MAP

(b) PH2

0.4
0.2

0.05

0.12
16

SH
KLSH
AGH
LSH
SSH
RSH
LWH

0.6

0.5
Recall

0
0

1

(c) PR @ 128 bits

2
4
6
# of top retrieved examplesx 104

(d) Recall @ 128 bits

Figure 1: The results on CIFAR-10 data set.

0.3
0.25
0.2
0.15

0.3

0.4
0.35

16

0.8

0.3

0.1
0.29

1

SH
KLSH
AGH
LSH
SSH
RSH
LWH

Recall

0.31

SH
KLSH
AGH
LSH
SSH
RSH
LWH

0.35

PH2

0.32

MAP

0.45

0.4
SH
KLSH
AGH
LSH
SSH
RSH
LWH

0.33

Precision

0.34

0.6
0.4
0.2

0.05
32

48

64

96

128

0
16

# of hash bits

32

48

64

96

128

0.25
0

# of hash bits

(a) MAP

(b) PH2

0.5
Recall

1

(c) PR @ 128 bits

0
0

SH
KLSH
AGH
LSH
SSH
RSH
LWH
0.5
1
1.5
2
# of top retrieved examplesx 105

(d) Recall @ 128 bits

Figure 2: The results on NUS-WIDE data set.

3.3 Results Discussion

6. REFERENCES

Figure 1 and 2 demonstrate the performance of LWH against the other comparative methods on CIFAR-10 and
NUS-WIDE data sets, respectively.
From the results, we have the following observations: 1)
the results on both data sets show that LWH outperforms the other methods significantly; 2) supervised or semisupervised approaches such as SSH, RSH and LWH outperform the unsupervised approaches. The observation may
be explained as hashing function learned by the unsupervised approaches only preserve the similarity of original data, but do not actually preserve the semantic similarity; 3)
among the supervised approaches, the methods incorporate
list-wise supervision (i.e., LWH and RSH) have a remarkable
enhancement due to their aptitude to minimize the listwise
ranking loss.

[1] A. Andoni and P. Indyk. Near-optimal hashing
algorithms for approximate nearest neighbor in high
dimensions. In FOCS, pages 459–468, 2006.
[2] S. Chakrabarti, R. Khanna, U. Sawant, and
C. Bhattacharyya. Structured learning for non-smooth
ranking losses. In SIGKDD, pages 88–96, 2008.
[3] T. Joachims, T. Finley, and C.-N. J. Yu.
Cutting-plane training of structural svms. Machine
Learning, 77(1):27–59, 2009.
[4] B. Kulis and K. Grauman. Kernelized
locality-sensitive hashing for scalable image search. In
ICCV, pages 2130–2137, 2009.
[5] W. Liu, J. Wang, S. Kumar, and S. Chang. Hashing
with graphs. In ICML, pages 1–8, 2011.
[6] M. Norouzi, D. Fleet, and R. Salakhutdinov.
Hamming distance metric learning. In NIPS, pages
1070–1078, 2012.
[7] S. Shalev-Shwartz, Y. Singer, N. Srebro, and
A. Cotter. Pegasos: Primal estimated sub-gradient
solver for svm. Mathematical Programming,
127(1):3–30, 2011.
[8] I. Tsochantaridis, T. Joachims, T. Hofmann, and
Y. Altun. Large margin methods for structured and
interdependent output variables. In JMLR, pages
1453–1484, 2005.
[9] J. Wang, S. Kumar, and S. Chang. Semi-supervised
hashing for scalable image retrieval. In CVPR, pages
3424–3431, 2010.
[10] J. Wang, W. Liu, A. Sun, and Y. Jiang. Learning hash
codes with listwise supervision. In ICCV, 2013.
[11] Y. Weiss, A. Torralba, and R. Fergus. Spectral
hashing. In NIPS, pages 1753–1760, 2008.
[12] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A
support vector method for optimizing average
precision. In SIGIR, pages 271–278, 2007.

4.

CONCLUSIONS

In this paper, we propose a hashing approach leveraging
the list-wise supervision in a max-margin learning manner.
In LWH, the hash functions are optimized by employing
structural SVM in order to explicitly minimize the ranking loss of the whole list-wise permutation. The results over
two real-world data sets demonstrates the superiority of the
proposed LWH over the existing state-of-the-art hashing approaches.

5.

ACKNOWLEDGEMENT

This work was supported in part by 973 program (No.
2010CB327904), Chinese Knowledge Center of Engineer-ing
Science and Technology (CKCEST), NSFC (No. 61105074,
61103099), 863 program(2012AA012505) , Program for New
Century Excellent Talents in University and Zhejiang Provincial Natural Science Foundation of China (No. LQ13F020001,
LQ14F010004).

1002

