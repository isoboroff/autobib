Probabilistic Text Modeling with Orthogonalized Topics
Enpeng Yao1 , Guoqing Zheng2 , Ou Jin1 , Shenghua Bao3 ,
Kailong Chen1 , Zhong Su3 , Yong Yu1
1

Shanghai Jiao Tong University, Shanghai, 200240, China
2
Carnegie Mellon University, PA, 15213, USA
3
IBM China Research Laboratory, Beijing, 100094, China
{yaoenpeng, kingohm, chenkl, yyu}@apex.sjtu.edu.cn
gzheng@cs.cmu.edu, {baoshhua, suzhong}@cn.ibm.com

ABSTRACT

which facilitates further applications such as classiﬁcation,
clustering and retrieval. Traditional topic models, including
Probabilistic Latent Semantic Analysis (PLSA) [7], Latent
Dirichlet Allocation (LDA) [1] and many of their variations,
most of which put certain constraints on the document-topic
distributions based on speciﬁc considerations, have enjoyed
impressive success in tasks [2, 9], etc.
Topic models usually contain two parts of parameters,
which are documents-topic distributions and topic-word distributions, and intuitively topic-word distributions also play
an important role in the whole model. A model with better topic-word distributions can mine more diversiﬁed and
reasonable topics, and therefore it may achieve better performance on many text mining tasks, like text classiﬁcation and
clustering. To the best of our knowledge, none of previous
topic models pay any extra consideration on the topic-word
distributions, such as orthogonalizing them, which is to be
discussed in this paper.
Orthogonalizing techniques have been widely used in dimension reduction models [6, 8]. So we hope adding the orthogonality constraint to the topic models also can achieve
performance improvement.
In this paper, we propose a new topic model, the Orthogonalized Topic Model (OTM), to focus on orthogonalizing the
topic-word distributions. In order to address the importance
of orthogonalized topics, we put a regularized factor measuring the degree of topic orthogonalities to the objective function of PLSA. The OTM model is able to take advantage
of statistical foundation of PLSA without losing orthogonal
property of LSA. We present an eﬃcient algorithm to solve
the proposed regularized log-likelihood maximization problem using the Expectation-Maximization(EM) algorithm [5]
and the Newton-Raphson method.
To evaluate the proposed model, we apply it to two widely
used text corpora on the classiﬁcation task. The higher text
classiﬁcation accuracy based on OTM further stresses the
important role played by topic orthogonalization.

Topic models have been widely used for text analysis. Previous topic models have enjoyed great success in mining the
latent topic structure of text documents. With many efforts made on endowing the resulting document-topic distributions with diﬀerent motivations, however, none of these
models have paid any attention on the resulting topic-word
distributions. Since topic-word distribution also plays an
important role in the modeling performance, topic models
which emphasize only the resulting document-topic representations but pay less attention to the topic-term distributions are limited. In this paper, we propose the Orthogonalized Topic Model (OTM) which imposes an orthogonality constraint on the topic-term distributions. We also propose a novel model ﬁtting algorithm based on the generalized Expectation-Maximization algorithm and the NewthonRaphson method. Quantitative evaluation of text classiﬁcation demonstrates that OTM outperforms other baseline
models and indicates the important role played by topic orthogonalizing.

Categories and Subject Descriptors
H.3.1 [Information Storage and Retrieval]: Content
Analysis and Indexing—Indexing methods

Keywords
Probabilistic Text Modeling; Latent Semantic Analysis; Text Classiﬁcation

1. INTRODUCTION
Topic modeling has attracted increasing attention and
widely used for data analysis in many domains including
text documents. By assuming that each document is modeled by a distribution over a certain number of topics, each
of which is a distribution over words in the vocabulary, topic
models can reveal the underlying latent semantic structure

2.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00.
http://dx.doi.org/10.1145/2600428.2609471.

2.1

BACKGROUND AND NOTATIONS
Latent Semantic Analysis

Latent Semantic Analysis (LSA) [4], as one of the most
useful tools for learning the latent concepts from text, has
widely been used in the dimension reduction task. Speciﬁcally, given a term-document matrix X ∈ RN ×M , where N is
the number of words in the vocabulary and M is the number
of documents in the corpus, by using singular value decom-

907

position (SVD), LSA tries to ﬁnd a low-rank construction of
X such that
X ≈ UΣVT

as possible. The constraint we proposed is to minimize the
following orthogonalization factor:

(1)

O=

where Σ ∈ RK×K is a diagonal matrix containing the K
largest singular values of X. Orthogonal matrices U ∈
RN ×K (UT U = I) and V ∈ RM ×K (VT V = I) represent
word and document embeddings onto the latent space.

• Select a document di with probability P (di );

Minimizing this objective function will directly enforce the
topic-word distributions of diﬀerent topics to be orthogonal.
One can conclude from the above factor that as O decreases
to 0 all the topics are more likely to be orthogonal with other
topics.

3.2

Objective Function of OTM

Preserving the statistical properties of the resulting topicword distributions while ensuring that the resulting topicword distributions are as orthogonal to others as possible, we
formulate OTM as combining the statistical foundation of
PLSA and the orthogonalized constraint mentioned above.
Formally, OTM tries to maximize the following objective
function:
Q =L − λO

• Pick a latent topic zk with probability P (zk |di );

=

• Generate a word wj with probability P (wj |zk ).

M ∑
N
∑

(5)
n(di , wj ) log

i=1 j=1

By summing all the latent variable zk , the joint probability of an observed pair (di , wj ) can be computed as

−λ

K
∑

P (wj |zk )P (zk |di )

k=1

K ∑
K ∑
N
∑

P (wj |zk )P (wj |zk′ )

k=1 k′ =1 j=1
k′ ̸=k

P (wj |zk )P (zk |di )

where λ ≥ 0 is the regularization parameter. Note that if
λ = 0 OTM boils down to PLSA and we assume λ > 0
thereafter.

k=1

(2)
So we can calculate the data log-likelihood as
)
(
M ∑
N
K
∑
∑
L=
n(di , wj ) log P (di )
P (wj |zk )P (zk |di )
i=1 j=1

(4)

k′ ̸=k

Probabilistic Latent Semantic Analysis (PLSA) [7] deﬁnes
a proper generative model based on a solid statistical foundation.
Given a corpus that consists of M documents {d1 , d2 , ..., dM }
with words from a vocabulary of N words {w1 , w2 , ..., wN },
PLSA, by assuming that the occurrence of a word w in a
particular document d is assigned with one of K latent topic variables {z1 , z2 , ..., zK }, deﬁnes the following generative
process:

P (di , wj ) = P (di )P (wj |di ) = P (di )

P (wj |zk )P (wj |zk′ )

k=1 k′ =1 j=1

2.2 Probabilistic Latent Semantic Analysis

K
∑

K ∑
K ∑
N
∑

3.3

Model Fitting with EM

When a model involves unobserved latent variables, the
EM algorithm is the standard procedure for maximum likelihood estimation. EM alternates between two steps: (a)
an Expectation(E) step where posterior probabilities of the
latent variables are computed given the current estimates of
the parameters, (b) a maximization(M) step where parameters are updated by maximizing the expected complete data
log-likelihood given the posterior probabilities computed in
the E-step. The parameters of OTM are P (zk |di ), P (wj |zk ).
Thus, M K + N K parameters are to be estimated, which is
the same as PLSA.
E-step: The E-step of OTM is exactly the same as that
of PLSA. The posterior probabilities for the latent variables
P (zk |di , wj ) can be computed using applying Bayes’ formula.
P (zk |di )P (wj |zk )
P (zk |di , wj ) = ∑K
(6)
′
′
′
k =1 P (zk |di )P (wj |zk )

k=1

(3)
where n(di , wj ) denotes the number of times word wj occurs
in document di . Following the principles of maximum likelihood estimation, we can estimate P (wj |zk ) and P (zk |di ).
According to the above modeling, PLSA successfully endows its resulting document-topic and topic-word assignments a statistical meaning which LSA lacks; however, it
discards the orthogonal properties of the resulting dimensions originally possessed by LSA.

3. ORTHOGONALIZED TOPIC MODEL
In this section, we ﬁrstly introduce the measure to evaluate the degree to which topics are orthogonal to each other
and then formalize our proposed model, named Orthogonalized Topic Model (OTM). We also present an eﬃcient
algorithm to solve the proposed optimization function using
the Expectation Maximization (EM) algorithm [5].

M-step: In the M-step of OTM, we improve the expected
value of the complete data log-likelihood which is

3.1 Orthogonality Measure

Q=

In order to measure the degree to which topics are orthogonal to each other without harming their probabilistic property, the orthogonal constraint in LSA which ensures that
each column vector of the resulting dimension are strictly
orthogonal to other column vectors, such as UT U = I and
VT V = I, can no longer be used. Instead, in this paper, we
put a soft orthogonality constraint on the topic-word distributions in order to force topics as orthogonal to each other

M ∑
N
∑

n(di , wj ) log

i=1 j=1

−λ

K
∑

P (wj |zk )P (zk |di )

k=1

K ∑
K ∑
N
∑

P (wj |zk )P (wj |zk′ )

k=1 k′ =1 j=1
k′ ̸=k

with the constraints
∑N
j=1 P (wj |zk ) = 1.

908

∑K
k=1

P (zk |di ) = 1 and

(7)

The M-step re-estimation for P (zk |di ) is exactly the same
as those of PLSA because the orthogonalized term of OTM
does not include P (zk |di ).
∑N
j=1 n(di , wj )P (zk |di , wj )
P (zk |di ) =
(8)
∑N
j=1 n(di , wj )

4.1

Yahoo! News K-series is a collection consisting of 2,340
news articles belonging to 20 diﬀerent categories. The numbers of articles per categories are almost the same. We get
the preprocessed version with a vocabulary size of 8104 from
D. L. Boley’s page1 . The Reuters-21578 dataset is a corpus
of news stories made available by Reuters, Ltd. The preprocessed version is downloaded from R. F. Corrêa’s web page2
which consists of 9,821 documents and 5,180 distinct words.
Table 1 shows the statistical details of the two datasets.
All experiments are conducted on a computer with 2.6GHz
Pentium Dual Core CPU and 2 GB physical memory.

Now we turn to the re-estimation part for P (wj |zk ). Maximization of Q with respect to P (wj |zk ) leads to the following
set of equations:
∂Q
(9)
∂P (wj |zk )
∑M
K
∑
i=1 n(di , wj )P (zk |di , wj )
−λ
P (wj |zk′ ) − αk
P (wj |zk )
′

=

4.2

= 0
where αk ≥ 0 is the Lagrange multiplier for topic k. So we
get
∑M
i=1 n(di , wj )P (zk |di , wj )
(10)
P (wj |zk ) =
∑
αk + λ K
k′ =1 P (wj |zk′ )
Letting

k′ ̸=k

P (wj |zk ) = 1 we get
N ∑M
∑
i=1 n(di , wj )P (zk |di , wj )
=1
∑K
α
k +λ
k′ =1 P (wj |zk′ )
j=1

j=1

(11)

k′ ̸=k

Due to the coupling between the summation in the denominator and the summation outside the fraction, it is not easy
to give an analytic solution for the P (wj |zk ) in Eq.(10).
Thus we turn to ﬁnd a numerical solution. We apply NewtonRaphson method to calculate αk from (11) as below:
αkn+1 = αkn −
where
fk (x) =

N
∑
j=1

fk (αkn )
fk′ (αkn )

• Using raw word features without any dimension reduction as input for SVM
• Laplacian Probabilistic Semantic Analysis (LapPLSA)
[2]

(12)

LapPLSA is a topic model based on PLSA with the modiﬁcation on the document-topic distribution. It assumes that
the document space is a manifold, either linear or nonlinear. By adding a term related to document-topic distribution as regularization to the objective function of PLSA,
this model achieves high performance on the text clustering
task. We choose LapPLSA as a representation which add
constraints on the resulting document-topic distribution to
compare with our OTM which endows the resulting topicwords distribution. We use the source code downloaded from
D. Cai’s webpage3 .
For LSA, PLSA, LapPLSA, the dimension reduction results of each document are set as the input for SVM, similar
to OTM. For each model, we vary the number of latent topics K from 10 to 100. We report the average of classiﬁcation
accuracies after 10 test runs.
Figures 1 and 2 demonstrate the classiﬁcation performance
of OTM and other baseline models. On both text sets, OTM
outperforms LSA, PLSA, LapPLSA in terms of classiﬁcation
accuracies due to the orthogonality of the topics. This indicates that the OTM model, which combines the statistical
foundation of PLSA and the orthogonalized constraint, improves topic representation of documents to a certain degree.

∑M

i=1 n(di , wj )P (zk |di , wj )
−1
∑
x+λ K
k′ =1 P (wj |zk′ )

Classification Evaluation

We evaluate the performance of OTM on the tasks of document classiﬁcation using the method similar to [9]. To address real-world problems in a semi-supervised setting, We
ﬁrst using OTM to generate the document-topic and wordtopic distributions from the whole dataset, and randomly select a small number of documents (one of 10,20,and 40) from
each category as labeled for training and use the remaining
documents for test. The selected training data number from
each category are the same. Then a linear kernel support
vector machine (SVM) [3] is trained on the document-topic
representations of the training set and make prediction on
the test set. This means that we use the rank-reduced representations of each document (the document-topic distributions) as the input space of SVM, and the output space
is the categories of documents.
Beside the LSA and PLSA, we also provide two more baselines:

k =1
k′ ̸=k

∑N

Datasets and Experimental Settings

(13)

k′ ̸=k

αkn is the value of αk in the nth iteration. We initialize
αk with a random real number αk0 and get the root of the
function fk , or the numerical solution of αk by updating
the value of αk iteratively until a suﬃciently
accurate val∑
ue is reached. By noting that αk , λ K
k′ =1 P (wj |zk′ ) and
k′ ̸=k
∑M
i=1 n(di , wj )P (zk |di , wj ) are non-negative real numbers,
it is easy to prove that the left hand side of Eq.(11) is a
monotone function for the parameter αk . This indicates
that there is only one accurate numerical solution of αk ,
with nothing to do with the initial number αk0 . By using
the value of ak , P (wj |zk ) is calculated by Eq.(10). After obtaining P (zk |di ) and P (wj |zk ) in the M-step, our algorithm
continues to calculate the EM step cycles, until the value of
objective function (5) converges.

4. EXPERIMENTS
To evaluate the eﬀectiveness of the proposed OTM model,
we apply it on two widely used text corpora, Yahoo! News
K-series and Reuters 21578 corpora. We will report the
quantitative evaluation of document classiﬁcation based on
the topics extracted from various topic models.

1

http://www-users.cs.umn.edu/~boley/ftp/PDDPdata/
https://sites.google.com/site/renatocorrea02/
3
http://www.zjucadcg.cn/dengcai/LapPLSA/index.html
2

909

Yahoo News K
Reuters-21578
Number of labeled documents for training = 10

60

70
60
55

45

Accuracy(%)

40
35
30
25

OTM
PLSA
LapPLSA
LSA
Word

20
15
10
20

30

40

50

70

Number of topics

50

40
35
30

OTM
PLSA
LapPLSA
LSA
Word

25
20
15
10

0
10

45
40
35

OTM
PLSA
LapPLSA
LSA
Word

30
25
20
15
10

5
100

Number of labeled documents for training = 40

65

50

45

Accuracy(%)

Number of labeled documents for training = 20

55

50

5
10

avg. terms/doc
105.7
70.3

Accuracy(%)

55

Table 1: Datasets details
# of docs # of terms # of labels
2,340
8,104
20
9,821
5,180
20

5
20

30

40

50

70

0
10

100

Number of topics

20

30

40

50

70

100

Number of topics

Figure 1: Classification performance on Yahoo! News K-series. The “number of labeled documents for
training” is per category.
80
75

70

70

65

65

60

60

55

55

50
45

OTM
PLSA
LapPLSA
LSA
Word

40
35
30
25
20

50

70

Number of topics

65

OTM
PLSA
LapPLSA
LSA
Word

35
30
25
20
10
10

40

70

40

15
100

Number of labeled documents for training = 40

75

45

10
10

30

80

50

15
20

Number of labeled documents for training = 20

Accuracy(%)

Number of labeled documents for training = 10

75

Accuracy(%)

Accuracy(%)

80

20

30

40

50

70

Number of topics

60
55
50
45

OTM
PLSA
LapPLSA
LSA
Word

40
35
30
25

100

20
10

20

30

40

50

70

Number of topics

100

Figure 2: Classification performance on Reuters-21578. The “number of labeled documents for training” is
per category.

5. CONCLUSION AND FUTURE WORK

[3] C. Cortes and V. Vapnik. Support-vector networks. In
Machine Learning, pages 273–297, 1995.
[4] S. Deerwester, S. Dumais, T. Landauer, G. Furnas, and
R. Harshman. Indexing by latent semantic analysis.
JASIS, 41(6):391–407, 1990.
[5] A. P. Dempster, N. M. Laird, and D. B. Rubin.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society.
Series B, 39(1):1–38, 1977.
[6] I. S. Dhillon and D. S. Modha. Concept decompositions
for large sparse text data using clustering. In Machine
Learning, pages 143–175, 2000.
[7] T. Hofmann. Probabilistic latent semantic indexing. In
Proceedings of SIGIR 1999, pages 50–57, 1999.
[8] P. O. Hoyer and P. Dayan. Non-negative matrix
factorization with sparseness constraints. Journal of
Machine Learning Research, 5:1457–1469, 2004.
[9] S. Huh and S. E. Fienberg. Discriminative topic
modeling based on manifold learning. In Proceedings of
the 16th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 653–662,
2010.

In this paper, we propose an Orthogonalized Topic Model
(OTM) which aims to increase the resulting topic diversity
by putting a constraint on topic-word distributions to orthogonalize the topics. The consideration to constrain the
topic-word distribution has not been made in previous works. We formulate the model as a maximization problem and
ﬁt the model eﬃciently using the EM algorithm and the
Newton-Raphson method. We conduct experiments on two
real world text corpora. The application of topic models to
text classiﬁcation veriﬁes the eﬀectiveness of the proposed
model.
In future, we plan to add the orthogonality constraint into
other topic modeling algorithms such as Latent Dirichlet
Allocation(LDA) to improve their performance. In another
direction, we will use some other measure to express the
orthogonality and obtain faster optimization process.

6. REFERENCES
[1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent
dirichlet allocation. Journal of Machine Learning
Research, 3:993–1022, 2003.
[2] D. Cai, Q. Mei, J. Han, and C. Zhai. Modeling hidden
topics on document manifold. In Proceeding of the 17th
ACM conference on Information and knowledge
management (CIKM’08), pages 911–920, 2008.

910

