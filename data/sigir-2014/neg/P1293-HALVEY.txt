SIGIR 2014 Workshop on Gathering Efficient Assessments
of Relevance (GEAR)
Martin Halvey

Robert Villa

Paul Clough

Interactive and Trustworthy
Technologies Group
School of Engineering and Built
Environment
Glasgow Caledonian University, UK

Information Retrieval Group
Information School
University of Sheffield, UK

Information Retrieval Group
Information School
University of Sheffield, UK
p.d.clough@sheffield.ac.uk

r.villa@sheffield.ac.uk

martin.halvey@gcu.ac.uk

H3.3 [Information Search and Retrieval]

to revisit and discuss some of the foundational IR work on
relevance in light of recent research. Some questions to be
addressed include: What techniques or mix of techniques can be
used to most efficiently create relevance judgments? For modern
IR, should binary relevance judgment be used, or graded? How
difficult is judging relevance for different types of individual,
different types of content, and different types of topic? And
finally, can we learn more about how assessors go about the
relevance judging process, and apply this knowledge to more
efficiently create sets of relevance judgments?

Keywords

2. SCOPE OF WORKSHOP

ABSTRACT
Evaluation is a fundamental part of Information Retrieval, and in
the conventional Cranfield evaluation paradigm, sets of relevance
assessments are a fundamental part of test collections. This
workshop revisits how relevance assessments can be efficiently
created, seeking to provide a forum for discussion and exploration
of the topic.

Categories and Subject Descriptors

Relevance assessment; Evaluation; Test collections

The workshop focuses on how relevance assessments can be
efficiently created, including: how the method of generating
assessments, via conventional means or crowdsourcing, affects
the judgments gathered, such as issues of assessor expertise and
payment; the process by which individuals, or groups of
individuals, assess documents; issues relating to the effort
required to generate relevance assessments for different types of
topic, and different types of material (text, web, image, video, etc.
and multiple languages); and to revisit the concept of “relevance”,
from a practical, operational standpoint, for the purposes of IR
evaluation.

1. OVERVIEW
Evaluation has always played a vital part in Information Retrieval,
exemplified by the importance of TREC and other similar efforts,
such as CLEF and NTCIR. The problems and issues surrounding
the building of test collections have been studied in detail by
many researchers, and in particular, the problem of creating
relevance assessments.
A wide variety of methods have been proposed for creating
relevance assessments, including the sampling of documents from
judgment pools, the use of interactive search and judge (ISJ), the
simulation of queries and relevance judgments based on search
logs, and crowdsourcing. Relevance, has, of course, been
extensively studied from a wide variety of perspectives. The
dynamic human judgment process during searching has also been
studied in great detail, as part of the overall information seeking
process. The focus has often been on the criteria by which users
(rather than assessors) judge the relevance of a document to a
task, but much of this work has also investigated how a user’s
conception of relevance changes over time, as the search process
develops. Less work in Information Retrieval, however, has
focused in detail on individual assessor behavior, with a notable
exception of [1], while [2] investigated the effort required by
assessors to judge the relevance of different types of document.

The aim of the workshop will be to provide a forum where short
research papers can be presented, reporting work which may not
conventionally be published in papers at formal venues, including
“practice and experience” papers concerning relevance assessment
gathering, and position papers concerning the concepts and issues.

3. Acknowledgements
This work was supported in by the Arts & Humanities Research
Council UK, Digital Transformations in the Arts and Humanities
(grant AH/L010364/1).

2. REFERENCES
[1] Al-Harbi, A. L. and Smucker, M. D. User Expressions of
Relevance Judgment Certainty. 7th Annual Symposium on
Human-Computer Interaction and Information Retrieval
(HCIR 2013)

Given the importance of relevance assessments to IR, and the rise
of new methods of gathering relevance assessments, there is need

[2] Villa, R. and Halvey, M. Is relevance hard work?: evaluating
the effort of making relevant assessments. In Proceedings of
the 36th international ACM SIGIR conference on Research
and development in information retrieval. (Dublin, Ireland).
ACM, New York, NY, USA, 2013, 765-768.

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the owner/author(s). Copyright is held by the
author/owner(s).
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
ACM 978-1-4503-2257-7/14/07.

http://dx.doi.org/10.1145/2600428.2600735

1293

