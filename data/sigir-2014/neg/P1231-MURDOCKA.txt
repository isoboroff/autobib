Dynamic Location Models
Vanessa Murdock
Microsoft

vanmur@microsoft.com

ABSTRACT

Geotagged social media is a rich source of information
about locations. Users of services such as Foursquare1 and
Flickr2 provide text, images, and tags describing the places
they visit in their every day lives, associated with the geographic coordinates of the location. Location models built
on social media have been shown to be an important step
toward understanding places in queries, but most systems
look at large areas of 100 square kilometers, or larger, which
is roughly the size of a metropolitan area. This is because
the coverage in social media in general is sparse, and models of places as small as one square kilometer are not rich
enough to make accurate predictions.
We propose dynamic location models (DLMs), which leverage the fact that social media is not uniformly distributed
across all locations. In fact, within a city, there will be
small clusters of data associated with specific businesses,
landmarks and other points of interest. For example, in the
city of Paris, there will be hundreds of thousands of images
posted to Flickr from around the city, but there will be dense
clusters associated with the Eiffel Tower, the Cathedral of
Notre Dame, the Louvre, and other popular places. Popular points of interest will have sufficient coverage that they
can support their own language model, distinct from their
surroundings.
In this work we propose an algorithm for constructing hyperlocal models of places that are as small as half a city
block. We show that DLMs are computationally efficient,
and provide better estimates of the language models of hyperlocal places than the standard method of segmenting the
globe into approximately equal grid squares. We evaluate
the models using a repository of 25 million geotagged public
images from Flickr. We show that the indexes produced by
DLMs have a larger vocabulary, and smaller average document length than their fixed grid counterparts, for indexes
with an equivalent number of locations. This produces location models that are more robust to retrieval parameters,
and more accurate in predicting locations in text.
In the remainder of this paper we describe the related
work in Section 2, and the algorithm for estimating DLMs
in Section 3. We evaluate the system in Sections 4 and 5.
Finally we end with a discussion of the results and a conclusion in Sections 6 and 7.

Location models built on social media have been shown to be
an important step toward understanding places in queries.
Current search technology focuses on predicting broad regions such as cities. Hyperlocal scenarios are important because of the increasing prevalence of smartphones and mobile search and recommendation. Users expect the system
to recognize their location and provide information about
their immediate surroundings.
In this work we propose an algorithm for constructing hyperlocal models of places that are as small as half a city
block. We show that Dynamic Location Models (DLMs) are
computationally efficient, and provide better estimates of
the language models of hyperlocal places than the standard
method of segmenting the globe into approximately equal
grid squares. We evaluate the models using a repository of
25 million geotagged public images from Flickr. We show
that the indexes produced by DLMs have a larger vocabulary, and smaller average document length than their fixed
grid counterparts, for indexes with an equivalent number of
locations. This produces location models that are more robust to retrieval parameters, and more accurate in predicting
locations in text.

Keywords
Location, geographic information, mobile search, local search,
social media

1.

INTRODUCTION

Hyperlocal search and recommendation scenarios are important because of the increasing prevalence of smartphones
and other mobile devices. Users expect the system to recognize their location and provide information about their
immediate surroundings. Local search systems built on predicting broad regions such as cities are not able to serve
many scenarios relevant to users’ information needs. Hyperlocal search focuses on identifying businesses, points of
interest, or areas as small as a few city blocks.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00.
http://dx.doi.org/10.1145/2600428.2609552.

1
2

1231

www.foursquare.com visited May 2014
www.flickr.com visited May 2014

2.

RELATED WORK

frequency. Once the system is constructed, it can be used for
ranking in the same way as an inverted index for document
retrieval, only in this system the cells are the “documents”.
We define a location as a set of coordinate pairs, and a
term distribution representing the location. The algorithm
starts by aggregating all terms associated with a given coordinate pair, and computing their user frequency distribution
and vocabulary size. If the vocabulary size does not meet
a pre-defined threshold, we reduce the decimal precision of
the coordinate pairs. Otherwise, we store the location represented by its coordinates and term distribution. We repeat
this process, until we have aggregated all data, the last decimal precision being zero decimal places. The process is
shown in Algorithm 1.
After processing, the locations will be defined by coordinates rounded to zero places, one place, two places, and
three places. Smaller grid cells may be surrounded by larger
grid cells, thus it is not a partition of the globe. However,
the terms in a smaller place are not included in the distributions of larger enclosing places.
The coordinates represent the center of a cell whose radius correlates with the decimal precision. Three decimals is
approximately 100 meters. Two decimals is approximately
one kilometer. One decimal is approximately 10 kilometers.
Zero decimals is approximately 100 kilometers. Thus by
rounding the data across the board to two decimal places,
we approximate the system described in O’Hare and Murdock [4], estimated from one kilometer grid cells. We regard
this as the baseline system.
At each iteration no more than one instance of the input
data is stored, so the space complexity is O(N ) where N is
the size of the input data. The algorithm makes one pass
over the data for each level of decimal precision, so the time
complexity is also O(N ). Once a cell has been defined, it
is not necessary to look at the data in that cell again, so in
practice each iteration of the algorithm examines less data.

In one of the early works on modeling locations, Serdyukov
et al. [5] explore multiple ways of estimating term distributions, for locations defined by a fixed-sized grid. They
compare the language modeling approach, using term estimates derived from term frequency, with several approaches
that use the GeoNames3 ontology, and an expert finding approach. They showed results for grid cells of 1 km x 1 km,
10 km x 10 km and 100 km x 100 km. Their metric is cell
accuracy: the percentage of examples for which the correct
cell was predicted. O’Hare and Murdock [4] follow on with
the same fixed-width grid cells, but estimating the term distribution according to the user frequency. They also show
a median distance of 400 meters when using the system to
predict the location of known points of interest extracted
from the captions of Getty images. The difference between
our work and theirs is that our grid cell sizes are determined
at indexing time, depending on the amount of data representing a place. Note that we removed a parameter, which
is the size of the grid cell.
In other work, Crandall et al. [1] use both the image content and textual metadata to predict the city of a photograph. They also predict which of a given set of landmarks
within a fixed set of cities is portrayed in an image. Theirs is
more of a classification task because the landmarks and cities
are known in advance, and they are a small set.Furthermore,
in their system, the place boundaries are well-defined and
available, whereas for the current work we make no such
assumption.
Hauff and Houben [2] define location boundaries using a
quad tree. The quad trees are defined by the global coordinate system, and do not depend on the size of the vocabulary
representing the quads, thus they have different sized grid
cells, but the data in a given cell may be sparse. The term
distributions are estimated by the term frequency, using a
language modeling approach as in [5]. They further enrich
the textual representation of the location by including social
media data from users who have both tweeted and uploaded
photos to Flickr, by matching the photos to the tweets for
the same user according to the time stamps in the data.
Van Laere et al. [3] refine the language modeling approach
based on term frequencies, to derive more accurate term
statistics. They further model location boundaries at multiple granularities to reflect that in the same data set some
locations cover a larger area (such as France), and others
cover a small area (such as Darmstadt). They normalize the
score for a given location by computing the probability mass
assigned to the location compared to other locations in the
same area. The basic idea is to leverage the spatial relationships between ranked locations with similar scores. For
example, if two neighborhoods in the same city both receive
similar scores, the model prefers an area that encompasses
both of them. The evaluation is non-standard, so it is not
possible to compare their results with the other systems in
the literature. Roughly 32% of Flickr images were identified
within the correct city.

3.

Algorithm 1 Computing a DLM
for all locations do
while precision >= 0 do
termDistribution ←ComputeUserFrequency(text)
vocabSize ← ComputeVocab(termDistribution)
coordinates ← RoundCoordinates(precision)
if vocabSize < threshold then
precision ← precision − 1
end if
end while
end for
return location
Much of the prior work, and this work as well, models
locations using a language modeling approach, similar to
the language model approach to document retrieval. The
basic idea is to define a location boundary, and associate
textual data with that location, according to the geographic
coordinates assigned to the data. The data is aggregated
by estimating a term distribution over the text associated
with a location. The locations can then be ranked with respect to a query by the probability that the query came
from the same distribution as the text associated with the
location. In the interest of space, we omit the description
of the model here. We used the implementation of language

DYNAMIC LOCATION MODELS

Estimating DLMs is done in two steps. First the data is
partitioned into grid squares, and then the term distributions for each grid square is estimated according to the user
3

http://www.geonames.org visited January 2013

1232

modeling, with dirichlet smoothing, in Indri.4 In principle,
any retrieval model would work, once the “documents” are
defined by the DLM. We replaced the term frequency distribution with the user frequency distribution, as in O’Hare
and Murdock [4].

4.

EXPERIMENTATION

We compare the DLMs to the fixed-cell representation
where the coordinates are rounded to two decimal places.
The models are built on the tag sets from public Flickr photos. To understand the properties of the models, we evaluate
the systems’ ability to recover the original coordinates assigned to a random sample of held-out Flickr photos. No attempt was made to select tag sets that are location-related,
and many of the tag sets in the evaluation data have no
location relatedness at all.
After filtering for photos that have both geotags and tags,
and removing bulk uploads, we selected a sample of 25 million image tag sets. From this subset of the data, we partitioned one percent of users from which to draw our test set.
This is to prevent the same user from being represented in
both the test set, and the set used to build the models.
Flickr normalizes the data such that spaces are removed
from the original tags, and commas are converted to spaces.
Thus, if the user tagged an image with “London Bridge” the
tag will be converted to “londonbridge” in the data. We did
not make an effort to reverse this normalization process. We
did eliminate any tag that was longer than 20 characters, as
these tags likely represent multiple concatenated terms, and
are not likely to occur more than once.
From the test sample, we randomly selected 100,000 image
tag sets for evaluation, and 10,000 image tag sets for tuning
the Dirichlet smoothing parameter. Each example in the
test set consists of an image tag set from a given user, with
its geographic coordinates. We evaluate the median distance
of the predicted location of the image tags to the actual
geographic coordinates assigned by the user. The distance
computed is the Vincenty Distance, which takes into account
the ellipsoid shape of the earth.
We rounded the latitude and longitude coordinates to
3 decimals, prior to any other processing, because we felt
that this level of precision was sufficient for any scenarios
we would consider. Rounding to 3 decimals in the preprocessing means that in the best case we can predict the
location within 50 meters. We discard cells associated with
fewer than five users. We eliminate duplicate tags by limiting the data to one instance of a tag per user per location.
This eliminates the problem of near-duplicate tag sets applied to multiple images.
While the median distance is our primary metric, we report the average distance as well. The average distance is
not as informative because the earth is roughly 40,000 km
circumference at its widest, and so a prediction that is completely wrong, can be wrong by 20,000 km. We did not filter
out tag sets that are not informative or that are not related
to any location. Some of the tags in the evaluation set are
generic, for example “John, Birthday, 2007.” The median
distance is more informative, as it shows the accuracy of at
least 50% of the data, and is not skewed by a relatively few
examples whose location predictions are very far off.
4

Figure 1: DLMs are more robust to the Dirichlet
smoothing parameter, due to the lower variance in
document lengths.

5.

RESULTS

To evaluate the sensitivity of the model to the Dirichlet smoothing parameter, we conducted a parameter sweep,
with values {1, 100, 200, 500, 1000, 2000, 5000, 10000},
shown in Figure 1. We see that the 100 m x 100 m grid
cells (the data rounded to three decimals) are the least robust with respect to the Dirichlet smoothing parameter,
and also have the least accurate predictions. Aggregating
the locations to enforce a minimum vocabulary size reduces
the model’s sensitivity to the smoothing parameter, which
makes sense when you consider that it also reduces the variance in the length of the cell representation. In the experiments reported below, we use the optimal smoothing parameter as determined by the tuning set, but note that one
advantage of the DLM is that the model is robust to the
smoothing parameter.
The baseline system quantizes the coordinates by rounding to three decimals, two decimals, and one decimal. This
corresponds to grid cells of approximately 100 m x 100 m,
1 km x 1 km, and 10 km x 10 km. The baseline system
with rounding at two decimal places, estimating the term
statistics from the user frequency represents the baseline approach. In the experimental system, we examined thresholds
of 50, 100 and 250 terms.
The results In Table 1 show that not only is the DLM
more robust to the parameter setting, but shows a significant performance advantage over the baseline. All of the
models returned results for approximately 98750 queries out
of 100,000. With the exception of the models rounded to 1
decimal place, the 25th percentile result was around 800 meters. (For the model rounded to one decimal place, the 25th
percentile was 4.12 km.)

www.lemurproject.org/indri visited February 2014

1233

Table 1: User frequency distribution, comparing
fixed sized grid cells and DLMs with different vocabulary thresholds. The baseline system, and the
best performing DLM are highlighted.
Median
Average
Dirichlet
Distance
Distance
µ
1 decimal
10.94 km
1717.87 km
10000
2 decimals 10.31 km 1707.72 km
5000
3 decimals
22.06 km
1795.19 km
1000
vocab 250
8.82 km
1480.6 km
2000
vocab 100
8.40 km
1432.11 km
2000
vocab 50
8.72 km
1510.25 km
2000

6.

Table 2: Characteristics of the index of locations
estimated by the user frequency distribution
1 decimal 2 decimals 3 decimals
num. locations
141424
502169
498881
vocabulary size 10191616
8935929
5202325
average length
1951
469
244
vocab 250 vocab 100
vocab 50
num. locations
289929
647519
918121
vocabulary size
9957035
9449868
8558829
average length
787
354
227
distance from the true location, in spite of the vocabulary
also being smaller. This fails with cells rounded to three
decimal places, because of the tradeoff between the width
of the cell, and the richness of the representation. Clearly a
richer vocabulary helps. The vocabulary size for the DLMs
is large by comparison, and does not change much in spite of
the cell sizes being smaller. In the best case we would have
small locations represented by a rich vocabulary contributed
by many users, and this is what DLMs provide.

DISCUSSION

Modeling locations at fine granularities is necessary to
support the hyperlocal scenarios that are expected by users
of mobile devices. Unfortunately, unless there is sufficient
data to represent very small locations, it is not possible to
produce a reliable prediction. This is evident from the location boundaries created by rounding the data to three
decimals. Three decimals represents a location that is approximately 100 m x 100 m, but the results show that the
model is not robust to the smoothing parameters, and the
results are far from hyperlocal. By contrast, the model based
on variable sized grid cells, with a vocabulary threshold of
50 or 100 terms allows for hyperlocal predictions, in spite of
the vocabulary size being limited. The distribution of interesting locations is not uniform, and some hyperlocal places
will generate more interest (and thus data) than others.
Flickr data is unique in that the vast majority of tags
applied to images are applied by the person who took the
photo, and uploaded it to Flickr. The tags typically represent the content and the context of the image, and contain
very few non-content terms. Flickr data differs from other
types of data typically associated with information retrieval
tasks because the terms are predominantly nouns or noun
phrases. They lack a grammar, and they may or may not be
related to each other, outside of the context of the image. In
this sense, the data resembles passage retrieval or sentence
retrieval data that has had the stop words removed.
Because of the flickr normalization process where tags separated by a space are concatenated, there are more singleton
terms than typically appear in a natural language distribution. Finally, while the predominance of tags are in English,
people often tag in other languages, or in their native language as well as another language. So unlike document or
sentence retrieval, where the distribution of terms contain
non-content terms as well as content terms, and are typically limited to one language, the Flickr tag sets are more
compact with respect to parts of speech, and more diverse
with respect to vocabulary.
In previous work [5, 4], larger cells were deemed more
accurate because the evaluation metric was cell prediction
accuracy. Since the language models represented a richer
vocabulary, they made more accurate predictions. Looking
at Table 2 we see the columns that correspond to the system
proposed by Serdyukov et al. and O’Hare and Murdock
(decimal rounding) shows a vast difference in vocabulary
size, however in general the performance of the system is
better at smaller granularities (compare one decimal place
to two decimal places in Table 1), in terms of the actual

7.

CONCLUSIONS

There are many potential applications for this work, such
as disambiguating points of interest, associating social media to businesses, characterizing places according to what
people say about a place, and defining regions of specific interest (dining, shopping, or events) within a city. We show
that DLMs are computationally efficient, and provide better
estimates of the language models of hyperlocal places than
the standard method of segmenting the globe into approximately equal grid squares. We evaluate the models using a
repository of 25 million geotagged public images from Flickr.
In this work we presented Dynamic Location Models, which
alter the tradeoff between a small cell size, and a rich representation of the cell by making the cell size dependent on the
size of the vocabulary of the underlying data. This produces
location models that are more robust to retrieval parameters, and more accurate in predicting locations in text.

8.

REFERENCES

[1] D. Crandall, L. Backstrom, D. Huttenlocher, and
J. Kleinberg. Mapping the world’s photos. In
Proceedings of the 18th International Conference on
World Wide Web, pages 761–770. ACM, 2009.
[2] C. Hauff and G.-J. Houben. Placing images on the
world map: A microblog-based enrichment approach. In
Proceedings of the 35th International ACM SIGIR
Conference on Research and Development in
Information Retrieval, 2012.
[3] O. V. Laere, S. Schockaert, and B. Dhoedt.
Georeferencing flickr photos using language models at
different levels of granularity: An evidence based
approach. Journal of Web Semantics, 16, 2012.
[4] N. O’Hare and V. Murdock. Modeling locations with
social media. Journal of Information Retrieval, 16(1),
2013.
[5] P. Serdyukov, V. Murdock, and R. van Zwol. Placing
Flickr Photos on a Map. In Proceedings of the 32nd
International ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 484–491.
ACM, 2009.

1234

