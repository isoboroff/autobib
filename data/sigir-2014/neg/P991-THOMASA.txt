Compositional Data Analysis (CoDA) Approaches to
Distance in Information Retrieval
Paul Thomas

David Lovell

CSIRO, Australia

CSIRO, Australia

paul.thomas@csiro.au

david.lovell@csiro.au

ABSTRACT
Many techniques in information retrieval produce counts
from a sample, and it is common to analyse these counts as
proportions of the whole—term frequencies are a familiar
example. Proportions carry only relative information and
are not free to vary independently of one another: for the
proportion of one term to increase, one or more others must
decrease. These constraints are hallmarks of compositional
data. While there has long been discussion in other fields of
how such data should be analysed, to our knowledge, Compositional Data Analysis (CoDA) has not been considered in
IR.
In this work we explore compositional data in IR through
the lens of distance measures, and demonstrate that common measures, naı̈ve to compositions, have some undesirable
properties which can be avoided with composition-aware
measures. As a practical example, these measures are shown
to improve clustering.

(0,0,1,0)
(0,1,0)
(1,0)
(1,0,0,0)
(1,0,0)

(0,1,0,0)

Figure 1: The set of vectors with D positive components that
sum to the constant κ form the D-part simplex, denoted by
S D [3]. From left to right, and with κ = 1, we see S 2 , S 3
and S 4 .
In a compositional analysis the absolute size—mass of a
rock, amount of DNA in the sample, amount of food, length
of document vector—is not of interest. Instead, the interest
is in the relative information, the proportions of the whole.
We can illustrate these ideas with a simple example: the
content of soils. Suppose a 50 g soil sample is found to
contain 38 g of sand; 10 g of silt; and 2 g of clay. This threecomponent datum, (38, 10, 2), is the basis vector. This vector
is sum-constrained: if we have less sand, we must have more
silt and/or clay to make up the 50 g. Now, since we are
interested in the relative information but not the size of the
sample, it is easy to constrain (or “close”) the basis vector
to the equivalent (0.76, 0.20, 0.04). This is a composition: a
vector of D parts, (x1 , . . . , xD ), that sum to a constant κ
which, in this case, we choose to be 1.
Geometrically, compositions are points on a D-part simplex
(Figure 1). CoDA theory, as pioneered by Aitchison [1] and
developed by Pawlowsky-Glahn, Egozcue and others [2] is
founded on logratio transformations that map the simplex
S D onto RD where the full arsenal of statistical methods can
be applied. If necessary, the results from these methods can
be back-transformed to the simplex, secure in the knowledge
that the constant-sum constraint will be satisfied.
Compositional data is common in IR (Section 2), but
compositional data analysis is not. This paper aims to
raise awareness of CoDA approaches, focusing especially on
distances and dissimilarities common in IR and their compositional counterparts (Sections 3 and 4). Section 5 explores
how these distances perform in a document clustering task.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
search and retrieval—clustering, retrieval models.

Keywords
Aitchison’s distance; compositions; distance; similarity, ratio

1.

(0,0,0,1)

(0,0,1)

(0,1)

COMPOSITIONAL DATA

Compositional data analysis (CoDA) deals with data that
carry only relative information, such as proportions, percentages, and probabilities [1]. Information is carried only
in the ratios of different components. Examples include
the chemical composition of rocks, the fraction of a certain
DNA sequence in a sample, nutritional content of food, or
employment data by industry. In text processing, compositional data can include genres, documents, or probability
distributions.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2257-7/14/07 ...$15.00.
http://dx.doi.org/10.1145/2600428.2609492.

2.

COMPOSITIONS IN IR

Compositions are common in information retrieval for
various reasons. IR often deals with fixed-size samples: the
genres of pages on a website, for example, or the terms in

991

While this has many of the hallmarks of compositional
data, IR often uses cosine dissimilarity as a means to compare documents, an approach based on transforming the
data to the positive orthant of the D-dimensional unit hypersphere, rather than working within the D-dimensional
simplex. Figure 2 illustrates that the cosine dissimilarity is
unaffected by the magnitude of vectors, only their direction.
However, as we shall see, distances on the hypersphere are
constrained—there are limits to how different two documents
can be in terms of cosine dissimilarity—whereas the logratio
transformation used in CoDA enforces no such limits.
Transforming compositional data to the hypersphere is a
concept that has been discussed and debated within CoDA;
it appeals because it handles zero values simply but lacks the
robust theoretical foundation of the logratio approach [7].

Figure 2: S 3 and the positive orthant of the hypersphere.
Cosine similarity is unchanged whether we project onto the
hypersphere or the simplex.
1000 scientific abstracts, where the sum is constrained to
the size of the sample. We may have proportions, when
the denominator is not known but the sum is constrained
to 100% (or 1). Further, even individual documents or
queries can be considered compositions. This can happen
directly, if we are using probabilistic language models; or as
an alternative representation, if we are using vector-space
(geometric) models.

3.

FIVE DISTANCES FOR IR DATA

Conceptualising documents as points on the simplex opens
up similarity and distance measures that are new to IR. As
well as the familiar cosine and Euclidean measures, Aitchison’s (compositional) distance can be applied, as can Euclidean distance between log-transformed data and KullbackLeibler divergence [5, 6]. In this section we will quickly
review these distances and dissimilarities, then look at their
behaviours and implications for information retrieval.
Distances are measured between two objects, here two
documents. We will treat these as basis vectors X and Y,
where each element Xi and Yi is a count of terms. We will use
x and y to refer to the sameP
two documents, as compositions:
i.e. x is a vector such that i xi = 1.
In addition, we require all xi to be strictly positive to avoid
the issue of dealing with zeros which, for approaches using
logarithms or ratios, is a research topic in its own right [7].
Various language modelling approaches, including Bayesian
methods, can ensure that there are no zeros in IR data [9].

Count data. Count data are often constrained in IR settings.
If we count the number of in- and out-links for a web page,
the sum is constrained to that page’s degree. If we count the
number of nouns, verbs, or specific terms, in a 10,000-word
sample, the sum is constrained to 10,000.
While observational count data does not carry purely relative information1 it is often modelled and analysed using
compositional methods. (Expected counts, on the other hand,
do carry only relative information but, in general, these are
not known to us and have to be estimated.)
Multinomial and Dirichlet distributions. Both of these
distributions play important roles in modelling discrete data.
Clearly, the vector of event probabilities in a multinomial
probability distribution, and the vector of concentration
parameters in a Dirichlet distributions are compositional,
summing to one over the set of categories they describe.
In IR, multinomial and Dirichlet distributions are often
used as the basis of probabilistic document models, topic and
topic/term distributions, and Markov transition probabilities.
We note that CoDA offers a more flexible alternative to
the Dirichlet distribution: the logistic normal, which allows
dependencies between components to be expressed and opens
up opens up the full range of linear modelling available for
the multivariate Normal distribution in RD [2].

Cosine dissimilarity. Vector-space models of text typically
measure similarity between two documents by the cosine of
the angle between them. Cosine dissimilarity can be defined:
P
x i yi
dC (x, y) = 1 − pP i2 pP 2 .
x
i i
i yi

Euclidean distance. Euclidean distance is the length of
the shortest line segment connecting our two points, and is
straightforward (note that here we use squared distance):
X
d2E (x, y) =
(xi − yi )2 .

Vector-space models. Documents are often represented as
points in vector-space where each dimension corresponds to
a term, and the coordinate value represents the count of that
term in the document. In IR, interest commonly lies in the
relative abundance of different terms, rather than the actual
counts observed since the former relates more strongly to the
meaning and content of a document. For this reason, count
vectors are generally normalised to a constant magnitude
so that documents of different lengths become meaningfully
comparable.

i

√
On the simplex
dE is bounded by [0, 2]: two points can
√
be at most 2 units apart and that happens when each
is at a different “corner”. In the case of high-dimensional
data such as text, it is also clear that differences in any one
component will count for little as they are “drowned out” by
the large number of other components.

Euclidean distance of log-transformed data. When dealing with count data, especially counts which vary over several
orders of magnitude, it is common to use a logarithmic transformation. This leads to a distance of the form:
X
d2EL (x, y) =
(log xi − log yi )2 .

1

“Two heads and one tail” in a coin toss experiment tell a
different story than “two thousand heads and one thousand
tails”, even though the ratio of heads and tails is the same
in both instances.

i

992

X
(1,
(1,
(1,
(1,

Y
10)
100)
1000)
10000)

(2,
(2,
(2,
(2,

10)
100)
1000)
10000)

Cosine

Euclidean

Euclidean log

Kullback-Leibler

Aitchison

0.00477
5 × 10−5
5 × 10−7
5 × 10−9

0.107
0.0137
1.41 × 10−3
1.41 × 10−4

0.612
0.682
0.692
0.693

0.0525
6.73 × 10−3
6.91 × 10−4
6.93 × 10−5

0.490
0.490
0.490
0.490

Table 1: Some distances and divergences. Modified from Lovell et al. [5].

20

0.05
Aitchison
Cosine
Euclidean
Euclidean log
Kullback−Leibler

0.04
Distance from X

Distance from X

15

10

0.03

0.02

5
0.01

0

Count of term Y 1000

20

19

18

17

16

15

14

13

12

11

9

10

8

7

6

5

1M

1k

1

0.00
Count of term Y 1000

Figure 3: Distances between 1000-term synthetic documents X = (20, 20, . . . , 10) and Y = (20, 20, . . . , Y1000 ). The plots show
how the different distances vary as Y1000 changes from 100 to 107 . The right-hand plot gives a zoomed-in view.

4.

Note that, unlike plain Euclidean distance, there is no upper
limit to the distance between log-transformed components.

BEHAVIOURS OF DISTANCES

Table 1 uses vectors with two components as a simple illustration of how various distances and dissimilarities behave.
Vectors X and Y differ only in their first component by a
factor of 2. Aitchison’s distance depends only on the ratios
of components, that is X1 : Y1 and X2 : Y2 : these ratios
are always the same here and so dA does not change. Other
measures depend on both these ratios and the absolute values
of each component, and vary from case to case.
Figure 3 shows further aspects of behaviour, based on the
distances between two fictional documents. Here X has 999
“noise” terms repeated 20 times each, and one rarer “signal”
term repeated ten times, that is X = (20, 20, . . . , 10). Y has
the same first 999 “noise” terms, but the final “signal” is varied
from one occurrence up to ten million (Y1000 ∈ [1, 107 ]). As
the “signal” grows large, both cosine and Euclidean distance
saturate and do not change appreciably although counts vary
over several orders of magnitude.
The right-hand graph illustrates another behaviour. As
we halve or double the amount of the “signal” term, from 5
through 10 to 20, intuitively it seems that distances should
differ. However, the cosine, Euclidean, and Kullback-Leibler
measures barely change: the less-important terms “drown
out” the more-important one.
We have several apparently undesirable behaviours, displayed by three of the five metrics at different times: (1) the
distance between two documents which vary only in one
component depends on the size of the other (nonvarying)
components—but these other components are presumably
not interesting; (2) the distance between documents which
vary only in one component depends on the number of dimensions; (3) the distance can saturate; (4) the distance is not
sensitive to even quite large relative changes in the counts

Kullback-Leibler divergence. Kullback-Leibler divergence
can express the dissimilarity between two probability distributions [4]. It is not strictly a distance metric, but is
commonly used in information retrieval to compare two sets
of normalised counts—that is, to compare two compositions
[8]. In its symmetric form it is written:

X
xi
yi
dKL (x, y) =
xi log
+ yi log
yi
xi
i
X
=
(xi − yi )(log xi − log yi ) .
i

Kullback-Leibler divergence depends on both the ratio of
compositional components and their absolute values.

Aitchison’s distance. This distance depends solely on the
ratios of components and can be written:

2
xi
yi
1 X
d2A (x, y) =
log
− log
.
D i<j
xj
yj

Relationships between distances. If vectors X and Y are
normalised to lie on the unit hypersphere (as is often done
in IR applications) then d2E (X, Y) = 2dC (X, Y). This does
not hold for vectors on the unit simplex.
By Napier’s inequality, d2E (x, y) < dKL (x, y) < d2EL (x, y),
and, as shown by Lovell et al. [5], d2A (x, y) ≤ d2EL (x, y). Note
that both Aitchison and Euclidean-log distances are invariant
to re-scaling (i.e., weighting) of components, such as by the
inverse document frequency of terms.

993

asked for. Clustering with dA never produces worse results
than clustering with the standard measures.
Similar effects were seen with different numbers of documents from the same collection, and using the Rand index
instead of purity. We also saw similar effects building clusters
with complete linkage, although no effect using single linkage.

1.00

Purity

0.75

0.50

Aitchison
Cosine
Euclidean
Euclidean log
Kullback−Leibler

0.25

6.

CoDA is relevant to IR. It provides a well-founded theoretical basis for analysing data on the relative abundance of
document topics and terms, and gives a distance metric that
depends solely on these relative abundances.
Our initial clustering experiment suggests logarithm- and
ratio-based distances (dA , dEL , dKL ) warrant further investigation as alternatives to distances on the hypersphere (dE ,
dC ).
Our hope is that this paper will encourage exploration of
CoDA methods on other IR data sets and tasks, including
ranking. We note that both dA and dEL are invariant to
re-scaling of components, such as weighting by inverse document frequency. Our suspicion is that idf-weighting will not
improve dE and dC to the point that they surpass logarithmand ratio-based distances in clustering performance, but this
too demands further experiments to verify.

1000

750

500

250

0.00

Number of clusters

Figure 4: Cluster purity as number of clusters varies, using
different distance measures to form the clusters. With this
data, clusters formed using dEL , dA , or dKL are better than
those formed with dE or dC .
of uncommon terms, although these changes are presumably
interesting.

5.

DISCUSSION AND DIRECTIONS

DISTANCES IN PRACTICE

Thus far, we have looked at the theoretical properties
of distances and dissimilarities commonly used in IR and
CoDA. This section explores their use in a typical IR task—
clustering—in which performance depends on both the distribution of the data to be clustered and the way in which we
convert differences between multivariate data points into a
univariate distance or dissimilarity. The aim here is not some
kind of “performance shoot out”, rather than to see whether
and how these measures affect distance-based analyses.
We explore the behaviour of the distances and dissimilarities presented so far by using them to cluster Usenet articles
from the 20-newsgroups collection. These experiments used
50 articles from each of 20 groups, without headers such as
Newsgroups: or Path: which would identify the group, and
with no stemming or stopping.2 Heirarchical clustering used
Ward’s method, as implemented in R, and a tree was built
using each of the distance measures above.
We report the quality of the clustering with purity. Purity
for a clustering of 1000 documents is the mean proportion of
documents in each
P cluster which are in the majority class:
purity = 1/1000 i maxj |ci ∩ gj |, where ci is the ith cluster
and gj is the jth newsgroup. Purity ranges up to 1, and scores
highest when each cluster consists entirely of documents from
the same newsgroup.
Figure 4 plots purity when the heirarchies formed with
each distance measure were cut to 20, 40, . . . 1000 clusters.
At 1000 clusters, each cluster is a single document and purity
must be 1; but at all other points, different distance measures
produce clusterings of different quality.
dC and dE produce very similar distances, and hence very
similar clusterings; dEL and dA are also related, and dKL
sits somewhere in between. Aitchison’s distance produces
somewhat better clusters than the commonly-used cosine and
Euclidean, with purity a relative 7% better than dC across
all cuts and as much as 20% better when fewer clusters are

7.

REFERENCES

[1] J. Aitchison. The Statistical Analysis of Compositional
Data. Chapman & Hall, London, 1986.
[2] J. Bacon-Shone. A short history of compositional data
analysis. In V. Pawlowsky-Glahn and A. Buccianti,
editors, Compositional Data Analysis, pages 1–11. John
Wiley & Sons, Ltd, 2011.
[3] J. J. Egozcue and V. Pawlowsky-Glahn. Basic concepts
and procedures. In V. Pawlowsky-Glahn and
A. Buccianti, editors, Compositional Data Analysis,
pages 12–28. John Wiley & Sons, Ltd, 2011.
[4] S. Kullback and R. A. Leibler. On information and
sufficiency. The Annals of Mathematical Statistics,
22(1):79–86, 1951.
[5] D. Lovell, W. Müller, J. Taylor, A. Zwart, and
C. Helliwell. Caution! Compositions! Can constraints on
omics data lead analyses astray? Technical Report
EP10994, CSIRO, Mar. 2010.
[6] J. A. Martı́n-Fernández, C. Barceló-Vidal, and
V. Pawlowsky-Glahn. Measures of difference for
compositional data and hierarchical clustering methods.
In Proc. Int. Assoc. for Mathematical Geology, pages
526–531, 1998.
[7] J. A. Martı́n-Fernández, J. Palarea-Albaladejo, and
R. A. Olea. Dealing with zeros. In V. Pawlowsky-Glahn
and A. Buccianti, editors, Compositional Data Analysis,
pages 43–58. John Wiley & Sons, Ltd, 2011.
[8] J. Xu and W. B. Croft. Cluster-based language models
for distributed retrieval. In Proc. ACM SIGIR, pages
254–261, 1999.
[9] C. Zhai. Statistical Language Models for Information
Retrieval: A Critical Review, volume 2 of Foundations
and Trends in Information Retrieval. now Publishers,
Delft, 2008.

2
This is the “bydate” collection from Jason Rennie, http:
//qwone.com/~jason/20Newsgroups/.

994

