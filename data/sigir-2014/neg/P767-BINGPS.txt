Web Page Segmentation with Structured Prediction and its
∗
Application in Web Page Classification
†
†

Lidong Bing†

Rui Guo

Wai Lam†

Zheng-Yu Niu

Haifeng Wang

Key Lab of High Confidence Software Techs., Ministry of Education (CUHK Sub-Lab), Hong Kong
Dept. of Systems Engg. & Engg. Management, The Chinese University of Hong Kong, Hong Kong

Baidu Inc., Beijing, China

ldbing@se.cuhk.edu.hk, wlam@se.cuhk.edu.hk
grxkwok@gmail.com, niuzhengyu@baidu.com, wanghaifeng@baidu.com

ABSTRACT

1. INTRODUCTION

We propose a framework which can perform Web page segmentation with a structured prediction approach. It formulates the segmentation task as a structured labeling problem on a transformed Web page segmentation graph (WPSgraph). WPS-graph models the candidate segmentation boundaries of a page and the dependency relation among the adjacent segmentation boundaries. Each labeling scheme on
the WPS-graph corresponds to a possible segmentation of
the page. The task of ﬁnding the optimal labeling of the
WPS-graph is transformed into a binary Integer Linear Programming problem, which considers the entire WPS-graph
as a whole to conduct structured prediction. A learning
algorithm based on the structured output Support Vector
Machine framework is developed to determine the feature
weights, which is capable to consider the inter-dependency
among candidate segmentation boundaries. Furthermore,
we investigate its eﬃcacy in supporting the development of
automatic Web page classiﬁcation.

Web pages are typically designed to facilitate visual interaction with the human readers. The designer normally
organizes the information of a page into diﬀerent units or
functional types [22], which are arranged in coherent visual segments in the page, such as header, footer, navigation
menu, major content, etc. It is a trivial step to recognize
those visual segments for readers. However, it is still a challenging problem for computer since the source code of Web
pages is not encoded in such a way to diﬀerentiate the semantic blocks. To overcome the gap between the manner of
the pages designed/read by human and the manner of the
pages operated by the computer, Web page segmentation is
regarded as an essential task in Web information mining.
The aim of Web page segmentation is to decompose a Web
page into sections that reveal the information presentation
logic of the page designer and appear coherent to the readers.
For example, if we consider the page in Figure 1, Web page
segmentation should recognize those segments separated by
the red boundary lines. Identifying such segments is useful
for diﬀerent downstream applications. One application is to
re-organize a Web page so that it can be properly displayed
or redecorated for devices with small-sized screen [1, 17, 20].
Then, people with visual impairment can easily digest them
with their screen readers [21]. Link analysis, Web document
indexing, and pseudo-relevance feedback can be more eﬀective with the appropriate segments detected [12, 22, 29].
Duplicate content detection, Web page classiﬁcation, and
content change detection can be conducted on ﬁner information units so as to obtain better performance [7, 19, 27].
One group of previous works on Web page segmentation is
heuristics-based [6, 19]. Cai et al. employed heuristic rules
to capture structure features and visual properties [6]. Their
method recursively segments the larger blocks into smaller
ones in a top-down manner. However, this greedy manner
is myopic and it may be trapped locally and stop to search
better solutions. In addition, the patterns used in page design are unlimited and cannot be covered by a ﬁnite set of
rules. Kohlschütter and Nejdl employed text density in different portions of a Web page as a clue to conduct segmentation [19]. Their method ignores other types of information
such as image, frames and whitespace which provide useful
clues for page segmentation. Chakrabarti et al. proposed a
graph-theoretic approach to deal with Web page segmentation [7]. They cast the problem as a minimum cut problem
on a weighted graph with the nodes as the DOM tree nodes
and the edge weights as the cost of placing the end nodes in

Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Information
Search and Retrieval

Keywords
Web page segmentation, Web page classiﬁcation, structured
prediction, integer linear programming
∗ The work described in this paper is substantially supported by
grants from the Research Grant Council of the Hong Kong Special
Administrative Region, China (Project Code: CUHK413510) and
the Direct Grant of the Faculty of Engineering, CUHK (Project
Code: 4055034). This work is also aﬃliated with the CUHK MoEMicrosoft Key Laboratory of Human-centric Computing and Interface Technologies.
The work was partially done when the ﬁrst author visited Baidu.
We thank Dr. Baiyi Wu for discussing the optimization problem.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00.
http://dx.doi.org/10.1145/2600428.2609630.

767

b3

b4

b1

b1 b2
o
8
o
9
o
10
b
10

s
s
s

b6
b7

s

b8
b9
b10

b3 b4 b5

s5o
s5b

b6 b7 b8 b9 b10

(a) An example page.

(b) WPS-graph of (a).

b2
b5

Figure 2: An example page and its WPS-graph.
The learning algorithm can consider the inter-dependency
among the vertices of a WPS-graph during the training process. Therefore, the feature weights are determined with
a global view on WPS-graphs but not merely on individual segmentation boundaries. To infer the optimal labeling,
we consider the entire WPS-graph as a whole to conduct a
structured prediction in which the labeling determination of
one vertex, i.e., boundary, is coordinated with the others.
To do so, we transform the labeling task into a binary Integer Linear Programming (ILP) problem [35], whose linear
programming relaxation can be eﬃciently solved with the
simplex algorithm [11].
Extensive experiments have been conducted on a large
data set. The results demonstrate that our framework achieves
better performance compared with two state-of-the-art methods. To investigate the eﬃcacy of our framework in supporting other Web mining tasks, another experiment, namely
Web page classiﬁcation, is conducted. In this task, we propose a novel method to assemble the segmentation output
of a page for constructing a more eﬀective feature vector.

Figure 1: An example of Web page segmentation.
the same segment or diﬀerent segments. A learning based
method was developed to determine the weights of edges.
This method sometimes reports non-rectangular segments
which are generally inaccurate. Diﬀerent from [6], both [19]
and [7] only obtain a ﬂat segmentation of a Web page without knowing the hierarchical structure of the segmentation.
In this paper, we propose a framework to solve the above
shortcomings of the existing works. Our framework performs Web page segmentation with a structured prediction
approach by formulating the segmentation task as a structured labeling problem on a transformed Web page segmentation graph (WPS-graph). WPS-graph is a directed acyclic
graph, as exempliﬁed in Figure 2(b), and its vertices are
composed of the candidate segmentation boundaries of the
corresponding page, as depicted in Figure 2(a). Each vertex,
i.e. boundary, is able to split the current segment into two
sub-segments. The directed edges capture the dependency
relation between the associated boundaries. Each labeling
scheme of the WPS-graph, which assigns a binary label for
each vertex to indicate whether or not the boundary should
split the current segment, corresponds to a possible segmentation of the page. Diﬀerent from the heuristic rule-based
top-down search in [6], our framework performs the label
prediction simultaneously for all vertices of the WPS-graph
with a trained statistical model. The hierarchical structure
of the intermediate segmentation steps is recorded by the
layers in the WPS-graph. Thus, the output of our framework provides a good structural analysis of pages enabling
better utilization for diﬀerent Web mining tasks such as page
classiﬁcation. Furthermore, as exempliﬁed with Figure 2,
our framework can automatically avoid non-rectangular segments via the horizontal and vertical boundaries.
DOM structure, visual property, and text content features
of a Web page are jointly considered in our framework. DOM
structure features capture the structure characteristics of
the segments, such as the structure similarity of the neighboring segments, the regularity of the DOM structure, etc.
Visual property features capture visual clues of segments,
such as background color, whitespace, font size, etc. The
text content features capture some important semantic characteristics of the segments, such as the text similarity of the
neighboring segments, the title keywords in a segment, etc.
To allow diﬀerent impacts of the features, a weight is associated with each feature. A machine learning algorithm based
on the structured output Support Vector Machine framework [32] is developed to determine the feature weights.

2. RELATED WORK
Web page structure analysis has been one hot research
area for the past decade. There are several directions in
this area, including single page oriented segmentation [6, 7,
19], site-oriented page segmentation [14], informative content extraction [13, 25, 31, 34], template (or boilerplate)
detection [2, 33, 37], data record detection and entity extraction [3, 4, 5, 16, 24], etc. These directions are closely interwoven. Some directions share similar methodologies but
aim at diﬀerent products, such as site-oriented page segmentation and template detection. Some can be regarded as a
preparation step for another purpose.
Single page oriented segmentation aims at segmenting an
input Web page into distinct coherent segments or blocks
(such as main content, navigation bars, etc.) based on its
own content. Besides the works [6, 7, 19] discussed above,
Chen et al. [9] distinguish ﬁve block types, namely, header,
footer, left side bar, right side bar, and main content. Fernandes et al. [14] proposed a site-oriented method for Web
page segmentation. Hattori et al. proposed a segmentation
method based on calculating the distance between content
elements within the HTML tag hierarchy [17].
Aiming at improving the performance of Web page clustering and classiﬁcation, Yi et al. proposed a site style tree
(SST) structure that labels DOM nodes with similar styles
across pages as uninformative [37]. In [15], the URL features were shown to be eﬀective in homepage identiﬁcation,
which can be regarded as a special case of page functional
type classiﬁcation.

768

b3
b1

s1o

b4

s1o

b1

b3

b4

s1o

b1

b3

b4

s1o

b1

s6o

b6

s0

s3o

s2o
b2

s2b



b1 b2

(a)

(b)

b2

s4o

s3o

s4b

s2b

s4b

s5o
s5b

b2
b5

b3 b4

(c)

b7

s3o

s7o

s4b

b
7
o
5
b
5

s
s
s

b2
b5

b1 b2

b1 b2

b1 b2
b3 b4

s4o

b5

b3 b4

b5

b6 b7

(d)

(e)

be regarded as simple DOM structure and visual property
features. Diﬀerent from these methods, our model conducts
a global evaluation on the ﬁtness of a segmentation for a
page.

Figure 3: Steps of WPS-graph construction.

3.

PROBLEM FORMULATION

3.1 Web Page Segmentation

3.2 WPS-graph

Web page segmentation is the task of breaking a page into
sections that reveal the information presentation structure
of the page designer and appear coherent to the readers. To
facilitate the description of our framework, one illustration
page example is given in Figure 2(a). The lines in the page
are candidate boundaries for splitting the page into visual
segments. The solid lines are the boundaries that should
split the segments where the boundaries locate into subsegments. For example, b1 and b2 split the entire page into
the upper, middle, and lower parts. While the dashed lines
are the boundaries that do not perform splitting operation,
such as b8 , b9 , and b10 . Therefore, the ﬁnal segmentation
result is as depicted by the solid boundaries. Performing
page segmentation is equivalent to determining a label assignment which assigns the label Y (splitting operation) or
the label N (not a splitting operation) to each boundary. For
the page in Figure 2(a), the solid boundaries take the label
Y, while the dashed boundaries take the label N. A segmentation should follow the structure constraints of the page.
For the above example, only after the boundaries b1 , b2 , b3
and b4 simultaneously decide to split, it becomes meaningful
to consider which label b6 and b7 should have. If any one of
b1 , b2 , b3 and b4 is labeled with N, b6 and b7 are automatically labeled with N. This kind of constraints compose of
a topology graph G, named Web page segmentation graph
(WPS-graph) in this paper. The WPS-graph of the page in
Figure 2(a) is given in Figure 2(b).
Let C denote a label assignment by taking a label from {Y,
N} for each vertex, i.e., boundary, in G. The segmentation
task is formulated as the following optimization problem:
C∗ = argmax F (G, C; w),
C

Algorithm 1: Construction of WPS-graph.
1: initialization: s0 ← p, Q.enqueue(s0 ), G ← ∅
2: while ¬Q.empty() do
3: s ← Q.dequeue()
4: if s is separable then
5:
add the boundaries bi..j in s as vertices into G
6:
add new edges for bi..j
7:
Q.enqueue(subsegments of s)
8: end if
9: end while

Definition 1 (WPS-graph). For a Web page p, its
WPS-graph G = {B, E} is an acyclic directed graph. Each
vertex b in B is a candidate segmentation boundary in p.
Each directed edge e : bi , bj  in E indicates the constraint
(C(bj ) = Y) −→ (C(bi ) = Y), where C is a label assignment
for the vertex set B of G.
The construction of WPS-graph for a page is described in
Algorithm 1. Initially, the entire page is regarded as one
segment and denoted as s0 . Meanwhile, the WPS-graph is
∅. Refer to Line 1 in Algorithm 1 and part (a) in Figure 3.
While the segment queue Q is not empty, the segment at
the front of Q is processed, as depicted in Line 3. Take
s0 as an example as depicted in part (b) in Figure 3. The
candidate horizontal boundaries b1 and b2 split s0 into three
sub-segments and they are added as vertices into G, as given
in Line 5 in Algorithm 1. For the new vertices, namely, b1
and b2 , no new edges need to be added since s0 is the entire
page and b1 and b2 do not depend on any previous boundary.
The current G is as depicted in the lower section of part (b)
in Figure 3. The sub-segments, namely, so1 (on the boundary
b1 ), so2 (also known as sb1 ), and sb2 (beneath the boundary
b2 ) are enqueued, as shown in Line 7 in Algorithm 1. To
continue, after so1 is found inseparable, so2 is processed as
depicted in part (c) in Figure 3. The boundaries b3 and b4
are added as new vertices in G. b3 and b4 depend on b1 and
b2 so that the edges are added accordingly as depicted in
the lower section of part (c). The sub-segments, namely, so3 ,
so4 and sb4 , in the upper section of part (c) are enqueued.
For the sake of simplicity, we also use soi to denote the subsegment on the left of bi and sbi to denote the sub-segment
on the right of bi . Then, sb2 is processed as depicted in part
(d) in Figure 3. The boundary b5 and the edge b2 , b5  are
added into the graph. The construction process terminates
until Q is empty.
Several issues should be noted in the construction. First,
a candidate boundary cannot cut across any sub DOM tree.
For example, there is no subtree whose one part is in so1 and
the other part is in so2 . Second, when judging the separability
of a segment, the horizontal boundaries inside it are examined ﬁrst since they are more commonly used than vertical
boundaries. Third, in separability judgment, if a segment is
composed of a single DOM tree, we recursively use the lower
level subtrees of it instead. For example, if s0 is composed
of a single <table> and the table has several <tr>’s, we

(1)

where F is an objective function that evaluates the ﬁtness
of C for G. The variable w gives the weights of the DOM
structure, visual property and text content features. Such
design globally evaluates the ﬁtness of a label assignment C
for G so that the determined segmentation is more accurate.
Note that each legal label assignment must satisfy all the
dependency constraints depicted by the edges of G. Fundamentally, some existing methods such as [6, 7, 19] can also
be represented in the form of F (G, C; w). For example, the
visual block tree approach in VIPS [6] can be transformed
into our WPS-graph. And recursively segmenting the larger
blocks can be transformed into determining the labels of the
corresponding boundaries. The DoC criteria in VIPS can

769

will ﬁnd the boundaries between each neighboring pair of
<tr>’s. If so2 is composed of a single <table> with a single
<tr>, we will ﬁnd the boundaries between each neighboring
pair of <td>’s of the <tr>. Fourth, each vertex at most
directly depends on two other vertices. Theoretically, a candidate boundary b inside the segment s depends on all four
boundaries of s, among which b indirectly depends on at
least two and at most three of them. In part (e) in Figure 3,
we can see that b6 and b7 indirectly depend on b1 and b2 .

more suitable than the label N for the boundary b5 . If one
text segment contains very similar terms as in the title of
a news page, we probably should not split the boundary
beneath this segment since it will separate the title and the
main content of the news. According to the sources, the features are categorized into two types, namely, local feature,
and context feature,

3.3 Informative Boundary

Local features of a boundary bi are computed based on
the characteristics of its surrounding sub-segments, i.e., soi
and sbi . We design two types of local features, namely, local
segment features, and local segment relation features.
Local segment features are designed to capture the characteristics of a single segment soi or sbi . Let Φ(soi ) denote
the feature vector related to soi . The combined feature map
of soi and the label ci of bi is denoted as:

4.1 Local Features

The segments with essential information are normally arranged in conspicuous position of a page, such as the middle of the ﬁrst screen. Such segments are known as informative segment [13, 23]. Incorrectly segmenting informative segments causes larger loss. Taking the news content segment in a news page as an example, any segmentation that mistakenly segments the paragraphs of the content into diﬀerent segments is not a favorable result. This is
called over-segmentation mistake. On the other hand, if the
news content segment is combined with other segments and
becomes a subpart of the combined segment, insuﬃcientsegmentation mistake occurs. To make the informative segments accurately segmented, we deﬁne the related boundaries as informative boundaries and give them some special
treatment.

Ψl (soi , ci ) ≡ Φ(soi ) ⊗ Λc (ci ),

where ⊗ is the operator of tensor multiplication, Λ (ci ) is
the canonical representation of the label ci :
Λc (ci ) ≡ (δ(Y, ci ), δ(N, ci )),

Ψl (sbi , ci ) ≡ Φ(sbi ) ⊗ Λc (ci ).

(5)

Local segment features include basic features, geographic
features, color features, content features, text appearance
features, text richness features, tag richness features, font
size features, etc. Some examples are the number of links
in the segment, the background color of the segment, the
number of terms in a segment that also appear in the page
title, the token based text density over the segment size, etc.
Local segment relation features reveal the relation of the
two segments. Let Φ(soi , sbi ) represent the features summarized for capturing the relations between soi and sbi . The
combined feature map is denoted as:

Suppose so4 is an informative segment in the demo page in
Figure 3, b1 , b2 , b3 , b4 , b6 , and b7 are informative boundaries.
Since b3 and b4 depend on b1 and b2 , we only need to ensure
that b3 and b4 are correctly labeled as Y. Similarly, after b6
and b7 are correctly labeled as N, the boundaries that depend
on b6 and b7 will be labeled as N automatically. Therefore,
we deﬁne proper informative boundary as follows.
Definition 3 (Proper Informative Boundary).
Suppose bi is an informative boundary and outside the informative segment, if there is no bj which directly depends
on bi and is also outside the informative segment, bi is a
proper informative boundary. Suppose bk is an informative
boundary and inside the informative segment, if bk directly
depends on a proper informative boundary outside the informative segment, bk is a proper informative boundary.

Ψl (soi , sbi , ci ) ≡ Φ(soi , sbi ) ⊗ Λc (ci ).

(6)

Such features include height diﬀerence in the DOM, text
length diﬀerence, color diﬀerence, size diﬀerence, font size
diﬀerence, typeface diﬀerence, text similarity, etc.

4.2 Context Features

In the above example, b3 , b4 , b6 , and b7 are the proper informative boundaries.

Human readers also consider the context information in
identifying page segments. Take the page given in Figure 2(a) as an example. In addition to the local features
from so10 and sb10 , the sibling segments so8 and so9 also provide useful hints to determine the label of b10 . Suppose
the DOM structures of these four segments are similar, it is
very likely that they present four records of the same type
of information. Thus, it is probably not preferred to split
the boundary b10 . We design two types of context features,
namely, context segment features, and context segment relation features, for each bi to capture the characteristics of
the segments in the sibling sequence. Context segment features include the average DOM structure similarity with the
sibling segments, occurrence frequency based on DOM structure similarity, mean and standard deviation of occurrence

FEATURES

Let Ψ(G, C) denote the combined feature representation
of G and its label assignment C. Thus, the objective function
F in Equation 1 is formulated as:
F (G, C; w) ≡ Ψ(G, C), w,

(4)

where δ(Y, ci ) is an indicator function and has the value 1 if
ci = Y and the value 0 otherwise. As revealed by Equation 3,
each single feature is mapped to a dimension according to
the label ci . Similarly, the combined feature map of the
segment sbi and the label ci is denoted as:

Definition 2 (Informative Boundary). If a boundary is mistakenly labeled, the related informative segment
will be overly or insuﬃciently segmented. Such boundary is
an informative boundary.

4.

(3)
c

(2)

which is the linear combination of the features in Ψ(G, C)
with their corresponding weights given in w. For each boundary b in B of G, we deﬁne a group of features from its surrounding segments to assist the determination of its label.
For the example in Figure 2(a), if the segments so5 and sb5
have diﬀerent background colors, the label Y is probably

770

intervals, etc. Context segment relation features reveal the
relation of the two segments’ context features, such as the
diﬀerence of occurrence frequency, occurrence characteristics of the forest of them, etc. The combined context feature
maps are denoted as:
Ψc (soi , ci )

≡ Υ(soi ; S) ⊗ Λc (ci ),

c

Ψ (sbi , ci )
c o b
Ψ (si , si , ci )

≡
≡

The second constraint ensures that if there is an edge from
bi to bj in E of G and bi has the label N, i.e., xi = 0, bj must
also have the label N, i.e., xj = 0; if bj has the label Y, i.e.,
xj = 1, bi must also have the label Y, i.e., xi = 1. After
removing the unchanged term F i in Fi∗ , Formula 15 can be
equivalently written as the following compacted form:

(7)

Υ(sbi ; S) ⊗ Λc (ci ),
Υ(soi , sbi ; S) ⊗ Λc (ci ),

max f  x, s.t. x ∈ {0, 1}n and Ax ≤ {0}m ,

(8)
(9)

where f  is the coeﬃcient vector and fi = Fi −F i , A ∈ Rm×n
is the constraint matrix, where m = |E|. Each constraint
xi ≥ xj corresponds to one row in A whose j-th element is
+1, i-th element is -1 and other elements are 0.
However, binary ILP has been proved to be NP-hard [35].
To solve it, we ﬁrst relax the binary ILP in Equation 16 to
a linear programming (LP) problem which has eﬃcient and
widely used solvers such as simplex [11]. Then, it can be
proved that the linear relaxation has an integral optimal solution, which is thus also the optimal solution of the original
binary ILP problem. The constraint x ∈ {0, 1}n is written as
n
n
m
x ∈ Zn
+ and x ≤ {1} . x ≤ {1} and
 Ax≤ {0} are jointly

A
{0}m
written as Bx ≤ b, where B =
, b =
.
n
{1}
In
Thus, we get an ILP problem:

where S is the sequence of the corresponding sibling segments, Υ is the features extracted according to the characteristics of the segments in S.

4.3 Aggregated Feature Representation
By aggregating the above features, the combined feature
map of a boundary bi and its label ci is presented as:
⎛⎛
⎜
Ψ(bi , ci ) ≡ ⎝⎝

(16)

⎞ ⎛
⎞ ⎞
Ψc (soi , ci )
Ψl (soi , ci )

⎠ ⎝ Ψc (sb , ci ) ⎠ ⎟
Ψl (sbi , ci )
⎠ . (10)
i


l
o
b
Ψ (si , bi , ci )
Ψc (soi , sbi , ci )

The combined feature representation of a WPS-graph G and
its label assignment C is the combination of the feature map
from each boundary:

Ψ(G, C) ≡
Ψ(bi , ci ).
(11)

max f  x, s.t. x ∈ Zn
+ and Bx ≤ b.

bi

(17)

As shown above, diﬀerent features are combined and the
diﬀerence of their impacts will be captured by the corresponding weights in w.

The linear relaxation of the problem in Formula 17 is:

5.

One nice property of the LP problem in Formula 18 is that
the constraint matrix B is totally unimodular [35], which
can be proved straightforwardly and is omitted due to the
tight space. This property guarantees that the LP problem
has an integral optimal solution for any integer vector b
for which it has a ﬁnite optimal value [35]. Therefore, the
integral optimal solution of the LP in Formula 18 is also an
optimal solution of the ILP in Formula 17. Obviously, it is
also an optimal solution of the binary ILP in Formula 16,
from which the ILP is transformed.
To speed up the inference algorithm, the leaf vertex bi in
G whose value satisﬁes F i ≥ Fi can be safely labeled as N
and removed from the inference procedure. The reason is
that the labeling of its ancestor vertices does not aﬀect bi .
Obviously, this removal preprocessing is recursive until each
of the remaining leaf vertex bj has F j < Fj .

max f  x, s.t. x ∈ Rn
+ and Bx ≤ b.

INFERENCE OF SEGMENTATION

To infer a label assignment satisfying the dependency constraints, one strategy is to consider the vertices one by one in
the topological order as depicted by the WPS-graph. Only
when all the ancestors of a vertex are labeled with Y, we
will evaluate Y and N for it. Otherwise, we assign the label
N to this vertex. However, this manner is myopic and cannot achieve an optimal solution. To overcome this problem,
we formulate the label inference on G as a binary Integer
Linear Programming (ILP) problem with the label dependency constraints transferred as the constraints of the binary
ILP. The source code of the inference is publicly available at
http://www.se.cuhk.edu.hk/~textmine/.
Let Fi∗ denote the partial objective value achieved by the
vertex bi with the label c∗i in the optimal label assignment
C∗ . Fi∗ can be represented as:
Fi∗ = (Fi − F i )xi + F i ,
where xi =

δ(Y, c∗i ).

(12)

6. TRAINING

Fi and F i are calculated as follows:
= Ψ(bi , Y), w,
= Ψ(bi , N), w.

Fi
Fi

We develop a machine learning algorithm based on the
structured output Support Vector Machine framework [32]
to determine the feature weights. Our learning algorithm
considers the inter-dependency among the vertices of a WPSgraph during the training process. Therefore, the feature
weights are determined with a global view on WPS-graphs
but not merely on individual segmentation boundaries. The
source code of this learning framework is publicly available
at http://www.se.cuhk.edu.hk/~textmine/.

(13)
(14)

If xi = 0, we have Fi∗ = F i and c∗i = N. Otherwise, we have
Fi∗ = Fi and c∗i = Y.Thus, the optimal value of F can be
n
∗
computed as F ∗ =
i=1 Fi , where n = |B|. The task of
ﬁnding the optimal label assignment as given in Equation 1
can be transformed as solving a binary ILP problem:
max

n

i=1

Fi∗

=

n

max (
(Fi − F i )xi + F i ),

6.1 Learning Framework

i=1

s.t.

xi ∈ {0, 1},
bi , bj  ∈ E ⇒ xi ≥ xj .

(18)

Let {(Gi , Ci )}N
i=1 denote a set of training data instances.
The quadratic program form of the SVM model with slack

(15)

771

re-scaled by the loss is:
min
w,ξ

s.t.

C
1
||w||2 +
2
N

N


Algorithm 2: Finding feature weights via structured
output SVM learning.
1: initialization: {(Gi , Ci )}N
i=1 , C, ε, ∀i : Si ← ∅
2: repeat
3: for i = 1, · · · , N do

ξi

i=1

∀i, ∀C ∈ Y \ Ci : δΨi (C), w ≥ 1 −

ξi
, (19)
Δ(Ci , C)

4:

where ξi ≥ 0 is the slack variable of Gi , C > 0 is a tradeoﬀ constant of the two parts and takes value 1 in this paper, Y is the set of all possible label assignments of Gi ,
δΨi (C), w = F (Gi , Ci ; w) − F (Gi , C; w) is the margin between the objective values of Ci and C, and Δ(Ci , C) denotes the loss caused by the label assignment C. Similarly,
the quadratic program form of the SVM model with margin
re-scaled by the loss is:
min
w,ξ

s.t.

5:
6:
7:
8:
9:
10:
11:
12:

N
C 
1
||w||2 +
ξi
2
N i=1

boundaries wrongly labeled in C∗ . Let {Binf o } be all subsets

of Binf o having more elements than B×
inf o , and let C be the
label assignment that achieves the largest F value when all
boundaries in Binf o are wrongly labeled and all boundaries in
Binf o \ Binf o are correctly labeled. The label Ĉ maximizing
H(C) is from ∪Binf o {C } ∪ {C∗ }.

∀i, ∀C ∈ Y \ Ci : δΨi (C), w ≥ Δ(Ci , C) − ξi . (20)

Tsochantaridis et al. proposed a cutting plane based algorithm to solve this optimization problem in its dual formulation [32]. It selects a subset of constraints from the
exponentially large set Y to ensure a suﬃciently accurate
solution. The procedure of ﬁnding the feature weights is
brieﬂy summarized in Algorithm 2. Si is the working set of
selected constraints for the instance Gi , α’s are the Lagrange
multipliers, and ε is the precision parameter. The algorithm
proceeds by ﬁnding the most violated constraint for Gi involving Ĉ (refer to Line 5). If the margin violation of this
constraint exceeds the current ξi by more than ε (refer to
Line 7), the working set Si of Gi is updated. α’s and w are
also updated with the updated working set accordingly. We
refer the reader to [32] for more details of the algorithm.
In the learning procedure as depicted in Algorithm 2, it is
required to optimize the cost function in Line 4 for ﬁnding
the most violated constraint corresponding to Ĉ:
Ĉ = argmax H(C).

The proof of Proposition 1 is straightforward and omitted
due to the space limitation.

6.3 Optimization for Margin Re-scaling
The margin re-scaling formulation in Formula 20 is designed to perform a general segmentation of pages. For this
design, we deﬁne a hamming distance based loss function:

Δ(Ci , C) ≡
(24)
δ̄(Ci (b), C(b)),
b∈B

where each boundary is treated equally. To optimize the
second cost function as given in Equation 25:

(21)

C∈Y

H(C) ≡ Δ(Ci , C) − δΨi (C), w,
H  (C) ≡ Δ(Ci , C) + F (Gi , C; w).

Recall that the missing of the informative segments of a
page causes larger loss. Our slack re-scaling formulation as
given in Formula 19 is able to take this into consideration
with a loss function deﬁned based on informative segments:
b∈Binf o

|Binf o |

},





Ĥi = (Hi − H i )xi + H i ,
where xi = δ(Y, ĉi ). Hi and

(22)


Hi

(27)

are calculated as follows:

Hi = δ(ci , Y) + Ψ(bi , Y), w,

H i = δ(ci , N) + Ψ(bi , N), w.

where Binf o denotes the set of proper informative boundaries, C(b) is the label of b in C, δ̄ is an indicator function
which takes the value 0 if Ci (b) = C(b) and takes the value
1 otherwise. If Binf o = ∅, we set Δ(Ci , C) = 1.
To optimize the following cost function:
H(C) ≡ (1 − δΨi (C), w)Δ(Ci , C),

(26)

Let Ĉ denote the label that achieves the largest value of
H  , and Ĥi denote the partial value of H  achieved by the
vertex bi having the label Ĉ (bi ), denoted as ĉi . Ĥi can be
represented as:

6.2 Optimization for Slack Re-scaling

Δ(Ci , C) ≡ exp {

(25)

it is equivalent to optimize:

The upper and the lower forms of H(C) in Line 4 correspond
to the slack re-scaling and margin re-scaling deﬁnitions as
given in Formulae 19 and 20 respectively.

δ̄(Ci (b), C(b))

(1 − δΨi (C), w)Δ(Ci , C)
Δ(Ci , C) − δΨi (C), w
Ĉ = argmaxC H(C)
ξi = max {0, maxC∈Si H(C)}
if H(Ĉ) > ξi + ε then
Si ← Si ∪ {Ĉ}
update α’s and w with ∪i Si
end if
end for
until no Si has changed during iteration
H(C) ≡

(28)
(29)

Referring to Equations 12, 13, and 14, the task of ﬁnding
the label assignment Ĉ maximizing H  can also be solved
in the same way as given in Section 5.

(23)

7. EXPERIMENTS

we enumerate a subset of possible loss value levels as deﬁned in Equation 22. The derivation of the optimal Ĉ for
Equation 23 is summarized as Proposition 1.

7.1 Experimental Setting
Data Preparation. We categorize the pages on the Web
into 10 broad types as given in the ﬁrst column of Table 1.
1,000 pages of the ﬁrst 9 types were randomly picked from a

Proposition 1. Let C∗ denote the label that achieves the
optimal value for F and B×
inf o denote the proper informative

772

Table 2: Comparison of segmentation results on the
entire data set.

Table 1: Diﬀerent types of pages on the Web and
the number of each type in our data set.
Functional Description
type (ID)
Index (1) The major part of the page is composed of links
for the navigation purpose, such as the thread list
of forums, the home page of a portal site, etc.
Image (2) The page is designed to present images.
Forum (3) Presenting the major content of forum posts.
Product (4)Presenting the detailed descriptions of products.
Search Re- Presenting the search result of various search ensult (5)
gines.
Blog (6)
Presenting the major content of Blog posts.
Download Presenting the download information for soft(7)
wares, music, and videos.
News (8) Presenting the major content of the daily news, or
the major content of various articles.
Video (9) The page is designed to present videos.
Other (10) Including adult pages, wap pages, etc.
Sum

Page
#
366

180
100
71
70
69
50
48
46
0
1,000

large page repository, since adult pages are normally omitted by applications and wap pages are designed diﬀerently
compared with normal Web pages. The number of pages
in diﬀerent types is given in the third column of Table 1.
The total number of pages in our data set is almost 10 times
of that used in some previous works [7, 19]. In addition,
the smallest type contains 46 pages, which is a reasonable
number for conducting type-speciﬁc experiment. To perform
data annotation, we developed a browser-based user-friendly
tool for annotators to specify the label of each boundary. After the annotators ﬁnish annotating one page, they label the
informative segment in the page. If the page is not an index
page, the segment that presents the major information of
the page is annotated as informative segment, such as the
news content segment of a news page and the result segment
of a Web search page.
Evaluation Metrics. The segmentation result generated by a Web page segmentation method groups the visual
elements of a Web page into cohesive regions visually and semantically. Similar to the previous works [7, 19], we regard
each generated segment as a cluster of visual elements and
employ cluster correlation metrics to conduct the evaluation. The ﬁrst metric is the Adjusted Rand Index (ARI) [18].
Rand Index is deﬁned to measure the agreement between an
output clustering and the ground truth clustering by counting the pairs of elements on which two clusterings agree [28].
The Rand Index lies between 0 and 1, with 0 indicating that
the two clusters do not agree on any pair of elements and 1
indicating that the two clusters are exactly the same. ARI
is a corrected-for-chance version of the Rand Index, which
equals 0 on average for random partitions, and 1 for two
identical partitions. Therefore, the larger the ARI value is,
the better the performance is. Mutual Information (MI) is
a symmetric measure to quantify the statistical information
shared between two distributions [10]. It can provide an
indication of the shared information between a pair of clusterings. The second metric employed in this paper is the
Normalized Mutual Information (NMI) introduced in [30],
which is the MI between two clusterings normalized with
the geometric mean of their entropies. NMI ranges from 0
to 1 and larger value indicates better performance.
Comparison Methods. Kohlschütter and Nejdl observed that the number of tokens in a text fragment, i.e.

WPS Slack
WPS Margin
BF-RULEBASED (ϑmax = 0.65)
CCLUS (λ = 0.62)
GCUTS (λ = 0.53)

ARI
0.732
0.749
0.605
0.489
0.630

NMI
0.824
0.841
0.761
0.662
0.770

text density, is a valuable feature for segmentation decisions [19]. Therefore, they proposed a block fusion model
that utilizes the text density ratios of subsequent blocks to
identify segments, where the Web page segmentation problem is reduced to solving a 1D-partitioning task. Among
the variants of their model, BF-RULEBASED achieves the best
performance. BF-RULEBASED constrains the density-based fusion operation between subsequent blocks with a set of gapenforcing tags and a set of gap-avoiding tags. We implemented BF-RULEBASED for conducting comparison. The optimal fusing threshold ϑmax is tuned with the training set of
our data. Chakrabarti et al. proposed a graph-theoretic approach to deal with Web page segmentation [7]. They cast
the problem as a minimization problem on a weighted graph
with the nodes as the DOM tree nodes and the edge weights
as the cost of placing the end nodes in the same segment
or diﬀerent segments. They presented a learning framework
to learn these weights from manually labeled data. The
proposed CCLUS algorithm solves this problem with correlation clustering on a graph that only contains leaf DOM
nodes of a page as the nodes of the weighted graph. The
proposed GCUTS algorithm solves this problem with energyminimizing cuts on a graph that regards each DOM node as
a node of the graph. GCUTS involves a rendering constraint
to ensure that, if the root node of a subtree is in a particular segment, all the nodes in the entire subtree are in the
same segment. We implemented both CCLUS and GCUTS to
conduct comparison. Our training data is employed to learn
the feature weights as well as the trade-oﬀ parameter λ of
two counterbalancing costs in the objective functions.

7.2 Overall Segmentation Results
Recall that, in Section 6.1, we employ two quadratic forms
of SVM model, namely, slack re-scaling and margin re-scaling,
which incorporate informative segment oriented loss and
Hamming loss respectively. Accordingly, we have two variants of our model, named WPS Slack and WPS Margin respectively. The learning precision ε for the feature weight
estimation in Algorithm 2 is set to 0.1.
We ﬁrst conduct experiment on the entire data set containing 1,000 pages. 4-fold cross-validation is employed and
the average performance evaluated with ARI and NMI is
reported in Table 2. Both variants of our model can outperform the comparison methods signiﬁcantly. The improvements in ARI values over BF-RULEBASED and GCUTS are about
25%. In addition, paired t-tests (with P < 0.01) comparing
the variants of our model with the comparison methods show
that the performance of our variants is signiﬁcantly better. Among diﬀerent variants of our model, WPS Margin
with Hamming loss can achieve better performance than
WPS Slack with informative segment oriented loss. It is
because WPS Slack favors the segmentations that generate
more accurate informative segments, which makes it perform
less accurately on the uninformative segments. While the

773

90

ARI interval

WPS_Slack
WPS_Margin
BF−rulebased
Cclus
Gcuts

80
70
60

Stacked percentage

Percentage of pages below an ARI value

50

100

50
40

[0.9,
[0.8,
[0.7,
[0.6,
[0.5,
[0.4,
[0.3,
[0.2,
[0.1,
[0.0,

40
30
20
10

30

0

20

Index Image Forum Product Search

0.1

Blog DownloadNews

Page type

10
0.2

0.3

0.4

0.5

ARI

0.6

0.7

0.8

0.9

Table 4: Results of informative block extraction.

Table 3: Type-speciﬁc segmentation results in ARI.
1
.745
.778
.617
.528
.674

2
.771
.795
.640
.532
.687

3
.761
.787
.662
.529
.665

4
.763
.786
.657
.547
.691

5
.802
.829
.687
.564
.703

6
.794
.819
.671
.553
.697

7
.769
.793
.653
.540
.684

8
.757
.775
.701
.532
.669

Video

Figure 5: Stacked percentage in diﬀerent ARI value
intervals by WPS Margin.

1.0

Figure 4: Percentage of pages below an ARI value.

Type ID
WPS Slack
WPS Margin
BF-RULEBASED
CCLUS
GCUTS

1]
0.9)
0.8)
0.7)
0.6)
0.5)
0.4)
0.3)
0.2)
0.1)

9
.782
.807
.654
.543
.698

Type ID
WPS Slack
WPS Margin
BF-RULEBASED
GCUTS

2
.91
.88
.62
.64

3
.90
.88
.59
.61

4
.92
.86
.63
.60

5
.93
.88
.72
.78

6
.93
.88
.75
.78

7
.90
.83
.73
.71

8
.94
.85
.75
.71

9 ALL\Index
.90
.88
.83
.84
.63
.68
.65
.70

annotated for them, which makes WPS Slack less eﬀective
evaluation metrics ARI and NMI do not consider the imporin tackling index pages compared with tackling the other
tance diﬀerence among the segments, which gives WPS Margin
types. Note that our framework does not have type-speciﬁc
more advantage in the reported performance.
features, since we assume that the type of the pages is unGCUTS achieved slightly better performance than BF-RULEBASED. known. The stacked percentage in diﬀerent ARI value interAnd CCLUS achieved the lowest accuracy, because it revals for individual page types is given in Figure 5. Besides
ports many non-rectangular segments since the built graph
the types of Product and News, the percentage of ARI value
of it only contains the leaf DOM nodes. Generally, nonlower than 0.6 is no more than 20%.
rectangular segment should not exist according to common
sense. We observe that the page designers now prefer us7.4 Informative Segment Results
ing <div> and <span> tags together with Cascading Style
Sheets (CSS) in page design. This makes the heuristics
Recall that besides the index pages, we annotated inforbased on gap-enforcing tags in BF-RULEBASED less eﬀective.
mative segments in the pages of other types. We conduct
Similar to our method, GCUTS is more adaptable since its
evaluation on the performance of our method for segmentfeature weights are tuned with training examples. It also
ing these informative segments. If the informative segment
solves the problem of reporting non-rectangular segments to
of a page is accurately segmented, we regard this page sucsome extent with the rendering constraint. The cumulative
cessfully handled. If the informative segment is regarded
percentage of Web pages for which the segmentation perforas a subpart of any other segment or it is separated into
mance of a particular method is less than a certain ARI value
several sub-segments, this page is not successfully handled.
is plotted in Figure 4. The slower the curve goes up from
We calculate the percentage of the pages whose informative
left to right, the better the corresponding method is. For
segments are successfully segmented.
CCLUS, BF-RULEBASED and GCUTS, the percentages of ARI
In addition to the eight individual sets of pages, we have
value lower than 0.6 are about 82%, 56% and 52% respecanother data set that aggregates the pages of these eight
tively. For the variants of our framework, such percentages
types and it is called ALL\Index. The average results from
are between 20% to 30%.
4-fold cross-validation are reported in Table 4. The WPS Slack
with informative segment oriented loss achieved the best
7.3 Type-specific Segmentation Results
performance and dominated other methods signiﬁcantly. It
demonstrates that the design of informative segment oriTo evaluate the type-speciﬁc performance of diﬀerent segented loss is helpful for capturing informative segments for
mentation methods, we employ the page set of each type
diﬀerent types. It also shows that our variants with diﬀerent
as an individual experimental data set. Also 4-fold crossloss designs have their own advantages to handle segmenvalidation is conducted on each of the nine page sets. The
tation tasks with diﬀerent focuses, making our framework
results evaluated with ARI are reported in Table 3. We
more adaptable compared with previous works. The trained
ﬁnd that all methods can achieve better performance on
segmentation models on the individual types are more tailoran individual page type compared with on the entire data
made so as to achieve better results compared with that on
set. This is because the trained or tuned parameters in difALL\Index. After some manual checking, we found that
ferent methods are more tailor-made for a particular type
most unsuccessful cases include some noise elements as part
so as to achieve better accuracy. Among diﬀerent types of
of the informative segments, such as the comments in the
pages, Search Result and Blog are relatively easier to hannews and blog pages. In BF-RULEBASED, one heuristic rule is
dle. The main reason is that these two types of pages have
relatively simple structures. Index and News are the most
that a segmentation gap should be enforced after the tags
diﬃcult types. The reason is that these two types of pages
<h1>-<h6>. Consequently, BF-RULEBASED always splits the
have more heterogenous structures and various information
title formatted with <h1>-<h6> of an informative segment
topics. In addition, the loss function in WPS Slack is set
and the main content of the segment, which results in oversegmentation mistakes.
to 1 for the index pages since no informative segments are

774

Algorithm 3: In-segment position ﬁnding.
1: initialization: s(t) ← ∅, s(b) ← ∅, s(l) ← ∅, s(r) ← ∅,
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:

8.

Table 5: The results of page classiﬁcation.

s(m) ← ∅, {bi }n
i=1 : boundaries having the label Y in s
if n = 0 then
s(m) ← s
else if n = 1 then
if {bi }n
i=1 are horizontal then
s(t) ← {so1 }, s(b) ← {sb1 }
else
s(l) ← {so1 }, s(r) ← {sb1 }
end if
else
if n = 2 then
b ← b1 , b ← b2
else
calculate Fi = Ψ(bi , Y), w for each bi
b ← argmaxbi Fi , b ← argmaxbj ∈{bi }n \b Fj
i=1
end if
n
if {bi }i=1 are horizontal then
s(t) ← the segments above b
s(m) ← the segments between b and b
s(b) ← the segments below b
else
s(l) ← the segments on the left of b
s(m) ← the segments between b and b
s(r) ← the segments on the right of b
end if
end if

Our method
Unstructured Text
BF-RULEBASED Segment
GCUTS Segment

Precision
0.945
0.698
0.755
0.772

Recall
0.926
0.673
0.735
0.745

F1
0.936
0.685
0.745
0.758

by segmenting the entire page. In Figure 3, the major segments are s0 in (a) and so2 in (b). Suppose b1 , b2 , b3 , and
b4 are labeled as Y by our segmentation model. Thus, s0
is segmented into s0 (t) containing so1 , s0 (m) containing so2 ,
and s0 (b) containing sb2 as shown in Figure 3(b). so2 is segmented into so2 (l) containing so3 , so2 (m) containing so4 , and
so2 (r) containing sb4 as shown in Figure 3(c). After a page is
segmented by our model and the in-segment positions of the
major segments are determined by Algorithm 3, we calculate
the in-segment position based TF-ISF value for each term t
in the individual positions, where TF is the frequency of t
in this position of the page and ISF is the inverted segment
frequency of t calculated based on the same position across
the corpus. Therefore, a single term is decomposed into 10
diﬀerent dimensions in the feature vector according to its
in-segment positions in the two major segments. After some
basic preprocessing such as stop word removal, we perform
feature selection with information gain (IG) [36]. Only the
top 10% of terms are retained in the construction of the
feature vector so as to control the dimensionality.

APPLICATION IN WEB PAGE CLASSIFICATION

8.2 Experimental Setting and Results
We prepare another collection of 4,000 pages from the
eight types having informative segments as indicated in Table 4. Each type contains about 500 pages. LibSVM [8] with
linear kernel is employed to train eight classiﬁers under oneagainst-the-rest strategy for multi-class classiﬁcation. To
conduct comparison, three baseline methods are designed.
The ﬁrst baseline, named Unstructured Text, is a purely
text-based method without considering page structure. It
employs all terms after preprocessing to form the feature
vectors. We design the other two comparison methods based
on the informative segments detected by BF-RULEBASED and
GCUTS. The informative segment of each page is identiﬁed
with the following rule. The largest segment that appears
(maybe partially) in the ﬁrst screen of a page is regarded as
its informative segment. The ﬁrst screen of a page is deﬁned
as the top fraction of the page with the height of 1,000 pixels.
Then, the feature vector of a page employs the terms appearing in its informative segment as the dimensions. These two
baselines are named BF-RULEBASED Segment and GCUTS Segment
respectively. For our method, the employed segmentation
model is the variant of WPS with margin re-scaling. All the
segmentation models are tuned or trained with the entire
data set in Table 1.
The average results, evaluated with macro-averaged Precision, Recall and F1 measure, of 4-fold cross-validation are
reported in Table 5. Our method achieves signiﬁcantly better performance compared with the other methods. The percentages of improvements in F1 are about 23% to 37%. This
demonstrates that the page structure information, revealed
by the in-segment positions in our method, is very useful in
the classiﬁcation of page functional types. The F1 values
of diﬀerent page types are given in Figure 6. We observe
that BF-RULEBASED Segment and GCUTS Segment face more
diﬃculties in handling three types of pages, namely, Blog,

8.1 Feature Extraction for Classification
As discussed above, the output of our segmentation model
provides a good structural analysis of Web pages enabling
better utilization for diﬀerent Web page mining tasks. Such
eﬃcacy of our model is examined in the task of page functional type classiﬁcation [26]. Diﬀerent from topical type,
functional type describes the role that a Web page plays,
such as image page mainly presenting an image, video page
mainly presenting a video, etc. The identiﬁcation of functional type is very useful for diﬀerent Web mining problems.
For example, search result ranking normally considers functional type as one factor. Page crawler can also trigger a
better crawling strategy given the type of crawled pages.
The functional type of a page is closely related to the
page structure and the functional terms appearing in diﬀerent positions of the page. For example, the functional terms
“reply” and “post” in the informative segment of a forum
page are indicative features. The term “forum” in the header
and bottom sections is also an important feature. To utilize
the output of our segmentation model in this classiﬁcation
task, ﬁve diﬀerent in-segment positions are deﬁned, namely,
top, bottom, left, right, and middle. For a segment s, these
positions are denoted as s(t), s(b), s(l), s(r), and s(m) respectively. Let b1..n denote the boundaries having the label
Y inside s. The procedure of ﬁnding the in-segment positions
of s is given in Algorithm 3. Note that an in-segment position can contain more than one subsegments when n > 2.
To construct the feature vector of each input page, we
consider the in-segment positions in two major segments of
the page obtained from diﬀerent layers of the segmentation
procedure. The ﬁrst major segment is the entire page and
the second major segment is the largest segment obtained

775

[10] T. M. Cover and J. A. Thomas. Elements of information
theory. 1991.
[11] G. B. Dantzig and M. N. Thapa. Linear Programming 1:
Introduction. Springer-Verlag New York, Inc., 1997.
[12] E. S. de Moura, D. Fernandes, B. Ribeiro-Neto, A. S. da Silva,
and M. A. Gonçalves. Using structural information to improve
search in web collections. J. Am. Soc. Inf. Sci. Technol.,
61(12):2503–2513, 2010.
[13] S. Debnath, P. Mitra, N. Pal, and C. L. Giles. Automatic
identiﬁcation of informative sections of web pages. IEEE
Trans. on Knowl. and Data Eng., 17(9):1233–1246, 2005.
[14] D. Fernandes, E. S. de Moura, A. S. da Silva, B. Ribeiro-Neto,
and E. Braga. A site oriented method for segmenting web
pages. In SIGIR, pages 215–224, 2011.
[15] S. D. Gollapalli, C. Caragea, P. Mitra, and C. L. Giles.
Researcher homepage classiﬁcation using unlabeled data. In
WWW, pages 471–482, 2013.
[16] Q. Hao, R. Cai, Y. Pang, and L. Zhang. From one tree to a
forest: a uniﬁed solution for structured web data extraction. In
SIGIR, pages 775–784, 2011.
[17] G. Hattori, K. Hoashi, K. Matsumoto, and F. Sugaya. Robust
web page segmentation for mobile terminal using
content-distances and page layout information. In WWW,
pages 361–370, 2007.
[18] L. Hubert and P. Arabie. Comparing partitions. Journal of
Classification, 2(1):193–218, 1985.
[19] C. Kohlschütter and W. Nejdl. A densitometric approach to
web page segmentation. In CIKM, pages 1173–1182, 2008.
[20] H. Lam and P. Baudisch. Summary thumbnails: readable
overviews for small screen web browsers. In CHI, pages
681–690, 2005.
[21] J. Lazar, A. Allen, J. Kleinman, and C. Malarkey. What
frustrates screen reader users on the web: A study of 100 blind
users. Int. J. Hum. Comput. Interaction, 22(3):247–269, 2007.
[22] X. Li, T.-H. Phang, M. Hu, and B. Liu. Using micro
information units for internet search. In CIKM, pages 566–573,
2002.
[23] S.-H. Lin and J.-M. Ho. Discovering informative content blocks
from web documents. In KDD, pages 588–593, 2002.
[24] C. Lu, L. Bing, and W. Lam. Structured positional entity
language model for enterprise entity retrieval. In CIKM, pages
129–138, 2013.
[25] J. Pasternack and D. Roth. Extracting article text from the
web with maximum subsequence segmentation. In WWW,
pages 971–980, 2009.
[26] X. Qi and B. D. Davison. Web page classiﬁcation: Features and
algorithms. ACM Comput. Surv., 41(2):12:1–12:31, 2009.
[27] K. Radinsky and P. N. Bennett. Predicting content change on
the web. In WSDM, pages 415–424, 2013.
[28] W. M. Rand. Objective Criteria for the Evaluation of
Clustering Methods. Journal of the American Statistical
Association, 66(336):846–850, 1971.
[29] R. Song, H. Liu, J.-R. Wen, and W.-Y. Ma. Learning block
importance models for web pages. In WWW, pages 203–211,
2004.
[30] A. Strehl and J. Ghosh. Cluster ensembles — a knowledge
reuse framework for combining multiple partitions. J. Mach.
Learn. Res., 3:583–617, 2003.
[31] F. Sun, D. Song, and L. Liao. Dom based content extraction
via text density. In SIGIR, pages 245–254, 2011.
[32] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun.
Large margin methods for structured and interdependent
output variables. Journal of Machine Learning Research,
6:1453–1484, 2005.
[33] K. Vieira, A. S. da Silva, N. Pinto, E. S. de Moura, J. a. M. B.
Cavalcanti, and J. Freire. A fast and robust method for web
page template detection and removal. In CIKM, pages 258–267,
2006.
[34] J. Wang, C. Chen, C. Wang, J. Pei, J. Bu, Z. Guan, and W. V.
Zhang. Can we learn a template-independent wrapper for news
article extraction from a single training site? In KDD, pages
1345–1354, 2009.
[35] L. A. Wolsey. Integer programming. Wiley, 1998.
[36] Y. Yang and J. O. Pedersen. A comparative study on feature
selection in text categorization. In ICML, pages 412–420, 1997.
[37] L. Yi, B. Liu, and X. Li. Eliminating noisy information in web
pages for data mining. In KDD, pages 296–305, 2003.

1
0.9
0.8

F1

0.7
0.6

Our method
Unstructured_Text
BF−rulebased_Segment
Gcuts_Segment

0.5
0.4
0.3

Image

Forum

Product

Search

Blog

Page type

Download

News

Video

Figure 6: Classiﬁcation performance for diﬀerent
page types.
News, and Search Result. It is because these types of pages
have very few functional terms in their main content. The
two informative-segment-based methods cannot well distinguish the main content of a news page and that of a blog
page. Although the baseline Unstructured Text keeps the
functional terms outside the informative segments, it does
not have the structure information to diﬀerentiate the appearances of the same term as functional term in a speciﬁc
position and as a normal term in the main content. Therefore, its performance is even degraded by the negative eﬀect
of the noise.

9.

CONCLUSIONS

We propose a framework which can perform page segmentation with a structured prediction approach. The segmentation task is formulated as a structured labeling problem on
the WPS-graph. Each labeling scheme on the WPS-graph
corresponds to a possible segmentation of the page. The
feature weight learning algorithm is developed based on the
structured output Support Vector Machine framework so
that it is able to consider the inter-dependency among the
vertices of a WPS-graph. Extensive experiments demonstrate that our framework achieves better performance compared with state-of-the-art methods.

10. REFERENCES
[1] S. Baluja. Browsing on small screens: recasting web-page
segmentation into an eﬃcient machine learning framework. In
WWW, pages 33–42, 2006.
[2] Z. Bar-Yossef and S. Rajagopalan. Template detection via data
mining and its applications. In WWW, pages 580–591, 2002.
[3] L. Bing, W. Lam, and Y. Gu. Towards a uniﬁed solution: Data
record region detection and segmentation. In CIKM, pages
1265–1274, 2011.
[4] L. Bing, W. Lam, and T.-L. Wong. Robust detection of
semi-structured web records using a dom
structure-knowledge-driven model. ACM Trans. Web,
7(4):21:1–21:32, 2013.
[5] L. Bing, W. Lam, and T.-L. Wong. Wikipedia entity expansion
and attribute extraction from the web using semi-supervised
learning. In WSDM, pages 567–576, 2013.
[6] D. Cai, S. Yu, J.-R. Wen, and W.-Y. Ma. VIPS: a vision-based
page segmentation algorithm. Technical report, Microsoft
(MSR-TR-2003-79), 2003.
[7] D. Chakrabarti, R. Kumar, and K. Punera. A graph-theoretic
approach to webpage segmentation. In WWW, pages 377–386,
2008.
[8] C.-C. Chang and C.-J. Lin. Libsvm: A library for support
vector machines. ACM Trans. Intell. Syst. Technol.,
2(3):27:1–27:27, 2011.
[9] Y. Chen, W.-Y. Ma, and H.-J. Zhang. Detecting web page
structure for adaptive viewing on small form factor devices. In
WWW, pages 225–233, 2003.

776

