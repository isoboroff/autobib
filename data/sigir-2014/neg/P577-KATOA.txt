Investigating Users’ Query Formulations
for Cognitive Search Intents
Makoto P. Kato, Takehiro Yamamoto, Hiroaki Ohshima, and Katsumi Tanaka
Kyoto University, Yoshida Honmachi, Sakyo, Kyoto, Japan 6068501

{kato,tyamamot,ohshima,tanaka}@dl.kuis.kyoto-u.ac.jp

ABSTRACT

the estimation of search intents has been intensively tackled in recent years [11, 13, 18, 19], and a better understanding of search intents has been considered as one of the most demanding challenges.
Although search intents on the topic of documents (or topical
search intents) have been dominantly addressed in the literature,
not only the topic but also the cognitive characteristics of documents can be specified by users’ search intents. The cognitive characteristics of documents are defined as the document characteristics
perceived by readers, and these include comprehensibility, subjectivity, and concreteness. Cognitive Search Intents (CSIs), which
are users’ needs for the cognitive characteristics of documents, can
be seen in a wide variety of searches, e.g. “I want to find comprehensible documents on black holes,” “a concrete explanation
of monopolies,” or “documents subjectively written about Black
Berry.” There have been several information retrieval (IR) research
projects that have estimated the cognitive characteristics of documents, e.g. those estimating the readability or comprehensibility
of documents [3, 17, 28], and the concreteness of contents in documents [38]. However, existing studies have not focused much attention on users, i.e. the need for CSIs or ways of expressing CSIs.
We especially need to understand users’ query formulations so that
we can correctly interpret their CSIs from user-input queries.
We investigated users’ query formulations for CSIs as an extension of our previous work [22]. The research questions we asked
were: (a) how do users express their CSIs in the form of a query?
(b) how different are verbalized search intents and queries for CSIs?
(c) how much demand for CSIs is there, and has been met by Web
search engines given queries input with CSIs? and (d) to what extent can a query expansion improve the search performance for
each type of CSIs? We administered a questionnaire-based user
study to answer these questions, where 1,800 subjects were recruited online and were asked to input queries for given CSIs. We
also evaluated how well a current Web search engine could satisfy
their CSIs in response to users’ and expanded queries.
Our four main contributions are summarized as follows: (i) we
proposed an example-based method of specifying search intents to
observe query formulations by users without biasing them by presenting a verbalized task description; (ii) we conducted a questionnairebased user study and found that about half our subjects did not input
any keywords representing CSIs, even though they were conscious
of CSIs; (iii) our user study also revealed that over 50% of subjects
occasionally had experiences with searches with CSIs, while our
evaluations demonstrated that the performance of a current Web
search engine was much lower when we not only considered users’
topical search intents but also CSIs; and (iv) we demonstrated that
a machine-learning-based query expansion could improve the performances for some types of CSIs.

This study investigated query formulations by users with Cognitive Search Intents (CSIs), which are users’ needs for the cognitive
characteristics of documents to be retrieved, e.g. comprehensibility, subjectivity, and concreteness. Our four main contributions are
summarized as follows: (i) we proposed an example-based method
of specifying search intents to observe query formulations by users
without biasing them by presenting a verbalized task description;
(ii) we conducted a questionnaire-based user study and found that
about half our subjects did not input any keywords representing
CSIs, even though they were conscious of CSIs; (iii) our user study
also revealed that over 50% of subjects occasionally had experiences with searches with CSIs, while our evaluations demonstrated
that the performance of a current Web search engine was much
lower when we not only considered users’ topical search intents but
also CSIs; and (iv) we demonstrated that a machine-learning-based
query expansion could improve the performances for some types of
CSIs. Our findings suggest users over-adapt to current Web search
engines, and create opportunities to estimate CSIs with non-verbal
user input.

Categories and Subject Descriptors
H.3.3. [Information Search and Retrieval]

Keywords
cognitive search intent; query formulation; Web search

1.

INTRODUCTION

While search engines have become an essential tool for a wide
range of purposes such as learning and investigations, users usually
input a few keywords into search engines despite having diverse
search intents [6, 40]. For example, users may input two keywords
“harry potter” with different search intents such as “I want to go to
the Harry Potter official site,” “buy Harry Potter books,” or “find a
page that subjectively introduces the story of Harry Potter.” Relevant search results would be different for those users. Therefore,
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2257-7/14/07 ...$15.00.
http://dx.doi.org/10.1145/2600428.2609566.

577

New types of search intents have been recently proposed as the
use of search engines and devices for searches have become more
diverse. Teevan et al. demonstrated that around 40% of queries
in Yahoo’s logs were input for re-finding information [39]. Church
and Smyth proposed a search intent taxonomy for mobile searches [14].
Based on a four-week diary study of mobile information needs,
they proposed personal information management search intents,
which they defined as the goal of finding something private that
related to an individual. Moshfeghi and Jose studied two types of
entertainment-based search intents: entertainment by adjusting the
arousal level, and entertainment by adjusting the mood [27]. They
investigated the cognition, emotion, and action aspects of different
video search tasks, and found a significant difference in the characteristics of the search tasks they studied.
The search intents discussed here can be regarded as the goals
of searchers, and are distinct from what users want to find, as users
can have a clear goal but for him/her not to know what is required
to achieve that goal.

Our findings suggest users over-adapt to current Web search engines, and create opportunities to estimate CSIs with non-verbal
user input. Moreover, we discuss problems of topic-dependent
query expansion for CSIs as well as estimating user-dependent cognitive characteristics toward developing a CSI-aware search engine.
The rest of this paper is organized as follows. Section 2 summarizes the previous work on search intents and query formulations.
Section 3 classifies search intents to clarify the definition of CSI.
Section 4 describes a questionnaire-based user study to investigate
users’ query formulations for CSIs, and Section 5 reports our findings from this user study. Section 6 discusses the findings from,
and implications of, this study, and Section 7 concludes the paper.

2.

RELATED WORK

We review work on search intents as well as query formulations
in the following subsections. Search intents have been defined
in two different ways: 1) a goal a user wants to achieve through
searches, and 2) information a user wants to find. We will discuss
these two types of search intents separately, and then describe some
work on query formulations.

2.2 Topics as Search Intents
Search intents can be used for the information a user wants to
find as opposed to work focusing on the goal of searchers. Search
intents within this context are used almost interchangeably with
information needs, or topics, that users expect in retrieved documents.
Cheng, Gao, Liu proposed a method of predicting search intents based on a page read by a user [13]. They utilized the users’
search queries triggered by a page to learn a model for estimating the search intents. Cao et al. mined clickthrough and session
logs to deduce the users’ search intents [11]. Umemoto et al. tried
to predict users’ search intents based on their eye movements[41].
Agrawal et al. addressed the problem of diversifying search results, in which search intents were modeled at the topical level, and
were diversified on a search engine result page [2]. As mentioned
earlier, IR evaluation campaigns have also modeled search intents
as (sub)topics, and conducted topic-diversity-aware evaluation in
recent years [16, 35].
As we mentioned in the previous subsection, a user’s goal and
topics s/he wants to read are not always the same. Furthermore,
topics and cognitive characteristics of documents are also different
in many cases. There are documents that are topically relevant but
cognitively irrelevant (e.g. documents on a desired topic that are
too difficult for the user).

2.1 User Goals as Search Intents
An early study on goal-directed search intents was conducted
by Broder [10], where he proposed a taxonomy of Web search
queries in terms of their potential goals: navigational (e.g. looking for a particular home page), informational (e.g. finding information on a certain topic), and transactional (e.g. trying to buy a
ticket). Rose and Levinson later proposed a finer-grained taxonomy, in which the type of informational query was further classified into five categories, and the type of transactional query was
replaced with a broader type called resource [33]. IR evaluation
campaigns such as TREC and NTCIR have recently paid attention
to the diversity of search intents in the evaluation of ad-hoc retrieval
systems [16, 35]. They assume that queries have different interpretations or subtopics, and these subtopics can be classified into either
navigational or informational in terms of their goals. Several evaluation metrics have been proposed to evaluate the performance of
IR systems for queries with different subtopics and goals [15, 34].
Marchionini proposed a boarder classification schema for search
intents, and introduced a concept of exploratory search [26]. While
Broder treated search intents as relatively short-term activities [10],
Marchionini’s classification included long-term search activities such
as learn and investigate, and he argued that exploratory searches
were searches pertinent to the learn and investigate search activities.
Searches are often motivated by information-seeking tasks, and
accomplishing tasks is considered as a search intent. Several studies investigated users’ behavior in different kinds of informationseeking tasks [42]. Liu et al. reported an investigation of users’
behaviors associated with four information-seeking task types [25].
The four types, which were characterized based on a classification
schema proposed by Li and Belkin [24], are background information collection, interview preparation, advance obituary, and copy
editing. They mainly focused on analysis of quantitative features
of search behaviors such as task completion time, query length, and
eye movements, while the detailed differences of queries in different task types were not reported in their work.
Another example along these lines was the classification of commercial and non-commercial intents that Dai et al. conducted [18].
They detected queries and Web pages that users were likely to input/visit with commercial intents. Guo and Agichtein addressed a
similar problem, which was the classification of research and purchase intents, by using mouse behavior and scrolling [19].

2.3 Query Formulations in Web Search
Jansen, Spink, and Saracevic analyzed query logs of Excite and
showed general trends of query formulations in Web search [20].
Aula investigated factors that affected the way of query formulations by conducting a questionnaire-based study with 32 subjects [4]. The experimental results revealed that experience with using computers, the Web, and Web search engines affected the query
formulation process including the query length, and frequency of
broad/narrow queries. Subjects were asked to formulate a query for
twenty search tasks in the questionnaire. White and Morris studied
the query formulation process of advanced search users, and found
that advanced search engine users queried less frequently in a session and composed longer queries than non-advanced users [44].
Aula, Khan, and Guan demonstrated how users change their search
behaviors when they faced difficulty with searches [5]. According to their large-scale study with 179 participants, users started
to formulate more diverse queries, used advanced operators more,
and spent longer on the search result page when they had difficulty
with finding information compared to the successful tasks. White,

578

Exhaustiveness. The exhaustiveness (or coverage) of a docu-

Dumais, and Teevan conducted a large-scale, log-based study of
the effect of domain expertise on Web search behaviors [43]. They
investigated logs of experts and non-experts in four domains, and
demonstrated that queries generated by experts were longer and
contained more of the vocabulary from the domain-specific lexicon.
There have been much work that investigates factors affecting
users’ query formulations, while few studies have been conducted
on query formulations for CSIs.

3.

ment for a query is the fraction of query topics covered by the
document, where the query topics are information generally expected from the query. Exhaustiveness has been considered to be
an important factor by information seekers, and has been used as
a document ranking criterion [47] as well as a summarization criterion [23]. We selected ten topics from the queries of NTCIR-10
INTENT-2 Japanese Subtask to investigate CSIs regarding exhaustiveness [35], which is an IR evaluation campaign for search result
diversification and its queries were designed to have multiple topics, e.g. “Yahoo!” and “South Africa”.

COGNITIVE SEARCH INTENT
Comprehensibility. The comprehensibility of a document means

In this section, we propose classifying search intents based on
the types of relevance proposed by Saracevic [37], and introduce
six types of Cognitive Search Intents (CSIs) used in our user study.
To avoid confusion by the multiple senses of search intent, we
alternatively use topical, cognitive, situational, and motivational
search intents in this paper. Each search intent type corresponds
to relevance types proposed by Saracevic [37]. Saracevic defined
relevance as the degree of changes caused by information transmission from file1 (e.g. topics, cognitive information needs, situations,
and motivations) to another (e.g. texts retrieved in IR). We simply define the types of search intents based on the types of files,
i.e. topical, cognitive, situational, and motivational search intents
are equivalent to topics, cognitive information needs, situations (or
tasks), and motivations (or goals), respectively.
We distinguish search intents modeled by topics (i.e. topical
search intents) [2, 11, 13, 41], and ones modeled by users’ goals
(i.e. motivational search intents) [10, 14, 18, 19, 26, 27, 33] and situations or tasks (i.e. situational search intents) [24, 25, 42]. Although goals (or motivations) and tasks (or situations) were separately classified in Saracevic’s relevance classification, it seems difficult to clearly distinguish them since task is defined as an activity
in order to achieve a goal (e.g. [42]) and a task is characterized by
the goal that can be achieved by conducting the task [24]. On the
other hand, we emphasize the difference between CSIs (or needs
for cognitive characteristics of documents) and the other types of
search intents. First, CSIs are similar to topical search intents but
different from motivational and situational search intents in that the
former types are needs for the contents and characteristics of documents to be retrieved, while the latter types are needs for goals
and tasks to be accomplished by search. Second, CSIs are different
from topical search intents in terms of the level of needs for documents. Topics are what are actually written in documents, while
cognitive characteristics of documents are ones perceived by readers, which can be affected by the writing style, diction, structure,
appearance, and meta data such as the author, domain, creation
date, etc.
According to Saracevic’s work, cognitive correspondence, informativeness, novelty, information quality, and the like are characteristics by which cognitive relevance is inferred. We further extended the cognitive characteristics to exhaustiveness, comprehensibility, subjectivity, objectivity, concreteness, and abstractness of
document content. These types were used mainly because some
work has proposed methods of estimating the degree of each criterion, which can be applied to a wide variety of applications such as
IR, Q&A, and opinion mining.
Below, we briefly discuss the six types of CSIs as well as topics
used in our user study.

how easily users can read and understand the document. Even
though a retrieved document is perfectly relevant in terms of the
topic, it is useless unless the user can comprehend it. Some researchers have tried to estimate comprehensibility by using a Wikipedia
link structure [28], and a PageRank-based method that propagates
comprehensibility scores in a semi-supervised manner [3]. We selected ten topics that were used in the research by Nakatani, Jatowt,
and Tanaka, which came from difficult domains such as medicine
and economics [28].

Subjectivity and Objectivity. The subjectivity of a document
is represented by the fraction of personal opinions in the document,
while objectivity is the opposite concept of subjectivity. There
has been some work on estimating the subjectivity of documents,
mainly within the context of opinion mining. For example, Yu and
Hatzivassiloglou proposed a method of separating opinions from
fact at the document and sentence levels [46]. The ten topics we
used were extracted from queries used in TREC 2008 Blog Track,
where opinion-finding and polarity opinion-finding retrieval tasks
were carried out, and they used topics that could be described subjectively such as “Mozart” and “Blackberry” [29].
Concreteness and Abstractness. Tanaka et al. defined concreteness by combining perceivability and imaginability in the first
trial to computationally estimate the concreteness of documents [38].
Perceivability is a measure of the extent to which people can sense
an object, while imaginability is a measure of how quickly and easily people can imagine a given referent. Since it is difficult to convey to subjects notions of their own concreteness, we defined the
concreteness of a document in a simpler way: to what extent the
content of the document describes a topic with a specific scope and
actual examples. In contrast, abstractness is defined as to what extent the content of the document describes a topic in a general and
definitional way. We selected ten topics from the ones that Tanaka
et al. used [38] for concreteness and abstractness, which were either queries used in TREC Million Query Tracks [12], or the titles
of Wikipedia articles. Examples of these topics are “arithmetic”
and “monopoly”.

4. METHODOLOGY
We investigated users’ query formulations for CSIs based on a
questionnaire-based user study. A significant challenge was to observe unbiased users’ queries while controlling their search intents.
To achieve this end, we propose an example-based method specifying search intents, in which we present positive and negative document sets to the subject, and ask him/her to search for documents
not similar to the negative set but to the positive set. We then explain the content of the questionnaire and labeling process regard-

1

The term file was used in a broader sense in the original paper: a
storage of subject knowledge and/or its representation in an organized manner [36].

579

Group A

ing resulting data. We discuss the limitations with our methodology
at the end of this section.

4.1 Example-based Specifications of Search Intents
When we try to observe users’ search behaviors, the most popular method is to present a task description and to ask subjects to
conduct the task [9]. However, this methodology has a drawback
in that users’ query formulations can be highly biased by the task
description. For example, if we explain a task as “please find comprehensible documents on black holes,” the user would be likely to
input the query “comprehensible black holes” despite unfamiliarity
with the term “comprehensible.” Moreover, the search task is not
usually verbalized. Thus, we may not be able to observe natural,
common query formulations when the task is explained through
text.
Therefore, we came up with an implicit, non-verbalized way of
specifying search intents that presents two types of examples to
the subject and enables her/him to understand search intents. The
first type of example is called a positive document set that consists of documents relevant to the search intents we want to convey
to the user, while the second type is called a negative document
set that is composed of documents irrelevant to the search intents.
When we want the subject to search with the intent of “I want to
find comprehensible documents on black holes,” for example, we
use comprehensible documents on black holes as the positive document set and incomprehensible ones on black holes as the negative
document set. Having presented the two types of examples to the
subject, we can ask him/her to search for documents that are not
similar to the negative set but to the positive set. This examplebased specification of search intents does not verbalize the search
intents, and is especially effective when we want to study the query
formulation process that can be highly biased by task descriptions.
Although the method described here can prevent the subject from
inputting a biased query, another concern is that s/he cannot clearly
understand the CSI that we intended. Our preliminary trial suggested that people tended to select some words from the positive
document set and were unaware of the CSIs. Thus, we slightly
changed the way the subject was informed of the task. The revised
six step procedure is summarized below:

Group B

Parkinson's disease is a
degenerative disorder of the
central nervous system.

Parkinson's disease is a disease
that affects parts of the brain
that are associated with normal
movement

Dark matter is a type of matter
hypothesized in astronomy and
cosmology to account for a large
part

Dark matter is the name we give
to matter we can not observe
directly.

Figure 1: Negative document set (Group A) and positive document set (Group B).
on multiple topics other than topic x as positive and negative document sets, and (2) instructing the subject to search for documents
on topic x. Those two changes were expected to help the subject
understand common properties across documents in the positive
document set (i.e. the CSI that was intended), and to prevent the
simple use of keywords in the positive document set.

4.2 Questionnaire
We asked the following questions in our questionnaire:
1. How many times per day do you input search keywords on
average?
2. How familiar are you with search engines (e.g. Google, Yahoo!, and Bing)? (on a five-point scale: not at all, no, neutral, yes, and extremely)
3. Please suppose that you are looking for information on x
and want to find documents not like the group A but like
the group B. How would you describe what documents you
want to somebody else? Please describe it briefly.
4. What kinds of keywords would you input to find documents
you want? Please input keywords in the search box below.
5. How would you modify the keyword below if you want to
find documents similar to the group B? Please modify the
keywords input in the search box below.
6. Have you ever searched to find exhaustive, comprehensible,
objective, subjective, concrete, or abstract documents? (on a
four-point scale: never, rarely, sometimes, and often)

1. Prepare topic x and CSI c with which the subject is supposed
to search,

We asked Questions 1 and 2 to estimate the subject’s search expertise from their search experience. We then used the examplebased method of specifying search intents to inform the subject of
a search intent, saying “Please compare document groups A and
B. Although both of the groups contains documents on t1 and t2 ,
there is a different point” by showing positive and negative document groups on the two topics (see Figure 1). Note that we showed
only the contents of documents without their titles and URLs. Having presented the positive and negative document sets, we asked
him/her Question 3 to obtain a verbalized search intent so that we
would know how the subject perceived the search intent conveyed
by examples, which was used to validate to what extent the subject
could clearly understand the search intent. Question 4 presented
a mimic search box and asked the subject to input an appropriate
query into the search box to find documents relevant to the search
intent presented in Question 3. The order in which of Questions 3
and 4 were presented was randomly switched to prevent order bias:
writing down the search intent before a query was input might have
influenced the way a query was formulated and vice versa. Question 5 asked subjects to investigate the difference between query input and query reformulation, where we presented a query on topic
y other than topic x used in Questions 3 and 4, as well as a search

2. Prepare multiple topics other than x and let T denote them,
3. Find documents on each topic in T that satisfy CSI c, and
use them as a positive document set,
4. Find documents on each topic in T that do not satisfy CSI c,
and use them as a negative document set,
5. Show both document sets to the subject, and
6. Ask him/her to search for “documents on topic x that are
not similar to the negative document set but to the positive
document set”.
In the case of x =“black hole” and c =“comprehensible”, for
example, the positive and negative document sets can be composed
of documents on “Parkinson’s disease” and “’dark matter’. An actual example is shown in Figure 1. Having presented these document sets to the subject, we can ask him/her to input a query to find
documents on x =“black hole” that are not similar to the negative
(incomprehensible) document set but to the positive (comprehensible) document set. The two main differences from the original
method of specifications described earlier are (1) using documents

580

Table 1: Demographics of subjects.
Sex

male
female

%

Age

%

50.8
49.2

10s
20s
30s
40s
50s
60s

16.8
17.6
18.2
17.8
15.8
13.8

Job type

company employee
business manager
government employee
contract employee
self-employed worker
profession
part-time employee
full-time homemaker
student
unemployed
others

Table 2: Search experiences of subjects.
%

Search frequency

24.8
1.3
5.3
4.8
4.9
2.7
11.2
15.5
19.2
8.3
2.0

0–4
5–9
10–14
15–19
20–24
25–

%

25.2
23.3
22.6
3.4
10.3
15.3

Search familiarity

unfamiliar at all
unfamiliar
neutral
familiar
very familiar

%

0.7
5.8
43.4
40.9
9.2

4.4 Labeling
We labeled responses from subjects to filter out those who could
not understand a search intent that we tried to inform them of, and
to classify queries from several aspects to enable further investigations. Three assessors were hired for this labeling, one of whom
was an author of this paper.
We first excluded responses that were generated within five minutes, as short completion times might have indicated poor-quality
responses. There were 1,452 (80.7% of the original data) responses
left at this stage.
We then asked the assessors to label search intents written by the
subjects for Question 3 in the questionnaire, and removed responses
by which we could estimate that the subject could not fully understand the search intents. To this end, the assessors were asked to
evaluate two points for each verbalized search intent: whether any
CSI was verbalized by the subject, and whether the CSI given to the
subject was correctly verbalized by the subject. These two points
were slightly different in that the former did not take into consideration the correctness of the written search intent. These two criteria
were separately and sequentially assessed, i.e. we first labeled all
the responses for the former criterion, excluded responses labeled
as false by two or three assessors, and then started working on the
latter criterion. After the two criteria had been labeled, we excluded
responses in the same way as those for the former criterion. Eight
hundred forty responses (46.7% of the original data) passed after
the former criterion and 422 (23.4%) passed after the latter. The
inter-rater agreement was measured by Fleiss’ Kappa, and this was
considered to be substantial being 0.76 for the former criterion and
0.73 for the latter.
We further labeled each query input for Question 4 in the questionnaire. The purpose of this labeling was to exclude queries that
contained an incorrect topic word, e.g. “black hole easy” for a
given search intent “I want to find comprehensible documents on
stagflation”. Responses labeled false (i.e. incorrect topic) by two
or three assessors were filtered out. The inter-rater agreement was
0.87 (substantial), and 375 (20.8% of the original responses) responses remained. We used these 375 responses in our analysis if
not otherwise specified.
We also labeled each query to answer the research question (a)
how do users express their CSIs in the form of a query? The assessors were instructed to classify queries into the following types:
direct (a term representing the CSI directly used in the query, e.g.
“concrete monopoly”), transformed (a term representing a CSI somehow transformed in the query, e.g. “example monopoly” and “review blackberry”), and none (no term in the query related to the
CSI, e.g. “monopoly”). The queries input for Questions 4 and 5 in
the questionnaire were labeled in this way. The inter-rater agreements were 0.71 for Question 4 and 0.61 for Question 5 in terms of
Fleiss’ Kappa. The query type of each query was decided by votes,
i.e. we used the query type labeled by two or more assessors. A
few queries (0.4%) were labeled as other, since the three assessors
labeled them differently.

result for the presented query. The search result consisted of documents that did not satisfy the CSIs we intended. The subject was
then asked to reformulate the presented query so that s/he could
obtain documents that met the CSIs asked for Questions 3 and 4.
Question 6 was asked for us to estimate the popularity of a CSI. At
this point, we explicitly verbalized the CSIs used in Questions 3, 4,
and 5, and let the subject select from the four options.
As we explained in Section 3, we used the six types of CSIs and
40 topics (10 for exhaustiveness, 10 for comprehensibility, 10 for
subjectivity and objectivity, and 10 for concreteness and abstractness) in our user study. The search intents we used were combinations of CSIs and topics; thus, there were 60 search intents (10
topics per CSI).

4.3 Procedure
We recruited subjects through an online-questionnaire company
in Japan. The entire process for the questionnaire was completed on
the Web, and all the instructions, questions, and topics were written
in Japanese. We first presented the subjects with three easy questions to filter out bots and cheaters that could be easily answered
by those who had an average search literacy.
A total number of 1,800 subjects finished the main questionnaire.
As there were 60 search intents, we randomly assigned 30 subjects
to each search intent. Table 1 lists the demographics of the subjects
we recruited, where we controlled the sex and age distributions so
that subjects were sampled from each demographic as equally as
possible. However, the number of students and part-time employees seemed high compared to the actual job type distribution in
Japan.
Table 2 summarizes the statistics on the subjects’ search experience, where the first column “Search frequency” indicates the
number of times subjects searched per day on average (Question
1 in the questionnaire), and the second column “Search familiarity” indicates the search familiarity subjects reported on a fivepoint scale (Question 2). The average search frequency per day
was 15.7, which far surpassed 3.2, which is the average search frequency per day in Japan2 . In addition, about 50 percent of subjects
answered that they were familiar or very familiar with searches.
It follows from these findings that the subjects we recruited had
above-average search expertise.

http://www.comscore.com/Insights/Press_
Releases/2009/3/Japan_Search_Engine_Rankings
[October 1, 2013].
2

581

4.5 Limitations

Table 3: Query types for CSIs.

We clarified the limitations with the methodology we used for
our user study. Even though these limitations may have reduced
the value of our reported findings, readers can still interpret and
use the results by duly taking into consideration the possible biases
discussed here.
First, the example-based method of specifying search intents was
appropriate to achieve our goal, whereas subjects could not always
understand the search intent we intended to convey with examples.
One reason is that online workers are usually poorly skilled due to
low pay and unsupervised environments. Another reason is that the
example-based method was too complex for subjects to understand
our instructions. Note, however, that the quality of data left was
ensured by using multiple assessors.
Second, our samples were biased in terms of their demographic
and search expertise. As summarized in Tables 1 and 2, the recruited subjects included more students and part-time employees,
as well as more search experts than those in an ideal sampling result. Since it is particularly well known that search expertise affects
users’ search behaviors [4], it might be inappropriate to generalize
our findings to all users of search engines including novices.
Third, our user study was conducted in Japanese, and queries
in the questionnaire were written in Japanese. However, we do not
think that this is a serious limitation because there is little difference
from the other languages at the keyword level.

5.

CSI

#

direct

Query type (%)
trans. none

other

Exhaustive
Comprehensible
Objective
Subjective
Concrete
Abstract

134
166
114
128
124
84

0.7
13.3
2.6
0.8
5.6
0.0

36.6
32.5
31.6
62.5
44.4
45.2

62.7
53.0
65.8
36.7
49.2
54.8

0.0
1.2
0.0
0.0
0.8
0.0

Total

750

4.5

41.6

53.5

0.4

Table 4: Most frequent terms in transformed queries.
CSI

Topic independent (%)

Topic dependent (%)
type (1.5%)

Exh.

details (13.4%)
wiki (6.7%)
summary (5.2%)

Com.

easy-to-understand (6.6%)
concrete example (3.0%)
simply (3.0%)

domestic medicine (0.6%)
highschool biology (0.6%)

Obj.

wiki (8.8%)
Wikipedia (5.3%)
details (4.4%)

data (1.8%)
record (0.9%)
scientific data (0.9%)

Sub.

blog (10.2%)
opinion (2.3%)
about (1.6%)

impression (20.3%)
usability (3.9%)
word-of-mouth (3.1%)

Con.

concrete example (15.3%)
example (8.9%)
details (3.2%)

real experience (0.8%)
real world (0.8%)

Abs.

summary (8.3%)
wiki (7.1%)
Wikipedia (4.8%)

FINDINGS

This section attempts to answer the four research questions by reporting findings from our user study. First, we explain how queries
were formulated for different types of CSIs. Second, the transformation from a verbalized CSI to a query is analyzed to gain a better
understanding of the query formulation process. Third, we estimate the demand for CSIs, and evaluate the supply for this demand,
i.e. the performance of a Web search engine for CSIs. Finally,
a machine-learning-based query expansion is applied to testing its
effectiveness for searches with CSIs.

used “blog” to find subjective documents. Those keywords specifying domains are topic-independent, and can be used for any topics.
We can also see that users selected keywords that were effective
for different types of topics, namely, topic-dependent keywords for
CSIs. Users tended to use the term “usability” for product-related
topics (e.g. Blackberry) for CSIs on subjectivity, while they tended
to use “impression” for person-related topics (e.g. Barry Bonds).
This topic-dependency of query formulations leads to a discussion
on automatic query expansion for CSIs in Section 6.

5.1 Queries for Cognitive Search Intents
Table 3 lists the percentages for query types for CSIs. Note that
we merged queries that were input for Questions 4 and 5 in the
questionnaire, as there were only small differences between queries
that were input from scratch (Question 4) and those that were reformulated (Question 5). Overall, the fraction of direct queries was
quite small, while that of transformed and none queries were dominant in this categorization. When we compared these two dominant query types, there were more none queries than transformed
queries. It follows that users were not likely to input a term that
directly represented their CSIs, but were more likely to transform
such a term into another term that they thought would be effective,
or to only use keywords related to a topic that they wanted to read,
even though they were conscious of given CSIs (see Section 4.4).
Many none queries posed huge challenges in estimates of search
intents regarding how silent CSIs could be detected. Possible solutions to this problem are discussed in Section 6.
Table 4 summarizes the most frequent topic-independent and
topic-dependent terms used in transformed queries, with the percentage per CSI. We simply used a dictionary to translate Japanese
words into English in this table and we can see users’ transformation techniques in these examples. For instance, many users might
have tried to restrict sites by adding keywords such as “Wikipedia”
and “blog”. Users frequently used “Wikipedia” for CSIs regarding exhaustiveness, objectivity, and abstract, while they frequently

5.2 From Cognitive Search Intents to Queries
Figure 2 compares the differences between verbalized search intents (Question 3 in the questionnaire) and input queries (Question
4) in terms of their component words. “Nouns (overlapping)” indicates terms that are included in both the verbalized search intent
and query, while “Nouns (unique)” indicates terms that appear only
in a verbalized search intent but not in its query, or those that only
appear in a query but not in its intent. It is obvious that queries
contain few verbs and adjectives. As the cognitive characteristics
of documents are often verbalized in the form of adjectives (e.g.
exhaustive, objective, and concrete), this trend might prevent users
from explicitly inputting their CSIs as discussed in the previous
subsection.
Moreover, it can be seen that about two nouns in the verbalized
search intents were not used in subjects’ queries, while on average
1.3 nouns in the queries were not used in their verbalized search
intents. This finding might imply that users input some of the nouns
from their verbalized search intents; on another front, they generate
nouns suitable for keyword queries as alternatives to unused nouns,

582

4.5
4

Number of terms

investigated the performance of Google’s search engine because
the search results of Yahoo! Japan were provided by Google as
of March 27, 2013. We crawled the ten top-ranked documents returned in response to subjects’ queries on March 27, 2013, and did
query-based pooling for each search intent. We then created crowdsourcing tasks that asked workers to evaluate the topical relevance
as well as cognitive relevance on a two-point scale (irrelevant and
relevant). Six workers were assigned to each document. We paid
5 JPY per document for both topical and cognitive relevance judgments. The inter-rater agreement was measured with Randolph et
al.’s Free-Marginal Multirater Kappa, since the number of tasks
completed varied across workers [32]. The agreement on 5,213
documents for the evaluations of topical relevance was substantial
(0.67), whereas that for the evaluations of cognitive relevance was
moderate (0.43). As we expected, the agreement in the cognitive
relevance judgements was lower than the topical relevance judgements, since cognitive relevance was usually subjective by design.
Figure 4 shows the mean nDCG@10 with the standard error of
the mean (SEM) for all CSIs and query types in terms of topical relevance as well as topical and cognitive relevance. The weight for
nDCG was computed in two ways: the number of workers who labeled the document as topically relevant (topical), and that of workers who labeled the document as both topically and cognitively relevant (topical+cognitive). The weight was normalized so that it
ranges from 0 to 2. The figure only presents the mean nDCG@10
for transformed and none query types, due to a small number of
samples for direct and other types.
There is a gap between topical and cognitive relevance in terms
of nDCG@10. Although it is obviously harder for a Web search
engine to satisfy CSIs than to meet topical search intents, this difference suggests that the current search engine lacks adequate capabilities to process queries input with CSIs, even with transformed
queries in which users transformed a term that directly represented
a CSI into terms that they thought were effective for finding cognitivelyrelevant documents. Table 5 lists queries that achieved the highest
nDCG@10 for each CSI. Note that Japanese queries were again
translated into English. It seems that a few top queries were as
effective as those for topical relevance, whereas the third most effective queries for CSIs regarding exhaustiveness, comprehensibility, objectivity, and concreteness could only achieve nDCG lower
than the average nDCG in terms of topical relevance. Furthermore,
transformed queries were not very effective but were worse than
none queries in some cases in which no keywords were related to
CSIs. Thus, users’ skills with query formulations did not always
solve the problem with the search engine’s inability to deal with
CSIs.

Adjectives
Verbs
Nouns (overlapping)
Nouns (unique)

3.5
3
2.5
2
1.5
1
0.5
0

Verbalized search
intents

Queries

Figure 2: Difference between verbalized search intents and
queries in terms of their component words.
0%
Exhaustive
Comprehensible

20%

40%

60%

80%

100%
Never
Hardly
Sometimes
Often

Objective
Subjective
Concrete
Abstract

Figure 3: Experience with searches having CSIs.
verbs, and adjectives. Although not conclusive, this hypothesis plus
a large portion of transformed queries together sketch the shapes of
query formulations for CSIs: users usually avoid inputting CSIs as
adjectives, and translate their CSIs into nouns.

5.3 Supply and Demand for Cognitive Search
Intents
We first briefly discuss the demand for searches with CSIs by
using responses to Question 6 in the questionnaire, and then investigate the performance of a current Web search engine for CSIs.
Figure 3 outlines answers to the question “Have you ever searched
with a CSI?” Note that we explicitly informed the subjects of the
CSI we intended when they started to answer Question 6. Therefore, we assumed that all the subjects could understand the CSI
used in the user study, and we used 1,452 responses obtained just
after filtering by using the completion time. The figure indicates
that half the subjects had CSIs on exhaustiveness, comprehensibility, objectivity, and concreteness, of which about 10% answered
that they often searched with CSIs. Thus, this result indicates reasonable demand for executing searches with CSIs. Subjects, on the
other hand, did not very frequently experience searches with CSIs
regarding subjectivity and abstractness.
Searches with CSIs were at a certain level of demand. Then,
how well can Web search engines meet demand given queries input
with CSIs? To answer this question, we downloaded documents
that were returned in response to subjects’ queries, assessed their
topical and cognitive relevance, and evaluated a Web search engine’s performance in terms of normalized Discounted Cumulative
Gain (nDCG) [21].
Yahoo! Japan and Google are two major search engines that accept Japanese queries, and they occupied ∼90% of the Japanese
Web search engine market according to a survey in 20092 . We only

5.4 Effectiveness of Query Expansion for Cognitive Search Intents
We further investigate the performance of the Web search engine
by automatic query expansions based on judged documents. We
compared the performances of users’ and expanded queries to clarify whether a machine-learning-based query expansion is effective
enough to develop CSI-aware search engines.
We used a query expansion method proposed by Oyama, Kokubo,
and Ishida [30], which was designed for developing domain-specific
Web search engines. Their proposed method first trains a decision
tree to classify training data that consists of relevant and irrelevant
documents, where the presences of terms in a document are used as
features. The trained decision tree is composed of nodes representing terms T , leaves L, and branches E ⊂ V × V (V = T ∪ L).
Each leaf l is labeled as irrelevant (r(l) = 0) or relevant (r(l) = 1),
while each branch e = (vi , vj ) is labeled as absent (p(e) = 0)

583

0

0.2

nDCG@10 (±SEM)
0.4
0.6

0.8

0

1

0.2

nDCG@10 (±SEM)
0.4
0.6

0.8

1

Exhaustive

Exhaustive

Comprehensible
Comprehensible

Objective
Objective

Subjective
Subjective

User

Concrete
Concrete

User Max

Trans. (topical)

Abstract

Expanded

None (topical)
Trans. (topical+cognitive)
Abstract

None (topical+cognitive)

Figure 5: Mean nDCG@10 for users’ transformed queries
(User), users’ transformed queries that achieved the highest
nDCG for each topic (User Max), and expanded queries (Expanded).

Figure 4: Mean nDCG@10 for each query type in terms of topical relevance as well as topical and cognitive relevance.

Table 6: Examples of expanded queries. Bold font indicates
terms representing given topics.

Table 5: Most effective transformed queries. Bold font indicates
terms representing given topics.
CSI

Queries

Exh.

projector detailed explanation
Yahoo details
mahjong origin explanation details

0.91
0.78
0.66

Com.

RNA easy-to-understand
Meniere disease all about
superconductivity simply

0.82
0.68
0.66

Obj.

olympics summary
global warming summary
hybrid car details

0.84
0.77
0.67

Sub.

Macbook Pro word-of-mouth usability
olympics discussion
hybrid car reviewal

0.85
0.81
0.80

Con.

risk of asbestos case
arithmetic concrete example
monopoly case

0.83
0.80
0.72

Abs.

car discussion
summary of car maker
asbestos wiki

0.87
0.78
0.75

CSI

Queries

nDCG@10

Exh.

projector -regarding

0.57

Com.

relativity theory -biology -save -flat terminology -create

0.56

Obj.

cholesterol -think -readable -landing
special

0.71

Sub.

barry bonds want

0.73

Con.

monopoly -help -definitely

0.60

Abs.

uncertainty -not -want

0.58

nDCG@10

-

Dvalidation = (Xc −{x})−Dtraining . We generated one expanded
query for each pair of a topic and a CSI. Some of the examples are
shown in Table 6. Note that we did not use documents retrieved by
a query to be expanded, as we wanted to develop a query expansion
method applicable for any queries. Thus, our query expansion was
topic-independent.
Figure 5 shows the mean nDCG@10 in terms of topical and cognitive relevances, with SEM. The mean nDCG@10 was computed
for (1) users’ transformed queries (User), (2) users’ transformed
queries that achieved the highest nDCG for each topic (User Max)
(see examples in Table 5), which approximates the upper-bound
performance achieved by keyword queries, and (3) queries generated by the query expansion method (Expanded).
The query expansion improved the performances for CSIs on
comprehensibility, objectivity, and concreteness, while it failed to
improve the performances for the other types of CSIs. Table 6
shows examples of the expanded queries. We can see some terms
that may be effective for the CSIs, e.g. “-biology” for comprehensibility, “-think” for objectivity, and “want” for subjectivity. Such
terms might work to filter out scientific articles (“-biology”) or include/exclude subjective documents (“-think” and “want”). Especially, it is interesting that the query expansion method found verbs
effective for searches with CSIs, which were not frequently used
by our subjects. On the other hand, the other terms in the table
are difficult to discuss their effectiveness, e.g. “-help”, “-flat”, and
“-landing”, which were possibly selected as a result of over-fitting
due to the small training data. Another possible explanation for
the low performances is that there are few terms that improve some

or present (p(e) = 1) indicating the presence of the term vi in a
document. Each path from the root t0 to a leaf labeled as relevant, i.e. q = (t0 , t1 , . . . , ti , l) (r(l) = 1), represents a Boolean
query -t0 ∧ t1 ∧ · · · ∧ ti when p((t0 , t1 )) = 0, p((t1 , t2 )) = 1,
and p((ti , l)) = 1. From each Boolean query q generated from
the induced tree, their method removes literals that improve the Fmeasure on validation data when it is removed. After the literal
pruning, the query expansion method makes a disjunction of the
pruned Boolean queries Q = {q1 , q2 , . . .}, i.e. q1 ∨ q2 ∨ · · · . The
method further simplifies the disjunction by removing conjunctive
components that improve the F-measure on the validation data. The
resulting query is used as an expansion to a given query.
We utilized the method above to generate queries by feeding
judged documents as training and validation data, where relevant
documents were ones judged as topically and cognitively relevant
by at least four assessors. Recall that ten topics Xc were selected
for each CSI c. For CSI c and topic x ∈ Xc , we used documents retrieved for six topics as training data Dtraining ⊂ Xc −
{x}, and ones retrieved for the other three topics as validation data

584

6.2 Toward Cognitive-search-intent-aware
Search Engine

types of cognitive relevances. It is indeed hard to select terms indicating high exhaustiveness or abstractness even manually.

6.

Our results uncovered a situation where there was reasonable demand (frequent experience) but limited supply (poor performance
by Web search engines) for CSI-aware searches. According to the
experimental results, simple modifications to queries do not necessarily help users find cognitively relevant documents. In particular,
a topic-independent query expansion was not effective for all of
the CSIs. Thus, Web search engines are required either to automatically expand the query in a topic-dependent manner or to rank
documents by taking into account their cognitive characteristics in
response to queries input with a CSIs.
Query expansion for CSIs would be an easier approach to developing CSI-aware search engines, since query expansion can be
installed to on search engines without having to modify their document rankers. Effective query expansion might depend on the topics of the queries as observed in Table 4. This is also supported
by the result that a topic-independent query expansion failed to improve search performances for some of the CSIs. For example,
adding a keyword “home” would effectively find comprehensible
documents on a disease, while the same query expansion would
not be helpful for economic topics.
Estimating the cognitive characteristics of documents in advance
and utilizing their priority in ranking is another approach to achieving CSI-aware searches. Methods of predicting the cognitive characteristics of documents have been proposed in the literature as
was discussed in Section 4.2. Although certain types of CSIs can
be handled even with existing techniques, a demanding challenge
is to identify documents that are cognitively relevant for specific
users. As cognitive relevance is that recognized by users, it is inherently subjective and depends on individuals. For example, documents may differ that are comprehensible by different types of
users. Other examples include the likes, usability, and visibility
of documents. Therefore, estimating the cognitive characteristics
of documents for certain types of users can be one of the greatest
challenges in this line of work.

DISCUSSION AND IMPLICATIONS

Our user study uncovered users’ query formulations in which
they were likely not to use terms related to CSIs, and revealed that
the current Web search engine is not able to satisfy the CSIs as
adequately as topical ones. This section discusses some questions
raised in our analysis, and indicates approaches to the problem with
silent CSIs and the development of a CSI-aware search engine.

6.1 Silent Cognitive Search Intents
We will first discuss the reason why half our subjects did not
input any keywords related to CSIs, and then suggest some possible
solutions to the problem with silent CSIs.
Our results demonstrated that half the subjects did not include
terms representing CSIs in their queries, even though their verbalized search intent included such terms (this was ascertained by
three assessors). A possible explanation for this phenomenon is that
subjects over-adapted to the Web search engine. An early study on
Web searches conducted by Pollock and Hockley found that some
novices tried to enter natural language queries [31]. In addition, Bilal reported that 35 % of 22 seventh-grade children tried to search
with a natural language question [8]. On the other hand, experienced users do not usually input natural language queries into
search engines. Thus, the way users formulate queries might be acquired through experience with Web searches. Putting these findings all together, our hypothesis is that few experienced subjects
input keywords related to CSIs because they knew Web search engines could not effectively process such words through their experience with search engines (as is also empirically shown in Figure
4). To support this hypothesis, we conducted an independent test
on query types and search expertise measured by Question 2 in the
questionnaire, though a null hypothesis was not rejected probably
due to the small number of novices. Thus, the problem with silent
CSIs bears further investigation.
Although much work has utilized query logs to investigate users’
search activities, the problem with silent CSIs suggests that only
query-log-based analysis is not adequate for detecting users with
CSIs. Therefore, clickthrough and interaction data are necessary to
precisely estimate CSIs as can be seen in some previous work [11,
13, 19, 41].
Another type of approach is to encourage users to explicitly input their CSIs. Belkin et al. reported that users input longer queries
when they are shown a message near the query box ”Information
problem description (the more you say, the better the results are
likely to be)” [7]. Agapie, Golovchinsky, and Qvarfordt proposed
an interface designed to encourage users to type longer queries,
which produced a halo around the query box that changed in color
and size as the length of a query changed [1]. These approaches
can be applied to eliciting users’ CSIs by telling users they can input adjectives that describe documents they want, or just encouraging them to input longer queries to increase the chance of obtaining
terms regarding their CSIs. In addition, techniques of interactive
IR such as query suggestion and relevance feedback would also
help users better express their CSIs. In particular, Yamamoto and
Tanaka proposed a method that predicted users’ criteria to assess
the credibility of search results by asking them to click on credible
search results by visualizing the estimated cognitive characteristics
of results such as exhaustiveness and objectivity [45]. Their techniques can be easily applied to predicting CSIs.

7. CONCLUSIONS
This study investigated users’ formulations of queries for CSIs
including exhaustiveness, comprehensibility, subjectivity, objectivity, concreteness, and abstractness. We proposed an exampledbased method of specifying search intents to observe users’ query
formulations without bias by presenting a verbalized task description. Moreover, we were able to find plausible answers to the four
research questions by means of a user study that was questionnairebased. Our findings are summarized as follows: (a) we found that
about half our subjects did not input any keywords representing
CSIs, even though they were conscious of CSIs; (b) our user study
also revealed that over 50% of subjects occasionally had experiences with searches with CSIs; (c) our evaluations demonstrated
that the performance of a current Web search engine was much
lower when we not only considered users’ topical search intents
but also CSIs; and (d) we demonstrated that a machine-learningbased query expansion could improve the performances for some
types of CSIs.
Future work includes further studies on silent CSIs, and the development of a CSI-aware search engine. We also have a plan to
investigate the most frequent CSIs per query based on clickthrough
data analysis, and the relationship between users’ queries and descriptions of situational and motivational search intents.

585

8.

ACKNOWLEDGEMENTS

[25] J. Liu, M. J. Cole, C. Liu, R. Bierig, J. Gwizdka, N. J. Belkin,
J. Zhang, and X. Zhang. Search behaviors in different task types. In
JCDL, pages 69–78, 2010.
[26] G. Marchionini. Exploratory search: from finding to understanding.
Communications of the ACM, 49(4):41–46, 2006.
[27] Y. Moshfeghi and J. Jose. On cognition, emotion, and interaction
aspects of search tasks with different search intentions. In WWW,
pages 931–941, 2013.
[28] M. Nakatani, A. Jatowt, and K. Tanaka. Easiest-first search: towards
comprehension-based web search. In CIKM, pages 2057–2060, 2009.
[29] I. Ounis, C. Macdonald, and I. Soboroff. Overview of the trec-2008
blog track. In TREC, 2008.
[30] S. Oyama, T. Kokubo, and T. Ishida. Domain-specific web search
with keyword spices. IEEE TKDE, 16(1):17–27, 2004.
[31] A. Pollock and A. Hockley. What’s wrong with internet searching. In
D-Lib Magazine, 1997.
[32] J. J. Randolph, A. Thanks, R. Bednarik, and N. Myller.
Free-marginal multirater kappa (multirater κfree): an alternative to
fleiss’ fixed-marginal multirater kappa. In Joensuu learning and
instruction symposium, 2005.
[33] D. E. Rose and D. Levinson. Understanding user goals in web search.
In WWW, pages 13–19, 2004.
[34] T. Sakai. Evaluation with informational and navigational intents. In
WWW, pages 499–508, 2012.
[35] T. Sakai, Z. Dou, T. Yamamoto, Y. Liu, M. Zhang, R. Song, M. P.
Kato, and M. Iwata. Overview of the NTCIR-10 INTENT-2 Task. In
NTCIR-10, 2013.
[36] T. Saracevic. Relevance: A review of and a framework for the
thinking on the notion in information science. Journal of the
American Society for Information Science, 26(6):321–343, 1975.
[37] T. Saracevic. Relevance reconsidered. In Proceedings of the second
conference on conceptions of library and information science, pages
201–218, 1996.
[38] S. Tanaka, A. Jatowt, M. P. Kato, and K. Tanaka. Estimating content
concreteness for finding comprehensible documents. In WSDM,
pages 475–484, 2013.
[39] J. Teevan, E. Adar, R. Jones, and M. A. Potts. Information
re-retrieval: repeat queries in yahoo’s logs. In SIGIR, pages 151–158,
2007.
[40] J. Teevan, S. Dumais, and E. Horvitz. Potential for personalization.
ACM TOCHI, 17(1):4:1–4:31, 2010.
[41] K. Umemoto, T. Yamamoto, S. Nakamura, and K. Tanaka. Search
intent estimation from user’s eye movements for supporting
information seeking. In AVI, pages 349–356, 2012.
[42] P. Vakkari. Task-based information searching. Annual review of
information science and technology, 37(1):413–464, 2003.
[43] R. W. White, S. T. Dumais, and J. Teevan. Characterizing the
influence of domain expertise on web search behavior. In WSDM,
pages 132–141, 2009.
[44] R. W. White and D. Morris. Investigating the querying and browsing
behavior of advanced search engine users. In SIGIR, pages 255–262,
2007.
[45] Y. Yamamoto and K. Tanaka. Enhancing credibility judgment of web
search results. In CHI, pages 1235–1244, 2011.
[46] H. Yu and V. Hatzivassiloglou. Towards answering opinion
questions: Separating facts from opinions and identifying the polarity
of opinion sentences. In EMNLP, pages 129–136, 2003.
[47] B. Zhang, H. Li, Y. Liu, L. Ji, W. Xi, W. Fan, Z. Chen, and W.-Y. Ma.
Improving web search results using affinity graph. In SIGIR, pages
504–511, 2005.

This work was supported in part by the following projects: Grantsin-Aid for Scientific Research (Nos. 24240013, 24680008, and
26700009) from MEXT of Japan, and Microsoft Research CORE
Project.

9.

REFERENCES

[1] E. Agapie, G. Golovchinsky, and P. Qvarfordt. Leading people to
longer queries. In CHI, pages 3019–3022, 2013.
[2] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying
search results. In WSDM, pages 5–14, 2009.
[3] K. Akamatsu, N. Pattanasri, A. Jatowt, and K. Tanaka. Measuring
comprehensibility of web pages based on link analysis. In WI, pages
40–46, 2011.
[4] A. Aula. Query formulation in web information search. In ICWI,
pages 403–410, 2003.
[5] A. Aula, R. M. Khan, and Z. Guan. How does search behavior change
as search becomes more difficult? In CHI, pages 35–44, 2010.
[6] P. Bailey, R. White, H. Liu, and G. Kumaran. Mining historic query
trails to label long and rare search engine queries. ACM TWEB,
4(4):15:1–15:27, 2010.
[7] N. J. Belkin, D. Kelly, G. Kim, J.-Y. Kim, H.-J. Lee, G. Muresan,
M.-C. Tang, X.-J. Yuan, and C. Cool. Query length in interactive
information retrieval. In SIGIR, pages 205–212, 2003.
[8] D. Bilal. Children’s use of the yahooligans! web search engine: I.
cognitive, physical, and affective behaviors on fact-based search
tasks. Journal of the American society for information science,
51(7):646–665, 2000.
[9] P. Borlund and J. W. Schneider. Reconsideration of the simulated
work task situation: A context instrument for evaluation of
information retrieval interaction. In IIiX, pages 155–164, 2010.
[10] A. Broder. A taxonomy of web search. ACM SIGIR Forum,
36(2):3–10, 2002.
[11] H. Cao, D. Jiang, J. Pei, Q. He, and Z. Liao. Context-Aware Query
Suggestion by Mining Click-Through and Session Data. In KDD,
pages 875–883, 2008.
[12] B. Carterette, V. Pavlu, H. Fang, and E. Kanoulas. Million query
track 2009 overview. In TREC, 2009.
[13] Z. Cheng, B. Gao, and T. Liu. Actively Predicting Diverse Search
Intent from User Browsing Behaviors. In WWW, pages 221–230,
2010.
[14] K. Church and B. Smyth. Understanding the Intent Behind Mobile
Information Needs. In IUI, pages 247–256, 2009.
[15] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan,
S. Büttcher, and I. MacKinnon. Novelty and diversity in information
retrieval evaluation. In SIGIR, pages 659–666, 2008.
[16] C. L. A. Clarke, N. Craswell, and E. M. Voorhees. Overview of the
TREC 2012 Web Track. In TREC, 2012.
[17] K. Collins-Thompson and J. P. Callan. A language modeling
approach to predicting reading difficulty. In HLT-NAACL, pages
193–200, 2004.
[18] H. K. Dai, L. Zhao, Z. Nie, J.-R. Wen, L. Wang, and Y. Li. Detecting
online commercial intention (OCI). In WWW, pages 829–837, 2006.
[19] Q. Guo and E. Agichtein. Ready to buy or just browsing?: detecting
web searcher goals from interaction data. In SIGIR, pages 130–137,
2010.
[20] B. J. Jansen, A. Spink, and T. Saracevic. Real life, real users, and real
needs: a study and analysis of user queries on the web. Information
processing & management, 36(2):207–227, 2000.
[21] K. Järvelin and J. Kekäläinen. Cumulated gain-based evaluation of ir
techniques. ACM TOIS, 20(4):422–446, 2002.
[22] M. P. Kato, T. Yamamoto, H. Ohshima, and K. Tanaka. Cognitive
search intents hidden behind queries: a user study on query
formulations. In WWW, pages 313–314, 2014.
[23] L. Li, K. Zhou, G.-R. Xue, H. Zha, and Y. Yu. Enhancing diversity,
coverage and balance for summarization through structure learning.
In WWW, pages 71–80, 2009.
[24] Y. Li and N. J. Belkin. A faceted approach to conceptualizing tasks in
information seeking. Information Processing & Management,
44(6):1822 – 1837, 2008.

586

