Learning Temporal-Dependent Ranking Models
Miguel Costa 1,2
miguel.costa@fccn.pt

Francisco M Couto 2
fcouto@di.fc.ul.pt

Mário J. Silva 3
mjs@inesc-id.pt

1

2

Foundation for National Scientific Computing, Portugal
Departamento de Informática, Faculdade de Ciências, Universidade de Lisboa, Portugal
3
INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Portugal

ABSTRACT

from typical IR and web IR, because a web archive corpus is
distinctively composed by a stack of content collections harvested from the web over time. Thus, each document may
have several versions and the relevance of a version depends
on the user’s period of interest. Another main difference of
WAIR is that its multi-version web collections have different characteristics over time, which causes variations in the
discriminative power of features used in ranking.
WAIR has been applied in at least 68 web archive initiatives1 undertaken by national libraries, national archives and
consortia of organizations that are acquiring and preserving
parts of the web. Web archives already hold more than 534
billion files (17 PB), of which some are historical records,
such as opinions, decisions and photos of events. Pieced together, these records form our collective memory and the
possibility of looking into the past opens space for novel applications. In the last years, applications based on data from
web archives include tools for assessing the trustworthiness
of statements [31], detecting web spam [5], improving web IR
[10] or forecasting events [26]. However, despite web archive
technology having achieved a good maturity level, the effectiveness of the search services they provide still presents
unsatisfactory results [8]. As result, information cannot be
found and web archives are useless for their users.
In this work, we cope with the poor retrieval effectiveness of web archives by addressing three identified limitations. First, the ranking relevance of document versions is
currently computed based only on the similarity of each content with the query, ignoring many other features which have
shown to improve web search engines. By employing stateof-the-art learning to rank (L2R) algorithms on such features
we immediately obtained significant improvements, increasing more than three times the search effectiveness of state-ofthe-art WAIR. Second, web archives preserve many years of
collected web snapshots, but current WAIR approaches ignore the time dimension in such collections. We researched
what relevant information to WAIR can be extracted from
this time dimension, by exploring, for the first time, the
long-term persistence of web documents. In our experiments, conducted over 14 years of web snapshots, we found
that for navigational queries, relevant documents tend to
have a longer lifespan and more versions. This result enabled
us to obtain significant gains by modeling the persistence of
web documents into novel ranking features. These features
are especially important in web archives, because the queryindependent features typically used to identify popular or
important documents based on click-through data and the

Web archives already hold together more than 534 billion
files and this number continues to grow as new initiatives
arise. Searching on all versions of these files acquired throughout time is challenging, since users expect as fast and precise
answers from web archives as the ones provided by current
web search engines. This work studies, for the first time,
how to improve the search effectiveness of web archives, including the creation of novel temporal features that explore
the correlation found between web document persistence and
relevance. The persistence was analyzed over 14 years of web
snapshots. Additionally, we propose a temporal-dependent
ranking framework that exploits the variance of web characteristics over time influencing ranking models. Based on
the assumption that closer periods are more likely to hold
similar web characteristics, our framework learns multiple
models simultaneously, each tuned for a specific period. Experimental results show significant improvements over the
search effectiveness of single-models that learn from all data
independently of its time. Thus, our approach represents an
important step forward on the state-of-the-art IR technology
usually employed in web archives.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Retrieval
models

General Terms
Algorithms; Performance; Experimentation

Keywords
web archives; temporal-dependent ranking

1.

INTRODUCTION

Web archive information retrieval (WAIR) addresses the
retrieval of document versions from web archives, according
to topical and temporal criteria of relevance. WAIR differs
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from Permissions@acm.org.
SIGIR ’14, July 06 - 11 2014, Gold Coast, QLD, Australia
Copyright 2014 ACM 978-1-4503-2257-7/14/07$15.00.

1

757

http://en.wikipedia.org/wiki/List_of_Web_archiving_initiatives

the most recent documents is desirable for queries where a
user intends to find recent events or breaking news. The
distribution of the documents’ dates can also be explored,
since it reveals time intervals that are likely to be of interest
to the query [16]. For instance, when searching for tsunami,
the peaks in the distribution may indicate when tsunamis occurred. Another idea is to favor more dynamic documents,
since documents with higher relevance are more likely to
change or change to a greater degree [10]. More popular
and revisited documents are also more likely to change [1].
On the other hand, the most persistent terms are descriptive of the main topic and likely added early in the life of
a document [1]. These persistent terms are especially useful for matching navigational queries, because the relevance
of documents for these queries are expected to not change
over time. To the best of our knowledge, we are the first
studying the relation between long-term web document persistence and relevance for improving search effectiveness.

web-graph, are not available in this context. Web archives
receive a much smaller volume of queries and clicks than
web search engines, and the web-graphs are sparser since
only a small part of the web is commonly collected and preserved by each archive. Third, the characteristics of the
web vary over time. For instance, the sites in the 90s did
not have the richer layouts and more interactive interfaces
of the early 00s with CSS and JavaScript. Other examples
include the dynamics of the web link structure, which grows
following a power law [19], and the dynamics of language
in web contents, which have many terms appearing and disappearing every year [29]. We believe that a single general
ranking model cannot predict the variance of web characteristics over such long periods of time. As a result, we
present as the main contribution of this paper, an approach
that learns and combines multiple ranking models specific
for each period. Experimental results show that our approach outperforms the search effectiveness of single-model
approaches that fit all data independently of when it was
created or updated. We refer to our approach as temporaldependent ranking.
The remainder of this paper is organized as follows. In
Section 2, we cover the related work. Section 3 analyzes
the long-term web document persistence, while a temporaldependent ranking framework is proposed in Section 4. In
Section 5, we present the experimental setup and report the
results in Section 6. Section 7 finalizes with the conclusions
and future work.

2.
2.1

2.3

RELATED WORK
Web Archives Access

Much of the current effort on web archive development
focuses on acquiring, storing, managing and preserving data
[22]. However, the data must also be easily accessible to
users who need to exploit and analyze them. Full-text search
has become the dominant form of information access, especially in web search systems, such as Google, which has a
strong influence on the way users search in other settings.
Surveys indicate that full-text search is also the preferred
tool for accessing web archive data and the most used when
supported [7]. Even with the high computational resources
required for this purpose, 67% of world-wide web archives
support full-text search for at least a part of their collections [13]. However, the large majority of web archives that
support full-text search are based on the Lucene search engine2 or extensions of Lucene to handle web archives, such
as NutchWAX3 . The search services provided by these web
archives are visibly poor and frequently deemed unsatisfactory [13]. An WAIR evaluation confirmed the low quality of
search results retrieved with such technology [8]. This last
work, like ours, focus in navigational queries, since this is
the main information need of web archive users [6].

2.2

3.

3

WEB DOCUMENTS PERSISTENCE

Most ranking models have a static view of web documents
and only consider their last version. We posit that web document persistence can be used to create discriminative features for improving the performance of ranking models. In
this section, we analyze the correlation between the relevance of web documents and their long-term persistence.

Temporal Features

Some works leveraged temporal information to improve
full-text search results of web search engines. One of the
most common ideas is incorporating in language models the
heuristic that the prior probability of a document being relevant is higher in the most recent documents [20]. Boosting
2

Learning Multiple Models

The L2R framework learns ranking models that fit all
training data. However, a generic model is not always the
best solution and may be overcome by a criteria-dependent
model. For instance, Kang and Kim automatically classified
queries and created a ranking model for each query type
[17]. However, it is often hard to classify a given web search
query due to its small number of terms, which makes this
technique unfeasible in some cases or imprecise when the
wrong model is chosen. To avoid the misclassification problem, Geng et al. created a ranking model for each query
q by using the k-nearest training queries of q measured by
the similarity of their feature values [12]. The query feature values were computed as the mean of the feature values of the top search results ranked by a reference model
(BM25). However, the training time required to create all
these models is quite large and each model is learned with
just a part of the training data. Bian et al. employed a
clustering method to identify a set of query topics based on
features extracted from the top search results [3]. Then, a
ranking model was learned for each query topic. Each query
contributed to learn each model according to the similarity
between the query and the respective topic. Dai et al. followed this work, but integrated freshness with relevance to
simultaneously optimize both [9]. In a different work, Salles
et al. created a classification model per time interval and
weighted them based on the temporal distance between the
creation time of documents and the interval [28]. Our work
is the first that learns ranking models taking into account
the specificities of each time period.

3.1

Collection Description

We chose for this analysis a test collection built for WAIR
research [8]. The general statistics are detailed in Table 1.
This collection includes a corpus with 269 801 assessed web
document versions using a three-level scale of relevance (not-

http://lucene.apache.org/
http://archive-access.sourceforge.net/projects/nutch/

758

40

255 million
8.9 TB
1996 to 2009
50
2.23
269 801
3-level

% of documents

document versions
data volume
date range
navigational queries
average query length
assessed document versions
assessment scale of relevance

30
20

10
0

Table 1: Test collection statistics.

1

% of documents

40

[101,1400]

Figure 2: Distribution of the number of versions of
documents over 14 years.

30
20

mic distribution, the lifespan resembles a long tail distribution. When inspecting the documents, we saw that the
document with most versions is the homepage of a newspaper (http://www.correiomanha.pt/) with 1301 versions
and a lifespan of 12 years and a half. The document with
the longest lifespan contains a list of scientific books for
the younger (http://nautilus.fis.uc.pt/softc/Read_c/
l_infantis/infantis.html) with a lifespan of 13 years and
2 months, but with just 8 versions. While all the documents
with the highest number of versions have a long lifespan,
the opposite is not true. In fact, the top ten documents
with the longest lifespans have less than 15 versions. The
Pearson correlation coefficient between the number of versions and the lifespan of web documents is 0.52.

10
0

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14
lifespan in years

Figure 1: Distribution of the lifespan of documents
in years.
relevant, relevant and very relevant). The assessed document versions were returned by different ranking models in
response to 50 navigational queries randomly sampled from
the logs of a public web archive. This selection strategy
enables to get a high coverage of relevant documents, especially because navigational queries tend to have only one
(very) relevant document. The queries have 2.23 terms on
average and 1/3 are restricted by date range.
The documents range over a period of 14 years, from 1996
to 2009. Such characteristics make this collection unique to
study long-term persistence of web documents and their relation to relevance ranking. For instance, to study content
change, Elsas and Dumais used a collection of 2 million documents crawled for a period of 10 weeks [10], Adar et al.
used 55 thousand documents crawled during 5 weeks [1],
Fetterly et al. crawled 150 million documents over a period
of 11 weeks [11] and Ntoulas et al. 150 web sites over the
course of 1 year [24]. These are much shorter periods of
analysis not so adequate to this study.

3.2

[2,10]
[11,100]
number of versions

3.3

Document Persistence & Relevance

We found some interesting patterns when analyzing the
relationship between the long-term persistence of web documents and their relevance. Figure 3 shows the fraction of
documents that have a lifespan longer than 1 year for each
relevance level, i.e. the number of documents with a given
relevance level and a lifespan longer than 1 year, divided
by the total number of documents with that same relevance
level. The figure shows that these documents are likely to
have a higher relevance. The same correlation exists for
documents between 1 and 5 years. The percentage of very
relevant documents with more than 5 years is only 1% of the
total documents for the 50 queries analyzed, which makes
it difficult to identify any meaningful correlation. Nevertheless, the sum of the relevant and very relevant fractions of
documents is always superior to the not-relevant when considering the documents with a lifespan longer than 1 year.
This indicates that the relevant documents tend to have a
longer lifespan.
Figure 4 shows the fraction of documents that have more
than 10 versions for each relevance level. These documents
tend to have a higher relevance, such as the documents between 1 and 30 versions. The percentage of very relevant
documents with more than 30 versions is only 1% of the
total documents for the 50 queries analyzed. The 1% is
the threshold where once again the correlation starts to be
insignificant. However, the sum of the relevant and very
relevant fractions of documents is always superior to the
not-relevant when considering until 300 versions. After this
number, the 4% of remaining documents present a different pattern. Even so, in general, these results indicate that
relevant documents tend to have more versions.

Document Persistence

We analyzed the persistence of web documents measured
by their lifespan (i.e. difference in days between the first
and last versions) and their number of versions. For simplification, we identified the versions of a URL by comparing
their MD5 checksums.
Figure 1 shows the lifespan distribution of web documents.
Around 36% of documents have less than one year and hence,
we assigned a lifespan of 0 years. This percentage is inferior
to the 50% reported by Ntoulas et al. [24]. 14% have a
lifespan between 1 and 2 years and near 8% have a lifespan
longer than 10 years. Figure 2 shows the distribution of the
number of versions of documents. Around 36% have just 1
version, 29% have between 2 and 10, and 35% have more
than 10.
The lifespan and number of versions present different distributions. While the number of versions fits a logarith-

759

fraction of documents

100%

d. This model f produces a vector ŷ of m ranking scores,
one per query-document pair < q, d >, trying to predict the
real relevance of document d for query q:

80%
60%

40%

ŷ = f (X; ω)

20%

Manually finding and optimizing f is a laborious and prone
to error work, especially when f combines multiple features.
Hence, L2R algorithms automatically learn the best model
fˆ, such that fˆ minimizes the given loss function L:

0%

not-relevant

relevant
relevance level

very relevant

fraction of documents

Figure 3: Fraction of documents with a lifespan
longer than 1 year for each relevance level.

fˆ = arg min
f ∈F

80%
60%

40%

4.2

20%
0%

relevant
relevance level

very relevant

Modeling Document Persistence

The lifespan and number of versions of documents are not
correlated between them, but both are correlated with the
relevance of documents. Hence, to leverage this correlation
we modeled these measures of persistence with a logarithmic
function that gives a higher score to: (1) documents with a
longer lifespan; (2) documents with more versions. Both are
defined by the same function:
f (d) = logy (x)

(1)

where, for the first case, x is the number of days between
the first and last versions of document d, and for the second
case, x is the number of versions of document d. The y is
the maximum possible x for normalization. This is just an
example of a function that can be used to create ranking
features, such as these two features that we will use ahead
in this study.

4.3

Temporal Intervals

Temporal-Dependent Models

It is hard to establish clear temporal boundaries in web
data, because the ranking features tend to change gradually over time rather than abruptly. Thus, a model Mk is
learned using all training instances of all intervals T , but
each training instance contributes with a different weight to
the learning of Mk . The instances of interval Tk contribute
with a maximum weight, while the instances of other intervals Tj 6= Tk contribute with a weight defined by their
temporal distance to Tk . Consider Figures 5(a), 5(b) and
5(c) as illustrative examples. They depict the weights of a
collection with web snapshots between time points t1 and
t4 . Let’s assume that we want to create 3 different models, M = {M1 , M2 , M3 }, taking into account the different
characteristics of the web snapshots over time. For that,
we divide the collection in 3 time intervals T = {T1 , T2 , T3 }
or T = {[t1 , t2 ], ]t2 , t3 ], ]t3 , t4 ]}. Figure 5(a) shows that the
training instances of interval T1 , such as v1 , are used with

TEMPORAL-DEPENDENT RANKING

In this section, we present our temporal-dependent ranking framework for improving search effectiveness. First, we
formalize the ranking problem. Second, we explain how to
divide the training data by time, and third, how to use these
data to create temporal-dependent models. Fourth, we describe how to learn all models simultaneously and how to
combine them to produce a final ranking score. Last, we
present how to implement our framework.

4.1

(3)

Contrary to the traditional ranking problem, we learn
multiple ranking models, each taking into account the specific characteristics of a period. In order to achieve that, we
first identify a set of temporal intervals T = {T1 , T2 , ..., Tn },
from which we then learn multiple ranking models M =
{M1 , M2 , ..., Mn }. Each interval Tk has associated a set of
query-document feature vectors for training, where each feature vector i belongs to Tk if and only if the timestamp of
the respective document version ti ∈ Tk .
There are several timestamps associated to a document
version, such as the dates of creation, modification, crawling or archiving. The creation and modification dates are
good choices, since they refer to the time when a version
was created. However, identifying them is not straightforward. The metadata from the document’s HTTP header
fields, such as Date, Last-Modified and Expires are not always available, nor reliable. Studies estimate that from 35%
to 64% of web documents have valid last-modified dates [14],
but these percentages can be significantly improved by using the dates of the web document’s neighbors, especially of
web resources embedded in the selected document (e.g. images, CSS, JavaScript) [25]. Nevertheless, for simplification,
in this work we adopted the crawling date.

Figure 4: Fraction of documents with more than 10
versions for each relevance level.

4.

L(f (Xi ; ω), yi )

i=1

where Xi represents the ith query-document feature vector
and yi the corresponding relevance label. As Eq. 3 shows,
the typical L2R outcome is a single general model that ranks
documents independently when they were created or updated.

100%

not-relevant

3.4

m
X

(2)

Ranking Problem

The traditional ranking problem is to find a ranking model
f with parameters ω that receives X as input, where X is
an m×d matrix of m query-document feature vectors of size

760

time

0
t1

t2
v1

t3
v2

t1

t4
v1

v3

time

0
t1

t2

t3

weight

weight

v2

v3

0.5
time
t1

t4

t3

v2

t4

v3

1

0

(a)

t2

v1

1

1
0.5

time

0

t2

t3
(b)

t4

weight

we

0.5

0.5
0

time
t1

t2

t3

t4

(c)

weight
weight

weight

v1
v2
v3
Figure 5: Weights
ofv2training v3
instances, such as v1 , v2 and v3 , when learning ranking models (a) M1 , (b) M2
v1
1
and (c) M3 .
1
0.5
0.5
time
time instances
weight 1 when learning M1 , while the other
receive
and overlap between models, i.e. instead of minimizing Eq.
0
0
t2
t4
a weight t1
that decreases
as t3
the timestamps
of thet1instances
4 t3
for each model,
we minimize:
t2
t4
move away from T1 , such as v2 and v3 . Figures 5(b) and
m
n
5(c) show the values returned by temporal weight functions
X
X
ˆ
ˆ = arg min
when learning M2 and M3 , respectively.
L(
γ(Xi , Tj )fj (Xi ; ω), yi ) (6)
v1
v2 f1 , ..., fn
v3 f ,...,f ∈F
n
1
Contrary to typical learning to rank, our goal is to learn
i=1
j=1
1
the best model fˆ for a temporal interval Tk , such
that fˆ
where n is the number of temporal-dependent ranking modminimizes the following loss function L:
els. The minimization of this global loss function enables
0.5
learning all models
time simultaneously to optimize a unified relem
X
0
vance
target.
Notice that each training instance Xi is shared
ˆ
f = arg min
L(γ(Xi , Tk )f (Xi ; ω), yit1
)
(4)
t2
t4 f and the closer the time interval T to X
byt3each model
f ∈F
j
j
i
i=1
the greater this sharing. Models based on data learned from
where γ is the temporal weight function. We can adopt
time intervals far apart, will share little or no information of
several γ functions with the underlying idea that the weight
Xi . This is important for distant time intervals do not end
decreases as the temporal distance increases, such as the
up influencing negatively each other.
following function:
After learning all temporal-dependent models, we employ
an unsupervised ensemble method to produce the final rank
1
if Xi ∈ Tk
ing score. We run each of the n ranking models fj against
γ(Xi , Tk ) =
i ,Tk )
if Xi 6∈ Tk
1 − α distance(X
a testing instance Xi multiplied by its temporal weight γ
(5)
|T |
to the corresponding interval Tj . Then, we sum all scores
s.t. 0 ≤ γ ≤ 1
produced by all ranking models:
where distance(Xi , Tk ) is the absolute difference between
the date of document version in Xi and the closer date to
n
X
interval Tk , i.e. to the begin or end of Tk . |T | denotes the
γ(Xi , Tj )fj (Xi ; ω)
(7)
score(Xi ) =
total time covered by the collection. The γ function may
j=1
have a larger or a smaller slope α to learn ranking models
This ensemble method follows the global loss function (Eq.
with higher or lower contribution of the training instances.
6)
used in the learning phase, trying to minimize the overall
For instance, by having a α of 2 instead of 1, the ranking
prediction
error and improve the final search effectiveness.
model will be learned with half the contribution of the training instances and will ignore the instances in the half most
4.5 L2R Algorithm
distant intervals.
Our temporal-dependent ranking framework is quite flexible and can be implemented using different L2R algorithms
4.4 Multi-task Learning
as long as we adapt them to use the global loss function of
A temporal-dependent model has two advantages over a
Eq. 6. We followed the work of Bian et al. and adapted the
model that only learns from data of a segment of time. First,
RankSVM algorithm [3].
solutions where each model learns from a part of the training
The goal of RankSVM is learning a linear model that mindata tend to present bad performance results, because more
imizes the number of pairs of documents ranked in the wrong
data usually beats better machine learning algorithms [2].
relative order [15]. Formally, RankSVM minimizes the folThus, each temporal-dependent model considers all training
lowing objective function:
instances during learning, avoiding the problem of the lack
X
of data. Second, a temporal-dependent model considers the
1
ξq,i,j
min ||ω||2 + C
dependency between datasets of different temporal intervals.
ω,ξq,i,j 2
q,i,j
A model will learn more from instances of closer intervals
(8)
s.t. ω T Xiq ≥ ω T Xjq + 1 − ξq,i,j ,
than from instances of intervals more far apart.
Another important aspect is that we want to minimize
∀Xiq  Xjq , ξq,i,j ≥ 0
the overall prediction error of all temporal-dependent modwhere Xiq  Xjq implies that document i is ranked ahead of
els, since all will be employed to rank query results. Hence,
document j with respect to query q. C is a trade-off coeffiwe minimize a global relevance loss function, which evaluates
cient between
the model complexity ||ω||2 and the training
the overall training error, instead of minimizing multiple inP
error
ξq,i,j .
dependent loss functions without considering the correlation

761

We modified the objective function of RankSVM following
our global loss function, which takes into account the feature
specificities of web snapshots over time. Each temporaldependent ranking model Mk is learned by minimizing the
following objective function:

min

ω,ξq,i,j

s.t.

n
X

γ(Xiq , Tk )ωkT Xiq ≥

k=1

n
X

Grade
# judgments

n
X
1X
||ωk ||2 + C
ξq,i,j
2
q,i,j

γ(Xjq , Tk )ωkT Xjq + 1 − ξq,i,j ,

k=1

EXPERIMENTAL SETUP

1. How much can we improve the search effectiveness of
state-of-the-art WAIR using the L2R framework? We
believe that the observations made in the context of
L2R applied to document retrieval hold in relation to
WAIR, but this hypothesis has not been tested.
2. Do temporal features intrinsic to web archives improve
WAIR, such as the features based on the long-term
persistence of web documents described in Section 3?
3. Does our temporal-dependent ranking framework described in Section 4 improve WAIR over a single general model that fits all data independently of its time?
Next, we give a brief description of the L2R dataset and
the ranking features used in the experiments. Then, we
present the compared ranking algorithms and models, and
for last, the evaluation methodology and metrics.

5.3

L2R Dataset

Ranking Algorithms

The way L2R algorithms learn can be categorized into
three approaches: pointwise, pairwise and listwise [21]. We
employed three state-of-the-art L2R algorithms that cover
the three approaches:
pointwise: Random Forests consists of multiple regression
trees, where each tree is built from a bootstrap sample
of the training data and a random subset of features is
selected to split each node of a tree [4]. The relevance
score of each document is the average of the outputs of
the individual regression trees.
pairwise: RankSVM which is described in Section 4.5.
listwise: AdaRank is a boosting algorithm that linearly combines ”weak learners”, which are iteratively selected as the
feature that offers the best performance among all others
[30]. Each new learner focus on the queries not ranked
well on previous iteration, by giving more weight to them.
RankSVM and AdaRank produce linear models, while
Random Forests produce nonlinear models. In all experiments we used the RankSVM implementation available in
the SVMrank software5 and the implementation of the other
two L2R algorithms in the RankLib software6 .

The L2R dataset is composed by a set of <query, document version, grade, features> quadruples, where the grade
indicates the relevance degree of the document version to
the query. The features represent a vector of ranking feature values, each describing an estimate of relevance for the
<query, document version> pair.
From the 269 801 <query, document version> pairs assessed in the test collection described in Section 3.1, we extracted 39 608 quadruples with 68 features. This is the size
of the dataset, which has on average 843 versions per query.
3 queries were excluded from the 50, because their relevant
and very relevant versions did not contain all features.
Table 2 shows the distribution of relevance judgments per
relevance grade. As expected, the number of relevant and
very relevant versions is much less than the not-relevant. Notice that for each of these navigational queries there is usually only one very relevant version and/or one relevant version. The dataset is publicly available for research at http:
//code.google.com/p/pwa-technologies/wiki/L2R4WAIR.

5.2

4 357

not
relevant
30 641

Each class explores a different type of data:
term-weighting features estimate the similarity between
the query and the different sections of a document version
(anchor text of incoming links, text body, title and URL),
such as Okapi BM25 [27].
term-distance features use the distance between terms in
the different sections of a document version to quantify
the relatedness between them, such as the Minimal Span
Weighting function [23].
URL features compute an importance measure based on
the probability of URLs representing an entry page, using
the number of slashes, their length, or if they refer to a
domain, sub-domain or page [18].
web-graph features estimate the popularity or importance
of a document version inferred from the graph of hyperlinks between versions. These features include the number
of inlinks to a version.
temporal features consider the time dimension of the web.
They include the age of a document version and the two
features described in Section 3.4 based on the long-term
persistence of web documents.
Some of these features are typically used in web search
engines and their results have been proven over time. The
temporal features, however, were implemented specifically
for this research. The complete list of features can be consulted online4 . All feature values were scaled to a range
between 0 and 1 using a min-max normalization.

k=1

This section presents our experimental setup that enabled
us to answer the following questions:

5.1

relevant

Table 2: Distribution of relevance judgments in the
L2R dataset per relevance grade.

∀Xiq  Xjq , ξq,i,j ≥ 0
(9)

5.

very
relevant
4 610

Ranking Features

The effectiveness of the ranking models greatly depends
on the quality of the features they use. We give an overview
of the classes of the 68 features released in the L2R dataset.

4

http://pwa-technologies.googlecode.com/files/featureList.pdf
http://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html
6
http://www.cs.umass.edu/~vdang/ranklib.html
5

762

5.4

Ranking Models Compared

(NDCG@1, NDCG@5 and NDCG@10). P @k measures the
relevance of the top k document versions in a ranking list
with respect to a query and is calculated as follows:

To compare the search effectiveness of the proposed approaches we evaluated the following ranking models:
1. Models with manually tuned features: these are
baseline models. For comparison we included the results
of three ranking models with manually tuned features,
obtained from a related work [8]. The first model is
the Okapi BM25 with parameters k1=2 and b=0.75 [27].
The second is Lucene’s term-weighting function7 , which is
computed over five fields (anchor text of incoming links,
text body, title, URL and hostname of URL) with different weights. The third is a small variation of Lucene
used in NutchWAX, with a different normalization by field
length. These last two models can be considered the stateof-the-art in WAIR, since the most advanced IR technology currently used in web archives is based on the Lucene
and NutchWAX search engines [13].
2. Models with regular features combined with L2R:
these are another class of baseline models, but based on
the technology usually employed in web search engines.
These models contain all ranking features described in
Section 5.2, except the temporal features. The regular
features were automatically combined using the L2R algorithms to create a single ranking model. These models
are denoted as the single-model approach with regular
features.
3. Models with all features combined with L2R: these
are the same models as in the previous point, but with all
ranking features, regular and temporal. All these features
were automatically combined by L2R algorithms to create
a single ranking model. We refer to these models as the
single-model approach with all features.
4. Models with regular features combined with the
temporal-dependent ranking framework: unlike the
previous models created independently of the time of each
document version, these ranking models were created using the temporal-dependent ranking framework proposed
in Section 4. The framework used equal intervals of time
with an approximate number of training instances. The
models only contain regular features.
5. Models with all features combined with the temporal-dependent ranking framework: these are the same
models as in the previous point, but with all ranking features, regular and temporal.

5.5

P @k =

Pk

i=1

r(i)

k

where r(i) is the relevance of the document version ranked
at position i. Precision works over binary judgments. Due
to that, the very relevant and relevant judgments were both
taken as relevant when using P@k.
NDCG@k handles multiple levels of relevance and gives a
higher score to relevant documents in higher ranking positions. It is calculated as follows:
P
2r(i) −1
N DCG@k = Zk ki=1 log
(1+i)
2

where Zk is a normalization constant for the perfect list to
get a NDCG@k of 1.
The past experience in web archive assessment has shown
that users do not want to see multiple versions of a URL
on the search results, but rather only one URL with a link
to a list of all the other versions of that URL [8]. This
corresponds to the common behavior already implemented
in the user interfaces of existing WAIR systems. Hence, we
evaluate only the first document version shown in the search
results and ignore all the other versions of the same URL,
before applying P@k or NDCG@k.

6.

EXPERIMENTAL RESULTS

In this section we report and discuss the results of the
tested ranking models, summarized in Table 3.

Baselines.
The NutchWAX model performs better than the Lucene
and BM25 models. However, its performance is significantly
worse than the models produced by the L2R algorithms using regular features. For instance, the model produced with
the Random Forests algorithm, which presents the best results of the three L2R algorithms, has a NDCG@10 of 0.650,
while NutchWAX gets 0.174. This is more than a three times
increase. All models derived from L2R algorithms achieved
better results than NutchWAX in all metrics with a statistical significance of p<0.01 using a two-tailed paired Student’s
t-test. This strongly indicates, as expected, that the use of
L2R with ranking features typically used in web search engines, improves the search effectiveness of web archives, but
also that the commonly used WAIR engines have a quite
poor performance.

Evaluation Methodology and Metrics

We chose a five-fold cross-validation to compare the average performance of the different ranking models. The L2R
dataset was divided in five folders, where each folder has
three subsets: training, validation and testing. Each ranking model was created, for each folder, using the training
data. The validation data was used to tune the parameters
of the L2R algorithms and the test data was used only on
the evaluation of the model to avoid overfitting. The final
results are the averages of the five tests.
Each of the 50 evaluated navigational queries may have
one very relevant version and several relevant versions. Considering this fact, the ranking models were evaluated with
two of the most used evaluation metrics: Precision at three
cut-off values (P@1, P@5 and P@10) and the Normalized
Discount Cumulative Gain at the same three cut-off values

Temporal features.
All previous models are baselines. Hence, we compared
only against the strongest baseline, i.e. the models with regular features combined with L2R algorithms. We analyzed
the discriminative power of the temporal ranking features
by running the L2R algorithms with and without these features. We can see a clear pattern. The L2R algorithms
almost always present statistically significant improvements
for all metrics when using the temporal features. For instance, Random Forests has a NDCG@1 superior in 10% to
the same algorithm learning without the temporal features
and RankSVM increased 3 percentage points. Therefore,
the temporal features intrinsic to web archives can be used
to improve WAIR.

7

http://lucene.apache.org/java/2_9_0/api/all/org/apache/lucene/
search/Similarity.html

763

Metric
NDCG@1
NDCG@5
NDCG@10
P@1
P@5
P@10

models with features
manually tuned
BM25 Lucene NutchWAX
0.250
0.220
0.250
0.145
0.157
0.215
0.119
0.133
0.174
0.300
0.280
0.320
0.140
0.164
0.236
0.108
0.132
0.168

models with regular features
combined with L2R
AdaRank RankSVM R. Forests
0.380 †
0.500 †
0.550 †
0.427 †
0.485 †
0.610 †
0.470 †
0.523 †
0.650 †
0.460 †
0.560 †
0.640 †
0.264 †
0.276 †
0.390 †
0.182 †
0.194 †
0.236 †

models with all features
combined with L2R
AdaRank RankSVM R. Forests
0.400 †
0.530 †‡
0.650 †‡
0.426 †
0.546 †‡
0.665 †‡
0.476 †
0.571 †‡
0.688 †‡
0.480 †
0.580 †‡
0.760 †‡
0.260 †
0.324 †‡
0.396 †‡
0.182 †
0.196 †
0.238 †

† shows a statistical significance of p<0.01 against NutchWAX with a two-sided paired t-test, while ‡ shows a statistical significance of
p<0.05 against the models with regular features combined with L2R (i.e. we compare the same model with and without temporal features).
The bold entries indicate the best result achieved in each metric.

Table 3: Results of the tested ranking methods.

Temporal-dependent ranking framework.

BM25 over all fields
TF-IDF over all fields
Number of versions of a URL
TF-IDF over the hostname of URL
Length of the shortest text with all query terms in title
Days between the first and last versions of a URL

Finally, we analyzed the single-model approach versus the
temporal-dependent ranking framework, with and without
temporal features. Figures 6 and 7 show the NDCG@1,
NDCG@5 and NDCG@10 values obtained with the temporaldependent ranking framework, when using regular features
or all features. We tested the framework with different time
intervals (1, 2, 4, 7 and 14) and different slopes α in the
temporal weight function (0.25, 0.5, 0.75, 1, 1.25 and 1.5).
Notice that we used a test collection with 14 years of web
snapshots. Thus, when we use 14 or 7 time intervals, it
means that a model is created for each year or two years,
respectively. The use of 1 time interval is similar to creating
just one model, i.e. the single-model approach.
The results show that the proposed temporal-dependent
ranking framework outperforms the single-model approach,
with and without temporal features. We achieved improvements for all time intervals, but the highest improvements
were obtained when we used 4 or 7 intervals. Results depicted in Figure 6 without temporal features, show that the
major increase for NDCG@1 was from 0.500 to 0.560 (+6%)
when using 4 and 7 intervals, while for a NDCG@5 was
from 0.485 to 0.551 (+6.6%) and for NDCG@10 was from
0.523 to 0.572 (+4.9%), both when using 4 intervals. Results depicted in Figure 7 with temporal features, show that
the major increase for NDCG@1 was from 0.530 to 0.590
(+6%) when using 7 intervals, while for a NDCG@5 was
from 0.546 to 0.583 (+3.7%) and for NDCG@10 was from
0.571 to 0.604 (+3.3%), both when using 4 intervals. All
these improvements, which present a statistical significance
(p<0.05), indicate that the values of the ranking features
change considerably over time in a way that can be learned
by ranking models to better differentiate between relevant
and not-relevant documents.
The slope α of the temporal weight function in Eq. 5 has
an important impact in the final results. We obtained the
worst results when α is larger than 1, i.e. when the contribution of the training instances is smaller. On the other
hand, a small α, such as 0.25, caused a larger than desired
contribution of the training instances. The best results were
achieved with α between 0.5 and 1.
The temporal features and the temporal-dependent ranking framework, are independent approaches that demonstrate promising results. However, both approaches also
work well together. In fact, the results displayed in Figure
7 show that we achieved the best results when we combined

Table 4: Top 6 most important ranking features for
the temporal-dependent ranking framework.
them. The NDCG@1, NDCG@5 and NDCG@10 are superior in 9%, 10% and 8%, respectively, over the single-model
approach using just regular features.

6.1

Results Analysis

We analyzed why the temporal-dependent ranking framework produces better results than the typical single ranking model created by L2R algorithms. We sorted the ranking features by their importance, measured by the absolute
weight assigned by RankSVM. We found that the top features are almost the same, whether using just one model
or multiple temporal-dependent models. The difference between ranking models created for different time intervals lies
on small changes in the weights of features. This finding corroborates our observations that the characteristics of web
documents evolve smoothly rather than abruptly and the
temporal-dependent ranking models can adjust the feature
weights to provide fine-grained ranking over time.
Table 4 shows the top 6 most important ranking features
for the temporal-dependent ranking framework. From this
table, we can see that BM25 and TF-IDF over all fields
are the features with higher weight. The features based on
long-term persistence of web documents, using the number
of versions and the number of days between the first and
last versions, are also at the top. RankSVM weighted some
of these as the best features to identify relevant document
versions for navigational queries.

7.

CONCLUSION & FUTURE WORK

The retrieval effectiveness of state-of-the-art WAIR systems is poor, preventing users from unfolding the full potential of web archives. This work made a few contributions
to face this problem. We studied, for the first time, the
effects of long-term web document persistence in relevance
ranking. In our experiments, conducted over 14 years of

764

0.58

0.61
0.59
α = 0.25

0.54

NDCG@1

NDCG@1

0.56
α = 0.5

0.52

α = 0.75

0.5

α=1

0.48

α = 1.25

0.46

α = 1.5

14

7

4

2

α = 0.25

0.57

α = 0.5

0.55

α = 0.75
α=1

0.53

α = 1.25

0.51

1

α = 1.5

14

7

time intervals
(a)

1

0.61

0.56

0.59

α = 0.25

0.54

NDCG@5

NDCG@5

2

time intervals
(a)

0.58

α = 0.5

0.52

α = 0.75

0.5

α=1

0.48
7

4

2

α = 0.5

0.55

α = 0.75
α=1
α = 1.25

0.51

α = 1.5

14

α = 0.25

0.57

0.53

α = 1.25

0.46

α = 1.5

14

1

7

4

2

1

time intervals
(b)

time intervals
(b)
0.61

0.58
0.56

0.59

α = 0.25

0.54

NDCG@10

NDCG@10

4

α = 0.5

0.52

α = 0.75

0.5

α=1

0.48

α = 1.25

0.46

α = 1.5

14

7

4

2

α = 0.25

0.57

α = 0.5

0.55

α = 0.75
α=1

0.53

α = 1.25

0.51

α = 1.5

14

1

7

4

2

1

time intervals
(c)

time intervals
(c)

Figure 6: (a) NDCG@1, (b) NDCG@5 and (c)
NDCG@10 results of the temporal-dependent ranking framework using different time intervals and α
values of the temporal weight function. These models contain regular features.

Figure 7: (a) NDCG@1, (b) NDCG@5 and (c)
NDCG@10 results of the temporal-dependent ranking framework using different time intervals and α
values of the temporal weight function. These models contain regular and temporal features.

web snapshots, we found that relevant documents tend to
have a longer lifespan and more versions. Significant gains
were achieved by modeling these persistence characteristics
of web documents as novel ranking features. Additionally,
since the characteristics of the web vary over time, both in
structure and content, we proposed a temporal-dependent
ranking framework that learns a different ranking model
for each successive web period. Our experimental results
show that the proposed multi-model framework outperforms
a simpler approach based on a single ranking model, when
both use the same L2R algorithms.
This work is focused in WAIR, but we believe that our
approach could bring similar improvements to any digital
libraries dealing with versioned content spanning long periods. As future work, we intend to study the evolution of
URLs over time and their impact on search, since we detected changes in many URLs’ top-level domains and sub-

domains. By tracking this evolution, we can better measure
the long-term persistence of web documents. We also plan to
investigate how to extend the temporal-dependent ranking
framework to handle temporal diversity in search results.

8.

ACKNOWLEDGMENTS

We thank FCT for the financial support of the Research
Units of LaSIGE (PEst-OE/EEI/UI0408/2014) and INESCID (Pest-OE/EEI/LA0021/2013), and the DataStorm Research Line of Excellency (EXCL/EEI-ESS/0257/2012).

9.

REFERENCES

[1] E. Adar, J. Teevan, S. Dumais, and J. Elsas. The web
changes everything: understanding the dynamics of
web content. In Proc. of the 2nd ACM International
Conference on Web Search and Data Mining, pages
282–291, 2009.

765

[2] M. Banko and E. Brill. Scaling to very very large
corpora for natural language disambiguation. In Proc.
of the 39th Annual Meeting on Association for
Computational Linguistics, pages 26–33, 2001.
[3] J. Bian, X. Li, F. Li, Z. Zheng, and H. Zha. Ranking
specialization for web search: a divide-and-conquer
approach by using topical RankSVM. In Proc. of the
19th International Conference on World Wide Web,
pages 131–140, 2010.
[4] L. Breiman. Random forests. Machine learning,
45(1):5–32, 2001.
[5] Y. Chung, M. Toyoda, and M. Kitsuregawa. A study
of link farm distribution and evolution using a time
series of web snapshots. In Proc. of the 5th
International Workshop on Adversarial Information
Retrieval on the Web, pages 9–16, 2009.
[6] M. Costa and M. J. Silva. Understanding the
information needs of web archive users. In Proc. of the
10th International Web Archiving Workshop, pages
9–16, 2010.
[7] M. Costa and M. J. Silva. Characterizing search
behavior in web archives. In Proc. of the 1st
International Temporal Web Analytics Workshop,
pages 33–40, 2011.
[8] M. Costa and M. J. Silva. Evaluating web archive
search systems. In Proc. of the 13th International
Conference on Web Information Systems Engineering,
pages 440–454, 2012.
[9] N. Dai, M. Shokouhi, and B. Davison. Learning to
rank for freshness and relevance. In Proc. of the 34th
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages
95–104, 2011.
[10] J. Elsas and S. Dumais. Leveraging temporal
dynamics of document content in relevance ranking. In
Proc. of the 3rd ACM International Conference on
Web Search and Data Mining, pages 1–10, 2010.
[11] D. Fetterly, M. Manasse, M. Najork, and J. L. Wiener.
A large-scale study of the evolution of web pages. In
Proc. of the 12th International Conference on World
Wide Web, pages 669–678, 2003.
[12] X. Geng, T. Liu, T. Qin, A. Arnold, H. Li, and
H. Shum. Query dependent ranking using k-nearest
neighbor. In Proc. of the 31st International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pages 115–122, 2008.
[13] D. Gomes, J. Miranda, and M. Costa. A survey on
web archiving initiatives. In Proc. of the International
Conference on Theory and Practice of Digital
Libraries, pages 408–420, 2011.
[14] D. Gomes and M. Silva. Modelling information
persistence on the web. In Proc. of the 6th
International Conference on Web Engineering, pages
193–200, 2006.
[15] T. Joachims. Optimizing search engines using
clickthrough data. In Proc. of the 8th ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining, pages 133–142, 2002.
[16] R. Jones and F. Diaz. Temporal profiles of queries.
ACM Transactions on Information Systems, 25(3),
2007.
[17] I. Kang and G. Kim. Query type classification for web

[18]

[19]

[20]

[21]

[22]
[23]

[24]

[25]

[26]

[27]

[28]

[29]

[30]

[31]

766

document retrieval. In Proc. of the 26th International
ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 64–71,
2003.
W. Kraaij, T. Westerveld, and D. Hiemstra. The
importance of prior probabilities for entry page search.
In Proc. of the 25th International ACM SIGIR
Conference on Research and Development in
Information Retrieval, pages 27–34, 2002.
J. Leskovec, J. Kleinberg, and C. Faloutsos. Graph
evolution: densification and shrinking diameters. ACM
Transactions on Knowledge Discovery from Data,
1(1):2, 2007.
X. Li and W. B. Croft. Time-based language models.
In Proc. of the 12th International Conference on
Information and Knowledge Management, pages
469–475, 2003.
T. Liu. Learning to rank for information retrieval,
volume 3 of Foundations and Trends in Information
Retrieval. Now Publishers Inc., 2009.
J. Masanès. Web Archiving. Springer-Verlag New York
Inc., 2006.
C. Monz. Minimal span weighting retrieval for
question answering. In Proc. of the SIGIR 2004
Workshop on Information Retrieval for Question
Answering, pages 23–30, 2004.
A. Ntoulas, J. Cho, and C. Olston. What’s new on the
web?: the evolution of the web from a search engine
perspective. In Proc. of the 13th International
Conference on World Wide Web, pages 1–12, 2004.
S. Nunes, C. Ribeiro, and G. David. Using neighbors
to date web documents. In Proc. of the 9th Annual
ACM International Workshop on Web Information
and Data Management, pages 129–136, 2007.
K. Radinsky and E. Horvitz. Mining the web to
predict future events. In Proc. of the 6th ACM
International Conference on Web Search and Data
Mining, pages 255–264, 2013.
S. Robertson and H. Zaragoza. The probabilistic
relevance framework, volume 3 of Foundations and
Trends in Information Retrieval. 2009.
T. Salles, L. Rocha, G. L. Pappa, F. Mourão,
W. Meira Jr, and M. Gonçalves. Temporally-aware
algorithms for document classification. In Proc. of the
33rd International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 307–314, 2010.
N. Tahmasebi, G. Gossen, and T. Risse. Which words
do you remember? Temporal properties of language
use in digital archives. In Proc. of the 2nd
International Conference on Theory and Practice of
Digital Libraries, pages 32–37, 2012.
J. Xu and H. Li. Adarank: a boosting algorithm for
information retrieval. In Proc. of the 30th
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages
391–398, 2007.
Y. Yamamoto, T. Tezuka, A. Jatowt, and K. Tanaka.
Honto? Search: estimating trustworthiness of web
information by search results aggregation and
temporal analysis. Advances in Data and Web
Management, pages 253–264, 2007.

