Statistical Significance Testing in Information Retrieval:
Theory and Practice
Ben Carterette
carteret@udel.edu
Dept. of Computer & Information Sciences, University of Delaware, Newark, DE, USA

ABSTRACT

ii. t-test (including ANOVA and the linear model)
iii. randomization and bootstrap tests
iv. meta-analysis

The past 20 years have seen a great improvement in the rigor
of information retrieval experimentation, due primarily to
two factors: high-quality, public, portable test collections
such as those produced by TREC (the Text REtrieval Conference [2]), and the increased practice of statistical hypothesis testing to determine whether measured improvements
can be ascribed to something other than random chance.
Together these create a very useful standard for reviewers,
program committees, and journal editors; work in information retrieval (IR) increasingly cannot be published unless it
has been evaluated using a well-constructed test collection
and shown to produce a statistically significant improvement
over a good baseline.
But, as the saying goes, any tool sharp enough to be useful is also sharp enough to be dangerous. Statistical tests
of significance are widely misunderstood. Most researchers
treat them as a “black box”: evaluation results go in and a
p-value comes out. Because significance is such an important factor in determining what research directions to explore and what is published, using p-values obtained without
thought can have consequences for everyone doing research
in IR. Ioannidis has argued that the main consequence in the
biomedical sciences is that most published research findings
are false [1]; could that be the case in IR as well?

(c) Bayesian approaches
2. Theory of significance testing and what it means for you
(a) Terms and definitions—a test-independent foundation
i. null hypothesis and alternate hypothesis
ii. paired/unpaired; one-tailed/two-tailed
iii. test statistic; confidence interval; p-value; critical
value; effect size
iv. power and accuracy; false positives and false negatives in testing
v. sources of variance; within-group and between-group
variance
(b) Myths and misconceptions
i. statistical significance has intrinsic meaning
ii. when data violates the assumptions of a test, the
test cannot be used
iii. the p-value has an intrinsic meaning
iv. smaller p-values indicate greater significance
v. a p-value less than 0.05 indicates significance
vi. there is no harm in performing multiple tests

Categories and Subject Descriptors: H.3 [Information
Storage and Retrieval]; H.3.4 [Systems and Software]:
Performance Evaluation

3. How significance testing affects us all
(a) Three classes of scientist engineers and how they make
use of significance tests:

Keywords: statistical significance, information retrieval

1.

i. as readers of research papers: to guide choice of
baseline systems
ii. as working scientists/engineers: to guide experimentation and determine what to publish or deploy
iii. as reviewers and editors: to guide publication recommendations and decisions

OUTLINE

1. Introduction to testing significance in IR
(a) Why test significance?

(b) How adherence to the misconceptions listed above can
set back research and development in IR

(b) Common tests used in IR
i. Wilcoxon signed rank test

(c) Experimental design and validity
(d) Recommendations for using and interpreting tests

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the owner/author(s). Copyright is held by the
author/owner(s).
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
ACM 978-1-4503-2257-7/14/07.
http://dx.doi.org/10.1145/2499178.2499204.

2.

REFERENCES

[1] John P. A. Ioannidis. Why most published research
findings are false. PLoS Medicine, 2(8), 2005.
[2] Ellen M. Voorhees and Donna K. Harman. TREC:
Experiments and evaluation in information retrieval.
The MIT Press, 2005.

1286

