PRISM: Concept-preserving Social Image Search Results
Summarization
Boon-Siew Seah

seah0097@ntu.edu.sg

Sourav S Bhowmick
assourav@ntu.edu.sg

Aixin Sun

axsun@ntu.edu.sg

School of Computer Engineering, Nanyang Technological University, Singapore

ABSTRACT
Most existing tag-based social image search engines present search
results as a ranked list of images, which cannot be consumed by
users in a natural and intuitive manner. In this paper, we present a
novel concept-preserving image search results summarization algorithm named prism. prism exploits both visual features and tags of
the search results to generate high quality summary, which not only
breaks the results into visually and semantically coherent clusters
but it also maximizes the coverage of the summary w.r.t the original search results. It ﬁrst constructs a visual similarity graph where
the nodes are images in the search results and the edges represent
visual similarities between pairs of images. This graph is optimally
decomposed and compressed into a set of concept-preserving subgraphs based on a set of summarization objectives. Images in a
concept-preserving subgraph are visually and semantically cohesive and are described by a minimal set of tags or concepts. Lastly,
one or more exemplar images from each subgraph is selected to
form the exemplar summary of the result set. Through empirical
study, we demonstrate the eﬀectiveness of prism against state-ofthe-art image summarization and clustering algorithms.

(a) query: “fruit”

Figure 1: [Best viewed in color] Sample query results.
out a user’s search intent [18]. An immediate aftermath of such results diversiﬁcation strategy is that often the search results are not
semantically or visually coherent. For example, the results of a
search query “fruit” may include images of strawberries, apples, oranges, and even fruit-related concepts such as market and fruit juice
as illustrated in Figure 1(a). Similarly, consider Figure 1(b) which
depicts results of the query “ﬂy”. Observe that the results contain
a medley of visually and semantically distinct objects and scenes
(hereafter collectively referred to as concepts) such as parachutes,
aeroplanes, insects, birds, and even the act of jumping.
Image search results are typically presented as a ranked list of
images often in the form of thumbnails (e.g., Figure 1). Such
thumbnail view suﬀers from two key limitations. First, it fails to
provide a view of common visual objects or scenes collectively.
For example, the result images of “fruit” and “ﬂy” queries can be
clustered by visual objects (e.g., strawberry, aeroplane, insect) and
activities (e.g., jump). Such organized image search results will
naturally enable a user to quickly identify and zoom into a subset of
results that is most relevant to her query intent. Second, a thumbnail
view fails to provide a bird eye view of diﬀerent concepts present
in a query results. For instance, reconsider Figure 1(b) containing
a medley of concepts. It will be beneﬁcial to users if a suitable exemplar image from each type of concept can be selected to create
a “summary” of the search results. In this paper, we take a systematic step towards addressing these limitations associated with social
image search results.
An appealing way to organize social image search results of a
search query is to generate a set of image clusters from them such
that images in each cluster are semantically and visually coherent
and the clusters maximally cover the entire result set. Subsequently,
at least one exemplar image from each cluster can be selected to
generate an exemplar summary of the entire result set to give a
bird’s-eye view of diﬀerent concepts in it. We advocate that such
image clusters must satisfy the following desirable features.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval

Keywords
Image Search Summarization; Tag-based Image Search; Flickr

1.

(b) query: “ﬂy”

INTRODUCTION

The rising prominence of image sharing platforms like Flickr
and Instagram has led to an explosion of social images. Consequently, the need for superior social image search engines to support eﬃcient and eﬀective tag-based image retrieval (TagIR) has
become increasingly pertinent. Queries in a tag-based social image
search engine are often short and ambiguous. As a result, search
engines often diversify the search results to match all possible aspects of a query in order to minimize the risk of completely missing
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2257-7/14/07 ...$15.00.
http://dx.doi.org/10.1145/2600428.2609586.

• Concept-preserving. Each cluster should be annotated by a minimal set of tags generated from the images within to semanti-

737

([HPSODUV

VWUDZEHUU\

6KDUHG1HDUHVW1HLJKERU &RQFHSW9LVXDO >@

VWUDZEHUU\ 

RUDQJH  \HOORZ  OHPRQ  UHG 

([HPSODUV

RUDQJH\HOORZOHPRQ

IUXWDVPDUNHWYHJHWDOHV
PHUFDGR

FKHUU\

OHPRQ

PHUFDGR 

FKHUU\ 

OHPRQ 

IUXWDV  PDUNHW  YHJHWDOHV  PHUFDGR 



+03 &RQFHSW9LVXDO >@
+

RUDQJH  PDFUR  VWLOOOLIH  EODFN 

UHG  GD\  VWHPV  VQDFN 

RUDQJHPDFURVWLOOOLIH

UHGGD\VWHPV

&DQRQLFDO9LHZ 9LVXDO >@
VSODVK

NLZL

VSODVK 

NLZL 

UHG  IRRG  PDFUR  VWUDZEHUU\ 

VWUDZEHUU\  VN\  EOXH  JDUGHQ 

UHGIRRGPDFUR

VWUDZEHUU\VN\EOXH

$IILQLW\3URSDJDWLRQ 9LVXDO >@
FLWUXV

FLWUXV 
UHG  YHJHWDEOHV  RYHUWKHH[FHOOHQFH  IRRG 

SHDUV

SHDUV 

IRRG  UHG  EODFN  VWUDZEHUU\
\ 

D &RQFHSW3UHVHUYLQJ6XPPDUL]DWLRQ 35,60

UHGYHJHWDEOHVRYHUWKHH[HOOHQFH

IRRGUHGEODFN

E 1RQ&RQFHSW3UHVHUYLQJ6XPPDUL]DWLRQ

Figure 2: [Best viewed in color] Sample summarizations from 5 methods for the query “fruit”. The percentage associated with
each tag represents the percentage of images in the cluster having the speciﬁc tag. The types of features used by a method is indicated
after the method name in parentheses. A summary is constructed by selecting 1 to 3 images per cluster as exemplar.
cally1 describe all images in the cluster. Users therefore can easily associate the tag(s) with the images in a cluster at a glance.
We refer to such a cluster as concept-preserving where a set
of images shares at least one concept (tag)2 . Figure 2 depicts
the distinction between concept-preserving (Figure 2(a)) and
non-concept-preserving clusters (Figure 2(b)). In the conceptpreserving “pear” cluster, a single “pear” tag is suﬃcient to represent all images in it and describe them semantically. In contrast, the “orange, yellow, lemon, red” cluster requires four tags
to represent all images and furthermore the content of these images is unclear from the tags. Intuitively, it is easier to quickly
digest the concept associated with the former cluster compared
to the latter due to the cognitive burden imposed by multiple
tags (concepts) even if these tags are related.

Figure 3: [Best viewed in color] Google Images results (“fly”).

• Visual coherence. Images in a cluster must be visually coherent.
Visually similar images must be clustered together and dissimilar images must be separated in diﬀerent clusters.
• Coverage. The image clusters should cover as much of the result set as possible for maximizing incorporation of all possible
query intent. In other words, image clusters should represent
majority of the original result images. For instance, reconsider
the set of image clusters in Figure 2(a). Assume that the “splash”
and “cherry” clusters are missing. In this case, the image clusters
are considered to be less complete than expected as their coverage is not maximized. Obviously, this will lead to an exemplar
summary that does not maximally cover the result images.

Figure 4: [Best viewed in color] Bing Images results (“fly”).
anywhere near 100% of the images, all the four tags are needed
to describe the images in the cluster. An alternative representation
is to select the “best” tag (e.g., the most probable tag for a given
cluster [15]). However, the “best” tag often fails to represent all
images in the cluster, which may mislead and confuse users. For
instance, consider the “strawberry, sky, blue, garden” cluster generated by [15] where no single tag can correctly represent all images.
Note that the aforementioned limitations are not only conﬁned
to social images search engines. Even for query-speciﬁc image
categorization techniques provided by Web image search engines
(e.g., Google Images (images.google.com), Bing Images (www.
bing.com/images)), where data associated with images are not as
sparse as social images, there is little evidence whether they maximally cover the results. For example, consider the image categories
generated by Google Images (Figure 3) and Bing Images (Figure 4)
for the query “fly”3 . Despite having signiﬁcantly larger datasets

Recently, early fusion [13, 20] and late fusion [10] approaches
have attempted to summarize image search results. The former
exploits the tags and visual content of the images jointly whereas
the latter considers them independently. However, these techniques
do not ensure that the generated summaries are concept-preserving
and maximally covers the image results. To illustrate this, consider
the “orange, macro, stilllife, black” cluster and exemplar summary
in Figure 2(b) generated by [20]. With no single tag representing
1
We assume that the tags are high-level semantic concepts assigned by image uploaders or annotators.
2
In the sequel, we use tag and concept interchangeably.

3

738

The results are last accessed on August 13, 2013.

 





 

based on some summarization objectives that encompass the aforementioned features of image clusters. Particularly, images in each
subgraph represents a concept-preserving cluster. Following that,
prism performs a series of image set compression to simplify the
subgraphs to form the ﬁnal set of concept-preserving subgraphs.
Lastly, one or more exemplar images from each subgraph is selected to form the exemplar summary as depicted in Figure 2(a).
The rest of the paper is organized as follows. Section 2 reviews
related research and Section 3 deﬁnes the research problem. Section 4 presents the prism algorithm. We investigate the performance
of prism in Section 5. Section 6 concludes the paper. A preliminary
two-page poster of prism is presented in [14].







 



2.
 



  



RELATED WORK

Exemplar-based Summarization. One approach of image summarization is to ﬁnd a set of exemplars that summarize the image
set (e.g., Bing Images). Raguram and Lazebnik [12] propose a
method that constructs a joint clustering on image descriptors and
tag topic vectors independently before obtaining their intersection.
Following that, a quality ranking learned from labeled images is
used to select iconic images. In [7], a set of exemplars is identiﬁed using a sparse Aﬃnity Propagation (ap) approach. Simon et
al. [15] formulate the scene summarization problem for selecting
a representative set of images of a given scene. A k-means-based
greedy method is proposed to compute clusters using visual features. The mostly likely tag associated with each cluster is then
determined using a probabilistic measure. Xu et al. [20] evaluates
visual and textual information jointly using a technique known as
homogeneous and heterogeneous message propagation to identify
exemplar images. The method extends the ap algorithm to support
heterogeneous messages from visual and textual feature spaces. In
contrast to prism, these approaches do not attempt to ensure that
all other images can be properly clustered by their exemplars (and
their tags) in a concept-preserving manner. Additionally, they do
not ensure that the exemplars maximally cover the image set.
Clustering-based Summarization. Clustering an image collection to ﬁnd blocks of similar images is another approach to address
the summarization problem. Several methods cluster images purely
based on the semantic concepts associated with the images, such as
tags [17, 19]. These methods, however, cannot assess and guarantee the visual coherence of the clustered images. Other methods
consider only the visual similarity among images [8].
Clustering of tagged social images by considering both visual
and textual features can be viewed as multi-modal clustering consisting of two types, namely early fusion and late fusion. In early
fusion, the modalities are combined and evaluated simultaneously.
Cai et al. [2] exploit a combination of visual, textual, and edge information of Web images to construct a relationship graph. Spectral clustering is then applied to obtain clusters of related images.
No attempt, however, is made to associate a concept with each cluster. Instead, surrounding texts around the cluster of images are
used to index the images. Heterogeneous clustering of visual, textual, and edge data is also studied by Li et al. [9]. Blaschko and
Lampert [1] introduce a correlational spectral clustering approach
on images with associated text. The technique is based on kernel
canonical correlation analysis that ﬁnds projections of the image
and text data. Rege et al. [13] propose a tripartite graph partitioning
framework on clustering Web images and text. The framework obtains partitions of correlated web images, textual information, and
visual features. Late fusion computes the clustering on each modality independently. These clusterings are then integrated to form the
ﬁnal multi-modal clustering. Moëllic et al. [10] propose a clustering method based on shared nearest neighbors. Unlike prism which

Figure 5: [Best viewed in color] Concept-preserving image
clusters generated by prism for the query “fly”.
and richer set of web text annotations, these search engines still
construct relatively limited variety of concepts. The concepts suggested by them are mostly restricted to insects. Clearly, they have
missed out other ﬂy-related concepts such as the act of jumping,
planes, helicopter, and birds.
In this paper, we propose a novel query-speciﬁc social image
search results summarization algorithm called prism4 (conceptPReserving social Image Search suMmarization) that constructs
high quality summary of image search results based on conceptpreserving and visually coherent clusters which maximally cover
the result set. Figures 2(a) and 5 depict subsets of clusters constructed by prism for the queries “fruit” and “ﬂy”, respectively. Each
cluster is represented by minimal tag(s) shared by all images in it.
Due to the concept-preserving nature, the images in a cluster form
an equivalence class with respect to the tags. Consequently, any
image in each cluster can be selected as an exemplar. For instance,
any image in the “pear” cluster can be chosen as an exemplar to represent it (e.g., ﬁrst three images are chosen in this example). Also
observe that in contrast to Google Images and Bing Images, prism
generates clusters representing wider variety of concepts related to
“ﬂy” (Figure 5) that maximally cover the result set.
Any query-speciﬁc image search results summarization presents
several non-trivial challenges. The set of images to be summarized is not predetermined. Hence, the summarization method cannot preprocess the underlying images apriori. Additionally, simply
leveraging traditional image clustering techniques may not generate high-quality summary due to the requirement that any summary
must be concept-preserving and cover as many images as possible
in the result set. Furthermore, it has to be robust to a wide variety of
queries and result sizes. To address these challenges, prism explores
the concept space (i.e., tag space) to seek for visually coherent cluster of images. Note that a single-dimensional exploration of the
concept space, however, may not yield visually related images. As
such, prism models the exploration of the visual-concept space using a graph model. Speciﬁcally, it ﬁrst constructs a visual similarity
graph G where the nodes are images in the search results and the
edges represent visual similarities between pairs of images. Then it
optimally decompose G into a set of concept-preserving subgraphs
4
A prism can be used to break a beam of light up into its constituent spectral colors
(the colors of the rainbow). Similarly, the prism algorithm breaks the result image set
into distinct image clusters.

739

R, such that the image set in G is union of all images in S and R.
Each CT i ∈ S can be represented by an exemplar node; the remainder subgraph R represents the region of G 
not covered by S (i.e.,
R is the subgraph induced by the set V \ CT ∈S VT ). For example, the visual similarity graph in Figure 6(a)(i) is decomposed into
{Csur f , Cbeach , Csea , Csun } and R where Csur f , Cbeach , Csea , and Csun are
represented by exemplar nodes “surf”, “beach”, “sea”, and “sun”, respectively, and R = {v15, v16}. Our decomposition allows overlap
among subgraphs in S (e.g., overlap between Cbeach and Csea ).
A keen reader may observe that there are numerous ways of decomposing G into S and R. However, not all decompositions result
in high quality summary. For instance, suppose we decompose G
into concept subgraphs {v1, v8, v11} and {v1, v14} represented by
exemplar nodes “nikon” and “boat”, respectively. Clearly, this decomposition poorly summarizes G because the images within each
subgraph have low visual similarities (e.g., subgraph {v1, v8, v11}
contains no edges) and only 4 out of 16 images are represented by
exemplar nodes. Hence, it is important to optimally decompose G
so that it can facilitate high quality summary construction.
Let E be the family of all concept subgraphs of G representing
all potential concept-preserving clusters. Obviously, E can easily
comprise of prohibitively large number of overlapping subgraphs;
rendering it impractical for summary construction. It is therefore
pertinent to identify a small subset of E that is suﬃcient to represent and summarize G. Hence, we want to ﬁnd a subset S ⊂ E that
optimally decomposes G based on some summarization objectives
from which a concise summary can be generated. Speciﬁcally, a
summary of G is the set of exemplars obtained from S by mapping
every concept subgraph CT ∈ S to its associated exemplar(s). In
Figure 6(a), the summary of the visual similarity graph is the set
of exemplars that represents the concepts “surf”, “beach”, “sea”, and
“sun”. The remainder subgraph R = {v15, v16} represents images
“missed” by the summary. We consider the following summarization objectives for optimal decomposition of G:

considers the modalities in tandem, it clusters images in a sequential manner–ﬁrst based on image tags, then on visual descriptors.
Generalized multi-modal clustering methods in most cases do
not associate each cluster with a tag concept for user interpretation and visualization. As such, one has to associate tag(s) to each
image cluster as a post-processing step. As remarked earlier, such
tag-image cluster associations rarely preserve concepts as opposed
to the tight tag-cluster integration attained by prism where all images in a cluster share the same concept(s). Lastly, unlike prism
these techniques do not seek to ﬁnd a concise set of images that
can maximally cover the entire result set.

3.
3.1

PROBLEM FORMULATION
Terminology

Given a search query Q = {q1 , q2 , . . . , qc } consisting of one or
more keywords (tags), suppose that a social image search engine
(e.g., Flickr) retrieves a list of result images D satisfying Q. By
abusing the notation of lists, let D = {i1 , i2 , . . . , in } and |D| = n.
Each image i ∈ D comprises of: (a) a d-dimensional visual feature
vector representing visual content of the image; and (b) a set of tags
Ti = {t1 , t2 , . . . , t|Ti | } associated with i. Note that Q ⊆ Ti .
The visual similarities among images in D is represented as a visual similarity graph G = (V, E, w), where V is the set of images in
D and E is a set of undirected edges between visually similar images. The function w : E →  assigns weight to each edge to indicate the degree of visual similarity between images. Figure 6(a)(i)
illustrates a visual similarity graph.
Given a set of tags T , a concept-preserving subgraph (concept
subgraph for brevity), denoted by CT = (VT , ET , T ), is a subgraph
of G induced by VT ⊆ V . Every image in the subgraph shares the set
of tags T , i.e., T ⊆ Ti ∀ i ∈ VT . We use concept subgraphs to model
a set of images that preserves a set of concepts represented by T . In
Figure 6(a)(i), the subgraph induced by the node set {v1, v2, v3} is
an example of a concept-preserving subgraph where T = {“surf”}.
Every image in the subgraph shares all concepts in T .
A concept subgraph in G can be concisely represented by an exemplar node labeled with T . Figure 6(a)(ii) depicts a set of exemplar nodes (represented by dashed circles) with labels “surf”,
“beach”, “sea”, and “sun”. These nodes represent the concept subgraphs induced by {v1, v2, v3}, {v8, v9, v10}, {v4, v5, v6, v7, v9}, and
{v11, v12, v13, v14}, respectively.

3.2

• Visual coherence. The visual coherence of S is deﬁned as:

1  e∈ET w(e)
 
(1)
coherence(S) =
|S|
E 
CT ∈S

T

The coherence(S) value reﬂects the average weight of visually
similar images in each CT ∈ S. Higher visual coherence means
the images are more visually similar to each other.
• Distinctiveness. Intuitively, a pair of exemplar nodes that represent two disjoint subgraphs is more informative that a pair
that represent identical subgraphs. Thus, a decomposition that
creates clean separation of concept subgraphs is desirable. We
quantify this objective with the distinctiveness measure. It measures concept subgraph redundancies, such that the greater the
redundancies, the lower the distinctiveness value. Formally, distinctiveness of S is deﬁned as:

 
 CT ∈S VT 
(2)
distinctiveness(S) = 
CT ∈S |VT |

Search Results Summarization Problem

We now formally deﬁne the problem of social image search results summarization. Intuitively, it can be formulated as the optimal decomposition of a visual similarity graph G into a set of concept subgraphs from which exemplar images are drawn to create
the summary. Let us elaborate on it with an example. Consider the
subgraph in Figure 6(a)(i) induced by the node set {v3, v4, v6, v9}
sharing no common concept and the concept subgraph induced by
{v1, v2, v3} sharing the concept T = {“surf”}. Notice that any image
represented by the exemplar node of {v1, v2, v3} (Figure 6(a)(ii))
can be selected as an exemplar summary for the “surf” images (Figure 6(a)(iii)). However, with no shared concepts in the node set
{v3, v4, v6, v9}, it is less obvious how the entire subgraph can be
represented with an exemplar image. Hence, if one can optimally
decompose G into concept subgraphs, then one can meaningfully
represent G with a concise set of exemplar nodes from which the
exemplar summary of the result set can be generated. This is the
key intuition behind summarization of G using concept subgraphs.
More speciﬁcally, a decomposition of G generates a set of concept subgraphs S = {CT 1 , CT 2 , . . . CT k } and a remainder subgraph

• Coverage. A set of concept subgraphs S that well represents G
is preferable. We use the notion of coverage to measure this.
Intuitively, it quantiﬁes how many images from the image set V
appears in S. Formally, it is deﬁned as:

 
 CT ∈S VT 
(3)
coverage(S) =
|V |
Note that coverage(S) is 1 if all images in V are selected in S.
As we shall see later, there is a trade-oﬀ between maximizing
coverage or distinctiveness.

740


 




'


 


'!

!
 
'

%



'!

 
"


'

'#

&




'#



#





GHSWK



GHSWK

+,

'#

'!
'!

'"

 



'"




 

'!

'

'"

'"



 

^VHD`

^IRRG`

^URFN`


 #


'



'"

GHSWK



 
 

'"

 !



$


'

'
'"







 

'

 "








(   )* 

(a) Summarization process.

GHSWK

^IRRGFKHHVH`
^IRRG

^VHDEHDFK`

^VHDEHDFKVXUI`

^VHDURFN`

^VHDEHDFKVDLO`

^URFNPXVLF`

^VHDURFNFOLII`

(b) Reﬁnement.

Figure 6: [Best viewed in color] Illustration of the social image search results summarization in prism.
Definition 1. Let Q be a search query on a social image database
and D be the set of search results. Given the visual similarity graph
G of D, the goal of the social image search results summarization problem is to ﬁnd an optimal set of concept subgraphs S such
that coherence(S), coverage(S) and distinctiveness(S) are maximized. Following that, the exemplar summary M is constructed
by selecting from each concept subgraph CT ∈ S an exemplar set
(comprising of 1 ≤ m ≤ 3 images in CT ) and its associated concept.

low). We now prove that an optimal solution of the problem is a set
of concept subgraphs S and at most a single remainder subgraph R,
and the remainder subgraph does not overlap with S.
Theorem 1. If the solution S0 ∪ R0 of the social image search
results summarization problem is optimal, then |R0 | ≤ 1.

Let us illustrate the problem deﬁnition with an example. Consider Figure 6(a) and two sets of concept subgraphs S1 = {{v1, v2, v3},
{v8, v9, v10}, {v4, v5, v6, v7}, {v11, v12, v13, v14}} and S2 = {{v1, v8,
v11}, {v1, v14}}. Observe that S2 has lower distinctiveness (while
every image belongs to at most one concept in S1 , several images
belong to two concepts in S2 ). The clusters of images in S2 also
have lower visual coherence (fewer edges within subgraphs). The
coverage of S2 is also lower than S1 . Hence, S1 is superior to S2 .
To solve the problem in Deﬁnition 1, we propose a weighted
minimum k-set cover optimization model [5]. It includes a cost
model that incurs a weight (i.e., cost) every time a subgraph is
added as concept subgraph or as remainder subgraph. For each
concept subgraph, we incur a visual incoherence cost, the inverse
of visual coherence of a concept subgraph, for choosing visually
incoherent images (maximize coherence(S)). For each remainder
subgraph, we incur a remainder penalty cost for choosing large remainder subgraphs (maximize coverage(S)). Given the cost model,
we ﬁnd the minimum weight (cost) of subgraphs needed to cover V ,
penalizing redundant subgraphs that add little to the summary since
every subgraph added incurs a cost (controlling distinctiveness(S)).
Definition 2. Given the visual similarity graph G of D, let E be
the family of all concept subgraphs of G and F be the family of all
subgraphs of G. Let k be the cardinality constraint. The optimal
S ∪ R, where S ⊂ E (set of concept subgraphs) and R ⊂ F (set of
remainder subgraphs), is the minimum cost set that covers V :


c(CT ) +
c(R)
(4)
arg min f (S ∪ R) = arg min
S∪R

S∪R





CT ∈S

R∈R

subject to V = CT ∈S VT ∪ VR ∈R VR and |S| + |R| ≤ k, where the
visual incoherence cost function c : E →  and the remainder
penalty cost function c : F →  are deﬁned as follows:
 
ET 
c(R) = (|VR | + 1) max c(CT )
c(CT ) = 
CT ∈E
e∈ET w(e)
Observe that that we model the scenario whereby having images
in a remainder subgraph will always incur higher penalty than representing them with a concept subgraph (even if visual coherence is

741

Proof.
Assume by contradiction that |R0 | > 1. R0 covers the

cost incurred by sets in R0 is |R0 | maxCT ∈E c(CT )
set R∈R0 VR . The 
+(maxCT ∈E c(CT )) R∈R0 |VR |. We show that we can replace R0 with
asingle remainder subgraph and incur a lower cost. Let R =
{ R∈R0 VR }. The singleton R covers the same set of vertices with
lower cost and lower set cover cardinality.
2. If S0 ∪ R0 is optimal, then the following holds:
 Theorem 
CT ∈S0 VT ∩
R∈R0 VR = ∅.


Proof. Assume
by contradiction
that CT ∈S0 VT ∩ R∈R0 VR  ∅.


Let R = { R∈R0 VR \ CT ∈S0 VT }. S0 ∪ R covers the same set of
vertices with lower cost incurred.
Since weighted k-set cover problem is NP-hard [5], in the next
section we present a greedy algorithm to address it.

4.

THE PRISM ALGORITHM

An algorithm that solves the aforementioned summarization problem must resolve two key issues: (a) a structure to allow eﬃcient
enumeration of concept subgraphs in E and (b) a method to efﬁciently ﬁnd an optimal subset of E and F that maximizes the
summarization objectives. The prism algorithm (Algorithm 1) is
designed to achieve these. It consists of ﬁve key phases: the visual
similarity graph construction phase (Line 1), the E-construction
phase (Line 2), the decomposition phase (Line 3), the summary
compression phase (Line 4), and the exemplar summary generation
phase (Lines 5-9). Given a search results D, a visual similarity
graph G is ﬁrst constructed. The E-construction phase then constructs the family of concept subgraphs of G. Subsequently, the
decomposition phase performs a combinatorial optimization to decompose G into a set of concept subgraphs S ∪ R based on the
three summarization objectives. Images in each subgraph represent a concept-preserving cluster (Recall from Section 1). Note
that state-of-the-art graph clustering techniques [1, 3, 13] cannot be
directly leveraged to identify these clusters as they do not preserve
concepts, typically generate non-overlapping clusters, and do not
maximally cover the entire graph. The summary compression process “compresses” S to form a summary at reduced level of detail
(denoted by V). The ﬁnal phase involves selection of one to three
exemplar images from each concept subgraph in V to form M.
Since the last phase is straightforward, we now proceed to elaborate on the ﬁrst four phases.

Algorithm 2: The E − Constructor algorithm.
Input: Visual similarity graph G.
Output: A set of concept-preserving subgraphs E∗ .

Algorithm 1: The prism algorithm
Input: User query Q, Set of images D, δ , k.
Output: Exemplar summary M.
1
2
3
4
5
6
7
8
9

G ← ConstructVisSimGraph(D, δ );
E∗ ← E − Constructor(G);
S ← Decompose(E∗ , k);
V ← Compress(S);
M ← ∅;
forall the CT = (VT , ET , w, T ) ∈ V do
select m images in VT as exemplar;
associate m images with tag T ;
M ← M ∪ (m, T );

1
2
3
4
5
6
7
8
9

10 return M;

(V, E, w) ← G;
i ← 0;
Vi ← {CT0 = (V, E, ∅)};
E∗ ← ∅;
while Vi is not empty do
Vi+1 ← ∅;
forall the CT ∈ Vi do
Ri+1 ← reﬁnements of CT ;
Vi+1 ← Vi+1 ∪ Ri+1 ;

10
11

E∗ ← E∗ ∪ Vi+1 ;
i ← i + 1;

12 return E∗ ;

4.1

Visual Similarity Graph Construction Phase

Since the visual similarity graph G is query-dependent, it needs
to be constructed on-the-ﬂy. To this end, we adopt cosine similarity to measure the visual similarity between any two images as
follows: Sim = L−1/2 AT AL−1/2 where A is the n × d matrix of image set visual features, AT A encodes the inner-product of the image
feature vectors, and L−1/2 is a n × n diagonal matrix that encodes
normalization of each feature vector. Given the similarity matrix,
we construct the visual similarity graph G as follows. Let V be the
set of images. We add an edge in E between two images i and j if
Simi j > δ . The weight of this edge is Simi j and the edge density
parameter δ is user-deﬁned, controlling the edge density of G.

similar way until V d = ∅. Algorithm 2 outlines the construction of
of the dag be d. Then the worst case
E∗ . Let the maximum depth
  
time complexity is O( di mi ) where m = | ∪i∈V Ti | and at any depth
 
i > 0, one can construct up to mi concept subgraphs. Note that
despite its exponential complexity, as we shall see later, in practice this phase completes quickly as users are typically interested
in summary of the top-n (e.g., n < 2000) results instead of the
entire result set.

4.3

Decomposition Phase

In this phase, we ﬁnd a subset S ⊂ E∗ and R ⊂ F that optimally
decomposes G. Recall from Deﬁnition
 2 our goal of ﬁnding the
subset S∪R ⊂ E∗ ∪F that minimizes CT ∈S c(CT )+ R∈R c(R) subject to vertex cover and cardinality constraints (Equation 4). Due
to its computational hardness,
we adopt a Hk -approximation greedy

the greedy
algorithm, where Hk = ki=1 1i [5]. Algorithm
 3 outlines 
strategy of selecting S ∪ R to minimize CT ∈S c(CT ) + R∈R c(R).
The basic idea is to select, at each iteration, X ∈ E∗ ∪F (X is either a
concept subgraph or a remainder subgraph) so that X has the lowest
c(X)/n cost incurred, where n is the number of new vertices covered by X (Lines 5-11). Intuitively, we pay c(X) to cover an extra
n vertices, and the subgraph with lowest c(X)/n contributes maximum value by having the lowest cost per vertex coverage gained.
Recall from Theorems 1 and 2 that there should be at most one
remainder subgraph that is disjoint with S. Since c(X)/n of a remainder subgraph is always larger than c(X)/n of a concept subgraph, the greedy algorithm will always add concept subgraphs
before remainder subgraphs, as long as there is gain in coverage.
Therefore, CT is always added until k concept subgraphs have been
selected. Then R is the ﬁnal remainder subgraph induced by the
unselected images, which incurs a cost c(R). Notice that each iteration involves a single pass through the subgraphs in E∗ . With a total
of k iterations, the algorithm involves processing k|E∗ | subgraphs,
which in the worst case evaluates O(k|E∗ ||V |) images.

4.2

E-Construction Phase
Recall that it is unrealistic to exhaustively explore E. Hence,
we propose a method that explores E selectively using a directed
acyclic graph (dag) exploration model (dag model for brevity). The
main objective is to provide an exploration structure for enumerating concept subgraphs. We denote this exploration by E∗ .
We ﬁrst outline the construction of the dag model. With exception of the root node, every node in the dag represents a concept
subgraph. Let CT0 be the root node of the dag at depth d = 0, where
CT0 = (V, E, ∅) represents G with no shared concepts (i.e., not a
concept subgraph). Given CTd , we construct CTd+1 as follows. For
each CTi in CTd , a reﬁnement of CTi is a concept subgraph CTi+1 =
(VTi+1 , ETi+1 , T i+1 ) that satisﬁes the following:
1. T i+1 is T i and one additional concept t , i.e., T i+1 = T i ∪ t
2. VTi+1 is the set of all images in VTi sharing T i+1 and VTi+1  VTi
3. CTi+1 induced by VTi+1 has at least one edge (at least a pair of
images are visually similar)
For example, consider the dag model in Figure 6(b) where each
node represents a concept subgraph (labeled with the shared concept T for brevity). The {sea, beach} node is a reﬁnement of {sea}.
Similarly, {sea, beach, sur f } node is a reﬁnement of {sea, beach}.
Observe that a reﬁnement CTi+1 represents images that share one
more concept than in CTi (by ﬁrst criteria). In fact, each subgraph at
depth d contain images that share d concepts. Also, it is always a
proper subgraph of CTi so that there are no redundant subgraphs (by
second criteria). We ignore any CTi+1 without an edge because it has
no potential to form visually coherent images (by third criteria).
Intuitively, the reﬁnements as subgraphs of CTi represent ﬁnergrained concepts. At each depth d, we construct ﬁner-grained reﬁnements of its parent graphs. Hence, starting with CT0 , we build
a hierarchy of reﬁnements to form the dag model. We recursively
identify the next set of reﬁnements of the dag at d = 1, 2, 3, . . . in

4.4

Summary Compression Phase

The preceding phase ﬁnds an optimal collection of conceptpreserving clusters without constraining each cluster size. This is
beneﬁcial as it enables us to select the “best” combination of clusters with highest visual coherence. On the other hand, there is a lack
of control over the summary granularity if each concept subgraph
in the constructed S is used for creating the exemplar summary as
S may contain too ﬁnely-grained clusters for presentation to users.
We assume that a user expects a summary at a particular summary

742

granularity. For instance, if a user wants a broad overview of the
search result, then a summary of 5 exemplars may be preferable to
a summary of 50 exemplars. On the other hand, if a user prefers a
detailed summary, then the summary with 50 exemplars is better.
At ﬁrst glance it may seem that one may adjust the parameter k to
achieve the desired summary granularity. However, as we shall see
later, k signiﬁcantly aﬀects the coverage and distinctiveness of the
summary. Hence an alternative approach that can modify the summary granularity without aﬀecting coverage and distinctiveness is
desirable. In this phase, we address this issue by building multiple
summaries at varying summary granularity by aggregating concept
subgraphs. Speciﬁcally, a compressed concept subgraph set Si is
formed by aggregating concept subgraphs in S to form another set
of subgraphs of lower summary granularity. For example, assume
that S contains two subgraphs with T 1 = {boat, sail, rock} and
T 2 = {rock, cli f f }. Then these two subgraphs can be aggregated
into a larger subgraph sharing the {rock} concept. Consequently, it
compresses two concept subgraphs into a single subgraph.
We introduce a multilevel compression scheme that aggregates
concept subgraphs iteratively. Given the initial S, we construct a
list of concept subgraph set with increasingly smaller size. Formally, we construct a list [S, S1 , S2 , . . . , Sd ] such that ∀i, j, |Si | >
|S j | if i < j. We call each Si a compressed concept subgraph
set of S. Each successive set Si+1 is a compressed representation
of its predecessors. Observe that if a user wants a detailed summary of the search result, then S is most appropriate for generating
exemplar summaries. If a broader overview is desired, then a compressed set provides more concise view of the result set. In prism,
by default we use Sd to create the exemplar summary. If desired,
the user may drill into more detailed summaries.
We now elaborate on the construction of Si+1 from Si . Given
i
S , the successor Si+1 is constructed by contracting pairs of concept subgraphs. The contraction of pairs CT 1 and CT 2 removes both
subgraphs from the set and replaces them with CT 1 ∪T 2 = (VT 1 ∪
VT 2 , ET 1 ∪ ET 2 ). Figures 7(a)-(b) illustrate the contraction of two
concept subgraphs with T 1 = {sea, sur f , hawaii} and T 2 = {sea,
sur f , nikon} into a subgraph with T 3 = {sea, sur f }. Through successive subgraph pair contractions, we obtain increasingly compressed concept subgraph set.
How do we determine which pairs of concept subgraphs in S to
contract? Intuitively, one prefers to contract conceptually similar
subgraphs while keeping conceptually distinct subgraphs uncontracted. Given CT 1 ∈ Si and CT 2 ∈ Si , we say that CT 1 and CT 2 is
coupled if all images in VT 1 ∪VT 2 share a non-empty set of concepts
(i.e., all images have at least one common concept). Only coupled
concept subgraphs can be contracted; if not, Si+1 may contain subgraphs that violate the concept preservation property of concept
subgraphs. We can represent these couplings in Si using a coupling graph. A coupling graph of Si is a graph Gic = (Si , Eci ) where
each node is a concept subgraph. We add an edge in Gc between
CT 1 ∈ Si and CT 2 ∈ Si iﬀ CT 1 and CT 2 is coupled (thus valid candidate for contraction). Each edge is weighted, indicating the degree
of coupling between the coupled subgraphs. Here ω12 is the coupling weight
of the edge between CT 1 and CT 2 and is deﬁned as:

ω12 = t∈T OR(Q, t) where T is the set of concepts shared among
all images in VT 1 ∪ VT 2 and OR(t, Q) is the relevance of a tag t to
query Q using odds ratio:
OR(t, Q) = max
q∈Q

Pr(q, t)Pr(qc , t c )
Pr(qc , t)Pr(q, t c )

Algorithm 3: The Decompose algorithm.
Input: E∗ , k.
Output: A set of concept-preserving subgraph S
1 S ← ∅;
2 repeat
3
mincost ← ∞;
4
bestcluster ← ∅;
5
forall the CT ∈ E∗ \ S do

6
n ← VT \ C∈S V ;
7
f ← c(CT )/n;
8
if f < mincost and n > 0 then
9
mincost ← f ;
10
bestcluster ← {CT };
11
S ← S ∪ bestcluster;
12 until |S| > k;
13 return S;
^VHDVXUIQLNRQ`

^QLNRQERDW`

'%



^ERDWVDLOURFN`

^VHDVXUI`

^QLNRQERDW`

^ERDWVDLOURFN`
'

'
'%

!'

'%

!'#

!'

!'#



"'
^VHDVXUIKDZDLL`

^QLNRQURFNFOLII`

^URFNFOLII`

D 6
^VHDVXUI`

^QLNRQERDW`

^ERDWVDLOURFN`

E 6



^VHDVXUI`

^QLNRQURFNFOLII`

^QLNRQERDW`

^URFNFOLII`

^URFN`

'



F 6

^URFNFOLII`

G 6

Figure 7: Summary compression phase. For clarity, we depict
a node representing a concept subgraph by its images only.
ability values. Given tags q and t, let Iq and It be the sets of images
having tags q and t, respectively. The co-frequency between q and
t is simply |Iq ∩ It | and Pr(q, t) = |Iq ∩ It |/|V |. Observe that the
coupling weight depends on concept relevance to the user query as
well as number of shared concepts. Figure 7(a) is an example of a
coupling graph.
Algorithm 4 outlines the summary compression phase. We describe it using the example in Figure 7(a). To select pairs of subgraphs for contraction, we employ the following contraction scheme.
(a) For each Si , choose the highest weighted edge in the coupling
graph and contract the nodes of this edge (Lines 5-12). This results in compression of Si to Si+1 (Lines 13-15). (b) Repeat the
process for the next Si until its coupling graph has no edges (Lines
4-16). Figure 7 shows an example of this scheme. Notice that each
iteration evaluates the pairwise concept subgraphs in |S|. Thus, every iteration evaluates |S|2 subgraphs. If we assume the worst case
which merges all concept subgraphs until |S| = 1, then this phase
evaluates O(|S|3 ) concept subgraphs.

5.

EXPERIMENTS

prism is implemented in Java 1.7. In this section, we present the
performance of prism. All experiments were executed on a Intel
Core 2 Duo Linux machine with 4GB memory.

(5)

5.1

In the above equation, Pr(x, y) is the probability of co-occurrence
of events x and y and xc denotes the event of x not occurring. We
utilize the co-frequency of the relevant tags to determine the prob-

Experimental Setup

All experiments were conducted on the nus-wide dataset [4] containing 269,648 Flickr images with visual features, tags and human-

743

5

Algorithm 4: The Compress Algorithm.
Input: Set of concept subgraphs S
Output: Compressed set of concept subgraphs Sc

4

3
rating

rating
2

2

1

1

0

0
Bing

Google

PR

AP

CV

Bing

HY

Google

PR

AP

CV

HY

(b) Multi-tag queries

(a) Single-tag queries

Figure 8: User study.
Evaluation criteria. In addition to the coverage and distinctiveness measures outlined earlier, we introduce the mean weighted
global clustering coeﬃcient [11] to quantitatively measure the visual coherence of a summary S. We deﬁne the visual cohesiveness
score of a summary, denoted by VCS, as follows:
 
1  TΔ e∈TΔ w(e)
 
(6)
VCS(S) =
|S| C ∈S T e∈T w(e)

11
{CT 1 , CT 2 } ← best pair;
12
Si+1 ← Si \ {CT 1 , CT 2 };
13
Si+1 ← Si+1 ∪ CT 1 ∪T 2 ;
14
i ← i + 1;
15 until bestscore = 0;
16 Sc ← Si+1 ;
17 return S;

T

Table 1: Representative queries.

Multi-tag

VisualAppeal
Relevance
Comprehensiveness
Organization

4

3

1 i ← 0;
2 Si ← S;
3 repeat
4
bestscore ← 0;
5
best pair ← ∅;
6
forall the CT 1 ∈ Si do
7
forall the CT 2 ∈ Si st. CT 1  CT 2 do
8
if ω(CT 1 , CT 2 ) > bestscore then
9
bestscore ← ω(CT 1 , CT 2 );
10
best pair ← {CT 1 , CT 2 };

Type
Single-tag

5

VisualAppeal
Relevance
Comprehensiveness
Organization

Queries
asia (1.5), party (1.2), wedding (1.9), animals (1.4), art (1.3),
city (1.5), rock (1.5), food (1.5), sun (1.4), sea (1.4), sky(1.7)
nature (1.8), church (1.3), street (1.2), macro (1.7), bird (1.5),
[sun, sea] (1.5), [sun, silhouette] (1.7), [blue, sea] (2.3)
[street, art] (1.7), [sea, rock] (2.1), [blue, sky] (2.4),
[rock, music] (2.2), [macro, insect] (2.7), [city, lights] (1.4),
[ﬂower, macro] (1.6), [cute, animals] (1.9), [red, food] (2.7),
[graﬃti, art] (2.3), [birthday, party] (1.2)

where wis the
 visual similarity weight function of CT , the numerator
TΔ
e∈TΔ w(e) sums over all closed triplets TΔ in CT , and
 
T
e∈T w(e) sums over all triplets T in CT [11].
To measure how well a concept is preserved in a cluster, we introduce the concept preservation metric. Given a summary S, concept
preservation of S is deﬁned as:
ConceptPreservation(S) =

1  maxt |{t : t ∈ Ti , i ∈ VT }|
(7)
|S| C ∈S
|VT |
T

where ConceptPreservation(S) ∈ (0, 1] and its value is 1 if for
each cluster, every image shares at least a concept tag.

5.2

Comparison with the State-of-the-art

assigned labels in 81 categories. We use this dataset instead of
original Flickr images due to the following reasons. First, the 81
human-assigned categories available in this dataset enable us to undertake quantitative evaluation of prism. Second, since users typically browse only top-n search results, it is reasonable to summarize only these results using prism. Consequently, the impact of
dataset size on the summarization technique diminishes as the cost
of retrieving these top-n results is orthogonal to prism.
As search results summarization is query-dependent, we selected
30 representative queries for our study. Since information related
to most frequent queries on Flickr is not publicly available, we use
a subset of frequent tags in Flickr5 as a proxy for single-tag queries.
Multi-tag queries are formed by adding tags to single-tag queries.
Table 1 lists these queries (ignore for the time being the numeric
values in parenthesis). For each query we selected up to 1000 topranked images (|D| = n = 1000) from its search results to form
its result set. Note that the query tag is ignored in the summarization process for all experiments to avoid bias due to the tag. All
search results are obtained using a TagIR system following the best
performing conﬁguration in [16] on nus-wide data collection.
Recall that the ﬁrst step of prism is to construct a visual similarity
graph. For this purpose, we used all 6 types of low-level visual
features provided by nus-wide dataset: 1) 64-D color histogram,
2) 144-D color correlogram, 3) 73-D edge direction histogram, 4)
128-D wavelet texture, 5) 225-D block-wise color moments, and 6)
500-D bag of words based on sift descriptions. Unless speciﬁed
otherwise, we set k = 150 and δ = 0.05.

We compare prism (denoted by pr) with three representative summarization and clustering techniques: Canonical View Summarization (cv) [15], Aﬃnity Propagation (ap) [6] and H2 MP (hy) [20]. ap
and cv utilize only the visual features of the images in the summarization (or clustering) process. Tags are used in the post-processing
to annotate the resultant clusters. The hy method utilizes both the
visual and textual features of images. All tested methods share the
same visual similarity matrix, and also share the same concept similarity matrix of tag co-occurrences. Where possible, the default
parameters for each method were used. Otherwise, the parameters were adjusted to obtain reasonable results empirically and then
remain ﬁxed for multiple test sets. In addition, we qualitatively
compare prism to visual summaries constructed by Google Images
(Image Categories) and Bing Images (Related Topics).
User study. We ﬁrst qualitatively evaluate the summarization
results produced by the six approaches through a user study. We
invited 12 unpaid volunteers (undergraduate and graduate students
in computer science and business majors) to rate quality of the summaries. Nine of them had the experience of using image search engines. The remaining subjects are unfamiliar with image search. To
avoid any bias on the evaluation, all the participants were selected
such that they did not have any knowledge about the summarization technique deployed in prism6 . Summaries generated by the algorithms are presented as a set of exemplars but without the names
of the speciﬁc algorithms producing the summaries. For all methods, each exemplar is visually represented by three most relevant
images and one or more concepts (e.g., exemplars in Figure 2). For
Google Images and Bing Images, the visual summary sections are

5

6

Available at www.flickr.com/photos/tags/.

744

None of the volunteers are authors of this paper.

QC4
0.594
0.638
0.627
0.956

QC6
0.478
0.543
0.520
0.911

QC8
0.412
0.474
0.450
0.930

VisualAppeal
Relevance
Comprehensiveness
Organization

5
4

1

rating

QC2
0.854
0.855
0.867
0.955

1.2

score

Algorithm
AP
CV
HY
PR

6

VisualCoherence
Coverage
Distinctiveness
ConceptPreservation

1.4

Table 2: Separating power of the algorithms.

0.8

3

0.6

2
0.4

1

0.2

0

0
PRISM

AP

CV

HY

(a) Evaluation metrics

presented. Each participant was given one query at a time in random order (all 30 queries). They were allowed to take a break to
refresh themselves if they feel tired during the evaluation process.
From the 30 queries in Table 1, a participant rates the quality of
the summaries based on the following four questions.

0

25

50

75

100

(b) Summary compression

Figure 9: Performance results.
maries. For hy, the Aﬃnity Propagation-based method lends naturally to cluster construction by assigning each image to its exemplar. Likewise, for the cv approach, clusters are constructed by
assigning each image to its closest canonical view. The label of a
cluster in a summary is assigned based on majority voting. For instance, if a cluster contains 70% insect images and 30% sports
images, then it assumes the insect class through majority vote,
and the sports images are deemed mismatched.
Table 2 shows for each mixture set the fraction of images with
matched assignments. For every mixture set, pr has the best separating power. This shows the merit of preserving concepts and
selecting strong visual clusters during the clustering process.
Comparison by evaluation metrics. Here, we aim to evaluate summarization methods based on the following evaluation metrics: visual coherence (VCS), coverage, distinctiveness and concept
preservation. Figure 9(a) shows the scores of the summaries generated using the tested queries. The results indicate that ap and cv
has superior VCS, coverage and distinctiveness compared to our
method. This is unsurprising given that these methods are unconstrained by concept and cluster images purely on their visual similarities. Furthermore, they construct a partition on G, thus their
perfect coverage and distinctiveness scores. However, this comes
at a cost of low concept preservation scores, implying that association between a concept and a cluster is weak. On the other hand,
the hy method has better concept preservation score, although in
this case both VCS and concept preservation scores are inferior to
pr. In summary, prism achieves the best balance of maintaining
concept preservation and visual coherence of a summary. Using
pr, the p-value in t-test against each method/metric is < 0.0001.

QT1 : Is the summary visually appealing? (visual appeal)
QT2 : Are the exemplar summaries relevant to the query? (relevance)
QT3 : Is the summary comprehensive? (comprehensiveness)
QT4 : Is the summary well organized? Is it easy to understand at a
glance? (organization)
For each question, a participant rates the summary using a Likert
scale, from 1 for most unsatisfactory to 5 for most satisfactory.
Figure 8(a) shows the results of the user study for single-tag
queries. The rating for each question-algorithm pair is the average rating from multiple queries. The results clearly demonstrate
the superiority of prism for QT1 -QT4 (p-value in t-test is < 0.05 for
each method) justifying the importance of concept preservation in
order to obtain precise clusters. Figure 8(b) reports the results for
multi-tag queries. We observe similar results for prism having the
highest rating for visual appeal, relevance, and organization.
Notice that Google, Bing and prism summaries are perceived
to be signiﬁcantly better organized than other summarization approaches. We argue that this justiﬁes the usefulness of having concept preserving summary with sparse tag exemplars. Figure 2 illustrates how exemplars with minimal tags are easier to interpret. We
also observe that ap has the lowest relevance rating as it is likely
to prioritize visually similar images over conceptually relevant images. Hybrid methods like prism and hy beneﬁt from exploiting a
richer set of heterogeneous data to guide the summarization process – visual features provide visual relationships between images,
while textual/concept features provide semantic relationships. Notice the lower relevance rating for Bing compared to Google and
prism. Upon closer inspection, we found that the relevance ratings
for Bing summaries vary widely across diﬀerent queries. Meanwhile, Google suﬀers from having lowest comprehensiveness because all query summaries only have up to ﬁve exemplars.
Separating power. Human evaluation is mostly limited by the
scale of the evaluation. In this set of experiments, we evaluate
the separating power of the algorithms using the nus-wide dataset.
More speciﬁcally, we combine the result sets of two or more queries
to form a mixture set and then evaluate the eﬀectiveness of a method
in separating these images. To construct a mixture set of N queries
(denoted by QCN ), an equal number of images are retrieved from
N out of 81 ground-truth concepts. The ground-truth concepts selected are randomly determined, and for each test, we repeat with
10 random combinations of N concepts and obtain the mean score
for the test. Every mixture set comprises 1000 images from the corresponding N query result sets (i.e., each query in mixture set has
1000/N images). The combined images are then summarized using
the summarization algorithms. We assume that a superior summary
will partition the mixed images into their underlying query result
sets with high accuracy.
As we are comparing clusterings in this evaluation, we perform
the following post-processing for methods that construct only sum-

5.3

Analysis of PRISM

Eﬀects of k. The parameter k controls the number of concept
subgraphs of G in the decomposition. Figure 10(a) shows the effect of k on summary coverage with diﬀerent result set sizes for
all queries. Observe that the coverage of summaries increases with
increasing k. At the same time, from Figure 10(b), the distinctiveness of summaries reduces with k. Figures 10(c) and (d), on the
other hand, show that VCS reduces with increasing k values, while
running time remain largely unaﬀected by k.
The results show that k controls the trade-oﬀ between summary
distinctiveness and coverage. Unlike clustering methods that form
a clustering that partitions the image set (e.g., ap and cv), the need
for concept preserving clusters imply that not all summaries constructed using our approach can achieve perfect uniqueness (distinctiveness) or representativeness (coverage). Often, to achieve
maximum coverage, a certain amount of redundancies have to be
allowed for, by creating overlapping concept-preserving clusters.
Likewise, to achieve maximum distinctiveness, some images may
have to be omitted because they could not be represented as nonoverlapping concept-preserving clusters.
Eﬀects of summary compression. Next, we study the importance of the summary compression phase using a user study. For

745

1
0.8

distinctiveness

coverage

1
0.8
0.6
0.4
0.2
0

dard deviation among diﬀerent queries. The running time of prism
scales relatively well with result size. Generally for top-1000 images, summarization can be completed in less than 3 seconds including construction of the visual similarity graph.

0.6
0.4
0.2
0

0

50 100 150 200 250 300 350 400

0

50 100 150 200 250 300 350 400

k

(b) Distinctiveness

1

5

0.8

4
time (s)

visual coherence

6.

k

(a) Coverage

0.6
0.4
0.2

The quest for high quality social image search results visualization has become more pressing due to explosive growth of social
image sharing platforms and search engines. In this paper, we have
introduced three desirable features of a good social image search
results summary, namely, concept-preservation, visual coherence,
and coverage. We present a novel algorithm called prism which
meets these desirable features. Speciﬁcally, prism utilizes both visual and concept features to construct a concept-preserving summary by reﬁning, selecting, and compressing concept subgraphs.
Based on this, an exemplar summary is easily created by selecting one or more exemplar image from each concept subgraph. Our
empirical study demonstrated that prism produces superior quality
summaries compared to state-of-the-art summarization techniques.

3
2
1

0

0
0

50 100 150 200 250 300 350 400
k

0

50 100 150 200 250 300 350 400
k

1

1
0.8

distinctiveness

coverage

(c) VCS
(d) Running Time
Figure 10: Eﬀect of k.
0.8
0.6
0.4
0.2
0

0.6
0.4
0.2

500

1000

1500

2000

0

500

size

1

time (s)

0.6
0.4
0.2
0
500

1000
size

1500

2000

(b) Distinctiveness

0.8

0

1000
size

(a) Coverage
visual coherence

Acknowledgements: Boon-Siew Seah and Aixin Sun were supported by Singapore MOE AcRF Tier-1 Grant RG13/10.

0
0

1500

2000

7.

9
8
7
6
5
4
3
2
1
0
0

500

1000

1500

CONCLUSIONS

REFERENCES

[1] M. B. Blaschko and C. H. Lampert. Correlational spectral clustering.
In IEEE CVPR, 2008.
[2] D. Cai, et al. Hierarchical clustering of WWW image search results
using visual, textual and link information. In ACM MM, 2004.
[3] H. Cheng, et al. Clustering large attributed information networks: an
eﬃcient incremental computing approach. Data Mining and
Knowledge Discovery, 25(3), March 2012.
[4] T-S. Chua, et al. NUS-WIDE: a real-world web image database from
National University of Singapore. In ACM CIVR, 2009.
[5] V. Chvatal. A Greedy Heuristic for the Set-Covering Problem.
Mathematics of Operations Research, 4(3):233–235, 1979.
[6] B. J Frey and D. Dueck. Clustering by passing messages between
data points. Science, 315(5814):972–976, 2007.
[7] Y. Jia, et al. Finding image exemplars using fast sparse aﬃnity
propagation. In ACM MM, 2008.
[8] Y. Jing and S. Baluja. VisualRank: applying PageRank to large-scale
image search. IEEE PAMI, 30(11):1877–90, November 2008.
[9] Z. Li, et al. Grouping WWW Image Search Results by Novel
Inhomogeneous Clustering Method. In Proc. IEEE International
Multimedia Modelling Conference, 2005.
[10] P.-A. Moëllic, et al. Image clustering based on a shared nearest
neighbors approach for tagged collections. In ACM CIVR, 2008.
[11] T. Opsahl and P. Panzarasa. Clustering in weighted networks. Social
Networks, 31(2):155–163, 2009.
[12] R. Raguram and S. Lazebnik. Computing iconic summaries of
general visual concepts. In CVPR Workshops, 2008.
[13] M. Rege, M. Dong, and J. Hua. Graph theoretical framework for
simultaneously integrating visual and textual features for eﬃcient
web image clustering. In ACM WWW, 2008.
[14] B. S. Seah, et al. Summarizing Social Image Search Results. In ACM
WWW, 2014.
[15] I. Simon, et al. Scene Summarization for Online Image Collections.
In IEEE ICCV, 2007.
[16] A. Sun, S. S. Bhowmick, et al. Tag-based social image retrieval: An
empirical evaluation, JASIST, 62(12), 2011.
[17] B. Q. Truong, et al. CASIS: a system for concept-aware social image
search. In ACM WWW, 2012.
[18] R. H. van Leuken, et al. Visual diversiﬁcation of image search
results. In ACM WWW, 2009.
[19] S. Wang, et al. IGroup: presenting web image search results in
semantic clusters. In ACM CHI, 2007.
[20] H. Xu, et al. Hybrid image summarization. In ACM MM, 2011.

2000

size

(c) VCS
(d) Running Time
Figure 11: Robustness of prism (at k = 250).
each summary, we performed 0% to 100% summary compression
and evaluated the quality of each summary. We say that 100% compression is achieved when the summary cannot be compressed further. If we assume that the number of iterations needed to achieve
100% compression is n, then the m% percent compression is simply the summary after mn/100 compression iterations. Table 1
shows the compression ratio (computed as |S0 |/|Sn |) achieved for
each tested query at 100% compression (numeric values associated
with each query). Observe that our summary compression phase
reduces the set of concept subgraphs for every query (up to a factor
of 2.7). Next, for each query, assessors are presented a set of summaries with varying summary compression from 0% to 100% and
requested to evaluated the visual appeal, relevance, comprehensiveness and organization quality of the summaries. Figure 9(b) reports
the results. We observe that summary compression increases the
perceived relevance of the summary. Summary is also seen as being better organized and more visually appealing. Similar to the
eﬀects of exemplar tag sparsity, summary compression reduces the
complexity of the summary to create a more interpretable visual
landscape of the query images. However, this comes at a cost of
reduced perception of summary comprehensiveness. Nevertheless,
the beneﬁts gained on three other summarization qualities outweigh
this loss of comprehensiveness.
Robustness of prism. We now investigate the robustness of prism
to varying queries and result set sizes. We set k = 250 and study
the distinctiveness, coverage and visual coherence of summaries for
diﬀerent queries and result sizes. Figures 11(a)-(c) show the results
of the study. We observe that the error bars are small enough to
justify that the summary quality is robust for varying result sizes.
Running time. Lastly, Figure 11(d) plots the running time of
prism at varying result sizes. The error bars represent the stan-

746

