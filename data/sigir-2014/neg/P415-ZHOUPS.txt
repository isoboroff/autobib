Latent Semantic Sparse Hashing for Cross-Modal
Similarity Search
Jile Zhou

School of Software
Tsinghua University
Beijing, China

zhoujile539@gmail.com

Guiguang Ding

School of Software
Tsinghua University
Beijing, China

dinggg@tsinghua.edu.cn

Yuchen Guo

School of Software
Tsinghua University
Beijing, China

yuchen.w.guo@gmail.com

ABSTRACT

General Terms

Similarity search methods based on hashing for eﬀective
and eﬃcient cross-modal retrieval on large-scale multimedia databases with massive text and images have attracted
considerable attention. The core problem of cross-modal
hashing is how to eﬀectively construct correlation between
multi-modal representations which are heterogeneous intrinsically in the process of hash function learning. Analogous to
Canonical Correlation Analysis (CCA), most existing crossmodal hash methods embed the heterogeneous data into a
joint abstraction space by linear projections. However, these
methods fail to bridge the semantic gap more eﬀectively, and
capture high-level latent semantic information which has
been proved that it can lead to better performance for image retrieval. To address these challenges, in this paper, we
propose a novel Latent Semantic Sparse Hashing (LSSH) to
perform cross-modal similarity search by employing Sparse
Coding and Matrix Factorization. In particular, LSSH uses
Sparse Coding to capture the salient structures of images,
and Matrix Factorization to learn the latent concepts from
text. Then the learned latent semantic features are mapped
to a joint abstraction space. Moreover, an iterative strategy
is applied to derive optimal solutions eﬃciently, and it helps
LSSH to explore the correlation between multi-modal representations eﬃciently and automatically. Finally, the uniﬁed
hashcodes are generated through the high level abstraction
space by quantization. Extensive experiments on three different datasets highlight the advantage of our method under
cross-modal scenarios and show that LSSH signiﬁcantly outperforms several state-of-the-art methods.

Algorithms; Performance; Experimentation

Keywords
Hashing; Cross-Modal Retrieval; Heterogeneous Data Sources;
Correlation; Sparse Coding; Matrix Factorization

1. INTRODUCTION
Similarity or Nearest Neighbor (NN) search, a method of
searching semantically related results from a collection of
objects for a query, lays the foundation for many important
applications, such as information retrieval, data mining, and
computer vision. Hashing-based methods [10, 6], one of
the most well-known Approximate Nearest Neighbor search
(ANN) methods, has garnered considerable interest in recent
years for their great eﬃciency gains in massive data. The
goal of hashing is to learn binary-code representation for
data while preserving the similarity structure in the original
feature space. One of the most famous models, localitysensitive hashing (LSH) [1], which employs random linear
projections to map feature vectors to binary codes, is quite
eﬃcient in both space and time. However, LSH may lead to
ineﬀective codes in practice because it is data-independent
[34]. Several machine learning techniques are used to design
more eﬀective hashing to overcome this problem, such as
Boosting algorithm, Restricted Boltzmann Machines, Manifold Learning, Supervised Learning, Kernel Learning and
PCA, which respectively generate Parameter sensitive Hashing [26], Semantic Hashing [25], Spectral Hashing [30], Supervised Hashing [16], Kernelized Hashing [13] and PCA
Hashing [29]. Moreover, several literatures take the quantization of Hamming space into account, and have achieved
superior results, such as K-means Hashing [8], ITQ Hashing
[7] and Double-Bit Hashing [12].
Most existing hashing methods can only be applied to
unimodal data. However, with the fast growth of multimedia
content on the Web, like Wikipedia, Flickr and Twitter, the
cross-modal retrieval problem, returning similar results of all
modals for a given query, have attracted increasing attention
and more studies about it emerge. Taking Wikipedia as
example, it contains images and text. When a query word
or picture is given, the system should return both relevant
articles and images. This is central to many applications
of practical interest [23]. However, designing eﬀective and
eﬃcient hashing methods over heterogeneous cross-modal
datasets is still remaining as an open issue.

Categories and Subject Descriptors
H.3.1 [Information Storage and Retrieval]: Content
Analysis and Indexing; H.3.3 [Information Storage and
Retrieval]: Information Search and Retrieval

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00.
.

415

L in e

ar Pr
o je c ti

on

Linear Projection

Figure 1: The diﬀerence between the proposed LSSH and existing cross-modal hashing, illustrated with toy
data. Up) LSSH maps the text and images from their respective natural spaces to two isomorphic latent
semantic spaces ﬁrstly, then projects the semantic spaces to a joint high level abstraction space. The latent
semantic spaces are learned using sparse coding and matrix factorization respectively. Bottom) Existing
cross-view models map the text and images to a joint low level abstraction space directly. At last, the
learned abstraction space is quantized to the Hamming space in all hashing methods.
tion space by quantization. The contributions of LSSH can
be summarized as follows:

The core problem of cross-modal hash function learning
(HFL) is how to construct correlation between multi-modal
representations which are heterogeneous intrinsically in the
process of HFL. Recently, a few studies designed new hashing techniques to index multi-modal data into a common
Hamming space [33, 14, 11, 3, 36, 27]. As shown in Figure
2, analogous to Canonical Correlation Analysis (CCA) [9],
these models ﬁnd the linear projections to embed the heterogeneous data into a joint abstraction space while maximizing
the cross-correlation between images and text on a training
set. Then a quantization rule is applied to map the abstraction representations to binary hash codes. In complex
situations, i.e. the semantic gap between multi-modal data
(e.g. visual features and text features) is large, however,
these models do not extract useful joint features because
they fail to capture the common latent information. Therefor, they fail to generate eﬀective hashcodes when dealing
with complex multi-modal data.
Prior works have shown that the model which combines
semantic abstraction for both images and text with explicit
modeling of cross-correlations in a joint space can achieve
better results for cross-multimedia retrieval [19, 23, 24].
Motivated by this observation, we propose a novel Latent
Semantic Sparse Hashing (LSSH) algorithm to learn binary
codes for multimedia data sources with text and images. As
illustrated in Figure 1. up, LSSH represents text and image features in a new latent semantic space respectively, in
which the heterogeneous representations of the same topic
will show more common properties [23, 24]. In fact, LSSH
uses Sparse Coding (SC) to capture the salient structures
(e.g. edges) of images, and Matrix Factorization (MF) to
learn the latent concepts from text. Then the learned latent semantic features are mapped to a joint abstraction
space. Furthermore, an iterative strategy is applied to derive optimal solutions, and it helps LSSH to explore the
cross-correlation between multi-modal representations eﬃciently and automatically in the process of HFL. Finally,
the uniﬁed hashcodes are generated from high level abstrac-

1. We propose a novel cross-modal hashing framework to
eﬃciently construct the correlations between heterogeneous data. Moreover, the proposed method utilizes
SC and MF to merge multiple latent semantic descriptions to generate discriminate binary codes.
2. An iterative strategy is used to help LSSH explore the
cross-correlation between multi-modal representations
eﬃciently and automatically.
3. Extensive experiments on three datasets highlight the
advantages of LSSH under cross-view scenarios and
show that LSSH signiﬁcantly outperforms several stateof-the-art methods. Especially, LSSH shows signiﬁcant
improvement for cross-modal retrieval with long codes.
The rest of this paper is organized as follows. We formulate several related cross-modal hashing methods and
Canonical Correlation Analysis (CCA) within the same framework in Section 2. Section 3 presents our proposed method.
Section 4 provides extensive experimental validation on
three datasets. The conclusions are given in Section 5.

2. RELATED WORK
In this section, we show that a variety of cross-modal
methods (CMH), including CCA [9], Data Fusion Hashing
(DFH) [3], and Cross-View Hashing (CVH) [14] can be formulated within the framework of correlation analysis where
correlation is used as the objective function. Obviously, correlation of heterogeneous features is directly related to the
empirical ANN performance for cross-modal retrieval tasks.

2.1 Canonical Correlation Analysis
Consider random vector of the form (x,y) (i.e. image and
text feature), and the given samples {(xi , yi )}n
i=1 . CCA
project x respectively y onto directions wx and wy :
x → wxT x,

416

y → wyT y

Any Type
Query

then maximise the correlation between the two modalities
which can be deﬁned as follows:
 Λ [xyT ]wy
max wxT E
wx ,wy
(1)
 Λ [xxT ]wx = 1, wyT E
 Λ [yyT ]wy = 1
s.t.wxT E

n


Pij f (xi , yj )

0101

Hash Tables
Image
Text
0101

0001

1010

1010

0100

0101

Image,Text
0101

0101

Top-2 Results
Image

Text

0101

0101

0100

0001

Image,Text

0010
0001

0101
0001

Figure 2: Flowchart of LSSH and existing CMH
methods, illustrated with toy data. Up) Existing
CMH methods learn independent hash codes for
each modal of instances. Bottom) LSSH, an integrated hashing method for cross-modal, represents
image and text feature by uniﬁed hashcodes.

(2)

The optimization of (1) can be solved as a generalized eigenvalue problem (GEV) :

 
 

 Λ [xyT ] wx
 Λ [xxT ]
0
E
wx
0
E
=
λ
 Λ [yxT ]
 Λ [yyT ] wy
wy
0
E
0
E

between multi-modal data (e.g. visual features and text features) is large, it may reduce the accuracy of cross-modal
similarity search signiﬁcantly. Moreover, prior works have
shown that the high-level latent semantic information can
lead to better performance for image retrieval and bridge
the semantic gap more eﬃciently [19, 23, 24]. Hence, the
proposed LSSH constructs correlation between two modalities in latent semantic spaces.

where λ is the generalized eigenvalue.

2.2 Data Fusion Hashing
DFH [3] embeds the input data from two arbitrary spaces
into the Hamming space in a supervised way. Given sample pair (xi , yi ) and similarity label si ∈ {+1, −1}, DFH
maximizing:

si sign(wxT xi )sign(wyT yi )
(3)

3. LATENT SEMANTIC SPARSE HASHING
FOR CROSS MODAL

i

In this section, we present a novel approach for crossmodal similarity search. We restrict the discussion to multimodal instances consisting of images and text as they are
the most common and important scene in real world.

where sign(u) = −1 if u < 0, or 1 otherwise, ∀u ∈ R is
sign function. Discarding the sign function, Equation (3) is
closely related to a simpler correlation function:
 Λ [xyT ] − E
 Λ [xyT ])wy
max wxT (E
+
−

CMH

LSSH

i,j=1

wx ,wy

Hash
Codes

/

where Λ is a diagonal matrix whose diagonal entry Λii =
 P [f (x, y)] denotes the weighted empirical expec1/n, and E
tation of the function f (x, y), which is computed by the
following equation
 P [f (x, y)] =
E

Hash
Functions

3.1 Model Formulation

(4)

s.t.wxT wx = 1, wyT wy = 1
the constraints are added to avoid trivial solutions, and Λ+
is a diagonal matrix whose the i-th diagonal entry equals
1/|S+ | if si = 1, or 0 otherwise, where S+ = {(xi , yi )|si =
+1, ∀i}. The deﬁnition of Λ− is analogous. And formula (4)
can be solved by Singular Value Decomposition (SVD).

2.3 Cross-View Hashing
CVH [14] maximises the weighted cumulative correlation:
 W [xyT ]wyT − wxT E
 L [xxT ]wxT − wyT E
 L [yyT ]wyT
max 2wxT E

wx ,wy

 Λ [xxT ]wx = 1, wyT E
 Λ [yyT ]wy = 1
s.t.wxT E
(5)


where Wij be the similarity between instances i and
j, L =
2L + D, D is a diagonal matrix such that Dii =
i Wij
and L = D − W is the Laplacian matrix. Anyway, formula
(5) can be transform to a GVE problem [14]:


 
 
 L [xxT ] E
 W [xyT ] wx
 I [xxT ]
−E
wx
0
E
=
λ
 L [yyT ] wy
 W [yxT ] −E
 I [yyT ] wy
E
0
E
Actually, CCA can be viewed as a special case of CVH by
setting W = I [14].
All aforementioned cross-modal models assume that heterogeneous data can be embedded into a common abstraction space directly. However, the assumption may not ﬁt
into real world scenarios, especially when the semantic gap

Suppose that O = {oi }n
i=1 is a set of multi-modal instances, which only consists of an image and its accompanying text, i.e. oi = (xi , yi ), where xi ∈ Rm is the
m-dimensional image descriptor, and yi ∈ Rd is the ddimensional text feature (usually, m = d). Given the codewords length k, the purpose of LSSH is to learn a integrated
binary code which can bridge the semantic gap between heterogeneous data (i.e. image and text features) eﬀectively
while preserving the intrinsic similar structure of instances.
As illustrated in Figure 2, queries of any type would be
mapped to a common Hamming space according to related
learned hash functions, which makes LSSH deal with queries
with partial missing modalities. Scanning over the hash table linearly, the system returns similar results of all modalities for the given mapped query. CMH is quite eﬃcient for
online similarity search task, since only bit XOR operations
are applied when calculating Hamming distance between binary codes. Moreover, compared with existing CMH, which
learn independent hash codes for each modal of one instance,
LSSH can cut down the online search time and the storage
space of binary codes by half, while also promoting the retrieval precision signiﬁcantly.

3.2 Latent Semantic Cross Correlation
Previous works have shown that the semantic modeling
has at least two advantages for cross-modal retrieval [23,
24]. Firstly, it provides a high-level abstraction which can
lead to substantially better performance for image retrieval.
Secondly, the semantic spaces of heterogeneous data which

417

3.4 Overall Objective Function

describe the same instances are isomorphic. Motivated by
these observations, LSSH constructs the cross-correlations
in the latent semantic spaces.
As in Figure 1. up, we project the original image and
text features to the latent semantic space respectively:
PI : Rm → SIM ,

Let each bit of hashcodes represents a latent semantic concept in Matrix Factorization, i.e. D = k, then the formula
(6) can be rewrite by left multiplication inverse of RT :
A·i = R−1
T RI si = Rsi , ∀i

PT : Rd → STD

where R =
is the linear projection. There is an
intuitive interpretation about formula (9), that is a latent
concept can be described by several salient structures from
images. Moreover, we can approximate formula (9) by optimizing the cross-correlation:

where PI and PT denote the projections, and M and D is the
dimension of SIM and STD respectively. Then the isomorphic
latent semantic features are mapped into a common high
level abstraction space by linear projection, which is the
simplest isomorphic function:
RI : SIM → Ak ,

Occ (R) = A − RS2F

RT : STD → Ak

(6)

At last, binary hashcodes are obtained by the non-linear
quantization function:

min

s.t.B·i 2 ≤ 1, U·j 2 ≤ 1, R·t 2 ≤ 1, ∀i, j, t

k

3.5 Optimization Algorithm

3.3 Learning Latent Semantic Representation

The optimization problem (11) is non-convex with ﬁve
matrices variables B, A, R, U, S. Fortunately, it is convex
with respect to any one of the ﬁve variables while ﬁxing
the other four. Therefore, the optimization problem can be
solved by an iterative framework with the following listed
steps until convergency.
Step1: Learn sparse representations S by ﬁxing others
variables, the problem (11) becomes

Image
Sparse Coding has been popularly used as an
eﬀective image representation in many applications, such
as image classiﬁcation [31], face recognition [32], image
denoising [5] and image restoration [20]. The standard
sparse coding, describing each sample using only several active vectors of dictionary, has at least two advantages for
image representation. Firstly, the natural images may generally be described in terms of a small number of structural primitives [22], and the sparsity constraint in function
(7) allows the learned representation to capture the salient
structures. Secondly, the over-complete dictionary provides
suﬃcient descriptive power for low-level features. Based on
these observations, we use the Sparse Coding to capture
the salient structures of images in LSSH. Let X be a set
of m-dimensional image descriptors, i.e. X = [x1 , ..., xn ] ∈
Rm×n , the standard space coding with 1 regularization is:
n


λ|si |1

min ||X − BS||2F +
S

λ|si |1 + γ||A − RS||2F

i=1

(12)

i=1

We solve the problem (12) by using SLEP (Sparse Learning
with Eﬃcient Projections) package 1 .
Step2: Learn latent semantic concepts A by ﬁxing others
variables, the problem (11) becomes

(7)

min μ||Y − UA||2F + γ||A − RS||2F
A

where B ∈ Rm×M is the overcomplete basis set, i.e. M > m,
|| · ||F denotes Frobenius norm, and λ > 0 is the parameter
to balance the reconstruction error and sparsity.
Text Matrix Factorization, as one of the most successful
tools for learning the concepts or latent topics from text, has
a wide range of applications in text mining and information
retrieval. Let Y be a set of d-dimensional text descriptors,
i.e. Y = [y1 , ..., yn ] ∈ Rd×n , LSSH learns the latent concepts by matrix factorization:

(13)

By taking the derivative of formula (13) with respect to A
and setting it to 0, we can obtain the close form solution:
γ
γ
(14)
A = (UT U + I)−1 ( RS + UT Y)
μ
μ
where I is the identity matrix.
Step3: Learn B, R, U respectively using the Lagrange
dual [15]. In fact, the learning problem of B, R, U is essentially same, hence we only show how to optimize B. Fixing
others variables, the problem (11) becomes the least squares
problem with quadratic constraints:

(8)

,A∈R
. In fact, each column vector
where U ∈ R
U·i captures the higher-level features of original data, and
each column vector A·i is the D-dimensional representation
in latent semantic space [4].
d×D

n


 


n

B
X
− √
S||2F +
⇔ min || √
λ|si |1
γA
γR
S

i=1

Omf (U, A) = Y − UA2F

(11)

where μ > 0 leverages the discrimination power of images
and text latent representations, γ > 0 controls the linear
connection of latent semantic spaces, and · ≤ 1 is typically
applied to avoid trivial solutions.

where Hk is the k-dimension Hamming space. Several quantization algorithm has been proposed [17, 7, 12], however,
this is not the focus of our research, hence, we simply regard
the quantization function Q as a sign function.

Osc (B, S) = X − BS2F +

O(B, A, R, U, S) = Osc + μOmf + γOcc

B,A,R,U,S

Q:A →H
k

(10)

The overall objective function, combining the Sparse Coding on image features Osc given in formula (7), the Matrix
Factorization on text features Omf given in formula (8), and
the cross-correlation between the latent semantic spaces Occ
given in formula (10), is written as below:

where RI ∈ RM ×k and RT ∈ RD×k . In order to construct
cross-correlation between two modalities, we require image
and text features of the same instance to be equal in Ak :
RI PI (xi ) = RT PT (yi ), ∀i

(9)

R−1
T RI

D×n

min X − BS2F
B

1

418

s.t.B·i 2 ≤ 1, ∀i

(15)

http://parnec.nuaa.edu.cn/jliu/largeScaleSparseLearning.htm

Moreover, a∗ also can be obtained according formula (14),
which uses both image and text information, and then h =
sign(a∗ ). We investigate the performance of cross-modal
retrieval for these diverse queries in Section 4.2.6.

Algorithm 1 Latent Semantic Sparse Hashing
Input:
Image representation matrix X and text feature matrix
Y, parameters λ, μ, γ and hashcodes length k.
Output:
Integrated hash codes H, matrix variables B, U, R.
1: Initialize U, A, R and B by random matrices respectively, and normalizing each column of X respectively
Y by 2 norm.
2: repeat
3:
Fix U, R, B and A, update S as illustrated in Step1;
4:
Fix U, R, B and S, update A by Equation (14);
5:
Fix U, R, A and S, update B as illustrated in Step3;
6:
Fix U, B, A and S, update R by optimizing:
min ||A − RS||2F
R

7:

3.7 Discussion
In this section, we will show that LSSH is available for
large-scale datesets. The time consuming for training LSSH
includes sparse coding learning, latent semantic concepts
learning, and Lagrange dual learning. Typically, solving (12)
and (13) requires O(nM 2 ) 2 and O(d3 ) respectively. The Lagrange dual (18), which is independent of sample size n, can
be solved by using Newtons method or conjugate gradient,
which has been shown more eﬃcient than gradient descent
[15]. In a word, the total time complexity of training LSSH
is linear to n, which is really scalable for large-scale datesets
compared with most existing cross-modal hashing methods.

s.t.||R·i ||2 ≤ 1, ∀i

Fix R, B, A and S, update U by optimizing:
min ||Y − UA||2F
U

s.t.||U·i ||2 ≤ 1, ∀i

4. EXPERIMENT
We conduct experiments on three real-world datasets for
cross-modal similarity search to verify the eﬀectiveness of
LSSH. Speciﬁcally, datasets involved in our experiments consist of text and images, and we use text as query to search
similar images and image as query to search similar texts.
Furthermore, we analysis the parameter sensitivity, check
the convergence property of Algorithm 1 and the inﬂuence
of diﬀerent query type to the similarity search performance.

8: until convergency.
9: H = sign(A).

Consider the Lagrangian:
L(B, 
θ) = X − BS2F +

n


θi (B·i  − 1)

(16)

i=1

4.1 Experiment Settings

where θi > 0 is the Lagrange multipliers. Letting the derivative of (16) with respect to B equal to zero, the close form
solution of (15) can be obtained by
B = XST (SST + Θ)−1

4.1.1 Datasets

(17)

where Θ is diagonal matrix whose diagonal entry Θii = θi ,
and is got by optimizing following Lagrange dual problem
min T r(XST (SST + Θ)−1 SXT ) + T r(Θ)
Θ

s.t.Θii ≥ 0, ∀i (18)

where T r(·) denotes the trace of matrix, i.e. the sum of
diagonal. Problem (18) can be solved by using Newtons
method or conjugate gradient. The algorithm is summarized
in Algorithm 1.

3.6 Extension to Out-of-Sample
In practice, the components of a new query can be quite
diverse, now we discuss it in the following three situations.
 as original image feature of
Image only. We denote x
the query, and then obtain the sparse coding by solving
min ||
x − Bs||2 + λ|s|

(19)

s

where dictionary B is given by Algorithm 1. Let s∗ be the
optimal solution, then the hash codes h = sign(Rs∗ ).
 as original text feature of the
Text only. We denote y
query, and the close form matrix factorization factor is
)
a∗ = (UT U)−1 (UT y

(20)
T

−1

In most cases, U is full column rank, then (U U)
exists. Otherwise, we may approximate a∗ by 
a∗ = (UT U +
 ), where  > 0 is a small real number (e.g. 0.001),
I)−1 (UT y
then we can get the hash codes h = sign(a∗ ).
Both Text and Image. We can use the same way to
get hash codes described in Image only and Text only.

Wiki3 .The Wiki dataset was collected from Wikipedia
consisting of 2,866 multimedia documents. Each document
contains 1 image and at least 70 words. Each image is represented by a 128-dimension SIFT [18] histogram and each
text is represented by a 10-dimension topics’ vector generated by latent Dirichlet allocation (LDA) model [2]. Totally
10 categories are considered in this dataset and each document (image-text pair) is labeled by one of them. Documents are considered to be similar if they belong to the same
category.
LabelMe4 . The LabelMe dataset is created by MIT
Computer Science and Artiﬁcial Intelligence Laboratory which
is made up of 2688 images. Each image is annotated by several tags which denote the objects in this image, such as
”sea” and ”beach”. Tags occurs in less than 3 images are
discarded and 245 unique tags remain. This dataset is divided to 8 unique outdoor scenes such as ”coast”, ”forest”
and ”highway” and each image belongs to one scene. Each
image is represented by a 512-dimension GIST [21] feature
and each text is represented by an index vector of selected
tags. Image-text pairs are regarded as similar if they share
the same scene label.
NUS-WIDE5 . The NUS-WIDE dataset is a real-word
image dataset created by Lab for Media Search in National
University of Singapore [28]. This dataset contains 81 concepts but some are scarce. So we select 10 most common
2
The complexity of lasso algorithms is O(nM 2 + M 3 ), but
usually, n
M.
3
http://www.svcl.ucsd.edu/projects/crossmodal/
4
http://people.csail.mit.edu/torralba/code/spatialenvelope/
5
http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm

419

which is deﬁned as the ratio between the number of relevant
instance and the number of retrieved instance r, and δ(r) is
a indicator function which equals to 1 if the rth instance is
relevant to query or 0 otherwise. Then the AP of all queries
are averaged to obtain the mAP.
Furthermore, we also report two types of performance
curves. One is precision-recall curve which shows the precision at diﬀerent recall level. The other is topN-precision
curve which reﬂects the change of precision with respect to
the number of retrieved instances.

concepts, and thus 186,577 images are left from 269,648 images. Furthermore, we select 1000 most frequent tags from
5,018 unique tags in this dataset. Each image is represented
by a 500-dimension SIFT histogram and each text is represented by an index vector of selected tags. Each image-text
pair is annotated by at least 1 of 10 concepts. Pairs are
considered to be similar if they share at least one concept.

4.1.2 Baseline Methods
Our method is compared against four state-of-the-art hashing methods for cross-modal similarity search as below.

4.1.4 Implementation Details

• Cross-view Hashing6 (CVH) [14] extends spectral hashing to the multi-view case and is a special case of IMH.

The experiments are carried out as follows. For image
data, we ﬁrst apply Principle Component Analysis (PCA)
to reduce the feature dimension to 64, which can also remove
noise from image data. Then, the length of sparse codes, i.e.,
the size of dictionary B, is set to 512, and the sparsity parameter λ is set to 0.2. LSSH has two model parameters,
μ which leverages the discrimination power between images
and texts, and γ which controls the linear connection of latent semantic spaces. In the coming sections, we provide
empirical analysis on parameter sensitivity, which veriﬁes
that LSSH can achieve stable and superior performance under a wide range of parameter values. When comparing
with baseline methods, we use the following parameter settings for all experiments: μ = γ = 1, which shows good
performance on all three datasets. For baseline methods,
we carefully tune the parameters for them and report the
best results of them.
Furthermore, considering IMH and CHMIS requires too
much resource to learn hash functions on NUS-WIDE with
all data. We select randomly 10, 000 instances for all methods to train hash functions and then they are applied to the
other instances in database to generate hash codes for them
as in [27]. Moreover, hashing methods should have the ability to handle out-of-sample instances since data is keeping
coming into database as time goes by in real world. So we
simulate the situation by this experiment setting. For LSSH,
hash codes for instances in database is generated as follows.
We ﬁrst generate sparse codes S for images by (7). Then we
use (14) to generate uniﬁed codes A for all instances combining both images and texts. Finally, we apply sign function
on A to obtain hash codes. And for query instances which
only have one modal, i.e., image or text, we use methods
introduced in 3.6 to generate hash codes. Moreover, we set
R = 100, and all of the results are averaged over 10 runs.
All the experiments are conducted on a computer which has
Intel Xeon E5520 2.27GHz CPU, 16GB RAM.

• Data Fusion Hashing7 (DFH) [3] constructs two groups
of linear hash functions to preserve the similarity structure in each individual media type.
• Inter-media Hashing6 (IMH) [27] introduces inter-media
and intra-media consistency to discover a common hamming space, and uses linear regression with regularization model to learn view-speciﬁc hash functions.
• Composite Hashing with Multiple Information Sources7
(CHMIS) [33] combines information from multi sources
into integrated hash codes by optimizing the relaxed
hash codes and combination coeﬃcients alternatively.
CVH, IMH and DFH generates diﬀerent hash codes for
each modal of an instance, i.e., the hash codes of diﬀerent modals of an instance is diﬀerent, but these methods
try to make these codes more similar for one instance. In
our experiment, they will generate diﬀerent hash codes for
image and text separately of an instance(image-text pair).
When a new image(text) query comes, they ﬁrst generate its
hash codes, and then search similar data from text(image)
database. CHMIS generates uniﬁed hash codes for an instance combining all modals. However, if any one modal of
an instance is unavailable, it can’t generate hash codes for
this instance, which is too demanding for real-world scenarios. This method improves search accuracy by combining
multiple information sources of one instance, and actually is
not implemented for cross-modal similarity search. Yet we
still compare LSSH to CHMIS to verify the ability of LSSH
to promote search performance by merging knowledge from
heterogeneous data sources.

4.1.3 Evaluation Metrics
We adopt mean Average Precision(mAP) as the evaluation metric for eﬀectiveness in our experiment. This evaluation metric has been widely used in literatures [27][35].
mAP has shown especially good discriminative power and
stability to evaluate the performance of similarity search.
A larger mAP indicates better performance that similar instances have high rank. Given a query and a set of R retrieved instances, the Average Precision(AP) is deﬁned as
AP =

4.2 Results and Discussions
4.2.1 Results on Wiki
We select 75% of the data as database and the rest as
the query set. The mAP of LSSH and baseline methods are
shown in Table 1., and the performance curves are shown
in Figure 3. We can observe that LSSH can signiﬁcantly
outperform baseline methods on both cross-modal similarity
search tasks which veriﬁes the eﬀectiveness of LSSH.
The semantic gap between two views of Wiki is quite large.
In fact, text can better describe the topic of the image-text
pair than image. When a image query comes, since it’s not
quite related to it’s topic, it’s diﬃcult to ﬁnd semantically
similar texts. So the mAP of image query is low for all meth-

R
1
P (r)δ(r)
L r=1

where L is the number of relevant instances in retrieved
set, P (r) denotes the precision of top r retrieved instances
6
We implemented it ourselves because the code is not publicly available.
7
The source code is kindly provided by the authors.

420

Table 1: mAP Comparison on Three Datasets.
Method
Img
to
Txt

Txt
to
Img

CVH
IMH
DFH
CHMIS
LSSH
CVH
IMH
DFH
CHMIS
LSSH

16 bits
0.1984
0.1922
0.2097
0.1942
0.2330
0.2590
0.3717
0.2692
0.1942
0.5571

Wiki
32 bits
64 bits
0.1490
0.1182
0.1760
0.1572
0.1995
0.1943
0.1852
0.1796
0.2340
0.2387
0.2042
0.1438
0.3319
0.2877
0.2575
0.2524
0.1852
0.1796
0.5743
0.5710

128 bits
0.1133
0.1351
0.1898
0.1671
0.2340
0.1170
0.2674
0.2540
0.1671
0.5577

16 bits
0.4704
0.3593
0.4994
0.4894
0.6692
0.5778
0.4346
0.5800
0.4894
0.6790

LabelMe
32 bits
64 bits
0.3694
0.2667
0.2865
0.2414
0.4213
0.3511
0.4010
0.3414
0.7109
0.7231
0.4403
0.3174
0.3323
0.2771
0.4310
0.3200
0.4010
0.3414
0.7004
0.7097

128 bits
0.1915
0.1990
0.2788
0.2967
0.7333
0.2153
0.2258
0.2313
0.2967
0.7140

16 bits
0.4694
0.4564
0.4774
0.3596
0.4933
0.4800
0.4600
0.5174
0.3596
0.6250

NUS-WIDE
32 bits
64 bits
0.4656
0.4705
0.4566
0.4589
0.4677
0.4674
0.3652
0.3565
0.5006
0.5069
0.4688
0.4636
0.4581
0.4653
0.5077
0.4974
0.3652
0.3565
0.6578
0.6823

128 bits
0.4777
0.4453
0.4703
0.3594
0.5084
0.4709
0.4454
0.4903
0.3594
0.6913

each hash bits uncorrelated to each other. However, these
orthogonality constraints sometimes lead to practical problem. It is well known that for most real-world datasets, most
of the variance is contained in top few projections. With
longer codes, these constraints will force them to progressively pick directions with low variance, which may reduce
the quality and discriminative power of hash codes [29].

ods. Even so, LSSH can achieve best performance, especially
with long hash codes. LSSH can reduce the semantic gap
between modals in database which makes the hash codes of
images quite related to the topics of instances. Actually, the
hash codes for images are also for the instances. So when a
text query which is highly related to its topic comes, it can
obtain semantically similar images of it. But the hash codes
of images generated by baseline methods still show little relevance to their topics. That’s why LSSH can improve mAP
by 18% at least which also shows the importance to reduce
semantic gap between diﬀerent modals.
We further observed that the PR-curve of several methods
looks strange, e.g. the PR-curve of CVH for text query to
image database at 64 bits shows that it behave like random
guess in experiments. This phenomenon also happened in
[35] and [36] and a reasonable explanation is given by [29].
Actually, all baseline methods are solved by eigenvalue decomposition and have orthogonality constraints on each bit
so that each bit shows no correlation to each other. The
ﬁrst few projection directions may have high variance and
their corresponding hash bits can be quite discriminative,
which is quite useful to similarity search. However, as the
code length increases, the hash codes will be dominated by
bits with very low variance. Actually, since the variance
is too low, the lower bits are meaningless and ambiguous.
So these indiscriminative hash bits may lead the method to
make random guess in experiments.

4.2.3 Results on NUS-WIDE
We select 2% of the data as the query set and the rest
as the database. As mentioned above, we select 10, 000 instances from databases randomly as the training set to learn
hash functions and then they are applied to other instances
in database to generate hash codes. The mAP and performance curves are shown in Table 1. and Figure 5. respectively. Similar to results above, we observe that LSSH
outperforms baseline methods signiﬁcantly and it performs
better with longer hash codes.
In real-world applications, the size of database can be
so large that it’s impossible to learn hash functions on the
whole database because of the limitation of computational
resources. And new data is keep coming into database as
time goes by and hash codes for them need to be computed.
So the hashing methods should be able to deal with outof-sample instances (other instances in database and new
coming instances). The common solution is to learn hash
functions which project new data to a feature space and
then quantize new features to binary hash codes by sign
operation. The ability to deal with out-of-sample instances
can test the eﬀectiveness of hash functions which can further
judge the ability of hashing methods to apply to practical
problems. The experiment settings on NUS-WIDE is quite
similar to real-world scenario. The experiment results show
that LSSH can deal with out-of-sample instances easily and
it has superior ability to handle large-scale database.

4.2.2 Results on LabelMe
75% of the data are chosen as the database and the remaining to form the query set. The mAP of LSSH and baseline methods are shown in Table 1. and Figure 4. shows
the performance curves of them. LSSH shows more superior performance than baseline methods with diﬀerent code
length. Furthermore, the images and texts of LabelMe are
quite related to each other, thus LSSH can learn more eﬀective hash functions to increase similarity search performance
by merging knowledge from heterogeneous data. Moreover,
the mAP of image query is even higher than text query, this
also shows the power of Sparse Coding to capture high-level
semantic information of images.
We can also observe that as code length increase, LSSH
performs better because LSSH can learn more precise descriptions for instances with more latent concepts and longer
codes can encode more information. However, the performance of baseline methods degrades signiﬁcantly as the increase of code length. This phenomenon has also been observed in [29] and [17]. The main reason is that the baseline
methods are spectral-hashing-based methods which have orthogonality constraints on the projection directions to make

4.2.4 Parameter Sensitivity Analysis
We conduct empirical analysis on parameter sensitivity on
all datasets, which validates that LSSH can achieve stable
and superior performance under a wide range of parameter
values, verifying that LSSH is robust to parameters.
When analyzing one parameter, we keep other parameters
ﬁxed to the settings mentioned in Section 4.1.4. Due to
the limit of space, we only present the results at 64 bits
on all datasets in Figure 6. The dashed lines are the best
performance of baselines with all experiment settings, e.g.
the red dashed line in the ﬁrst ﬁgure shows the result of DFH
at 16 bits, which, as be observed from Table 1., is the best
result of all baselines varying code length for ”Image to Text”

421

Image → Text @ 32 bits

Image → Text @ 64 bits

0.26

0.26

0.2
0.18

0.2
0.18

0.16

0.16

0.14

0.14

0.12

0.12

0.1

0.1

0

0.2

0.4

Recall

0.6

0.8

1

Image → Text @ 32 bits

0.26

0

0.2

0.22

0.4

Recall

0.6

0.8

0.4

0.3

0.3

0.2

0.2

0.1

1

0.5

0.4

0

0.2

Image → Text @ 64 bits

0.26
CVH
IMH
DFH
CHMIS
LSSH

0.24

0.4

Recall

0.6

0.8

0.1

1

0

0.2

Text → Image @ 32 bits
CVH
IMH
DFH
CHMIS
LSSH

0.24
0.22

0.4

Recall

0.6

0.8

1

Text → Image @ 64 bits
CVH
IMH
DFH
CHMIS
LSSH

0.6

0.5

CVH
IMH
DFH
CHMIS
LSSH

0.6

0.5

0.16

0.18
0.16
0.14

0.14

0.12

0.12

Precision

0.18

Precision

0.2

0.2

Precision

Precision

0.22

CVH
IMH
DFH
CHMIS
LSSH

0.6

0.5

Precision

0.22

Text → Image @ 64 bits
CVH
IMH
DFH
CHMIS
LSSH

0.6

0.24

Precision

Precision

0.24

0.1

Text → Image @ 32 bits
CVH
IMH
DFH
CHMIS
LSSH

0.28

Precision

CVH
IMH
DFH
CHMIS
LSSH

0.28

0.4

0.3

0.2

0.4

0.3

0.2

0.1

0

100

200

300

400

500

600

N

700

800

900

0.08

1000

0

100

200

300

400

500

600

N

700

800

900

0.1

1000

0

100

200

300

400

500

600

N

700

800

900

0.1

1000

0

100

200

300

400

500

600

N

700

800

900

1000

Figure 3: PR-Curves and topN-precision Curves on Wiki Varying Code Length
Image → Text @ 32 bits

Image → Text @ 64 bits

0.7

0.7

0.7

0.4

0.5
0.4

0.3

0.2

0.2

0.2

0.2

0.4

Recall

0.6

0.8

0.1

1

0

0.2

Image → Text @ 32 bits

0.4

Recall

0.6

0.8

0.1

1

0.7

0.7

0.4

0.2

0.2

0.2

300

400

500

600

N

700

800

900

0.1

1000

0

100

200

300

400

500

600

N

700

0

0.2

800

900

0.1

1000

0.4

Recall

0.6

0.8

1

Text → Image @ 64 bits
CVH
IMH
DFH
CHMIS
LSSH

CVH
IMH
DFH
CHMIS
LSSH

0.8
0.7
0.6

0.4
0.3

200

0.1

1

0.5

0.3

100

0.8

0.6

0.5

0.3

0

Recall

0.6

0.7

Precision

Precision

0.4

0.4

0.8

0.6

0.5

0.1

0.2

Text → Image @ 32 bits
CVH
IMH
DFH
CHMIS
LSSH

0.8

0.6

0.4

0.2

0

Image → Text @ 64 bits
CVH
IMH
DFH
CHMIS
LSSH

0.8

0.5

0.3

Precision

0

0.6

0.4

0.3

0.1

0.7

0.5

0.3

CVH
IMH
DFH
CHMIS
LSSH

0.8

0.6

Precision

0.5

Text → Image @ 64 bits
CVH
IMH
DFH
CHMIS
LSSH

0.8

0.6

Precision

Precision

0.6

Precision

Text → Image @ 32 bits
CVH
IMH
DFH
CHMIS
LSSH

0.8

Precision

CVH
IMH
DFH
CHMIS
LSSH

0.8

0.5
0.4
0.3
0.2

0

100

200

300

400

500

600

N

700

800

900

0.1

1000

0

100

200

300

400

500

600

N

700

800

900

1000

Figure 4: PR-Curves and topN-precision Curves on LabelMe Varying Code Length

0.46

0.46

0.55

0.4
0.38

0.42
0.4
0.38

0.36

0.36

0.34

0.34

0.32

0.32
0

0.2

0.4

Recall

0.6

0.8

0.3

1

Precision

0.42

0

0.2

Image → Text @ 32 bits

0.4

Recall

0.6

0.8

0.5

0.55

0.45
0.4

0.4
0.35

0

0.2

0.4

Recall

0.6

0.8

0.3

1

0.5

0.65

0.4

0.3

0.35

0

500

1000

1500

2000

2500

N

3000

3500

4000

4500

5000

0.3

0

500

1000

1500

2000

2500

N

3000

3500

4000

4500

Recall

0.6

1

CVH
IMH
DFH
CHMIS
LSSH

0.65

0.5

0.55
0.5
0.45

0.4

0.4

0.35

0.35

0

500

1000

1500

2000

2500

N

3000

3500

4000

4500

5000

0.3

0

500

1000

1500

2000

2500

N

3000

Figure 5: PR-Curves and topN-precision Curves on NUS-WIDE Varying Code Length

422

0.8

0.6

0.55

0.3

5000

0.4

0.7

Precision

0.45

0.45
0.35

0.2

Text → Image @ 64 bits
CVH
IMH
DFH
CHMIS
LSSH

0.7

Precision

Precision

Precision

0.4

0

Text → Image @ 32 bits
CVH
IMH
DFH
CHMIS
LSSH

0.6
0.45

0.5
0.45

0.35
0.3

1

CVH
IMH
DFH
CHMIS
LSSH

0.6

0.5

Image → Text @ 64 bits
CVH
IMH
DFH
CHMIS
LSSH

Text → Image @ 64 bits
CVH
IMH
DFH
CHMIS
LSSH

0.6

0.44

Precision

Precision

Text → Image @ 32 bits
CVH
IMH
DFH
CHMIS
LSSH

0.48

0.44

0.3

Image → Text @ 64 bits

0.5
CVH
IMH
DFH
CHMIS
LSSH

Precision

Image → Text @ 32 bits

0.5
0.48

3500

4000

4500

5000

Image → Text @ 64 bits

0.8

Text → Image @ 64 bits

0.8

Image → Text @ 64 bits

Text → Image @ 64 bits

0.75
0.7

0.75
0.7

0.7
0.65

0.6

0.7
0.65

0.6

0.6

0.6

0.55

mAP

0.4

0.5

mAP

0.5

mAP

mAP

0.5

0.4

0.45

0.55
0.5
0.45

0.3

0.3
0.4

0.2

0.4
0.2

0.35

Wiki
LabelMe
NUS−WIDE

0.1

0.01

0.05

0.1

0.2

0.5

1

mu

2

Wiki
LabelMe
NUS−WIDE

0.3
0.25
5

10

50

100

0.01

0.05

0.1

0.2

0.5

1

mu

2

0.35

Wiki
LabelMe
NUS−WIDE

0.1

5

10

50

100

0.0001 0.0005 0.001 0.005

0.01

0.05

0.1

0.5

gamma

Wiki
LabelMe
NUS−WIDE

0.3
0.25
1

5

10

50

100

0.0001 0.0005 0.001 0.005

0.01

0.05

0.1

0.5

gamma

1

5

10

50

100

Figure 6: Parameter Sensitivity Analysis
Objective w.r.t. #iterations @ 64 bits

mAP on Wiki

2

Image
Text
Both

0.65

mAP on NUS−WIDE
Image
Text
Both

0.78

0.6

1.6

Image
Text
Both

0.75

0.76

0.7

0.55

1.4

0.74

1
0.8

0.45

0.7

0.35

0.68

0.3

0.4

0.25
0

5

10

#iterations

15

20

0.72

0.4

0.6

0.2
16

mAP

1.2

mAP

0.5

mAP

Objective

mAP on LabelMe
0.8

0.7
Wiki
LabelMe
NUS

1.8

0.65

0.6

0.55

0.66
0.5
0.64
32

#bits

64

128

16

32

#bits

64

128

16

32

#bits

64

128

Figure 7: Convergence and Query Diversity Study
a single text or both. Here we test the inﬂuence of query
type on the similarity search performance. When an image
query comes, we use (7) to generate its codes while (20) is
applied to text query. And as mentioned above, we use (7)
and (14) to generate codes for instances with both image and
text. Figure 7. shows the mAP results on three datasets of
diﬀerent query type varying code length from 16 to 128.
We can observe that query with both modals can slightly
outperform the query with partial missing modal. This is
reasonable because query with both modals contains more
information and the hash codes for it can be more semantically informative, which leads to better performance.
Furthermore, the performance gap between image query
and text query diﬀers from datasets. In fact, for all datasets,
texts belonging to one category show much similarity, so
text query works well. The images in LabelMe belonging to
one category, such as ”highway”, are quite similar to each
other, that’s why image query in LabelMe can achieve superior performance. But the images in Wiki are quite diverse.
For example, images belonging to ”History” can be related
to a building, a man or a weapon. So it’s diﬃcult to ﬁnd
semantically similar instances for an image query.

task. We can observe that LSSH can outperform all best
results of baselines when μ ∈ [0.05, 10] and γ ∈ [0.005, 10].
The parameter μ leverages the power of images and texts.
Actually, utilizing the information from both modals can
lead to better results. When μ is too small, e.g., μ < 0.05,
our model just focuses on images while ignoring texts. When
μ is too large, e.g., μ > 10, our model prefers information
from texts. Speciﬁcally, it’s easy to choose a proper value
for μ because we can observe that LSSH shows stable and
superior performance when μ ∈ [0.05, 10].
The parameter γ controls the connection of latent semantic spaces. If γ is too small, the connection between different modals is weak with imprecise projection in formula
(10), which will lead to poor performance for cross-modal
similarity search. However, if γ is too large, the strong connection will make the learning of latent representations of
images and texts, i.e., Sparse Coding and Matrix Factorization, to be quite imprecise. Because images and texts are
represented by imprecise features, it’s reasonable the the
performance will degrade. Fortunately, it’s also eﬀortless to
choose proper γ from the range [0.005, 10];

4.2.5 Convergence Study

5. CONCLUSIONS

Since LSSH is solved by an iterative procedure, we empirically check its convergency property. Figure 7. shows that
the value of objective function (averaged by the number of
training data) can decrease steadily with more iterations and
it can converge with 20 iterations on all datasets at 64 bits,
which validates the eﬀectiveness of Algorithm 1. The results
at other code length are similar to at 64 bits.
Furthermore, the average time cost for each iteration at 64
bits on Wiki, LabelMe and NUS-WIDE is 3.98 seconds, 3.65
seconds and 12.73 seconds respectively. So we can see that
LSSH can be solved with 10, 000 training data in less than
5 minutes, which shows the high eﬃciency of Algorithm 1.

In this paper, we propose a novel hashing method, referred
to as Latent Semantic Sparse Hashing, for large-scale crossmodal similarity search between images and texts. Speciﬁcally, we utilizes Sparse Coding to capture high level salient
structures of images, and Matrix Factorization to extract
latent concepts from texts. Then these high level semantic features are mapped to a joint abstraction space. The
search performance can be promoted by merging multiple
comprehensive latent semantic descriptions from heterogeneous data. We propose an iterative strategy which is highly
eﬃcient to explore the correlation between multi-modal representations and bridge the semantic gap between heterogeneous data in latent semantic space.
We conduct extensive experiments on three multi-modal
datasets consisting of images and texts. Superior and stable

4.2.6 Query Diversity Study
As mentioned in Section 3.6, a coming query instance can
be quite diverse, e.g., it may consists of a single image, or

423

[13] B. Kulis and K. Grauman. Kernelized
locality-sensitive hashing for scalable image search. In
ICCV, pages 2130–2137. IEEE, 2009.
[14] S. Kumar and R. Udupa. Learning hash functions for
cross-view similarity search. In IJCAI, 2011.
[15] H. Lee, A. Battle, R. Raina, and A. Ng. Eﬃcient
sparse coding algorithms. In NIPS, 2006.
[16] W. Liu, J. Wang, R. Ji, Y.-G. Jiang, and S.-F. Chang.
Supervised hashing with kernels. In CVPR, pages
2074–2081. IEEE, 2012.
[17] W. Liu, J. Wang, S. Kumar, and S. F. Chang.
Hashing with graphs. In ICML, 2011.
[18] D. G. Lowe. Distinctive image features from
scale-invariant keypoints. IJCV, 60(2):91–110, 2004.
[19] Z. Lu and Y. Peng. Latent semantic learning by
eﬃcient sparse coding with hypergraph regularization.
In AAAI, 2011.
[20] J. Mairal, M. Elad, and G. Sapiro. Sparse
representation for color image restoration. Image
Processing, IEEE Transactions on, 2008.
[21] A. Oliva and T.Torralba. Modeling the shape of the
scene: a holistic representation of the spatial envelope.
IJCV, 42:145–175, 2001.
[22] B. A. Olshausen and D. J. Field. Sparse coding with
an overcomplete basis set: A strategy employed by
v1? Vision research, 1997.
[23] N. Rasiwasia, J. Costa Pereira, E. Coviello, G. Doyle,
G. R. Lanckriet, R. Levy, and N. Vasconcelos. A new
approach to cross-modal multimedia retrieval. In ACM
Multimedia. ACM, 2010.
[24] N. Rasiwasia, P. J. Moreno, and N. Vasconcelos.
Bridging the gap: Query by semantic example. IEEE
Transactions on Multimedia, 2007.
[25] R. Salakhutdinov and G. Hinton. Semantic hashing.
IJAR, 50(7):969–978, 2009.
[26] G. Shakhnarovich, P. Viola, and T. Darrell. Fast pose
estimation with parameter-sensitive hashing. In ICCV.
IEEE, 2003.
[27] J. Song, Y. Yang, Y. Yang, Z. Huang, and H. T. Shen.
Inter-media hashing for large-scale retrieval from
heterogeneous data sources. In ICMD. ACM, 2013.
[28] R. H. H. L. Z. L. T. Chua, J. Tang and Y. Zheng.
Nus-wide: A real-world web image database from
national university of singapore. In CIVR, 2009.
[29] J. Wang, S. Kumar, and S.-F. Chang. Semi-supervised
hashing for scalable image retrieval. In CVPR, 2010.
[30] Y. Weiss, A. Torralba, and R. Fergus. Spectral
hashing. NIPS, 2008.
[31] J. Yang, K. Yu, Y. Gong, and T. Huang. Linear
spatial pyramid matching using sparse coding for
image classiﬁcation. In CVPR. IEEE, 2009.
[32] M. Yang, L. Zhang, J. Yang, and D. Zhang. Robust
sparse coding for face recognition. In CVPR, 2011.
[33] D. Zhang, F. Wang, and L. Si. Composite hashing
with multiple information sources. In SIGIR, 2011.
[34] D. Zhang, J. Wang, D. Cai, and J. Lu. Self-taught
hashing for fast similarity search. In SIGIR, 2010.
[35] Y. Zhen and D. Yang. A probabilistic model for
multimodal hash function learning. In SIGKDD, 2012.
[36] Y. Zhen and D.-Y. Yeung. Co-regularized hashing for
multimodal data. In NIPS, pages 1385–1393, 2012.

performances of LSSH veriﬁes the eﬀectiveness of it compared against several state-of-the-art cross-modal hashing
methods. With longer hash codes, LSSH can conduct matrix
factorization more accurately and encode more information,
which leads to better performance, while the baseline methods perform worse with longer hash codes because of the orthogonality constraints on their objective function. Experiments on NUS-WIDE, which is a large-scale datasets, show
that LSSH can deal with out-of-sample easily and has the
ability to handle large-scale database. The analysis on parameter sensitivity shows that LSSH is very robust to model
parameters which can achieve stable and superior performance under a wide range of parameter values. Our convergence study shows the proposed learning algorithm is indeed
eﬀective and can be solved eﬃciently. And the study on
query diversity shows the inﬂuence of diﬀerent query types
on the search performance and combining information from
multiple source can help increase search performance.

6.

ACKNOWLEDGMENTS

This research was supported by the National Basic Research Project of China (Grant No. 2011CB70700), the National HeGaoJi Key Project (No. 2013ZX01039-002-002),
and the National Natural Science Foundation of China (Grant
No. 61271394). And the authors would like to thank the reviewers for their valuable comments.

7.

REFERENCES

[1] A. Andoni and P. Indyk. Near-optimal hashing
algorithms for approximate nearest neighbor in high
dimensions. In FOCS’06, 2006.
[2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent
dirichlet allocation. JMLR, 3:993–1022, 2003.
[3] M. M. Bronstein, A. M. Bronstein, F. Michel, and
N. Paragios. Data fusion through cross-modality
metric learning using similarity-sensitive hashing. In
CVPR, pages 3594–3601. IEEE, 2010.
[4] S. C. Deerwester, S. T. Dumais, T. K. Landauer,
G. W. Furnas, and R. A. Harshman. Indexing by
latent semantic analysis. JASIS, 41(6):391–407, 1990.
[5] M. Elad and M. Aharon. Image denoising via sparse
and redundant representations over learned
dictionaries. TIP, 2006.
[6] A. Gionis, P. Indyk, R. Motwani, et al. Similarity
search in high dimensions via hashing. In VLDB,
pages 518–529, 1999.
[7] Y. Gong and S. Lazebnik. Iterative quantization: A
procrustean approach to learning binary codes. In
CVPR, pages 817–824. IEEE, 2011.
[8] K. He, F. Wen, and J. Sun. K-means hashing: an
aﬃnity-preserving quantization method for learning
binary compact codes. In CVPR, 2013.
[9] H. Hotelling. Relations between two sets of variates.
Biometrika, 1936.
[10] P. Indyk and R. Motwani. Approximate nearest
neighbors: towards removing the curse of
dimensionality. In STC. ACM, 1998.
[11] S. Kim, Y. Kang, and S. Choi. Sequential spectral
learning to hash with multiple representations. In
ECCV, pages 538–551. Springer, 2012.
[12] W. Kong and W.-J. Li. Double-bit quantization for
hashing. In AAAI, 2012.

424

