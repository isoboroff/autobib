A Fusion Approach to Cluster Labeling
Haggai Roitman, Shay Hummel, Michal Shmueli-Scheuer
IBM Research - Haifa
Haifa 31905, Israel

{haggai,shayh,shmueli}@il.ibm.com

ABSTRACT
We present a novel approach to the cluster labeling task using fusion methods. The core idea of our approach is to
weigh labels, suggested by any labeler, according to the estimated labeler’s decisiveness with respect to each of its
suggested labels. We hypothesize that, a cluster labeler’s labeling choice for a given cluster should remain stable even
in the presence of a slightly incomplete cluster data. Using
state-of-the-art cluster labeling and data fusion methods,
evaluated over a large data collection of clusters, we demonstrate that, overall, the cluster labeling fusion methods that
further consider the labeler’s decisiveness provide the best
labeling performance.
Categories and Subject Descriptors: H.3.3 [Information Search
and Retrieval]
General Terms: Algorithms, Experimentation
Keywords: Cluster labeling, Fusion

1.

INTRODUCTION

In this work we present a novel approach to the cluster
labeling task using fusion methods [14]. The core idea of
our approach is to weigh labels, suggested by any labeler,
according to the estimated labeler’s decisiveness with respect to each of its suggested labels.
We hypothesize that, a cluster labeler’s labeling choice for
a given cluster should remain stable even in the presence of
a slightly incomplete cluster data. The stability of a cluster
labeler’s decisiveness, therefore, dictates how much should
one relay on its cluster labeling decisions.
We first describe how a given cluster labeler’s decisiveness may be estimated given any of its suggested labels. We
then demonstrate how such estimate can be further used to
combine several cluster labelers within state-of-the-art fusion methods for boosting the overall cluster labeling performance.
To the best of our knowledge, our work is the first to suggest a meta-cluster labeling solution based on fusion methods.

2.

The cluster labeling task aims at finding a single label to
a given cluster of documents [1, 6]. Such label should best
describe (to a human) the cluster’s main topic [6].
Numerous previous works have focused on the development of various cluster labeling methods [6, 12, 13, 11, 2,
4, 7]. Two main types of cluster labeling methods may be
employed, namely direct and indirect methods. Cluster
labels may be directly extracted from the content of the
cluster’s documents. For example, cluster labels can be extracted using various feature selection methods [9], choosing
the most frequent terms (keywords, n-grams, phrases, etc)
in the cluster or the top weighed cluster centroid’s terms [5].
In addition, cluster labels can be extracted using anchor
text [6], named entities [12], utilizing the cluster’s hierarchy [13], etc. Cluster labels may be further indirectly extracted using external relevant label sources, e.g, using
Wikipedia’s categories [11, 2], Dbpedia’s graph [7], Freebase’s concepts [4], etc.

2.1

MAIN APPROACH
Preliminaries

Let C denote a cluster of documents, obtained by some
clustering algorithm [1]. For a given cluster C, a cluster
labeler L suggests one or more labels, presumed to best represent the cluster’s main topic.
Our main goal is to combine the scores of labels that
were initially extracted by several cluster labeling methods.
Therefore, we assume a cluster meta-labeling framework,
where a set of cluster labelers L = {L1 , . . . , Lm } is provided
as an input together with the cluster C, which we wish to
label. Each labeler L ∈ L takes a cluster C as an input
and may suggest a pool of total of nL distinct candidate
cluster labels L(C). Each candidate label l ∈ L(C) is scored
by labeler L according to its (internal) “belief” on how well
label l represents the main topic of cluster C.
Let SL (l|C) denote the score assigned to label l ∈ L(C)
by labeler L ∈ L and let L[k] (C) denote the list of top-k
scored labels. In addition, let rl (L(C)) denote the rank of
label l ∈ L(C) according to its relative score SL (l|C).

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for prof t or commercial advantage and that copies bear this notice and the full citation on the f rst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specif c permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00.
http://dx.doi.org/10.1145/2600428.2609465.

2.2

Cluster Labeling Fusion

We now present a novel fusion approach, tailored to the
cluster labeling task. We first describe how a given cluster
labeler’s decisiveness about some suggested cluster label can
be effectively estimated. We then shortly describe how the

883

For a given label position 1 ≤ q ≤ k, let L[q] (Ci,j ) =
L (Ci ) ∩ L[q] (Cj ) denote the intersection between the pair
of (ranked) label lists, considering only thosenlabels that are
o

new estimate can be used to combine several cluster labelers
within state-of-the-art fusion methods.

2.2.1

[q]

ranked at some position 1 ≤ q ′ ≤ q. Let I l ∈ L[q] (Ci,j )
be an indicator, given the value of 1 iff label l is included in
the intersection L[q] (Ci,j ) (i.e., when positioned in both lists
at most at position q), otherwise 0. Further following [8],
for any pair of sub-clusters Ci , Cj ∈ Cθ labeled by labeler
L, the corresponding expected (overall) agreement between
their corresponding top-k label lists L[k] (Ci ) and L[k] (Cj ) is
given by:

Cluster Labeler Decisiveness Estimation

For a given cluster labeler L ∈ L and a label l ∈ L[k] (C)
suggested by L and ranked at position rl (L[k] (C)), we now
derive an estimate of the decisiveness of labeler L with respect to that specific label choice at that specific position. We hypothesize that, a cluster labeler’s labeling choice
for a given cluster should remain stable even in the presence
of a slightly incomplete cluster data.
The stability estimate is derived by measuring what effect
may slight changes applied on the original input cluster C
have on the labeler L’s labeling decisions. Such change effects are simulated in the form of incomplete versions of the
cluster C. For that, several sub-clusters are sampled, each
contains a subset of documents from the original cluster C.
That is, given a noise level θ ∈ [0, 1], a sub-cluster Ci ⊂ C is
sampled by (randomly) choosing (1 − θ) × |C| of the original
cluster C’s documents, where |C| denotes the number of documents in cluster C. Note that, the random noise θ should
be kept as limited as possible, not to drastically effect each
sub-cluster Ci ’s coherency with the original cluster C [1].
Overall, N (random) sub-clusters Cθ = {C1 , C2 , ..., CN } are
sampled.
For a given sub-cluster Ci ∈ Cθ , let L[k] (Ci ) be the corresponding list of top-k labels suggested by labeler L for Ci .
Inspired by the idea of cluster consensus [10], the labeler
L’s decisiveness with respect to a given label l ∈ L[k] (C)
(position) choice is derived by measuring the amount of
agreement among the sampled sub-clusters’ top-k label lists
L[k] (Ci ) with that label (position) choice. The higher the
agreement, the more we claim that labeler L is decisive with
respect to its original choice of l as the label of cluster C.
Following [10], the extent of agreement is measured by
averaging the pairwise agreements between N (N − 1)/2
possible pairs of the sampled sub-clusters’ top-k label lists
L[k] (Ci ). Lets now assume that a given label l ∈ L[k] (C) is
ranked by labeler L at some position q (i.e., rl (L[k] (C)) = q).
For a given pair of sub-clusters Ci , Cj ∈ Cθ , the pairwise
agreement between the two corresponding top-k sub-cluster
label lists, L[k] (Ci ) and L[k] (Cj ), with respect to that label
is confirmed by checking that: (1) label l is also included
in both top-k label lists L[k] (Ci ) and L[k] (Cj ) (2) label l
is further positioned at most at position q in both lists
(i.e., rl (L[k] (Ci )) ≤ q ∧ rl (L[k] (Cj )) ≤ q). The more such
agreements are gathered for label l, the more it implies that
labeler L may be decisive with respect to that specific label
choice.
Noting that a “local” measurement of the pairwise-label
list agreement based on a single label’s position may not
be robust enough by itself, we further measure the (overall)
expected agreement between the two lists. For that, we
adapt the stability index of [8], previously suggested in the
context of sequential forward selection (SFS) methods [8].
Following [8], the expected agreement between any pair of
label lists L[k] (Ci ) and L[k] (Cj ) is measured relatively to
their intersection size. Such intersection was shown to follow
an hypergeometric distribution and the expected agreement
is derived according to the (normalized) difference between
the expected and the observed intersection size [8].

L

def

ϕi,j (k) =

|L[k] (Ci,j )| · nL − k2
k · (nL − k)

(1)

Note that, in the case of a full agreement (i.e., |L[k] (Ci,j )| =
L
k): ϕL
i,j (k) = 1; while in the case of no agreement: ϕi,j (k) →
−1 as k → n2L .
Finally, the cluster labeler L’s decisiveness with respect to
a given label choice l ∈ L[k] (C) at position rl (L[k] (C)) = q
is given by the average pairwise-list agreements:
h
i
L
n
o
X ϕi,j (k) + 1
1
[q]
× I l ∈ L (Ci,j ) (2)
wL (l|C) =
2N (N − 1) i,j
2
def

Therefore, a label l that has a high consensus about its
specific position in L[k] (C) among highly agreeable pairs of
lists hL[k] (Ci ), L[k] (Cj )i is estimated to be a more “reliable”
label for cluster C based on labeler L’s decisions.

2.2.2

Fusion Methods

We now shortly describe how the new estimate can be
used to combine several cluster labelers within state-of-theart fusion methods.
S
Let L[k] (C) = L∈L L[k] (C) denote the overall label pool
based on the union of all top-k scored labels suggested by
each labeler L ∈ L for cluster C. Our goal, is therefore, to
find a combined cluster labeling (fusion) score, such that the
top-k labels returned by scoring labels l ∈ L[k] (C) according
to that score may result in a better cluster label suggestion.
As a proof of concept, we now introduce two baseline
state-of-the-art data fusion methods, frequently used for various information retrieval tasks, namely the CombSUM and
CombMNZ fusion methods [14].
norm
Given a label l’s score SL (l|C), let SL
(l|C) further denote its normalized score. The CombSUM fusion method
then simply sums over the normalized label scores given by
the various labelers in L [14]:
X norm
CombSU M (l|L[k] (C)) =
SL
(l|C),
(3)
L∈L

norm
where for any labeler L ∈ L, if l 6∈ L[k] (C), then SL
(l|C) =
0 [14].
The CombMNZ method further boosts labels based on
the number of top-k label lists that include each label [14]:
n
o X
norm
CombM N Z(l|L[k] (C)) = # l ∈ L[k] (C) ×
SL
(l|C)
L∈L

(4)
Finally, the two baseline fusion methods are now extended
by further boosting the original (normalized) label score of
each labeler according to the labeler’s estimated decisiveness

884

as follows1 :
def

CLD
norm
SL
(l|C) = wL (l|C) × SL
(l|C)

3.
3.1

method. The core idea behind the SP method is to map important terms that were extracted by a given direct labeler
(e.g., JSD terms) and are supposed to represent the cluster, to Wikipedia categories that may better capture the
cluster’s main topic [2]. Such mapping is done by first submitting the list of top-k important cluster terms as a query
to an inverted index of Wikipedia documents. Then, using a voting approach, cluster labels are chosen by picking
those categories that obtained the highest votes, relatively to
the scores propagated from relevant Wikipedia documents to
their associated categories [2]. Cluster labels obtained from
Wikipedia’s categories were shown to better agree with human labelers, sometimes even providing labels that cannot
be found in the content of the cluster’s documents [2].
The top-k labels suggested by each labeler (i.e., JSD and
SP methods) were further combined using the various baseline and extended fusion methods that were described in
Section 2.2.2. As additional baselines, we choose the CombMAX and Borda-Count fusion methods [14].
Recall that, a cluster labeler’s decisiveness estimation depends on two parameters which are θ (the amount of cluster
noise) and N (the number of random sub-cluster samples).
Trying to keep the evaluation as robust as possible, the two
parameters were tuned using the (relatively small) 20NG collection, while the large ODP collection (with 150 clusters)
was left untouched for testing purposes only. The best parameter configuration was therefore: θ = 0.05 and N = 20.
Finally, for each ODP cluster, the top-20 (i.e., k = 20)
labels suggested by each cluster labeling (fusion) method
were judged for correctness.

(5)

EVALUATION
Data Collections

We evaluated the proposed cluster labeling fusion methods using two sources of clusters data, previously also used
in other works on cluster labeling [13, 2]. The first is based
on the 20 News Group (20NG) collection, containing documents that were manually classified into 20 different categories (each category with about 1000 documents). The second is a data collection that was gathered using the Open
Directory Project (ODP) RDF dump2 . For that, a random sample of documents from 150 different ODP categories
(each category with about 30-100 documents) was obtained
by crawling their contents from the web. Gathered ODP
clusters (categories) have diverse topics, including among
others topics related to arts, technology, business, science,
etc.

3.2

Label Quality Measurements

We closely followed previous cluster labeling evaluation
frameworks [13, 2]. Therefore, for each given cluster, its
ground truth labels used for the evaluation were obtained
by manual (human) labeling. According to [13, 2], a label
suggested by some labeler for some cluster should be considered as correct if it is “identical, an inflection, or a Wordnet
synonym of the cluster’s correct label” [2].
The Match@k and MRR@k (Mean Reciprocal Rank)
label quality measures were used for the evaluation [13, 2].
The two measures evaluate a given labeler’s capability of
providing a single correct label for a given cluster, which
best describes the cluster’s main topic. The Match@k measure returns 1 iff at least one correct label is located among
the top-k labels proposed by the labeler. The MRR@k measure, on the other hand, returns the inverse of the rank of
the first correct label in the top-k list. Otherwise, both measures return the zero value.

3.3

3.4

Studied Cluster Labeling Methods

To evaluate the relative performance of the various cluster
labeling fusion methods that were described in Section 2.2.2,
two baseline cluster labelers were implemented. The first
is a direct cluster labeler, termed hereinafter as the JSD
method, previously employed by [2] and is based on the
query difficulty model of [3]. Cluster terms are scored by
the JSD method according to their relative contribution to
the Jensen-Shannon divergence between the cluster and the
whole collection [3, 2]. The top-k scored terms are then
suggested as the cluster’s labels [2]. The JSD method has
been shown to be superior to several other state-of-the-art
direct cluster labelers [2].
The second, termed hereinafter the Score Propagation
(SP) method, is an indirect cluster labeler proposed by [2]
which utilizes the Wikipedia’s categories for cluster labeling. The SP method was shown to provide a superior performance to that of several other direct and indirect cluster labelers [2]; and among those, to that to of the JSD
1
2

Results

The results of our evaluation are depicted in Fig. 1. The
two extended fusion methods are further named as CombSUM(CLD) and CombMNZ(CLD) in Fig. 1. Overall, the
extended fusion methods provided the best performance for
both label quality measures. The CombMNZ(CLD) method
was the most superior among all methods, providing a statistically significant improvement3 over the best performing baseline cluster labelers (i.e., JSD or SP), with an average improvement (over k) of 6.5(±0.4)% and 8(±1.6)% for
the MRR@k and Match@k measures, respectively.
We can further observe that, the overall cluster labeling
quality of the two baseline labelers (i.e., JSD and SP) varies
with k, the number of top suggested labels. For the MRR@k
measure, the SP method demonstrated a consistent superiority over the JSD method, supporting similar results that
were previously reported by [2] for this measure. On the
other end, we observe that, a similar superiority of the SP
method over the JSD method that was also previously reported for the Match@k measure in [2] is not supported by
our evaluation. While for k ≤ 5, the SP method provided
a better performance, for k ≥ 6, we observe an opposite
trend4 .
Such inconclusive result of which baseline cluster labeler
to prefer (i.e., JSD or SP) actually serves as the main motivation of our work. As can be further observed in Fig.
1, no “traditional” baseline fusion method has managed to
provide consistent improvements over the baseline cluster la3

paired t-test, p-value < 10−11 .
We note that, such differences in the reported results are
actually possible, as we may have used a different (and a
larger set by 50%) collection of ODP clusters to that of [2].
4

CLD initials stand for “Cluster Labeler Decisiveness”
http://rdf.dmoz.org/

885

0.75

1
0.95

0.7

0.9

0.65

0.85

0.6

0.75

Match@K

MRR@K

0.8

0.55
CombMNZ(CLD)
CombSUM(CLD)
CombMNZ
CombSUM
SP
JSD
CombMAX
Borda Count

0.5
0.45
0.4

0.7
0.65
CombMNZ(CLD)
CombSUM(CLD)
CombMNZ
CombSUM
SP
JSD
CombMAX
Borda Count

0.6
0.55
0.5
0.45
0.4

0.35

0.35

1

2

3

4

5

6

7

8

9

10

11 12

13

14 15

16 17

18 19

20

1

K

2

3

4

5

6

7

8

9

10 11

12

13 14

15 16

17

18 19

20

K

(a) MRR@k

(b) Match@k

Figure 1: Average MRR@k and Match@k values obtained for the 150 ODP clusters by the two baseline
cluster labelers (JSD and SP), the baseline fusion methods and the extended fusion methods enhanced with
the cluster labeler decisiveness (CLD) estimate. For each k, the values reported for the CLD-based fusion
methods are statistically significant (paired t-test, p-value < 10−11 ).
belers compared to the new extended fusion methods. This
serves as another evidence of the usefulness of using the new
cluster labeler decisiveness (CLD) estimate for boosting the
cluster labeling performance.

4.

[7]

REFERENCES

[1] CharuC. Aggarwal and ChengXiang Zhai. A survey of
text clustering algorithms. In Charu C. Aggarwal and
ChengXiang Zhai, editors, Mining Text Data, pages
77–128. Springer US, 2012.
[2] David Carmel, Haggai Roitman, and Naama
Zwerdling. Enhancing cluster labeling using wikipedia.
In Proceedings of the 32Nd International ACM SIGIR
Conference on Research and Development in
Information Retrieval, SIGIR ’09, pages 139–146, New
York, NY, USA, 2009. ACM.
[3] David Carmel, Elad Yom-Tov, Adam Darlow, and
Dan Pelleg. What makes a query difficult? In
Proceedings of the 29th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, SIGIR ’06, pages 390–397, New
York, NY, USA, 2006. ACM.
[4] Jackie Chi Kit Cheung and Xiao Li. Sequence
clustering and labeling for unsupervised query intent
discovery. In Proceedings of the Fifth ACM
International Conference on Web Search and Data
Mining, WSDM ’12, pages 383–392, New York, NY,
USA, 2012. ACM.
[5] Douglass R. Cutting, David R. Karger, Jan O.
Pedersen, and John W. Tukey. Scatter/gather: A
cluster-based approach to browsing large document
collections. In Proceedings of the 15th Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR ’92,
pages 318–329, New York, NY, USA, 1992. ACM.
[6] Eric Glover, David M. Pennock, Steve Lawrence, and
Robert Krovetz. Inferring hierarchical descriptions. In
Proceedings of the Eleventh International Conference

[8]

[9]

[10]

[11]

[12]

[13]

[14]

886

on Information and Knowledge Management, CIKM
’02, pages 507–514, New York, NY, USA, 2002. ACM.
Ioana Hulpus, Conor Hayes, Marcel Karnstedt, and
Derek Greene. Unsupervised graph-based topic
labelling using dbpedia. In Proceedings of the Sixth
ACM International Conference on Web Search and
Data Mining, WSDM ’13, pages 465–474, New York,
NY, USA, 2013. ACM.
Ludmila I. Kuncheva. A stability index for feature
selection. In Proceedings of the 25th Conference on
Proceedings of the 25th IASTED International
Multi-Conference: Artificial Intelligence and
Applications, AIAP’07, pages 390–395, Anaheim, CA,
USA, 2007. ACTA Press.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schütze. Introduction to Information
Retrieval. Cambridge University Press, New York, NY,
USA, 2008.
Nam Nguyen and Rich Caruana. Consensus
clusterings. In Proceedings of the 2007 Seventh IEEE
International Conference on Data Mining, ICDM ’07,
pages 607–612, Washington, DC, USA, 2007. IEEE
Computer Society.
Zareen Saba Syed, Tim Finin, and Anupam Joshi. In
Proceedings of the Second International Conference on
Weblogs and Social Media, ICWSM ’08. The AAAI
Press, 2008.
Hiroyuki Toda and Ryoji Kataoka. A clustering
method for news articles retrieval system. In Special
Interest Tracks and Posters of the 14th International
Conference on World Wide Web, WWW ’05, pages
988–989, New York, NY, USA, 2005. ACM.
Pucktada Treeratpituk and Jamie Callan.
Automatically labeling hierarchical clusters. In
Proceedings of the 2006 International Conference on
Digital Government Research, dg.o ’06, pages 167–176.
Digital Government Society of North America, 2006.
Shengli Wu. Data fusion in information retrieval,
volume 13. Springer, 2012.

