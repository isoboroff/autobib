Web Search without ’Stupid’ Results
Aleksandra Lomakina

Nikita Povarov

Pavel Serdyukov

Yandex
Moscow, Russia

{s-lomakina, saintnik, pavser}@yandex-team.ru

ABSTRACT

tems actually compete with each other in finding the best
answers among good ones by using different methods, e.g.,
personalization, when they try to show users the documents
perfectly answering their initial queries. However, even having provided the user with the requested result, we should
not forget that she may finish the search annoyed and frustrated [1]. Such frustration may be, of course, caused by
the presence of irrelevant documents in the result list [4].
Still, even such documents can be related to the query’s
topic to some extent. For example, here is the definition
of irrelevant documents in TREC 2010 Web Track Ad-hoc
task: “The content of this page does not provide useful information on the topic, but it may provide useful information
on other topics, including other interpretations of the same
query” [3]. “Spam” and “Junk” documents, which serve no
purpose other than to abuse search engines with keyword
stuffed pages and to deceive users in different ways, if presented to the users among search results might also lead to
the loss of users’ trust to the search engine.
However, there can be another reason for frustration: the
irrelevant and non-spam documents, which do not only lack
any answer to the query, but also make users bewildered and
puzzled about how such answers could have ever been found
by the search engine for this particular query, even considering the variety of its interpretations. Such “extremely”
irrelevant results, in contrast to “moderately” irrelevant documents, do not contain any information about user queries
and are completely out of the query’s subject area (e.g. documents about King Henry VIII diseases on the “stephen king
insomnia” query). If such documents appear in search results, they can be harmful as they make users think about
severe inferiority of search mechanisms and the search engine in general [10]. Some users may even click such pages
out of pure curiosity, while others may stop searching at
all or go to other search systems in order to continue their
search there (and some of such users would not ever come
back [8]). Some users may share examples of “stupidity” of a
search system in social networks (and, in this case, the negative influence will be spread to other users). In our study
we decided to refer to such extremely irrelevant documents
as “stupids” meaning that they make an impression of “stupidity” of a search system. There is a serious threat in the
fact that negative effect of frustration tends to accumulate.
For example, Song et al [13] observed that relevance degradation can lead to user behavior changes and deterioration
of user attitude toward the search engine on long term.
Motivated by the above-described intuitions, we consider
that it is necessary to allocate “stupid” documents to a sep-

One of the main targets of any search engine is to make
every user fully satisfied with her search results. For this
reason, lots of efforts are being paid to improving ranking
models in order to show the best results to users. However,
there is a class of documents on the Web, which can spoil
all efforts being shown to the users. When users receive
results, which are not only irrelevant, but also completely
out of the picture of their expectations, they can get really
frustrated. So, we attempted to find a method to determine
such documents and reduce their negative impact upon users
and, as a consequence, on search engines in general.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information
Search and Retrieval

General Terms
Classifier Learning, Online experiments

Keywords
user frustration, classifier, search results ranking, searcher
behavior

1.

INTRODUCTION

With the improvement of search engines, more and more
various types of documents’ relevance labels and relevance
scales appear: topical relevance, freshness, readability, etc.
On the one hand, they are used for training ranking algorithms in order to improve the search engine’s ranking and,
on the other hand, they play an important role in evaluation of systems’ quality. The most commonly used relevance
scales are the following: binary scale[14] and 5-grade scale
(Nav (grade 4), Key (grade 3), HRel (grade 2), Rel (grade
1), Non (grade 0), Junk (grade 0)) [3]. It is worth mentioning that, traditionally, the documents which are even
slightly related to the users’ intent are allocated into one
of the first 4 groups of the latter scale, whereas all nonspam irrelevant documents get the “Non” label. Search sysPermission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00.
http://dx.doi.org/10.1145/2600428.2609480.

943

arate subclass (hence, using 7 relevance labels instead of 6
used in [3]) and find ways to minimize their impact upon
users. On the one hand, our study is aimed at training the
classifier to make it capable of separating “stupids” from irrelevant documents, as well as other documents. On the
other hand, the study is aimed at using the classifier for reducing the impact of “stupid” documents upon users. There
are several methods of using such a classifier, and our study
focused on two of them: filtering-out “stupid” results from
SERPs and reducing their weight in rankings. In order to
understand, whether any users are affected by the changes
in the rankings obtained in these two ways, the search results produced with the help of that classifier were evaluated
first with offline metrics (we had to make sure that the initial ranking has not been degraded in terms of the 5-grade
scale) and then with online metrics.
The remainder of the paper is organized as follows. The
next section describes the classifier and the methods we use
for dealing with “stupid” documents in search results. Our
experimental results are presented in Section 3. Section 4
concludes the paper and describes a few directions for the
future work.

Table 1: Feature ablation experiments
Single feature Without feature
Feature groups
group (AUC)
group (AUC)
Query
0.7689
0.8878
Link
0.7513
0.826
Text
0.7692
0.8880
All
0.9337
• Text features: based on page-content properties: the
length of the document, n-gram representation, etc.
(e.g. occurrence of synonyms of query terms);
• Links features: designed to consider the quality, frequency and other properties of external references to
documents, etc. (e.g. the number of query words in
external reference);
• Other features: these are features that do not belong
to any of the categories above (e.g. web-page language,
Type of the document);

In this section, we introduce our approach. The first part
contains the details of our classifier construction, which was
used to change search results in two ways: by re-ranking and
by filtering.

Table 1 shows the prediction performance,in terms of the
popular Area Under Curve (AUC) measure, when a single
feature group or all feature groups without one group were
used in training.
In total, we use over 400 features and 10000 trees in training of this classifier and split up the dataset into training
(90%) and test (10%) sets. As a result, we have a binary
classifier with AUC = 0,9337.

2.1 Classifier of “stupid” documents

2.2

2.

METHODOLOGY AND DATA

At first, to reduce the number of stupid results displayed
to the users and to decrease the effect produced by such
results, we had to find out how to identify such documents
automatically.
Our dataset contained a sample from the logs of a large
commercial search engine Yandex. The logs contained queries
and search results returned by the engine. These queryURL pairs were manually labeled using the 6 relevance labels described in [3] by experts, who paid attention to documents’ relevance depending on all most probable query intents. After that, we chose documents with “Non” labels
and additionally asked the experts to divide them into two
groups: “Non” and “Stupid”. ”Stupid” labels were assigned
to documents, which had no connection to the query topic,
including all imaginable interpretations of the same query,
and made the puzzled experts wonder how could such answers have ever been found by a search engine. Other documents were left with “Non” labels. For our dataset we
used about 113,000 queries, which provided 2,762,098 judged
query-URL pairs. The share of “stupids” among all “Non”
documents was 6.7%. For the purpose of the classifier training we split up the relevance labels and used the binary classification: “Stupid” vs. all labels. The classifier was trained
using a proprietary implementation of the gradient boosted
decision trees algorithm [5] - MatrixNet1 . We have divided
all features into groups and conducted a series of feature
ablation experiments.
In general, we had four groups of features, which basically
correspond to a subset of features used to train the ranking
algorithm of the search engine under study:
• Query features: based on the different query properties: language, number of words, frequency, category,
etc. (e.g. is query from a “long-tail” class);
1

Combining the ranker and the classifier

Filtering.
One of the ways to get rid of “stupid” documents in search
results is to exclude them once they are detected with our
classifier. This method is aimed directly at reducing the
amount of such documents and, as a result, their impact on
users. Thus, what we get is not just an ordinary re-ranking
when users still have a chance to stumble upon “stupids” in
their search results, it is about “cleaning-up” of search results. However, this method is also the crudest one - there
is a chance to remove not only really “stupid” results, but
also relevant to some degree, as our classifier is, of course,
not perfect. Hence it was essential to determine some appropriate threshold, which allows us to maximize the benefits of
filtering and to minimize any possible damage. The results
of the threshold’s tuning are presented in Section 3.1.

Re-ranking.
Re-ranking is another method of applying the classifier to
the initial results from a ranker. It relies on demoting documents which are highly probable to be “stupid”. Thereby
it is also about “cleaning” the top of search results, but still
there is a possibility of finding such documents in the results on other pages for those who may need them. For
the purpose of this study we trained a re-ranking method,
which lowers the initial scores of documents, depending on
their classifier’s probability to be “stupid” under otherwise
equal conditions. In other words, the new document scores
were obtained by linearly combining the initial scores and
the probabilities of documents to be “stupid.”

3.

EVALUATION

In order to find out, whether our methods result into any
profit to the search engine or at least do not lead to its
performance deterioration, we first evaluated filtering and

http://api.yandex.com/matrixnet

944

re-ranking by offline search quality metrics. Then we conducted some online experiments to determine whether there
are any effects for users. The proprietary spam filtering
algorithms had been used to remove “spam” and “junk” documents from search results before all evaluations and experiments started. Our baseline involves no re-ranking or
filtering of “stupids”.

Table 2: Evaluation of filtering. Differences in ERR
and Stupids’ share with different thresholds.
”Common” set
”Stupids” ”Stupids”
Threshold
ERR
top 5
top 10
0,6
-0.57%*
-6.43%
-2.62%
0,65
0.12%
-6.43%
-3.2%
0,7
0.08%
-7.14%*
-7.56%*
0,75
0.02%
-1.43%
-3.2%*
0,8
0.0007%
-0.71%
-0.29%

3.1 Offline evaluation
To evaluate our approach, we additionally collected two
sets of queries sampled from Yandex query log. One of them,
named “long-tail”, contained 1500 low-frequency queries2 ,
which are often hard to answer. Therefore, search engines
show many irrelevant and “stupid” results in SERPs for such
queries. Another set, named “common”, consists of 900
queries sampled randomly from the query log.
As the quality evaluation metric, we chose the Expected
Reciprocal Rank (ERR)[2] the primary effectiveness measure used in TREC [3]. We computed the ERR (see Equation 1) of search results with respect to the experts’ labels
and the 5-grade relevance scale. Since “Non”, “Stupid” and
“Junk” documents do not satisfy users, we used their relevance labels with the same zero-weight (zero grade for “Non”
and “Junk” documents was also used in official TREC evaluations [3]). The main target of such evaluation is to make
sure, that our classifier is careful enough to deal with irrelevant documents only. So we did not use negative grades
for “stupids” in this study as we did not want to get false
positive growth in ERR.
i−1
k
X
R(gi ) Y
(1 − R(gi ))
(1)
ERR@k =
i j=1
i=1

Threshold
0,6
0,65
0,7
0,75
0,8

Set

ERR

Common
Long-tail

-0.4%
-0.17%

”Stupids”
top 5
-13.64%*
-9.11%**

”Stupids”
top 10
-14.81%**
-9.34%**

the ranking quality in terms of relevance metrics. On the
one hand, these results allow us to confirm the hypothesis
that such a method of re-ranking does not lead to a significant degradation of top results’ quality. It means that good
documents are not demoted to the bottom of search engine
result pages. On the other hand, there is only one significant improvement that can be seen - the share of “stupid”
documents at the top has really decreased.
However, we also conducted a series of online experiments
to find out how our changes of search results really affect
the behavior of users. For online evaluation we used “reranking” as it removes more “stupid” documents from rankings according to Tables 2 and 3.

Filtering Evaluation.
In order to use the classifier as a filter which removes only
“stupid” documents from the search results, it was necessary
to choose some appropriate threshold. We have established
the following criteria for the threshold: there should be a
significant negative difference in the share of “stupid” documents and zero or positive growth of ERR. The classifier
returns the document’s probability of being “Stupid” in the
range of [0,1]. The probabilities from 0.1 to 0.9 with step of
0.05 were estimated. As Table 2 shows, a good decision is to
choose the threshold of 0.7 - there is no significant difference
in search results quality, but there is a significant3 reduction
of “stupid” documents in search results.

3.2

Online evaluation

Team-Draft Interleaving.
Firstly we compared our production system with “antistupid” re-ranking and the production ranking system without the re-ranking, using Team Draft Interleaving. The
main idea of this method is to blend the results from two
search rankings to get one interleaved ranking without duplicates. The system that gets the majority of clicks over
many queries and users is considered a better one [11]. We
performed this experiment for a week in September 2013,
presenting the interleaved ranking for more than a million
queries. As a result, our system with re-ranking was preferred more than the baseline system for 1,5%∆ of all queries
and for 1.87%∆ queries from the long-tail. These differences
are statistically significant4 .

Re-ranking Evaluation.
To measure the effectiveness of our “anti-stupid” re-ranking,
we used the same sets of queries. We assumed that the
search engine was good enough in ranking results, hence
we did not expect to see significant relevance improvements
here. However we do need to validate that re-ranking does
not tend to reduce the number of relevant documents and
lowers the scores of “Non” documents only. The ERR results
in Table 3 show that our changes do not significantly degrade
3

”Stupids”
top 10
-8.18%**
-7.27%**
-3.32%**
-2.3%**
-0.53%**

Table 3: Evaluation of Re-ranking. Differences in
ERR and Stupids’ share.

where R(g)= 2g−1
and g1 , g2 , ..., gk are the relevance grades
16
associated with the top-k documents.
Besides we calculated the share of “stupid” documents in
the top-5 and top-10 of search results. Thus, we assessed
whether there is a significant reduction in the number of
“stupid” documents in the filtered search results.

2

”Long-tail” set
”Stupids”
ERR
top 5
-3.55%** -9.86%**
-2.12%*
-5.94%**
-1.1%
-3.65%**
-0.73%
-1.91%**
-0.02%
-0.32%*

A/B Testing.
We also conducted another online experiment, described
by Kohavi et al. [9] Equal portions of online users were
randomly assigned to the “control” group (baseline ranking

asked less than 300 times in two months
** - p-value < 0,01; * - p-value < 0,05. T-test was used

4

945

∆- p-value < 0.001; Bootstrap test was used

was shown for these users) and the “treatment” group (the
results of the production system with re-ranking were shown
to these users). We performed this experiment within two
weeks in September 2013.
As a result of the experiment, we observed that the clickthrough rates (CTR) had significantly5 grown up (0,97%‡ on
the average) for all positions for the treatment group. Just
like clicks in TDI, it also demonstrates that users started
getting into more interactions with the SERPs generated
by the re-ranking. Also positive changes were observed for
the following click metrics: “position of the first click” (0.71%‡ ), “position of the first long click6 ” (-0.43%‡ ). It
means that top search results also became more attractive
to users. Whereas, “Abandonment Rate” and “Time to first
click” metrics stayed without any significant changes.
Based on the results of the conducted experiments we are
able to draw a conclusion that the demotion of “stupids”
have positive impact on absolute click metrics. In addition,
along with the changes in the mentioned metrics, we have
noticed one more change in users behavior in the treatment
group: their dwell time after click decreased. It may indicate
changes not only in the number of user actions, but also in
their character. It makes us think that transformation of
user attitude towards the changed search system in general
should also be studied in addition to click metrics.

4.

training different classifiers for those groups of queries. Probably, it is worth trying to use a dedicated classifier of moderately irrelevant results together with stupids’ classifier to
perform a higher quality pessimization of all irrelevant documents.
3) During our experiment we have noticed changes not
only in the number and positions of users’ clicks, but also in
“dwell time” after clicks as well. It motivates us to study further the behavior of users who faced the re-ranking/filtering.
It is worth measuring user satisfaction/frustration not only
by looking at the changes in clicking behavior, but also looking at the changes in the general attitude, probably with a
questionnaire and a controlled experiment. In our future
work we plan to also analyze which impact can such ranking changes have upon the periods of user absence in the
search system [6] and the probability of user switching to
another search engine [12, 7].

5.

CONCLUSION AND FUTURE WORK

In the scope of the present study we aimed at investigating whether reducing of the number of ”stupid” documents
in search results has any impact on users. To answer this
research question, we trained a classifier and used it for filtering and re-ranking. It should be noted, that with the use of
the generally accepted 5-grade relevance scale such ranking
modifications are not visible in the results of the ERR metric. In fact, two systems, having no significant differences
in ERR results, might be perceived by users differently. We
demonstrate this with the results of two different online experiments. Applying the classifier for filtering or decreasing
of “stupid” documents’ scores caused changes in users behavior: they started making more clicks on changed SERPs.
We observed significant improvements in terms of click metrics in spite of the fact that only “stupid” documents have
been removed/demoted, leaving the irrelevant ones intact.
As a result, we may conclude that it is necessary to distinguish stupids from not so extremely irrelevant documents
and grade them differently too to let rankers prefer moderately irrelevant documents over stupids.
Relying on our findings, we consider the following directions for our future work:
1)Along with allocating “stupids” to a separate subclass,
we suppose that it is useful to introduce an additional grade
for “stupid” documents into relevance scale. We are going to
try to train list-wise learning to rank algorithms taking this
grade into account. It is also necessary to give additional
instructions to experts: user psychology causing different
reactions in case of “stupids” and moderately irrelevant documents should be taken into account.
2) It is crucial to spot “stupids”, but they should be removed correctly. We plan to study focused stupids filtration
schemes for some groups of queries (e.g. dividing them into
navigational/informational or commercial/regional) and try
5
6

REFERENCES

[1] I. Ceaparu, J. Lazar, K. Bessiere, J. Robinson, and
B. Shneiderman. Determining causes and severity of
end-user frustration. International Journal of
Human-Computer Interaction, 2004.
[2] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan.
Expected reciprocal rank for graded relevance. In
Proc. CIKM, pages 621–630, 2009.
[3] C. L. A. Clarke, N. Craswell, I. Soboroff, and G. V.
Cormack. Overview of the trec 2010 web track. In
Proc. TREC, 2010.
[4] H. A. Feild, J. Allan, and R. Jones. Predicting searcher
frustration. In Proc. SIGIR, pages 34–41, 2010.
[5] J. H. Friedman. Stochastic gradient boosting. Comput.
Stat. Data Anal., 38(4):367–378, Feb. 2002.
[6] Q. Guo and E. Agichtein. Beyond dwell time:
Estimating document relevance from cursor
movements and other post-click searcher behavior. In
Proc. WWW, pages 569–578, 2012.
[7] Q. Guo, R. W. White, Y. Zhang, B. Anderson, and
S. T. Dumais. Why searchers switch: Understanding
and predicting engine switching rationales. In Proc.
SIGIR, pages 335–344, 2011.
[8] V. Hu, M. Stone, J. Pedersen, and R. W. White.
Effects of search success on search engine re-use. In
Proc. CIKM, pages 1841–1846, 2011.
[9] R. Kohavi, R. Longbotham, D. Sommerfield, and
R. M. Henne. Controlled experiments on the web:
Survey and practical guide. Data Min. Knowl.
Discov., 18(1):140–181, Feb. 2009.
[10] A. Kohn. Wtf! @ k: Measuring ineffectiveness, 2012.
[11] F. Radlinski, M. Kurup, and T. Joachims. How does
clickthrough data reflect retrieval quality? In Proc.
CIKM, pages 43–52, 2008.
[12] D. Savenkov, D. Lagun, and Q. Liu. Search engine
switching detection based on user personal preferences
and behavior patterns. In Proc. SIGIR, pages 33–42,
2013.
[13] Y. Song, X. Shi, and X. Fu. Evaluating and predicting
user engagement change with degraded search
relevance. In Proc. WWW, pages 1213–1224, 2013.
[14] E. M. Voorhees. Overview of trec 2001. In Proc.
TREC, 2001.

‡- p-value < 0.001; Mann-Whitney test was used
clicks followed by no further clicks for 30 seconds or more

946

