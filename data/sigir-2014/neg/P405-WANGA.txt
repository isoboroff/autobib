Active Hashing with Joint Data Example and Tag Selection
Qifan Wang, Luo Si, Zhiwei Zhang and Ning Zhang
Department of Computer Science
Purdue University
West Lafayette, IN 47907, US

{wang868, lsi, zhan1187, zhan1149}@purdue.edu

ABSTRACT

General Terms

Similarity search is an important problem in many large
scale applications such as image and text retrieval. Hashing
method has become popular for similarity search due to its
fast search speed and low storage cost. Recent research has
shown that hashing quality can be dramatically improved
by incorporating supervised information, e.g. semantic
tags/labels, into hashing function learning. However, most
existing supervised hashing methods can be regarded as
passive methods, which assume that the labeled data are
provided in advance. But in many real world applications,
such supervised information may not be available.
This paper proposes a novel active hashing approach,
Active Hashing with Joint Data Example and Tag Selection
(AH-JDETS), which actively selects the most informative
data examples and tags in a joint manner for hashing
function learning. In particular, it first identifies a set
of informative data examples and tags for users to label
based on the selection criteria that both the data examples
and tags should be most uncertain and dissimilar with
each other. Then this labeled information is combined
with the unlabeled data to generate an effective hashing
function. An iterative procedure is proposed for learning the
optimal hashing function and selecting the most informative
data examples and tags. Extensive experiments on four
different datasets demonstrate that AH-JDETS achieves
good performance compared with state-of-the-art supervised
hashing methods but requires much less labeling cost,
which overcomes the limitation of passive hashing methods.
Furthermore, experimental results also indicate that the
joint active selection approach outperforms a random (nonactive) selection method and active selection methods only
focusing on either data examples or tags.

Algorithms, Performance, Experimentation

Keywords
Hashing, Active Learning, Similarity Search, Data Selection

1.

INTRODUCTION

Similarity search is a key problem in many information
retrieval applications including image and text retrieval,
content reuse detection and collaborative filtering. The
purpose of similarity search is to identify similar data
examples given a query example. With the explosive growth
of the internet, a huge amount of data such as texts, images
and video clips have been generated, which indicates that
efficient similarity search with large scale data becomes
more important. Traditional similarity search methods
are difficult to be used directly for large scale data since
computing the similarity using the original features (i.e.,
often in high dimensional space) exhaustively between the
query example and every candidate example is impractical
for large applications. There are two major challenges for
using similarity search in large scale data: storing the large
data and retrieving desired data efficiently.
Hashing methods [27, 40, 42, 43] have been proposed for
addressing these two challenges and have achieved promising
results. These hashing methods design compact binary
code in a low-dimensional space for each data example so
that similar data examples are mapped to similar binary
codes. In the retrieval process, these hashing methods
first transform each query example into its corresponding
binary code. Then similarity search can be simply conducted
by calculating the Hamming distances between the codes
of available data examples and the query and selecting
data examples within small Hamming distances. In this
way, the two major challenges for large scale similarity
search can be addressed as: data examples are encoded and
highly compressed within a low-dimensional binary space,
which can usually be loaded in main memory and stored
efficiently. The retrieval process is also very efficient, since
the Hamming distance between two codes is simply the
number of bits that differ, which can be calculated using
bitwise XOR.
Recently, some supervised hashing methods [23, 34, 37]
have incorporated labeled data/information, e.g. semantic
tags, for learning more effective hashing function than
unsupervised hashing methods.
It has been shown
that hashing quality could be dramatically improved by

Categories and Subject Descriptors
H.3.1 [Information Storage and Retrieval]: Content
Analysis and Indexing; H.3.3 [Information Storage and
Retrieval]: Information Search and Retrieval
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
Copyright 2014 ACM 978-1-4503-2257-7/14/07$15.00.
http://dx.doi.org/10.1145/2600428.2609590.

405

leveraging supervised information. For example, in text
retrieval applications, semantic tags (e.g. documents labeled
with the same tag) reflect the semantic relationship between
documents and thus can be very important and helpful
for learning hashing function. However, most existing
supervised hashing methods can be regarded as passive
methods, which assume that the labeled data are provided
beforehand. But in many real world applications, such
supervised information may not be available and it is often
expensive to acquire for a large dataset. Therefore, it is
important to design effective methods to actively identify
only the most informative data examples for users to label.
On the other hand, the labeling cost will also depend
on the total number of tags that the users label to the
selected data examples. In many large scale applications,
there are often hundreds or thousands of tags for users
to label. Moreover, similar tags usually carry similar
semantic meanings. For instance, ‘car’ and ‘automobile’
have similar meanings and choosing both of them may not
gain substantial new information over just selecting one.
Therefore, selecting a small set of most informative tags is
also important to lower the efforts of user labeling.
This paper proposes a novel active hashing approach,
Active Hashing with Joint Data Example and Tag Selection
(AH-JDETS), to achieve the goal of learning accurate
hashing function with a limited amount of labeling efforts.
AH-JDETS actively selects the most informative data
examples and tags in a joint manner for hashing function
learning. Specifically, it first identifies a set of informative
data examples and tags for users to label based on the
selection criteria that both the data examples and tags
should be most uncertain and dissimilar with each other.
Then the supervised information is combined with the
unlabeled data to generate an effective hashing function.
An iterative procedure is proposed for learning the optimal
hashing function and selecting the most informative data
examples and tags. Extensive experiments on four different
datasets have been conducted to demonstrate that AHJDETS can achieve good performance with much less
labeling cost when compared to state-of-the-art supervised
hashing methods, which overcomes the limitations of
passive hashing methods.
Moreover, the experiments
have clearly shown the advantages of the proposed AHJDETS approach against several other selection methods
for obtaining training data.

2.

Principal Component Analysis (PCA) and Latent Semantic
Indexing (LSI) [8, 14]. Existing hashing methods can
be divided into two groups: unsupervised and supervised
hashing methods.
Among the unsupervised hashing methods, LocalitySensitive Hashing (LSH) [7] is one of the most popular
methods, which simply uses random linear projections to
map data examples from a high dimensional Euclidean
space to a low-dimensional binary space. This method
has been extended to Kernelized and Multi-Kernel LocalitySensitive Hashing [19, 41] by exploiting kernel similarity for
better retrieval efficacy. The Principle Component Analysis
(PCA) Hashing [22] method represents each example by
coefficients from the top k principal components of the
training set, and the coefficients are further binarized using
the median value. A Restricted Boltzman Machine (RBM)
[13] is used in [27] to generate compact binary hashing
codes. Recently, Spectral Hashing (SH) [40] is proposed to
design compact binary codes with balanced and uncorrelated
constraints. Self-Taught Hashing (STH) [43] combines an
unsupervised learning step with a supervised learning step to
learn hashing codes. More recently, the work in [42] proposes
a Composite Hashing with Multiple Information Sources
(CHMIS) method to integrate information from different
sources. In work [24], a hyperplane hashing method is
proposed for efficient active learning, which can find nearest
points to a query hyperplane in sublinear time.
For the supervised hashing methods, a Canonical Correlation Analysis with Iterative Quantization (CCA-ITQ)
method has been proposed in [10] which treats the content
features and tags as two different views. The hashing codes
are then learned by extracting a common space from these
two views. Recently, several pairwise hashing methods
have been proposed. The semi-supervised hashing (SSH)
method in [34] utilizes pairwise knowledge between data
examples besides their content features for learning more
effective hashing codes. A kernelized supervised hashing
(KSH) framework proposed in [23] imposes the pairwise
relationship between data examples to obtain good hashing
codes. More recently, a ranking-based supervised hashing
(RSH) [35] method is proposed to leverage listwise ranking
information to improve the search accuracy. Most recently,
the work in [37] proposes a Semantic Hashing method
which combines Tag information with Topic Modeling
(SHTTM) by extracting topics from texts and exploiting the
correlation between tags and hashing codes for document
retrieval. It has been shown that supervised hashing
methods achieve better performance than unsupervised
methods.
However, as aforementioned, most existing
supervised hashing methods can be regarded as passive
methods which assume the labeled data are provided
beforehand but in practice, such supervised information may
not be available or can be expensive to obtain. Therefore,
it is important to design effective methods to actively
identify only the most informative data for users to label
for generating accurate hashing codes with low cost.

RELATED WORK

This section reviews the related work in two research
areas: hashing function learning and active learning.

2.1

Hashing Function Learning

Hashing methods [6, 18, 44, 36, 38] are proposed to
address the similarity search problem within large scale
data. These hashing methods try to encode each data
example by using a small fixed number of binary bits
while at the same time preserve the similarity between data
examples as much as possible. In this way, data examples
are transformed from a high-dimensional space into a
low-dimensional binary space and therefore, the similarity
search can be done very fast [30] by only computing the
Hamming distance between binary codes. Hashing methods
generate binary codes for efficient search, which is different
from traditional dimensionality reduction methods such as

2.2

Active Learning

The purpose of active learning [11, 15, 32] is to select
data examples from an unlabeled pool which will be very
beneficial in training the model, thereby reducing the
labeling cost since noninformative instances are not selected.
Many strategies have been proposed to measure the

406

Figure 1: An overview of the proposed AH-JDETS approach.
informativeness of unlabeled data examples. Uncertainty
sampling [20] selects the data examples whose predicted
labels are the most uncertain.
A batch mode active
learning method is proposed in [15] that applies the Fisher
information matrix to select a number of informative
examples simultaneously based on the criteria that the
selected set of unlabeled examples should result in the
largest reduction in the Fisher information. The work in
[11] proposes a discriminative batch mode active learning
strategy that exploits information from an unlabeled set to
learn a good classifier directly, which obtains high likelihood
on the labeled training instances and low uncertainty on
labels of the unlabeled instances. A comprehensive survey
of active learning can be found in [28].
Recently, active learning has been extended to various
tasks including image classification [15], learning to rank [9,
25], query selection [4, 39] and collaborative filtering [12,
17]. For example, the work in [9] addresses active rank
learning based on expected hinge rank loss minimization.
Inspired by the expected loss reduction strategy, [25]
recently introduces an expected loss optimization framework
for ranking, where the selection of query and documents
is integrated in a principled manner. The work in [39]
generalizes the empirical risk minimization principle to
active learning which identifies the most uncertain and
representative queries by preserving the source distribution
as much as possible. A Bayesian selection approach is
proposed in [17] for collaborative filtering, which identifies
the most informative items such that the updated user model
will be close to the expected user model.
The only work we found using active learning in hashing
is [45], which directly chooses the most uncertain data
examples based on the hashing function. A batch mode
algorithm is also proposed in this work to speed up their
active selection. However, the method in [45] only considers
identifying the most informative data examples and tries
to label all possible tags to these selected examples, which
requires a great amount of labeling efforts for those datasets
associated with a huge number of tags. Therefore, in this
paper, we develop a novel active hashing approach that
jointly selects the most informative data examples and tags
such that the hashing function can be learned efficiently with
only a small number of labeled data, which greatly reduces
the labeling cost.

3.

hashing function learning. In this work, we modify the
recent supervised hashing method in [37] to learn effective
hashing function and present the details in next section.
(2) Joint data example and tag selection, which actively
selects a set of most informative data examples and tags
for users to label. These newly labeled data are added to
existing labeled information for learning a more accurate
and effective hashing function.

4.

SUPERVISED HASHING

Our active hashing method is related to a recent
supervised hashing method, Sematic Hashing using Tags
and Topic Modeling (SHTTM) [37], which utilizes tags and
topic modeling together to learn effective hashing function.
One main advantage of this method is that it not only
learns high quality hashing codes but extracts a set of tag
correlation variables, which reflect the correlation between
tags and learned hashing codes. Therefore, the relationship
among different tags is also represented in the tag correlation
variables (more details will be given later).
We first introduce some notation. Assume there are n
training examples total, denoted as: X = {x1 , x2 , . . . , xn } ∈
R m×n , where m is the dimensionality of the content feature.
Denote the labeled tags as: T ∈ {0, 1}l×n , where l is the
total number of possible tags associated with each data
example. A label 1 in T means an example is associated
with a certain tag, while a label 0 means the example is
not associated with that tag. The goal is to obtain optimal
binary hashing codes Y = {y1 , y2 , . . . , yn } ∈ {−1, 1}k×n for
the training examples, and a hashing function f : R m →
{−1, 1}k , which maps each example to its hashing code (i.e.,
yi = f (xi )). Here k is the code length. A linear hashing
function is utilized:
W xi )
yi = f (xi ) = sgn(W

(1)

where W is a k × m parameter matrix representing the
hashing function and sgn is the sign function.
There are two key problems that need to be addressed
in supervised hashing methods: (1) how to incorporate
the labeled tag information into learning effective hashing
codes, and (2) how to preserve the similarity between
data examples in the learned hashing codes. The SHTTM
method solves the first problem by ensuring the learned
hashing codes to be consistent with labeled tag information
through a tag consistency component. In particular, a latent
variable uj for each tag tj is first introduced, where uj is a
k × 1 vector indicating the correlation between tag tj and
hashing codes. Then a tag consistency component can be

APPROACH OVERVIEW

The proposed AH-JDETS approach mainly consists of
two components as shown in Figure 1: (1) Supervised
hashing, which incorporates the labeled information into

407

5.

formulated as follows:
l
n X
X

T ij − yiT uj k2 + α
kT

i=1 j=1

l
X

kuj k2

j=1

The main research problem in our AH-JDETS approach
is to actively select a small set of informative data examples
and tags for users to label, which can be utilized for learning
a better hashing function. The goal of joint data example
and tag selection is that we hope to identify a set of L data
examples, Ad , together with a set of M tags, At , such that
the labeling information of the selected data can best boost
the performance of supervised hashing. During the labeling
process, for a given data example and a tag, users label
1 or 0 to denote whether this specific tag is assigned to
this data example or not. The labeling cost will depend on
the total number of selected tags that the users need assign
to the selected data examples, which can be measured as
LM . The reason is that users need to label either 1 or 0
for each data example and tag pair, and there are total LM
such pairs. We will provide more details and possibilities of
measuring the labeling cost with respect to the sizes of L and
M for learning hashing function later in the experiments. As
mentioned, for large scale datasets, there might be millions
of data examples associated with thousands of different tags,
which makes it impractical for labeling all tags to every data
example. Therefore, it is important to select only a small
set of most informative data examples and tags for users to
label to save user labeling efforts.
An important question to ask is how to measure the
informativeness of data examples and tags? In this work, we
propose to combine two measuring criteria, data uncertainty
and data dissimilarity, which are widely used in active
learning literature [4, 25, 39]. In particular, the selected
data (both data examples and tags) are more informative if
they are more uncertain. For example, the hashing codes of
some potential data examples are not certain/well-learned
or the predicted labeling results for some potential tags
are not certain. The intuitive idea is that we would gain
more knowledge by labeling on uncertain data than on
certain ones. On the other hand, the selected data are more
informative if they are more dissimilar to each other, since
similar data may provide redundant information which is
not helpful to the learning process. In the following sections,
we first describe the data certainty and similarity modeling
based on the selection criteria respectively. Then the final
objective together with the optimization algorithm will be
elaborated. Finally, we discuss the computational cost of
the learning algorithm.

(2)

T − Y U k2F + αkU
U k2F
= kT
where T ij is the binary label of the j-th tag on the i-th
data example. yiT uj can be viewed as a weighted sum that
indicates how the j-th tag is related to the i-th example,
and this weighted sum should be consistent with the label
T ij as much as possible. The second regularization term,
Pl
2
j=1 kuj k , is introduced to avoid the overfitting issue [33].
α is trade-off parameter and kkF is the matrix Frobenius
norm. By minimizing this component, the consistency
between tags and the learned hashing codes can be ensured.
Note that semantically similar tags will have similar latent
variables, since these tags are often associated with common
data examples, and thus the learned corresponding latent
variables will be similar by ensuring the tag consistency
term. In the extreme case, if two tags are assigned in
exactly the same set of examples, their latent variables will
be identical.
The second problem in supervised hashing methods is
similarity preserving, which indicates that semantically
similar examples should be mapped to similar hashing
codes within a short Hamming distance. In SHTTM, it
points out that the similarity calculated using the original
feature vector may not reflect the semantic similarity
between data examples. Therefore, it proposes to utilize
features extracted from topic modeling [2] to measure
the semantic similarity between data examples instead
of original features, since topic modeling provides an
interpretable low-dimensional representation of the data
examples associated with a set of topics. SHTTM exploits
the Latent Dirichlet Allocation (LDA) [3] approach of topic
modeling to extract k latent topics from the data examples.
Each example xi corresponds to a distribution θi over
the topics where two semantically similar examples have
similar topic distributions. In this way, semantic similarity
between data examples is preserved in the extracted topic
distributions θ and the similarity preservation component in
SHTTM is defined as follows:
n
X

Y − θ ||2F
||yi − θi ||2 = ||Y

(3)

i=1

By minimizing this component, the similarity between
different examples is preserved in the learned hashing codes.
Combining the tag consistency and similarity preservation
components from Eqns.2 and 3, and substituting Eqn.1, the
overall objective function for learning the hashing function
and tag correlation can be formulated as:
T −WX
U k2F + αkU
U k2F + γkW
W X − θ k2F
min kT
XU
W,U

JOINT DATA EXAMPLE AND TAG
SELECTION

5.1

Data Certainty Modeling

Recall that in our supervised hashing model, the binary
hashing code yi , is obtained by thresholding a linear
W xi ),
projection of a data example xi , i.e., yi = sgn(W
and the linear hashing function W can be viewed as k
decision/classification hyperplanes, each of which is used to
generate one bit of a code. More precisely, if a data example
sits on the positive side of a decision hyperplane, then its
corresponding hashing bit is 1, otherwise -1. The data
example certainty with respect to coding can be measured
by its distance to the hyperplane, which is |wx|. Intuitively
speaking, the smaller the distance of a data example to
a hyperplane, the more uncertain the data example is.
Considering an extreme case where a data example lies
exactly on a decision hyperplane, then it is highly uncertain
to decide whether its corresponding bit should be 1 or -1.

(4)

where α and γ are trade-off parameters to balance the weight
between the components. Note that the sgn operator in
Eqn.1 is relaxed to make the above optimization problem
tractable. Since the objective function in Eqn.4 is convex
W
with respect to either one of the two sets of parameters (W
and U ) when the other one is fixed, the above problem can
be solved iteratively by coordinate descent optimization with
guaranteed convergence similar to that in SHTTM.

408

Since there are total k decision hyperplanes, we use the l2
norm1 to calculate the certainty, cdi , of a data example as:
W xi k2
cdi = kW

where λ is also a trade-off parameter to balance two
data similarity terms. By minimizing the above objective
function, we can select a set of data examples and tags that
are most dissimilar to each other.

(5)

It can be seen that the data example certainty is inversely
related to the informativeness, i.e., the smaller the certainty
value is, the more informative it is. Then the totalP
certainty
of the selected data examples can be written as i∈Ad cdi ,
where Ad is the active set of data examples.
Besides exploiting data example certainty, we also need to
model the tag certainty. In our supervised hashing method
based on SHTTM, the correlation between tags and hashing
codes are learned in the latent variables U . Inspired by
the data example selection, we use the magnitude of uj to
represent the tag certainty as:
ctj = kuj k2

5.3

Ad ,At

(6)

min

µd ,µt

Similar data may contain similar knowledge, which
provides redundant information in the learning process
and is not desirable. For instance, the tags of ‘car’ and
‘automobile’ have similar meanings and choosing both of
them may not gain substantial new information over just
selecting one. Therefore, selecting a set of dissimilar data
to label is very important for acquiring more information,
which can make the learning process more effective. The
pairwise similarity S ij between data example xi and xj can
be pre-calculated as:

(i,j)∈Ad

µd T Cd + φµt T Ct + β µd T S µd + λ µt T U T U µt

min
µd

(8)

µd T C d + β µ d T S µ d
(12)

s.t. 0 ≤ µd ≤ 1 ,

where σ 2 is the bandwidth parameter. Note that we use the
Gaussian function/kernel to calculate the similarity in this
work due to its popularity in many hashing methods [42,
43], but other similarity criteria may also be used, such as
cosine similarity or inner product similarity. Then the total
sum of similarity values
P among the selected data examples
can be calculated as (i,j)∈Ad S ij .
For the tag similarity between ti and tj , we directly
calculate it as the inner product of their corresponding tag
correlation vectors, uTi uj . Then the sum of similarity
values
P
T
between selected tags can be written as
(i,j)∈At ui uj .
Combining the above two similarity terms, the joint data
similarity can be modeled as:
X
X
min
S ij + λ
uTi uj
(9)
Ad ,At
1

(i,j)∈At

where Cd = [cd1 , cd2 , . . . , cdn ]T is the data example certainty
vector and Ct = [ct1 , ct2 , . . . , ctl ]T is the tag certainty vector.
φ, β and λ are trade-off parameters. 1 is a vector of all
ones and the constraints µd T 1 = L and µt T 1 = M mean
we wish to select exact L data examples together with M
tags. The first two terms in the objective function are
the sum of certainty values of the selected data, and the
last two terms are the sum of similarity values between
them. By minimizing the above objective function, we can
jointly select a set of data examples and tags that are most
uncertain and dissimilar with each other.
Directly minimizing the objective function in Eqn.11 is
intractable due to the integer constraints, which makes the
problem NP-hard to solve. Therefore, we propose to relax
these constraints to the continuous constraints 0 ≤ µd ≤ 1
and 0 ≤ µt ≤ 1 and further decompose the optimization
problem into two sub-problems as:

Data Similarity Modeling

S ij = e

(i,j)∈Ad

s.t. µd ∈ {0, 1}n , µt ∈ {0, 1}l , µd T 1 = L, µt T 1 = M
(11)

j∈At

kx −x k2
− i 2j
σ

j∈At

To formalize the above objective, we introduce an indicating
vector µd ∈ {0, 1}n whose entries specify whether or not the
corresponding data examples are selected, i.e., µd i = 1 when
xi is selected and µd i = 0 when xi is not selected. Similarly,
an indicating vector µt ∈ {0, 1}l for tags is also used. Then
the above formulation can be rewritten as:

where φ is a trade-off parameter to balance the weight
between two data certainty terms. By minimizing the above
objective function, we can select a set of data examples and
tags that are most uncertain.

5.2

i∈Ad

(10)

where small value indicates an uncertain tag. u = 0 is an
extreme which means this tag contributes no information
in the hashing function learning.
Therefore,Pwe can
compute the total certainty of the selected tags as j∈At ctj .
Combining the data example certainty and tag certainty
terms, the joint data certainty can be modeled as:
X d
X t
min
ci + φ
cj
(7)
Ad ,At
i∈Ad

Overall Objective and Optimization

The entire objective function of joint data example
and tag selection integrates two components such as
data certainty component in Eqn.7 and data similarity
component given in Eqn.9 as:
X d
X t
X
X
min
ci + φ
cj + β
S ij + λ
uTi uj

µd T 1 = L

and
min
µt

µt T C t + λ µ t T U T U µ t

s.t. 0 ≤ µt ≤ 1 ,

(13)
µt T 1 = M

where 0 is a vector of all zeros. These two relaxed subproblems are standard constrained quadratic programs (QP)
which can be solved efficiently using convex optimization
methods, such as successive linear programming (SLP) [1]
and the bundle method [31]. After obtaining the relaxed
solution from Eqn.12, we select L data examples with the
largest µd values to form the active data example set Ad .
Similarly, we choose M tags with the largest µt values based
on the relaxed solution from Eqn.13 to form the active
tag set At . Finally, we will request the users to label all
tags from the selected tag set At to every data example in
Ad , and update the labeled information T to retrain the

(i,j)∈At

Other norms such as l1 norm may also be used.

409

Algorithm 1 Active Hashing with Joint Data Example and
Tag Selection (AH-JDETS)
Input: Training examples X , initial labeled data T and
parameters α, β, γ and λ.
Output: Hashing function W , tag correlation U and
hashing codes Y .
1: Compute S and θ , initialize L and M .
2: Supervised Hashing
3:
Solve the optimization problem in Eqn.4 to obtain
4:
W and U .
5: Joint Data Example and Tag Selection
6:
Calculate data certainty Cd and Ct using Eqns.5
7:
and 6 based on W and U .
8:
Calculate tag similarity U T U .
9:
Solve the optimization problem in Eqn.12 to select
10:
a set of data examples Ad .
11:
Solve the optimization problem in Eqn.13 to
12:
select a set of tags At .
13: Labeling tags from At to data examples in Ad and
update labeled data T .
14: Repeat step 2 to 13 for several iterations.
15: Generate the hashing codes Y using Eqn.1.

1. F lickr1m [16] is collected from Flicker images for
image annotation and retrieval tasks. This dataset
contains 1 million image examples associated with
more than 7k unique tags. A subset of 250k image
examples with the most common 3k tags is used in
our experiment. We randomly choose 240k image
examples as a training set and 10k for query testing.
2. NUS-WIDE [5] is created by NUS lab for evaluating
image retrieval techniques. It contains 270k images
associated with about 5k different tags. We use a
subset of 110k image examples with the most common
3k tags in our experiment. We randomly partition this
dataset into two parts, 100k for training and 10k for
query testing.
3. ReutersV 1 (Reuters-Volume I): This dataset contains
over 800k manually categorized newswire stories [21].
There are in total 126 different tags associated
with this dataset. A subset of 130k documents of
ReutersV 1 is used in our experiment by discarding
those documents with less than 3 labels. 120k text
documents are randomly selected as the training data,
while the remaining 10k documents are used as testing
queries.

supervised hashing model (see Figure 1). The alternative
process of learning hashing function and actively selecting
data can be repeated for several iterations. We will discuss
more in the experiments. The full AH-JDETS algorithm
including supervised hashing and joint data example and
tag selection is summarized in Algorithm 1.

5.4

4. Reuters (Reuters21578)2 is a collection of text
documents that appeared on Reuters newswire in 1987.
It contains 21578 documents, and 135 tags/categories.
In our experiments, documents with less than 3 labels
are removed. The remaining 13713 documents are
randomly partitioned into a training set with 12713
documents and 1000 test queries.

Computational Complexity Analysis

The learning algorithm of AH-JDETS consists of two
main parts: supervised hashing and joint selection of data
examples and tags. For the supervised hashing method,
we iteratively solve the optimization problem in Eqn.4 to
obtain the hashing function and tag correlation, where the
time complexity is bounded by O(nlk + nk2 ). The second
part involves selecting data examples and tags by solving the
two relaxed optimization problem in Eqns.12 and 13. Since
these are standard quadratic program problems, the time
complexity for obtaining µd and µt is bounded by O(n2 +l2 ).
Thus, the total time complexity of the learning algorithm
is bounded by O(nlk + nk2 + n2 + l2 ). For large scale
dataset, O(n2 ) cost for data example selection might not be
feasible. In practice, we reduce the computational cost in the
experiments by only considering the top 10% data examples
corresponding to the smallest certainty values without much
loss in accuracy. However, the training process is always
conducted off-line and our focus of efficiency is on the
retrieval process. This process of generating hashing code
for a query example only involves some dot products and
comparisons between two binary vectors, which can be done
in O(mk + k) time.

6.

512-dimensional GIST descriptors [26] are used as image
features and tf -idf features are used to represent the
documents.
We implement our method using Matlab on a PC with an
Intel Duo Core i5-2400 CPU 3.1GHz and 8GB RAM. The
parameters α, β, γ and λ are tuned by 3-fold cross validation
on the training set through the grid {0.01, 0.1, 1, 10, 100} and
we will discuss more details how they affect the performance
of our approach later. We randomly select 2k labeled data
examples in the training set to form the initial tag matrix T .
We repeat each experiment 10 times and report the result
based on the average over the 10 runs. Each run adopts a
random split of the dataset.

6.2

EXPERIMENTS

This section presents an extensive set of experiments to
demonstrate the advantages of the proposed approach.

6.1

Evaluation Metrics

To conduct similarity search, each example in the testing
set is used as a query example to search for similar examples
in the corresponding training set based on the Hamming
distance of their hashing codes. We follow two evaluation
criteria that are commonly used in the literature [10, 37,
42]: Hamming Ranking and Hash Lookup. Hamming
Ranking ranks all the data examples in the training set
according to their Hamming distance from the query and
the top k examples are returned as the desired neighbors.
Hash Lookup returns all the data examples within a small
Hamming radius r of the query. The performance is
measured with standard metrics of information retrieval:
precision as the ratio of the number of retrieved relevant

Datasets and Implementation

2
http://daviddlewis.com/resources/textcollections/reuters2
1578/.

We conduct experiments on four datasets, including two
image datasets and two text collections as follows:

410

Selection Methods
AH-JDETS (Ad At )
Ad Rt
Rd At
Rd Rt

F lickr1m
Precision
Recall
0.459
0.114
0.431
0.101
0.437
0.104
0.412
0.092

NUS-WIDE
Precision
Recall
0.348
0.085
0.334
0.081
0.315
0.077
0.287
0.070

ReutersV 1
Precision
Recall
0.675
0.306
0.663
0.295
0.658
0.292
0.653
0.286

Reuters
Precision
Recall
0.742
0.237
0.738
0.236
0.727
0.232
0.724
0.230

Table 1: Precision and recall of the top 200 retrieved examples of different selection methods on four datasets
with 32 hashing bits.

Selection Methods
AH-JDETS (Ad At )
Ad Rt
Rd At
Rd Rt

F lickr1m
Precision
Recall
0.403
0.186
0.385
0.189
0.367
0.175
0.346
0.171

NUS-WIDE
Precision
Recall
0.312
0.134
0.301
0.129
0.296
0.123
0.278
0.116

ReutersV 1
Precision
Recall
0.422
0.246
0.405
0.227
0.387
0.232
0.372
0.215

Reuters
Precision
Recall
0.521
0.149
0.516
0.147
0.504
0.144
0.498
0.132

Table 2: Precision and recall of the retrieved examples within Hamming radius 2 of different selection methods
on four datasets with 32 hashing bits.
gives overall the best performance among all four selection
methods on all datasets. From these comparison results,
we can see that Rd Rt does not perform well in terms of
both precision and recall. The reason is that the randomly
selected data examples and tags may be noninformative. For
example, these selected data might carry much redundant
information if they are very similar to each other. It is
also possible that the selected data examples have high
certainties. In other words, the hashing codes of these
data examples are already well-learned with high quality,
and thus cannot contribute much for learning more effective
hashing codes and function. We can also observe from
the results that our AH-JDETS method outperforms the
two methods Ad Rt and Rd At which either actively select
data examples or select tags. This can be attributed to
the joint selection strategy in our method, since it not
only identifies the most informative data examples but
at the same time selects the most informative tags. In
this way, the learner could gain most information from
labeling these selected data. Moreover, a two-sided paired
t-test [29] is used to determine the statistical significance
improvements in Tables 1 and 2. T-test shows that AHJDETS significantly outperforms Rd Rt (p < 0.01), Rd At (p
< 0.03) and Ad Rt (p < 0.03) on all datasets. The precisionrecall curves of different selection methods with 32 hashing
bits on F lickr1m and ReutersV 1 are reported in Fig.2. It
can be seen that among all of these comparison methods,
AH-JDETS shows the best performance, which is consistent
with the results in Tables 1 and 2. We have also observed
similar results on the other two datasets. But due to the
limit of space, they are not presented here.
To evaluate the effect of different code lengths, we conduct
another experiment by varying the number of hashing bits in
the range of {8, 16, 32, 64, 128}. The precisions for the top
200 retrieved examples with different numbers of hashing
bits on four datasets are shown in Fig.3. From this figure,
we observe that larger code length gives better precision
results on all datasets. It also can be seen that our method
consistently outperforms the other three methods under
different code lengths.

Figure 2: Results of Precision-Recall curve with 32
hashing bits on F lickr1m and ReutersV 1 datasets.

examples to the number of all returned examples and recall
as the ratio of the number of retrieved relevant examples to
the number of all relevant examples. The performance is
averaged over all test queries in the datasets.

6.3
6.3.1

Results and Discussions
Comparison of different selection methods

In this set of experiments, we compare our joint selection
method against three other selection methods: 1. Actively
select data examples and randomly select tags (Ad Rt ). 2.
Randomly select data examples and actively select tags
(Rd At ). 3. Randomly select data examples and Randomly
select tags (Rd Rt ). Clearly, our method can be regarded as
Ad At . The size of data examples, L, is set to be 1000 for all
datasets and the size of tags, M , is set to be 30 for the two
image datasets and 10 for the other two text datasets. Note
that for fair comparison, we adopt a modified version of [45],
the Ad Rt selection method, which has the same labeling cost
as our method by randomly selecting a set of tags instead
of all tags.
We report the precision and recall for the top 200 retrieved
examples with 32 hashing bits in Table 1. The precision and
recall for the retrieved examples within Hamming radius
2 are shown in Table 2. It can be seen that AH-JDETS

411

Methods
SHTTM [37]
AH-JDETS (5L, 2M )
AH-JDETS (L, M )
STH [43]

F lickr1m
Precision
Cost
0.508
33.5m
0.499
300k
0.459
30k
0.393
0

NUS-WIDE
Precision
Cost
0.363
16.6m
0.358
300k
0.348
30k
0.285
0

ReutersV 1
Precision
Cost
0.712
4.8m
0.704
100k
0.675
10k
0.633
0

Reuters
Precision
Cost
0.754
2.64m
0.751
100k
0.742
10k
0.721
0

Table 3: Comparison with two state-of-the-art hashing methods. Results of precision and labeling cost of the
top 200 retrieved examples with 32 hashing bits.
SHTTM (especially on (5L, 2M ) setting) but requiring much
less labeling cost. For example, the labeling cost for
SHTTM is about 1100 times more than our AH-JDETS
with 1k data examples and 30 tags on F lickr1m, which
is impractical. Therefore, our AH-JDETS approach can be
viewed as a good trade-off between unsupervised hashing
and passive supervised hashing methods, which achieves
good performance but saving much labeling efforts and thus
overcomes the limitation of passive hashing methods.

6.3.3

Varying set size L and M
In this set of experiments, we evaluate the performance of
AH-JDETS and Ad Rt methods by varying the set size L and
M . It is obvious that we would gain more information by
selecting more data examples and tags simultaneously. An
interesting question would be: given the same labeling effort,
how should we choose L and M to achieve best performance?
Therefore, to answer this question, we fix the labeling cost
LM = 30k for image datasets and LM = 10k for text
datasets, and vary both L and M in the experiments. The
code length is set to be 32 in all experiments.
The precision results of top 200 retrieved examples with
respect to data example size on all datasets are shown in
Fig.4. We can observe that the performances of both two
methods are not satisfactory when selecting either very few
data examples or tags (correspond to two end points in the
figure). For example, in F lickr1m dataset, the right most
red point corresponds to the combination of selecting 15000
data examples with 2 tags, while the left most red point
represents the choice of 10 data examples with all 3000 tags.
Although these two configurations have the same labeling
cost 30k, the performance is the worst among all possible
combinations. It can be seen that the optimal combination
is around 1200 data examples with 25 tags for our method on
F lickr1m, which is roughly proportional to the total number
of data examples and tags. Therefore, it is important to
choose a good combination of L and M to achieve the best
performance.
Consider a more realistic labeling cost measure scenario,
where users are assigning M possible tags to a document or
an image. Usually people first read through the content of
the document or view the content of an image, which takes
a reading cost r (may vary for documents and images), and
then assign M labels to it. In this case, the labeling cost for
a data example is r + M and the total labeling cost for L
examples is (r + M )L. Using this cost measure, we conduct
another experiment on F lickr1m by fixing r=5 and have
found similar pattern as in Fig.4(a). However, we observe
that the optimal combination drifts toward smaller number
of data examples with more tags (around 940 data examples

Figure 3: Results of precision of top 200 examples
on four datasets with different hashing bits.

6.3.2

Comparison with SHTTM and STH

We compare our AH-JDETS with two state-of-the-art
hashing methods, Semantic Hashing using Tags and Topic
Modeling (SHTTM) [37] and Self-Taught Hashing (STH)
[43], on all datasets to demonstrate the advantages of our
active hashing approach. SHTTM is a passive supervised
hashing method which is briefly discussed in section 4. STH
is an unsupervised hashing method that does not utilize any
of the label information.
Precisions for the top 200 retrieved examples and the
labeling costs of all three methods using 32 hashing bits are
reported in Table 3. Note that there is no labeling cost for
STH since it does not require any label knowledge. SHTTM
is a supervised hashing method that incorporates all labels
into hashing function learning. Its labeling cost is simply the
number of total labels associated with the training examples.
For our AH-JDETS, the labeling cost is LM since we have
to label each data example and tag pair and there are total
LM such pairs. We evaluate AH-JDETS with two different
labeling costs, (L,M ) and (5L,2M ), where L=1k and M =30
for image datasets and L=1k and M =10 for text datasets.
From these comparison results we can see that AH-JDETS
achieves much better performance than the unsupervised
method STH, while it also obtains comparable results with

412

Figure 4: Precision results of varying batch sizes
while fixing the labeling cost. Hashing bits are set
to be 32 for all datasets.

Figure 5: Precision results of increasing number of
iterations on four datasets with 32 hashing bits.

from {0.5, 1, 2, 4, 8, 16, 32, 128}. We report the results on
F lickr1m and ReutersV 1 in Fig.6. It is clear from these
experimental results that the performance of AH-JDETS
is relatively stable with respect to β and λ. We have also
observed similar results of the proposed method in the other
two datasets. On the other side, it has already been shown in
SHTTM [37] that the supervised hashing method is robust
with respect to a wide range of α and γ.

with 27 tags), which is consistent with our expectation due
to the reading cost associated with each data example.

6.3.4

Varying number of iterations

In our AH-JDETS approach, after obtaining the label
information on the active set, we will update the labeled tags
T to retrain the supervised hashing model as shown in Figure
1. The alternative process of learning hashing function and
actively selecting data can be repeated for several iterations.
In this set of experiments, we evaluate the performance of
AH-JDETS by varying the number of iterations from 1 to
15 on all datasets. Note that the labeling cost grows linearly
with the number of iterations since during each iteration the
labeling cost is identical, i.e., LM .
Precisions for the top 200 examples with 32 hashing bits
are reported in Fig.5. Not surprisingly, we can observe
that the precision increases with the increasing number of
iterations. However, we have found that the performance
of AH-JDETS does not increase much after the first few
iterations. Our hypothesis is that we have gained sufficient
knowledge to learn a good hashing function within the
first few iterations. In other words, the labeled data from
the later iterations contains more and more redundant
information, which does not contribute much for retraining
a better hashing function. Therefore, the proposed AHJDETS approach can obtain good performance in a few
iterations, which also saves the labeling cost.

6.3.5

7.

CONCLUSIONS

This paper proposes a novel active hashing approach,
Active Hashing with Joint Data Example and Tag Selection
(AH-JDETS), which actively selects the most informative
data examples and tags in a joint manner for learning
hashing function. The selection principle is to identify data
examples and tags that are both uncertain and dissimilar
with each other.
The labeled information is utilized
together with unlabeled data to generate an effective hashing
function. Extensive experiments on four different datasets
demonstrate the advantage of the proposed AH-JDETS
approach for learning a more effective hashing function with
small labeling costs than the baseline passive supervised
learning and some other active learning methods. In future,
we plan to conduct new research for automatically selecting
the optimal L and M . We also plan to model the information
gain from the labels that the users input for better data
selection.

Parameter Sensitivity

8.

There are four trade-off parameters in AH-JDETS, α and
γ in supervised hashing component, and β and λ in the
joint data example and tag selection method. To prove
the robustness of the proposed joint selection method, we
conduct parameter sensitivity experiments of β and λ on
all datasets. In each experiment, we tune the parameter

ACKNOWLEDGMENTS

This work is partially supported by NSF research grants
IIS-0746830, CNS-1012208, IIS-1017837 and CNS-1314688.
This work is also partially supported by the Center
for Science of Information (CSoI), an NSF Science and
Technology Center, under grant agreement CCF-0939370.

413

[19] B. Kulis and K. Grauman. Kernelized locality-sensitive
hashing for scalable image search. In ICCV, 2009.
[20] D. D. Lewis and W. A. Gale. A sequential algorithm for
training text classifiers. In SIGIR, pages 3–12, 1994.
[21] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. Rcv1: A new
benchmark collection for text categorization research.
Journal of Machine Learning Research, 5:361–397, 2004.
[22] R.-S. Lin, D. A. Ross, and J. Yagnik. Spec hashing:
Similarity preserving algorithm for entropy-based coding.
In CVPR, pages 848–854, 2010.
[23] W. Liu, J. Wang, R. Ji, Y.-G. Jiang, and S.-F. Chang.
Supervised hashing with kernels. In CVPR, 2012.
[24] W. Liu, J. Wang, Y. Mu, S. Kumar, and S.-F. Chang.
Compact hyperplane hashing with bilinear functions.
ICML, 2012.
[25] B. Long, O. Chapelle, Y. Zhang, Y. Chang, Z. Zheng, and
B. L. Tseng. Active learning for ranking through expected
loss optimization. In SIGIR, pages 267–274, 2010.
[26] A. Oliva and A. Torralba. Modeling the shape of the scene:
A holistic representation of the spatial envelope. IJCV,
42(3):145–175, 2001.
[27] R. Salakhutdinov and G. E. Hinton. Semantic hashing. Int.
J. Approx. Reasoning, 50(7):969–978, 2009.
[28] B. Settles. Active learning literature survey. Technical
report, 2010.
[29] M. D. Smucker, J. Allan, and B. Carterette. A comparison
of statistical significance tests for information retrieval
evaluation. In CIKM, pages 623–632, 2007.
[30] B. Stein. Principles of hash-based text retrieval. In SIGIR,
pages 527–534, 2007.
[31] C. H. Teo, S. V. N. Vishwanathan, A. J. Smola, and Q. V.
Le. Bundle methods for regularized risk minimization.
Journal of Machine Learning Research, 11:311–365, 2010.
[32] A. Tian and M. Lease. Active learning to maximize
accuracy vs. effort in interactive information retrieval. In
SIGIR, pages 145–154, 2011.
[33] V. Vapnik. The nature of statistical learning theory.
Springer Verlag, 2000.
[34] J. Wang, S. Kumar, and S.-F. Chang. Semi-supervised
hashing for large-scale search. IEEE Trans. Pattern Anal.
Mach. Intell., 34(12):2393–2406, 2012.
[35] J. Wang, W. Liu, A. Sun, and Y.-G. Jiang. Learning hash
codes with listwise supervision. In ICCV, 2013.
[36] Q. Wang, L. Ruan, Z. Zhang, and L. Si. Learning compact
hashing codes for efficient tag completion and prediction. In
CIKM, pages 1789–1794, 2013.
[37] Q. Wang, D. Zhang, and L. Si. Semantic hashing using tags
and topic modeling. In SIGIR, pages 213–222, 2013.
[38] Q. Wang, D. Zhang, and L. Si. Weighted hashing for fast
large scale similarity search. In CIKM, pages 1185–1188,
2013.
[39] Z. Wang and J. Ye. Querying discriminative and
representative samples for batch mode active learning. In
KDD, pages 158–166, 2013.
[40] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In
NIPS, pages 1753–1760, 2008.
[41] H. Xia, P. Wu, S. C. H. Hoi, and R. Jin. Boosting
multi-kernel locality-sensitive hashing for scalable image
retrieval. In SIGIR, pages 55–64, 2012.
[42] D. Zhang, F. Wang, and L. Si. Composite hashing with
multiple information sources. In SIGIR, pages 225–234,
2011.
[43] D. Zhang, J. Wang, D. Cai, and J. Lu. Self-taught hashing
for fast similarity search. In SIGIR, pages 18–25, 2010.
[44] Q. Zhang, Y. Wu, Z. Ding, and X. Huang. Learning hash
codes for efficient content reuse detection. In SIGIR, pages
405–414, 2012.
[45] Y. Zhen and D.-Y. Yeung. Active hashing and its
application to image and text retrieval. Data Min. Knowl.
Discov., 26(2):255–274, 2013.

Figure 6: Parameter Sensitivity for β and λ. Results
of precision of the top 200 examples with 32 hashing
bits.

9.

REFERENCES

[1] M. S. Bazaraa, H. D. Sherali, and C. M. Shetty. Nonlinear
programming: theory and algorithms (3. ed.). Wiley, 2006.
[2] D. Blei and J. Lafferty. Topic models. Text Mining: Theory
and Applications, 2009.
[3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet
allocation. Journal of Machine Learning Research,
3:993–1022, 2003.
[4] P. Cai, W. Gao, A. Zhou, and K.-F. Wong. Relevant
knowledge helps in choosing right teacher: active query
selection for ranking adaptation. In SIGIR, pages 115–124,
2011.
[5] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y. Zheng.
Nus-wide: a real-world web image database from national
university of singapore. In CIVR, 2009.
[6] N. N. Dalvi, V. Rastogi, A. Dasgupta, A. D. Sarma, and
T. Sarlós. Optimal hashing schemes for entity matching. In
WWW, pages 295–306, 2013.
[7] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni.
Locality-sensitive hashing scheme based on p-stable
distributions. In Symposium on Computational Geometry,
pages 253–262, 2004.
[8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.
Furnas, and R. A. Harshman. Indexing by latent semantic
analysis. JASIS, 41(6):391–407, 1990.
[9] P. Donmez and J. G. Carbonell. Active sampling for rank
learning via optimizing the area under the roc curve. In
ECIR, pages 78–89, 2009.
[10] Y. Gong, S. Lazebnik, A. Gordo, and F. Perronnin.
Iterative quantization: A procrustean approach to learning
binary codes for large-scale image retrieval. IEEE TPAMI,
2012.
[11] Y. Guo and D. Schuurmans. Discriminative batch mode
active learning. In NIPS, 2007.
[12] A. Harpale and Y. Yang. Personalized active learning for
collaborative filtering. In SIGIR, pages 91–98, 2008.
[13] G. E. Hinton and R. R. Salakhutdinov. Reducing the
dimensionality of data with neural networks. Science,
313(5786):504–507, 2006.
[14] T. Hofmann. Probabilistic latent semantic indexing. In
SIGIR, pages 50–57, 1999.
[15] S. C. H. Hoi, R. Jin, J. Zhu, and M. R. Lyu. Batch mode
active learning and its application to medical image
classification. In ICML, pages 417–424, 2006.
[16] M. J. Huiskes, B. Thomee, and M. S. Lew. New trends and
ideas in visual concept detection: the mir flickr retrieval
evaluation initiative. In Multimedia Information Retrieval,
pages 527–536, 2010.
[17] R. Jin and L. Si. A bayesian approach toward active
learning for collaborative filtering. In UAI, pages 278–285,
2004.
[18] W. Kong, W.-J. Li, and M. Guo. Manhattan hashing for
large-scale image retrieval. In SIGIR, pages 45–54, 2012.

414

