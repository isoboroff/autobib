Load Balancing for Partition-based Similarity Search
Xun Tang, Maha Alabduljalil, Xin Jin, Tao Yang
Department of Computer Science, University of California
Santa Barbara, CA 93106, USA

{xtang,maha,xin_jin,tyang}@cs.ucsb.edu

ABSTRACT

other candidate vectors. This approach outperforms other
approaches [19, 8] by an order of magnitude because of the
simplified parallelism management and aggressive elimination of unnecessary I/O and comparison.
Given a large number of data partitions, we need to assign
them to parallel machines and decide the direction of similarity comparison due to the symmetric property of comparison. Load imbalance can hugely affect scalability and
overall performance. In this paper, we explore load balancing issues and develop a two-stage assignment algorithm to
improve the execution efficiency of APSS. The first stage
constructs a preliminary load assignment over tasks. The
second stage refines the assignment to be more balanced.
Our analysis shows that the developed solution is competitive to the optimum with a constant ratio. We further improve the dissimilarity detection ability in the static partitioning step, while producing partitions with relatively even
sizes to facilitate the load balancing step.
The paper is organized as follows. Section 2 reviews some
background and related work. Section 3 discusses the design
framework. Section 4 introduces the two-stage load assignment algorithm and its performance properties. Section 5
presents the improved data partitioning scheme. Section 6 is
the experimental evaluation. Section 7 concludes this paper
and the appendix lists the proofs of the analytic results.

All pairs similarity search, used in many data mining and
information retrieval applications, is a time consuming process. Although a partition-based approach accelerates this
process by simplifying parallelism management and avoiding unnecessary I/O and comparison, it is still challenging
to balance the computation load among parallel machines
with a distributed architecture. This is mainly due to the
variation in partition sizes and irregular dissimilarity relationship in large datasets. This paper presents a two-stage
heuristic algorithm to improve the load balance and shorten
the overall processing time. We analyze the optimality and
competitiveness of the proposed algorithm and demonstrates
its effectiveness using several datasets. We also describe a
static partitioning algorithm to even out the partition sizes
while detecting more dissimilar pairs. The evaluation results
show that the proposed scheme outperforms a previously developed solution by up to 41% in the tested cases.
Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Clustering, Search Process
Keywords: All-pairs similarity search; load balancing; partitioning; competitiveness analysis

1.

INTRODUCTION

All Pairs Similarity Search (APSS) [9], which identifies
similar objects in a dataset, is used in many applications
including collaborative filtering based on user interests or
item similarity [1], search query suggestions [22], web mirrors and plagiarism recognition [23], coalition detection for
advertisement frauds [20], spam detection [11, 17], clustering [7], and near duplicates detection [16]. The complexity
of naı̈ve APSS can be quadratic to the dataset size. Previous researches on expediting similarity computing developed
filtering methods [9, 27, 4], inverted indexing [19, 21], or partitioning and parallelization techniques [3]. It is shown in [3]
that APSS can be performed through a number of independent tasks where each compares a partition of vectors with

2.

BACKGROUND AND RELATED WORK

Following the work in [9], the APSS problem is defined as
follows. Given a set of vectors di = {wi,1 , wi,2 , · · · , wi,m },
where each vector contains at most m features and may
be normalized to a unit length, the cosine-based similarity
between two vectors is computed
Xas:
Sim(di , dj ) =
wi,t ×wj,t .
t∈(di ∩dj )

Two vectors di , dj are considered similar if their similarity
score exceeds a threshold τ , namely Sim(di , dj ) ≥ τ . The
time complexity of APSS is high for a big dataset. There are
application-specific methods applied for data preprocessing.
For example, text mining removes stop-words or features
with extremely high document vector frequency [7, 13, 19].
There are several groups of optimization techniques developed in the previous work to accelerate APSS.
Dynamic computation filtering. Partially accumulated similarity scores can be monitored at runtime and dissimilar document pairs can be detected dynamically without
complete derivation of final similarity scores [9, 27, 21].
Similarity-based grouping in data pre-processing.
The search scope for similarity can be reduced when po-

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609624.

193

Partitioning with
Dissimilarity Detection

3.

read

P1
Input
Input
vectors
vectors

po
te
s ntia
poimila lly
sim ten r
ila tial
P2
r ly

Task1
re
mpa

with

co

pa
om

c

P3

Similar
Similar
pairs
pairs

th

re

wi

P4

....

Pn

Taskn

Figure 1: Illustration of partition-based similarity search.
Task Tk
Read all vectors from assigned partition Pk
Build inverted index of these vectors
Conduct self-comparison among vectors in Pk
repeat
Fetch a potentially similar partition
for dj ∈ fetched partition do
Compare (Pk , dj )
until all non-dissimilar partitions are fetched
Figure 2: Definition of each PSS task.
The partitioning algorithm sorts the vectors based on their
1-norm values first. It then uses the sorted list to identify dissimilar pairs (di , dj ) satisfying inequality kdi k1 <
τ
. A different τ value would affect the outcome of the
||dj ||∞
dissimilarity-based partitioning.
Once the dataset is separated into v partitions, v independent tasks are scheduled. Each task is responsible for
a partition and compares this partition with all potentially
similar partitions. We assume that the assigned partition for
each task fits the memory of one machine as the data partitioning can be adjusted to satisfy such condition. Other
partitions to be compared with may not fit the remaining
memory and need to be fetched gradually from a local or
remote storage. In a computing cluster with a distributed
file system such as Hadoop, tasks can seamlessly fetch data
without concerning about the physical locations of data.
Figure 2 describes the function of each task Tk in partitionbased similarity search. Task Tk loads the assigned partition Pk and produces an inverted index to be used during
the partition-wise comparison. Next, Tk fetches a number
of vectors from potentially similar partitions and compares
them with the local partition Pk . Fetch and comparison is
repeated until all candidate partitions are processed.

FRAMEWORK

This section gives an overview of the partition-based framework [3] and presents the load balancing problem.

3.1

Similarity Comparison
with Parallel Tasks

....

tentially similar vectors are placed in one group. One can
use an inverted index [27, 19, 21] developed for information retrieval [7]. This approach identifies vectors that share
at least one feature as potentially similar, so certain data
traversal is avoided. Similarly, the work in [25] maps featuresharing vectors to the same group for group-wise parallel
computation. This technique is more suitable for vectors
with low sharing pattern, otherwise it suffers from excessive
redundant computation among groups. Locality-sensitive
hashing (LSH) can be considered as grouping similar vectors
into one bucket with approximation [15, 24]. This approach
has a trade-off between precision and recall, and may introduce redundant computation when multiple hash functions
are used. A study [4] shows that exact comparison algorithms can deliver performance competitive to LSH when
computation filtering is used. In partition-based APSS [3],
dissimilar vectors are identified in the static partitioning
step. The APSS problem is then converted to executing
a set of independent tasks each compares one partition with
some of the other partitions. These tasks can be executed
in parallel with much simplified parallelism management.
Load balancing and scheduling. Exploiting parallel
resources over thousands of machines for scalable performance is important and challenging. Load balancing is considered in the context of search systems for index serving [6,
18]. A recent study [26] introduces a division scheme to improve load balance for dense APSS problems using multiple
rounds of MapReduce computation. In order to minimize
the communication overhead while maintaining the computational load balance, in this paper, we focus on load balancing of APSS with record-based partitioning. The general
load balancing and scheduling techniques for clusters and
parallel systems have been extensively addressed in previous work. A simple greedy policy [14] that maps a ready
task to a computation unit once it becomes idle is widely
adopted (e.g. [10]). Scheduling for MapReduce systems such
as Hadoop [12, 28] has followed the greedy policy to execute
queued tasks on available cores and exploit data locality
whenever feasible. Assuming that parallel tasks are scheduled following such a greedy policy, we address how these
tasks should be formed considering scalability and efficiency.

Partition-based Similarity Search

3.2

The framework for Partition-based Similarity Search (PSS)
consists of two phases. The first phase divides the dataset
into a set of partitions. During this process, the dissimilarity among partitions is identified so that unnecessary data
I/O and comparisons among them are avoided. The second
phase assigns a partition to each task at runtime and each
task compares this partition with other potentially similar
partitions. These tasks are independent when running on a
set of parallel machines. Figure 1 depicts the whole process.
Dissimilarity-based partitioning identifies dissimilar vectors without explicitly computing the product of their features. One approach [3] utilizes the following inequality that
calculates the 1-norm and ∞-norm of each vector:

Load Assignment Problem

We formalize the load assignment problem as follows. The
data partitioning phase defines a set of v partitions and their
potentially similar relationship. This can be represented as
a graph, called a similarity graph defined next.
Definition 1 Similarity graph (G): Let G be an undirected graph where each node represents a data partition and
each edge indicates potential similarity relationship between
the two partitions it connects.
Since the similarity result of two vectors is symmetric,
comparison between two partitions Pi and Pj should be only
conducted by one of the corresponding tasks Ti or Tj . A
load assignment algorithm determines which task performs
this comparison. The load assignment process converts the

Sim(di , dj ) ≤ min(||di ||∞ ||dj ||1 , ||dj ||∞ ||di ||1 ) < τ.

194

undirected similarity graph into a directed graph in which
the direction of each edge indicates which task conducts the
corresponding comparison. We call this a comparison graph
and it is defined as follows.
Definition 2 Comparison graph (D): Let D be a directed graph where each node represents a data partition.
An edge ei,j from partition Pi to Pj indicates that task Tj
compares Pj with Pi .
P2

P4

6
2

P6

56.1

P4

(a)

P4

56.1

51

86.7

67.1

P7

P7

67.1

79.3

16.8

6.3

P5

27.9

9.3

54.9

P7

8.8

P1

5
6

P3

P3
1.1

P1

3

5

P2

P2

P3

1

P1

P6

P5

(b)

P6

P5

(c)

Figure 3: (a) An undirected similarity graph; node weights
are partition sizes. (b) A directed comparison graph for (a);
node weights are the corresponding task cost. (c) Another
comparison graph for (a).
Comparison graph D contains the same set of nodes and
edges as the corresponding similarity graph G, except that
the edges in D are directed. The directed edges reveal the
data flow direction when comparing two potentially similar
partitions. Figure 3(a) illustrates a similarity graph with
seven nodes. P1 is potentially similar to P2 , P4 and P5 , for
instance. The comparison between P1 and P2 can be performed by either T1 or T2 . The numbers marked inside the
graph nodes are partition sizes, proportional to the number of vectors in the partition. Figures 3(b) and 3(c) show
two comparison graphs with different load assignments. The
number marked inside a comparison graph node is the corresponding task cost and we explain the cost model below.
The cost function of each task consists of computation
cost and data I/O cost. For each task defined in Figure 2,
the computation cost includes the cost of an inverted index
look-up, multiplication and addition, and memory/cache accesses. While a thorough cost model involves memory hierarchy analysis [2], the overall computation cost can be approximated as proportional to the size of the corresponding
partition Pi multiplied by the size of the potentially similar
partitions to be compared with. The data I/O cost occurs
when fetching Pi and other partitions from local or remote
machines, and also when storing the detected similarity results on disk. Since the start-up I/O cost and transmission
bandwidth difference to the local or remote storage are relatively small, the I/O cost is approximately proportional to
the size of the partitions involved. Note that the runtime
scheduling that maps tasks to machines is affected by data
locality. As we discuss later, the computation cost is dominating in APSS and thus the I/O cost difference caused by
data locality is not sufficient enough to alter our optimization results in terms of competitiveness to the optimum.
Define the cost of task Ti corresponding to partition Pi in
comparison graph D as:
X
Cost(Ti ) = f (Pi , Pi ) + fc (Pi ) +
(f (Pi , Pj ) + fc (Pj ))

where f (Pi , Pi ) is the self comparison cost for partition i
and is quadratically proportional to the size of Pi . f (Pi , Pj )
is the comparison cost between partition i and j. It satisfies that f (Pi , Pj ) = f (Pj , Pi ) and this cost is proportional to the size of Pi multiplied by size of Pj . fc (Pi )
is the I/O and communication cost to fetch partition Pi
from local and/or remote storage and output the results
of self-comparison. fc (Pj ) is the cost to fetch partition Pj
and output the similar pairs between Pi and Pj . For Figures 3(b) and 3(c), f (Pi , Pj ) is a multiplication of the sizes
of Pi and Pj , and fc (Pi ) is estimated as 10% of the size of
Pi . In Figure 3(c), Cost(T5 )=67.1 because f (P5 , P5 )=36,
f (P5 , P4 )=30, fc (P5 )=0.6 and fc (P4 )=0.5.
Different edge direction assignments can lead to a large
variation in task weights. Let Cost(D) = maxPi ∈D Cost(Ti ).
For example, in Figure 3(b) Cost(D)=86.7 based on Cost(T4 ).
In Figure 3(c) Cost(D)=67.1. Deriving a comparison graph
that minimizes the maximum cost among all tasks is a key
strategy in our design. As the load is shifted from the heaviest task to the other tasks, better load balancing is achieved.
A circular mapping solution in [3] compares a partition
with half of other partitions, if they are potentially similar.
When the number of partitions is odd, task Ti compares
Pi with partitions Pj where j belongs to the set: i%v +
)%v + 1. Figure 3(b) shows
1, (i + 1)%v + 1, · · · , (i + v−3
2
the circular solution for the similarity graph in Figure 3(a).
T1 is assigned to compare with partitions from P2 to P4 ,
hence the edge is directed from P2 and P4 to P1 . Similarly,
the comparison between P1 and P5 is assigned to P5 . The
circular approach is reasonable when the distribution of node
connectivity and partition sizes is not skewed. In practice,
that is often not true.
Dataset
Twitter
ClueWeb
YMusic

Partition size (# of records per partition)
Avg
Std. Dev/Avg
Max/Avg
143,042
1.75
6.85
337,720
0.67
2.37
21,550
0.82
4.35

Task cost
Max/Avg
2.14
4.25
8.97

Table 1: Distribution statistics for partition size and parallel
execution time with circular load assignment.
Table 1 shows the variance of partition sizes and task costs
in three datasets. The largest partition size could be many
times larger than the average partition size and the standard deviation compared to the average size is also high.
Additionally, the similarity relationship among partitions is
highly irregular. Some partitions have lots of edges in similarity graph while others have sparse connections. Circular
load assignment treats all partitions equally regardless of
such variations and as a result, a task could be assigned all
the comparison loads while its counterpart tasks are very
light. Column 5 of Table 1 shows the maximum divided by
average task cost using circular assignment.
The ultimate goal of load assignment is to schedule computation to parallel machines with minimum job completion
time. Since undirected edges in a similarity graph creates
uncertainty in task workload, the key question here is what
to optimize. Will balancing the task costs computed from
the comparison graph help speedup the runtime execution
without knowing the allocated computing resource in advance? In the next section, we discuss our optimization
strategy and present a two-stage assignment algorithm.

ej,i ∈D

195

4.

LOAD BALANCING OPTIMIZATION

P2

Our algorithm for load assignment consists of two stages
to derive a comparison graph with balanced load among
tasks. The design considers uneven partition sizes and irregular dissimilarity relationship. The derived tasks are scheduled at runtime to q cores and the tasks with reduced variation in sizes contribute to better performance after scheduling. We will show that such a strategy can produce a solution competitive to the optimal solution for scheduling a
similarity graph on a given number of cores. We discuss the
two-stage algorithm in the following two subsections.

4.1

P1

P1
P2
P3
P4
P5
P6
P7

G2

P7

36.6
16.8

P6

67.1
16.8

P5

P5

P6

(a)

(b)

Figure 5: (a) The assignment produced in Stage 1. (b) The
first refinement step in Stage 2: reversing edge e5,4 to e4,5 .
1

Std. Dev/Avg of task costs

0.9
0.8
0.7
0.6
0.5
0.4
0.3
51

101

151

Steps

Figure 6: Monotonic decrease of the cost standard deviation
in the first 200 steps in Stage 1 for Twitter dataset. The
values are normalized by the average task computation cost.
P W if it is undetermined. The step-wise trend illustrates
that Stage 1 gradually reduces the variation of task costs.
Stage 1 pushes the computation load to the tasks with
potentially low weight. This technique works better when
partitions have highly skewed sizes since the lightest partitions absorb as much workload as possible. However, this
greedy heuristic may cause some tasks to carry an excessive
amount of computation. Another issue is that Stage 1 does
not consider data I/O and communication cost, so the effect
of optimization might be weakened. Hence, we introduce
Stage 2 to further refine the assignment produced by Stage
1 and mitigate the aforementioned weakness.

P2

G1

51
67.1

P7

It represents the largest possible computation weight for task
Tx given the undirected edges in Gk . Gk+1 is derived from
Gk by removing the selected partition Px and its edges in
Gk . These edges connecting Px in Gk are chosen to point to
Px in the generated directed graph.
Step 2
G3
80
37
110
96
84

P4

81.6

ex,y ∈ Gk

Step 1
G2
80
37
110
108
16
84

P3
27.9

56.1

P4

67.1

Stage 1: Initial Load Assignment

Init
G1
85
8
37
110
108
18
84

8.8

P1

27.9

56.1

The purpose of Stage 1 of this algorithm is to produce
an initial load assignment such that tasks with small partitions conduct more comparisons. This stage performs v
steps where v is the total number of partitions in the given
similarity graph. Each step identifies a partition, determines
the direction of its similarity edges, and adds this partition
along with these directed edges to comparison graph.
More specifically, each step works on a subgraph of the
original undirected graph G, called Gk at step k. G1 is
the original graph G. At step k, the algorithm identifies
partition Px with the lowest potential computation weight
(P W ). The potential computation weight for task Tx based
on subgraph Gk is defined as:
X
P W (Gk , Px ) = f (Px , Px ) +
f (Px , Py ).

Node

P2

P3

8.8

P6

G3
P3

P1

4.2

P4

Stage 2: Assignment Refinement

Stage 2 conducts a number of refinement steps to reduce
the load of the heavy tasks by gradually shifting part of
their computation to their lightest neighbors. It performs
the following procedure:

P7
P5

Figure 4: The first two steps in Stage 1 in the right figure,
along with the PW values in the left table.

1. Find the task with the highest assigned cost Cost(Tx ).
Identify one of Px ’s incoming neighbors, say Py , with the
lowest cost among these neighbors, and reverse the direction of this edge from ey,x to ex,y . Such a reversion causes
a cost increase for Ty and a cost decrease for Tx . However,
if the new cost of Ty becomes the same or larger than the
original cost of Tx , this edge reversion is rejected. When
an edge reversion is rejected, we continue with the incoming neighbor that has the second lowest cost. Repeat this
process until a suitable neighbor is found so that the edge
reversion successfully reduces Cost(Tx ). If all incoming
neighbors of Px are probed but no flip reduces Cost(Tx )
successfully, mark Cost(Tx ) as non-reducible.
2. Repeat the above step for the task with the highest weight
after the update. If such a task is non-reducible, try
the reducible task with the next highest weight. If all
nodes are marked non-reducible or the number of iterations tried reaches a predefined limit, the algorithm stops.

Figure 4 illustrates the first two steps in Stage 1. The
left part of the figure lists the initial PW values of each
node, as well as the corresponding values after the first step
and second step. Partition P2 has the lowest PW value
initially and is selected at Step 1. Edges connecting P2 are
all directed to P2 in the formed directed graph. The PW
values of the partitions adjacent to P2 are changed from G1
to G2 . Step 2 identifies P6 as the the lowest PW in G2 ,
removing it and its edges from G2 . Finally the outcome of
Stage 1 produces a comparison graph shown in Figure 5(a).
The cost of a task at Step k is considered to be determined
if its corresponding partition has been selected before Step
k. Otherwise, a task has a potential cost that equals to P W
value plus possible I/O cost. Figure 6 shows the standard
deviation of task costs at the first 200 steps using Cost(Ti ) if
this task is determined, or its potential computation weight

196

Figure 5(b) depicts the first refinement upon the output of
Stage 1. The first edge probed in Figure 5(a) is e5,4 because
T4 has the highest cost and T5 has the lowest cost among
all incoming neighbors of P4 (i.e. P1 and P5 ). The reversion
of edge e5,4 to e4,5 reduces Cost(T4 ) from 81.6 to 51 and
boosts T5 to be the task with the highest assigned weight,
ready for the next probe. Since the flip of any incoming
edge to P5 does not further reduce Cost(T5 ), we do not flip.
Finally, Stage 2 produces a comparison graph as shown in
Figure 3(c) with Cost(D)=67.1.

4.3

Tasks in ready queue
T1

...

T4

Tv

1 2

...

...

...

...

Cores

q

Figure 7: Greedy execution of v tasks at runtime on a cluster
of machines with q cores.

Competitiveness Analysis

Pi ∈G

T3

Machine

Theorem 2 The two-stage load assignment with a greedy
scheduler produces a solution with job completion time P Tq
competitive to the optimal solution with completion time P Topt .
Their relative ratio for dedicated q cores satisfies
P Tq
2
≤ (3 − )(1 + δ).
P Topt
q

We do not know how the optimum scheduling solution
dynamically maps tasks to machines at runtime as shown
in Figure 7. However, we can use a bound analysis to show
that our heuristic approach performs competitively in a constant factor compared to the optimum. We first address the
load balancing issue without awareness of the machine location. Network distances impact the I/O and communication
cost, but this cost is relatively less significant compared to
computation load imbalance in PSS. Define
δ = max (

T2

Our analysis in the appendix shows that with computationdominating tasks and a greedy scheduling policy, the upper
bound of execution time is affected by the weight of the
heaviest task. This supports our load balancing optimization that targets the minimization of the maximum task
weight during load assignment.
Stage 1 may produce an unbalanced initial assignment in
which some nodes absorb too much computation, especially
in dense graphs. Stage 2 mitigates this issue with a sequence
of refinements. The following theorem illustrates that for a
fully connected graph, our approach delivers a near-optimal
solution, and it can be inferred from the proof that the refinement process carried out in Stage 2 is the main reason
that this goal is accomplished.

fc (Pj )
fc (Pi )
, max
).
f (Pi , Pi ) ej,i ∈G f (Pi , Pj )

This ratio represents the overhead ratio of I/O and communication involved in each task compared to its computation.
In our experiments as shown in Table 3, I/O overhead is relatively small. Given this computation-dominating setting,
for a cluster of machines with multiple CPU cores, we will
simply view that the whole cluster has q cores without differentiating their machine location. The overhead in accessing
data locally or remotely is captured in ratio δ.
Theorem 1 shows the result of two-stage load assignment
algorithm is competitive to the smallest possible cost without knowing the number of cores available. Theorems 2 and
3 characterize the competitiveness of the algorithm to the
optimum when the similarity graph is scheduled to q cores.
The theorem proofs are listed in the appendix.

Theorem 3 The two-stage load assignment with a greedy
scheduler is competitive to the optimum for a fully connected
similarity graph with equal partition sizes and equal computation costs in self-comparison and inter-partition comparison. Their relative ratio satisfies
P Tq
≤ 1 + δ.
P Topt

Theorem 1 Define Costmin (G) as the smallest cost of a
comparison graph derived from a given similarity graph G.
The two-stage load assignment algorithm produces a comparison graph D with Cost(D) competitive to Costmin (G).
Their relative ratio satisfies

5.

DATA PARTITIONING OPTIMIZATION

This section presents an improved partitioning method
for Phase 1 of partition-based similarity search presented in
[3]. The goal of this improvement is twofold: 1) to detect
more dissimilarity among partitions to avoid unnecessary
data I/O and comparison, and 2) to reduce the size gap
among partitions and facilitate the load balancing process.

Cost(D) ≤ 2(1 + δ)Costmin (G).
The above result shows that the tasks produced by the
two-stage algorithm have a fairly balanced cost distribution. As illustrated in Figure 7, a simple runtime scheduling
heuristic is to assign tasks to idle computing units whenever they become available [14]. For example, the Hadoop
MapReduce [12] scheduler works by assigning ready tasks
in a greedy fashion with the best effort of preserving data
locality. Once the central job tracker detects the availability
of a task tracker, it assigns a ready task to the task tracker
as long as there exists an unassigned task. When deciding
which task to assign, it favors the tasks processing data local to or close to the machine of the task tracker. What is
the performance behavior of our comparison tasks scheduled
under such a greedy policy?
The next theorem shows that under a greedy scheduler,
the tasks produced by the two-stage algorithm perform competitively compared to an optimum solution.

5.1

Dissimilarity Detection with Hölder’s
Inequality

To identify more dissimilar vectors without explicitly computing the product of their features, we use Hölder’s inequality to bound the similarity of two vectors:
Sim(di , dj ) ≤ kdi kr kdj ks
where r1 + 1s = 1. k · kr and k · ks are r-norm and s-norm
values. r-norm is defined as
X
kdi kr = (
|wi,t |r )1/r .
t

With r = 1, s = ∞, the inequality becomes Sim(di , dj ) ≤
kdi k1 kdj k∞ , which is a special case introduced in [3].

197

If the similarity upper-bound is less than τ , such vectors are not similar and comparison between them can be
avoided. The algorithm that produces partitions following
Hölder’s inequality is described as follows.

reaching a partition size threshold. Each partition inherits
the dissimilar relationship from its original sublayer. The
new partitions together with the undivided sublayers form
the undirected similarity graph G ready for load assignment.

1. Divide all vectors evenly to produce l consecutive layers
L1 , L2 , · · · , Ll such that all vectors in Lk have lower rnorm values than the ones in Lk+1 .
2. Subdivide each layer further as follows. For the i-th layer
Li , divide its vectors into i disjoint sublayers Li,1 , Li,2 ,
· · · , Li,j . With j < i, members in sublayer Li,j are extracted from Li by comparing with the maximum r-norm
value in layer Lj :
Li,j = {dx |dx ∈ Li and max kdy kr <
dy ∈Lj

6.
6.1

τ
}.
kdx ks

Proposition 1 Given i > j, vectors in sublayer Li,j are
not similar to the ones in any sublayer Lk,h where k ≤ j
and k ≥ h.

L 1,1
L 2,2

6.2
L 3,1

.
..
L i,1

L 3,2

.
..
L i,2

L 3,3

.
..
L i,3

..
...

i,i

Figure 8: Dissimilarity relationship among data partitions.
Figure 8 illustrates the dissimilarity relationship among
these sublayers as partitions and each pointing edge represents a dissimilarity relationship. For example, Li,2 is not
similar to L1,1 , L2,1 , or L2,2 in the top two layers.

5.2

Datasets and Metrics

The following datasets are used: 1) Twitter dataset containing 100 million tweets with 18.5 features per tweet on average after pre-processing. Dataset includes 20 million real
user tweets and additional 80 million synthetic data generated based on the distribution pattern of the real Twitter data but with different dictionary words. 2) ClueWeb
dataset containing about 40 million web pages, randomly
selected from the ClueWeb collection [5]. The average number of features is 320 per web page. We choose 40M records
because it is already big enough to illustrate the scalability. 3) Yahoo! music dataset (YMusic) used to investigate
the song similarity for music recommendation. It contains
1,000,990 users rating 624,961 songs with an average feature
vector size 404.5.

.
L

Implementation Details

We have implemented our algorithms in Java using Hadoop
MapReduce. Prior to the comparison computation, records
are grouped into dissimilar partitions and this partitioning
step including norm value sorting is parallelized. The cost of
parallel partitioning is relatively small and is roughly 3% of
the total parallel execution time in our experiments. During
the load balancing step, the two-stage algorithm defines the
comparison direction among potentially similar partitions,
generates a comparison graph stored in a distributed cache
provided by Hadoop, and derives a set of parallel tasks defined in Figure 2.
Hadoop runtime scheduler monitors the load of live nodes
in the cluster and assigns a PSS task to the first idle core.
Such a dynamic and greedy scheme can absorb potential
skewness in data that fluctuates the actual computational
cost. Theorem 2 reflects the competitiveness of PSS tasks
scheduled under Hadoop greedy policy. During execution,
each task loads the assigned partition with a user-defined
reader, obtains a list of partitions to be compared with from
the comparison graph file, and loops through the partition
list to conduct partition-wise comparison.

This partitioning algorithm has a complexity of O(n log n)
for n vectors and can be easily parallelized. Each sublayer
is considered as a data partition and these partitions have
dissimilarity relationship with the following property.

L 2,1

PERFORMANCE EVALUATION

Even Partition Sizes

To facilitate load balancing in the later phase, we aim at
creating more evenly-sized partitions at the dissimilarity detection phase. One way is to divide the large sublayers into
smaller partitions. Its weakness is that it introduces more
potential similarity edges among these partitions, hence the
similarity graph produced becomes denser, more communication and I/O overhead are incurred during runtime. Another method targets at approximately the same Li,j size
for any i ≤ j using a non-uniform layer size. For example,
let the size of layer Lk be proportional to the index value k,
following the fact that the number of sublayers in Lk is k in
our algorithm. The main weakness of this approach is that
less dissimilarity relationships are detected as the top layers
become much smaller.
We adopt a hierarchical partitioning that identifies large
sublayers, detects dissimilar vectors inside these sublayers,
and recursively divides them using the procedure discussed
in Section 5.1. The recursion stops for a sublayer when

Dataset
Size
AMD
Intel
AMD/df-limit

Twitter
4M
100M
45
45,157∗
26.7 25,438∗
1.27
797∗

ClueWeb
1M
40M
50
79,845∗
29.3 46,946∗
4.55
7,286∗

YMusic
625K
31.95
17.8
6.23

Table 2: Sequential time in hours on AMD Opteron 2218
2.6GHz and Intel X5650 2.66GHz processors (τ =0.8).
Experiments are conducted on a cluster of servers each
with 4-core AMD Opteron 2218 2.6GHz processors and 8G
memory and a cluster with Intel X5650 6-core 2.66GHz dual
processors and 24GB of memory per node. Rows 3 and 4
of Table 2 list the sequential execution time in hours for
three benchmarks with different input sizes when running
PSS with static partitioning and two-stage load balancing.
We set similarity threshold τ as 0.8 throughout our experiments unless otherwise specified. The values marked with ∗

198

are estimated by sampling part of its computation tasks and
considering the fact that computation load grows quadratically as problem size grows. From the results in Rows 3
and 4, APSS is a time consuming process. Even for a Twitter dataset with 20M tweets, the entire dataset can fit in
the memory; but it still takes many days to produce the results. Parallelization can shorten the job turnaround time
and speedup iterative data analysis and experimentation.
Stop words are removed in the Twitter and ClueWeb input datasets; additional approximated preprocessing may be
applied to reduce sequential time significantly if the trade-off
in accuracy is acceptable [13, 19]. For example, the bottom
row of Table 2, marked as “df-limit”, lists the sequential time
on an AMD core after removing features with their vector
frequency exceeding an upper limit proposed in [19]. After
sampling a 8M ClueWeb dataset, 49 words with document
frequency above 200,000 are excluded in web page comparison and the sequential time is shortened by 11x. Using
this df-limit strategy reduces the sequential time by 35.3x
or more for Twitter and by 5.1x for YMusic. In the rest of
this section, we report performance without using approximated preprocessing such as df-limit.
Noted that the algorithms discussed in this paper conduct exact similarity comparison with no approximation for
a given input dataset. One may consider using approximation such as LSH mapping which roughly groups similar
vectors into buckets and our algorithm can be applied within
each big bucket produced by LSH. This is beyond the scope
of this paper and we will investigate this in the future work.
In the rest of this section, we mainly report performance
on the AMD cluster because a larger Intel cluster environment was less available for us to conduct experiments. Our
evaluation has the following objectives: 1) Demonstrate the
problem complexity and the execution scalability of tasks
produced by the two-stage load balancing method. We report the speedup over the sequential time as we scale the
number of cores. 2) Assess the effectiveness of the proposed
two-stage optimization compared with the circular load balancing scheme. 3) Evaluate the performance of the generalized static partitioning algorithm in detecting dissimilarity
and narrowing the size gaps among partitions.

Scalability and Comparisons
300

45

Clueweb speedup
Clueweb time
Twitter speedup
Twitter time
YMusic speedup
YMusic time

40.59

40
35

Speedup

200

30
25

23.16

20
17.31
100

50

8.47
5.06

.695
0

15

12.17
9.53

50

.419
100

.267
200

3.52
.25
300

Parallel time (hr)

6.3

Due to the time constraint in our shared cluster environment, we report the average execution time of multiple runs
after randomly selecting 10% of ClueWeb parallel tasks and
20% of Twitter tasks. Such a sampling methodology follows
the one used in [19]. Speedup is defined as the sequential
time of these tasks divided by the parallel time. The performance of our scheme scales well as the number of CPU cores
increases. The efficiency is defined as the speedup divided
by the number of cores used. For the two larger datasets,
the efficiency is about 83.7% for ClueWeb and 78% for Twitter when 100 cores are used. When running on 300 cores,
the efficiency can still reach 75.6% for ClueWeb and 71.7%
for Twitter. The decline is most likely caused by the increased I/O and communication overhead among machines
in a larger cluster. Efficiency for YMusic with 31.95 hour sequential time are 76.2% with 100 cores and 42.6% with 300
cores. There is no significant reduction of parallel time from
200 cores to 300 cores, remaining about 15 minutes. The
problem size of this dataset is not large enough to use more
cores for amortizing overhead. Still parallelization shortens
search time and that can be important for iterative search
experimentation and refinement.
We also calculate the average time for comparing each pair
of vectors normalized by their average length in a dataset.
Namely Parallel time×No of cores . The normalized pairNo of pairs×Vector length
wise comparison time is about 1.24 nanoseconds for Twitter
and 0.74 nanoseconds for ClueWeb using 300 AMD cores
given τ = 0.8. Varying the number of cores affects due to
the difference in parallel efficiency. Varying τ also affects because it changes the results of dissimilarity-based partitioning and graph structure. This number can become smaller
if approximated preprocessing is adopted [13, 19].
To confirm the choice of partition-based search, we have
also implemented an alternative MapReduce solution to exploit parallel score accumulation following the work of [19, 8]
where each mapper computes partial scores and distributes
them to reducers for score merging. The parallel score accumulation is much slower because of the communication
overhead incurred in exploiting accumulation parallelism.
For example, to process 4M Twitter data using 120 cores,
parallel score accumulation is 19.7x slower than partitionbased similarity search which has much simpler parallelism
management and has no shuffling between mappers and reducers. To process 7M Twitter data, parallel score accumulation is 25x slower. As a sanity check, we also estimate the normalized pair-wise comparison times reported
in [19]. To compare 90K vectors with 4.59 million MEDLINE abstracts using at most 60 terms per vector on about
120 cores each with 2.8GHz CPU, it takes a MapReduce
solution called PQ [19] 448 minutes with approximated preprocessing, meaning about 130.1 nanoseconds to compare
each normalized vector pair.

10

Dataset

Cores

Twitter
ClueWeb
YMusic

100
300
20

5
0

Number of cores

Figure 9: X axis is the number of cores used. Left Y axis is
speedup. Right Y axis is parallel execution time.

Static
Partitioning
2.8%
2.1%
3.0%

Similarity Comparison
Read Write
CPU
0.9% 11.7% 84.6%
1.9%
7.8%
88.2%
2.3%
1.8%
92.9%

Table 3: Cost of static partitioning and runtime cost distribution of PSS in parallel execution.

Figure 9 shows the speedup and parallel time for processing 40M ClueWeb dataset, 100M Twitter dataset and 625K
YMusic dataset when varying the number of AMD cores.

Table 3 shows that static partitioning which is also parallelized takes 2.1% to 3% of the total parallel execution time.

199

The table also shows the time distribution in terms of data
I/O and CPU usage for similarity comparison. Data I/O
is to fetch data and write similarity results in the Hadoop
distributed file system. This implies that the computation
cost in APSS is dominating and hence load balance of the
computation among cores is critical for overall performance.

Effectiveness of Two-Stage Load Balance

Percentage of skipped vector pair comparison

6.4

of pairs detected as dissimilar. r=1 reflects the results in [3].
For ClueWeb, 19% of the total pairs under comparison are
detected as dissimilar with r=3 while only 10% for r=1.
For Twitter, the percentage of pairs detected as dissimilar
is 34% for r=4 compared to 17% for r=1. The results show
that choosing r as 3 or 4 is most effective. We have used the
best r value for partitioning each dataset.

The experiments discussed in the previous sub-section have
adopted the two-stage load balancing and improved static
partitioning. In the rest of this section, we assess the impact of optimization using 100 AMD cores for 20M Twitter,
300 cores for 8M ClueWeb, and 20 cores for YMusic. We
choose these sizes for faster experimentation while the performance impact of optimization for larger sizes is similar.
100%

Improvment percentage

80%

Max/Avg
Twitter
2.14
ClueWeb
4.25
YMusic
8.97
Std. Dev/Avg
Twitter
1.44
ClueWeb
2.68
YMusic
2.87

Stage2
Stage1

60%

40%

20%

0%
Twitter

Clueweb

YMusic

2-stage
1.45
3.25
6.68

70%

r=1
r=3
r=4
r=8
r = 16
r= inf.

60%
50%
40%
30%
20%
10%
0%

Twitter

Clueweb

YahooMusic

Benchmarks

Figure 11: Improved partitioning with different r-norms.
70%

Percentage of skipped comparison pairs

Circular

80%

0.83
1.77
2.06

(b)

(a)
Figure 10: (a) Parallel time reduction contributed by Stages
1 and 2 compared to the circular assignment. (b) Maximum
task cost and standard deviation over the average task cost
with circular assignment or with two-stage assignment.

60%
Uniform
Nonuniform

50%
40%
30%
20%
10%
0%

Twitter(r:4)

Clueweb(r:3)

YahooMusic(r:3)

Benchmarks

Figure 12: Uniform v.s. non-uniform layer size.

Figure 10(a) shows the improvement percentage in parallel time using two-stage load assignment compared to the
baseline circular assignment. Parallel time with two-stage
assignment is about 23.2 hours for Twitter, 14 hours for
ClueWeb, and 1.7 hours for YMusic respectively. The figure also marks the improvement percentage contributed by
Stage 1 and Stage 2 respectively. The overall improvement
from the two-stage load assignment is 41% for Twitter, 32%
for ClueWeb, and 27% for YMusic. Stage 1 contributes a
large portion of the total improvement. Stage 2 contributes
about 4% for Twitter, 12% in ClueWeb, and 10% for YMusic. Similarity graphs of ClueWeb and YMusic are denser
and Stage 1 can be too aggressive in making the light partitions absorb too much comparison computation. Hence, the
refinements in Stage 2 become more effective in such cases.
To examine the weight difference across all tasks, Figure 10(b) shows the maximal task weight with circular mapping or with the two-stage balancing method divided by the
average task cost. It also lists the cost standard deviation
divided by the average task cost. The larger these two ratios
are, the more severe load imbalance is. Compared to circular
mapping, the two-stage assignment reduces the Max./Avg.
ratio by 32.2%, 23.5%, and 25.5% for Twitter, ClueWeb, and
YMusic datasets respectively. For Std. Dev./Avg. ratio, the
reduction is 42.4%, 34.0%, and 28.2% respectively.

6.5

As discussed in Section 5.2, the initial layer size selection
affects the size variation of the final partitions. Figure 12
gives a comparison of using uniform layer size and using nonuniform size with the marked r-norm settings. The uniformsized layers yields better results. For ClueWeb, the uniform
layers detect 2.6x as many dissimilar pairs compared to the
non-uniform layers. Thus we opt for the uniform layers and
recursively apply hierarchical partitioning to even out the
sizes of sublayers.
Table 4 shows the effectiveness of recursive hierarchical
data partitioning. The ratio of standard deviation of partition sizes over the average size drops by 9.7% for Twitter,
22.3% for ClueWeb, and 3.7% for YMusic. The relatively
even workload benefits the task load balancing process and
reduces parallel execution time by 5% to 18% additionally.
Dataset
Twitter
ClueWeb
YMusic

Std. Dev/Avg
(Without)
1.75
0.67
0.82

Std. Dev/Avg
(With)
1.58
0.52
0.79

Parallel time
reduction
8.23%
18.23%
5.29%

Table 4: Change of partition sizes and parallel time with or
without the recursive hierarchical partitioning.

7.

Improved Data Partitioning

CONCLUDING REMARKS

The main contribution of this paper is a two-stage load
balancing algorithm for efficiently executing partition-based

Figure 11 provides a comparison of the improved data
partitioning with different r-norms. Y axis is the percentage

200

similarity search in parallel. The analysis provided shows
its competitiveness to the optimal solution. This paper also
presents an improved and hierarchical static data partitioning method to detect dissimilarity and even out the partitions sizes. Our experiments demonstrate that the two-stage
load assignment improves the circular assignment by up to
41% in the tested datasets. The improved static partitioning avoids more unnecessary I/O and communication and
reduces the size gaps among partitions with up to 18.23%
end-performance gain in the tested cases.
Static partitioning can be processed efficiently in parallel, and could be further extended to handle incremental
updates. Another future work is to study a hybrid scheme
that integrates approximate methods such as LSH with our
exact method for larger datasets when a trade-off between
speed and accuracy is acceptable.
Acknowledgments. We thank Paul Weakliem, Kadir
Diri, and Fuzzy Rogers for their help with the computer
cluster, and Xifeng Yan, Ömer Eğecioğlu, and the anonymous referees for their insightful comments. This work is
supported in part by NSF IIS-1118106 and Kuwait University Scholarship. Equipment access is supported in part by
the Center for Scientific Computing at CNSI/MRL under
NSF DMR-1121053 and CNS-0960316. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect
the views of the National Science Foundation.

without loss of generality, these partition nodes are called
Pk , Pk+1 , · · · , Pv . Costmin (Gk ) satisfies
P
Pv
j=k f (Pj , Pj ) +
k≤i<j≤v,ei,j ∈Gk f (Pi , Pj )
Costmin (Gk ) ≥
v−k+1
Pv
Pv
j=k P W (Gk , Pj )
j=k f (Pj , Pj ) +
=
2(v − k + 1)
Pv
j=k P W (Gk , Pj )
>
2(v − k + 1)
(v − k + 1)P W (Gk , Pk )
≥
2(v − k + 1)
1
= P W (Gk , Pk ).
2
Also notice that graph Gk+1 is a subgraph of Gk , then
Costmin (Gk ) ≥ Costmin (Gk+1 ).
Also following the definition of δ and the setting of Cost(Tk )
in Stage 1 of two-stage load assignment,
Cost(Tk ) ≤ P W (Gk , Pk )(1 + δ).
With the induction assumption and the above three inequalities, the outcome of Stage 1 with respect to Dk satisfies
Cost1 (Dk ) = max{Cost1 (Dk+1 ), Cost(Tk )}

APPENDIX

≤ max{2(1 + δ)Costmin (Gk+1 ), P W (Gk , Pk )(1 + δ)}
≤ (1 + δ) max{2Costmin (Gk ), 2Costmin (Gk )}

Theorem 1

= 2(1 + δ)Costmin (Gk ).

Proof. Let Cost1 (D) be the value of Cost(D) after Stage
1. Refinements in Stage 2 do not increase Cost(D) and thus
Cost(D) ≤ Cost1 (D). We just need to show that Stage 1
can reach a solution competitive to Costmin (G). Namely
Cost1 (D) ≤ 2(1 + δ)Costmin (G).
Let Di be a directed graph with all nodes ∈ Gi and all
edge orientations determined through the steps from Gi to
Gv−1 in stage 1, given a total of v partitions and D1 =D,
G1 =G.
We use an induction to prove this theorem. The induction
goes from Dv−1 to D1 , reversing to the creation process in
Stage 1. Towards the end of Stage 1, subgraph Gv−1 has
two nodes left, and at most one edge between them. Choosing the partition with the smaller computation weight to
perform the inter-partition comparison will add some communication and I/O cost, but leads to the balanced solution
in this special case. Thus Cost1 (Dv−1 ) = Costmin (Gv−1 ).
Dk

Therefore
Cost(D) ≤ Cost1 (D) = Cost1 (D1 ) ≤ 2(1 + δ)Costmin (G).

Theorem 2
Proof. First we examine the Gantt chart of the schedule
from time 0 to P Tq , identifying the total computation and
I/O cost, and
P the total computation
Pthe idle time. Define
cost as π = Pi ∈D f (Pi , Pi ) + ej,i ∈D f (Pi , Pj ), where D
is the comparison graph generated by two-stage load assignment. Then the total computation and I/O cost is bounded
by π(1 + δ). Since the scheduling algorithm assigns a task
whenever there is an idle core available, the total idle time in
all q cores from time 0 to time P Tq is at most (q−1)Cost(D).
Then
max(Cost(D),

Dk+1

Given an optimal schedule for similarity graph G on q
cores, a comparison graph can be derived. Let Costopt (G)
be the largest task cost in this comparison graph. Notice

Gk+1
Gk

(q − 1)Cost(D) + π(1 + δ)
π
) ≤ P Tq ≤
.
q
q

Costmin (G) ≤ Costopt (G).

Pk

The optimal solution satisfies

Figure 13: Illustration of Dk and Dk+1 for induction proof.

max(Costopt (G),

Our induction assumption is that the solution for subgraph Dk+1 is competitive. Namely Cost1 (Dk+1 ) ≤ 2(1 + δ)
Costmin (Gk+1 ). We want to show the solution for Dk is also
competitive. Figure 13 illustrates subgraphs Dk and Dk+1 .
Note that subgraph Dk and Gk both have v−k+1 nodes and

π
) ≤ P Topt .
q

Following Theorem 1,
P Tq ≤

201

q−1
2(1 + δ)Costmin (G) + (1 + δ)P Topt .
q

Thus

[2] M. Alabduljalil, X. Tang, and T. Yang. Cache-conscious
performance optimization for similarity search. In ACM
Conference on Research and Development in Information
Retrieval (SIGIR), pages 713–722, 2013.
[3] M. Alabduljalil, X. Tang, and T. Yang. Optimizing parallel
algorithms for all pairs similarity search. In ACM Inter. Conf.
on Web Search and Data Mining, pages 203–212, 2013.
[4] A. Arasu, V. Ganti, and R. Kaushik. Efficient exact
set-similarity joins. In Proc. of VLDB ’2006, pages 918–929.
[5] L. T. I. at Carnegie Mellon University. The clueweb09 dataset,
http://boston.lti.cs.cmu.edu/data/clueweb09.
[6] C. Badue, R. Baeza-Yates, B. Ribeiro-Neto, a. Ziviani, and
N. Ziviani. Analyzing imbalance among homogeneous index
servers in a web search system. Information Processing &
Management, 43(3):592–608, May 2007.
[7] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information
Retrieval. Addison Wesley, 1999.
[8] R. Baraglia, G. D. F. Morales, and C. Lucchese. Document
similarity self-join with mapreduce. In 2010 IEEE Inter. Conf.
on Data Mining, pages 731–736.
[9] R. J. Bayardo, Y. Ma, and R. Srikant. Scaling up all pairs
similarity search. In Proc. of Inter. Conf. on World Wide
Web, WWW ’2007, pages 131–140. ACM.
[10] R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E. Leiserson,
K. H. Randall, and Y. Zhou. Cilk: An efficient multithreaded
runtime system. In Journal of Parallel and Distributed
Computing, pages 207–216, 1995.
[11] A. Chowdhury, O. Frieder, D. A. Grossman, and M. C.
McCabe. Collection statistics for fast duplicate document
detection. ACM Trans. Inf. Syst., 20(2):171–191, 2002.
[12] J. Dean and S. Ghemawat. Mapreduce: simplified data
processing on large clusters. In OSDI’04, pages 137–150.
[13] T. Elsayed, J. Lin, and D. W. Oard. Pairwise document
similarity in large collections with mapreduce. In ACL ’2008,
pages 265–268.
[14] M. R. Garey and R. L. Grahams. Bounds for multiprocessor
scheduling with resource constraints. SIAM Journal on
Computing, 4:187–200, 1975.
[15] A. Gionis, P. Indyk, and R. Motwani. Similarity search in high
dimensions via hashing. In VLDB, pages 518–529, 1999.
[16] H. Hajishirzi, W. tau Yih, and A. Kolcz. Adaptive
near-duplicate detection via similarity learning. In Proc. of
ACM SIGIR, pages 419–426, 2010.
[17] N. Jindal and B. Liu. Opinion spam and analysis. In Proc. of
ACM WSDM’2008, pages 219–230.
[18] E. Kayaaslan, S. Jonassen, and C. Aykanat. A Term-Based
Inverted Index Partitioning Model. ACM Transactions on the
Web (TWEB), 7(3):1–23, 2013.
[19] J. Lin. Brute force and indexed approaches to pairwise
document similarity comparisons with mapreduce. In Proc. of
ACM SIGIR’2009, pages 155–162.
[20] A. Metwally, D. Agrawal, and A. El Abbadi. Detectives:
detecting coalition hit inflation attacks in advertising networks
streams. In Proc. of WWW’2007, pages 241–250. ACM.
[21] G. D. F. Morales, C. Lucchese, and R. Baraglia. Scaling out all
pairs similarity search with mapreduce. In 8th Workshop on
Large-Scale Distri. Syst. for Information Retrieval, 2010.
[22] M. Sahami and T. D. Heilman. A web-based kernel function for
measuring the similarity of short text snippets. In Proc. of
WWW ’2006, pages 377–386. ACM.
[23] N. Shivakumar and H. Garcia-Molina. Building a scalable and
accurate copy detection mechanism. In Proc. of DL’96, pages
160–168.
[24] F. Ture, T. Elsayed, and J. Lin. No free lunch: brute force vs.
locality-sensitive hashing for cross-lingual pairwise similarity. In
Proc. of SIGIR’2011, pages 943–952.
[25] R. Vernica, M. J. Carey, and C. Li. Efficient parallel
set-similarity joins using mapreduce. In Proc. of ACM
SIGMOD 2010, pages 495–506.
[26] Y. Wang, A. Metwally, and S. Parthasarathy. Scalable all-pairs
similarity search in metric spaces. Proc. of ACM SIGKDD,
pages 829–837, 2013.
[27] C. Xiao, W. Wang, X. Lin, and J. X. Yu. Efficient similarity
joins for near duplicate detection. In Proc. of WWW 2008,
pages 131–140. ACM.
[28] M. Zaharia, K. Elmeleegy, D. Borthakur, S. Shenker, J. S.
Sarma, and I. Stoica. Delay scheduling: A simple technique for
achieving locality and fairness in cluster scheduling. In Proc. of
EuroSys, pages 265–278, 2010.

2
P Tq
q−1
≤
2(1 + δ) + (1 + δ) = (3 − )(1 + δ).
P Topt
q
q

Theorem 3
Proof. Assume that the number of partitions v is an
odd number and we show that all tasks formed have equal
weights. The optimality for an even number v can be proved
similarly.
Since all nodes have the same self-comparison cost, the
same cost to compare with others, and the same cost for
communication and data I/O, the cost of each task is proportional to the number of incoming edges for the corresponding node in D. We claim that every node at the end
incoming edges in comparison
of load assignment has v−1
2
neighbors.
graph D, namely it compares with v−1
2
We prove by contradiction. If some nodes have the number of incoming edges different from v−1
, then some nodes
2
must have more than v−1
incoming
edges
while some other
2
edges since the total number
nodes must have less than v−1
2
of edges is v(v−1)
for a fully connected graph. Assume the
2
incoming edges, and
heaviest nodes Px has more than v−1
2
there exists an incoming edge from node Py with the number
− 1. Figure 14
of incoming edges less than or equals to v−1
2
illustrates an example with contradiction.
Px

Py

Figure 14: An example for proof by contradiction.
Given all partitions have the equal size, Stage 2 of load assignment should not have stopped since it could reverse the
edge between Tx and Ty , causing the decrease of Cost(Tx )
while Cost(Ty ) does not exceed the new value of Cost(Tx ).
That is a contradiction.
Thus each task Ti formed fetches from its v−1
neighbors.
2
Tasks have the same weight, leading to a perfect task distribution among q cores. Without loss of generality, we use
f (Pi , Pi ), f (Pi , Pj ), and fc (Pi ) to represent the cost of selfcomparison, inter-partition comparison, and data I/O respectively for all tasks. Then
v
v−1
(f (Pi , Pi ) + fc (Pi ) +
(f (Pi , Pj ) + fc (Pj )))
q
2
v
v−1
f (Pi , Pj ))(1 + δ).
≤ (f (Pi , Pi ) +
q
2

P Tq =

The above upper bound without factor 1 + δ is the lower
bound for any schedule including the optimum. Thus the
solution derived is within 1 + δ of the optimum.

A.

REFERENCES

[1] F. Aiolli. Efficient top-n recommendation for very large scale
binary rated datasets. In Proc. of 7th ACM Conf. on
Recommender Systems, pages 273–280, 2013.

202

