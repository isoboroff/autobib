Discriminative Coupled Dictionary Hashing
for Fast Cross-Media Retrieval
Zhou Yu

Fei Wu

Yi Yang

College of Computer Science
Zhejiang University, China

College of Computer Science
Zhejiang University, China

yuz@zju.edu.cn

wufei@zju.edu.cn

School of ITEE
The University of Queensland,
Australia

yi.yang@uq.edu.au

Qi Tian

Jiebo Luo

Yueting Zhuang

Dept. of Computer Science
University of Texas, USA

Dept. of Computer Science
University of Rochester, USA

College of Computer Science
Zhejiang University, China

qitian@cs.utsa.edu

jluo@cs.rochester.edu

yzhuang@zju.edu.cn

ABSTRACT

Keywords

Cross-media hashing, which conducts cross-media retrieval
by embedding data from diﬀerent modalities into a common
low-dimensional Hamming space, has attracted intensive attention in recent years. The existing cross-media hashing
approaches only aim at learning hash functions to preserve
the intra-modality and inter-modality correlations, but do
not directly capture the underlying semantic information of
the multi-modal data. We propose a discriminative coupled dictionary hashing (DCDH) method in this paper. In
DCDH, the coupled dictionary for each modality is learned
with side information (e.g., categories). As a result, the coupled dictionaries not only preserve the intra-similarity and
inter-correlation among multi-modal data, but also contain
dictionary atoms that are semantically discriminative (i.e.,
the data from the same category is reconstructed by the
similar dictionary atoms). To perform fast cross-media retrieval, we learn hash functions which map data from the
dictionary space to a low-dimensional Hamming space. Besides, we conjecture that a balanced representation is crucial in cross-media retrieval. We introduce multi-view features on the relatively “weak” modalities into DCDH and
extend it to multi-view DCDH (MV-DCDH) in order to enhance their representation capability. The experiments on
two real-world data sets show that our DCDH and MVDCDH outperform the state-of-the-art methods signiﬁcantly
on cross-media retrieval.

Coupled dictionary learning; Cross-media retrieval; Hashing

1. INTRODUCTION
With the rapid development of Internet and social network, it has attracted increasing attention to study the correlations among multi-modal data. For example, an uploaded image on the Flickr web site is always tagged with
some related descriptions or labels; a microblog may consist
of a short text and correlative images. The relevant data
from diﬀerent modalities may have semantic correlations.
Therefore, it is desirable to support cross-media retrieval
across the data of diﬀerent modalities, e.g., the retrieval
of semantically-related textual documents in response to a
query image and vice versa. Due to the large-scale nature
of the existing multimedia data over the Internet, eﬃcient
retrieval of cross-media is particularly important.
An eﬀective way to speed up the similarity search is the
hashing-based method, which makes a tradeoﬀ between accuracy and eﬃciency by approximate nearest neighbor search.
The principle of hashing method is to map the high dimensional data into compact hash codes and generate the same
or similar hash codes for similar data.
The motivation of hashing is to solve the approximate
nearest neighbor (ANN) search problem. However, in the
cross-media retrieval, the NN cannot be directly obtained
as the data may come from diﬀerent modalities. Therefore,
most of the existing hashing approaches are not applicable to cross-media retrieval and cross-media hashing method
should be speciﬁcally studied.
Generally speaking, the existing hashing approaches can
be classiﬁed into three categories:

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information
Search and Retrieval

• uni-modal hashing: uni-modal hashing utilizes only a single type of feature (homogeneous feature) from
uni-modal data as input, aiming at learning hash functions to project the homogeneous feature to compact
hash codes.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00.
http://dx.doi.org/10.1145/2600428.2609563.

• multi-view hashing: multi-view hashing utilizes multiple types of features (heterogeneous features) from
uni-modal data as input, and learns hash functions to
project the heterogenous features to hash codes.

395

• cross-media hashing: cross-media hashing utilizes
data from multi-modalities (e.g, images and texts) as
input, and preserves the intra-modality similarity and
inter-modality correlation to learn hash functions. Thus,
the correlation of the data from diﬀerent modalities is
measurable.

• We propose a two-stage cross-media hashing framework consisting of the learning of discriminative coupled dictionaries and hash functions, respectively. The
learned discriminative coupled dictionaries in the ﬁrst
stage have both discriminative and similarity-preserving
capability.
• The discriminative coupled dictionary learning is formulated as an optimization problem of submodular
function and an approximation solution can be eﬃciently obtained using a greedy algorithm.

Most of the existing hashing approaches are uni-modal
hashing. One of the well-known uni-modal hashing method
is Locality Sensitive Hashing (LSH) [2], which uses random
projections to obtain the hash functions. However, due to
the limitation of random projection, LSH usually needs a
quite long hash code and hundreds of hash tables to guarantee good retrieval performance. To make the hash codes
compact, several learning based approaches are proposed.
Weiss et al. proposed Spectral Hashing (SH) [23] which
utilizes the distribution of training data and uses eigenfunction to obtain the hash functions. Compared with LSH, SH
achieves better performance since the hash functions capture the manifold structure of the data. Since then, many
extensions of SH have been proposed [27, 11, 20, 21, 5, 12].
However, in the real world applications, we can extract
heterogenous features from the data and some multi-view
hashing approaches are therefore leveraged to boost the retrieval performance[26, 17]. The principal idea of them is
to learn the hash functions while preserving the local structures of each individual feature and globally considering the
consistency of multi-view features.
Cross-media hashing is a new research area and there has
been only limited research eﬀorts focusing on it so far [3, 9,
28, 29, 15, 30, 25]. Most of the existing cross-media hashing approaches share the common idea of learning diﬀerent
hash functions individually for each modality and map the
data from diﬀerent modalities to a shared low-dimensional
Hamming space. However, such a binary embedding strategy often results in poor indexing performance for the shared
embedding space is not semantically discriminative, which
is signiﬁcantly important for cross-media retrieval.
In this paper, we propose a cross-media hashing framework titled Discriminative Coupled Dictionary Hashing (DCDH). Firstly, data from diﬀerent modalities along with
their classes or categories are jointly utilized to learn the
both discriminative and coupled dictionaries. The discriminative capability indicates that data from same category will
have similar sparse representation (i.e., sparse codes), and
the coupling means not only intra-modality similarity but
also inter-modality correlation will be preserved. As a result, DCDH assigns an explicit semantic meaning (i.e., topic) to each dictionary atom in multi-modal dictionaries and
thus makes the sparse representation for the multi-modal
data interpretable. Secondly, the obtained sparse codes for
the data over their corresponding dictionary are exploited
to learn the hash functions and further transform the sparse
codes to compact binary hash codes.
Furthermore, we ﬁnd that the representation capability of
the dictionaries from diﬀerent modalities varies and an “unbalanced” representation may adversely inﬂuence the performance of cross-media hashing. To address this problem,
we additionally incorporate multi-view features into DCDH
to enhance the representation capability of the dictionaries
from the relatively “weak” modalities. This extended version
of DCDH is named Multi-View DCDH (MV-DCDH).
The main contributions of this paper are three-fold:

• Multi-view and multi-modal data are jointly considered in the MV-DCDH framework. The multi-view
features is incorporated to strengthen the representation capability for the dictionary from a relatively
“weak” modality and lead to a balanced cross-media
representation. This enhancement improves the crossmedia retrieval performance of DCDH signiﬁcantly.
The rest of the paper is organized as follows: In Section
2, we review the related work of dictionary learning and
cross-media hashing approaches. In Section 3, we give out
the detailed explaination of our DCDH and its multi-view
extension MV-DCDH. The complexity of DCDH is analyzed
in Section 4. Experimental results and comparisons on two
real-world data sets are demonstrated in Section 5. Finally,
the conclusions are given.

2. RELATED WORK
2.1 Dictionary Learning
Beyond the traditional dictionary learning approaches [24,
1], coupled or semi-coupled dictionary learning approaches
[6, 22] attempt to learn dictionaries for multi-modal data
by minimizing the reconstruction error of each dictionary
and preserving the pairwise correspondence across diﬀerent modalities. However, these approaches are unsupervised
so that the class or category information is not exploited
and can not signiﬁcantly boost the performance of learned
coupled dictionaries. Zhuang et al. proposed a supervised
semi-coupled dictionary learning approach which introduces
the category side information into multi-modal dictionary
learning via a ℓ2,1 -norm regularization term.
Our proposed DCDH bears some resemblance to submodular dictionary learning (SDL) [7] that takes advantage of
the submodularity to learn dictionary eﬃciently. We extend
the idea from uni-modal data into multi-modal data in order
to learn discriminative coupled dictionaries .

2.2 Cross-media Hashing
Cross-media retrieval is a hot research focus in recent
years [16, 32, 31]. With the rapid advance of hashing, some
cross-media hashing approaches have been proposed [3, 9,
28, 29, 15, 30, 25].
The problem of cross-media hashing was ﬁrst proposed
by Bronstein et al. in CMSSH [3]. Speciﬁcally, given two
modalities of data sets, CMSSH learns two groups of hash
functions to ensure that if two data points (with diﬀerent modalities) are relevant, their corresponding hash codes
are similar and otherwise dissimilar. However, CMSSH only preserves the inter-modality correlation but ignores the
intra-modality similarity. Kumar et al. extended Spectral
Hashing [23] from the traditional uni-modal setting to the

396

multi-modal scenario and proposed CVH [9]. CVH attempts to generate the hash codes by minimizing the distance of
hash codes for the similar data and maximizing the distance
for the dissimilar data. The inter-view and intra-view similarities are both preserved in CVH. LCMH [30] adopts a
“two-stage” strategy to learn the cross-media hash functions:
First, the data within each modality are low-rank represented using the anchor graph[11]. Then, hash functions for each
modality are learned to project the data from each anchor
graph space into a shared Hamming space. MLBE employs a
probabilistic generative model to encode the intra-similarity
and inter-similarity of data across multiple modalities. According to the estimation of maximum a posteriori, the binary latent factors can be obtained and then be taken as the
hash codes in MLBE. However, the hash codes generated by
MLBE do not require the independency between diﬀerent
hash bits, and may obtain highly redundant hash bits.
Wu et al. introduced dictionary learning into cross-media
hashing [25]. By the joint modeling of the intra-modality
similarity and inter-modality correlation among multi-modal
data with a hypergraph, the coupled dictionaries with a hypergraph laplacian regularizer are learned in an iterative
manner. The learned dictionary for each modality is then
adopted as the hash functions, and the sparse code of each
data point over its corresponding dictionary is regarded as
the hash code to perform cross-media retrieval.
Inspired by the eﬀectiveness of using the coupled dictionary space to represent the data points from diﬀerent modalities, we additionally emphasize discrimination when learning coupled dictionaries in order to make the shared dictionary space interpretable. Furthermore, unlike [25] that
directly exploits the sparse codes as the hash codes, we further learn hash functions to map the sparse codes to binary
hash codes.

3.

Table 1: Notations used in this paper
Symbols
Explanation
M
the number of modalities
m
m ∈ {1, 2, ...M } is one of the M modalities
K
the common size of the coupled dictionaries
L
the length of the hash codes
N
the common size of each data set
p1 ,...,pM
the dimensionality of each modality
m
m
pm ×N
X 1 ,...,X M
data set X m = [xm
1 , x2 , ..., xNm ] ∈ R
m
1
M
m
m
m
m
D ,...,D
dictionary D = [d1 , d2 , ..., dK ] ∈ Rp ×K
1
M
m
K×N
m
Z ,...,Z
sparse codes Z ∈ R
of X w.r.t. Dm

3.1 Unified Graph Representation of Labeled
Multi-modal Data
To well model the intra-modality similarity and the intermodality correlation of M data sets, we resort to the uniﬁed
graph G(V, E, w) similar to [19]. The vertex set V denotes
the data from all the data sets, and the edge set E models
the pairwise intra-modality similarity or the inter-modality
correlation between data points. The weight of an edge is
measured by some similarity functions which we will discuss
in the following.
To model the intra-modality similarity within the same
modality, we adopt the local similarity metric with a Gausm
sian kernel. The intra-modality similarity wi,j
of two data
m
m
points xi and xj from modality m is deﬁned as:

m 2
i −xj |
 − |xm
m
m
m
m
2σ2
e
, if xm
i ∈ Nk (xj ) or xi ∈ Nk (xj )
wi,j =
0
otherwise
(1)
where NK (x)P
represents the set of k-nearest neighbours of
m
m 2
x and σ = N1
i,j |xi − xj | is the expectation over all the
pairwise distance in X m .
It takes O(N 2 pm ) time to compute an intra-modality similarity matrix. When N is large, we can use some approximated methods such as the anchor graph structure to construct this similarity matrix eﬃciently [11]. In this paper,
we simply use the exact k-NN graph in Eq.(1).
To model the inter-modality correlation of the data from
two modalities (we name them as modality a and b, a 6= b
a,b
and a, b ∈ {1, 2, ..., M }), the similarity function wi,j
for two
data xai and xbj is deﬁned as:
(
1, if xai has known correlation with xbj
a,b
wi,j =
(2)
0, otherwise

THE OVERVIEW OF DCDH

In this section, we introduce the detail of DCDH. Figure
1 illustrates the algorithmic ﬂowchart of our DCDH. For
the sake of illustrative simplicity, we assume that only two
kinds of data (e.g., images and texts) are available in Figure
1. The proposed DCDH mainly consists of the following two
stages:
1. Discriminative coupled dictionary learning: In Figure
1, the clusters of multi-modal data can be learned by
a submodular function with the help of inter-modality
correlation, intra-modality similarity as well as the supervised side information (e.g.,category labels). Given
a cluster, its centroid is taken as a dictionary atom
of their corresponding dictionary. That is to say, the
dictionary atom from one modality is coupled with the
corresponding dictionary atoms from the other modalities in the same cluster.

Moreover, to better understand the semantics of data, we
additionally exploit the category information. Let C be the
category-labels set indicating the category label of each data point (i.e., each vertex in G), the ﬁnal category-labeled
uniﬁed graph is denoted as G(V, E, w, C).

2. Unified hash functions learning: Based on the learned
coupled dictionaries, the data points from diﬀerent modalities can be represented as sparse codes in a
uniﬁed dictionary space. Afterwards, utilizing the sparse property, hash functions which project the sparse
codes into compact binary hash codes can be learned
eﬃciently.

3.2 Discriminative Coupled Dictionary
Learning
Given the category-labeled graph G(V, E, w, C), we attempt to jointly learn the discriminative coupled dictionaries D1 ,...,DM for the data from each modality. The coupling of the dictionaries indicates that these dictionaries
have the same number of atoms (i.e, K) and the dictionary
atoms from M modalities have a one-to-one correspondence

The notations used in this paper are listed in Table 1.

397

Multi-modal Data

Discriminative Coupled Dictionaries Learning

5

1

Unified Hash Function Learning

1

Sp t Biology
Sport
Biol
Histo
History
tory

...
G1
Coupled Dictionary
Atoms

Hash Function

K ...

G2
2
1

1

3

Intra-modality Edges

Inter-modality Edges

Sport Biology History

Sparse codes

Binary
Hash codes

Figure 1: The algorithmic flowchart of DCDH. Without loss of generality, we assume that there are two
modalities of data (represented as squares and circles). The data points with the same stripe have the
same category label (e.g., ‘sports’, ‘biology’, ‘history’). Given multi-modal data, the submodular dictionary
learning is utilized to obtain discriminative coupled dictionaries. The “discriminative” capability is reflected
by the fact that each dictionary atom is assigned a dominant category label to enhance its interpretability
(i.e., category ‘sport’ for G1 and ‘history’ for G2). The “coupling” means that each dictionary atom in one
modality has its counterpart dictionary atom in another modality. The coupled dictionary atoms are combined
to characterize the multi-modal data. Each data point from a given modality can be sparsely represented as
a sparse code using its corresponding dictionary. Finally, hash functions is learned to transform the sparse
codes to binary hash codes.
(paired dictionary atoms), since the paired dictionary atoms
have their diﬀerent intrinsic power to characterize the multimodal data. Moreover, the paired dictionary atoms are discriminative in terms of semantics (i.e., category ) and is
consistent with only one category label. That is to say, the
data from diﬀerent modalities are semantically aligned in a
shared coupled dictionary space.
Inspired by the eﬃciency and eﬀectiveness of submodular
dictionary learning approach [7], we formulate our discriminative coupled dictionary learning as a graph partition problem on G(V, E, w, C). Learning coupled dictionaries with
size K is equal to partitioning the category-labeled graph G
into K subgraphs which can be further regarded as a problem of selecting a subset A of the edge set E (i.e., A ⊆ E)
[7, 10]. We can formulate an objective function with respect to A and maximize it to obtain the optimal partitions.
Our objective function has the property of submodularity
and thus can be approximately optimized with an eﬃcient
greedy algorithm.
Our objective function consists of three parts which corresponds to the following requirements: 1) each subgraph
should be compact so that the obtained dictionaries have a
good representative capability; 2) each subgraph is encouraged to be discriminative so that the sparse representation
of the data over learned dictionary (i.e. using the centroid
to represent subgraphs), from the same category to be similar; 3) to avoid the over-ﬁtting on the subgraphs’ size (some
subgraphs may be extremely large while the others are so
tiny), the size of each subgraph is in balance (nearly equal).
Compact Function: The entropy rate of the random
walk over the graph G is exploited to obtain the compact
subgraphs. The entropy rate measures the uncertainty of a
stochastic process S = {St |t ∈ T } where T is an index set.
For a discrete random process, the entropy rate is deﬁned as
an asymptotic measure as: H(S) = limt→∞ H(St |St−1 , ..., S1 ),

which is the conditional entropy of the last random variable
given the past. In the case of a stationary 1st-order Markov
chain, the entropy rate is: H(S) = limt→∞ H(St |St−1 ) =
limt→∞ H(S2 |S1 ) = H(S2 |S1 ).
We deﬁne the random walk model on graph G as S =
{St |t ∈ T }. The transition probability from the vertex vi to
the vertex vj is deﬁned
P as pi,j = P r(St+1 = vj |St = vi ) =
wi,j /wi where wi =
k:ei,k ∈E wi,k is the sum of incident
weights of the vertex vi , and the stationary distribution is
deﬁned as:
w|V | T
w1 w2
µ = (µ1 , µ2 , ..., µ|V | )T = (
,
, ...,
)
(3)
wall wall
wall
P |
where wall = |V
i=1 wi is the sum of incident weights of all
vertices. The entropy rate of the random walk is deﬁned as:
H(S)

P
= H(S
) = i µi H(S2 |S1 = vi )
P2 |S1P
= − i µi j Pi,j logPi,j

(4)

Leaving µ in Eq.(3) intact, the set functions for the transition probability Pi,j : 2E → R w.r.t. A are deﬁned as:
w
i,j

if i 6= j and ei,j ∈ A

 wi ,
if i 6= j and ei,j ∈
/ A (5)
Pi,j (A) = 0, P


1 − j:ei,j ∈A wi,j , if i = j
wi

Consequently, the compact function with respect to A can
be deﬁned as the entropy rate of the random walk on G:
X X
H(A) = −
µi
Pi,j (A)logPi,j (A)
(6)
i

j

Given the entropies of the transition probabilities, maximizing the entropy rate in Eq.(6) encourages the edges with
large weights (small distance) to be selected [7]. Hence the
compact function H(A) can generate compact subgraphs.

398

Discriminative Function: To encourage the discrimination of subgraphs which further guarantees the sparse representation of the data from the same category to be similar,
a discriminative function on G is proposed [7].
Let A be the selected edge set, NA be the number of subgraphs with respect to A, the partition of graph G with
selected edge set A is GA = {G1 , ..., GNA } where each Gi is a
subgraph. We construct a count matrix N = [N1 , ..., NNA ] ∈
Rc×NA for the count of each category label of the data assigned to each subgraph and c is the number of the categories
of the multi-modal data set. Each Ni = [N1i , ..., Nci ]T ∈ Rc
where Nci is the number of data points from the c-th category assigned to i-th subgraph. It is worth noting that the size
of the count matrix N is dynamic since NA changes when
new edges are added to the selected edge set A.
The purity for each subgraph Gi is deﬁned as: P(Gi ) =
1
maxy Nyi where y ∈ {1, 2, ..., c} is the category label, Ci =
Ci
Pc
i
y=1 Ny is the count for data points of all categories assigned to subgraph Gi . The overall purity of GA is:
P(GA ) =

NA
X
i=1

Ci
Ctotal

P(Gi ) =

NA
X
i=1

1
maxy Nyi
Ctotal

and γ =

NA
X
i=1

1
maxy Nyi − NA
Ctotal

(7)

(8)

Input: data sets X 1 ,...,X M , λ′ , γ ′ , K
Output: The learned coupled dictionaries D1 ,...,DM
1: Construct uniﬁed graph with labeled multi-modal data
G(V, E, w, C) for the data sets X 1 ,...,X M
2: Initialization: A ← ∅, D1 , ..., DM ← ∅
3: while NA > K do
4:
e∗ = argmax F(A ∪ {e}) − F(A)
e∈E

5:
A ← A ∪ {e∗ }
6: end while
# generation of coupled dictionaries.
7: for each subgraph Gi in G do
8:
for m = 1 to M do
9:
Vim = {vj |vj ∈ Gi and
P vj from modality m}
10:
Dm ← Dm ∪ { |V1m | j:vj ∈V m vj }
i
i
11:
end for
12: end for

(9)

The balancing function is deﬁned using the entropy maximum theory:
X
B(A) = −
pA (i)log(pA (i)) − NA
(10)
i

3.3 Unified Hash Function Learning

The aforementioned three functions are proved to be monotonically increasing and submodular with respect to A [10][7].
Furthermore, It has been proved that the linear combination
of submodular function is still submodular [14]. Therefore,
we deﬁne an overall function F = H(A) + λD(A) + γB(A)
which is also monotonically increasing and submodular. The
optimal solution of F(A) is achieved by maximizing the objective function with best A as:
max
A

s.t.

H(A) + λD(A) + γB(A)
A ⊆ E and NA ≥ K

As the coupled dictionary for each modality is learned,
the data from each modality can be encoded as sparse codes
using its corresponding learned dictionary.
For the data points in data set X m , their K-dimensional
sparse codes Z m can be eﬃciently computed using the dictionary Dm as follows:
min
m
Z

s.t.
(11)

maxei,j H(ei,j )−H(∅)
maxei,j D(ei,j )−D(∅)

kX m − Dm Z m k2F + βkZ m k1
Zm ≥ 0

(12)

The non-negative constraint on Z m is needed for the following hash functions learning step. Eq.(12) is a simple
non-negative LASSO problem [18], and we use the eﬃcient
LARS [4] solver to solve this problem. Moreover, despite of

where λ and γ control the contribution of the three terms.
Follow the settings of [7], we set λ =

γ ′ , where λ′ and γ ′ are pre-

Algorithm 1 Discriminative Coupled Dictionary Learning

D(A) measures the discriminative capability of the subgraphs. Maximizing D(A) encourages each subgraph to have
a consistent category label, i.e., the data within each subgraph are expected to have the same category label.
Balancing Function: If we only use the compact and
discriminative functions, there may exist some extreme cases where the majority of data belong to one subgraph and
the other data are sporadically dispersed. This makes the
learned dictionary suﬀer from over-ﬁtting. Therefore, a balancing function is used to regularize the subgraphs of similar
sizes.
Denote pA as the distribution of the subgraph membership, pA is formulated as:
|Gi |
pA (i) = P
, i = {1, 2, ..., NA }
i |Gi |

maxei,j B(ei,j )−B(∅)

deﬁned parameters. NA ≥ K is a constraint on the number
of subgraphs which enforces exactly K subgraphs since the
objective function is monotonically increasing.
Directly maximizing Eq.(11) is a NP-hard problem. However, since F(A) is a submodular function, we can obtain an
approximate solution by a simple greedy algorithm, which
gives a 21 -approximation lower bound on the optimality of
the solution [14]. When the optimal K subgraphs are obtained, we simply use the center of the data within each
subgraph as the corresponding dictionary atom. Since each
subgraph consists of the data from M modalities respectively, M coupled dictionary atoms are obtained. The coupled
dictionaries of the common size K are generated based on all
the subgraphs. The overall algorithm of the discriminative
coupled dictionary learning is summarized in Algorithm 1.
m
∈
Note that the weights of the intra-modality edges wi,j
a,b
(0, 1] are not larger than the inter-modality edges wi,j = 1,
and the two vertices connected by the inter-modality edges
are within the same category. Therefore, adding an intermodality edge satisﬁes the discriminative function and the
compact function at the same time and each inter-modality
edge has a high probability to be selected out at the early
iterations in the step 4 of Algorithm 1. This observation is
valuable which ensures that within each subgraph, there is
at least one data point for every modality and the trivial
zero-value dictionary atoms is avoided.

P
where Ctotal = i Ci = |V | is the sum of the count of all
subgraphs. The discriminative function is deﬁned as:
D(A) = P(GA ) − NA =

maxei,j H(ei,j )−H(∅)

λ′

399

diﬀerent choices of β, the sparsity (maximum number of the
zero-elements) of Z can be well controlled by LARS. This is
helpful since we expect the sparsity of each sparse code to
be equivalent. The sparsity is set to 0.9 (i.e., 90% elements
of a sparse code are 0) throughout the paper.
By solving the Eq.(12) for the data set of each modality,
the sparse codes Z 1 , ..., Z M are correspondingly generated.
Denote Z = [Z 1 , ..., Z M ] ∈ RK×M N as the joint sparse codes
for all M data sets , we intend to further learn hash functions
which linearly projects each sparse code zi ∈ Z onto Ldimensional compact binary hash codes (L < K).
The commonly used hash function learning strategy is
based on graph-laplacian [9, 27], etc. The hash functions
are learned by solving an eigenvalue-decomposition problem
of a laplacian matrix which takes O(N 3 ) time. However, it
is infeasible to learn hash functions when N is large. Therefore, we adopt the hash function learning strategy based on
the sparse characteristic of sparse codes.
Since Z is non-negative, we can use Z (each column has
been ℓ1 normalized) to construct an approximate adjacency
matrix Ŵ = Z T Λ−1 Z ∈ RN×N where Λ = diag(Z1) ∈
RK×K [11]. The approximate adjacency matrix Ŵ is: 1)
nonnegative and sparse; 2) low-rank (the rank is at most
K); 3) double stochastic, i.e., has unit row and column sum.
Afterwards, the laplacian matrix is formulated as L̂ = I − Ŵ
where I is the identity matrix.
The optimal hash functions can be acquired as the L
eigenvectors with smallest eigenvalues of the approximated
laplacian matrix L̂ (removing the trivial eigenvector corresponds to eigenvalue 0), which is equal to L eigenvectors
with largest eigenvalues of Ŵ . Due to the low-rank property of Ŵ , a smaller matrix Q = Λ−1/2 ZZ T Λ−1/2 ∈ RK×K
is substituted for eigenvalue-decomposition problem on L̂.
By solving the eigen-system of Q, L largest eigenvectoreigenvalue pairs{(vk , σk )}L
k=1 where 1 > σ1 ≥ ...σL > 0
are obtained. Denote V = [v1 , v2 , ..., vL ] ∈ RK×L and Σ =
diag(σ1 , σ2 , ..., σL ) ∈ RL×L , the hash functions are deﬁned
as follows:
T

h(z) = sign(P z)

Without loss of generality, assuming we have two modalities a and b, for modality a, we have a single-view feature
X a ; for modality b, we extract multi-view (e.g., 2 views)
features X b1 and X b2 . The construction of category-labeled
uniﬁed graph G(V, E, w, C) is similar with the aforementioned methods. The size of vertex set does not change
since each vertex represents one data point. The edge set E
is expanded as some relations between the vertices in V are
added with the introduction of multi-view features.
The multi-view discriminative coupled dictionary learning
method is similar to the DCDH in Algorithm (1) except for
the generation of coupled dictionaries. For the single-view
modality a, its corresponding dictionary Da is learned; for
the multi-view modality b, two dictionaries Db1 and Db2 are
learned, respectively.
For modality a, we use the dictionary Da to generate sparse codes Z a for the data set X a by Eq.(12). For modality
b, we use the dictionaries Db1 and Db2 to jointly learn the
sparse codes Z b as follows:
P
min
kX bi − Dbi Z b k2F + βkZ b k1
Zb

s.t.

i=1,2

⇔ kX b − Db Z b k2F + βkZ b k1
Zb ≥ 0

(14)

where Z b is the sparse codes over the multi-view dictionar1
2
ies, X b = [X b1 ; X b2 ] ∈ R(p +p )×N , Db = [Db1 ; Db2 ] ∈
1
2
R(p +p )×K . Here, p1 , p2 denote the dimensionality of the
two views. The optimization of Eq.(14) is similar to Eq.(12).

4. COMPLEXITY ANALYSIS
Our DCDH approach consists of an oﬀ-line stage to learn
the discriminative coupled dictionaries and uniﬁed hash functions; an on-line stage to encode an out-of-sample data point
into a binary hash code. We detail the time complexity for
each part respectively.

4.1 Off-line training
Discriminative coupled dictionary learning: Leaving out the time for constructing the graph G(V, E, w, C)
P
2 m
which takes O( M
m=1 N p ) time, the complexity of discriminative coupled dictionary learning can be implemented
eﬃciently. Using a well designed heap structure, the ideal
time complexity is O(|V |log|V |) (i.e., O(M N logM N )) [7].
Unified hash functions learning:
To learn the uniﬁed hash functions V , we ﬁrst learn the sparse codes of
Z 1 , ..., Z M using Eq.(12). which can be solved in
P
m
O( M
m=1 N Kp ) time using the LARS algorithm [4]. However, the generation of each sparse code is independent which
can be solved in O(pm K) time, some parallel implementation can be adopted to solve the problem eﬃciently 1 . After
the sparse codes for all training data are obtained, an eigensystem of a small matrix Q ∈ RK×K is solved in O(K 3 )
time to obtain the projection matrix W and corresponding
hash functions. Therefore, the overall uniﬁed hash functions
learning step can be very eﬃcient.

(13)

√
where P = M N Λ−1/2 V Σ−1/2 ∈ RK×L is the normalized
projection matrix, z ∈ RK is a sparse code and sign(·) is the
binary function.
When given a new data point, its hash code can be consequently generated using a two-stage mechanism: for example, given a data point xm from modality m, it is ﬁrst
nonlinearly transformed to a sparse code z m using its corresponding dictionary Dm similar with Eq.(12). After that,
with the learned projection matrix P , z m is linearly transformed to a L-dimensional compact binary hash code using
the learned hash functions in Eq.(13).

3.4 Multi-View Enhancement
It is natural that the representation capability of the dictionaries for diﬀerent modalities varies widely. For example, the representing capability of the dictionary for a text
modality is much stronger than the one for an image modality. This “unbalanced” representation may lead to an unsatisfying cross-media retrieval performance. Therefore, we
incorporate the multi-view representation into our coupled
dictionary learning to enhance the representing capability of
the relatively “weak” modalities.

4.2 On-line hash encoding
The on-line hash encoding step should be fast enough to
support the cross-media retrieval over the large scale data
set. The time for encoding a new data point is two-fold:
1

400

http://spams-devel.gforge.inria.fr/

6

7

CVH
CMSSH
MLBE
LCMH
DCDH
MV−DCDH

6
5
4
3
2
1
0
−1
8

16

24

32

Elapsed seconds (log)

Elapsed seconds (log)

8

40

Hash code length

(a) Oﬀ-line training time

Table 2: The details of the data sets used in the
experiments
Data Set
NUS-WIDE
Wiki-Potd

CVH
CMSSH
MLBE
LCMH
DCDH
MV−DCDH

4
2
0

Image modality

−2
−4
−6
8

16

24

32

40

Hash code length

Text modality
Data set size
Training set size
Validation set size*
Testing set size*

(b) On-line testing time

Figure 2: The time cost of the training and testing stages for DCDH, MV-DCDH and other crossmedia hashing approaches. The experiments are
conducted on Wiki-Potd data set with dictionary
size K = 100 and MV-DCDH uses two views of features.

(i.e., 500-D BoVW, 255-D Color Moments, 128-D Wavelet
Texture). For the text modality, the corresponding labels of
each image are represented by a 1,000-D BoW.
The details of the two data sets are shown in Table 2.
To evaluate the performance of the cross-media retrieval
results, we adopt the mean average precision (MAP) and
mean average top-R precision (MAP@R) deﬁned in [13].

Sparse Coding: Given a new data point x from modality m, its sparse code z m is obtained similar as Eq.(12).
Therefore, the time complexity is O(pm K).
Binary Embedding: The linear transformation from a
sparse code z m to a binary hash code is achieved by the hash
functions in Eq.(13) which takes O(KL) time.
An intuitive comparison of DCDH and other state-of-theart cross-media hashing methods on oﬀ-line training and online testing time are demonstrated in Figure 2. We can see
that our DCDH and MV-DCDH require the least time in the
training stage and is also very eﬃcient in the testing stage.

5.2 Compared Methods
We perform three types of retrieval schemes in the experiments : 1) Image-query-Texts: use image queries to retrieve
relevant texts. 2) Text-query-Images: use text queries to
retrieve relevant images. 3) Image-query-Images: use image
queries to retrieve relevant images. For the ﬁrst two retrieval schemes, we compare with the state-of-the-art crossmedia hashing methods CMSSH [3], CVH [9], MLBE [28],
LCMH [30]; for the third retrieval scheme, we additionally compare with some uni-modal hashing approaches: SH
[23], KLSH [8], AGH [11] and a multi-view hashing approach
MFH [17]. The reason why we don’t compare DCDH with
the approaches in [31] and [25] is that they can not generate
compact and binary hash codes, thus their performance can
not be fairly evaluated under the same settings.
Our DCDH method and its multi-view enhancement are
denoted as DCDH and MV-DCDH, respectively.
For the Image-query-Texts and Text-query-Images retrieval
schemes, the performances of CMSSH, CVH, MLBE, LCMH, DCDH, MV-DCDH are compared. Except for our
MV-DCDH which induces multi-view features, the remaining methods take the BoVW descriptors for the image modality and BoW for text modality; for MV-DCDH, the multiview features are utilized for image modality.
For the Image-query-Images retrieval scheme, we compare
with all the counterparts aforementioned. It is notable that
for MFH, the multi-view features are exploited.

EXPERIMENTS

In our experiments, we evaluate the performance of our DCDH. We ﬁrst introduce the data set, evaluation criteria and
the parameter setting we used in the experiments. Then,
we compare our DCDH with other state-of-the-art methods
and analyze the results. Finally, we further investigate the
learned coupled dictionary space to explain why our DCDH
and MV-DCDH achieve the superior performance.

5.1 Experimental Setup
We use two real-world data sets “Wikipedia-Picture of the
Day”(abbreviated as Wiki-Potd) 2 and NUS-WIDE3. Both
data sets are bi-modal containing images and texts.
The Wiki-Potd data set consists of 2866 Wikipedia documents. Each document contains one text-image pair. All
documents are labeled by one of 10 semantic categories. For
the image modality, we extract 1000-D Bag of visual words
(BoVW) and 512-D GIST descriptors for each image. For
the text modality, we calculate the frequency of all words
in the data set and select the most representative words to
quantize all texts into 5,000-D Bag-of-Words (BoW).
The NUS-WIDE data set contains 269,648 labeled images
and is manually annotated with 81 categories. Each image
with its annotated tags in NUS-WIDE can be taken as a
pair of image-text data. To guarantee that each category has
abundant training samples, we select those pairs that belong
to one of the 10 largest categories (e.g., ‘sky’, ‘buildings’,
‘person’) with each pair exclusively belonging to one of the
10 categories (discrimination on concepts are required when
learning coupled dictionaries.). For the image modality, over
three types of visual features are extracted for each image
2
3

BoVW(1000-D)
GIST(512-D)
BoW(5000-D)
2866
1000
866/866
866/1000

* Partitions are ordered by query/database set respectively. The
query set are random sampled from the database set.

m

5.

BoVW(500-D)
CM(255-D)
Wavelet(128-D)
BoW(1000-D)
60641
3000
2000 /10000
2000 / 47641

5.3 Parameters Sensitivity
There are four parameters in DCDH: the k-NN of the
intra-modality; λ′ , γ ′ in Eq.(11) when learning the coupled
dictionaries; the size of the coupled dictionaries K.
Following the prior settings in [7, 10], λ′ and γ ′ are set to
10 and 1 respectively throughout the experiments.
We ﬁx the code length L = 24 and evaluate the average MAP variations (the average MAP scores of Imagequery-Texts and Text-query-Images) in terms of K and kNN on the validation set. The tested combinations are K =
{50, 100, 200, 300, 400, 500} and k-NN = {5, 10, 20, 30, 50},
The optimal combination on NUS-WIDE is k-NN = 5, K =

http://www.svcl.ucsd.edu/projects/crossmodal/
http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm

401

0.45

0.6

Table 3: The MAP performance comparison on the
NUS-WIDE data set with code length L varying
from 8 to 40. The items in bold are the two best
results, and the results with asterisk are the best.
Hash code length
Task
Methods
L=8
L = 24
L = 40
CVH
0.3144
0.3139
0.3140
Image
CMSSH
0.3233
0.3131
0.3140
MLBE
0.3142
0.3138
0.3119
query
Texts
LCMH
0.3163
0.3117
0.3144
DCDH
0.3608
0.3573
0.3568
MV-DCDH 0.3645* 0.3627* 0.3608*
Hash code length
Task
Methods
L=8
L = 24
L = 40
CVH
0.3158
0.3152
0.3144
Text
CMSSH
0.3373
0.3309
0.3287
query
MLBE
0.3133
0.3156
0.3167
Images
LCMH
0.3142
0.3124
0.3133
DCDH
0.3452
0.3559
0.3554
MV-DCDH 0.3640* 0.3629* 0.3604*

0.4

0.5
SH
KLSH
AGH
MFH
CVH
CMSSH
MLBE
LCMH
DCDH
MV−DCDH

0.45

0.4

0.35

0.3
8

16

24

32

Hash code length

(a) NUS-WIDE

40

MAP@50

MAP@50

0.55

0.35
SH
KLSH
AGH
MFH
CVH
CMSSH
MLBE
LCMH
DCDH
MV−DCDH

0.3

0.25

0.2
8

16

24

32

40

Hash code length

(b) Wiki-Potd

Figure 3: The performance comparison of Imagequery-Images on two data sets.
It can be noted that DCDH signiﬁcantly outperforms the
counterparts over diﬀerent code lengths: 2% ∼ 7% on WikiPotd data set and 2% ∼ 5% on NUS-WIDE, respectively.
The improvement is due to the eﬀectiveness of the sparse
representation over the discriminative coupled dictionaries.
All the counterparts simply project the data from diﬀerent
modalities into a shared Hamming space using the learned
hash functions. However, the meaning of the shared Hamming space is ambiguous and cannot well clarify the semantic information of the data. By contrast, our DCDH exploits
the category information when learning the coupled dictionaries, thus making the coupled dictionary space semantic
interpretable. By utilizing the distribution of the sparse
codes, the manifold structure of the dictionary space is also
preserved in the embedding Hamming space. Therefore, the
binary hash codes represent the semantic information of the
data and lead to superior cross-media retrieval performance.
Moreover, we ﬁnd that the incorporation of the multi-view
features on the image modality, i.e., MV-DCDH, produces a
signiﬁcant improvement over DCDH on both of the Imagequery-Texts and the Text-query-Images tasks. This observation is explained as that exploiting multiple features over
the “weak” modality (image modality) gives better understanding of the semantics of the images. This observation
also veriﬁes our hypothesis that a balanced representation
is important in cross-media retrieval.
Although our main goal is cross-media retrieval, we can
readily use the hash functions we learned to perform unimodal retrieval. That is to say, all the cross-media hashing
approaches can be adapted to uni-modal hashing. We conduct the task of Image-query-Images and report the performance of the aforementioned cross-media hashing approaches. Besides, we add some state-of-the-art uni-modal hashing
and multi-view hashing approaches to perform fair experimental comparison. The results are shown in Figure 3.
From Figure 3, we ﬁnd that all the cross-media hashing
approaches except CMSSH achieve reasonable performance.
This is due to the fact that the learned hash functions for image modality can borrow strength from text modality. The
observation for the poor performance of CMSSH in this task
may be explained as the lack of intra-modality preservation
when learning hash functions in CMSSH.
In addition, since MV-DCDH induces multi-features, we
also add MFH [17] into comparison which also exploits multifeatures when learning hash functions. The results show
that MFH outperforms the other uni-modal hashing approaches and most of the cross-media hashing approaches,
which demonstrates the eﬀectiveness of multi-views in image understanding. Our MV-DCDH slightly outperforms

Table 4: The MAP performance comparison on the
Wiki-Potd data set.
Hash code length
Task
Methods
L=8
L = 24
L = 40
CVH
0.1490
0.1407
0.1381
Image
CMSSH
0.1448
0.1407
0.1431
MLBE
0.1445
0.1359
0.1371
query
Texts
LCMH
0.1267
0.1273
0.1258
DCDH
0.1821
0.1985
0.1934
MV-DCDH 0.2017* 0.2010* 0.1997*
Hash code length
Task
Methods
L=8
L = 24
L = 40
CVH
0.1435
0.1361
0.1351
Text
CMSSH
0.1412
0.1364
0.1380
query
MLBE
0.1449
0.1344
0.1363
Images
LCMH
0.1260
0.1237
0.1235
DCDH
0.1606
0.1648
0.1620
MV-DCDH 0.1745* 0.1734* 0.1716*
100 and k-NN = 20, K = 300 on Wiki-Potd. These settings
are adopted in the following experiments.

5.4 Performance Comparisons
We compare our DCDH and its extension MV-DCDH
with the three following methods: CMSSH [3], CVH [9] and
MLBE [28]. We evaluate the cross-media retrieval performance with code length varying from 8 to 40 and report
results in terms of MAP in Table 3 and 4, respectively.
Moreover, we report the Image-query-Images retrieval performance in terms of MAP@50 in Figure 3. The reason why
we choose MAP@50 rather than MAP for the Image-queryImages task is that the diﬀerences of MAP scores, especially for the counterparts, are not statically signiﬁcant, which
makes it diﬃcult to illustrate them explicitly in a line graph.
The MAP scores on Wiki-Potd data set is generally lower than that on NUS-WIDE even if the Wiki-Potd data set
contains rich textual information. This may be explained as
that the categories of Wiki-Potd data set (e.g., ‘art’, ‘biology’) is too general, so the feature vector can not precisely
capture its corresponding semantic meaning.

402

1

Table 5: Topic words and corresponding category labels of some selected dictionary atoms on the WikiPotd data set.
Biology
Sport
Warfare
Media
Geography

Discriminative Degree

Categories

Random
Non−discriminative
Discriminative
MV−Discriminative

0.9

Topic Words
Skull Dinosaurs Bone Fossils Prey
Whales Killer Meat Oil Atlantic
Goals Hockey Players Montreal NHL
Football Yard Bowl Champion Quarter
Natives Crops Australian Infantry Landing
Soviet, Moscow Marshall Defense Battle
Theater Broadway Movie Actor Disc
Creek Tree Parks Ridge Forest

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

Image

Text

Image & Text

Figure 4: The discriminative capability comparison
the MFH and achieves the overall best performance in the
Image-query-Images task.

coupling degree is signiﬁcantly improved when imposing the
discriminative constraint. This is mainly due to the better
representation of the image modality; 4) Exploiting multiview features further improves the performance on both the
understanding of the image modality and the coupling degree.
To give an intuitive illustration of the learned dictionary,
we give an insight into the textual dictionary Dy (the image
dictionary Dx is learned from the BoVW, which is diﬃcult
to illustrate). Since each dictionary atom dyk is obtained by
clustering a portion of BoW features, the values of the elements in dyk measure the occurrence frequency of the words.
Naturally, we can use the elements with largest values in
dyk to represent the topic words of this dictionary atom dyk .
Moreover, each dyk is learned with a discriminative constraint and has a dominated category label, we demonstrate the
relation between the category label and topic words of the
dictionary atoms in Table 5 (topic words correspond to the 5
largest elements of the selected dictionary atoms). From the
results, we can ﬁnd that the topic words for each dictionary
atom indeed reﬂect some certain semantic information and
are consistent with its belonging category. Besides, two dictionary atoms belong to the same category have an explicit
disparity in semantics. e.g., the two atoms from the category “Biology” describe diﬀerent topics of “Biology”: the
ﬁrst topic is about dinosaurs and the second one is about
whale killing. This observation can be explained as the collaborative eﬀect of the compact function and discriminative
function. The discriminative function encourages the topics
to be classiﬁed into the correct categories and the compact
function encourages each topic to reﬂect an individual aspect
of its corresponding category.

5.5 The Discriminative Capability of the
Coupled Dictionary Space
The results over both the data sets above outline the superior performance of DCDH over the other cross-media hashing approaches. This superiority mainly owes to the aptitude
of the discriminative capability of the coupled dictionary space in DCDH. To verify our hypothesis, we investigate the
coupled dictionary space. We choose the Wiki-Potd data set
since it has rich textual information and is convenient for illustrations. To show the eﬀect of discriminative capability of
the coupled dictionary space, we design a non-discriminative
version of DCDH by simply setting λ′ = 0. The rest settings
are same as the ones used aforementioned (K = 300 and kNN = 20).
Denote the learned coupled dictionaries for image and text
modalities as Dx amd Dy , respectively. Z x and Z y are the
sparse codes of the test data set in two modalities by the
corresponding dictionaries. To measure the discrimination
of the sparse codes, we deﬁne a metric called Discriminative
Degree (abbreviated as DD) as follows:
DD(Z) =

N
1 X
P (zi )
N i=1

(15)

where N is the number of the testing samples. P (zi ) = 1
if at least one of the selected dictionary atoms of the sparse
code zi indicates the true category label of the i-th data
point and 0 otherwise.
Moreover, DD can also be used to measure the coupling
degree of the sparse codes from diﬀerent modalities. Denote the dot product of Z x and Z y as Z xy and DD(Z xy )
reﬂects the one-to-one correspondence of the paired dictionary atoms .
Figure 4 shows the comparison results. The “Random”
method indicates that the dictionaries are randomly generated; The “Non-discriminative” method is the aforementioned DCDH with λ′ = 0. The “Discriminative” and “MVDiscriminative” methods correspond to our DCDH and MVDCDH. From the results, we get four observations: 1) the
sparse codes of the text modality is more semantically discriminative than the ones from the image modality (even
when we do not impose the discriminative constraints); 2)
The introduction of side information improves the discriminative capability especially for the image modality (without
the category side information, the DD score of the image
modality is almost equal to the random method); 3) The

6. CONCLUSIONS
In this paper, we propose a discriminative coupled dictionary hashing (DCDH) approach for fast cross-media retrieval. Our DCDH is two-stage in that we ﬁrst learn coupled dictionary for each modality discriminatively with the
side information of category labels, so that the data from
diﬀerent modalities are represented as the sparse codes in a
shared semantically discriminative dictionary space. Afterwards, the sparse codes are mapped to binary hash codes
by the learned uniﬁed hash functions to support fast crossmedia retrieval. Extensive experiments on two real-world
data sets demonstrate the superior performance of DCDH
over the existing state-of-the-art hashing approaches.
Moreover, we conjecture that a balanced cross-media representation beneﬁts the cross-media retrieval performance.

403

Therefore, we extend DCDH to MV-DCDH which introduces multi-view features on the relatively “weak” modalities (i.e., the image modality in our experiments) to obtain
a balanced representation. The experimental results verify
the eﬀectiveness of MV-DCDH.

7.

[14] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An
analysis of approximations for maximizing submodular
set functionsa֒li. Mathematical Programming,
14(1):265–294, 1978.
[15] M. Ou, P. Cui, F. Wang, J. Wang, W. Zhu, and
S. Yang. Comparing apples to oranges: a scalable
solution with heterogeneous hashing. In SIGKDD,
pages 230–238, 2013.
[16] N. Rasiwasia, J. Costa Pereira, E. Coviello, G. Doyle,
G. Lanckriet, R. Levy, and N. Vasconcelos. A new
approach to cross-modal multimedia retrieval. In ACM
MM, pages 251–260, 2010.
[17] J. Song, Y. Yang, Z. Huang, H. Shen, and R. Hong.
Multiple feature hashing for real-time large scale
near-duplicate video retrieval. In ACM MM, pages
423–432, 2011.
[18] R. Tibshirani. Regression shrinkage and selection via
the lasso. Journal of the Royal Statistical Society.
Series B (Methodological), pages 267–288, 1996.
[19] C. Wang and S. Mahadevan. A general framework for
manifold alignment. In AAAI, 2009.
[20] J. Wang, S. Kumar, and S. Chang. Semi-supervised
hashing for scalable image retrieval. In CVPR, pages
3424–3431, 2010.
[21] Q. Wang, D. Zhang, and L. Si. Semantic hashing using
tags and topic modeling. In SIGIR, pages 213–222,
2013.
[22] S. Wang, L. Zhang, Y. Liang, and Q. Pan.
Semi-coupled dictionary learning with applications to
image super-resolution and photo-sketch synthesis. In
CVPR, pages 2216–2223, 2012.
[23] Y. Weiss, A. Torralba, and R. Fergus. Spectral
hashing. In NIPS, 2008.
[24] J. Wright, A. Yang, A. Ganesh, S. Sastry, and Y. Ma.
Robust face recognition via sparse representation.
IEEE Trans.Pattern Anal. Mach. Intell.,
31(2):210–227, 2009.
[25] F. Wu, Z. Yu, Y. Yang, S. Tang, Y. Zhang, and
Y. Zhuang. Sparse multi modal hashing. IEEE Trans.
Multimedia, 16(2):427–439.
[26] D. Zhang, F. Wang, and L. Si. Composite hashing
with multiple information sources. In SIGIR, pages
225–234, 2011.
[27] D. Zhang, J. Wang, D. Cai, and J. Lu. Self-taught
hashing for fast similarity search. In SIGIR, pages
18–25, 2010.
[28] Y. Zhen and D. Yeung. A probabilistic model for
multimodal hash function learning. In SIGKDD, 2012.
[29] Y. Zhen and D.-Y. Yeung. Co-regularized hashing for
multimodal data. In NIPS, pages 1385–1393, 2012.
[30] X. Zhu, Z. Huang, H. T. Shen, and X. Zhao. Linear
cross-modal hashing for eﬃcient multimedia search. In
ACM MM, pages 143–152, 2013.
[31] Y. Zhuang, Y. Wang, F. Wu, Y. Zhang, and W. Lu.
Supervised coupled dictionary learning with group
structures for multi-modal retrieval. In AAAI, 2013.
[32] Y. Zhuang, Y. Yang, and F. Wu. Mining semantic
correlation of heterogeneous multimedia data for
cross-media retrieval. IEEE Trans. Multimedia,
10(2):221–229, 2008.

ACKNOWLEDGEMENT

This work is supported in part by National Basic Research
Program of China (2012CB316400), NSFC (No.61128007),
863 program (2012AA012505), the Fundamental Research
Funds for the Central Universities and Chinese Knowledge
Center of Engineering Science and Technology (CKCEST)
and Program for New Century Excellent Talents in University. Dr.Qi Tian is also supported by ARO grant W911NF12-1-0057, Faculty Research Award by NEC Laboratories
of America, and 2012 UTSA START-R Research Award respectively. Dr. Jiebo is also supported by Google Faculty
Research Awards.

8.

REFERENCES

[1] M. Aharon, M. Elad, and A. Bruckstein. K-svd: An
algorithm for designing overcomplete dictionries for
sparse representation. IEEE Trans.Signal Processing,
54(11):4311–4322, 2006.
[2] A. Andoni and P. Indyk. Near-optimal hashing
algorithms for approximate nearest neighbor in high
dimensions. In FOCS, pages 459–468, 2006.
[3] M. Bronstein, A. Bronstein, F. Michel, and
N. Paragios. Data fusion through cross-modality
metric learning using similarity-sensitive hashing. In
CVPR, pages 3594–3601, 2010.
[4] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani.
Least angle regression. The Annals of Sstatistics,
32(2):407–499, 2004.
[5] Y. Gong and S. Lazebnik. Iterative quantization: A
procrustean approach to learning binary codes. In
CVPR, pages 817–824, 2011.
[6] K. Jia, X. Tang, and X. Wang. Image transformation
based on learning dictionaries across image spaces.
IEEE Trans.Pattern Anal. Mach. Intell., 2012.
[7] Z. Jiang, G. Zhang, and L. S. Davis. Submodular
dictionary learning for sparse coding. In CVPR, pages
3418–3425, 2012.
[8] B. Kulis and K. Grauman. Kernelized
locality-sensitive hashing for scalable image search. In
ICCV, pages 2130–2137, 2009.
[9] S. Kumar and R. Udupa. Learning hash functions for
cross-view similarity search. In IJCAI, pages
1360–1365, 2011.
[10] M.-Y. Liu, O. Tuzel, S. Ramalingam, and
R. Chellappa. Entropy rate superpixel segmentation.
In CVPR, pages 2097–2104, 2011.
[11] W. Liu, J. Wang, S. Kumar, and S. Chang. Hashing
with graphs. In ICML, pages 1–8, 2011.
[12] Y. Liu, F. Wu, Y. Yi, Y. Zhuang, and A. Hauptman.
Spline regression hashing for fast image search. IEEE
Trans. Image Processing, 2012.
[13] X. Lu, F. Wu, S. Tang, Z. Zhang, X. He, and
Y. Zhuang. A low rank structural large margin
method for cross-modal ranking. In SIGIR, pages
433–442. ACM, 2013.

404

