Co-Training on Authorship Attribution with Very Few
Labeled Examples: Methods vs. Views
1

Tieyun Qian1, Bing Liu2, Ming Zhong1, Guoliang He1

State Key Laboratory of Software Engineering, Wuhan University, China
Department of Computer Science, University of Illinois at Chicago, USA
qty@whu.edu.cn, liub@cs.uic.edu, clock@whu.edu.cn, glhe@whu.edu.cn
2

This is possible if the author has written a large number of articles,
but will be difficult if he/she has not. For example, in the online
review domain, most authors (reviewers) only write a few reviews
(documents). It was showed that on average each reviewer only
has 2.72 reviews in amazon.com, and only 8% of the reviewers
have at least 5 reviews [7]. To make things worse, the labeling of
training documents is even more difficult. Due to the anonymous
posting behavior or multiple user-ids of an online user, it is hard
or expensive to collect labeled documents for an author. The small
number of labeled documents makes it extremely challenging for
supervised learning to train an accurate classifier.
In this paper, we consider the problem of authorship attribution
with very few labeled examples. By exploiting the redundancy in
the human languages, we tackle the problem of very few training
examples in a two view co-training framework. Specifically, we
first represent the documents as several natural views, and then
two classifiers are co-trained on every two of the views. The
predictions of each classifier on unlabeled examples are used to
augment the small training set. This process repeats until a
termination condition is satisfied, and the enlarged labeled set is
finally used to train a classifier to make predictions on the test
data. The focus in this paper is on evaluation and comparison of
the co-training framework on different views and learning
approaches. Starting from 10 training texts per author, we use two
classifiers, logistic regression (LR) and support vector machines
(SVM), to assess the effectiveness of three views, character,
lexical and syntactic views.
So far limited work has been done in this area. The AA
problem with limited training data was attempted in [15] and [12].
However, neither of them used a co-training framework. In a
pioneer work [11], Kourtis and Stamatatos introduced the cotraining technique into authorship identification. In their
experimental setting, about 115 and 129 documents per author on
average are used for two experimental corpora. This number of
labeled documents is still very large. We consider a much more
realistic problem, where the size of training samples is extremely
small. Only 10 samples per author are used in training.
Furthermore, instead of using two different views, co-training in
[11] is conducted on the character 3-gram view using two
different classifiers, common n-grams (CNG) and SVM. This
contradicts with the standard co-training algorithm [2], which
requires two sufficient and redundant views. This inspired our
study, which aims to seek the answers to the following questions:

ABSTRACT

Authorship attribution (AA) aims to identify the authors of a set
of documents. Traditional studies in this area often assume that
there are a large set of labeled documents available for training.
However, in the real life, it is hard or expensive to collect a large
set of labeled data. For example, in the online review domain,
most reviewers (authors) only write a few reviews, which are not
enough to serve as the training data for accurate classification.
In this paper, we present a novel two-view co-training
framework to iteratively identify the authors of a few unlabeled
data to augment the training set. The key idea is to first represent
each document as several distinct views, and then a co-training
technique is adopted to exploit the large amount of unlabeled
documents. Starting from 10 training texts per author, we
systematically evaluate the effectiveness of co-training for
authorship attribution with limited labeled data. Two methods and
three views are investigated: logistic regression (LR) and support
vector machines (SVM) methods, and character, lexical, and
syntactic views. The experimental results show that LR is
particularly effective for improving co-training in AA, and the
lexical view performs the best among three views when combined
with a LR classifier. Furthermore, the co-training framework does
not make much difference between one classifier from two views
and two classifiers from one view. Instead, it is the learning
approach and the view that plays a critical role.

Categories and Subject Descriptors
H.3.1 [Information Storage and Retrieval]: Content Analysis and
Indexing.

Keywords
Authorship attribution; co-training; very few labeled examples

1. INTRODUCTION
The problem of authorship attribution (AA) can be defined as
follows: Let A = {a1, …, ak} be a set of k authors and D = {D1, …,
Dk} be k sets of documents with Di being the document set of
author ai  A. Each (training or testing) document is represented
as a feature vector. A model or classifier is then built from the
training data and applied to the test data to determine the author a
of each test document d, where a  A.
Existing approaches to authorship attribution are mainly based
on supervised classification [16, 9, 13]. Although this is an
effective approach, it has a major weakness, i.e., for each author a
large number of his/her articles are needed as the training data.




Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for
components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to
post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.



Can the co-training technique benefit more from two views
with one classifier than one view with two classifiers?
What is the more important factor to the co-training
framework, the view or the classifier?
What are the most promising learning technique and view for
co-training in AA?

The rest of this paper is organized as follows. Section 2 reviews
the related work. Section 3 presents our co-training method for
authorship attribution. Section 4 provides experimental results.
Finally, Section 5 concludes the paper.

Copyright © 2014 ACM 978-1-4503-2257-7/14/07…$15.00.
903

2. RELATED WORK

Input: A small set of labeled documents L = {l1,…, lr}, a large set
of unlabeled documents U = {u1,…, us}, and a set of test
documents T = {t1,…, tt},
Parameters: the number of iterations k, the size of selected
unlabeled documents u
Output: tk’s class assignment

Existing methods for AA can be categorized into two main
themes: finding appropriate features and developing efficient and
effective techniques. Various features have been proposed for
modeling writing styles, including function words, length and
richness features, punctuation frequencies, character, word and
POS n-grams, and rewrite rules. On developing effective learning
techniques, an early study used Bayesian statistical analysis, but
later work focused exclusively on classification, including
decision trees, logistic regression, and SVM, etc.
The main problem in the traditional research is the unrealistic
size of the training set. Basically, a size of about 10,000 words per
author is regarded to be a reasonable size [1, 3]. Even when no
long documents available, hundreds of short texts can be selected
[6] such that the total amount of words is large enough for
training. Two studies [15, 12] introduced the problem of AA with
limited data. However, neither of them used the co-training
technique, which was a representative learning mechanism to
cope with limited training data [2]. More recently, Kourtis and
Stamatatos employed the co-training paradigm for AA [11]. The
large training set size there was far from the realities of modern
authorship attribution. In addition, their co-training method was
implemented using two classifiers built on one character n-gram
view. Our preliminary work also showed that co-training helped
improve the performance of AA with few labeled data [4] under
two views, i.e., lexical and syntactic views. Experiments were
conducted on only 10 authors using SVM. p documents with the
highest scores are used to augment the labeled set. Results showed
that co-training outperformed supervised classification on either
of the two views and the combined feature vectors, as well as the
ensemble method based on subspacing. However, when we
experimented with more authors, the accuracy dropped quickly.
Furthermore, co-training on different learning approaches and
views remained unknown. Later, we found that LR performed
significantly better than SVM, and the data augmentation strategy
was also crucial, which inspired this work. This work added a new
view and also performed extensive experiments to study views vs.
classification methods of SVM and LR.

1. Extract views Lc, Ll, Ls, Uc, Ul, Us, Tc, Tl, Ts from L, U, T
2. Loop for k iterations:
3.
Randomly select u unlabeled documents U' from U;
4.
Learn the first view classifier C1 from L1 (L1=Lc, Ll, or Ls);
5
Use C1 to label documents in U' based on Uc, Ul, Us;
6
Learn the second view classifier C2 from L2 (L2L1)
7
Use C2 to label documents in U' based on Uc, Ul, Us;
8
Up = {u | u U', u.label by C1 = u.label by C2};
9
U = U - U', L = L  Up;
10. Learn the first view classifier C1 from L;
11. Use C1 to label tk in T based on Tc, Tl, Ts;
12. Learn the second view classifier C2 from L;
13. Use C2 to label tk in T based on Tc, Tl, Ts;
Figure 1: A two-view co-training algorithm for AA (TCA)
steps. First, use two of the character, lexical and syntactic views
on the current labeled set to train two classifiers C1 and C2.
Second, allow each of these two classifiers to examine the
unlabeled set U’ and choose p samples that they agree to label as
positive. The selected examples are then added to the current
labeled set L with the label assigned, and the u documents are
removed from the unlabeled pool U’. Steps 10-13 are used to
assign the test document to a category (author) using the classifier
learned from the first and second view in the augmented labeled
data, respectively.

3.2 Character View

The character view is represented as the character n-gram features
of a document. Character n-grams are simple and easily available
for any natural language [11]. For a fair comparison with the
previous work [11], we extract frequencies of 3-grams on the
character-level. The vocabulary size for character 3-grams in our
experiment is 28584.

3. A TWO-VIEW CO-TRAINING METHOD
FOR AUTHORSHIP ATTRIBUTION

3.3 Lexical View

The lexical view consists of word unigrams of a document. We
represent each article by a vector of word frequencies. The
vocabulary size for word unigrams in our experiment is 195274.
We do neither word stemming nor stop word removal as in text
categorization. This is because some of the stop words are
actually function words which have been demonstrated
discriminative for authorship identification. In addition, stemming
can be also harmful to information extraction of an author.

3.1 Overall Framework
In the context of authorship attribution, each document can be
represented as several views of features: views about character,
lexical and syntactic features. A classifier can be learned from any
of these views. To deal with the problem of very few training
examples, we present a two-view co-training algorithm, which
uses the logistic regression (LR) or SVM as the learner and cotrained on every two of the views. The overall framework is
shown in Algorithm 1.
Given the labeled, unlabeled, and test sets L, U, and T, step 1
extracts the character, lexical, and syntactic views from L, U, and
T, respectively. Steps 2-9 iteratively co-train two classifiers by
adding the data which are assigned the same label by the two
classifiers into the training set. The algorithm first randomly
selects u unlabeled documents from U to create a pool U’ of
examples. Note that we can directly select from the large
unlabeled set U. However, it is shown in [2] that a smaller pool
can force the classifier C1 and C2 to select instances that are more
representative of the underlying distribution that generates U.
Hence we set the parameter u to a size of about 1 percent of the
whole unlabeled set, which allows us to observe the effects of
different number of iterations. It then iterates for the following

3.4 Syntactic View

The syntactic view is composed of the syntactic features of a
document. We use four typical content-independent structures
including n-grams of POS tags (n = 1..3) and rewrite rules [9].
The syntactic features are extracted from the parsed syntactic trees.
Each POS n-gram or rewrite rule is encoded like a single pseudoword and assigned a unique number id. The vocabulary sizes for
POS 1-grams, POS 2-grams, POS 3-grams, and rewrite rules in
our experiment are 63, 1917, 21950, and 19240, respectively.
These four types of syntactic structures are merged into a single
vector. Hence the syntactic view of a document is represented as a
vector with 43140 components.

904

From Figure 2, we also see that the lexical view performs much
better than the character view in all cases. See the red curve with
square mark and the blue curve with triangle mark. However, their
trends are similar. The reason can be that the lexical view shares
some common properties with the character view.

4. EXPERIMENTAL EVALUATION
multiclass

All our experiments use the SVM
classifier [8] with its
default parameter settings and the logistic regression classifier
with L2 regularization [5].

4.1 Experiment Setup
4.1.1 Data set

We conduct experiments on the IMDB data set [14]. This data set
contains the IMDb reviews in May 2009. It has 62,000 reviews by
62 users (1,000 reviews per user). It is publicly available upon
request from the authors of [14]. For each author/reviewer, we
further split his/her documents into the labeled, unlabeled, and test
sets. 1% of one author’s documents, i.e., 10 documents per author,
are used as the labeled data for training, 79% are used as
unlabeled data, and the rest 20% are used for testing. We extract
and compute the character and lexical features directly from the
raw data, and use the Stanford PCFG parser [10] to generate the
grammar structures of sentences in each document for extracting
syntactic features. We normalize each feature’s value to [0, 1]
interval by dividing the maximum value of this feature in the
training set. We use the micro-averaged classification accuracy as
the evaluation metric.

Figure 2: TCA using SVM (left) and LR (right) on lexical and
character views

The effects of SVM and LR on the lexical and syntactic views
are shown in Figure 3. Similar to Figure 2, the LR classifier shows
a dramatic performance improvement over SVM. And the lexical
view is a much better representation of authorship than the
syntactic view.

4.1.2 Baseline methods

We give two baselines which co-train on one view but using two
different classifiers. The first one is presented in [11], and the
second one is our extension.
CNG+SVM on Char3Gram: It co-trains two classifiers from the
character 3-gram view using CNG and SVM classifiers [11].
CNG is a profile-based method. It first merges all the available
training texts from each author into one file. And then a single
representation is extracted for the author. The representation is
based on the L most frequent character n-grams of the file. The
test document is represented similarly. The classification model
is based on the dissimilarity of the profile of the text from each
of the profiles of the candidate authors. The distance function is
defined as the one used in [11].
LR+SVM on Char3Gram: Since our preliminary results show that
the performance of CNG+SVM is extremely poor, we are
curious what the reason is. Can this be due to the classifier or to
the one view? Hence we present this baseline. This method also
uses a single character 3-gram view and the SVM algorithm as
one of the two classifiers. For the other classifier, we use LR
instead of CNG.

Figure 3: TCA using SVM (left) and LR (right) on lexical and
syntactic views

The effects of SVM and LR on the syntactic and character
views are shown in Figure 4. Once again, SVM is defeated by LR.
Starting from the initial accuracy value of 0.33, SVM-Char ends
increasing at k=20, with a small improved value of 0.40. On the
contrary, LR-Char continues to grow with the increasing k. Its
performance gets to 0.82 after 60 iterations. As for the views, the
character view is better than the syntactic view for LR. However,
the performance of SVM-Char drops quickly after 40 iterations.

4.2 Experimental Results
4.2.1 Effects of classifiers and views of TCA
We first evaluate the effectiveness of the classifiers and views in
our two-view co-training algorithm (TCA). We experimented with
0, 10, 20, 30, 40, 50, and 60 iterations. Note that an iteration
number of 0 means that no co-training is done. Only the initial
labeled set is used to learn a classifier and then classify the
documents in the test set.
We show the effects of SVM and LR on the lexical and
character views in Figure 2. At the starting point, the performance
of LR is very close to that of SVM, both about 0.33 and 0.45 for
character and lexical views, respectively. However, after 10
iterations, the accuracy for LR-Lex is 70.45%, significantly better
than that of SVM-Lex, which is only 52.69%. When iterating 40
times, the accuracy of SVM-Lex reaches its highest value, i.e.,
52.24%. In contrast, LR-Lex shows a clear upward trend. Its
performance continuously grows from 0.81 to 0.91 when
increasing the number of iterations from 20 to 60.

Figure 4: TCA using SVM (left) and LR (right) on syntactic and
character views

In summary, the LR classifier performs significantly better than
SVM in the co-training framework. As we can see in Figures 2-4,
on the same two views, the accuracies in the right figures (LR)
outperform those in the left figures (SVM) by a large margin. One
possible reason is that LR with L2 regularization is more tolerant
to the over-fitting problems caused by the small number of
training samples compared to the large number of feature
variables. It is also known that when the number of training
examples is small, probabilistic classifiers such as LR have an

905

approach than the SVM classifier, and the lexical view performs
the best among the three views. Another interesting finding is that
co-training on one view or two views does not make too much
difference. What is important is to choose an appropriate learning
algorithm and an expressive view.
Our current study focuses on co-training using two views. Our
next step will extend the work by integrating more views such as
stylistic or vocabulary richness view into the system. Moreover,
we evaluated the performance with only one data set in this paper.
Further experiments will be conducted to determine the general
behavior of the co-training approach for the AA problem. For
example, while the LR technique stably outperforms SVM by a
large margin, the superiority of the lexical view over the character
and syntactic views may be due to the special characteristic of the
data set we investigated.

edge while discriminative classifiers like SVM can easily find
wrong separation hyperplanes. Furthermore, while the
performance of SVM fluctuates with different numbers of
iterations, the accuracy of LR goes up steadily. This again
strongly indicates that the co-training framework favors the
probabilistic classifier LR. Finally, the lexical view performs the
best among three views when the LR approach is used as the
learning technique.

4.2.2 Comparing with baselines
We now compare our proposed two view co-training framework
with two baselines. The comparison results are given in Table 1.
From the results, we can see that CNG is almost unable to
correctly classify any test case. Its accuracy is only 1.26% at the
start point. And this directly leads to the failure of the whole cotraining framework. The reason is that the other classifier SVM
can augment nearly 0 documents from the unlabeled set. The
intersection of the documents predicted by SVM and CNG is close
to the null set. We also tuned the parameter L for CNG, but it
makes little difference. To distinguish the effects of views from
the classifiers, we conduct two types of evaluations. First, we
apply the CNG+SVM framework to the lexical view. The results
are even worse. Its accuracy drops to 0.58%. The details are
omitted due to space limitations. This suggests that CNG does not
work well with very few labeled examples in our experiments.
Secondly, we replace CNG with LR, which has been shown quite
effective for co-training. Results show that co-training on the
character 3-gram view can still improve the performance, as long
as two suitable learning approaches are used as the classifiers for
co-training. For example, the accuracy for LR on the character
view grows from 32.88% to 79.54%, an absolute accuracy
increase of 46.66%.

ACKNOWLEDGEMENTS Tieyun Qian was supported in part
by the NSFC Projects (61272275, 61232002, 61202036,
61272110), and the 111 Project (B07037). Bing Liu was
supported in part by a grant from National Science Foundation
(NSF) under no. IIS-1111092.

6. REFERENCES
[1] S. Argamon, C. Whitelaw, P. Chase, S. R. Hota, N. Garg, and S.
Levitan. Stylistic text classification using functional lexical
features. JASIST 58, pp.802–822, 2007.
[2] A. Blum, and T. Mitchell. Combining labeled and unlabeled data
with co-training. COLT. pp. 92–100, 1998.
[3] J. Burrows. All the way through: Testing for authorship in
different frequency data. LLC 22, 27–47, 2007.
[4] M. Fan, T. Qian, B. Liu, M. Zhong, and G. He. Authorship
Attribution with Very Few labeled Data: A Co-Training Approach.
WAIM 2014, to appear.

Table 1. Comparisons between co-training on one view two
methods and co-training on two views one method
k
0
10
20
30
40
50
60

OneView TwoMethods
CNG- SVM- LR- SVMChar Char Char Char
1.25 33.22 32.88 33.22
1.26 32.35 62.56 40.32
1.26 32.35 71.21 61.77
1.26 32.35 75.21 36.61
1.26 33.60 77.46 30.94
1.26 33.60 78.64 24.60
1.27 33.54 79.54 23.44

[5] R. Fan, K. Chang, C. Hsieh, K. Wang, and C. Lin. Liblinear: A
library for large linear classification. JMLR 9, pp. 1871–1874,
2008.

TwoViews OneMethod (TCA)
SVM- SVMLRLRSyn
Char
Lex
Char
34.48 33.22 45.75
32.88
36.55 35.00 70.45
61.87
40.73 39.94 81.22
72.86
36.65 36.91 85.81
77.90
34.24 36.92 88.91
81.02
37.47 35.96 90.33
82.66
37.94 31.92 91.23
84.06

[6] G. Hirst and O. Feiguina. Bigrams of syntactic labels for
authorship discrimination of short texts. LLC 22, pp. 405–417,
2007.
[7] N. Jindal and B. Liu. Opinion spam and analysis. WSDM. pp. 29–
230, 2008.
[8] T. Joachims. www:cs:cornell:edu/people/tj/svm_light/old/ svm
multiclass_v2.12.html, 2007.
[9] S. Kim, H. Kim, T. Weninger, J. Han, and H. D. Kim. Authorship
classification: a discriminative syntactic tree mining approach.
SIGIR. pp. 455–464, 2011.

Initially, we expect to see the proposed two-view co-training
framework to achieve a significant performance improvement
over its corresponding one-view co-training method. However, as
shown in Table 1, when using SVM to co-train on the syntactic
and character views, its performance is actually worse than that of
LR+SVM on the character view. Meanwhile, we observe a sharp
performance increase, i.e., from 45.75% to 91.23%, in our TCA
by using LR to co-train on the lexical and character views. All
these findings strongly demonstrate that it is both the view and
classifier that have big impacts on the co-training results.

[10] D. Klein and C. D. Manning. Accurate unlexicalized parsing. In:
ACL. pp. 423–430, 2003.
[11] I. Kourtis, and E. Stamatatos. Author identification using semisupervised learning. Notebook for PAN at CLEF 2011.
[12] K. Luyckx, W. Daelemans. Authorship attribution and verification
with many authors and limited data. COLING, pp. 513–520, 2008.
[13] Y. Seroussi, F. Bohnert, and I. Zukerman. Authorship attribution
with author-aware topic models. ACL. pp. 264–269, 2012.

5. CONCLUSION

[14] Y. Seroussi, I. Zukerman, and F. Bohnert. Collaborative inference
of sentiments from texts. UMAP. pp. 195–206, 2010.

In this paper, we investigate the problem of authorship attribution
with very few labeled examples. We present a novel two-view cotraining framework which utilizes natural views of human
languages, i.e., the character, lexical and syntactic views. We
conducted a comparative study on the effectiveness of learning
techniques and different views. Results show that the logistic
regression classifier is much more effective for the co-training

[15] E. Stamatatos. Author identification using imbalanced and limited
training texts. In TIR. pp. 237–241, 2007
[16] E. Stamatatos. A survey of modern authorship attribution methods.
JASIST 60, 538–556, 2009.

906

