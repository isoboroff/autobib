Continuous Word Embeddings for Detecting Local Text
Reuses at the Semantic Level
Qi Zhang, Jihua Kang, Jin Qian, Xuanjing Huang

Shanghai Key Laboratory of Intelligent Information Processing
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, P.R.China

{qz, 12201240059, 12110240030, xjhuang}@fudan.edu.cn
ABSTRACT

content without modiﬁcation or with few modiﬁcations,
has become a common phenomenon on the Web. Rather
than reusing a whole document, sentences, facts or passages
were also often reused and modiﬁed, especially in social
media [28]. Local text reuse is usually used to refer to this
kind of phenomenon. The task of detecting local text reuses
is important for many applications such as information ﬂow
tracking [20], plagiarism detection [29], partial-duplicate
detection [39], and so on.
Along with the increasing requirements, the task has
received considerable attention in recent years [28, 39, 34,
38]. Existing works on local text reuse detection have been
conducted from diﬀerent perspectives. However, we argue
that most of them worked on the lexical level. The semantic
similarities between text segments were not considered. Let
us consider the following examples:

Text reuse is a common phenomenon in a variety of usergenerated content. Along with the quick expansion of
social media, reuses of local text are occurring much more
frequently than ever before. The task of detecting these local
reuses serves as an essential step for many applications. It
has attracted extensive attention in recent years. However,
semantic level similarities have not received consideration in
most previous works. In this paper, we introduce a novel
method to eﬃciently detect local reuses at the semantic
level for large scale problems. We propose to use continuous
vector representations of words to capture the semantic level
similarities between short text segments. In order to handle
tens of billions of documents, methods based on information
geometry and hashing methods are introduced to aggregate
and map text segments presented by word embeddings
to binary hash codes. Experimental results demonstrate
that the proposed methods achieve signiﬁcantly better
performance than state-of-the-art approaches in all six
document collections belonging to four diﬀerent categories.
At some recall levels, the precisions of the proposed method
are even 10 times higher than previous methods. Moreover,
the eﬃciency of the proposed method is comparable to or
better than that of some other hashing methods.

S1. The company announced two directors.
S2. The company appointed two directors.
S3. The company ﬁred two directors.
From these examples, we can observe that although only
one word diﬀers between the three sentences, sentence S3
should not be considered as the local text reuses of sentence
S1 and sentence S2. The meaning expressed by S3 is almost
opposite to S1 and S2 ’s. Since existing methods are usually
based on lexical level similarities, this kind of issue cannot
be well addressed by these methods.
Inspired by the success of continuous space word representations in capturing the semantic similarities in various
natural language processing tasks, we propose in this work
to incorporate continuous space word representations, Fisher
kernel framework, and hashing methods to generate hash
codes for short text segments, while preserving semantic
level similarities. The generated hash codes can be used
to detect local reuses more eﬃciently and eﬀectively. Due
to the ability of continuous space word representations
(also called “word embeddings”) at capturing syntactic
and semantic regularities, we ﬁrstly transform words in
a text segment into continuous vector representations by
looking up tables. These word representations were learned
in advance using a feed-forward neural network language
model [3], continuous skip-gram model [22], or other
continuous space word representation learning methods.
Then, the variable sizes of word embedding sets will be
aggregated into a ﬁxed length vector, Fisher Vector (FV),
based on the Fisher kernel framework [27]. Since FVs are
usually high-dimensional and dense, it makes the system less
eﬃcient for large-scale applications. Hence, the ﬁnal step of

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval - Information Search and Retrieval;
H.3.7 [Digital Libraries]: Collection, Systems Issues

Keywords
Local Text Reuse, Word Embedding, Fisher Vector

1.

INTRODUCTION

With the rapid expansion of the World Wide Web, digital
information has become much easier to access, modify, and
duplicate. Text reuse, which is the practice of using existing
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profi or commercial advantage and that copies bear this notice and the full citation
on the firs page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specifi permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00.
http://dx.doi.org/10.1145/2600428.2609597 .

797

common terms results good document representations for
the near-duplicate detection task. Since I-Match signatures
are respect to small modiﬁcations, Kolcz et al. [18] proposed
the solution of several I-Match signatures, all derived from
randomized versions in the original lexicon.
Local text reuse detection focused on identiﬁng the reused
and modiﬁed sentences, facts or passages, rather than whole
documents. Seo and Croft [28] analyzed the task and deﬁned
six categories of text reuses. They proposed a general
framework for text reuse detection. Several ﬁngerprinting
techniques under the framework were evaluated under the
framework. Zhang et al. [39] also studied the partialduplicate detection problem. They converted the task into
two subtasks: sentence level near-duplicate detection and
sequence matching. Except for the similarities between
documents, the method can simultaneously output the
positions where the duplicated parts occur. In order to
handle the eﬃciency problem, they implement their method
using three Map-Reduce jobs. Kim et al. [17] proposed to
map sentences into points in a high dimensional space and
leveraged range searches in this space. They used MD5 hash
function to generate hash code for each word. File signature
is then created by taking the bitwise-or of all signatures of
words that appear in the ﬁle.
Diﬀerent with these existing methods, in this paper, we
propose to use aggregated word embeddings to capture the
semantic level similarities to reduce the false matches.

the proposed framework is to compress FVs to binary hash
codes using hashing methods [6, 36, 26]. Through these
steps, the proposed method can map text segments into
compact binary hash codes, while preserving the similarities
at the semantic level.
Hence, it can eﬃciently and
eﬀectively achieve the local text reuse detection problem.
Through evaluating on six large collection belonging to four
diﬀerent categories, experimental results demonstrate that
the proposed framework can achieve better performance
than state-of-the-art approaches.
The main contributions of this work are summarized as
follows.
• We propose and study the task of detecting local
text reuse at the semantic level. To the best of our
knowledge, this is the ﬁrst work to focus on this
problem.
• Inspired by the advantages of continuous space word
representations, we introduce a novel method to aggregate and compress the variable-size word embedding
sets to binary hash codes through Fisher kernel and
hashing methods.
• We manually constructed six evaluation datasets of
four diﬀerent kinds categories. Experimental results
show that the proposed method are signiﬁcantly better
than existing methods.

2.

2.2

RELATED WORK

Due to the ability of solving similarity search in high
dimensional space, hash-based methods have received much
more attention in recent years.
Extensive works on
similarity search have been proposed to ﬁnd good data-aware
hash functions using machine learning techniques.
Hinton and Salakhutdinov [10] proposed to train a
multilayer neural network with a small central layer to
convert high-dimensional input vectors into low-dimensional
codes. They used a two-layer network called a Restricted
Boltzmann machine (RBM) to do it. Spectral hashing [36]
was deﬁned to seek compact binary codes in order to
preserve the semantic similarity between codewords. They
deﬁned the criterion for a good code which is related to
graph partitioning and used a spectral relaxation to obtain
a solution. Norouzi and Fleet [24] introduced a method
for learning similarity-preserving hash functions, which is
based on latent structural SVM framework. They designed
a speciﬁc loss function taking Hamming distance and binary
quantization into consideration. Self-Taught Hashing (STH)
[37] divided the hash codes learning problem into two stages.
Firstly, they used unsupervised method, binarised Laplacian
Eigenmap, to optimize l -bit binary codes for all documents
in the given corpus. The classiﬁers were trained to predict
the l -bit code for unseen documents.
Besides the works on document level, based on the qSign
framework [17], Zhang et al. [38] proposed a method to
optimize hash codes of words/characters rather than directly
use MD5 for sentence level reuse detection. They also
proposed to use GPU to accelerate the Hamming distance
calculation. Wang et al. [35] proposed to use hashing
methods to address tag completion and prediction problem.
They used discrete optimization to construct the hash codes
for both data examples and tags. The constructed the hash
codes for the observed tags are consistent.

Our approach relates to the following three research areas:
text reuse detection, learning to hash, and ﬁsher kernel
framework. In this section, we brieﬂy describe the related
works on these areas.

2.1

Learning to Hash

Text Reuse Detection

The task of detecting text reuse has received considerable
attentions in recent years. Previous works studied the
problem from diﬀerent aspects such as ﬁngerprint extraction
methods with or without linguistic knowledge, hash codes
learning methods, diﬀerent reuse granularity, and so on.
Broder [4] proposed Shingling method, which uses contiguous subsequences to represent documents. It does not
rely on any linguistic knowledge. If sets of shingles extracted
from diﬀerent documents are appreciably overlap, these
documents are considered exceedingly similar, which are
usually measured by Jaccard similarity. In order to reduce
the complexity of shingling, meta-sketches was proposed to
handle the eﬃciency problem [5]. In order to improve the
robustness of shingle-like signatures, Theobald et al. [32]
introduced a method, SpotSigs. It provides more semantic
pre-selection of shingles for extracting characteristic signatures from Web documents. SpotSigs combines stopword
antecedents with short chains of adjacent content terms.
The aim of it is to ﬁlter natural-language text passages out
of noisy Web page components. They also proposed several
pruning conditions based on the upper bounds of Jaccard
similarity.
I-Match [7] is one of the methods using hash codes to
represent input document. It ﬁlters the input document
based on collection statistics and compute a single hash
value for the remainder text. If the documents have same
hash value, they are considered as duplicates. It hinges on
the premise that removal of very infrequent terms and very

798

Sentences
S1

Corpus

...

S1

S2

Bag-of-embedded words
000110000100
110001001101
001010010011

...

Sn

110100001010
010001010101
010101011110

Model
Parameters

Table Lookup

Large Scale
Unlabeled
Corpus

Word Embeddings
Learning

Generative Model
Estimation

Word
Embeddings

Fisher Vector
Generation

Hash Codes
Generation

Figure 1: An overview of the proposed cFV-BoEW method for local text reuse detection.

2.3

Fisher Kernel

corpus can also be included into the training data. Based
on the learned word embeddings, sentences are represented
by variable-size sets of word embeddings. Generative model
estimation step, which is the generative part in the FK
framework, tries to estimate the parameters for the models
used to describe the data. Fisher Vector generation step
uses the estimated parameters of generative model and bagof-embedded words to generate FVs for all the sentences.
After that, learning to hash methods are used in the hash
codes generation step to transfer the dense FVs to hash
codes. Semantic level local text reuses can be detected
through calculating the hamming distances between these
hash codes.
From the framework, we can see that although word
embeddings computation and generative model estimation
steps are time consuming, they run only once in advance.
Meanwhile, the computational requirements of FV generation and hash code generation are limited. Hence, the
proposed framework can eﬃciently achieve the local reuse
detection task at the semantic level. In the following of this
section, we detail the main steps of the proposed framework.

Fisher Kernel (FK) introduced by Jaakkola and Haussler [15] combines the beneﬁts of generative and discriminative approaches for classiﬁcation through the kernel
derived from a generative probability models. It has been
successfully applied to many tasks [14, 23, 12, 31, 8].
Jaakkola et al. [14] proposed to use the derived kernel
function from HMMs corresponding to the protein family
of interest to detect remote protein homologies.
In
[23], Moreno et al. explored their work in using Fisher
kernel methods for audio classiﬁcation problem. They
proposed to use FK to map audio sequences into a ﬁxed
length representation and then use discriminative models to
classify the data. Hofmann proposed a FK based method
to calculate the similarities between documents. Each
document is modeled as a memoryless information source.
Fisher kernel is derived from the learned multinomial
subfamily. Sun et al. [31] introduced a LDA-based Fisher
kernel method to achieve text segmentation task. They
proposed to use Latent Dirichlet Allocation to compute
words semantic distribution and use Fisher kernel to
measure semantic similarities.
The most similar work to ours is the recent work
by Clinchant and Perronnin [8]. They proposed to use
Fisher kernel to aggregate word embeddings to represent
documents. They studied the relationship with LSI, pLSA,
and LDA. They also evaluated the representation method
through clustering and retrieval. Diﬀerent with them, in
this paper, we study the FK framework in representing
the short text segments for the local text reuse detection.
To overcome the eﬃciency problem, we also propose to
incorporate hashing methods to map Fisher vectors to
binary hash codes.

3.

3.1

Word Embeddings Learning

Representation of words as continuous vectors recently has
been shown to beneﬁt performance for a variety of NLP
and IR tasks [11, 33, 30]. Similar words tend to be close
to each other with the vector representation. Moreover,
Mikolov et al. [21] also demonstrated the learned word
representations could capture meaningful syntactic and
semantic regularities. Hence, in this work, we propose to use
word embeddings to capture the semantic level similarities
between short text segments.
Fig. 2 shows three architectures used for learning word
embeddings. wi represents the ith words in the given words
sequence {w1 , w2 , ..., wT }. Fig. 2(a) shows the architecture
of the probabilistic neural network language model (NNLM)
proposed by Bengio et al. in [3]. It can have either one
hidden layer beyond the word features mapping or direct
connections from the word features to the output layer.
They also proposed to use sof tmax function for the output
layer to guarantee positive probabilities summing to 1. The
word vectors and the parameters of that probability function

THE PROPOSED METHOD

The processing ﬂow of the proposed method is shown
in Figure 1. Given a collection of documents, sentences
or short text segments are treated as the basic processing
units. Through table lookup, all the words in a sentence
are transformed to continuous vectors generated by the word
embeddings learning step. The dashed arrow between corpus
to word embeddings learning represents that in-domain

799

P(wt = i | context)
softmax

...

wt-2

wt

...

wt-1

wt+1

wt+2

tanh

...
wt-n+1

wt-3

wt-2

wt-1

wt-2

wt-1

(a) NNLM

wt+1

(b) CBOW

wt

wt+2

(c) Skip-gram

Figure 2: Methods used to learn word embeddings. The NNLM architecture predicates the probability of words based on the
existing words [3]. CBOW predicts the current word based on the context [21]. Skip-gram predicts surrounding words given
the current word [21].
can be learned simultaneously. In this work, we only use the
learned word vectors.
Fig. 2(b) and Fig. 2(c) show the architectures of the
methods proposed by Mikolov in [21]. The architecture of
CBOW, which is similar to NNLM, is shown in Fig. 2(b).
The main diﬀerences are that (i) the non-linear hidden layer
is removed; (ii) the words from the future are included;
(iii) the training criterion is to correctly classify the current
(wt ) word. The Skip-gram architecture, which is shown
in Fig. 2(c), is similar to CBOW. However, instead of
predicting the current word based on the history and future
words, it tries to maximize classiﬁcation accuracy of words
within a certain range before and after the current word
based on only the current word as input.
Besides the methods mentioned above, there are also
a large number of works addressing the task of learning
distributed word representations [33, 19, 13]. Most of
them can also be used in this work.
The proposed
framework has no limits in using which of the continuous
word representation methods.

3.2

by a probability density function. In this work,we use
Gaussian mixture model (GMM) to do it. We assume that
the continuous word embeddings ewi in the text segment
di are generated by GMM, which is denoted as uλ in the
following. λ = [λ1 , λ2 , ..., λK ] denotes the vector of K
parameters of uλ . The ith vector component in GMM
is characterized by normal distributions with weights φi ,
means μi and covariance matrices Σi . Hence, in this work, λ
includes {ωi , μi , Σi , 1 ≤ i ≤ K}. The parameters λ usually
can be estimated through the optimization of Maximum
Likelihood (ML) criterion using Expectation Maximization
(EM) method. In the following of this section, we follow the
notations used in [27].
Based on the learned uλ , the text segment di can be
characterized using the following score function:
i
Gdλi = ∇N
λ log uλ (di ),

(1)

Gdλi

is a vector whose size is only dependent on the
where
number of parameters in λ and not the number of words in
the text segment. The gradient describes the contribution
of each individual parameters to the generative process.
According to the theory of information geometry [1], U =
{uλ , λ ∈ Λ}, which is a parametric family of distributions,
can be regarded as a Riemanninan manifold MΛ with a local
metric given by the Fisher Information Matrix (FIM) Fλ ∈
RM ×M :


d
(2)
Fλ = Edi ∼uλ Gdλi Gλi .

Fisher Vector

Base on the learned word embeddings, short text segments can be represented by variable length sets of word
embeddings, which can be viewed as Bag-of-EmbeddedWords (BoEW) [8]. Semantic level similarities between
text segments represented by BoEW can be captured more
accurately than previous BoW methods. However, since
BoEWs are variable-size sets of word embeddings and most
of the index methods are not suitable for this kinds of issues,
BoEWs cannot be directly used for large scale problems.
Inspired by the success of Fisher Kernel framework in
various image processing techniques, we propose to adopt
it to aggregate BoEW to a ﬁxed-length vector.
Given a corpus D = {di , 1 ≤ i ≤ |D|}, where di is the ith
text segment and |D| is the number of text segments in the
corpus. The ith text segment di is composed by a sequence
of text words wi = {wij , 1 ≤ j ≤ Ni }, where Ni represents
the length of di . Through table lookup, the ith text segment
di can be represented by ewi = {ewij , 1 ≤ j ≤ Ni },
where ewij is the word embedding of wij . According to
the framework of Fisher Kernel, text segments are modeled

Based on this observation, the following equation can be
used to measure the similarity between two text segments
di and dj using the the Fisher Kernel [15]:
d

KF K (di , dj ) = Gλi Fλ−1 Gλj .
d

(3)
Fλ−1

can be
Since Fλ is symmetric and positive deﬁnite,
transformed to Lλ Lλ based on the Cholesky decomposition.
Hence, KF K (di , dj ) can be rewritten as follows:
d

d

KF K (di , dj ) = Gλ i Gλ j ,

(4)

Gλdi = Lλ Gdλi = Lλ ∇λ log uλ (di ).

(5)

where

800

Table 1: Statistics of the evaluation document collections
Corpus
Language
Category
#Docs
TIPSTER
English
News article
1,078,925
Tweets2011 Twitter
English
Microblog
15,204,939
ClueWeb09-T09B
English
Web Page
50,220,423
Baidu Zhidao
Chinese
Q&A Community
33,497,107
Sina Weibo
Chinese
Microblog
267,612,493
SogouT 2.0
Chinese
Web Page
37,205,218

where sign(·) represents the element-wise sign function, and
f (x; w) denotes a real-valued transformation from Rp to
Rq . In this work, Fisher vectors of text segments are
the x in mapping function b(x; w). A variety of existing
methods have been proposed to achieve this task under
this framework using diﬀerent forms of f and diﬀerent
optimization objectives. Most of the learning to hash
methods for dense vectors can be used in this framework.
In this work, we evaluated several state-of-the-arts hashing
methods, whose performances are shown in the experiment
section.

Under the assumption that ewij is sampled independently,
in this work, Gλdi can be rewritten as follows:
Ni


Lλ ∇λ log uλ (ewij ).

(6)

j=1

In [27], Gλdi is also referred to as Fisher Vector of di . The
dot product between Fisher vectors can be used to calculate
the semantic similarities.
Based on the speciﬁc probability density function, GMM,
we used in this work, FV of di is respect to the mean μ and
standard deviation σ of all the mixed Gaussian distributions.
Let γj (k) be the soft assignment of the jth word embedding
ewij in di to Gaussian k (uk ):
ωi uk (ewij )
γj (k) = p(k|ewij ) K
j=1 ωk uk (ewij )

4.

di
Gσ,k
=

1
√

Ni


N i ωi

1
√

Ni 2ωi


γj (k)

j=1

Ni

t=1


γj (k)

ewij − μk
σk

(7)



4.1

(8)

(ewij − μk )2
−1 ,
σk2

Hash Code Generation

Through the previoussteps, a variable length of text
segments can be transferred to a ﬁxed length vector
and the semantic similarities between text segments can
be preserved. However, Fisher vectors are usually high
dimensional and dense. It limits the usages of FVs for largescale applications, where computational requirement should
be studied. In this work, we propose to use hashing methods
to address the eﬃciency problem.
The task of generating hash codes for samples can be
formalized as learning a mapping b(x), referred to as a hash
function, which can project p-dimensional real-valued inputs
x ∈ Rp onto q-dimensional binary codes h ∈ H ≡ {−1, 1}q ,
while preserving similarities between samples in original
spaces and transformed spaces. The mapping b(x) can be
parameterized by a real-valued vector w as:
b(x; w) = sign(f (x; w)),

Datasets

We evaluate the proposed method on two diﬀerent
datasets. The ﬁrst one is used in [38] (CRD for short in the
following). It contains six collections includes: TIPSTER
(Volume 1-3), ClueWeb09-T09B, Tweets2011, SogouT 2.0,
Baidu Zhidao, Sina Weibo. Table 1 shows the statistics of
the six collections. From the statistics, we can see that two
languages (English and Chinese) and four categories (news,
web page, microblog, and Q&A community ) are included
in the dataset. They randomly selected 1 million sentences
from each collections as the evaluation dataset and 2,000
sentences as reuse detection queries. All the sentences whose
similarities the query in word level are bigger than 0.8 are
extracted as golden standards. In this work, we used the
same corpus as them to evaluate the proposed framework.
Since the golden standards used in [38] are constructed
based on only similarities in word level, semantic level
similarities were not taken into consideration. In this
work, we manually annotated another dataset, CRD-S. We
randomly selected 100 queries from the original 2,000 queries
in CRD for each of the collections. We calculated the
cosine similarities between them and all the sentences. The
sentences whose similarities with the query are higher than
0.7 were extracted as candidates for further processing.
Three annotator were asked to determine whether an
extracted candidate is a reuse of the query or not. To
evaluate the quality of corpus, we validated the agreements
of human annotations using Cohen’s kappa coeﬃcient. The
average κ among all annotators is 0.637. It indicates that
the annotations of the corpus are reliable. These manually
annotated pairs construct the golden standards of CRD-S.

where Ni is the number of word embeddings in di . The
division between vectors is as a term-by-term operation.
The ﬁnal gradient vector Gλdi is is the concatenation of the
di
di
Gμ,k
and Gσ,k
vectors for k = 1...K. Let T denotes the
dimensionality of the word embeddings. The ﬁnal gradient
vector Gλdi is therefore 2KT-dimensional.

3.3

EXPERIMENTS

In this section, we ﬁrstly describe how we construct the
collection. Then we introduce the experiment conﬁgurations
and baseline methods. Finally, the evaluation results and
discussions are given.

Mathematical derivations lead to:
di
=
Gμ,k

Size
3.25GB
2.13GB
490.4GB
22.8GB
418.6GB
558.0GB

(9)

801

4.2

Experiment Configuration

Table 2: The word embeddings used in this work. The top 8
rows of the table are English ones and the others are Chinese
word embeddings.

Following the parameters used in [17] and [38], in this
work, we also set the length of hash code to 32 bit.
Words for both English and Chinese collections are used
as the basic units for learning vector representations. Since
Chinese is written without spaces between words, we use
FudanNLP [25] to segment sentences into words. For
generating Fisher vectors for text segments represented by
word embeddings, we use INRIA’s Fisher vector implementation [16]. The number of Gaussian densities in GMM is
set to 32. Precision (P), Recall (R), and F1-score (F1 ) are
used as the evaluation metrics.
For comparing the eﬀectiveness, the following state-of-theart methods were also evaluated on both CRD and CRD-S
datasets.

Dim.

• Semantic Hashing (SmH): It was proposed by
Hinton and Salakhutdinov in [10]. Semantic hashing
is a multilayer neural network with a small central
layer to convert high-dimensional input vectors into
low-dimensional codes. We use the toolkit provided
by Ruslan Salakhutdinov and Geoﬀ Hinton.1
• Spectral Hashing (SpH): It was deﬁned to seek
compact binary codes preserving similarity using spectral relaxation to obtain an eigenvector solution [36].
We use the toolkit provided by the authors.2

Corpus

# Words

C&W [9]

50

Wikipedia

631M

C&W-T [33]

50

RCV1

63M

HLBL-T [33]

50

RCV1

63M

GCNLM [13]

50

Wikipedia

1M

CBOW [21]

50

In-Domain

–

CBOW-CL

50

Clueweb09

79M

Skip-gram [21]

50

In-Domain

–

Skip-gram-CL

50

Clueweb09

79M

GCNLM [13]

50

Sogou News

4M

CBOW [21]

50

In-Domain

–

CBOW-CL

50

Clueweb09

52M

Skip-gram [21]

50

In-Domain

–

Skip-gram-CL

50

Clueweb09

52M

generation methods, we evaluate several combinations of
them. CBOW+SpH presents the words embeddings learned
with CBOW and compressed with spectral hashing. Skipgram+SimH denotes the combination of Skip-gram and
similarity hash [6].

• Simhash (SimH): Charikar [6] proposed to use
random projection methods to estimate the cosine
similarity measure between two vectors. In this work,
we re-implement the method for evaluation.
• Expanded qSign (E-qSign): Zhang et al. [38]
extended the qSign framework with learned signatures
of words for local text reuse detection. Since we use
the same evaluation dataset as them, we directly use
the results reported in [38].

4.3
4.3.1

Experimental Results
Effectiveness evaluation

For comparing the performances of diﬀerent hash code
generation methods, we ﬁrstly evaluate the proposed method
and state-of-the-art methods in CRD corpus. For English
collections, we use the word embeddings provided by
Collobert et al. [9]. For Chinese datasets, we use CBOWCL method, which uses ClueWeb09 as the training corpus,
to generate word embeddings. Spectral hash and similarity
hash were used to compact Fisher vectors of sentences into
hash codes. The precision-recall curves graph of all six
collections are shown in Figure 3. From the results we can
see that, in all cases, both the proposed methods achieve
better performance than the other hash code generation
approaches. In all three English datasets, the proposed
method achieves signiﬁcantly better results than existing
methods. Among the three Chinese datasets, the proposed
methods achieve the highest relative improvement in SogouT
dataset. We think that the performance loss provided by
Chinese word segmentation method may be one of the main
reasons. Since the most of the documents in Baidu Zhidao
and Weibo are user generated, the performances of CWS
toolkit are worse than in SogouT. The error of CWS may
impact the generated Fisher vectors and the ﬁnal results.
In most of the cases, the performance of Simhash and
Spectral hash are the worse than others. We think that
the main reason is that the vector of short text segment is
too sparse with BoW presentation. However, Spectral hash
and Simhash can preserve the similarities between FVs well.

For SmH, SpH, and SimH methods, text segments are represented by bag-of-words model. The vocabulary construction
is based on the TF·IDF scores of words. We ﬁltered stop
words and low frequency words and selected 20,000 words
to construct the vocabulary.
As we mentioned in the previous section, most of word
embeddings and hashing methods can be used in the
proposed framework.
We select some of state-of-theart methods for evaluation. Tabel 2 shows the word
representations we used in this work. Due to the high
computing cost required by C&W and HLBL, we used the
publicly available word embeddings provided by Collobert
et al. [9]3 and Turian et al. [33]4 for English words. The
word representations trained by GCNLM are also public
available for English. 5 CBOW and Skip-gram represent
the word embeddings estimated based on the evaluation
data sets. CBOW-CL and Skip-gram-CL represent the
word embeddings trained based on ClueWeb09. Since
the proposed framework has no limitations about using
which word embeddings learning methods and hash codes
1

http://www.cs.toronto.edu/∼hinton/
http://www.cs.huji.ac.il/∼yweiss/SpectralHashing/
3
http://ml.nec-labs.com/senna/
4
http://metaoptimize.com/projects/wordreprs/
5
http://www.socher.org/
2

802

1

0.8

0.8

0.8

0.6

0.6

0.6

0.2
0
0.88

C&W+SpH
C&W+SimH
E-qSign
SmH
SpH
SimH

0.9

0.92

0.4
0.2

0.94

Recall

0.96

0.98

0
0.88

1

(a) TIPSTER

Precision

0.8
0.6

Precision

CBOW-CL+SpH
CBOW-CL+SpH
E-qSign
SmH
SpH
SimH

0.4
0.2

0.92

0.94

Recall

C&W+SpH
C&W+SimH
E-qSign
SmH
SpH
SimH

0.2

0.92

0.94

Recall

0.96

0.98

0

1

0.9

1

0.8

0.8

0.6

0.6

0.4

0.96

(d) Baidu Zhidao

0.98

1

0

CBOW-CL+SpH
CBOW-CL+SpH
E-qSign
SmH
SpH
SimH

0.9

0.92

0.92

0.94

0.96

0.98

Recall

1

(c) ClueWeb09-T09B

1

0.2

0.9

0.9

0.4

(b) Tweets2011 Twitter

1

0
0.88

C&W+SpH
C&W+SimH
E-qSign
SmH
SpH
SimH

Precision

0.4

Precision

1

Precision

Precision

1

CBOW-CL+SpH
CBOW-CL+SpH
E-qSign
SmH
SpH
SimH

0.4
0.2

0.94

0.96

Recall

(e) Sina Weibo

0.98

1

0

0.2

0.4

0.6

Recall

0.8

1

(f) SogouT 2.0

Figure 3: The precision-recall curves of diﬀerent hash code generation methods in all six collections of CRD.
lexical similarities drop. However, the performances of the
proposed methods are even better than it in CRD. We think
that it is due to the golden standards used in CRD, which
may contain false positive ones. While, testing instances
were manually annotated in CRD-S Since the vectors of
short text segments represented by BoW model extremely
sparse, the performances of Spectral hash and Simhash are
much lower than others. Due to space limited, we cannot
list all the results for SogouT in Table 3(f). When the
threshold of hamming distance is set to 10, the recall and
precision are 96.8% and 19.6% respectively. Among all the
methods, the proposed methods achieve the best result in all
six collections. At the same recall level, the precisions of the
proposed method are 2-10 times higher than E-qSign. It also
demonstrates that incorporating the word embeddings can
bring beneﬁts for calculating the semantic level similarities.
The following hash codes are generated by CBOW-SpH
for the sentences illustrated in the Section 1. The numbers

To detail the performance of our framework using diﬀerent
word embeddings and hashing methods, we select two
datasets, SogouT and TISPTER, for evaluating Chinese and
English respectively. Figure 4(a) and (b) give the results in
TIPSTER with diﬀerent hashing method to compact FVs.
From the results, we can see that the proposed method with
diﬀerent word embeddings and hashing methods can achieve
signiﬁcantly better performance than previous methods in
most of the cases. For TIPSTER, the word embeddings
C&W generated by [9] obtain the best performance. The
training data used by them is also the biggest among other
word embeddings used in this work. Comparing the results
shown in Figure 4 (a) and (b), we can observe that the
performances of Simhash and Spectral hash are similar with
each other when the same word embeddings are used. Figure
4(c) gives the results in SogouT. Because of the highly
computational requirements, we evaluate GCNLM, CBOW
and Skip-gram for SogouT. From the results, we can see that
the proposed methods with diﬀerent word embeddings and
hashing method can always achieve better performance than
existing methods.
As described in the previous section, in this work, we
also manually annotated another dataset CRD-S to evaluate
the semantic level duplicates.
Table 3 illustrates the
results of diﬀerent methods in CRD-S. The precision and
recall of diﬀerent methods are shown in the table. The
ﬁrst column in all the tables represent the thresholds of
diﬀerent hamming distances. Comparing to the results
in CRD, the performances of existing methods based on

[0xDD60EFE9] S1. The company announced two directors.
[0xDD60EFE9] S2. The company appointed two directors.
[0xDD20CFE9] S3. The company ﬁred two directors.

in square brackets are the generated hash codes for the
sentences in the right. From the examples we can see
that the hamming distance between the hashcodes of S1
and S3 is two. Although there is one word diﬀerence
between S1 and S2, the hash codes of S1 and S2 are equal.

803

bits diﬀ.
d=0
d=1
d=2
d=3
d=4

bits diﬀ.
d=0
d=1
d=2
d=3
d=4

bits diﬀ.
d=0
d=1
d=2
d=3
d=4

bits diﬀ.
d=0
d=1
d=2
d=3
d=4

bits diﬀ.
d=0
d=1
d=2
d=3
d=4

bits diﬀ.
d=0
d=1
d=2
d=3
d=4

SmH
P
0.125
0.046
0.014
0.002
0.001

Table 3: The performances of diﬀerent methods in CRD-s.
SpH
SimH
E-qSign
C&W+SimH
R
P
R
P
R
P
R
P
R

0.962
0.982
0.996
0.999
1.000

0.024
0.022
0.016
0.005
0.001

SmH

0.968
0.989
0.996
0.999
1.000

0.017 0.969
0.005 0.992
0.002 0.993
0.001 0.996
0.001 0.999
(a) TIPSTER

SpH

SimH

P

R

P

R

0.012
0.011
0.009
0.008
0.001

0.954
0.982
0.993
0.997
1.000

0.0030
0.0014
0.0007
0.0003
0.0001

0.949
0.953
0.984
0.990
0.998

SmH

P

0.0004 0.949
0.0002 0.987
0.0001 0.990
0.0001 0.999
0.0000 1.000
(b) Twitter

SpH

SimH

P

R

P

R

0.232
0.121
0.076
0.027
0.012

0.927
0.945
0.980
0.993
1.000

0.071
0.040
0.024
0.015
0.004

0.955
0.985
0.991
0.997
1.000

SmH

P

SimH

R

P

R

0.152
0.082
0.043
0.010
0.005

0.899
0.967
0.993
0.998
1.000

0.015
0.005
0.002
0.001
0.001

0.942
0.951
0.965
0.975
0.998

P

R

SimH

R

P

R

0.392
0.334
0.177
0.062
0.020

0.991
0.994
0.997
0.998
1.000

0.030
0.011
0.007
0.004
0.001

0.996
0.997
0.998
0.999
1.000

P

R

SimH

R

P

R

0.190
0.087
0.045
0.022
0.016

0.362
0.507
0.784
0.882
0.9515

0.030
0.011
0.004
0.002
0.001

0.356
0.411
0.459
0.520
0.689

P

R

0.790
0.887
0.971
0.987
0.999

E-qSign
P
R
0.907
0.928
0.940
0.999
1.000

E-qSign
P
R
0.914
0.936
0.978
0.999
1.000

E-qSign
P
R

0.030 0.396 0.611
0.009 0.421 0.228
0.004 0.452 0.108
0.002 0.520 0.030
0.001 0.605 0.010
(f) SogouT 2.0

804

0.851
0.884
0.943
0.985
0.998

E-qSign
P
R

0.048 0.162 0.983
0.026 0.490 0.678
0.003 0.605 0.362
0.004 0.692 0.215
0.001 0.714 0.129
(e) Sina Weibo

SpH

P

0.870
0.517
0.380
0.321
0.005

0.014 0.948 0.667
0.005 0.959 0.399
0.002 0.973 0.179
0.001 0.977 0.020
0.000 0.998 0.001
(d) Baidu Zhidao

SpH

P

SmH

R

0.907
0.922
0.957
0.986
0.998

E-qSign
P
R

0.027 0.974 0.984
0.011 0.982 0.909
0.005 0.990 0.744
0.003 0.999 0.436
0.002 1.000 0.166
(c) ClueWeb09

SpH

P

SmH

R

0.831
0.801
0.337
0.075
0.016

0.398
0.691
0.913
0.983
0.999

0.806
0.794
0.696
0.607
0.559

0.966
0.974
0.983
0.992
0.993

C&W+SimH
P
R
0.903
0.899
0.818
0.814
0.716

0.921
0.933
0.951
0.968
0.980

C&W+SimH
P
R
0.963
0.858
0.687
0.446
0.206

0.983
0.990
0.997
0.998
1.000

C&W+SpH
P
R
0.806
0.797
0.704
0.593
0.528

0.966
0.974
0.983
0.992
0.993

C&W+SpH
P
R
0.903
0.888
0.794
0.791
0.756

0.922
0.936
0.952
0.970
0.975

C&W+SpH
P
R
0.956
0.866
0.628
0.411
0.183

0.984
0.992
0.997
0.998
1.000

CBOW+SimH
P
R

CBOW+SpH
P
R

0.753
0.636
0.424
0.260
0.168

0.741
0.639
0.404
0.257
0.162

0.922
0.930
0.932
0.947
0.996

0.922
0.928
0.940
0.950
0.995

CBOW+SimH
P
R

CBOW+SpH
P
R

0.990
0.980
0.974
0.969
0.920

0.990
0.978
0.975
0.964
0.902

0.916
0.918
0.978
0.979
0.994

0.916
0.918
0.979
0.983
0.996

CBOW+SimH
P
R

CBOW+SpH
P
R

0.996
0.979
0.913
0.828
0.734

0.991
0.977
0.907
0.824
0.753

0.166
0.387
0.519
0.631
0.694

0.167
0.363
0.418
0.618
0.687

1

0.8

0.8

0.8

0.6

0.6

0.6

C&W+SpH
C&W-T+SpH
HLBL-T+SpH
GCNLM+SpH
CBOW+SpH
CBOW-CL+SpH
Skip-gram+SpH
Skip-gram-CL+SpH
E-qSign

0.4
0.2
0
0.88

0.9

0.92

C&W+SimH
C&W-T+SimH
HLBL-T+SimH
GCNLM+SimH
CBOW+SimH
CBOW-CL+SimH
Skip-gram+SimH
Skip-gram-CL+SimH
E-qSign

0.4
0.2

0.94

Recall

0.96

0.98

0
0.88

1

(a) TIPSTER(SpH)

Precision

1

Precision

Precision

1

0.9

0.92

CBOW+SpH
CBOW+SimH
CBOW-CL+SpH
CBOW-CL+SimH
Skip-gram+SpH
Skip-gram+SimH
Skip-gram-CL+SpH
Skip-gram-CL+SimH
GCNLML+SpH
GCNLM+SimH
E-qSign

0.4
0.2

0.94

Recall

0.96

0.98

1

0

0

(b) TIPSTER(SimH)

0.2

0.4

0.6

Recall

0.8

1

(c) SogouT

Figure 4: The precision-recall curves of diﬀerent word embeddings in SogouT and Tispter.
1000
900

CBOW-CL+SimH

800

Time (S)

that the computational complexity of the proposed method
is comparable with and state-of-the hashing methods. The
eﬃciency of CBOW-CL-SimH is even better than spectral
hash and semantic hash. It demonstrates that the proposed
method is applicable for large scale applications.

CBOW-CL+SpH

700

SmH

600

SpH

500

5.

400

In this work, we study the semantic level local text
reuse detection problem and introduce an novel framework
to eﬃciently achieve the task. To capture the semantic
level similarities between short text segment, we propose to
use continuous vector representations of words to represent
short text segments. For processing tens of billions of
documents, we propose to incorporate the Fisher vectors
to aggregate the variable size BoEW to represent text
segments. Moreover, we use hashing methods to compress
the dense and high dimensional FVs to binary hash codes.
Through experiments on six diﬀerent collections in both
Chinese and English, we demonstrate that the proposed
method can achieve better performance than state-of-theart approaches in all collections. Besides that, the eﬃciency
of the proposed method is comparable with most of hashing
methods.

300
200
100
0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

# Sentences(millions)
Figure 5: The eﬃciency comparison of diﬀerent hashing
methods.

It demonstrates that the proposed method can eﬀectively
preseve the semantic level similarities.

4.3.2

Efficienc Evaluation

6.

Due to the requirement of processing huge amounts of
data, eﬃciency is also an important issue. In this work, we
compare the running time of the proposed approach with
other hashing learning methods. Although the oﬄine stage
of the proposed framework requires massive computation
cost, the computational complexity of online stage is small
or comparable to other hashing methods. Figure 5 shows
the eﬃciency comparison of diﬀerent hashing methods. We
implement the all methods to run on single thread in
the same machine, which contains Xeon quad core CPUs
(2.53GHz) and 32GB RAM. All the methods take the
sentences as inputs. The processing time is calculated
from receiving the inputs to generating hash codes. For
processing out-of-sample extension of spectral hashing, we
propose to use the Nystrom method [2] to do it. Since the
computational cost of generating Fisher vector for diﬀerent
word embeddings are same, we only list the results of word
embeddings CBOW-CL. From the results, we can observe

CONCLUSIONS

ACKNOWLEDGEMENT

The authors wish to thank the anonymous reviewers for
their helpful comments. This work was partially funded
by 973 Program (2010CB327900), National Natural Science Foundation of China (61003092,61073069), Shanghai
Leading Academic Discipline Project (B114) and “Chen
Guang” project supported by Shanghai Municipal Education
Commission and Shanghai Education Development Foundation(11CG05).

7.

REFERENCES

[1] S. Amari and H. Nagaoka. Methods of information
geometry, volume 191. AMS Bookstore, 2000.
[2] Y. Bengio, O. Delalleau, N. Le Roux, J.-F. Paiement,
P. Vincent, and M. Ouimet. Learning eigenfunctions
links spectral embedding and kernel pca. Neural
Computation, 2004.

805

[22] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. Distributed representations of words and
phrases and their compositionality. In Proceedings of
NIPS. 2013.
[23] P. J. Moreno and R. Rifkin. Using the ﬁsher kernel
method for web audio classiﬁcation. In Proceedings of
ICASSP’00, 2000.
[24] M. Norouzi and D. Fleet. Minimal loss hashing for
compact binary codes. In Proceedings of ICML ’11.
[25] X. Qiu, Q. Zhang, and X. Huang. Fudannlp: A toolkit
for chinese natural language processing. In Proceedings
of ACL’13, 2013.
[26] R. Salakhutdinov and G. Hinton. Semantic hashing.
International Journal of Approximate Reasoning,
50(7):969–978, 2009.
[27] J. Sánchez, F. Perronnin, T. Mensink, and J. Verbeek.
Image classiﬁcation with the ﬁsher vector: Theory and
practice. International Journal of Computer Vision,
pages 1–24, 2013.
[28] J. Seo and W. B. Croft. Local text reuse detection. In
Proceedings of SIGIR ’08, 2008.
[29] A. Si, H. V. Leong, and R. W. Lau. Check: a
document plagiarism detection system. In Proceedings
of the 1997 ACM symposium on Applied computing,
1997.
[30] R. Socher, E. H. Huang, J. Pennin, C. D. Manning,
and A. Ng. Dynamic pooling and unfolding recursive
autoencoders for paraphrase detection. In Advances in
Neural Information Processing Systems, 2011.
[31] Q. Sun, R. Li, D. Luo, and X. Wu. Text segmentation
with lda-based ﬁsher kernel. In Proceedings of ACL’08,
2008.
[32] M. Theobald, J. Siddharth, and A. Paepcke. Spotsigs:
robust and eﬃcient near duplicate detection in large
web collections. In Proceedings of SIGIR ’08, 2008.
[33] J. Turian, L. Ratinov, and Y. Bengio. Word
representations: a simple and general method for
semi-supervised learning. In Proceedings of ACL’10,
2010.
[34] E. Varol, F. Can, C. Aykanat, and O. Kaya. Codet:
Sentence-based containment detection in news
corpora. In Proceedings of CIKM’11, 2011.
[35] Q. Wang, L. Ruan, Z. Zhang, and L. Si. Learning
compact hashing codes for eﬃcient tag completion and
prediction. In Proceedings of CIKM ’13, 2013.
[36] Y. Weiss, A. Torralba, and R. Fergus. Spectral
hashing. In Processings of NIPS, 2008.
[37] D. Zhang, J. Wang, D. Cai, and J. Lu. Self-taught
hashing for fast similarity search. In Proceeding of
SIGIR ’10, 2010.
[38] Q. Zhang, Y. Wu, Z. Ding, and X. Huang. Learning
hash codes for eﬃcient content reuse detection. In
Proceedings of SIGIR’12, 2012.
[39] Q. Zhang, Y. Zhang, H. Yu, and X. Huang. Eﬃcient
partial-duplicate detection based on sequence
matching. In Proceedings of SIGIR ’10, 2010.

[3] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin.
A neural probabilistic language model. J. Mach.
Learn. Res., 3, Mar. 2003.
[4] A. Z. Broder. On the resemblance and containment of
documents. In Proceedings of SEQUENCES 1997,
1997.
[5] A. Z. Broder. Identifying and ﬁltering near-duplicate
documents. In Combinatorial Pattern Matching, pages
1–10, 2000.
[6] M. S. Charikar. Similarity estimation techniques from
rounding algorithms. In Proceedings of STOC ’02,
2002.
[7] A. Chowdhury, O. Frieder, D. Grossman, and M. C.
McCabe. Collection statistics for fast duplicate
document detection. ACM Trans. Inf. Syst.,
20(2):171–191, 2002.
[8] S. Clinchant and F. Perronnin. Aggregating
continuous word embeddings for information retrieval.
August 2013.
[9] R. Collobert, J. Weston, L. Bottou, M. Karlen,
K. Kavukcuoglu, and P. Kuksa. Natural language
processing (almost) from scratch. The JMLR, 2011.
[10] G. Hinton and R. Salakhutdinov. Reducing the
dimensionality of data with neural networks. Science,
313(5786):504 – 507, 2006.
[11] G. Hinton and R. Salakhutdinov. Discovering binary
codes for documents by learning deep generative
models. Topics in Cognitive Science, 2010.
[12] T. Hofmann. Learning the similarity of documents:
An information-geometric approach to document
retrieval and categorization. 2000.
[13] E. H. Huang, R. Socher, C. D. Manning, and A. Y.
Ng. Improving word representations via global context
and multiple word prototypes. In Proceedings of
ACL’12, 2012.
[14] T. Jaakkola, M. Diekhans, and D. Haussler. Using the
ﬁsher kernel method to detect remote protein
homologies. In ISMB, volume 99, pages 149–158, 1999.
[15] T. Jaakkola, D. Haussler, et al. Exploiting generative
models in discriminative classiﬁers. Proceedings of
NIPS, 1999.
[16] H. Jégou, F. Perronnin, M. Douze, J. Sánchez,
P. Pérez, and C. Schmid. Aggregating local image
descriptors into compact codes. IEEE TPAMI, 2011.
[17] J. W. Kim, K. S. Candan, and J. Tatemura. Eﬃcient
overlap and content reuse detection in blogs and online
news articles. In Proceedings of WWW ’09, 2009.
[18] A. Kolcz, A. Chowdhury, and J. Alspector. Improved
robustness of signature-based near-replica detection
via lexicon randomization. In Proceedings of SIGKDD
2004, pages 605–610, 2004.
[19] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y.
Ng, and C. Potts. Learning word vectors for sentiment
analysis. In Proceedings of ACL’11, 2011.
[20] D. Metzler, Y. Bernstein, W. B. Croft, A. Moﬀat, and
J. Zobel. Similarity measures for tracking information
ﬂow. In Proceedings of CIKM ’05, 2005.
[21] T. Mikolov, K. Chen, G. Corrado, and J. Dean.
Eﬃcient estimation of word representations in vector
space. In Proceedings of Workshop at ICLR, 2013.

806

