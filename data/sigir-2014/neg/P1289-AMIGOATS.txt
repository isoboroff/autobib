A General Account of Effectiveness Metrics for Information
Tasks: Retrieval, Filtering, and Clustering
Enrique Amigó

Julio Gonzalo

Stefano Mizzaro

nlp.uned.es
E.T.S.I. Informática, UNED
c/ Juan del Rosal, 16, 28040
Madrid, Spain

nlp.uned.es
E.T.S.I. Informática, UNED
c/ Juan del Rosal, 16, 28040
Madrid, Spain

Dept. of Math. & CompSci
Udine University
Via delle Scienze, 206, 33100
Udine, Italy

enrique@lsi.uned.es

julio@lsi.uned.es

mizzaro@uniud.it

ABSTRACT
In this tutorial we will present, review, and compare the
most popular evaluation metrics for some of the most salient
information related tasks, covering: (i) Information Retrieval,
(ii) Clustering, and (iii) Filtering. The tutorial will make a
special emphasis on the specification of constraints for suitable metrics in each of the three tasks, and on the systematic comparison of metrics according to such constraints.
The last part of the tutorial will investigate the challenge of
combining and weighting metrics.

Figure 1: IR effectiveness metrics

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information
Search and Retrieval

poses, and it will also induce researchers to make explicit
and clarify the assumptions behind metrics.

Keywords

2.

Evaluation, Metrics

This tutorial relies on some recent results, that we have obtained applying measurement theory to derive properties,
constraints, and axioms of effectiveness metrics and metric
combinations for IR, Clustering, and Filtering.
The overall tutorial aim is to describe effectiveness metrics
with a general approach, to analyze their properties within
a conceptual framework, and to provide tools to select the
most appropriate metric when needed. More in detail, the
specific goals of this tutorial are: (i) to provide an overall
introduction to effectiveness metrics; (ii) to seek generality by analyzing several metrics, and from three different
fields (besides retrieval, also clustering and filtering); (iii)
to provide a general framework based on measurement theory to understand and define metrics and to state metric
axioms/constraints; (iv) to provide a taxonomy of metrics
and discuss how different metric families satisfy different
constraints; (v) to describe the tools for selecting an appropriate metric for each user specific scenario; and (vi) to
understand the effect of weighting metrics arbitrarily.

1.

BACKGROUND AND MOTIVATIONS

Figure 1 shows a tagcloud of most Information Retrieval
(IR) effectiveness metrics: we can count around one hundred
IR metrics, let alone user-oriented ones or metrics for related
tasks such as filtering, clustering, recommendation, summarization, etc. Evaluation metrics are not merely a tool to
assess and compare systems. In the space of solutions to a
problem, they work like a GPS that tells researchers where is
the final destination, providing the operational definition of
what systems should do. IR researchers can choose among
a set of over one hundred metrics, all pointing at different
places in the map, and in general there is no clear procedure
to choose the most adequate metric in a specific scenario.
And a wrong choice may imply falling off a cliff.
We believe that a better understanding of metrics, and of
their conceptual, foundational, and formal properties, would
help to avoid wasting time in tuning retrieval systems according to effectiveness metrics inadequate to specific pur-

3.

OBJECTIVES

OUTLINE

The tutorial is structured as follows:
1. Introduction: IR effectiveness metrics.
2. Measurement theory and basic axioms.
3. Meta-evaluating metrics with formal constraints.
4. Other tasks (metrics for clustering and filtering).
5. Combining metrics.
6. Summary and wrap-up.

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the owner/author(s). Copyright is held by the
author/owner(s).
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
ACM 978-1-4503-2257-7/14/07.
http://dx.doi.org/10.1145/2600428.2602296.

1289

