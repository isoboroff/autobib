Diversifying Query Suggestions
Based on Query Documents
Youngho Kim and W. Bruce Croft
140 Governors Drive, University of Massachusetts, Amherst, MA 01003, USA

{yhkim, croft}@cs.umass.edu
ABSTRACT
Many domain-specific search tasks are initiated by documentlength queries, e.g., patent invalidity search aims to find prior art
related to a new (query) patent. We call this type of search Query
Document Search. In this type of search, the initial query document is typically long and contains diverse aspects (or sub-topics).
Users tend to issue many queries based on the initial document to
retrieve relevant documents. To help users in this situation, we
propose a method to suggest diverse queries that can cover multiple aspects of the query document. We first identify multiple query aspects and then provide diverse query suggestions that are
effective for retrieving relevant documents as well being related to
more query aspects. In the experiments, we demonstrate that our
approach is effective in comparison to previous query suggestion
methods.

Abstract
A system for efficiently retrieving information from one of several databases. The system acts as an intermediary between users and databases, managing user access to the databases so that query specification, query execution,
and query result retrieval can occur … If the user selects a predefined report
form, the system …

Figure 1: Query Document Example
In this paper, to improve query suggestions for QDS, we introduce
the concept of diversifying query suggestions based on query
documents. Emphasizing diverse query suggestions is important
because otherwise the system may suggest multiple similar queries which would produce near-duplicate search results. In addition, diversified suggestions can help to retrieve more relevant
documents related to a query document. Typically a query document can be quite long (e.g., a patent document can contain thousands of terms) and would include several aspects (or sub-topics).
So, many relevant documents are related to these different aspects,
and suggesting queries related to multiple aspects can be effective
for retrieving more relevant documents. As an example, Figure 1
shows an example query document. This query document is a
United States patent, published in 2002, which describes Information Retrieval (IR) systems using multiple databases. The patent application mentions several components (or aspects) such as
“query specification”, “query execution”, “query result retrieval”,
etc., and the queries suggested for this patent would be more effective if they can cover such query aspects. In fact, many relevant
documents for this patent are related to the aspects. Table 1 lists
the relevant documents for the query document in Figure 1. In this
example, A and B are related to the aspect “query specification”,
whereas C refers to “query execution”. In addition, D describes
report systems, which forms another aspect (i.e., “report form”).

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval – Query Formulation, Search Process.

Keywords
Diversifying query suggestions; Patent retrieval; Literature search

1. INTRODUCTION
Many domain-specific search tasks can start from documentlength initial queries. For example, prior-art search aims to find
past relevant patents which may conflict with new patents [7][11];
in academic literature search, academic authors need to find relevant papers that should be cited in their writings. One unique
characteristic of these search tasks is more emphasis on recall, i.e.,
not missing relevant documents is more important than placing a
relevant document at the top rank. In this paper, we call this type
of domain-specific search task Query Document Search (QDS).
Note that we use the term “query document” to refer to the document-length initial query in domain-specific searches.
Query suggestion (e.g., [12]) can be particularly helpful for QDS.
For example, patent examiners use about 15 queries to validate a
new patent [11]. In addition, patent engineers have stated that
automatic suggestion of search vocabulary is required for patent
search systems [1]. Although a number of existing methods (e.g.,
[2][13]) can be used, these techniques need improvement for QDS
and do not consider diversity.

No
A
B

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from Permissions@acm.org.
SIGIR’14, July 06–11, 2014, Gold Coast, QLD, Australia.
Copyright 2014 ACM 978-1-4503-2257-7/14/07$15.00.
http://dx.doi.org/10.1145/2600428.2609467

C
D

Table 1: Relevant Documents for Figure 1.
Title
Aspect
System for generating structured query
query
language statements and integrating …
specificaCombining search criteria to form a
tion
single search …
Query language execution on heterogequery
neous database servers using …
execution
System and method for generating rereport
ports from a computer database …
form

Motivated by these types of examples, we propose a method to
suggest diverse queries based on query documents. To solve this,
we adopt a three-step process: (Step 1) Query Aspect Identification, (Step 2) Query Generation, and (Step 3) Diversifying Query

891

We now generate training examples as follows. For each query
document, N different term pairs are extracted, and we label each
,
pair as positive or negative, i.e.,
0,1 . A term pair is
positive if its terms are highly associated and effective for retrieving relevant documents; otherwise, the term pair is negative. To
determine this, we use the following conditions, and an example is
positive if it satisfies every condition; otherwise the example is
negative.

Table 2: Features for Similarity Learning.
Category
Features
PMI of , calculated by 8-word windows
recognized in all documents in a corpus
Topical
PMI of , measured by titles
Relatedness
PMI of , calculated by 8-word windows
identified in query document
Query Clarity (QC) [5]
Retrieval
Query Scope (QS) [9]
Effectiveness
Inverse Document Frequency (IDF)
Inverse Collection Term Frequency (ICTF)

i) Two terms involve high “retrieval effectiveness” if they have
a high generation probability based on the language model estimated for any relevant document.
ii) Two terms are highly “associated” if their PMI estimated from
any relevant document is greater than a threshold.

Suggestions. Given an initial query document, we extract diverse
query aspects by defining a “query aspect” as a set of related
terms from the query document and use term clustering algorithms
to identify n term sets. Once n query aspects (i.e., term clusters)
are identified, we generate multiple queries relevant to the identified aspects, and suggest the top k ranked queries. Our experiments show that diversified suggestions are effective for retrieving more relevant documents in comparison to existing suggestion
methods.

For each relevant document, we generate a unigram language
model and assume that the top 100 terms ranked by the language
model satisfy the first criteria. For the second constraint, we assume that PMI estimated from a relevant document indicates topical association effective for retrieving relevant documents.

2.2 Query Generation
In this step, based on n identified query aspects, we generate queries by exploiting the query generation method proposed in [13].
For each query aspect (i.e., a set of terms), we first retrieve pseudo-relevant documents (PRD) obtained by the terms in the aspect;
we use those terms as a query and assume that top k retrieved
documents are pseudo-relevant. In addition, we generate an equal
number of non-relevant documents (NRD) by randomly selecting
another k documents from those ranked below the top k. Then, we
train binary decision trees using PRD and NRD where the terms in
PRD are used as attributes. Once a decision tree is learned, we
generate a query by extracting attributes (terms) on a single path
from the root to a positive leaf node (i.e., pseudo-relevance). We
define a query as a list of keywords (e.g.,{battery, charger, cellular, phone}), and ignore the attributes associated with negation.
See [13] for more details.

2. FRAMEWORK
2.1 Query Aspect Identification
The first step is identifying n query aspects by representing a query aspect as a set of related terms from the query document. We
address this by using term clustering methods because they can
provide query-specific term analysis results (cf. global term analysis such as LSA [6] may not provide query-specific results). Specifically, for a query document, we extract m distinct terms using
their tfidf weights (stop-words are ignored), and generate
1 ⁄2 term pairs (the similarity is undirected). By estimating
the similarity for each term pair , , we can generate a m-by-m
symmetric similarity matrix whose diagonal value is 1. Then, we
apply a term clustering algorithm using this matrix for generating
n different term sets. In this paper, we extract 500 terms from each
query document, and use a spectral clustering algorithm. Next, we
describe how to estimate the similarity for , .

2.3 Diverse Query Suggestion
We define diversifying query suggestions as suggesting k queries
that will be effective for finding relevant and novel documents for
a query document. To do this, we exploit the xQuAD diversification model proposed in [14] and introduce the following probabilistic query suggestion framework. In this approach, among all
generated queries, we select the queries that are more relevant to
the query document and novel relative to the current suggestion
list. Figure 2 describes this framework.

We define similarity between terms by a mixture of topical relatedness (or association) and retrieval effectiveness when terms are
clustered together. In other words, we make clustering algorithms
group the terms if they are topically associated and are also effective for retrieving relevant documents. To achieve this, we introduce the similarity function.
Sim
where

and

,

1

·

,

·

,

(1)

and a list of generated queries , we
Given a query document
iteratively choose the most probable query obtained by:
1
·P
·P ,
(3)
where is the list of selected queries to be suggested and is a
candidate query from .

is a term pair from a query document.

In Eq. (1),
,
measures topical relatedness between and
, while
,
estimates retrieval effectiveness. is a controlling parameter. For , we utilize term statistics obtained from the
document corpus (e.g., Point-wise Mutual Information (PMI)). To
estimate , we leverage the features from query performance
predictors (e.g., query clarity [5], query scope [9], etc.).

denotes the relevance of to
, while
In Eq. (3), P
indicates the novelty of to . That is, these two
P ,
probabilities are optimizing relevance and diversity, controlled by
.P
can be computed by ∏ PLM
, i.e., the unigram language model estimated from , and P ,
can be
estimated using the identified query aspects.

Using the features listed in Table 2, we can rewrite Eq. (1) as:
∑
Sim ,
·
,
(2)
where indicates a feature defined in Table 2 and
is a weight
of the k-th feature. To predict more accurate similarity, we employ
a supervised learning approach. Given a term pair , , a supervised learner estimates its similarity score by learning an optimal
value of the feature weights (
,…,
).

By the set of query aspects
P
where

892

,

we can marginalize P

∑

is a query aspect in

P
.

·P , |

,

as:
(4)

Table 3: Query Aspect Evaluation. ‘QA’ is our query aspect
identification method (using 10 aspects). A * denotes a significant improvement over ‘BL0’ (the paired t-test with p < 0.05).

In Eq. (4), we consider P
as an importance of an aspect
PLM
.
for , which is estimated by ∏
ALGORITHM Diversifying Query Suggestions (DivQS)

Metric
\ Method
R100
Max. R100
Agg. R100

INPUT: L (a list of generated queries), k (the number of queries to be suggested),
(query document)
OUTPUT: (a list of query suggestions)
PROCESS:
1:
2: While | |
do
3:
1
·P
argmax
·P ,
4:
5:
6: End While
7: Return
Figure 2: A framework of Diversifying Query Suggestions.

(Evaluation Measures) Although there has been considerable
research on measuring diversity for search results (e.g., [4]), these
previous measures are not appropriate for our search environments; [4] only evaluates the retrieval results for a single query
but we suggest multiple queries for a query document and some
multi-query session-based metric is required; in addition, there
was no emphasis on recall in session search results. Thus, to evaluate “diversity” in multi-query sessions, we create the Session
Novelty Recall measure.

P , |
P |
·P |
(5)
|
P
measures the coverage of with respect to
, and
P |
provides a measure of novelty to the current suggestion
list for a given . To estimate these probabilities, we utilize
retrieval results obtained by , , and . Specifically, we assume
that a query’s top 100 retrieved documents can represent underlying topics of the query, and P |
can be estimated by how
much of topics in
are covered by . The equation is given as:

Session Novelty Recall (SNR) is a recall-based metric for multiquery sessions. First, given multiple retrieval results, we ignore
relevant documents already found by previous suggestions. Second, following the idea in [10], we discount the documents retrieved by later suggestions. The computation is given as follows.

(6)

where
is the set of the top 100 documents retrieved by .
Note that we use the terms in a query aspect as a query. For the
estimation of P |
, we further assume that the queries chosen
as suggestions in are independent to each other for , and the
following estimation can be given.
P |
where

P

,

is a query in

,…,
and P

|

∏
|

1

P

|

ACL (academic)
BL0
QA
0.4452
0.4695*
0.6369*

first baseline (BL1) is implemented by the method in [2] which
can suggest relevant n-grams without using query logs. We modify this method to fit in our search environments; we first extract
all n-grams of order 1, 2, 3, 4, and 5 from pseudo-relevant documents obtained by the BL0, rank them by the correlation between
candidate n-grams and the terms in the query document, and suggest the top k ranked n-grams. The other baseline (BL2) is a query
suggestion method proposed in [13]. We generate keyword queries by ignoring the terms associated with negation.

By assuming that the current candidate query is independent of
the queries already selected in , P , |
can be derived as:

P |

PAT (patent)
BL0
QA
0.1091
0.1491*
0.1918*

First, we construct a rank list, L, by concatenating the top 100
documents from each ranked list in a session. Next, in the list, we
discard any retrieved documents which are retrieved by any previous queries, i.e., the rank list contains only distinct retrieval results. In addition, each retrieved result is labeled by the query
which first retrieved it.

(7)
.

Using the above estimations, we select k queries as suggestions
for each query document.

SNR@100

3. EXPERIMENTS
3.1 Experimental Set-up

∑|

|

| |

(8)

is the document placed at the i-th rank in L and retrieved
where
by the j-th suggestion in a session, R is the set of relevant documents, k is # of queries that the user examines where
1,
rel
returns 1 if d is relevant; otherwise, 0. Ideally, if the first
query retrieved every relevant document, the value is maximized.
In addition, we employ normalized Session DCG (nSDCG) [10]
to measure retrieval effectiveness of the top k suggested queries.

We conduct experiments on two domains: the patent and academic domains. For the patent domain, we use the patent corpus provided by [7]. To develop query documents (new patents), we randomly selected 102 more recent patents, and consider patents
cited in each query patent as “relevant”. For the academic domain,
we use the ACL anthology reference corpus [3], and randomly
select 150 more recent query documents (papers). We regard the
articles cited in each query paper as “relevant”. For all query documents, references are hidden, and the sentences containing citations are removed. Queries and documents are stemmed by the
Krovetz stemmer. To identify query aspects and generate diverse
suggestions, we perform 5-fold cross-validation with random
partitioning. For each query suggestion, we use the query likelihood model implemented by Indri [17]. We assume that the
searchers only examine the top 100 of every query result since
100 patents are examined on average [11].

3.2 Results
(Query Aspect Identification Performance) In this experiment,
we hypothesize that more relevant documents are retrieved if the
identified query aspect is effective. We measure the retrieval effectiveness of each query aspect by formulating a query using the
terms in each query aspect. Table 3 shows the retrieval results of
query aspects and baseline. For each query document, 10 query
aspects are identified and a single baseline query is used. We
measure recall (R@100) in two different ways: (1) selecting the
best one among n different query aspects (Max R@100) and (2)
aggregating the retrieved relevant documents (within rank 100) by
all query aspects (Agg. R@100). We report an average value of
each metric over the query documents in each corpus.

(Baselines) For each query document, we generate an initial baseline query (BL0) by the query generation method described in [8],
used for evaluating query aspect identification. To evaluate diverse suggestion results, we employ two different baselines. The

893

First, regarding Max R@100, our method can generate at least
one query aspect which can significantly outperform the baseline.
Second, from Agg. R@100 we see that significantly more relevant

parison to baseline methods. Our method is easily reproducible
and general; we do not require any manually constructed data or
external resources, and effectiveness was verified in two different
domains. For future work, we plan to conduct experiments in the
legal domain (e.g., finding relevant cases).

Table 4: Session evaluation using 5 and 10 suggestions. #Q is #
of suggested queries. In each row, a significant improvement
over each baseline is marked by its number, e.g., 12 denotes
improvement over ‘BL 1&2’ (the paired t-test with p < 0.05).

6. ACKNOWLEDGEMENTS
This work was supported in part by the Center for Intelligent Information Retrieval. Any opinions, findings and conclusions or
recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.

PAT (patent)
Metric

#Q

BL1

BL2

SNR
@100
nSDCG
@100

5
10
5
10

SNR
@100
nSDCG
@100

5
10
5
10

0.1560
0.17151
0.1893
0.1989
0.0812
0.0827
0.0783
0.0959
ACL (academic)
0.5459
0.57311
0.6078
0.63511
0.3273
0.3116
0.33852
0.3099

DivQS
(n = 10)
0.18551
0.232212
0.120912
0.112712

DivQS
(n = 20)
0.196112
0.250912
0.131912
0.121212

0.632912
0.719212
0.420012
0.435712

0.651912
0.739212
0.434712
0.445712

7. REFERENCES
[1] Azzopardi, L., Vanderbauwhede, W., and Joho, H. (2010).
Search system requirements of patent analysts. SIGIR.
[2] Bhatia, S., Majumdar, D., and Mitra P. (2011). Query suggestions in the absence of query logs. SIGIR.
[3] Bird, S., Dale, R., Dorr, B., Gibson, B., Joseph, M., Kan, M.Y., Lee, D., Powley, B., Radev, D., and Tan, Y. F. (2008). The
ACL anthology reference corpus: A reference dataset for bibliographic research in computational linguistics. LREC.

documents are retrieved when using all identified aspects. This is
a useful result because query aspects can find relevant documents
that are missed by BL0 and the query suggestions generated by
these aspects should also perform well.

[4] Clarke, C. L. A., Kolla, M., Cormack, G. V., Vechtomova, O.,
Ashkan, A., Buttcher, S., and MacKinnon, I. (2008). Novelty
and diversity in information retrieval evaluation. SIGIR.
[5] Cronen-Townsend, S., Zhou, Y., and Croft, W. B. (2002).
Predicting query performance. SIGIR.

(Diverse Query Suggestion Performance) We now evaluate
diverse query suggestion results in terms of retrieval effectiveness
and diversity. For each query document, we suggest 5 and 10
queries by identifying 10 or 20 different query aspects in each
query document (i.e., n = 10 or 20). The baselines (BL1&2) generate the same number of query suggestions for the same query
document. Table 4 reports retrieval performance of each method.
First, in both domains, BL2 can outperform BL1 in terms of SNR.
Second, the queries suggested by our method (DivQS) can provide significantly more diversified results and retrieve more relevant documents. SNR verifies that our method is more effective at
finding new relevant documents missed by previous queries (since
SNR ignores the relevant documents retrieved by any previous
queries). Third, considering nSDCG, our method is significantly
better at placing relevant documents at higher ranks. This is because the queries generated by our method contain more discriminative terms from relevant documents.

[6] Dumais, S. T. (2005). Latent semantic analysis. Annual Review
of Information Science and Technology. 38 (1).
[7] Fujii, A., Iwayama, M., and Kando, N. (2007). Overview of
the patent retrieval task at the NTCIR-6 workshop. NTCIR-6.
[8] Ganguly, D., Leveling, J., Magdy, W., and Jones, G. J. F.
(2011). Patent query reduction using pseudo-relevance feedback. CIKM.
[9] He, B., and Ounis, I. (2004). Inferring query performance using pre-retrieval predictors. 18th Symposium on String Processing and Information Retrieval.
[10] Jarvelin, K., Price, S. L., Delcmbre, L. M. L., and Nielsen, M.
L. (2008). Discounted Cumulated Gain based Evaluation of
Multiple-query IR Sessions. ECIR.
[11] Joho, H., Azzopardi, L., and Vanderbauwhede, W. (2010). A
Survey of Patent Users: an analysis of tasks, behavior, search
functionality and system requirement. IIiX.

4. RELATED WORK
In this paper, we are interested in the diversity between querysuggestion pairs, which has been studied in recent work (e.g.,
[15][16]). Song et al. [16] selected query candidates from query
logs by ranking them in the order which maximizes the similarity
and diversity between the queries. Santos et al. [15] used the related queries, from query logs, which contain common clicks or
common sessions for diversifying suggestions. However, these
methods cannot be used for our task because they are based on
proprietary training data (to learn ranking functions) and query
logs (to generate suggestions), which are not available. Instead,
the query suggestion methods proposed in [2][13] are more easily
applied in QDS environments but do not consider diversity.

[12] Jones, R., Rey, B., Madani, O., and Greiner, W. (2006). Generating query substitutions. WWW.
[13] Kim, Y., Seo, J., and Croft, W. B. (2011). Automatic Boolean
Query Suggestion for Professional Search. SIGIR.
[14] Santos, R. L. T., Macdonald, C., and Ounis, I. (2010). Exploiting query reformaulations for web search result diversification.
WWW.
[15] Santos, R. L. T., Macdonald, C., and Ounis, I. (2012). Learning to rank query suggestions for adhoc and diversity search.
Information Retrieval, 16(4).

5. CONCLUSION

[16] Song, Y., Zhou, D., and He, L-w. (2011). Post-Ranking Query
Suggestion by Diversifying Search Results. SIGIR.

In this paper, we proposed a framework for diversifying query
suggestions to help domain-specific searchers. We identify diverse query aspects, generate many queries related to these, and
suggest diverse queries based on the identified aspects. Through
experiments, we showed that the suggestions generated by our
system produce more diverse and effective search results in com-

[17] Strohman, T., Metzler, D., Turtle, H., and Croft, W. B. (2005).
Indri: a language-model based search engine for complex queries (extended version). Technical Report, UMASS CIIR.

894

