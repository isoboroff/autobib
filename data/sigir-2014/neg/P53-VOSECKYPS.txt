Collaborative Personalized Twitter Search with
Topic-Language Models
Jan Vosecky, Kenneth Wai-Ting Leung, Wilfred Ng
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
Clear Water Bay, Kowloon, Hong Kong

{jvosecky,kwtleung,wilfred}@cse.ust.hk

ABSTRACT

results in an information overload for users when searching
microblog data. In particular, tweets cover a wide range of
topics and purposes, which makes a user’s search for relevant content challenging and time-consuming. As a result,
novel methods for search result personalization are needed
in the microblogging domain.
Although much work has been done on personalized Web
search [5, 12, 18, 19, 25] and collaborative Web search [17,
21, 23, 27], little work has been done on personalizing the
search experience in the social environment of Twitter. Recent work related to information retrieval in Twitter does
not consider individual users’ interests in the ranking [7, 8,
14, 16]. Thus, in this paper we develop an eﬀective framework for collaborative personalized Twitter search.
Similarly to personalized Web search, our goal is to rerank a set of search results based on their similarity with the
user’s preferences and thus, improve the retrieval eﬀectiveness. However, the microblogging environment diﬀers from
traditional Web signiﬁcantly, which calls for novel methods
to accurately model user’s preferences. The main challenges
related to personalized information retrieval in microblogs
can be summarized as follows:
• Highly social. Each user can be seen both as a content producer and consumer, and have rich interactions
with other users [11]. Utilizing this social environment
to model user’s preferences is not trivial and requires
a careful selection of relevant social content.
• Diversity of topics and purposes. Content in microblogs
covers very diverse topics and purposes [2, 11]. Failure to distinguish the various types of information may
result in noisy and inaccurate user models.
• Data sparseness. Eﬀective user modeling methods need
to tackle the sparseness of user’s data, such as the short
length and limited amount of user’s tweets, few interactions with other users, or a limited search history.
• Dynamic and real-time. The high volume of microblogs
calls for models that are rapidly updatable, adapt to
the constantly evolving semantics in microblogs and
reﬂect updates in the social network.
In this paper, we address the above challenges and propose
a novel probabilistic framework for Collaborative Personalized Twitter Search (CPTS). In the following paragraphs,
we highlight the main features of our framework.
Collaborative User Modeling. The user’s social connections can provide valuable clues about her preferences.
However, constructing a collaborative user model in not trivial, since not all information from the user’s social environment is equally useful. In fact, the collaborative model may

The vast amount of real-time and social content in microblogs
results in an information overload for users when searching
microblog data. Given the user’s search query, delivering
content that is relevant to her interests is a challenging problem. Traditional methods for personalized Web search are
insuﬃcient in the microblog domain, because of the diversity of topics, sparseness of user data and the highly social
nature. In particular, social interactions between users need
to be considered, in order to accurately model user’s interests, alleviate data sparseness and tackle the cold-start
problem. In this paper, we therefore propose a novel framework for Collaborative Personalized Twitter Search. At
its core, we develop a collaborative user model, which exploits the user’s social connections in order to obtain a comprehensive account of her preferences. We then propose a
novel user model structure to manage the topical diversity in
Twitter and to enable semantic-aware query disambiguation.
Our framework integrates a variety of information about the
user’s preferences in a principled manner. A thorough evaluation is conducted using two personalized Twitter search
query logs, demonstrating a superior ranking performance
of our framework compared with state-of-the-art baselines.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval—Retrieval models

Keywords
Collaborative personalized search; Twitter; topic modeling;
language modeling

1.

INTRODUCTION

In recent years, microblogging services, such as Twitter,
emerged as a popular platform for real-time information exchange. Every day, nearly 60 million short messages (tweets)
are published and over 2 billion search queries are issued in
Twitter1 . However, the vast amount of content in Twitter
1

http://www.statisticbrain.com/twitter-statistics/

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2257-7/14/07 ...$15.00.
http://dx.doi.org/10.1145/2600428.2609584.

53

Our proposed framework is presented in Section 4. Section
5 presents our evaluations. Section 6 concludes our ﬁndings.

become noisy if all information from the user’s “friends” is
included. Therefore, we analyze the user’s social interactions
and estimate the importance of each “friend”. Furthermore,
we analyze the importance of each friend’s topic, in order
to separate potentially relevant topics from irrelevant ones.
The proposed collaborative user model helps to tackle the
sparseness of individual user’s data while avoiding the injection of unnecessary noise from the social neighborhood.
Moreover, this method is suitable to new users who have
posted few tweets, thus addressing the cold-start problem.
Topic-Speciﬁc Language Modeling. Our approach
to user modeling and personalized re-ranking is based on
topics. Content posted by a user may be highly diverse in
terms of topics (e.g., “business”, “sport”, but also “emotional
comments”). Putting all information from a user into a single user model would lead to a noisy and inaccurate model.
Therefore, we distinguish the diﬀerent kinds of information
and propose a novel user model structure, referred to as
topic-specific user language models. The proposed structure
is beneﬁcial in several ways. First, it enables eﬀective query
disambiguation by estimating the latent meaning behind a
user’s query. Second, during personalized re-ranking, we
may identify tweets from relevant topics and promote them
in the ranking. Third, we consider the user’s topical preferences when building the collaborative user model.
Integrated Posting-Search Model. Each microblog
user is both a content producer and consumer. On the one
hand, a user’s tweets indicate her preferences as a content
producer. On the other hand, user’s search activity indicates
her preferences as an information consumer. Our framework
integrates both types of preferences in a principled manner.
Responsive and Dynamic Proﬁles. Our user models
are dynamically updatable, with adjustable weights of each
component. Furthermore, our framework does not require
any explicit input from the users to maintain their proﬁles.
We summarize contributions in this paper as follows:
• We propose a novel framework for Collaborative Personalized Twitter Search (CPTS). The framework integrates user’s preferences, social environment and search
history in a principled manner. The obtained user
models provide comprehensive evidence for query disambiguation and search result re-ranking.
• We develop a novel collaborative user modeling method,
based on the analysis of user’s interactions and topical
preferences. The model is built with detailed parameterization of the inﬂuence of each friend and each topic.
Our evaluation shows that the collaborative model signiﬁcantly improves ranking eﬀectiveness.
• We propose a user model structure, referred to as topicspeciﬁc user language models. The method enables
better organization of user preferences, topic-speciﬁc
analysis of user interactions and semantic-aware reranking of search results. Our evaluation shows that the
proposed user model structure consistently outperforms
state-of-the-art baselines for search personalization.
• We present a comprehensive experimental evaluation
of our framework using two personalized search query
logs and compare the ranking performance against multiple state-of-the-art baselines. Our evaluation shows
that our framework produces signiﬁcant ranking improvements over the baseline methods.
The rest of this paper is organized as follows. We review
related work in Section 2 and preliminaries in Section 3.

2. RELATED WORK
Personalized Web Search. In personalized Web search
[5, 12, 18, 19, 25], the principle of search result re-ranking
is usually applied. Given a set of search results to a user’s
query, we promote search results that have a higher similarity with the user’s preferences (represented as the user
model ), in addition to traditional user-independent metrics
used in the ranking, such as the query-document relevance
and document-speciﬁc features. Building the user model in
mostly relies on implicit data from user’s clicks. However,
only considering the information from single user’s clicks results in the problems of data sparseness and cold-start [27].
Collaborative Web Search. To alleviate the data sparseness problem in personalized Web search, collaborative Web
search techniques were developed [17, 21, 23, 27]. In collaborative Web search, the search preferences of a community
of users are mined and utilized in a similar way to collaborative ﬁltering. Search results are then re-ranked for a given
user based on the pages clicked by other similar users. For
example, CubeSVD [21] analyzes the correlation between
users, queries and documents in a search query log. The extracted click patterns among a community of users are then
employed for personalizing the results of a particular user.
Xue et al. [27] take a language modeling approach to build
user-speciﬁc language models and cluster similar users into
communities. A community-speciﬁc language model is then
used for smoothing the user models to inject community
knowledge. In contrast, our approach exploits the explicit
social neighborhood of a user to learn about her information
need. We measure the importance of each social connection
and construct a topic-sensitive collaborative user model.
Microblog Search. In terms of general information retrieval in Twitter, Massoudi et al. [14] presents a retrieval
model for microblogs, which takes into account tweet-query
relevance, quality features of tweets and incorporates a query
expansion model. Duan et al. [7] use a learning-to-rank approach for general tweet ranking. [8, 15] incorporate temporal aspects of tweets to improve microblog search.
Some attempts were made to construct a user proﬁle from
microblog data for the purpose of recommendation [1, 3, 4].
For example, Chen et al. [4] take a collaborative ranking
approach to tweet recommendation and employs a number
of tweet-speciﬁc features to inﬂuence the importance of a
tweet. However, the existing work does not address the diversity of topics or user’s social connections when constructing the user model. Moreover, the user’s query has not been
considered and thus the methods cannot be readily applied
to microblog search personalization.
To the best of our knowledge, our work is the ﬁrst to
establish a collaborative Twitter-based search personalization framework and present an eﬀective means to integrate
language modeling, topic modeling and social media-speciﬁc
components into a uniﬁed framework.

3. PRELIMINARIES
3.1 Language Modeling IR
Statistical language modeling (LM) has been successfully
applied in machine translation, speech recognition and information retrieval [28]. In information retrieval, LM typically adopts the Query Likelihood model [28]: Given a query

54

Q = {q1 , . . . , qi } and a document D = {w1 , . . . , wj }, the
score for D against Q is proportional to the probability that
the multinomial language model that generated D also generated Q. Formally,
PLM (D|Q) ∝ P (Q|D)P (D).
(1)
This ranking method captures both the document’s relevance to the query and also the document’s prior probability, P (D). The latter may be used to incorporate any
document-speciﬁc features, such as PageRank.
Assuming the unigram model of documents, we can decompose the query into individual words and compute the
overall score as a product of individual term scores,

P (Q|D) =
P (w|D).
(2)

worse results, since the topics are too coarse [26]. Matching a tweet to topic “IT” may not guarantee its relevance to
the user’s preferences. Thus, we develop a novel user model
structure, which enables a ﬁne-grained and topic-aware user
representation.

4. PROPOSED FRAMEWORK
First, we deﬁne the scope of our personalization framework. Given a microblog user u, a search query Q and a
set of N microblog documents returned by a base search engine, our goal is to re-rank the documents using query Q and
a user model Mu , such that documents matching the user’s
interests are ranked at top positions.
To achieve this goal, we need to solve two basic problems:
(1) how to construct the user model Mu , and (2) how to
utilize this model for document ranking. In this section, we
present our personalization framework to meet the above
goals. We note that throughout this paper, a document
refers to a single microblog message (i.e., a tweet).
We begin our discussion with some basic assumptions made
in our framework. In particular, we recognize the importance of analyzing user preferences in microblogs in terms of
topics. Our observation is that microblog content is highly
diverse in terms of topics and purposes. This diversity is
discussed in detail in [11, 20]. For example, Java et al. [11]
found that Twitter serves a wide range of purposes, such
as daily chatter, conversations or news sharing. In this paper, we simply use the concept of topics to broadly refer to
the diﬀerent kinds of content. An example of such topics
would be “pop music”, “IT news”, but also “personal feelings”. We note that even the interests of a single user may
be very diverse. As a result, our intuition is that by treating
all information with the same importance, we would obtain
an inaccurate and noisy personalization model. Therefore,
we propose to distinguish the diﬀerent kinds of information
within our framework.
State-of-the-art topic models such as LDA [9] may be employed for unsupervised topic discovery and for topic assignment of future documents. As the ﬁrst step, we therefore
build a global Topic Model using a large Twitter corpus,
which will be utilized throughout our framework. We will
refer to this model simply as TM.
We now deﬁne some basic operations done using the TM.
To obtain a topic distribution θD of a new document D, we
obtain each dimension i of θD as follows

P (w|φi )

θD,i =  w∈D
.
(6)
w∈D
k P (w|φk )

w∈Q

An important step in the estimation of the conditional
probability P (w|D) is to account for unobserved terms. To
this end, several smoothing methods were proposed. JelinekMercer smoothing [28] is one of the simplest and most popular smoothing methods that uses a ﬁxed smoothing parameter λ to interpolate a document’s language model with a
global (corpus) model. It is deﬁned as
c(w, D)
P (w|D) = (1 − λ)
+ λp(w|C),
(3)
|D|
where c(w, D) is the count of word w in document D and
p(w|C) is the probability of w in the entire corpus.

3.2 Topic Modeling and IR
Topic modeling (TM) has gained popularity in recent years
as a tool to perform unsupervised analysis of text collections
and organize documents by their latent topics. One of the
most popular topic models is Latent Dirichlet Allocation
(LDA) [9]. LDA can be used to discover a set of K latent
topics from a document corpus, and then to represent each
document D as a mixture θD of the latent topics. For each
word wi in D, we ﬁrst sample a topic zi from the document
mixture θD . Second, we sample wi according to topic z’s
word distribution φz .
Much work has been done on developing eﬃcient inference
methods for LDA. Recently, online inference for LDA has
been developed in [9], which enables LDA to be trained on
massive and streaming data. We use the algorithm in [9] to
train LDA on a large Twitter dataset in an online fashion.
Topic modeling has previously been applied for information retrieval [26]. In topic model-driven IR, the probability
of a query given document is
K

PT M (Q|D) =
PT M (Q|φz )P (θD,z ).
(4)
z=1

However, this approach often resulted in decreased ranking
accuracy compared with standard LM, since topics are too
coarse [26]. To alleviate this problem, a linear combination
of TM and document LM is usually employed,
(5)
P (Q|D) = λPT M (Q|D) + (1 − λ)PLM (Q|D).

To assign document D to a single topic, we choose the
topic that maximizes the probability of generating D,

P (w|φk ).
(7)
zD = arg max

In face of the short length and diverse topics of tweets,
neither LM nor TM alone are suitable to build user models for personalized microblog search. On the one hand,
simply employing LM and estimating user-speciﬁc language
models (e.g., in [27]) may easily promote irrelevant tweets
in the ranking. Using such a model, even the match of a
few words of an irrelevant tweet with the user model may
boost its ranking. On the other hand, a pure topic modeling approach to represent user’s preferences may yield even

4.1 Modeling an Individual User

k

w∈D

In contrast to previous approaches, which estimate a single language model for each user (e.g., in [27]), our approach
is to construct a two-layer user model. Each user model is
composed of a topic layer and a word layer. The topic layer
represents user’s high-level preferences and the word layer
represents the user’s words used within the respective topic.
We refer to this model the Individual User Model (IM).

55

Tweets by user U:
Manchester playing tonight
Doing some android coding
Great game, great win for manchester!
Had a great apple cake with chocolate
My java code keeps throwing exceptions

posted
1-Jan
2-Jan
5-Jan
6-Jan
10-Jan

where η is the prior probability of choosing a topic. In our
work, we choose a constant value for η.
IM
Additionally, along with each θu,k,w
, we also store the
timestamp of the latest document in topic k containing w.
This allows to track the recency of information in the IM.
The probability of w in the IM can then be re-deﬁned as

topic
Sport
IT
Sport
Food
IT

IM
IM
θu,k,w
∝ θu,k,w
· e−ρ·tw ,

Individual User Model for U:

T1 “Sport”
w
c(w) update
Manchester: 5
(5-Jan)
Play:
4
(1-Jan)
Win:
2
(5-Jan)

T3 “Food”

T2 “IT”
w
Android:
Coding:
Java:

c(w)
6
2
2

where ρ is the forgetting coeﬃcient. This time decay factor
assumes an exponential forgetting rate and was applied to
IR by Li and Croft [13].

P(T3)=0.2

P(T2)=0.4

P(T1)=0.4

update
(2-Jan)
(2-Jan)
(10-Jan)

w
Cake:
Apple:
Oven:

c(w)
6
5
2

update
(6-Jan)
(6-Jan)
(20-Dec)

4.2 Basic Personalization Model
Based on the individual user model deﬁned above, we formulate a basic personalized ranking function as follows:

Figure 1: Individual User Model
P (D, Q, u) ∝

4.3 Collaborative User Modeling
One of the most important features of microblogs is its social network structure, which enables interactions between
users. Users may follow other users, such as public ﬁgures or
their real-world friends, and are able to receive their tweets.
If a user ﬁnds a tweet interesting, they are able to re-tweet
it or add it to their favorites. Furthermore, users can have
conversations or mention each other in their tweets. This social environment presents rich additional information about
the user’s interests, which can increase the completeness of
the user model and tackle data sparseness of an individual
user. In this work, our main focus is on the followees of a
user (i.e., the users one has subscribed to), which we refer
to as friends for simplicity.
However, we observe that diﬀerent friends may have a
diﬀerent inﬂuence on a particular user u. For example, u
may follow hundreds of friends, but only frequently interacts with a small fraction of them. Furthermore, not all
content posted by a friend may be of interest to u. For example, u may be interested in tweets from friend f about
“IT news”, but may not be interested in f ’s comments about
“relationships”.
We therefore assign a weight to each friend of user u, which
is composed of four factors:
Popularity weight wP (f ): The popularity of user f ,
which may be indicated by f ’s number of followees, number of times f is listed in public lists, PageRank score, etc.
In our work, the log of the followee count is used as an indicator of popularity. The popularity weight is normalized

(9)

(10)

where λ is a parameter for Jelinek-Mercer smoothing.
If we further incorporate the topic-level probabilities of
user u, we get
ˆ = (1 − λ)θIM θIM + λP (w|φT M )η,
IM
θu,k,w
u,k,w u,k
k

(13)

w∈Q

Figure 1 illustrates an IM created from a set of user’s documents.
Before the IM can be used by a ranking function to get
IM
the user’s preference for word w in topic k (i.e., θu,k,w
), we
need to account for the case of unobserved words. To this
end, we smooth the topic-word distribution in the IM using
the underlying topic model,
ˆ = (1 − λ)θIM + λP (w|φT M ),
IM
θu,k,w
u,k,w
k


IM
IM
P (Q|θ̂u,k,w
)P (D|θ̂u,k,w
) P (D),

IM
IM
) and P (D|θ̂u,k,w
) are topic-speciﬁc perwhere P (Q|θ̂u,k,w
sonalized scores of D and Q, respectively, and P (D) is the
document prior.
This approach essentially decomposes the ranking into two
components. First, we perform query disambiguation using
u’s IM. That is, we predict which underlying topic the user
had in mind when formulating the query. Second, the obtained probability given topic k is multiplied with the probability that document D belongs to the respective topic in
u’s IM.
IM
IM
To obtain P (Q|θ̂u,k,w
) (and similarly, P (D|θ̂u,k,w
)), we
compute the product of the scores of each word,

IM
IM
P (Q|θ̂u,k,w
)=
P (w|θ̂u,k,w
).
(14)

where Du is the set of documents by user u, c(w, D) is the
count of word w in D, V is the vocabulary, zD is the topic
IM
of D. We refer to this probability as θu,k,w
for short.
On the topic level, the probability that u chooses topic k
is estimated as
|{D : D ∈ Du ∧ zD = k}|
.
|Du |

K

k=1

The two-layer structure has the advantage of organizing
user preferences related to diﬀerent topics separately. This
in turn enables semantic-aware query disambiguation and
search result re-ranking. For example, Figure 1 illustrates
the IM of user U. U often tweets about IT and mentions the
term “android”. Also, U tweets about food and mentions
the term “apple”. Thus, if U searches for “android”, U’s
IM suggests that U may be interested in IT-related tweets.
Also, if U issues another query related to IT (e.g., “mobile
apps”), tweets mentioning “android” will be ranked high. In
contrast, using a traditional single-layer user model would
also falsely promote tweets mentioning “apple”.
We estimate the IM for each user u in the following way.
First, we assign each microblog document D (i.e., a tweet)
from u to a topic using Equation (7). Second, we build a
language model for each u’s topic using all u’s documents
assigned to the respective topic. On the word level, the maximum likelihood (ML) estimate of the probability of word w
in topic k for user u is deﬁned as

D:D∈Du ∧zD =k c(w, D)
IM

θu,k,w
= 
.
(8)

w ∈V
D:D∈Du ∧zD =k c(w , D)

IM
=
θu,k

(12)

(11)

56

by wP (f ) = log(popularity)/ log(max), where max is the
maximum popularity of a user in Twitter2 .
Aﬃnity wA (u, f ): We measure the similarity of interests
of u and f as the inverse KL-divergence between their topicIM
IM
level proﬁles, wA (u, f ) = 1/KL(θu,◦
||θf,◦
).
Topic-interaction weight wI (u, f, k): We analyze the
interactions between u and f , which include the conversations between u and f , mentions of f by u, and re-tweets of
f ’s tweets by u. We ﬁrst retrieve all tweets containing the
above interactions and assign each tweet to a single topic (for
conversations, we assign the entire conversation to a topic).
The topic-interaction weight wI (u, f, k) is then based on the
count of interactions between u and f that are assigned to
topic k, denoted c(u, f, k). The weight is normalized by
wI (u, f, k) = log10 (1 + c(u, f, k)) if c < ι and wI (u, f, k) = 1
if c ≥ ι. We set ι empirically to ι = 10.
Topic bias wT (u, k): Apart from the above friend-dependent
weights, we also consider u’s bias towards content about
IM
topic k, i.e. wT (u, k) = θu,k
. If f ’s IM contains topic k, we
apply the topic bias as a prior probability of u’s interest.
The overall weight of friend f is then a vector ωu,f , where
each dimension k ∈ {1, . . . , K} is deﬁned as
⎞
⎛
wP (f )
⎜ wI (u, f, k) ⎟
⎟
(15)
ωu,f,k = σ T . ⎜
⎝ wA (u, f ) ⎠ ,
wT (u, k)
where 0 ≥ ωu,f,k ≥ 1 and σ is an optional weight vector to
enable diﬀerent inﬂuence of the weight components.
Finally, all friend weights are normalized such that
f ∈Fu ωu,f,k = 1 for each u and k, where Fu is the set of u’s
friends. The
 total weight of friend f may then be obtained
as ωu,f = k ωu,f,k . If a user has a large number of friends,
we limit the number of friends that are considered for the
collaborative user model by selecting top-n friends based on
the total friend weight.
Creating the Collaborative User Model. After obtaining the weight of each friend of u, we construct the Collaborative User Model (CM). Basically, we take the weighted
average of all the individual user models of u’s friends. The
topic-speciﬁc language model for topic k within in the collaborative user model is estimated as follows

CM
IM
θu,k,w
=
ωu,f,k θf,k,w
.
(16)
f ∈Fu

topic j

TM

TM

CM

w(u,f,k)

IM

user

CM
IM

TM
CM
IM

general users

topic k

Figure 2: Collaborative User Model
tive model and ﬁnally smooth using the topic model. Figure
2 illustrates the collaborative user modeling method.
The collaborative personalized ranking function is then
deﬁned as
K


IM.CM
IM.CM
P (D, Q, u) ∝
P (Q|θ̂u,k,w
)P (D|θ̂u,k,w
) P (D).
k=1

(19)

4.4 Modeling Search Behavior
In addition to analyzing the user’s individual microblog
content and building the collaborative model, we also model
the user’s search activity and construct the Search User
Model (SM). As implicit evidence of the user’s search interests, we mainly consider search queries issued by the user
and the user’s feedback on the search results. In microblogs,
there are several ways a user can provide implicit relevance
feedback. These include re-tweeting (re-sending) or “favoriting” an interesting tweet, or clicking a URL within the tweet.
We refer to these actions as clicks for convenience. Admittedly, a more thorough analysis of the importance of various
click types in user preference modeling is an interesting future work.
Let click(u, Q, D) denote a click by user u on document D
returned to query Q. The set of all clicked documents by u
is denoted Su . For each clicked document D, we ﬁrst assign
D to topic k using Equation (7). Second, we obtain the
following implicit relevance feedback from the user’s click:

SM
: user’s preference for
• Topic-word level feedback θu,k,w
words in topic k. This value is estimated by substituting Su in Equation (8).

where β is a parameter that controls the inﬂuence of CM on
the IM. We adopt Dirichlet prior smoothing, which allows
to smooth sparse individual models more aggressively than
rich individual models. In this method, β is deﬁned as
|Mu |
,
|Mu | + μ

w(u,f,j)

w(u,f,i)

topic i

SM
• Topic level feedback θu,k
: user’s search bias towards
SM
is estimated by substituting
topic k. The value of θu,k
Su in Equation (9).

The collaborative user model can now be integrated with
the individual user model as follows


IM.CM
IM
IM
CM
CM
=(1 − λ) βθu,k,w
θu,k
+ (1 − β)θu,k,w
θu,k
θ̂u,k,w
(17)
+ λP (w|φTk M )η,

β=

friends

SM
: user’s preference for topic
• Query-topic feedback θu,k,Q
k when issuing query Q. We estimate this value as the
maximum likelihood of a click in topic k among all
topics clicked for query Q.

The search user model can now be integrated with IM and
CM by a weighted sum as follows


IM.CM.SM
IM
IM
CM
CM
θu,k,w
= (1 − λ) βθu,k,w
θu,k
+ (1 − β)θu,k,w
θu,k

SM
SM
+ λP (w|φTk M )η,
+ γθu,k,w
θu,k

(18)

where μ is the Dirichlet smoothing parameter. In plain
words, we smooth the individual model using the collabora2
This information can be obtained from, e.g.,
http://twittercounter.com/pages/100

(20)

57

Table 1: Query Log Statistics
No. of queries
Avg. queries per user
No. of retrieved tweets
No. of relevance judgements
Avg. relevant tweets per query

Figure 3: Evaluation user interface, showing results
for query “android”.

IM.CM.SM
IM.CM.SM
P (Q|θu,k,w
)P (D|θu,k,w
) P (D).

k=1

(21)

By using the ranking function in Equation 21, we incorporate all three user models (IM, CM and SM) in a principled
manner. Moreover, our method allows to maintain each user
model separately, which has the advantage of fast updatability, and enables the model parameters (i.e., β, γ, λ) to be
updated at any time.
Notably, our framework is ﬂexible enough to incorporate
additional document-speciﬁc and author-speciﬁc features in
the ranking function, by means of the prior document probability P (D). However, a comprehensive study of document
and author features is not within the scope of this paper.

5.

Log IwS
235
9.42
15,669
4,054
17.94

the query. Our system presents all results to the user in a
random order, in order to avoid any bias. The user may
evaluate the relevance of each result by clicking on a star
icon, as shown in Figure 3. The system stores all submitted
queries, retrieved tweets and the user’s relevance ratings.
Query Log 1: Controlled User Study (Log CoS).
We obtained a search query log with relevance judgements
in a user study involving 11 active Twitter users. The user
study is divided into two days (referred to as Day 1 and Day
2). On Day 1, users are asked to prepare 10 queries about
their topics of interest. The 10 queries are categorized into
four types: recency, topical, entity-oriented and ambiguous.
The ﬁrst three types correspond to common search scenarios
in microblogs, as reported by Teevan et al. [24]. Additionally, we also consider query ambiguity, which serves as an
important motivation in classic personalization research [6].
We note that each query can be classiﬁed under multiple
types. The query types are detailed as follows:

where γ is a parameter to control the inﬂuence of the SM.
Parameter setting is discussed in more detail in Section 5.3.2.
SM
Incorporating query-topic feedback. When θu,k,Q
> 0 for
SM
topic k and a query phrase Q, we may replace θu,k
in Eq. 20
SM
to incorporate the user’s query-topic feedback.
with θu,k,Q
The full ranking function for collaborative personalized
search is deﬁned as

K

P (D, Q, u) ∝

Log CoS
174
15.8
13,712
1,251
7.19

• Recency-oriented. Queries for which relevant tweets
must be very fresh (e.g., a search for the results of
a football match). In contrast, non-recency queries
are those for which relevant tweets don’t need to be
completely new (e.g., “good bar in New York”).
• Topical. Queries related to the user’s long-term interests. For example, an IT professional may issue a query
“java” to search for content related to programming.
• Entity-oriented. These queries aim to ﬁnd information
about speciﬁc named entities, such as people, organizations, products or locations.
• Ambiguous. An ambiguous query may have multiple
meanings. For example, “java” may refer to a programming language or to an island.
Users are asked to choose at least one query of each type
and 10 queries in total. Users are then asked to submit
each query in the evaluation system, review the 50 tweets
returned by the system and mark relevant tweets.
On Day 2 of the study, users are asked to choose a new set
of queries by re-submitting 5 queries from Day 1 and choosing 5 new queries. Similarly to Day 1, users submit each
query in the evaluation system and mark relevant tweets.
We present overall statistics of the obtained query log,
referred to as Log CoS, in Table 1. We further examine the
type of each query, which has been indicated by the users.
Table 2 shows the proportion of queries of each type and
example queries from the log.
To learn about the topical diversity of queries in the dataset,
we manually inspect each query and assign a topical category. We utilize the Yahoo taxonomy6 and classify queries
into the top-level categories. For ambiguous queries, we
choose a category based on which tweets were marked as
‘relevant’ by the user. The distribution of queries by their
category is shown in Table 3.

EXPERIMENTAL EVALUATION

5.1 Datasets
5.1.1 Background Twitter Corpus
To train the global topic model (TM), we obtained a sample of public tweets from Twitter’s Streaming API3 . We
crawled a total of 44.5 million tweets over the course of 6
months in 2013. After ﬁltering non-English language tweets
and removing tweets of less than 20 characters in length, our
dataset contained 11.7 million tweets. This dataset is used
to train the global topic model in Section 5.3.1.

5.1.2 Twitter Search Query Logs
To evaluate the eﬀectiveness of diﬀerent personalization
approaches for Twitter search, a query log with associated
information about the user (incl. user’s tweets and social
connections) is needed. However, such information is not
available in commonly used datasets (e.g., the TREC Microblog Track4 ). Therefore, we developed a web-based Twitter search middleware to collect user’s search queries and
relevance judgements. Users can log in to the system using their Twitter account. Given a search query, the system
connects to Twitter’s Search API5 and retrieves 50 recent
tweets. The results consist of 3 ‘popular’ tweets as determined by Twitter and up to 47 ‘general’ tweets matching
3

https://dev.twitter.com
https://sites.google.com/site/microblogtrack/
5
https://dev.twitter.com/docs/api/1.1/get/search/tweets
4

6

58

http://dir.yahoo.com/

Training
Query Log
Collection

Table 2: Overview of query types from Log CoS
and Log IwS, including the proportion of queries
(“%Qrs”) and example queries of each type.
Log CoS
Query type
Recency
Topical
Entity

%Qrs
44.3%
39.1%
31%

Ambiguous

33.7%

Log IwS
Query type
News

%Qrs
9.8%

Topical
Entity
Ambiguous

49.2%
26.1%
25.6%

• Queries
• Tweet Results
• Relevance Ratings

Example queries
Boeing 787, iWatch, tv shows, Gun control
Humber bridge, Icing sugar, table tennis
Obama, NASA, Santa Fe, Lance Armstrong
hostages, galaxy, apple, giants, ﬂash

Twitter Crawling
• User’s Tweets
• User’s Followees

Example queries
typhoon hk, Kugan case, air crash, xinjiang riot
hong kong food, windsurﬁng, stock market
Google, david beckham, Nike, Syria
Chelsea, Surface, langham, simple plan

%Qrs
22.5%
15.9%
13.0%
10.1%
8.0%
7.2%
7.2%
2.2%
2.2%
11.6%

Log IwS
Category
Business & Economy
Entertainment
News & Media
Regional
Recreation & Sports
Education
Society & Culture
Computer & Internet
Government
Other

Query

Tweet Results

Build CM

Re-ranking Function

IM

CM

SM

Build SM
Training Queries

Ranked Tweets

Parameter
Optimization

Evaluation

Update SM
Relevance Ratings

Figure 4: Evaluation Methodology

focus and can be identiﬁed more objectively. Table 2 shows
the proportion and examples of queries of each type.
Similarly to Log CoS, we analyze the topical diversity of
queries and manually classify queries into topical categories.
The distribution of queries by category is shown in Table 3.

5.2 Evaluation Methodology

Table 3: Overview of queries by query category, including the proportions of queries (“%Qrs”).
Log CoS
Category
Business & Economy
Regional
News & Media
Computer & Internet
Society & Culture
Entertainment
Recreation & Sports
Education
Science
Other

Testing

Build IM

5.2.1 Evaluation Setup

%Qrs
30.5%
15.8%
9.9%
8.9%
8.4%
5.9%
4.9%
3.4%
3.0%
9.4%

Figure 4 shows an overview of the evaluation process.
First, we collect query logs (cf. Section 5.1.2) and crawl
Twitter data for each user. Second, we estimate the individual and collaborative user models described in Section 4.
Using the training query log, we estimate the search user
model (cf. Section 4.4) and tune the global parameters of
our framework. Third, we use the testing query logs (i.e.,
Day 2 of Log CoS and all queries in Log IwS) to measure
ranking performance. The testing process involves all aspects of our framework, which includes dynamic updating
of the search model (SM). For each testing query, we produce a ranking using our framework, evaluate the ranking
using relevance judgements from the query log and update
the SM. This cycle is repeated for each testing query. The
process simulates the behavior of our framework in a real usage scenario, in which a user submits a query, reads through
the results and clicks on (e.g., re-tweet) the relevant ones.

For the purpose of our evaluation, query log data from
Day 1 is treated as the training dataset and data from Day
2 is treated as the testing dataset.
In addition to the query log data, we also crawl the users’
Twitter data. Speciﬁcally, we obtain the latest 200 tweets of
each user and crawl the tweets of the top-20 friends, ranked
by friend weight (cf. Section 4.3).
Query Log 2: In-the-Wild User Study (Log IwS).
To obtain users’ search preferences in an unrestricted setting, we invite 24 users and conduct an open user study
over a 3-month period. Users are invited to use our evaluation system and submit search queries of their choice. As
an approximate guide, we ask users to submit at least 10
queries over the evaluation period. When a query is submitted, the user is asked to read through all tweet results and
provide relevance ratings.
The statistics of the obtained dataset are given in Table 1.
We ﬁnd that users submitted 9.42 queries on average during
the study period, with a standard deviation of 3.15. We also
observe that users identiﬁed 17.94 relevant results per query,
which is higher than 7.19 in the controlled study.
To gain more insight into users’ choice of queries and to
compare against the Log CoS dataset, we empirically analyze the query log. First, we perform a post-hoc assignment
of queries to the four query types utilized for the Log CoS
dataset. However, we note that the importance of query
recency may vary among diﬀerent users, which prevents an
objective decision whether a query was recency-oriented or
not7 . Therefore, we instead focus on identifying “newsrelated” queries, since such queries have a strong recency

5.2.2 Overview of Models and Baselines
We implement the following non-personalized and personalized baseline models:
• Query Likelihood (B-QL). Standard language modeling approach (cf. Eq. 3), used as a baseline nonpersonalized model. Used for ranking tweets in [14].
• Topic Model-based IR (B-TM). Ranking based on
the Topic Model and a background language model. In
this method, results are scored using Eq. (5).
• Personalized Search (B-PS). Personalized ranking
based on a single-layer user language model. This approach is used for personalizing Web search (e.g., [19,
22]). Ranking is determined by the KL-divergence between the personalized query LM θq,u and the document LM θd .
• Collaborative Search (B-CS). This model considers the preferences of a group of users, which is the
basis of collaborative Web search [17]. We implement
the method in Xue et al. [27], which utilizes a LM of
a cluster of users for document ranking.
• Collaborative Personalized Search (B-CPS). We
implement a model for collaborative personalized Web
search by Xue et. al [27]. In this approach, both the
user’s LM and the collaborative LM are integrated.

7

For example, user A may be interested in the latest news about
“Johnny Depp” (recency query), while user B may want to ﬁnd both
old and new stories about the actor’s life (non-recency query).

59

0.280

Table 4: Model parameters
Value
0.2
70
20
0.01

0.525

0.260
0.520

0.250

MAP

0.240

0.515

0.230
0.510

0.220
0.210

Proposed models. We evaluate each component of the proposed Collaborative Personalized Twitter Search framework:
• CPTS-IM. Ranking using the Individual User Model
(cf. Equation 11).
• CPTS-CM. Ranking using the Collaborative User
Model (cf. Equation 17, where β = 0).
• CPTS-SM. Ranking using the Search Model. This
method uses SM only in Equation 21.
• CPTS-All. Full Collaborative Personalized Search
Model (cf. Equation 21).

0.505

(a) Dataset Log_CoS

P
A
T
I
PA
PT
PI
AT
AI
TI
PAT
PAI
PTI
ATI
PATI

Eq.
(11)
(18)
(20)
(12)

P
A
T
I
PA
PT
PI
AT
AI
TI
PAT
PAI
PTI
ATI
PATI

Description
Weight of TM in J-M smoothing
Dirichlet prior for CM
Parameter of Search Model
Parameter for exp. time decay

MAP

Param.
λ
μ
γ
ρ

0.530

0.270

(b) Dataset Log_IwS

Figure 5: Performance of the Collaborative Model
with diﬀerent weight factors: Popularity (P), Aﬃnity (A), Topic-Interaction (I), Topic Bias (T).

5.2.3 Metrics
Ranking performance is evaluated using two standard metrics, namely Normalized Discounted Cumulative Gain (NDCG)
and Mean Average Precision (MAP).

5.3 Model Training
5.3.1 Topic Model
To train our global topic model, we use the Twitter corpus described in Section 5.1.1. We utilize an online inference
algorithm for LDA [9], which is based on stochastic variational inference and allows for processing of massive and
streaming data. It was shown in [10] that LDA trained on
grouped tweet-documents performs better than training on
individual tweet-documents. In our public tweet sample,
it is not practical to group tweets by their authors. Instead, we select all tweets containing one or more hashtags
and group each tweet to their respective hashtags. In this
way, we obtain longer and more semantically-rich “hashtagdocuments” for training. As an additional pre-processing
step, we remove spam-like hashtag-documents8 and hashtagdocuments of short length.

the ranking performance of each CM version in turn, using
the CPTS-CM ranking model. Figure 5 shows the results of
this experiment on both query logs.
When using a single weight to build the CM (i.e., ‘P’,
‘A’, ‘T’, ‘I’ in Fig. 5), the results suggest that popularity
and topic-interaction weights are the most eﬀective. This
suggests that it is beneﬁcial to assign a higher weight to
popular friends, as well as selectively promoting content in
topics and by friends with whom a user often engages. The
aﬃnity weight is eﬀective on the Log CoS dataset, but performs poorly on Log IwS. However, topic bias (‘T’) shows
the weakest performance on both datasets. This suggests
that it is not beneﬁcial to assign an ‘apriori’ weight to all
content in a particular topic produced by user’s friends.
Among all versions of CM, the best performance is achieved
with ‘PAI’ on Log CoS and ‘PA’ on Log IwS. These versions
of CM are therefore chosen when reporting overall ranking
performance in the following sections.

5.5 Results and Discussion
In this section, we evaluate the ranking eﬀectiveness of
individual components of our framework and compare them
with the baseline methods listed in Section 5.2.2. Additionally, we perform a paired student’s t-test to determine if
the diﬀerences between the results of our methods and each
baseline method are statistically signiﬁcant (p-value<0.05).
Table 5 shows the overall ranking accuracy on both query
logs, with indications of statistically signiﬁcant diﬀerences
over baseline methods.
From the results, we observe that standard language modeling (B-QL) is outperformed by each component of our
framework. This result is somewhat expected, given that BQL does not consider individual users or their social neighborhood. The topic model-based retrieval model (B-TM)
shows a superior performance to B-QL, in particular at higher
ranks (e.g., NDCG@5). This suggests that incorporating
the latent semantics for scoring tweets provides an advantage over standard query likelihood. The personalized and
collaborative baseline methods (B-PS, B-CS, B-CPS) fail to
outperform the non-personalized baselines on the Log CoS
dataset. On the Log IwS dataset, they achieve a marginal
improvement at lower ranks (NDCG@10 and beyond). This
shows that simply applying personalization techniques used
in Web search may perform poorly in microblog search. In
particular, we note that the collaborative and personalized
baseline (B-CPS) does not achieve a cumulative improvement over its individual components (B-PS, B-CS). This
further indicates that simply fusing user’s individual preferences with the group’s preferences may harm ranking effectiveness in the microblog domain.

5.3.2 Parameter Setting
We adopt a parameter selection approach commonly used
in probabilistic frameworks (e.g., in [8]). Using the training
dataset from Section 5.1.2, we optimize the global parameters for our framework. We proceed by optimizing one parameter at a time, while keeping all other parameters ﬁxed.
The obtained parameter values are listed in Table 4.

5.4 Analysis of Weight Factors in CM
The collaborative user model constitutes an integral and
non-trivial part of our framework. Intuitively, the criteria
for selecting which content to include in the CM will largely
inﬂuence the CM’s ranking eﬀectiveness. Therefore, we ﬁrst
study the importance of the friend weighting factors (popularity, aﬃnity, topic-interaction and topic bias) proposed
in Section 4.3. We are interested in ﬁnding which factor or
combination of factors yields the best results.
We build 15 versions of the CM for each user, based on
all combinations of the 4 weight factors. We then measure
8

Example of a spam-like hashtag is “#followback”, which is
included in automatically generated tweets sent around the
social network.

60

Table 5: Overall ranking results. Statistical signiﬁcance of each result against the baselines (p-value< 0.05) is
denoted using symbols representing each baseline and listed next to the respective baseline’s name.

CPTS-CM

0.250

CPTS-SM

0.251

CPTS-All

0.292

♦

♦

♦

◦♦


0.268

0.253

0.267
0.272
0.304

0.289
♦

♦

◦♦


0.292
0.295
0.321

0.321
♦

♦

◦♦


♦
0.319 
♦
0.323 
◦♦
0.35 

0.269
0.279
0.297

♦

♦

◦♦


Among the proposed models presented in this paper, the
IM alone outperforms all baseline models. We note that IM
signiﬁcantly outperforms its baseline counterpart (B-PS) by
NDCG@5 on Log CoS. This demonstrates the eﬀectiveness
of our two-level user model structure, which utilizes latent
topics in microblogs to organize user preferences.
A further improvement of MAP is achieved by the collaborative user model (CM). However, we observe that CM
is not as eﬀective as IM at top ranks (e.g., NDCG@5) on
Log CoS. This indicates that IM may be more eﬀective in
promoting relevant tweets to the top positions, if the IM
contains suﬃcient information from the user’s tweets. Regarding the results of CM, we note that user’s own tweets are
not considered within this model. This situation may arise
in practice if a user produces little own content, but follows
others. This may particularly beneﬁt newly registered users.
The search user model (SM) shows the best performance
among the proposed models. However, we note that the SM
is based on the user’s implicit feedback and hence faces the
cold-start problem. Our framework is designed to overcome
the cold-start problem by considering the user’s content and
social connections in the IM and CM, respectively. The overall results show the strength of the IM and CM, even when
no information about the user’s search behavior is available.
Finally, the full proposed framework (CPTS-All) achieves
the best overall ranking performance. On the Log CoS dataset,
the diﬀerence with all baselines except B-TM is statistically
signiﬁcant. On the Log IwS dataset, we conﬁrm statistical
signiﬁcance compared with all baselines. The results demonstrate that all three information sources in our framework
are complementary in improving ranking performance.

5.5.1 Comparison Among Proposed Models
In this section, we further compare the performance of
each model in our framework. Since each model uses a different source of evidence about the user’s preferences, each
model may be eﬀective under diﬀerent circumstances.
First, we focus on the Search Model (SM). On the one
hand, this model is most prone to the cold-start problem
when a user ﬁrst uses the system. On the other hand, the
model can be dynamically updated each time a user submits a query and provides relevance feedback (referred to as
a query-feedback step). We therefore study how the eﬀectiveness of SM evolves with each query-feedback step. For
each user, after the i-th query is processed and relevance
feedback is received, we calculate the average MAP since
the ﬁrst until the i-th query. In Figures 6 (a) and (b), we

61

k=10
0.471
0.479
0.481
0.480
0.479

0.472

0.485

0.485
0.508
0.513

0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05
0

0.496
◦♦

◦♦


0.518
0.522

0.504

◦

◦♦


0.513
0.532
0.536



0.544

◦♦

◦♦


0.560
0.564

0.65

SM
No SM (ɣ=0)

0.517


◦
◦♦


0.527
0.540
0.542


◦

◦
◦♦

◦♦


SM
No SM (ɣ=0)

0.6
0.55
0.5
0.45
0.4

1 2 3 4 5 6 7 8 9 10
No. of processed queries

(a) Impact of SM (Log_CoS)

1 2 3 4 5 6 7 8 9 10
No. of processed queries

(b) Impact of SM (Log_IwS)

0.35

0.6

0.3

0.58

0.25

0.56

0.2

0.54
0.52

0.15
IM
CM
SM

0.1
0.05
0

MAP
0.503
0.507
0.509
0.510
0.510

0.534

MAP

0.257

k=5
0.460
0.470
0.465
0.466
0.464

Dataset Log IwS
NDCG@k
k=20
k=50
0.486
0.520
0.496
0.527
0.496
0.529
0.496
0.529
0.495
0.528

MAP

CPTS-IM

MAP
0.244
0.244
0.229
0.239
0.230

MAP

k=5
0.191
0.236
0.178
0.203
0.182

MAP

Model
B-QL ()
B-TM (◦)
B-PS (♦)
B-CS ()
B-CPS ()

Dataset Log CoS
NDCG@k
k=10
k=20
k=50
0.214
0.236
0.268
0.249
0.275
0.304
0.202
0.230
0.262
0.221
0.253
0.284
0.204
0.232
0.264

1

2 3 4 5 6 7 8 9 10
No. of processed queries

(c) Comparison of models (Log_CoS)

IM
CM
SM

0.5
0.48
0.46

1

2 3 4 5 6 7 8 9 10
No. of processed queries

(d) Comparison of models (Log_IwS)

Figure 6: Average per-user ranking performance after processing i user’s queries.
show the average per-user MAP after each query-feedback
step. We observe that the eﬀectiveness of SM increases with
more queries and relevance feedback from the user.
In the next step, we compare the performance among the
three proposed models. Similarly to the previous experiment, we calculate the average per-user MAP after each
query-feedback step and show the results in Figures 6 (c)
and (d). For both query logs, the results suggest that for
the ﬁrst few queries (ﬁrst 5 queries for Log CoS and ﬁrst 4
queries for Log IwS), CM gives the best results among all
models. However, with more relevance feedback, the performance of SM improves, enabling it to outperform CM.
It is important to note that the previous results are averaged over a number of users and queries, which blurs some
details about each model’s performance. In particular, when
inspecting the ranking eﬀectiveness for a query Q, we may
determine which of the proposed models achieves the best
performance for Q. We therefore measure the “success rate”
of each model, in terms of the number of queries for which
the model achieved the highest MAP. On Log CoS, IM, CM
and SM achieved the highest MAP for 23.5%, 49% and
27.5% of queries, respectively. On Log IwS, IM, CM and
SM achieved the highest MAP for 19.1%, 31.8% and 49.1%
of queries, respectively. From the results, we see that no
single model produces the best performance for all queries.
This again conﬁrms that all three models are complementary

B-QL
B-TM
B-PS
B-CS
B-CPS
CPTS

(a) Dataset Log_CoS

MAP

MAP

0.35
0.30
0.25
0.20
0.15
0.10
0.05
0.00

7. REFERENCES

0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10

B-QL
B-TM
B-PS
B-CS
B-CPS
CPTS

[1] F. Abel, Q. Gao, G. Houben, and K. Tao. Analyzing Temporal
Dynamics in Twitter Proﬁles for Personalized
Recommendations in the Social Web. In WebSci’11, 2011.
[2] Y. Cha, B. Bi, C.-C. Hsieh, and J. Cho. Incorporating
popularity in topic models for social network analysis. In
SIGIR’13, 2013.
[3] J. Chen, R. Nairn, L. Nelson, M. Bernstein, and E. H. Chi.
Short and Tweet: Experiments on Recommending Content
from Information Streams. In CHI’10, 2010.
[4] K. Chen, T. Chen, G. Zheng, O. Jin, E. Yao, and Y. Yu.
Collaborative Personalized Tweet Recommendation. In
SIGIR’12, 2012.
[5] W. B. Croft, S. Cronen-Townsend, and V. Lavrenko. Relevance
Feedback and Personalization: A Language Modeling
Perspective. In DELOS Workshop, 2001.
[6] Z. Dou, R. Song, and J.-R. Wen. A Large-scale Evaluation and
Analysis of Personalized Search Strategies. In WWW’09, 2007.
[7] Y. Duan, L. Jiang, T. Qin, M. Zhou, and H.-Y. Shum. An
Empirical Study on Learning to Rank of Tweets. In
COLING’10, 2010.
[8] M. Efron and G. Golovchinsky. Estimation Methods for
Ranking Recent Information. In SIGIR’11, 2011.
[9] M. D. Hoﬀman, D. M. Blei, C. Wang, and J. Paisley. Stochastic
Variational Inference. Journal of Machine Learning Research,
2013.
[10] L. Hong and B. D. Davison. Empirical Study of Topic Modeling
in Twitter. In SOMA Workshop, 2010.
[11] A. Java, X. Song, T. Finin, and B. Tseng. Why We Twitter:
Understanding Microblogging Usage and Communities. In
WebKDD/SNA-KDD Workshop, 2007.
[12] K. W.-T. Leung, D. L. Lee, and W.-C. Lee. Personalized Web
Search with Location Preferences. In ICDE’10, 2010.
[13] X. Li and W. B. Croft. Time-based Language Models. In
CIKM’03, 2003.
[14] K. Massoudi, M. Tsagkias, M. de Rijke, and W. Weerkamp.
Incorporating query expansion and quality indicators in
searching microblog posts. In ECIR’11, 2011.
[15] T. Miyanishi, S. Kazuhiro, and U. Kuniaki. Improving
pseudo-relevance feedback via tweet selection. In CIKM’13,
2013.
[16] N. Naveed, T. Gottron, J. Kunegis, and A. Alhadi. Searching
microblogs: coping with sparsity and document quality. In
CIKM’11, 2011.
[17] B. Smyth. A Community-Based Approach to Personalizing Web
Search. Computer, 40(8):42–50, Aug. 2007.
[18] W. Song, Y. Zhang, T. Liu, and S. Li. Bridging Topic Modeling
and Personalized Search. In COLING’10, 2010.
[19] D. Sontag, K. Collins-Thompson, P. N. Bennett, R. W. White,
S. Dumais, and B. Billerbeck. Probabilistic Models for
Personalizing Web Search. In WSDM’12, 2012.
[20] B. Sriram, D. Fuhry, E. Demir, H. Ferhatosmanoglu, and
M. Demirbas. Short Text Classiﬁcation in Twitter to Improve
Information Filtering. In SIGIR’10, 2010.
[21] J.-T. Sun, Z. Chen, H. Liu, and Y. Lu. CubeSVD: A Novel
Approach to Personalized Web Search. In WWW’05, 2005.
[22] J. Teevan, S. T. Dumais, and E. Horvitz. Personalizing search
via automated analysis of interests and activities. In SIGIR’05,
2005.
[23] J. Teevan, M. R. Morris, and S. Bush. Discovering and Using
Groups to Improve Personalized Search. In WSDM’09, 2009.
[24] J. Teevan, D. Ramage, and M. R. Morris. #TwitterSearch: A
Comparison of Microblog Search and Web Search. In
WSDM’11, 2011.
[25] H. Wang, X. He, M.-W. Chang, Y. Song, R. W. White, and
W. Chu. Personalized ranking model adaptation for web search.
In SIGIR’13, 2013.
[26] X. Wei and W. B. Croft. LDA-based document models for
ad-hoc retrieval. In SIGIR’06, 2006.
[27] G.-R. Xue, J. Han, Y. Yu, and Q. Yang. User language model
for collaborative personalized search. ACM Transactions on
Information Systems, 27:1–28, Feb. 2009.
[28] C. Zhai. Statistical Language Models for Information Retrieval
A Critical Review. Foundations and Trends in Information

(b) Dataset Log_IwS

Figure 7: Eﬀectiveness for diﬀerent query types.
and contribute to the overall ranking score when integrated
in our framework.

5.5.2 Ranking Performance by Query Types
In this section, we study the eﬀectiveness of our framework
when dealing with diﬀerent types of queries. Intuitively,
diﬀerent types of queries may require personalization to a
diﬀerent extent. In the Web search scenario, it is reported
that personalization may even harm ranking quality for some
query types [6]. We focus on the four query types described
in Section 5.1.2. The proportion of each query type in our
datasets is given in Table 2.
Figure 7 shows the average MAP for each query type.
Among the personalized and collaborative baselines, we observe that B-CS achieves the best performance for all query
types on Log CoS. However for entity-oriented and ambiguous queries, we ﬁnd that the performance of B-CS is not
very stable across our datasets and fails to outperform nonpersonalized baselines in some cases. Moreover on Log IwS,
we do not observe signiﬁcant diﬀerences between the personalized and collaborative baselines. These results indicate
that the existing methods, which originate from Web search,
do not produce satisfactory results for microblog queries.
In contrast, the proposed framework improves the ranking performance for all query types on both datasets. Our
method is eﬀective even in cases when the personalized baselines perform poorly. For entity-oriented queries in Log CoS,
we improve the baseline MAP of 0.194 (B-QL) to 0.245,
while B-CS only achieves 0.169. For ambiguous queries in
Log IwS, the baseline MAP of 0.327 (B-TM) is improved by
our method to 0.341, while B-CS only achieves 0.31.

6.

CONCLUSION

In this paper, we present a novel probabilistic framework
for Collaborative Personalized Twitter Search. The framework integrates a variety of information about the user’s
posting and searching preferences. At the core, we develop a
topic-sensitive collaborative user model, which utilizes users’
social connections to augment the user proﬁle and improve
ranking performance. Furthermore, we propose a new twolayer user model structure, which eﬀectively handles the diversity of microblog users’ preferences. Our experimental
evaluation has demonstrated superior performance against
competitive baselines in a variety of settings.
As relevant issues for future work, we plan to categorize
the query types that arise in microblogs and design querydependent personalization strategies. In another direction,
we plan to incorporate more features into our framework,
such as spatial and temporal dimensions of user preferences.

62

