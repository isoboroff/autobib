Modeling Evolution of a Social Network using Temporal
Graph Kernels
Akash Anil

Niladri Sett

Sanasam Ranbir Singh

Department of CSE
Indian Institute of Technology
Guwahati
Guwahati, Assam-781039,
India

Department of CSE
Indian Institute of Technology
Guwahati
Guwahati, Assam-781039,
India

Department of CSE
Indian Institute of Technology
Guwahati
Guwahati, Assam-781039,
India

a.anil@iitg.ernet.in

niladri@iitg.ernet.in

ABSTRACT

work growth using spectral graph kernels (spectral transformation of the graph kernels). It proposes temporal version
of the spectral graph kernels by incorporating the temporal
effects. Given a sequence of snapshots of an evolutionary
social network graph taken at different instance in time i.e.,
G1 , G2 , G3 , ..., Gm (Gi represents a snapshot at time i), the
task is to predict Gt for some t > m. This paper focuses on
four popular spectral kernels on graphs namely triangle closing [13], exponential [2], path counting [14] and neumann [2],
and makes the following contributions.

Majority of the studies on modeling the evolution of a social
network using spectral graph kernels do not consider temporal effects while estimating the kernel parameters. As a result, such kernels fail to capture structural properties of the
evolution over the time. In this paper, we propose temporal
spectral graph kernels of four popular graph kernels namely
path counting, triangle closing, exponential and neumann.
Their responses in predicting future growth of the network
have been investigated in detail, using two large datasets
namely Facebook and DBLP. It is evident from various experimental setups that the proposed temporal spectral graph
kernels outperform all of their non-temporal counterparts in
predicting future growth of the networks.

1.

ranbir@iitg.ernet.in

• Devise temporal versions of the spectral graph kernels.
• Estimate temporal kernel parameters using regression.
• Compare performance of predicting future growth of a
network between different kernels (proposed temporal
and existing non-temporal) using two large datasets
namely Facebook and DBLP.

INTRODUCTION

Given a non-empty set of objects X , a kernel k : X × X →
R is a measure of similarity between two objects x ∈ X and
y ∈ X satisfying two properties; (i) symmetry i.e., k(x, y) =
k(y, x) and (ii) positive semi definite i.e.,
X
cx cy k(x, y) ≥ 0

2.

SPECTRAL KERNEL OF GRAPH

Spectral graph theory is a popular method to study graph
properties [15, 16], which exploits matrix decomposition of
the adjacency or Laplacian matrix of a graph. Considering
an adjacency matrix A of a graph, eigenvalue decomposition(EVD) can be used to decompose A as follows:

x,y∈X

where cx , cy ∈ R. For a finite set X , a kernel can be uniquely
represented by a matrix K of order |X | × |X | where K[x][y]
relates to k(x, y). For graph kernels, x and y are either nodes
in the graph or the graph itself quantifying how similar two
nodes in a graphs are or how similar two graphs are? [1,
2, 3, 4]. In recent studies, graph kernels (in both forms)
have been applied in various social network analysis problems such as link prediction [4, 5, 6, 7], network evolution [8,
9], community detection [10]. Social networks evolve over
time and solution to above problems can be greatly affected
by temporal parameters of the evolution [11, 12]. However,
majority of the above studies do not consider temporal parameters. This paper revisits the study [8] of modeling net-

A = UΛUT

(1)

where U is an n × r orthogonal matrix such that the ith
column vector corresponds to ith eigenvector of A, and Λ is
a r×r diagonal matrix with eigenvalues as diagonal elements
(set of eigenvalues represents the spectrum of the matrix and
they are arranged in descending order of their values) and r
is the rank of the matrix. Considering equation 1, spectral
transformation of a graph kernel K(A) is defined as
K(A) = K(UΛUT ) = UF (Λ)UT

(2)

for some function F (Λ). Like in [8], this paper also assumes
the eigenvectors U to be time invariant. The function F (Λ)
is defined by a set of real valued function f (λ) where λs are
the eigenvalues of adjacency matrix. The spectral transformation of the four kernels are discussed in Table 1. If λ is an
eigenvalue of a source graph, f (λ) defines the future growth
of the same eigenvalue. The formulation of f (λ) for each
kernel is briefly discussed in the 2nd and 3rd columns of the
table 1 (details of which can be found in [15, 8]).

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00.
Include the http://DOI string/url which is specific for your submission and included in
the ACM rightsreview confirmation email upon completing your ACM form.

1051

Table 1: Different spectral kernels on graph and their spectral transformation
Kernel Function
Spectral Kernel Transformation
LSS error for learning parameters
r
P
k
k T
k
T
k
Path Counting
K(A) = A = UΛ U
K(A) = UF (Λ )U where F (Λ ) is E(e) =
(λvi − (a + bλi + cλ2i ))2
i=1
defined by f (λ) = αλk and α > 0.
P
Triangle Closing
K(A) = A2 = UΛ2 UT
K(A) = UF (Λ2 )UT where F (Λ2 ) E(e) = ri=1 [λvi − (a + bλ2i )]2
is defined by f (λ) = αλ2 and α > 0.
r
P
Exponential kernel K(A) = exp(A) = K(eαA ) = UF (eαΛ )UT where E(e) =
(log λvi − (log a + bλi ))2
P∞ αi i
αΛ
αλ
i=1
αUΛUT
F
(e
)
is
defined
by
f
(λ)
=
e
.
i=0 i! A = e
r
P
1
Neumann kernel
K(A) = (I−αUΛUT )−1 K(A) = [UF (I − αΛ)UT )]−1 where E(e) =
(λvi − (a + b 1−αλ
))2
i
−1
1
i=1
where α > λ1
f (λ) = 1−αλ defines the kernel.
Kernel

3.

PROPOSED METHODS

Table 2: Characteristics
DBLP
Nodes
3,61,032
Edges (s)
13,93,802
Edges (t)
14,57,489
Periods
2001 to 2010
Snapshots 1st five years for first
snapshot and successive years for successive snapshots

Let As and At be the adjacency matrices of the source
graph and target graph respectively. The predicted target
0
0
graph At can be defined using spectral graph kernel as At =
T
UF (Λs )U where U is constant over time.

3.1

Learning kernel parameter

Kernel parameters can be learnt using two different approaches; (i) learn the parameter individually for all eigenvalues or (ii) learn a common parameter for all the eigenvalues that minimizes the average prediction error. This
paper uses the second approach and estimates the parameters using a polynomial based regression. For learning the
kernel parameters, we use a reference dataset. This study
uses a validation graph as a reference dataset to estimate
the parameters, which is a snapshot of the evolving network
taken at a particular time between the source and target
snapshots. Considering As , Av and the At as the adjacency matrices of the source, the validation and the target
networks respectively, the validation network Av satisfy the
following inequality.

pattern of the kernel parameters over time. It clearly shows
that parameters follows a pattern which can be defined suitably using a polynomial curve fitting. Let {a1 , a2 , a3 , ..} be
the values of parameter a at different instance in time, the
value of a at some future time t i.e., a(t) is defined using
a polynomial a(t) = z0 + z1 t + z2 t2 where z0 , z1 and z2
are the coefficients of the polynomial. We repeat the same
procedure for all the other kernel parameters. Considering
the temporal dimension into account, we finally define the
temporal graph kernel as follows:

As < Av < At
A matrix X < Y iff X[i][j] ≤ Y [i][j]. For learning parameters, we define an absolute square error using reference
dataset as follows:
0

2

T 2

2

f (λi , t) = a(t) + b(t)λi + c(t)λ2i

(5)

where a(t), b(t) and c(t) define the estimate of the temporal parameters. Depending on the type of underlying kernel, equation 5 is customized accordingly. Temporal triangle
closing kernel is formally formulated as f (λ, t) = a(t)+b(t)λ2
where a(t) and b(t) are estimated using polynomial curve fitting with degree two 1 . Similarly, temporal path counting
kernel is formulated as f (λ, t) = a(t) + b(t)λ + c(t)λ2 , temporal exponential kernel as f (λ, t) = a(t)eb(t)λ and temporal
1
. We have deneumann kernel as f (λ, t) = a(t) + b(t) 1−αλ
termined the value of α in temporal neumann kernel using
gradient descent (the α that returns the local minimal error).

T

e = (Av −Av ) = [U(Λv −F(Λ))U ] = U(Λv −F(Λ)) U
(3)
Since U is assumed to be time invariant, the square error
sum is defined as follows:
r
X
E(e) =
[λvi − f (λi )]2
(4)
i=1
th

where λi is the i eigenvalue and r is the rank of the matrix As and λvi is the ith eigenvalue of the matrix Av . Now
the task is to find the kernel’s parameters that minimize the
above square error sum E(e) using regression. Table 1 (4th
column) shows the criteria error functions of applying regression for all the kernels. The parameters a, b and c are
learnt using the reference dataset Av and respective regression function f (.).

3.2

of datasets.
Facebook
63,729
14,09,084
15,19,684
5/9/2006 - 21/1/2009
Till 21/9/2007 as 1st
snapshot and successive months for successive snapshots

4.

EXPERIMENTAL ANALYSIS

All the experimental results reported in this section use
two large datasets namely DBLP co-authorship [17] and
Facebook friendship [18] undirected unweighted networks.
The characteristics of the datasets are shown in Table 2.
Performance of different kernels are evaluated using the fol-

Temporal Kernels on Graphs

If we consider a sequence of snapshots taken at different
time stamps, we can estimate a sequence of kernel parameters defining the evolution of the network between subsequent snapshots. The plots in figure 1 show the changing

1
We observe that experiments return minimum error with
degree two.

1052

Exponential Intercept Evolution
5

Exponential Slope Evolution
0.02

FB
DBLP

0.018

4.5

Neumann Intercept Evolution
40000

FB
DBLP

FB
DBLP

20000
0

0.016

-20000
0.014
4

-40000

0.012

3.5

-60000

0.01

-80000

0.008

-100000
-120000

0.006
3

-140000
1

2

3

4
Snapshots

5

6

7

(a) Exponent intercept

1

2

3

4
Snapshots

5

(b) Exponent slope

6

7

1

2

3

4
Snapshots

5

6

7

(c)Neumann Intercept

Figure 1: Evolution of kernel parameters with the evolution of the network over time. Only three plots are
shown in the figure. All other kernels also follow a regular pattern fitting polynomial curve.
Table 3: Absolute average prediction error for fixed source and target networks and variable validation. TC:
triangle closing, Exp:Exponential, Neu:Neumann and PCE-2: Path Counting kernel with path length 2.
Non-temporal
Temporal
Dataset As → Av → At
TC
Exp
Neu
PCE-2
TC
Exp
Neu
PCE-2
1→2→9
41.45 41.37 41.45 41.45
NA
NA
NA
NA
1→3→9
39.31 39.23 39.31 39.31
26.49(+33%)
24.52(+37%) 23.412(+40%) 23.39(+40%)
1→4→9
37.00 36.93 37.00 37.00
22.68(+39%)
24.09(+35%)
12.53(+66%)
13.09(+65%)
Facebook
1→5→9
28.28 28.20 28.28 28.28
20.07(+29%)
19.83(+30%)
15.25(+46%)
20.52(+27%)
1→6→9
20.91 20.83 20.91 20.91
14.38(+31%)
14.09(+32%)
12.28(+41%)
12.18(+42%)
1→7→9
12.97 12.96 12.97 12.97
8.48(+35%)
8.95(+30%)
6.43(+50%)
6.55(+50%)
1→8→9
6.46
6.82
4.86
4.85
5.82(+10%)
5.84(+14%)
2.97(+39%)
2.61(+46%)
1→2→6
3.98
4.90
3.50
3.50
NA
NA
NA
NA
1→3→6
3.62
4.37
2.93
2.94
3.27(+10%)
3.73(+15%)
1.83(+38%)
1.76(+40%)
DBLP
1→4→6
3.16
3.50
1.99
1.99
3.01(+4%)
3.24(+7%)
1.53(+23%)
1.53(+23%)
1→5→6
2.81
2.87
1.23
1.24
2.80(+0.32%) 2.87(+0.31%) 1.22(+0.73%)
1.23(+1%)

lowing absolute error.
SE =

1
r

r
X

error decreases as the source graph moves closer to target
for almost all cases. From the above two experiments, we
can infer the following two points. Graph kernels predict the
evolution of the network well, if (i) partial graph captures
significantly large amount of target information (validation
graph is close to the target graph) and (ii) source graph is
close to the target graph.
For non-temporal graph kernels, the above two points lead
us to the following issues.

|λti − f (λsi )|

i=1

are the actual ith eigenvalues of the source
and
where
and target network graphs and r is the rank of the matrix.
In this paper, performance of non-temporal and temporal
graph kernels have been investigated using two experimental setups. In the first setup, the source and target graphs
are fixed and validation graph changes. It allows us to investigate the effect of validation graph in modeling the evolution of the graph. Table 3 compares the performance between different kernels. For all kernels (both temporal and
non-temporal), performance increases (error decreases) as
we move the validation graph closer to the target graph. It
indicates that validation graphs closer to source graph are
underspecified and do not capture much information of the
target graph. This observation is true for both Facebook
and DBLP networks and also for both temporal and nontemporal kernels.
In the second setup, validation and target graphs are fixed
and source graph changes. It allows us to investigate the effect of time gap between source and target graph in modeling the evolution. Since we get the best performance when
validation set is closest to the target (in first setup), we
therefore fix the validation graph to be closest snapshot (8th
for Facebook and 5th for DBLP). Table 4 compares the performance between different kernels. Since validation graph
is close to target, variation in prediction error is small. Still
λsi

λti

• Availability of the validation graph close to target graph
is not feasible in practice.
• One may often like to predict the growth of the network with longer time gap.
• Growth of a network after a longer time gap may be
modeled as a sequence of short gap growth prediction
in incremental fashion. For example, from As , predict
As+δ ; from As+δ , predict As+2δ and so on. However,
this approach will be computational intensive. Further, it may also have the effect of accumulated error
from successive prediction.
• Since we do not use the intermediate snapshots between source and validation graphs, the pattern of underlying structural growth of the kernel is lost.
The above problems can be addressed using the proposed
temporal spectral graph kernels. For temporal graph ker-

1053

Table 4: Absolute average prediction error for fixed validation and target networks and variable source. TC:
triangle closing, Exp:Exponential, Neu:Neumann and PCE-2: Path Counting kernel with path length 2.
Non-temporal
Temporal
Dataset As → Av → At
TC Exp Neu PCE-2
TC
Exp
Neu
PCE-2
1→8→9
6.46 6.82 4.86
4.85
5.82(+10%)
5.84(+14%)
2.97(+39%)
2.61(+46%)
2→8→9
6.46 6.82 4.90
4.89
5.71(+12%)
5.74(+16%)
2.91(+40%)
2.73(+44%)
3→8→9
6.48 6.84 4.91
4.90
6.00(+7%)
6.17(+10%)
3.48(+29%)
4.15(+15%)
Facebook
4→8→9
6.59 7.01 4.94
4.89
6.64(-0.62%)
7.09(-1%)
5.05(-2.2%)
2.76(+43%)
5→8→9
6.27 6.61 4.86
4.86
6.25(+0.2%)
6.58(+0.4%) 4.86(+0.04%)
3.74(+23%)
6→8→9
6.00 6.30 4.77
4.77
6:00
6.30
4.77
4.77
7→8→9
5.79 6.11 4.67
4.67
NA
NA
NA
NA
1→5→6
2.81 2.87 1.24
1.24
2.80(+0.3%)
2.86(+0.3%)
1.22(+1.5%)
1.23(+1%)
2→5→6
2.82 2.93 1.07
1.05
2.82(+0.14%) 2.91(+0.5%)
1.03(+3.5%)
1.04(+0.47%)
DBLP
3→5→6
2.78 2.86 0.94
0.94
2.78
2.86
0.94
0.94
4→5→6
2.78 2.93 0.94
0.95
NA
NA
NA
NA

nels, the absolute spectral error is modified as follows:
SE =

1
r

r
X

[6] B. M. Sarwar, G. Karypis, J. A. Konstan, and J. T.
Riedl. Application of dimensionality reduction in
recommender system – a case study. In IN ACM
WEBKDD WORKSHOP, 2000.
[7] A. K. Menon and C. Elkan. Link prediction via matrix
factorization. In Proc. of the 2011 European
conference on Machine learning and knowledge
discovery in databases - Volume Part II, ECML
PKDD’11, pages 437–452, Berlin, Heidelberg, 2011.
[8] J. Kunegis, D. Fay, and C. Bauckhage. Network
growth and the spectral evolution model. In Proc. of
the 19th ACM international conference on
Information and knowledge management, CIKM ’10,
pages 739–748, New York, NY, USA, 2010. ACM.
[9] T. Thorne and M. P. H. Stumpf. Graph spectral
analysis of protein interaction network evolution.
Journal of The Royal Society Interface, May 2012.
[10] S. Sarkar and A. Dong. Community detection in
graphs using singular value decomposition. Phys. Rev.
E, 83:046114, Apr 2011.
[11] S. G. Matus Medo, Giulio Cimini. Temporal effects in
the growth of networks. Physical Review Letters, 2011.
[12] D. M. Dunlavy, T. G. Kolda, and E. Acar. Temporal
link prediction using matrix and tensor factorizations.
ACM Trans. Knowl. Discov. Data, 5(2):10:1–10:27,
Feb. 2011.
[13] J. Leskovec, L. Backstrom, R. Kumar, and
A. Tomkins. Microscopic evolution of social networks.
In Proc. of the 14th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining,
KDD ’08, pages 462–470, New York, NY, USA, 2008.
ACM.
[14] L. Lü, C. Jin, and T. Zhou. Similarity index based on
local paths for link prediction of complex networks.
Physical Review E, 80(4):046122, 2009.
[15] F. Chung. Spectral Graph Theory. American Math.
Society, 1997.
[16] P. R. Dragos Cvetkovic and S. Simic. Eigenspaces of
Graphs. Cambridge University Press, 1997.
[17] Dblp dataset, 2012. http://dblp.uni-trier.de/.
[18] A. Mislove, M. Cha, B. Viswanath, and
K. P.Gummadi. On the evolution of user interaction in
facebook. In Proc. of the 2nd ACM workshop on
Online social networks,Aug.2009

|λi − f (λ, t)|

i=1

From the Tables 3 and 4, it is evident that temporal kernels outperform almost all of its non-temporal counterparts.
With proposed model, an improvement of upto 66% over
Facebook dataset and upto 40% over DBLP dataset can
be achieved. Temporal kernels also predict future evolution
better (referring to the experimental cases where validation
is closer to source and farther from target in Table 3). Further among all the kernels, path counting spectral graph
kernel outperforms all other kernels for both temporal and
non-temporal over both the Facebook and DBLP datasets.

5.

CONCLUSIONS

This paper proposes temporal graph kernels for four spectral graph kernels namely path counting, triangle closing,
exponential kernel,neumann kernel and compares their performances using two large datasets namely Facebook and
DBLP. From various experimental setups, it can be inferred
that non-temporal kernels fail to capture structural growth
of the network. In almost all the experimental cases, temporal kernels outperform their non-temporal counterparts.

6.

REFERENCES

[1] S. V. N. Vishwanathan, N. N. Schraudolph,
R. Kondor, and K. M. Borgwardt. Graph kernels. J.
Mach. Learn. Res., 11:1201–1242, Aug. 2010.
[2] R. I. Kondor and J. D. Lafferty. Diffusion kernels on
graphs and other discrete input spaces. In Proc. of the
Nineteenth International Conference on Machine
Learning, ICML ’02, pages 315–322, San Francisco,
CA, USA, 2002.
[3] A. Smola and R. Kondor. Kernels and regularization
on graphs. In Proc. Conf. on Learning Theory and
Kernel Machines, pages 144–158, 2003.
[4] F. Fouss, L. Yen, A. Pirotte, and M. Saerens. An
experimental investigation of graph kernels on a
collaborative recommendation task. In Proc. Int.
Conf. on Data Mining, pages 863–868, 2006.
[5] D. Liben-Nowell and J. Kleinberg. The link-prediction
problem for social networks. J. Am. Soc. Inf. Sci.
Technol., 58(7):1019–1031, May 2007.

1054

