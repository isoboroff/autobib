The Effect of Sampling Strategy on Inferred Measures
Ellen M. Voorhees
National Institute of Standards and Technology

ellen.voorhees@nist.gov

ABSTRACT

than would be required to reliably compute the traditional
scores directly [4, 5]. This framework has proved popular
because it is convenient to use in practice and supports a
variety of evaluation measures.
Empirical evaluation of the inferred measures shows the
estimates can have high fidelity to their directly-computed
counterparts, but the quality of the estimates depends on the
sampling strategy used to select which documents to judge.
Despite the importance of the sampling strategy on estimate
quality, there is little guidance in the literature on how best
to sample to obtain good estimates. This paper thus examines the question of how the sampling strategy used in
creating judgment sets affects the quality of the resulting
test collection when using extended inferred measures.

Using the inferred measures framework is a popular choice
for constructing test collections when the target document
set is too large for pooling to be a viable option. Within the
framework, different amounts of assessing effort is placed
on different regions of the ranked lists as defined by a sampling strategy. The sampling strategy is critically important
to the quality of the resultant collection, but there is little
published guidance as to the important factors. This paper
addresses this gap by examining the effect on collection quality of different sampling strategies within the inferred measures framework. The quality of a collection is measured by
how accurately it distinguishes the set of significantly different system pairs. Top-K pooling is competitive, though not
the best strategy because it cannot distinguish topics with
large relevant set sizes. Incorporating a deep, very sparsely
sampled stratum is a poor choice. Strategies that include
a top-10 pool create better collections than those that do
not, as well as allow Precision(10) scores to be directly computed.

2.

Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems
and Software—Performance Evaluation

Keywords
Test collections; Incomplete judgments; Sampling

1.

METHODOLOGY

A test collection is a triple containing a document set, a
set of topics, and a set of relevance judgments which we will
call a qrels. A run is the output of a retrieval system for
each of the topics in the collection. This output is assumed
to be a list of documents ranked by decreasing similarity to
the topic. The quality of a run is evaluated using the mean
over all topics in the collection of some metric that computes
a per-topic score based on the ranks at which relevant documents are retrieved. This work considers three measures,
mean average precision (MAP), normalized discounted cumulative gain (NDCG), and precision at 10 documents retrieved (P(10)) and their inferred counterparts.
Ideally, qrels would be complete meaning that all documents are judged for all topics. Complete judgments are
infeasible for all but the tiniest of document sets, however, so
instead some subset of documents is judged. In pooling [2],
the top K results from each of a set of runs are combined
to form the pool and only those documents in the pool are
judged. Runs are subsequently evaluated by assuming that
all unpooled (and hence unjudged) documents are not relevant. The inferred measures framework uses stratified sampling strategies and estimates the values of the measures
based on the judgments obtained on sampled documents.
Both pooling and the inferred measures framework construct a judgment set based on a set of runs. The framework focuses different amounts of assessing effort on different regions of the ranked lists (the strata) by assigning a
sampling rate to each stratum. The strata are defined by
disjoint sets of contiguous ranks. A document belongs to
the stratum defined by the smallest rank at which the document was retrieved across the set of runs. The sampling
rate is the probability of selecting a document from the stra-

INTRODUCTION

Test collections drive much of the research in information
retrieval. Obtaining human judgments regarding document
relevance is the most expensive direct cost of building a test
collection, so recent research has focused on developing evaluation mechanisms to support fair comparison of retrieval
results using relatively small amounts of judging effort. One
such mechanism is the judgment-set-building process that
supports the computation of extended inferred estimates of
traditional evaluation measures using many fewer judgments

This paper is authored by an employee(s) of the United States Government and is in the
public domain. Non-exclusive copying or redistribution is allowed, provided that the
article citation is given and the authors and agency are clearly identified as its source.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
2014 ACM 978-1-4503-2257-7/14/07
http://dx.doi.org/10.1145/2600428.2609524 .

1119

tum to be judged. The inferred measures use the knowledge
of a stratum’s size, sampling rate, and number of relevant
documents found from among the judged set to estimate
the total number of relevant documents in the stratum, and
combines the different strata’s estimates to compute a final
global estimate of the number of relevant documents for a
topic. Similar local estimates are made for the number of
relevant documents retrieved by an individual run based on
the number of judged documents retrieved by that run in
each of the strata.
A sampling strategy is the combination of strata definition
and sampling rate per stratum. Different sampling strategies
applied to the same run set create different test collections
since the judged set of documents depends on the sampling
strategy. The quality of a sampling strategy is thus measured by the quality of the test collections it induces. To
measure the quality of a test collection, we use the accuracy
of determining the set of significantly different run pairs as
compared to a gold standard set of runs pairs (similar to the
approach used by Bompada, et al. [1]).
In general, the larger the assessment budget (i.e., maximum number of human judgments that can be obtained),
the better a test collection will be. Thus it is important
to control for the total number of documents judged when
comparing sampling strategies. In practice, this means that
stratum size and sampling rate must be traded off against
one another: small strata can have large sampling rates but
large strata must be sampled more sparsely.
We consider the following strategies in this work.
pool: A single, exhaustively judged (thus shallow) stratum. The same documents as in pooling are judged,
but subsequent evaluation uses the inferred measures.
1stratum: A single stratum drawn to a moderate depth
and using a moderate sampling rate. In this study, the
1stratum strategy uses a depth of 100 and a sampling
rate of 30%.
2strata: An exhaustively judged small initial stratum plus
a moderate depth and sampling rate second stratum.
This strategy is motivated by the fact that it allows
P(depth) to be computed exactly, and it concentrates
the most assessment effort at ranks that have a large effect on the measures’ scores. In this study, the 2strata
strategy consists of all documents in ranks 1–10 plus a
10% sample of documents in ranks 11–100.
3strata: An exhaustively judged small initial stratum, a
moderate depth, moderate sampling rate second stratum, and a deep, very sparsely sampled final stratum.
The addition of a large, sparsely sampled stratum is
motivated by the hope that it will provide a better
estimate of the total number of relevant documents.
In this study, the 3strata strategy consists of all documents from ranks 1–10 in union with a 10% sample
of documents from ranks 11–20 and a 1% sample of
documents from ranks 21–1000.
To examine the quality of a sampling strategy, we use an
existing test collection and create a set of sampled collections
containing a subset of the original collection’s judgments.
We then compare the average behavior of the sampled collections to that of the original collection, which we treat as
gold standard behavior. We use two existing TREC collections as the base collection in this work: the collection built
in the the TREC-8 ad hoc retrieval task and the collection
built in the TREC 2012 Medical Records track.

The TREC-8 ad hoc track collection contains about
528,000 mostly newswire documents and 50 topics. Binary relevance judgments were created through pooling with
K = 100. This collection was chosen for this study because
it is a high quality test collection that has been used in many
similar studies; evaluation scores computed on this collection
are as close to Truth as we are likely to ever get. We use
traditional evaluation measures computed over the official
qrels as the gold standard for this collection.
The original motivation for this study was unexpectedly
noisy scores that resulted from using inferred measures in the
TREC 2011 Medical Records track [3]. The 2012 Medical
Records track collection is similar to the 2011 collection in
that it uses the same document set and similar topics, but
it contains a better set of relevance judgments to use as
ground truth. The document set is approximately 17,000
medical visit reports and there are 47 topics. Judgment sets
were created using a two strata strategy. The first stratum
contains all documents retrieved between ranks 1–15 from
88 runs, and the second stratum is drawn from ranks 16–
100 with a sampling rate of 25%. Documents were judged
on a three-way scale of relevant, partially relevant and not
relevant. For this collection, the gold standard scores are
the inferred scores computed using the entire qrels created
in the track. The computation of infNDCG uses a gain value
of 1 for partially relevant documents and 2 for fully relevant
documents.
Call a run that contributed to the judgment sets for the
base collection a judged run1 . To test a sampling strategy,
we randomly split the set of judged runs in half, and apply
the sampling strategy to only the runs in one half. Once
a judgment set is produced, we evaluate all runs submitted
to the original TREC track using it. The entire process is
repeated 50 times, each time called a trial, with each trial
using a different random split of judged runs. All sampling
strategies use precisely the same split of judged runs in each
trial, but necessarily produce different judgment sets from
that split.

3.

EXPERIMENTAL RESULTS

Using the set of sampled judgment sets, we can compare
the sampling strategies along three dimensions: the number
of judged documents required, the accuracy of determining
significantly different run pairs, and the accuracy of estimates of the total number of relevant documents, R.

3.1

Judgment set size

As mentioned above, comparisons of sampling strategies
must control for the total number of judgments a strategy
requires, called the judgment set size. The mean judgment
size over the 50 trials for each sampling strategy is given in
Table 1.
Note that it is possible that a document selected to be
judged might not actually have a judgment in the base collection’s qrels. When this happens, we ignore the selection
(i.e., we mark it as not selected). To account for this effect,
the sampling rate actually used in constructing the sample
judgment sets was greater than the target rate such that
the number of judgments obtained approximated the number the target rate would produce (while always sampling
1

While 129 runs were submitted to the TREC-8 ad hoc task, only
71 runs were judged. All submitted runs were judged for the TREC
2013 Medical Records collection.

1120

Table 1: Mean judgment set size per strategy
TREC-8
TREC 2012
Mean
Mean
ID
Size
ID
Size
pool15
179.7 pool15
197.9
pool25
296.0
1stratum 323.2 1stratum 255.9
2strata
221.6 2strata
227.8
3strata
199.4

Table 2: Accuracy of reproducing the set of significantly different run pairs
pool15
pool25
1stratum
2strata
3strata

from the appropriate stratum). The rates given in the definitions of the strata are the target rates, which are also
the effective sampling rates. The strata sizes and sampling
rates provide only coarse control of the resulting judgment
set size, however, and there are sampling strategies the base
collection cannot support because there are too many missing judgments in its qrels. The TREC 2012 collection cannot
support a pooled strategy with a depth greater than 15, nor
does it support the exploration of a 3strata strategy. We
use a pool depth of both 15 and 25 for the TREC-8 collection since depth 25 is a better match for the other methods’
judgment set sizes on that collection while depth 15 is used
for the TREC 2012 collection.
The lack of fine control means there is a wider spread in
the range of judgment set sizes across the strategies than is
ideal. However, the 1stratum strategy is the strategy with
the largest judgment set size, and as will be seen below, it
is not a very good strategy even with the larger number of
judgments.

3.2

pool15
1stratum
2strata

a) TREC-8
infAP
infNDCG
0.906
0.922
[0.855, 0.926]
[0.873, 0.945]
0.929
0.947
[0.889, 0.943]
[0.901, 0.966]
0.878
0.884
[0.839, 0.911]
[0.836, 0.918]
0.918
0.924
[0.891, 0.939]
[0.879, 0.943]
0.834
0.912
[0.786, 0.874]
[0.880, 0.934]
b) TREC 2012
infAP
infNDCG
0.888
0.883
[0.862, 0.911]
[0.855, 0.907]
0.930
0.820
[0.903, 0.948]
[0.783, 0.858]
0.935
0.904
[0.901, 0.953]
[0.880, 0.926]

infP10
0.959
[0.920, 0.975]
0.978
[0.950, 0.987]
0.868
[0.834, 0.904]
0.953
[0.932, 0.962]
0.926
[0.912, 0.937]
infP10
0.917
[0.885, 0.956]
0.895
[0.853, 0.924]
0.967
[0.953, 0.978]

the quality of the top of the ranked list—though MAP less
so than P(10) and NDCG—unjudged documents in these
ranks increase the variability of the final estimated score
much more than unjudged documents later in the ranked list
do. Thus it is beneficial to concentrate more of the judging
resources at the very top of the document ranked lists.
The 3strata strategy is the strategy used for the TREC
2011 Medical Records collection whose noisy score estimates
prompted this study. Its behavior on the TREC-8 collection
confirms that it is a poor choice of sampling strategy. The
problem is that in this strategy the stratum for which the
least information is known receives the greatest emphasis.
The computation of the inferred measures uses as an estimate of a run’s number of relevant retrieved in a particular
stratum, S, a smoothed fraction of the number of judged relevant documents in S retrieved by the run over the number
of judged documents in S retrieved by the run. When a run
retrieves few (or none) of the documents that were selected
to be judged in the stratum but relatively many documents
from the stratum, the estimate can be far afield of the true
value. For example, if a run retrieves only one document
in S that was judged, and that document is relevant, the
estimated number of relevant retrieved is .99998 times the
number of documents in S that are retrieved by the run.
Any strategy that includes a large stratum that is sampled
very sparsely will be affected by this behavior.
The relative quality of the pool and 2strata strategies is
more complex. The 2strata strategy is more accurate for
the TREC 2012 collection. The pooling strategy is generally
more accurate for the TREC-8 collection, but there is a clear
dependency on the pool depth. The next section will show
that the 2strata strategy produces better estimates of the
number of relevant documents for both collections.

Collection quality

We use the accuracy of detecting the set of significantly
different run pairs as the measure of a sampled collection’s
quality. For a given measure, we first compute the set of
significantly different run pairs in the base collection using
a paired t-test with α = 0.05. Using the same test for a
sampled collection, we calculate the number of pairs in each
of the following five categories:
true positive: the run pair is significantly different in both
collections and the collections agree as to which system
is better;
true negative: the run pair is not significantly different in
either collection;
miss: the run pair is significantly different in the base collection, but not the sampled collection;
false alarm: the run pair is not significantly different in
the base collection, but is in the sampled collection;
inversion: the run pair is significantly different in both collections, but the collections disagree as to which is the
better run.
The accuracy of the significant pairs determination is then
computed as the number of correct classifications over the
sum of correct and incorrect classifications where inversions
are counted as both a false alarm and a miss.
Accuracy scores are given in Table 2, which shows the
mean value as well as the minimum and maximum accuracy
values observed across the 50 trials.
The 1stratum strategy is consistently the worst strategy
for the NDCG and P(10) measures, and is only somewhat
more competitive for MAP. This strategy is the only strategy
examined that does not judge all of the documents retrieved
in the top 10 ranks, but instead samples uniformly from the
top 100 ranks. Since all three measures strongly emphasize

3.3

Estimating R

Part of the computation of extended inferred measures is
creating an estimate of the total number of relevant documents for a topic. This estimate has a large impact on the
quality of the final estimates of NDCG and MAP scores.
Since NDCG is defined over a fixed set of ranks (100 in this
work), while MAP considers the entire relevant set, the quality of the estimate of R has a bigger impact on infAP than
it does on infNDCG.

1121

Pool25

2strata

Figure 1: Estimates of R on the TREC-8 collection.
Table 3: Mean correlation between
gold-standard R
TREC-8 TREC
1stratum
0.97
2strata
0.94
pool25
0.91
pool15
0.87

smaller than 20. The gold-standard and estimated R values
for these topics are shown below, which has a correlation of
just 0.42.
Gold: 13 22 16 6 28 13 17 17 65
Est: 12.7 17.8 15.0 6.0 13.7 9.8 17.0 17.0 16.0

estimated and
2012
0.98
0.99
—
0.94

4.

CONCLUSION

The inferred measures framework can construct test collections that have high fidelity with their traditionallyconstructed counterparts using relatively few judgments provided an appropriate sampling strategy is used in the construction process. The framework does not work well with
large, sparsely sampled strata nor with strategies that do
not exhaustively judge a small top stratum. Restricting all
judgments to the very top ranks (i.e., pooling) is also not
a good strategy, though, because such samples are unable
to accurately estimate R. Thus, what this paper called the
2strata strategy—using an exhaustively judged small initial
stratum coupled with a moderate depth, moderate sampling
rate stratum—is a practical and effective sampling strategy
to use for inferred measures.

Table 3 gives the mean over the 50 trials of the Pearson correlation between the per-topic estimate and goldstandard values of R, the number of relevant documents.
The gold-standard value of R for the TREC 2012 collection
is the estimate produced using the entire set of runs submitted to the Medical Records track. The estimates based on
half of the runs are very highly correlated with the original
estimates, though the pooling strategy produces less good
estimates than the other strategies. The gold-standard value
of R for the TREC-8 collection is the count of the the number of relevant in the original qrels. The estimated R values
are again highly correlated with true R, but the correlations
are weaker for the TREC-8 collection than the TREC 2012
collection. The pooling strategies exhibit the weakest correlations, and the correlations get weaker as the pool depth
gets smaller.
Despite a good overall correlation, the pooling strategies
underestimate R for all but the smallest of relevant sets,
meaning that they cannot distinguish topics that truly have
a small relevant set. Figure 1 shows the estimates of R per
topic for the TREC-8 collection for the pool25 (left) and
2strata (right) sampling strategies. Individual topics are
plotted on the x-axis and the number of documents on the
y-axis. The minimum and maximum estimates of R across
the 50 trials are plotted by a line connecting the two points
(which runs off the top of the graph if the maximum estimate
exceeds the graph’s largest y-value). The mean across the
50 trials of the estimated R is plotted as an x on that line.
The gold-standard value of R is plotted as a circle. For
the pool25 strategy, nine topics have a mean estimated R

5.

REFERENCES

[1] T. Bompada, C.-C. Chang, J. Chen, R. Kumar, and

[2]

[3]

[4]

[5]

1122

R. Shenoy. On the robustness of relevance measures with
incomplete judgments. In Proceedings of SIGIR 2007, pages
359–366, 2007.
K. Sparck Jones and C. van Rijsbergen. Report on the need
for and provision of an “ideal” information retrieval test
collection. British Library Research and Development
Report 5266, 1975.
E. M. Voorhees. The TREC Medical Records track. In
Proceedings of ACM Conference on Bioinformatics,
Computational Biology and Biomedical Informatics, pages
239–246, 2013.
E. Yilmaz and J. A. Aslam. Estimating average precision
when judgments are incomplete. Knowledge Information
Systems, 16:173–211, 2008.
E. Yilmaz, E. Kanoulas, and J. A. Aslam. A simple and
efficient sampling method for estimating AP and NDCG. In
Proceedings of SIGIR 2008, pages 603–610, 2008.

