Extending Test Collection Pools Without Manual Runs
Gaya K. Jayasinghe

William Webber

Mark Sanderson

RMIT University
Melbourne, Australia
gaya.jayasinghe@rmit.edu.au

William Webber Consulting
Melbourne, Australia
william@williamwebber.com

RMIT University
Melbourne, Australia
mark.sanderson@rmit.edu.au

J. Shane Culpepper
RMIT University
Melbourne, Australia
shane.culpepper@rmit.edu.au

ABSTRACT

[14, 15] (typically i = 1, 000). The sets of documents are known
as automatic runs. Across the runs, the top-j ranked documents
for each topic are gathered for relevance assessment (typically j is
set to 50 or 100). Such a practice seems to consistently identify
most of the relevant documents, but provides no guarantee on the
judgment coverage for documents retrieved by new IR approaches
[4, 9, 16]. Test collections tend to have a bias towards the systems
contributing to the pool, and may not reliably evaluate novel IR
systems that retrieve unjudged but relevant documents.
In an attempt to “future proof” test collections, the organizers
of the evaluation conferences commonly encourage submissions of
manual runs, where humans can reformulate queries and/or merge
results from multiple queries [1] before a final set of top-i documents is submitted. Such runs are generally highly effective and
contribute many unique relevant documents to the judgment pool.
However, manual runs are not always available when building a
collection, so in this short paper we ask:

Information retrieval test collections traditionally use a combination of automatic and manual runs to create a pool of documents
to be judged. The quality of the final judgments produced for a
collection is a product of the variety across each of the runs submitted and the pool depth. In this work, we explore fully automated approaches to generating a pool. By combining a simple
voting approach with machine learning from documents retrieved
by automatic runs, we are able to identify a large portion of relevant documents that would normally only be found through manual
runs. Our initial results are promising and can be extended in future studies to help test collection curators ensure proper judgment
coverage is maintained across complete document collections.

Categories and Subject Descriptors
H.3.3 [Information Storage & Retrieval]: Information Search &
Retrieval—clustering, retrieval models, search & selection process

Research question: Can we construct reliable IR test collections
using only automatic retrieval runs?

General Terms

Our contribution: We describe a methodology that can be used
to construct reusable test collections in the absence of manual retrieval runs. We evaluate a simple voting approach combined with
machine learning to show that we can achieve collection coverage
similar to pooling generated with manual runs.

Information retrieval, Evaluation, Test collection construction

1.

INTRODUCTION

Successful evaluation and reproducibility of experiments in information retrieval (IR) depends on building reusable test collections composed of documents, topics, and relevance judgments.
Ideally every document in a collection would be assessed against
each topic, but this approach does not scale. So judgments are normally produced for a sample of the corpus, known as a pool, all
other documents are assumed to be not relevant. This sample needs
to be representative of the entire collection and robust enough to
evaluate entirely new search algorithms. The genesis of pooling
dates back to the 1970s [12].
To produce relevance judgments, the organizers of TREC, CLEF,
NTCIR, and other such conferences invite researchers to submit the
top-i documents retrieved for a set of topics from a specified corpus

2.

BACKGROUND

Efficiently building test collections for evaluation of IR systems
is a well-studied problem [10]. Early research concentrated on
more efficient ways for assessors to scan pools, with the objective of judging more documents with a given budget or identifying a sufficient number of relevant documents as quickly as possible. Zobel [16] showed that the number of relevant documents in
a collection varies from topic to topic. He suggested that assessors
should focus their effort on judging topics with more relevant documents. For each topic, the number of relevant documents found so
far were used to estimate the expected ratio of relevant documents
in the remaining unjudged block. Each topic was assessed until
relevant documents were depleted beyond an economically viable
limit to assess the block.
The idea of focusing assessor effort on the most fruitful sources
of relevant documents was also applied to IR systems that contribute to a pool. Just as some topics have more relevant documents
than others, some systems retrieve more relevant documents than
others. Using this insight, Cormack et al. [5] described a move-tofront pooling approach which ensured that documents from the IR
systems producing the most relevant documents were moved to the

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2257-7/14/07 ...$15.00.
http://dx.doi.org/10.1145/2600428.2609473.

915

916

We also use Kendall’s τ to measure pairwise inversions between
two rankings of runs, the first using full TREC relevance assessments and the second using relevance assessments generated from
the union of the first and second pools formed by each of our methods. Using a convention from Voorhees [13], if the Kendall’s τ
correlation is ≥ 0.9, the rankings are considered equivalent.
Combined
0.1507⋆
0.1714
0.1500
0.1367
0.0916

Relevant documents exclusively
pooled by manual retrieval runs

Combined
Estimates for combined

0

Table 1: Effectiveness on finding relevant documents in MRJ.
A ⋆ : significant improvement (p < 0.01) compared to Borda
count.

1000

ML
0.0268
0.0531
0.0378
0.0361
0.0167

10

Borda count
0.0778
0.1306
0.1122
0.1020
0.0743

Number of Relevant documents

Metric
MAP
P@10
P@20
P@30
P@100

runs would be underestimated and any improvements would go unnoticed. It would appear that manual retrieval runs still play a critical role in improving the re-usability of test collections.

Depth (k)
50
100
150
171 z
200

Borda count
15.22
24.19
29.30
31.36
33.75

ML
4.52
5.45
6.71
7.11
7.97

Combined
19.00⋆
29.83⋆
37.28⋆
39.53⋆
42.33⋆

0

150

200

Figure 2: The number of MRJ documents, and estimated number of relevant documents in the top-k of the combined ranked
list on TREC GOV2 dataset and TREC topics 801 – 850.

Metric
MAP
P@10
P@20
P@30
P@100

RESULTS

The analysis is presented in Table 1. The combined method is
significantly better than the other two when evaluated with MAP.
The same trend is observed when measuring using precision, but
none of the differences are significant. Using only the ML method
produces worse results than either Borda count or combined.
Note that the relatively low reported effectiveness numbers in
Table 1 are largely a byproduct of evaluating using only the unique
relevant documents in MRJ and not the entire second pool. We
cannot make any claims about new documents retrieved by the
ML method since a large portion of retrieved documents using this
method are not judged, compared to other two approaches. In fact,
9, 817 of the top-200 documents returned across all 50 topics using
only ML (98.17%) are currently unjudged. Therefore, we have to
assume that these documents are not relevant until all of the documents returned are judged. In future work, we hope to investigate
the full impact unjudged documents have on our classifier method
in more detail.
In Table 2 we measure the proportion of documents that were
found to be relevant in the second pool. Again a similar trend of
differences are seen, but with significant improvements across all
measurements up to k = 200 for the combined method.

5.1

100
Depth of ranking (K)

Table 2: Percentage of MRJ documents found in top (k) of
the proposed rankings. z implies a similar assessment effort
to traditional pooling method. A ⋆ : significant improvement
(p < 0.05) compared to Borda count.

5.

50

Borda count
0.3415
0.3571
0.3551
0.3401
0.2337

ML
0.4872⋆
0.5082⋆
0.4684⋆
0.4299⋆
0.2624⋆

Combined
0.5049⋆
0.5694⋆
0.4959⋆
0.4497⋆
0.2555•

Table 3: Just considering the documents in MRJ, how effective
are ranking algorithms on retrieving relevant documents? Significant improvements (p < 0.01 and p < 0.05) compared to
Borda count are denoted with a ⋆ and • .
Judging the ranked lists of the combined method up to a depth
k identifies a subset of the relevant documents uniquely pooled
by manual retrieval runs. However, we still know little about the
large number of unjudged documents in the ranked lists produced
by the combined method. If we assume the proportion of relevant
documents among unjudged documents in these ranked lists is the
same as the proportion found among judged documents in the same
ranked list up to the same depth, we can estimate the total number of relevant documents that would have been found in the same
depth of the ranking. Figure 2 illustrates the estimated number of
relevant documents, along with the number of known relevant documents found.
Missing judgments for a large portion of the ranked lists from
the proposed methods is one potential reason for the low retrieval
effectiveness of those methods. Therefore, we calculate retrieval
effectiveness on the intersection of the second pool with MRJ, Table 3. (Note, the first pool and the ranking functions remains the
same.) The ML method now re-ranks a subset of unique documents top-j ranked by manual runs. The ranking produced by ML
show significant improvements for all considered evaluation metrics compared to Borda count. The combined method achieves a
better effectiveness than ML for all evaluation metrics considered,
except p@100. This is due to ranking only the subset of documents
top ranked by the Borda count. Re-ranking a carefully retrieved

Discussion

As indicated in Figure 1, the majority of documents uniquely
judged in the manual runs (MRJ) are also retrieved by the automatic
runs (ARU+ARJ). However, few appear in the first pool as they (i.e.
ARJ) are not ranked highly enough to be judged. In fact, 88% of the
documents judged as relevant that are uniquely pooled by manual
runs could be found in the first pool, if a pool depth of 1, 000 was
used.
If there were no manual runs in a test collection (i.e. no MRJ),
the effectiveness of IR systems producing results similar to such

917

1
0.9
Kendall's Tau

0.8

1

MAP
P@10
P@20
P@30
P@100

0.7

0.9
0.8

Kendall's Tau

MAP
P@10
P@20
P@30
P@100

0

50

100

150

200

5

Depth

15

20

25

30

Automatic systems

Figure 3: Kendall’s τ correlation of IR system rankings for
varying depths of assessing documents using combined method.

Figure 4: Kendall’s τ correlation of IR system rankings with
varying number of automatic systems in the pool. Automatic
systems are added in the order least contributing system to
most, and the ranking produced by the combined method is
processed to a depth of 200.

subset of documents for topics with ML is an effective approach to
locate new documents to be pooled and judged.
Whenever a new approach for pool composition is proposed, we
would like to be able to quantify how well the approach ranks IR
systems compared to the original method. A Kendall’s τ ranking
correlation for varying depths of assessing documents with the proposed approach for various evaluation metric are shown in Figure 3.
Here, we consider all 80 submitted runs rather than only the subset
originally used for pooling. Manual retrieval runs are viewed as
novel approaches to retrieval. The Kendall’s τ correlation for MAP
is above 0.9 beyond a depth of 100. A budget similar to original assessment permits processing up to a depth of 171 documents,
which demonstrates the validity of the proposed approach in the
absence of manual retrieval runs.
Another question of interest is how small the automatic runs pool
can be when there are no manual runs. In Figure 4 we introduce
runs incrementally in order starting with the run contributing the
fewest relevant documents. When 20 or more automatic retrieval
runs are pooled the Kendall τ correlation for MAP exceeds 0.9.

6.

10

References
[1] C. Buckley, D. Dimmick, I. Soboroff, and E. Voorhees. Bias and the
limits of pooling for large collections. Information Retrieval, 10(6):
491–508, 2007.
[2] S. Büttcher, C. L. A. Clarke, and I. Soboroff. The TREC 2006 terabyte
track. In TREC-2006, volume 6, page 39, 2006.
[3] S. Büttcher, C. L. A. Clarke, P. C. K. Yeung, and I. Soboroff. Reliable
information retrieval evaluation with incomplete and biased judgements. In SIGIR, pages 63–70, 2007.
[4] B. Carterette, E. Gabrilovich, V. Josifovski, and D. Metzler. Measuring the reusability of test collections. In WSDM, pages 231–240,
2010.
[5] G. V. Cormack, C. R. Palmer, and C. L. A. Clarke. Efficient construction of large test collections. In SIGIR, pages 282–289, 1998.
[6] R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. LIBLINEAR: A
library for large linear classification. Journal of Machine Learning
Research, 9:1871–1874, June 2008.
[7] R. Krovetz. Viewing morphology as an inference process. In SIGIR,
pages 191–202, Pittsburgh, Pennsylvania, USA, 1993.
[8] A. Moffat, W. Webber, and J. Zobel. Strategic system comparisons
via targeted relevance judgments. In SIGIR, pages 375–382, 2007.
[9] T. Sakai. The unreusability of diversified search test collections. In
EVIA, June 2013.
[10] M. Sanderson. Test collection based evaluation of information retrieval systems. Foundations and Trends in Information Retrieval, 4
(4):247–375, 2010.
[11] I. Soboroff and S. Robertson. Building a filtering test collection for
TREC 2002. In SIGIR, pages 243–250, 2003.
[12] K. Spärck Jones and C. J. Van Rijsbergen. Report on the need for and
provision of an “ideal” information retrieval test collection. Technical
report, British Library Research and Development Report 5266, 1975.
[13] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Information processing & management, 36(5):697–716, 2000.
[14] E. M. Voorhees. The philosophy of information retrieval evaluation.
In Evaluation of cross-language information retrieval systems, pages
355–370. Springer, 2002.
[15] E. M. Voorhees and D. K. Harman. TREC: Experiment and evaluation
in information retrieval, volume 63. MIT press Cambridge, 2005.
[16] J. Zobel. How reliable are the results of large-scale information retrieval experiments? In SIGIR, pages 307–314, 1998.

CONCLUSION

In this paper, we present a methodology for building reusable
evaluation pools in the absence of manual retrieval runs. Our approach can discover many relevant documents that were previously
only found by manual retrieval runs. The approach demonstrates
the potential of finding relevant documents that are not currently
possible using current pooling approaches. However, the true efficacy of our approach cannot be properly assessed until all of the
newly retrieved documents are judged. We plan to investigate this
in future work. Nonetheless, our initial results are promising as we
are already able to achieve a similar IR system ranking to previous approaches which depended heavily on manual runs to add the
necessary diversity to the assessment pool.

Acknowledgments
This work was supported in part by the Australian Research Council (DP130104007). Dr. Culpepper is the recipient of an ARC DECRA Research Fellowship (DE140100275).

918

