An Enhanced Context-sensitive Proximity Model for
Probabilistic Information Retrieval
Jiashu Zhao1 , Jimmy Xiangji Huang2

1

Information Retrieval and Knowledge Management Research Lab
Department of Computer Science & Engineering, 2 School of Information Technology
York University, Toronto, Canada
1

jessie@cse.yorku.ca, 2 jhuang@yorku.ca

ABSTRACT
We propose to enhance proximity-based probabilistic retrieval models with more contextual information. A term
pair with higher contextual relevance of term proximity is
assigned a higher weight. Several measures are proposed
to estimate the contextual relevance of term proximity1 .
We assume the top ranked documents from a basic weighting model are more relevant to the query, and calculate
the contextual relevance of term proximity using the top
ranked documents. We propose a context-sensitive2 proximity model, and the experimental results on standard TREC
data sets show the eﬀectiveness of our proposed model.

Keywords
Context-Sensitive IR, Measure, Proximity, Probabilistic Model

1. INTRODUCTION AND MOTIVATION
The study of how to integrate the context information of
queries and documents into retrieval process draw a lot of
attention in recent years [3]. More speciﬁcally, many term
proximity approaches [2, 9, 10, 11], which reward the documents where the query terms occurring closer to each other,
show signiﬁcant improvements over basic Information Retrieval (IR) models. In these proximity-based approaches,
all the query term pairs are usually treated equally and the
diﬀerence among various query pairs are not considered, although there is a need to distinguish the importance of term
proximities. For example, given a query “recycle automobile
tires”, there is a stronger association between “automobile”
and “tire” than the association between “recycle” and “automobile”. In the top ranked documents, “automobile” and
“tire” are expected to occur close to each other, while “recycle” and “automobile” do not necessarily have to occur close.
In this paper, we focus on the problem of diﬀerentiating
the inﬂuence of associated query term pairs. We propose

a proximity enhancement approach to integrate the contextual relevance of term proximity into the retrieval process. We also propose four measures for estimating the contextual relevance of term proximity, and reward the query
term pairs according to both the proximity and the contextual relevance of proximity. There are several studies
that boost proximity retrieval models. A machine learning
method is proposed to determine the “goodness” of a span in
[8]. [1] learns the concept importance from several sources
(e.g. google n-gram corpus, query logs and wikipedia titles).
SVM is used to learn diﬀerent weights for various term dependencies in [6]. The importance of the global statistics is
examined for proximity weighting [4]. Phrases are treated
as providing a context for the component query terms in
[7]. The contribution of this paper is that we propose the
contextual relevance of term proximity, which represents to
what extent the corresponding term pair should be related
to the topic of the query. The contextual relevancy of term
proximity is combined with the value of term proximity to
characterize how much a document should be boosted.
The remainder of this paper is organized as follows. In
Section 2, we introduce four measures for estimating the contextual relevance of term proximity. In Section 3, we propose an enhanced context-sensitive proximity model using
the proposed measures. Section 4 presents the experimental
results and parameter sensitivities. Section 5 concludes the
ﬁndings and discusses possible future directions.

2. CONTEXTUAL RELEVANCE OF TERM
PROXIMITY
In this section, we propose how to estimate the contextual
relevance of term proximity. The contextual relevance of
term proximity is deﬁned as how much the corresponding
term pair should be related to the topic of the query in the
context. The notations used in this paper are shown as
follows.
• Q = {q1 , ..., qm } is a query
• D is a relevant document
• tf (qi , D) is the term frequency of qi in D
• {pos1,i , pos2,i , ..., postfi ,i } are the positions of qi in the
document D
• dist(posk1 ,i , posk2 ,j ) is deﬁned as dist(posk1 ,i , posk2 ,j ) =
|posk1 ,i − posk2 ,j |, which is the distance between two
positions.

1
The contextual relevance of term proximity is the relevancy
between the query term proximity and the topic of the query.
2
Context-sensitive means that the contextual relevance of
term proximity is considered.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
ACM 978-1-4503-2257-7/14/07 ...$15.00.
http://dx.doi.org/10.1145/2600428.2609527.

We measure the contextual relevance of term proximity
base on the assumption that distributions of qi and qj in
a relevant documents can represent the association between
qi and qj . If qi and qj occur closely in relevant documents,

1131

RelCoOccur
RelSqRecip
RelM inDist
RelKernel

q1 , q2
1.0000
0.2331
0.0486
1.3200

q1 , q3
1.0000
0.0408
0.0009
0.7200

q1 , q4
1.0000
0.0434
0.0025
0.7200

q1 , q5
0.0000
0.0000
0.0000
0.0000

q2 , q3
1.0000
0.0523
0.0067
0.7200

q2 , q4
1.0000
0.0434
0.0025
0.7200

q2 , q5
0.0000
0.0000
0.0000
0.0000

q3 , q4
1.0000
1.0000
0.3133
0.4800

q3 , q5
0.0000
0.0000
0.0000
0.0000

q4 , q5
0.0000
0.0000
0.0000
0.0000

Table 1: An example of the contextual relevance of term proximity
will have higher contextual relevance. RelM inDist is modiﬁed from [9], where the minimum distance is shown to be
more eﬀective than the other distance-based and span-based
proximity approaches. RelKernel utilizes the term proximity
approach proposed in [10], where a query term is simulated
by the kernel function, where the triangle kernel function
is recognized to be the most eﬀective. Diﬀerent types of
information are incorporated in these measures.
To better analyze the contextual relevance measurements
deﬁned above, we present an example for a given query Q =
{q1 , q2 , q3 , q4 , q5 } and a relevant document D.

the contextual relevance of term proximity between qi and
qj is high. On the contrary, if qi and qj do not co-occur
or occur far away to each other, the contextual relevance
of term proximity between qi and qj is low. Therefore we
propose the following four methods for estimating the contextual relevance of term proximity between qi and qj in a
relevant document D. For the extreme case when qi and qj
do not co-occur in D, we consider the contextual relevance
of term proximity equals 0. Otherwise, we deﬁne the following measures to generate a positive value for the contextual
relevance of term proximity.

D = {xq1 xq2 xxxxq3 q4 xxxxxq1 xq2 }

Definition 1. RelCoOccur (qi , qj , D) is deﬁned to be 1, if qi
and qj both occur in D.
RelCoOccur (qi , qj , D) = 1{qi ∈D∧qj ∈D}

(1)

Definition 2. The RelSqRecip is deﬁned as the sum of squared
reciprocal distances between qi and qj .
tf (qi ,D) tf (qj ,D)

RelSqRecip (qi , qj , D) =





k1 =1

k2 =1

1
dist(posk1 ,i , posk2 ,j )2
(2)

Definition 3. The RelM inDist is a deﬁned as the following
function of the minimum distance between qi and qj .
RelM inDist (qi , qj , D) = ln(α + e−M inDist(qi ,qj ,D) )

(3)

where α is a parameter, and M inDist(qi , qj , D) is the minimum distance between all co-occurring qi and qj in D.
M inDist(qi , qj , D)
= mink1 ∈{1..tf (qi ,D)},k2 ∈{1..tf (qj ,D)} (dist(posk1 ,i , posk2 ,j ))
Definition 4. The RelKernel is deﬁned as the sum of the
kernel functions of distances between qi and qj .
RelKernel (qi , qj , D)
tf (qi ,D) tf (qj ,D)

=





k1 =1

k2 =1

1
Kernel( dist(posk1 ,i , posk2 ,j ))
2

(4)

where Kernel(·) is kernel function. Here we use the triangle
kernel function.
u
Kernel(u) = (1 − ) · 1{u≤σ}
(5)
σ

where x represents a non-query term. By observing the
query and the document, we ﬁnd that the term q5 does not
present in D, which means it does not related to D, and
therefore do not have an association with other query terms.
Since q3 and q4 are adjacent to each other and far apart
from other query terms, q3 and q4 are more likely to have a
stronger association than the combination of q2 and q4 . We
calculate the contextual relevance of term proximity between
q2 and q4 as an instance, and the procedure will be the same
for the rest of the query term pairs. The term frequency of
terms q2 and q4 are tf (q2 , D) = 2 and tf (q4 , D) = 1. The
positions of q2 and q4 in D are pos(q2 , D) = {4, 18} and
pos(q4 , D) = {10} correspondingly. Therefore there are 2
co-occurrences of q2 and q4 , and the corresponding distances
between these co-occurrences are {6, 8}. Then we can calculate Rel(q2 , q4 , D) with these distances by formulae (1-4).
Table 1 shows the values of the contextual relevance of term
proximity in this example.
We can see that the contextual relevance measures deﬁned above show diﬀerent characteristics. RelCoOccur detects term pairs with or without an association. For example, q5 and other query terms do not have an association. The term pairs containing q5 are distinguished by
RelCoOccur . On the other hand, RelCoOccur does not consider the term distributions in D. RelM inDist takes into account the closest occurrences between a pair of quay terms,
and does not consider the frequency of occurrences. RelM inDist
and RelKernel accumulates over all the occurrences of two
query terms, with diﬀerent functions and therefore generates
diﬀerent values.

3.

where u is an input value, and σ is the kernel parameter.
These functions measure the contextual relevance from
diﬀerent perspectives. RelCoOccur measures whether qi and
qj are co-occurring in D. RelSqRecip , RelM inDist and RelKernel
considers the positions of qi and qj in D. In RelSqRecip , we
generate a squared reciprocal function for the distances between all the occurrences of qi and qj , and accumulate the
values over D. Then the query term pairs with terms occurring closer to each other and/or occurring more frequently

1132

CONTEXT-SENSITIVE PROXIMITY MODEL

In this section, we propose a context-sensitive proximity
retrieval model, by integrating the proposed measures for
contextual relevance of term proximity into retrieval process. Naturally we treat the values of contextual relevance
as weights to reward the query term pairs with higher contextual relevance and to penalize the query term pairs with
lower contextual relevance. In practice, we assume the top
ranked documents returned by a basic retrieval model (for
example, BM25) are more relevant than the rest of the documents. The averaged contextual relevance of term proximity
over the top ranked documents is multiplied by the proximity part in the weighting function. A general form of the

context-sensitive proximity model is


RelP rox(D )
= (1 − δ) ∗ Σqi w(qi , D )

(6)


+ δ ∗ Σqi ,qj AR(qi , qj , topDoc) ∗ P rox(qi , qj , D )
where D is a given document, w(qi , D ) is the weight of qi in
D by a basic probabilistic weighting function, P rox(qi , qj , D )
is a bigram proximity weighting approach, λ is a balancing
parameter, topDoc is the number of top ranked documents,
AR(qi , qj , topDoc) is the average contextual relevance value
of term proximity between qi and qj over the top ranked
documents

1
AR(qi , qj , topDoc) =
Rel(qi , qj , D)
(7)
topDoc D
where Rel(qi , qj , D) is one of the measures deﬁned in Section
2. Please note that AR(qi , qj , topDoc), P rox(qi , qj , D ) and
w(qi , D ) need to be normalized to the same scale.
In formula (6), we use the probabilistic BM25 [5] as the
basic weighting function. We adopt the proximity approach
used in CRTER [10], since it is an eﬀective pairwise proximity model for probabilistic IR. The BM25 weighting function
has the following form.

Data Sets
BM25
Dirichlet LM
CRTER
Improvement over BM25
RelCoOccur
Improvement over BM25
Improvement over CRTER
RelSqRecip
Improvement over BM25
Improvement over CRTER
RelM inDist
Improvement over BM25
Improvement over CRTER
RelKernel
Improvement over BM25
Improvement over CRTER

w(qi , D  )
=

N − n(qi ) + 0.5
(k1 + 1) ∗ tf (qi , D  ) (k3 + 1) ∗ qtf (qi )
∗
∗ log
K + tf (qi , D  )
k3 + qtf (qi )
n(qi ) + 0.5
(8)

where N is the number of documents in the collection, n(qi )
is the number of documents containing qi , qtf (qi ) is the
within-query term frequency, dl (D  ) is the length D , avdl is
the average document length, the ki s are tuning constants,
K equals k1 ∗ ((1 − b) + b ∗ dl(D)/avdl). The proximity part
of CRTER is shown as follows.
P rox( qi , qj , D ) = w(qi,j , D )

(9)

where qi,j represents the association between query terms qi
and qj , w(qi,j , D ) is the BM25 weighting function with the
following features of qi,j [10]


tf (qi,j , D ) =


tf (qi ,D  ) tf (qj ,D )





k1 =1

k2 =1

1
Kernel( dist(posk1 ,i , posk2 ,j ))
2

1
qtf (qi,j ) = Kernel( ) · min(qtf (qi ), qtf (qj ))
2
 tf (qi,j , D )
n(qi,j ) =
Occur(qi,j , D )

D

where Kernel(·) is a kernel function, and Occur(qi,j , D )
 i tfj
equals tf
k2 =1 1{Kernel( 1 dist(posk ,i ,posk ,j ))=0} .
k1 =1

4.

EXPERIMENTS

2

1

2

We evaluate the proposed approach on three standard
TREC data sets. They are AP88-89 with topics 51-100,
Web2G with topics 401-450, and TREC8 with topics 401450. AP88-89 contains articles published by Association
Press from the year of 1988 to 1989. The WT2G collection is a 2G size crawl of Web documents. The TREC8
contains newswire articles from various sources, such as Financial Times (FT), the Federal Register (FR) etc. For all
the data sets used, each term is stemmed using Porter’s English stemmer, and standard English stopwords are removed.
We have three baseline models, BM25, Dirichlet Language

1133

AP88-89
0.2708
0.2763
0.2744
1.329%
0.2768
2.216%
0.875%
0.2812*‡
3.840%
2.478%
0.2800
3.397%
2.041%
0.2812*‡
3.840%
2.478%

Web2G
0.3136
0.3060
0.3298*
5.166%
0.3375*
7.621%
2.335%
0.3444* ‡
9.821%
4.427%
0.3444* ‡
9.821%
4.427%
0.3425*‡
9.216%
3.851%

TREC8
0.2467
0.2552
0.2606 *
5.634%
0.2622*
6.283%
0.614%
0.2633*
6.729%
1.036%
0.2615*
5.999%
0.345%
0.2625*
6.405%
0.729%

Table 2: Overall MAP Performance (“*” indicates
significant improvement over BM25, and “‡” indicates significant improvement over CRTER)
Model (LM) and CRTER. The best parameters are chosen
in the baseline models for fair comparisons. In BM25, the
values of k1 , k2 , k3 and b are set to be 1.2, 0, 8 and 0.35
respectively, since they are recognized with a good performance. In CRTER model, we use the recommended settings
[10]., which are σ = 25, λ = 0.2, and triangle kernel function.
In our proposed context-sensitive proximity model, we use
the same parameters in the basic weighting model part (e.g.
BM25) and the proximity part (e.g. CRTER). In RelKernel ,
we set the kernel parameter σ = 25. In RelM inDist , we set
α = 1, which has the best performance in [9]. We normalize
AR(qi , qj , topDoc), P rox(qi , qj , D ) and w(qi , D ) in formula
(6) to the scale of [0,1]. We use the Mean Average Precision
(MAP) as our evaluation metric.
Table 2 shows the overall MAP performance. The proposed context-sensitive proximity model outperforms BM25,
Language Model (LM) and CRTER with all of the contextual relevance measuring approaches on all the data sets.
For the space limitation, we only include these comparisons.
It shows that using the contextual relevance of term proximity can further boost the retrieval performance. We can
see that the RelCoOccur , which measures whether two query
terms are co-occurring in the relevant documents, reaches
the lowest MAP among the contextual relevance measures,
which indicates the necessity of considering the term location information in the term pair contextual relevance deﬁnition. RelSqRecip has the highest MAP over the other approaches on all the data sets. In general, considering both
the closeness and frequency of two query terms in the contextual relevance deﬁnition beneﬁts the contextual relevance
estimation.
In Table 3, we investigate how the number of top relevant
documents aﬀects the retrieval performance. We take the
topDoc = 5, 10, 20, 30, 40, 50 60, 70, and 80 documents
as relevant, and calculate the average contextual relevance
obtained from these documents. The bolded values are the
best performance among diﬀerent topDoc values. We can see
that the best topDoc will be around 5 to 40. It means that
selecting too many top documents as relevant will introduce
noises to the model.
Figure ?? shows the sensitivity of δ on all the data sets.
We can see that with the growth of δ, MAP ﬁrst increases

5
0.2757
0.281
0.2800
0.2812
0.3348
0.3401
0.3351
0.3358
0.2622
0.2633
0.2612
0.2625

RelCoOccur
RelSqRecip
RelM inDist
RelKernel
RelCoOccur
RelSqRecip
RelM inDist
RelKernel
RelCoOccur
RelSqRecip
RelM inDist
RelKernel

AP88-89

Web2G

TREC8

10
0.2766
0.2812
0.2794
0.2796
0.3359
0.342
0.3444
0.3409
0.2612
0.263
0.2614
0.2622

20
0.2768
0.2801
0.2800
0.2801
0.3354
0.3409
0.3421
0.3425
0.2614
0.2627
0.2615
0.2619

30
0.2767
0.2801
0.2796
0.2801
0.3367
0.3401
0.3382
0.3406
0.2611
0.2623
0.2612
0.2615

40
0.2762
0.2789
0.2797
0.2789
0.3375
0.3444
0.3406
0.3423
0.2611
0.2617
0.2606
0.2607

50
0.2759
0.2781
0.2784
0.2781
0.3369
0.3435
0.3414
0.3418
0.261
0.2615
0.2605
0.2604

60
0.2758
0.278
0.279
0.2783
0.3367
0.3433
0.3427
0.3421
0.2608
0.2613
0.2606
0.2605

70
0.2755
0.2777
0.2787
0.2781
0.3374
0.3424
0.3434
0.3414
0.2601
0.2614
0.2607
0.2605

80
0.2753
0.2774
0.2786
0.278
0.3363
0.3419
0.3433
0.3399
0.2599
0.2612
0.2605
0.2606

Table 3: Performance over the change of topDoc
0.285

0.36

0.28

0.35

0.275

0.34

0.27

0.33

0.265

0.32

0.26

0.26
0.255

QualityCoOccur

0.25

QualitySqRecip
QualityMinDist

0.245
0.24

0.2

0.31
0.3

QualityCoOccur

0.29

QualitySqRecip

0.4

0.6

0.8

1

0.27

QualityCoOccur
QualitySqRecip

0.22

QualityMinDist
QualityKernel

QualityKernel
0

0.2

0.4

0.6

0.8

1

0.21

0





(a) AP88-89

0.24

0.23

QualityMinDist

0.28

QualityKernel
0

MAP

MAP

MAP

0.25

0.2

0.4

0.6

0.8

1



(b) Web2G
Figure 1: Sensitivity of δ

(c) TREC8

basic weighting models (e.g. Language Model) and/or other
proximity approaches.
We can also apply the proposed
model into relevance feedback.

and then decreases. Please note that when δ = 0, there is
no proximity utilized, which is our baseline BM25. When
δ = 1, only term proximity and the contextual relevance
of term proximity are considered. In CRTER, the recommended setting for the balancing parameter is 0.2. After introducing the contextual relevance of term proximity, we can
see that the balancing parameter δ with a value of around
0.3 or 0.4 is better. The reason is that the contextual relevance is normalized to [0,1]. The value for the second part
of formula (6) becomes smaller, therefore it requires a larger
balancing parameter.

6. ACKNOWLEDGEMENTS
This research is supported by the research grant from the
NSERC of Canada and the Early Researcher Award. We
thank four anonymous reviewers for their thorough review
comments on this paper.

7. REFERENCES

5. CONCLUSIONS AND FUTURE WORK
We propose a new approach to integrate the contextual
relevance of term proximity in retrieval. The contextual
relevance of term proximity evaluates how much we should
focus on a pair of query terms. In particular, we propose
four measures to estimate the contextual relevance of term
proximity, namely RelOcOccur , RelSqRecip , RelM inDist , and
RelKernel . They incorporate diﬀerent types of information
utilized in the contextual relevance of term proximity. We
further propose a context-sensitive proximity model via multiplying the contextual relevance of term proximity by the
proximity part in a retrieval model.
We evaluate our proposed context-sensitive proximity model
on several TREC standard data sets, and the experimental
results show the eﬀectiveness of our proposed model over
three baselines BM25, Dirichlet LM and CRTER with optimal parameter settings. In more detail, we discuss how
many top documents should be selected for calculating the
proximity contextual relevance, and how the balancing parameter δ aﬀects the retrieval performance.
In the future, we can extend the contextual relevance of
term proximity to more query terms. In addition, the contextual relevance of term proximity can be adopted in other

1134

[1] M. Bendersky, D. Metzler, and W. B. Croft. Learning
concept importance using a weighted dependence model. In
Proc. of WSDM, pages 31–40. ACM, 2010.
[2] S. Buttcher, C. Clarke, and B. Lushman. Term proximity
scoring for ad-hoc retrieval on very large text collections.
In Proc. of SIGIR, page 622. ACM, 2006.
[3] W. Croft, D. Metzler, and T. Strohman. Search engines:
Information retrieval in practice. Addison-Wesley, 2010.
[4] C. Macdonald and I. Ounis. Global statistics in proximity
weighting models. In Web N-gram Workshop, pages 30–37,
2010.
[5] S. E. Robertson, S. Walker, S. Jones, M. M.
Hancock-Beaulieu, M. Gatford, et al. Okapi at TREC-3. In
Proc. of TREC, pages 109–126. NIST, 1995.
[6] L. Shi and J.-Y. Nie. Using various term dependencies
according to their utilities. In Proc. of CIKM, pages
1493–1496. ACM, 2010.
[7] R. Song, M. J. Taylor, J.-R. Wen, H.-W. Hon, and Y. Yu.
Viewing term proximity from a diﬀerent perspective. In
Proc. of ECIR, pages 346–357. Springer, 2008.
[8] K. M. Svore, P. H. Kanani, and N. Khan. How good is a
span of terms? exploiting proximity to improve web
retrieval. In Proc. of SIGIR, pages 154–161. ACM, 2010.
[9] T. Tao and C. Zhai. An exploration of proximity measures
in information retrieval. In Proc. of SIGIR, pages 259–302.
ACM, 2007.
[10] J. Zhao, J. Huang, and B. He. CRTER: using cross terms
to enhance probabilistic information retrieval. In Proc. of
SIGIR, pages 155–164, 2011.
[11] J. Zhao, J. X. Huang, and Z. Ye. Modeling term
associations for probabilistic information retrieval. ACM
Transactions on Information Systems (TOIS), 32(2):7,
2014.

