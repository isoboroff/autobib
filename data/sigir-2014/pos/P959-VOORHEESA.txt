On Run Diversity in “Evaluation as a Service”
Ellen M. Voorhees,1 Jimmy Lin,2 and Miles Efron3
1

National Institute of Standards and Technology
The iSchool, University of Maryland, College Park
Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign
2

3

ellen.voorhees@nist.gov, jimmylin@umd.edu, mefron@illinois.edu

ABSTRACT

mental assumption in the paradigm is that researchers can
acquire a test collection’s document corpus. But what if
this is not possible? This was the challenge in the TREC
Microblog track evaluation, where the corpus is comprised
of tweets posted on Twitter. Since the company’s terms of
service forbid redistribution of tweets, a test collection built
on tweets cannot be distributed in a traditional manner.
The solution developed by the track organizers, termed
“evaluation as a service” (EaaS), was to provide an API
through which the tweet collection can be accessed for completing the evaluation task [4, 3]. This API, consisting of a
basic keyword search interface, was the only method through
which track participants could access the collection. The
operational deployment of the EaaS model was successful in
attracting participation at TREC 2013, which provides encouraging evidence that EaaS could be generalized to other
evaluation scenarios involving sensitive data (e.g., medical or
desktop search). Before expanding the use of the approach,
however, we need to know whether EaaS has any substantive
impact on the quality of the collections produced.
One concern about the EaaS model is a potential lack of
diversity in the retrieval runs used to build the test collection. The criticism is this: if everyone must use the API,
won’t they all end up doing basically the same thing? Diversity is important from at least two perspectives. Previous work has tied the reusability of a collection built using
pooling to the diversity of the retrieval runs that form the
pools [1]. Diversity in participant submissions also serves
as a proxy indicator that researchers are trying different
(novel) retrieval techniques. The contribution of this paper is an analysis of the diversity of the pools in the TREC
2013 Microblog track (using EaaS) compared to previous
evaluations—including the first two years of the Microblog
track in TRECs 2011 and 2012—where participants had access to the entire corpus. We first propose a novel metric,
called Run Average Overlap (RAO), that measures the overall distinctiveness of one retrieval run with respect to a given
set of runs. In a second analysis, we apply another test of
reusability, the leave out uniques (LOU) test. Analyses show
no obvious differences in the characteristics of the pools, suggesting that the EaaS model can be used to build reusable
collections.

“Evaluation as a service” (EaaS) is a new methodology that
enables community-wide evaluations and the construction
of test collections on documents that cannot be distributed.
The basic idea is that evaluation organizers provide a service
API through which the evaluation task can be completed.
However, this concept violates some of the premises of traditional pool-based collection building and thus calls into
question the quality of the resulting test collection. In particular, the service API might restrict the diversity of runs
that contribute to the pool: this might hamper innovation
by researchers and lead to incomplete judgment pools that
affect the reusability of the collection. This paper shows
that the distinctiveness of the retrieval runs used to construct the first test collection built using EaaS, the TREC
2013 Microblog collection, is not substantially different from
that of the TREC-8 ad hoc collection, a high-quality collection built using traditional pooling. Further analysis using
the ‘leave out uniques’ test suggests that pools from the Microblog 2013 collection are less complete than those from
TREC-8, although both collections benefit from the presence of distinctive and effective manual runs. Although we
cannot yet generalize to all EaaS implementations, our analyses reveal no obvious flaws in the test collection built using
the methodology in the TREC 2013 Microblog track.
Categories and Subject Descriptors: H.3.4 [Information
Storage and Retrieval]: Systems and Software—Performance
Evaluation
Keywords: test collections; reusability; meta-evaluation

1.

INTRODUCTION

Large-scale community-wide evaluations such as TREC
operationalize the Cranfield paradigm by building retrieval
test collections that support IR research [2]. A test collection consists of a corpus of documents, a set of information
needs called topics, and relevance judgments that specify
which documents are relevant for which topics. A fundaPermission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2257-7/14/07 ...$15.00.
http://dx.doi.org/10.1145/2600428.2609484.

2.

BUILDING POOLED COLLECTIONS

Pooling [8] is a method for building test collections when
the target document corpus is too large to make exhaustive
relevance judgments for every document for every topic. A
pooled collection is built from a set of retrieval runs sub-

959

tweets [4]. Relevance judgments for the MB collections were
made on a 3-point scale (“not relevant”, “relevant”, “highly
relevant”), but in this work we ignore the different degrees
of relevance and use both higher grades as “relevant”.
The final column of Table 1 gives the number of participants that contributed runs. TREC generally allows participating groups to submit more than a single run to a given
task. In practice, runs submitted by the same participant
tend to be much more similar to each other than they are
to runs from other participants. Analyses of run diversity
need to account for this phenomenon.

Table 1: Test collection statistics.
Name
TREC-8
MB2012
MB2013

Docs
528 K
16 M
243 M

Topics
50
60
60

Depth
100
100
90

Runs
71
121
71

Groups
40
33
20

mitted by participants of the process, typically as part of
a community-wide evaluation such as TREC.1 A run is the
output of a retrieval system for each topic in the collection;
documents are assumed to be ordered by decreasing likelihood of relevance to the topic. The pool for a topic is
comprised of the union of the top K (called the pool depth)
documents retrieved for that topic by each run. Only documents in the pool are judged for relevance by a human; all
other documents are assumed to be not relevant.
Test collections are research tools that are most useful
when they are reusable, that is, when they fairly evaluate
retrieval systems that did not contribute to their construction [1]. Thus, a reusable test collection can be used to
evaluate systems even after the original evaluation that created the collection has concluded. For a pooled collection
to be reusable, the pool must contain enough of the relevant
documents in the collection to be an unbiased sample of the
relevant documents. In practice, the completeness of a pool
is a function of (at least) the pool depth, the effectiveness of
the runs that comprise it, the diversity of the runs, and the
true number of relevant documents [10, 9].
Many of the test collections built in TREC, including the
ad hoc collections and the Microblog collections, were built
through pooling. The TREC ad hoc collections, especially
the TREC-8 collection, have been subject to a number of
analyses and have been used in hundreds of retrieval experiments whose outcomes have proven to generalize. Thus,
these collections are generally accepted as high-quality test
collections [5]. Our basic premise is that the TREC 2013
Microblog collection is reusable if it displays characteristics
similar to the TREC ad hoc collections. The particular characteristics of interest are the distinctiveness of the runs over
which the pools are formed and the number of relevant documents found by a single participant.
More details about the three collections used in this paper
are given in Table 1. (Our experiments also considered the
ad hoc test collections from TREC-6 and TREC-7, but they
yielded similar results, and so for brevity we only present
results from TREC-8 here.) The first column of the table
gives the name of the collection as used in the remainder
of this paper. The corpus for TREC-8 consists of approximately half a million (mostly) newswire articles. The pools
were predominantly constructed from the 71 ad hoc runs
that were judged (from a total of 129 submitted runs), but
runs from some other tracks contributed small numbers of
documents to the pools as well.2 The other two collections
were built as part of the TREC Microblog tracks in 2012 and
2013, whose document corpora consist of tweets posted to
Twitter. The document corpus used for the MB2012 track
(called Tweets11) contains approximately 16 million tweets,
which were distributed by publishing a list of tweet ids that
participants then fetched [7, 6]. The EaaS model enabled
the MB2013 corpus to be much larger, about 243 million
1
2

3.

RUN AVERAGE OVERLAP

Our biggest concern regarding the EaaS model is whether
the common API imposed on participants restricts their retrieval approaches to the extent that different participants
are compelled to submit essentially similar runs. In order
to answer this question, we introduce a novel metric, Run
Average Overlap (RAO), which is a measure of the tendency
of a set of runs to retrieve documents in common.
Run Average Overlap is computed for a given run with
respect to a specific set of runs. In what follows we compute
the RAO score for each run in a pool set with respect to
the pool set. Say that a run O retrieves document d for a
topic t if d occurs in the top K ranks for t in O. When K
is set equal to the pool depth, as done here, O retrieves d
if it contributes d to the pool. Let P be the total number
of participants that have runs in the run set, and Pd be the
number of distinct participants that retrieve d. Finally, let
T be the total number of topics and n be the number of
documents retrieved by O for t. Then:
1 X1X 1
RAO(O) =
T t n
Pd
d

In words, the RAO score for a run is the mean over all topics
of the score computed for individual topics, where the pertopic score is the mean over the documents retrieved by O of
the reciprocal of the number of participants (including itself)
that retrieve that document. The greater the RAO score the
more distinctive the result set. The minimum RAO score is
1
which signifies that O retrieved only documents that all
P
other participants also retrieved. The maximum score is 1.0,
which means that no other participant retrieved any document that O retrieved. Note that document distinctiveness
for the purposes of computing RAO is computed over participants, and thus the number of runs submitted by any given
participant is not an issue.
While RAO scores close to P1 for all runs in a pool set
indicate that the runs are not diverse, the presence of runs
with high scores is not sufficient to conclude that the pool
is appropriately diverse. The RAO score is computed over
all retrieved documents, not relevant documents. Distinctive but ineffective runs are easy to produce (for example,
by selecting random documents from the collection) but unhelpful for pool building. Thus, the RAO score must be
used in conjunction with an effectiveness score to ensure
the presence of distinctive and effective runs in the pool set.
We use R-precision as the effectiveness measure here since it
was the primary metric for the TREC 2013 Microblog track.
Figure 1 shows plots of RAO vs. R-precision for the three
collections. Each point in a graph represents one pool run.
Manual runs (runs for which there was some human processing in the construction of the run) are plotted as open

http://trec.nist.gov
Values reported in Table 1 include only the ad hoc runs.

960

TREC-8

MB2012

MB2013

0.8

0.8

0.8

0.6

0.6

0.6
RAO

1

RAO

1

RAO

1

0.4

0.4

0.4

0.2

0.2

0.2

0

0

0.2

0.4

0.6

0.8

1

0

0

0.2

R-Precision

0.4

0.6

0.8

1

0

0

0.2

R-Precision

0.4

0.6

0.8

1

R-Precision

Figure 1: Run Average Overlap (y-axis) vs. mean R-precision (x-axis) for pool runs. Manual runs are plotted
as open squares and automatic runs as filled circles.
squares while automatic (i.e., non-manual) runs are plotted
as filled circles. A highly distinctive and highly effective run
would fall in the upper right-hand corner of the graph.
The plots for the MB2013 and the TREC-8 collections are
very similar. These collections exhibit a general inverse relationship between effectiveness and distinctiveness (points
run roughly from upper left to lower right). This is simply a manifestation of the fact that there are many fewer
relevant documents than non-relevant documents, and thus
many fewer effective retrieved sets than ineffective retrieved
sets. But, importantly, we see that both the TREC-8 and
MB2013 collections contain outlier runs that are both distinctive and effective—these represent high-quality manual
runs that contributed to the pools.
The graph for the MB2012 collection, which was not constructed using the EaaS model, is different from the others.
The pool runs for MB2012 have a much narrower range of
effectiveness and have a much greater RAO score on average. Note, however, that this is the one collection that does
not have any runs that are outlier effective runs, and overall
effectiveness is relatively poor. Thus, the large RAO scores
are probably another manifestation of the tendency for ineffective runs to be different.

4.

Table 2: Collection characteristics on uniquely retrieved relevant documents and LOU test results.
% of total relevant that are
unique
max % of total uniques retrieved by a single participant
mean % diff in LOU scores
(auto runs only)
max % diff in LOU scores
(auto runs only)
# runs with diff in scores
> 1% (auto only) / total

TREC-8
24.0

MB2012
30.7

MB2013
39.6

42.2

19.7

43.2

0.14

0.64

1.12

0.84

8.36

7.95

0/54

16/112

15/57

when unique relevants are removed falls below the line, and
a run whose MAP score improves when unique relevants are
removed lies above the line.
The presence of distinctive and effective manual runs in
the TREC-8 and MB2013 collections is obvious in the LOU
test results. These runs fall well below the line of equal
scores, and are the only runs to do so. The degradation in
MAP scores is not surprising given that the participants who
submitted these runs contributed large numbers of unique
relevant documents to the pools. As shown in the second
row of Table 2, for these two collections the participant who
submitted the most effective manual runs contributed more
than 40% of the unique relevant documents. In contrast,
the largest percentage of unique relevant documents contributed by a single participant in the MB2012 collection is
just 20%. The percentage of total relevant documents that
are uniquely retrieved relevants is given in the first row of
the table. Here the TREC-8 and MB2013 collections differ:
the MB2013 collection has a much larger percentage of total
relevant documents that are unique.
A large percentage of total relevant documents that are
uniquely retrieved relevant documents is one indication that
a test collection may be less reusable, and the LOU test
results are consistent with this observation. In Figure 2,
all of the automatic runs are on the line of equal scores for
the TREC-8 collection, but there is more movement away
from the line for the two Microblog collections. The final
three rows of Table 2 quantify this movement. The table
gives statistics regarding the distribution of the percentage
difference in MAP scores in the LOU test when considering

UNIQUELY RETRIEVED RELEVANT

In our second analysis, we examined uniquely retrieved
relevant documents, which are relevant documents that were
contributed to the pool by only a single participant. The
leave out uniques test (LOU) [1] gauges the reusability of a
test collection by examining the effect on evaluation scores
of a participant’s runs when the participant’s uniquely retrieved relevant documents are removed from the judgment
set. The test computes the evaluation scores that would
have resulted had the participant not been involved in the
collection building process.
Figure 2 plots the results of the LOU test for each of
the three collections. Each run is plotted as a point, with
manual runs shown as empty boxes and automatic runs as
filled circles. The x-axis is the MAP score of the run as
computed using the official relevance judgment set for that
collection, and the y-axis is the MAP score as computed
using the judgment set with the participant’s unique relevants removed. When both scores are the same, the point
falls on the diagonal line. A run whose MAP score decreases

961

MB2012

MB2013
1

0.8

0.8

0.8

0.6

0.6

0.6

0.4

0.2

0

LOU MAP

1

LOU MAP

LOU MAP

TREC-8
1

0.4

0.2

0

0.2

0.4

0.6

0.8

1

0

0.4

0.2

0

0.2

Original MAP

0.4

0.6

0.8

1

0

Original MAP

0

0.2

0.4

0.6

0.8

1

Original MAP

Figure 2: Results of “leave out uniques” test. Original MAP scores are plotted on the x-axis and LOU MAP
scores on the y-axis. Manual runs are plotted as open squares and automatic runs as filled circles.
only automatic runs whose original MAP scores were at least
0.1 (because percentage differences are artificially magnified
for ineffective runs). The statistics computed are the mean
of the percentage difference over these automatic runs, the
maximum percentage difference observed in the automatic
runs, and the count of the number of runs that have a percentage difference greater than 1% (out of the total). No
TREC-8 automatic run has a percentage difference greater
than 1%, and since differences that small are within the level
of evaluation noise [9], the collection is regarded as highly
reusable. The MB2013 collection has a mean percentage difference of about 1%, with 15 runs (out of 57) having at least
a 1% difference and a maximum difference of about 8%.
Note that this amount of change in LOU test results is not
a consequence of using the EaaS model. In fact, the large
percentage of relevant documents that are unique is actually
confirmation that the evaluation method accommodated distinctive runs (i.e., the API did not hamper researchers’ ability to generate runs that are both effective and distinctive),
and the MB2012 collection, which was not built using EaaS,
has similar characteristics. The likely explanation is the size
of pools relative to the size of the corpus [1]. Corpora used
in the two Microblog collections are much larger than the
TREC-8 collection, so the judgment pools represent a much
smaller percentage of the collection than for the TREC-8
collection. Also note that this level of change is unlikely
to have much impact in the utility of the collection as a
research tool. As can be seen in Figure 2, the relative ordering of runs is largely stable even in the presence of MAP
score differences. At worst, researchers who encounter many
unjudged tweets in top ranks in new runs should be more
cautious in their conclusions.

5.

from TREC-8, but both collections strongly benefit from the
presence of distinctive and effective manual runs.
Of course, these findings are for a single collection and a
specific API. This implementation provides a generous number of documents in response to a query and a generous allotment of queries per participant. These choices contribute to
good design, as a more restrictive API would likely have impacted submitted runs and the resulting test collection negatively. Although we cannot yet make statements about the
EaaS model in general, the TREC 2013 Microblog collection
does offer an existence proof that a high-quality retrieval test
collection can be constructed using this new method.

6.

ACKNOWLEDGMENTS

This work has been supported in part by NSF under awards
IIS-1217279 and IIS-1218043. Any opinions, findings, conclusions, or recommendations expressed are the authors’ and
do not necessarily reflect those of the sponsor.

7.

REFERENCES

[1] C. Buckley, D. Dimmick, I. Soboroff, and E. Voorhees. Bias
and the limits of pooling for large collections. Information
Retrieval, 10:491–508, 2007.
[2] D. Harman. Information Retrieval Evaluation. Morgan &
Claypool Publishers, 2011.
[3] J. Lin and M. Efron. Evaluation as a service for information
retrieval. ACM SIGIR Forum, 47(2):8–14, 2013.
[4] J. Lin and M. Efron. Overview of the TREC-2013
microblog track. In TREC, 2013.
[5] C. D. Manning, P. Raghavan, and H. Schütze. Evaluation
in information retrieval. In Introduction to Information
Retrieval, chapter 8, pages 153–154. Cambridge University
Press, 2009.
[6] R. McCreadie, I. Soboroff, J. Lin, C. Macdonald, I. Ounis,
and D. McCullough. On building a reusable Twitter corpus.
In SIGIR, 2012.
[7] I. Ounis, C. Macdonald, J. Lin, and I. Soboroff. Overview
of the TREC-2011 microblog track. In TREC, 2011.
[8] K. Sparck Jones and C. van Rijsbergen. Report on the need
for and provision of an “ideal” information retrieval test
collection. British Library Research and Development
Report 5266, 1975.
[9] E. M. Voorhees. The philosophy of information retrieval
evaluation. In CLEF, 2002.
[10] J. Zobel. How reliable are the results of large-sacle
information retrieval experiments? In SIGIR, 1998.

CONCLUSION

The evaluation as a service model for constructing retrieval test collections can offer significant advantages over
traditional construction techniques, but only if it does not
fundamentally hamper innovation and it leads to highquality resources. This paper examined the one collection
that has been built using the EaaS model to date, the TREC
2013 Microblog collection, and found its run diversity to be
similar to the high-quality TREC-8 ad hoc collection. The
results of the leave out uniques test suggest that pools from
the Microblog 2013 collection are less complete than pools

962

