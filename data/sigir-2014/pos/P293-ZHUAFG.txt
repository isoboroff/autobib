Learning for Search Result Diversification
Yadong Zhu

Yanyan Lan

Jiafeng Guo

Xueqi Cheng

Shuzi Niu

Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China

{zhuyadong, niushuzi}@software.ict.ac.cn
{lanyanyan, guojiafeng, cxq}@ict.ac.cn

ABSTRACT

1.

Search result diversiﬁcation has gained attention as a way
to tackle the ambiguous or multi-faceted information needs
of users. Most existing methods on this problem utilize a
heuristic predeﬁned ranking function, where limited features
can be incorporated and extensive tuning is required for different settings. In this paper, we address search result diversiﬁcation as a learning problem, and introduce a novel
relational learning-to-rank approach to formulate the task.
However, the deﬁnitions of ranking function and loss function for the diversiﬁcation problem are challenging. In our
work, we ﬁrstly show that diverse ranking is in general a
sequential selection process from both empirical and theoretical aspects. On this basis, we deﬁne ranking function as
the combination of relevance score and diversity score between the current document and those previously selected,
and loss function as the likelihood loss of ground truth based
on Plackett-Luce model, which can naturally model the sequential generation of a diverse ranking list. Stochastic gradient descent is then employed to conduct the unconstrained
optimization, and the prediction of a diverse ranking list
is provided by a sequential selection process based on the
learned ranking function. The experimental results on the
public TREC datasets demonstrate the eﬀectiveness and robustness of our approach.

Most users leverage Web search engine as a predominant
tool to fulﬁll their information needs. Users’ information
needs, typically described by keyword based queries, are often ambiguous or multi-faceted. On the one hand, for some
ambiguous queries, there are multiple interpretations of the
underlying needs (e.g., query “band” may refer to the rock
band, frequency band or rubber band). On the other hand,
queries even with clear deﬁnition might still be multi-faceted
(e.g., “britney spears”), in the sense that there are many aspects of the information needs (e.g., news, videos, photos of
britney spears). Therefore, search result diversiﬁcation has
attracted considerable attention as a means to tackle the
above problem [1]. The key idea is to provide a diversiﬁed
result list, in the hope that diﬀerent users will ﬁnd some
results that can cover their information needs.
Diﬀerent methods on search result diversiﬁcation have
been proposed in literature, which are mainly non-learning
methods, and can be divided into two categories: implicit
methods and explicit methods. Implicit methods [3] assume
that similar documents cover similar aspects, and rely on
inter-document similarity for selecting diverse documents.
While explicit methods [29] directly model the aspects of
user queries and select documents that cover diﬀerent aspects for diversiﬁcation. However, most existing methods
utilize a heuristic predeﬁned utility function, and thus limited features can be incorporated and extensive tuning is
required for diﬀerent retrieval settings.
In this paper, we address search result diversiﬁcation as
a learning problem where a ranking function is learned for
diverse ranking. Diﬀerent from traditional relevance ranking based on the assumption of independent document relevance [17], diverse ranking typically considers the relevance
of a document in light of the other retrieved documents
[29]. Therefore, we introduce a novel Relational Learningto-Rank framework (R-LTR for short) to formulate the task
of search result diversiﬁcation. R-LTR considers the interrelationships between documents in the ranking process, besides the content information of individual documents used
in traditional learning-to-rank framework. However, the definitions of ranking function and loss function for the diversiﬁcation problem are challenging.
From the top-down user browsing behavior and the ubiquitous greedy approximation for diverse ranking, we ﬁnd
that search result diversiﬁcation is in general a sequential
ranking process. Therefore, we propose to deﬁne the ranking function and loss function in a sequential way: (1) The
ranking function is deﬁned as the combination of relevance

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information
Search and Retrieval – Retrieval Models

General Terms
Algorithms, Experimentation, Performance, Theory

Keywords
Diversity, Relational Learning-to-Rank, Sequential Selection,
Plackett-Luce Model
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00.
http://dx.doi.org/10.1145/2600428.2609634 .

293

INTRODUCTION

score and diversity score, where the relevance score only depends on the content of the document, and the diversity
score depends on the relationship between the current document and those previously selected. We describe diﬀerent
ways to represent the diversity score. (2) The loss function
is deﬁned as the likelihood loss of ground truth based on
Plackett-Luce model [18], which can naturally model the sequential generation of a diverse ranking list. On this basis,
stochastic gradient descent is employed to conduct the unconstrained optimization, and the prediction of diverse ranking list is provided by a sequential selection process based
on the learned ranking function.
To evaluate the eﬀectiveness of the proposed approach, we
conduct extensive experiments on the public TREC datasets.
The experimental results show that our methods can signiﬁcantly outperform the state-of-the-art diversiﬁcation approaches, with Oﬃcial Diversity Metrics (ODM for short) of
TREC diversity task including ERR-IA[1, 6], α-N DCG[11]
and N RBP [12]. Furthermore, our methods also achieve best
in the evaluations of traditional intent-aware measures such
as Precision-IA [1] and Subtopic Recall [37]. In addition, we
give some discussions on the robustness of our methods and
the importance of the proposed diversity features. Finally,
we also study the eﬃciency of our approach based on the
analysis of running time.
The main contributions of this paper lie in:

29], or multiple external resources [15]. Overall, the explicit
methods have shown better experimental performances comparing with implicit methods.
There are also some other methods which attempt to borrow theories from economical or political domains. The work
in [26, 33] applies economical portfolio theory for search result ranking, which views search diversiﬁcation as a means of
risk minimization. The approach in [13] treats the problem
of ﬁnding a diverse search result as ﬁnding a proportional
representation for the document ranking, which is like a critical part of most electoral processes.
The authors of [2, 27] try to construct a dynamic rankedretrieval model, while our paper focuses on the common
static ranking scenario. There are also some on-line learning methods that try to learn retrieval models by exploiting
users’ online feedback [25, 31, 35, 30, 28]. These research
work can tackle diversity problem to some extent, but they
focus on an ‘on-line’ or ‘coactive’ scenario, which is diﬀerent
from our work (i.e. oﬄine supervised learning scenario).
Recently, some researchers have proposed to utilize machine learning techniques to solve the diversiﬁcation problem. Yue et al. [36] propose to optimize subtopic coverage
as the loss function, and formulate a discriminant function
based on maximizing word coverage. However, their work
only focuses on diversity, and discards the requirements of
relevance. They claim that modeling both relevance and diversity simultaneously is a more challenging problem, which
is exactly what we try to tackle in this paper. In this paper, we propose a novel R-LTR approach to conduct search
result diversiﬁcation, which is diﬀerent from traditional approaches and shows promising experimental performance.

1. the proposal of a novel R-LTR framework to formulate search result diversiﬁcation as a learning problem, where both content information and relationship
among documents are considered;
2. the new deﬁnitions of ranking function and loss function based on the foundation of sequential selection
process for diverse ranking;

3.

3. an empirical veriﬁcation of the eﬀectiveness of the proposed approach based on public datasets.
The rest of the paper is organized as follows. We ﬁrst
review some related work in Section 2. We then introduce
the R-LTR framework in Section 3, and describe the speciﬁc
deﬁnitions of ranking function and loss function, learning
and prediction procedures in Section 4. Section 5 presents
the experimental results. Section 6 concludes the paper.

2. RELATED WORK
Most existing diversiﬁcation methods are non-learning methods, which can be mainly divided into two categories: implicit approaches and explicit approaches.
The implicit methods assume that similar documents cover
similar aspects and model inter-document dependencies. For
example, Maximal Marginal Relevance (MMR) method [3]
proposes to iteratively select a candidate document with the
highest similarity to the user query and the lowest similarity to the already selected documents, in order to promote
novelty. In fact, most of the existing approaches are somehow inspired by the MMR method. Zhai et al. [37] select
documents with high divergence from one language model
to another based on the risk minimization consideration.
The explicit methods explicitly model aspects of a query
and then select documents that cover diﬀerent aspects. The
aspects of a user query can be achieved with a taxonomy [1,
32], top retrieved documents [5], query reformulations [24,

294

RELATIONAL LEARNING-TO-RANK

Traditional relevance ranking has been well formulated as
a learning-to-rank (LTR for short) problem [17], where a
ranking function is deﬁned on the content of each individual
document and learned toward some loss functions. However,
in diverse ranking scenario, the overall relevance of a document ranking for a given query, should depend not only
on the individual ranked documents, but also on how they
related to each other [29]. Therefore, in this paper, we introduce a novel R-LTR framework to formulate the diverse
ranking problem. The diﬀerence between LTR and R-LTR
is that the latter considers both contents of individual document and relations between documents. In the following
paper, we use superscript to denote the id of a query and
subscript to denote the id of a document.
Formally, let X = {x1 , · · · , xn }, where xi denotes the d
dimensional feature vector of a candidate document xi for
query q; Let R ∈ Rn×n×l denote a 3-way tensor representing
relationships between the n documents, where Rijk stands
for the k-th feature of relation between documents xi and
xj . Let y be a ground-truth of the query q, in the form of
a vector of ranking scores or a ranking list. Supposing that
f(X, R) is a ranking function, and the goal of R-LTR is to
output the best ranking function from a function space F .
In training procedure, given the labeled data with N queries
as: (X (1) , R(1) , y(1) ), (X (2) , R(2) , y(2) ), · · · , (X (N ) , R(N ) , y(N ) ).
A loss function L is deﬁned, and the learning process is conducted by minimizing the total loss with respect to the given

training data.
f̂ = arg min
f∈F

N
∑

L(f(X

(i)

,R

(i)

(i)

), y ).

(1)

X

S

d11
d22

i=1

d3

In prediction, given X (t) and R(t) of nt documents for
query qt , we output ŷ(t) based on the learned ranking function f̂(X (t) , R(t) ).
In fact, the proposed R-LTR framework is very general,
in the sense that many traditional ranking problems are its
special cases.
(1) It is obvious to see that the conventional LTR framework is a special case of R-LTR. Speciﬁcally, if we ignore the
relation tensor R, then we get the same function as that in
traditional LTR, i.e. f(X, R) = f(X).
(2) The ‘learning to rank relational objects’ framework
[22, 23] is also a special case of R-LTR. Speciﬁcally, if we
restrict the relation tensor R to be a matrix, with Rij representing the relation between document xi and xj , then we
get the same function as that in the problem of learning to
rank relational objects.
The above framework gives a formulation of ranking problems involving relationship. When solving the speciﬁc problem, one needs to deﬁne the corresponding ranking function
and loss function according to the task.

Diversity

Diversity

d7
d8

d6

d5

Relevance
Relevance

X\S
d4

Figure 1: An illustration of the sequential way to deﬁne ranking function. All the rectangles represent
candidate documents of a user query, and diﬀerent
colors represent diﬀerent subtopics. The solid rectangle is relevant to the query, and the hollow rectangle is irrelevant to the query, and larger size means
more relevance. X denotes all the candidate document collection. S denotes previously selected documents, and X\S denotes the remanent documents.

4. SEARCH RESULT DIVERSIFICATION
VIA R-LTR FRAMEWORK
As mentioned in the previous section, it is natural to formulate search result diversiﬁcation under R-LTR framework.
In this paper, we mainly focus on the diverse ranking scenario. To apply the above framework to this speciﬁc task,
the most challenging problem is the deﬁnition of ranking
function and loss function.

document and the previously selected documents in S should
be considered. Noting that larger size of the rectangle means
the document is more relevant to the query, and diﬀerent colors represent diﬀerent subtopics. Therefore, the document
8 may be more preferred than document 4 given S, since it
is relevant to the query, and also provides diﬀerent aspects
additionally comparing with the selected set S.
Based on this ranking process, here we give the precise
deﬁnition of ranking function. Given a query q, we assume
that a set of documents have been selected, denoted as S,
the scoring function on the candidate document in X\S, is
then deﬁned as the combination of the relevance score and
the diversity score between the current document and those
previously selected, shown as follows.

4.1 Motivation
In order to properly deﬁne the ranking function and loss
function, we ﬁrst look into the diverse ranking problem.
(1) Empirically, users usually browse the Web search results in a top-down manner, and perceive diverse information from each individual document based on what he/she
have obtained in the preceding results [8].
(2) Theoretically, diverse ranking can be naturally stated
as a bi-criterion optimization problem, and it is NP-hard
[1, 4]. Therefore, in practice, most previous approaches on
search result diversiﬁcation are based on greedy approximation, which sequentially select a ‘local-best’ document from
the remanent candidate set [29].
From both empirical and theoretical analysis above, we
can see that it is better to view diverse ranking as a sequential selection process, in the sense that the ranking list is
generated in a sequential order, with each individual document ranked according to its relevance to the query and the
relation between all the documents ranked before it.

fS (xi , Ri ) = ωrT xi + ωdT hS (Ri ), ∀xi ∈ X\S,

(2)

where xi denotes the relevance feature vector of the candidate document xi , Ri stands for the matrix of relationships between document xi and other selected documents,
with each Rij stands for the relationship vector between
document xi and xj , represented by the feature vector of
(Rij1 , · · · , Rijl ), xj ∈ S, and Rijk stands for the k-th relation feature between documents xi and xj . hS (Ri ) stands
for the relational function on Ri , ωrT and ωdT stands for the
corresponding relevance and diversity weight vector. When
S = ∅, fS (xi , Ri ) is directly represented as ωrT xi . Then the
ranking function can be represented as the set of scoring
function:

4.2 Definition of Ranking Function
As discussed above, diverse ranking is in general a sequential selection process, where each individual document
is ranked according to its relevance to the query and the
relation between all the documents ranked before it. The
intuitive idea is illustrated in Figure 1, when ranking documents in X\S given the already ranked results S, both
content-based relevance and diversity relation between this

f(X, R) = (fS∅ , fS1 , · · · , fSn−1 )
where Si , denotes the previously selected document collection with i documents. From the above deﬁnition, we can
see that if we do not consider diversity relation, our ranking

295

function reduce to f(X) = (f (x1 ), · · · , f (xn )), which is the
traditional ranking function in learning-to-rank.

Title Diversity. The way of computing title diversity
feature is similar as that for text diversity feature, which is
denoted as Rij3 .
Anchor Text Diversity. The anchor text can accurately
describe the content of corresponding page and is important.
This type of feature is computed similarly as text and title
diversity features, denoted as Rij4 .
ODP-Based Diversity. The existing ODP taxonomy1
oﬀers a succinct encoding of distances between documents.
Usually, the distance between documents on similar topics
in the taxonomy is likely to be small. For two categories u
and v, we deﬁne the categorical distance between them as
following:

4.2.1 Relational function hS (Ri )
Please note that the relational function hS (Ri ) denotes
the way of representing the diversity relationship between
the current document xi and the previously selected documents in S. If we treat diversity relation as distance, hS (Ri )
can be viewed as the distance of xi to the set S. According
to diﬀerent deﬁnitions of the distance between an item and
a set of items, hS (Ri ) can be deﬁned as the following three
ways.
Minimal Distance. The distance between a document
xi and a set S is deﬁned as the minimal distance of all the
document pairs (xi , xj ), xj ∈ S.

c dis(u, v) = 1 −

hS (Ri ) = ( min Rij1 , · · · , min Rijl ).
xj ∈S

where l(u, v) is the length of their longest common preﬁx.
|u| and |v| is the length of category u and v. Then given two
documents xi and xj and their category information sets
Ci and Cj respectively, we deﬁne the ODP-based diversity
feature as:
∑
∑
u∈Ci
v∈Cj c dis(u, v)
Rij5 =
|Ci | · |Cj |

xj ∈S

Average Distance. The distance between a document
xi and a set S is deﬁned as the average distance of all the
document pairs (xi , xj ), xj ∈ S.
1 ∑
1 ∑
hS (Ri ) = (
Rij1 , · · · ,
Rijl ).
|S| x ∈S
|S| x ∈S
j

j

Maximal Distance. The distance between a document
xi and a set S is deﬁned as the maximal distance of all the
document pairs (xi , xj ), xj ∈ S.

where |Ci | and |Cj | are the number of categories in corresponding category sets.
Link-Based Diversity. By constructing a web link graph,
we can calculate the link similarity of any document pair
based on direct inlink or outlink information. The link-based
diversity feature is then deﬁned as follows.
{
0 if xi ∈ inlink(xj ) ∪ outlink(xj ),
Rij6 =
1 otherwise

hS (Ri ) = (max Rij1 , · · · , max Rijl ).
xj ∈S

xj ∈S

4.2.2 Diversity Feature Vector Rij
How to deﬁne discriminative features that can well capture diversity relation is critical for the success of R-LTR.
In this work, we provides several representative features for
the learning process, including semantic diversity features
(i.e. subtopic diversity, text diversity, title diversity, anchor
text diversity and ODP-based diversity) and structural diversity features (i.e. link-based diversity and url-based diversity).
Subtopic Diversity. Diﬀerent documents may associate
with diﬀerent aspects of the given topic. We use Probabilistic Latent Semantic Analysis (PLSA) [16] to model implicit subtopics distribution of candidate objects, which is
important for the diversiﬁcation task as mentioned before.
Therefore, we deﬁne the diversity feature based on implicit
subtopics as follows.

Rij1

|l(u, v)|
max{|u|, |v|}

URL-Based Diversity. Given the url information of
two documents, we can judge whether they belong to the
same domain or the same site. The url-based diversity feature is then deﬁned as follows.


if one url is another’s prefix
0
Rij7 = 0.5 if they belong to the same site or domain

1
otherwise
Based on these diversity features, we can obtain the diversity feature vector Rij = (Rij1 , Rij2 , · · · , Rij7 ). All the
feature values are normalized to the range of [0,1]. Please
note that there might be some other useful resources for the
deﬁnition of diversity features, e.g., clickthrough logs, which
will be further considered in our future work.

v
um
u∑
=t
(p(zk |xi ) − p(zk |xj ))2
k=1

4.3

Text Diversity. Text dissimilarity is also meaningful for
diversity. We propose to represent it as the cosine dissimilarity based on weighted term vector representations, and
deﬁne the feature as follows.
di · dj
,
Rij2 = 1 −
∥di ∥∥dj ∥

Definition of Loss Function

Motivated by the analysis that the process for diverse
ranking is in general a sequential selection process, we propose to model the generation of a diverse ranking list in a
sequential way, and deﬁne the loss function as the likelihood
loss of the generation probability.
L(f(X, R), y) = − log P (y|X).

where di , dj are the weighted document vectors based on tf ∗
idf , and tf denotes the term frequencies, idf denotes inverse
document frequencies. There also exists other computing
ways such as the work in [14], which is based on sketching
algorithm and Jaccard similarity.

(3)

Intuitively, the generation probability of a ranking list can
be viewed as a process to iteratively select the top ranked
1

296

http://www.dmoz.org/

Algorithm 1 Construction of Approximate Ideal
Ranking List
Input:
(i)
(i)
(qi , X (i) , Ti , P (xj |t)), t ∈ Ti , xj ∈ X (i)
(i)
Output: y
1: Initialize S0 ← ∅, y(i) = (1, · · · , ni )
2: for k = 1, ..., ni do
3:
bestDoc ← argmaxx∈X (i) \Sk−1 ODM (Sk−1 ∪ x)
4:
Sk ← Sk−1 ∪ bestDoc
5:
y (i) (k) = the index of bestDoc
6: end for
7: return y(i) = (y (i) (1), · · · , y (i) (ni )).

documents from the remaining documents. The precise definition is given as follows.
P (y|X) = P (xy(1) , xy(2) , · · · , xy(n) |X)

(4)

= P (xy(1) |X)P (xy(2) |X\S1 ) · · · P (xy(n−1) |X\Sn−2 ),
where y(i) stands for the index of document which is ranked
in position i in the ranking list y, X denotes all the candidate
documents, Si = {xy(1) , · · · , xy(i) }, denotes the previously
selected document collection with i documents, P (xy(1) |X)
stands for the probability that xy(1) is ranked ﬁrst among
the documents in X, and P (xy(j) |X\Sj−1 ) stands for the
probability that document xy(j) is ranked ﬁrst among the
documents in X\Sj−1 .

Algorithm 2 Optimization Algorithm

4.3.1 Plackett-Luce based Probability P (y|X)

Input: training data {(X (i) , R(i) , y(i) )}N
i=1 ,
parameter: learning rate η, tolerance rate ϵ
Output: model vector: ωr , ωd
1: Initialize parameter value ωr , ωd
2: repeat
3:
Shuﬄe the training data
4:
for i = 1, ..., N do
5:
Compute gradient ∆ωr (i) and ∆ωd (i)
6:
Update model: ωr = ωr − η × ∆ωr (i) ,
ωd = ωd − η × ∆ωd (i)
7:
end for
8:
Calculate likelihood loss on the training set
9: until the change of likelihood loss is below ϵ

The above sequential deﬁnition approach can be well captured by the Plackett-Luce Model [18]. Therefore, we propose to deﬁne P (xy(1) |X) and P (xy(j) |X\Sj−1 ) in a similar
way, shown as follows, j ≥ 2.
exp{f∅ (xy(1) )}
P (xy(1) |X) = ∑n
,
k=1 exp{f∅ (xy(k) )}
exp{fSj−1 (xy(j) , Ry(j) )}
.
P (xy(j) |X\Sj−1 ) = ∑n
k=j exp{fSk−1 (xy(k) , Ry(k) )}

(5)

(6)

Incorporating Eq.(5) and Eq.(6) into Eq.(4), the generation
probability of a diverse ranking list is formulated as follows.
P (y|X) =

n
∏
j=1

exp{fSj−1 (xy(j) , Ry(j) )}
∑n
,
k=j exp{fSk−1 (xy(k) , Ry(k) )}

(7)

4.4.1

Training Data

The labeled data in search result diversiﬁcation such as
TREC diversity task are usually provided in the form of
(i)
(i)
(qi , X (i) , Ti , P (xj |t)), t ∈ Ti , xj ∈ X (i) , where X (i) is a
candidate document set of query qi , Ti is the subtopics of
(i)
query qi , t is a speciﬁc subtopic in Ti , and P (xj |t) describes

where S0 = ∅, f∅ (x, R) = ωrT x.

4.3.2 Relation to ListMLE in Learning-to-Rank
Incorporating Eq.(7) into the deﬁnition of the loss function Eq.(3), we can obtain the precise deﬁnition of the loss
function as follows.
{
}
n
∑
exp{fSj−1 (xy(j) , Ry(j) )}
L(f(X,R), y) = − log ∑n
(8)
k=j exp{fSk−1(xy(k) ,Ry(k))}
j=1

(i)

the relevance of document xj to subtopic t. We can see that
the above form of labeled data deviates the formulation of
y(i) in our R-LTR framework, which requires a ranking list
of candidate documents. In order to apply R-LTR, we need
to construct y(i) from the provided form of labeled data.
We propose to construct an approximate ideal ranking list
by maximizing the ODM measures (e.g., ERR-IA), and use
the approximate ideal ranking list as the training groundtruth y(i) for query qi , as described in Algorithm 1.
According to the results in [20], if a submodular function is monotonic (i.e., f (S) ≤ f (T ), whenever S ⊆ T ) and
normalized (i.e., f (∅) = 0), greedily constructing gives an
(1 − 1/e)-approximation to the optimal. Since any member
of ODM is a submodular function, we can easily prove that
Algorithm 1 is (1 − 1/e)-approximation to the optimal (We
omit the proof here). And the quality of training groundtruth can be guaranteed.

We can see that our loss function is similar to that in
ListMLE [34], which is formulated as follows.
{
}
n
∑
exp{f (xy(j) )}
L(f(X), y) = −
log ∑n
,
k=j exp{f (xy(k) )}
j=1
where f (x) is the score function in traditional learning-torank, i.e. f (x) = ω T x.
Therefore, if we do not consider diversity relation in our
framework, our loss function will reduce to the same form of
that in ListMLE. That is to say, ListMLE is a special case
of our loss function.

4.4.2

4.4 Learning and Prediction

Learning

Given the training data {(X (i) , R(i) , y(i) )}N
i=1 , the total
loss is represented as follows.


(i)
(i)
ni
N ∑
 exp{ωrT xy(j) + ωdT hS (i) (Ry(j) )} 
∑
j−1
(9)
−
log ∑n
(i)
i
T x(i) + ω T h


exp{ω
r
i=1 j=1
d S (i) (Ry(k) )}
k=j
y(k)

Based on the deﬁnitions of ranking function and loss function, we present the learning and prediction process in this
section. Speciﬁcally, we ﬁrst describe how to construct the
training data, and then introduce the optimization procedure. Finally, we show how to make predictions based on
the learned ranking function.

k−1

297

Algorithm 3 Ranking Prediction via Sequential Selection
Input: X (t) , R(t) , ωr , ωd
Output: y(t)
1: Initialize S0 ← ∅, y(t) = (1, · · · , nt )
2: for k = 1, ..., nt do
3:
bestDoc ← argmaxx∈Xt fSk−1 (x, R)
4:
Sk ← Sk−1 ∪ bestDoc
5:
y (t) (k) ← the index of bestDoc
6: end for
7: return y(t) = (y (t) (1), · · · , y (t) (nt ))

diversity methods and the importance of our proposed diversity features. Finally, we study the eﬃciency of our approach based on the analysis of running time.

5.1

Here we give some introductions on the experimental setup,
including data collections, evaluation metrics, baseline models and detailed implementation.

5.1.1

 ∑n
(i)
(i)
T (i)
T
i
ni 
 k=j xy(k) exp{ωr xy(k) + ωd hS (i) (Ry(k) )}
∑
k−1
=
 ∑ni exp{ω T x(i) + ω T h (i) (R(i) )}
j=1 
r y(k)
k=j
d
y(k)
Sk−1

(i)
xy(j)

−

(i)
exp{ωrT xy(j)

+ ωdT h

(i)
Sj−1

(i)
Sj−1

∑n
i

ni 

∑
(i)
∆ωd =

j=1

h

k=j

(i)

(i)

Sk−1

−

(i)

exp{ωrT xy(k) + ωdT h

(Ry(k) )}

(Ry(k) )}

(i)

(Ry(j) ) exp{ωrT xy(j) + ωdT h

(i)

Sj−1

(i)

exp{ωrT xy(j) + ωdT h

,

(i)

(i)

Sk−1

(i)

(i)

Sj−1




(i)

(i)

Sk−1

(i)

k=j

h

(Ry(j) )}

(Ry(k) ) exp{ωrT xy(k) + ωdT h

∑ni

5.1.2


(i)
(Ry(j) )} 

(i)

(i)

exp{ωrT xy(j) + ωdT h

(i)

(i)

Sj−1


(i)
(Ry(j) )} 


(Ry(j) )}




Data Collections

Our evaluation was conducted in the context of the diversity tasks of the TREC2009 Web Track (WT2009), TREC2010
Web Track (WT2010), and TREC2011 Web Track (WT2011),
which contain 50, 48 and 50 test queries (or topics), respectively. Each topic includes several subtopics identiﬁed by
TREC assessors, with binary relevance judgements provided
at the subtopic level2 . All the experiments were carried out
on the ClueWeb09 Category B data collection3 , which comprises a total of 50 million English Web documents.

For such a unconstrained optimization problem, we employ Stochastic Gradient Descent (SGD) to conduct optimization as shown in Algorithm 2. According to Eq.(9), the
gradient at training sample X (i) is computed as follows.
(i)
∆ωr

Experimental Setup

.

The current oﬃcial evaluation metrics of the diversity task
include ERR-IA [6], α-N DCG [11] and N RBP [12]. They
measure the diversity of a result list by explicitly rewarding
novelty and penalizing redundancy observed at every rank.
We also use traditional diversity measures for evaluation:
Precision-IA [1] and Subtopic Recall [37].They measure the
precision across all subtopics of the query and the ratio of
the subtopics covered in the results, respectively. All the
measures are computed over the top-k search results (k =
20). Moreover, the associated parameters α and β are all
set to be 0.5, which is consistent with the default settings in
oﬃcial TREC evaluation program.

5.1.3

4.4.3 Prediction

Evaluation Metrics

Baseline Models

To evaluate the performance of our approach, we compare
our approach with the state-of-the-art approaches, which are
introduced as follows.
QL. The standard Query-likelihood language model is
used for the initial retrieval, which provides the top 1000
retrieved documents as a candidate set for all the diversiﬁcation approaches. It is also used as a basic baseline method
in our experiment.
MMR. MMR is a classical implicit diversity method in
the diversity research. It employs a linear combination of
relevance and diversity as the metric called “marginal relevance” [3]. MMR will iteratively select document with the
largest “marginal relevance”.
xQuAD. The explicit diversiﬁcation approaches are popular in current research ﬁeld, in which xQuAD is the most
representative and used as a baseline model in our experiments [29].
PM-2. PM-2 is also a explicit method that proposes to
optimize proportionality for search result diversiﬁcation [13].
It has been proved to achieve promising performance in their
work, and is also chosen as baseline in our experiment.

As the ranking function is deﬁned sequentially, traditional
prediction approach (i.e., calculating the ranking score of
each independent document simultaneously and sorting them
in descending order to obtain a ranking list) fails in our
framework. According to the sequential selection essence of
diverse ranking, we propose a sequential prediction process,
as described in Algorithm 3. Speciﬁcally, in the ﬁrst step,
the most relevant document with maximal relevance score
will be selected and ranked ﬁrst. If the top k items have
been selected, then the document in position k + 1 should
be with maximum fSk . At last, all the documents are ranked
accordingly, and we obtain the ﬁnal ranking list.
Assuming that the size of output ranking is K, the size
of candidate set is n, then this type of sequential selection
algorithm 3 will have time complexity of O(n ∗ K). Usually,
the original value of n is large, therefore, an initial retrieval
can be applied to provide a ﬁltered candidate set with relatively small size (e.g., top 1000 or 3000 retrieved documents).
With a small K, the prediction time is linear.

5. EXPERIMENTS
In this section, we evaluate the eﬀectiveness of our approach empirically. We ﬁrst introduce the experimental
setup. We then compare our approach with baseline methods under diﬀerent diversity evaluation measures. Furthermore, we analyze the performance robustness of diﬀerent

2
For WT2011 task, assessors made graded judgements.
While in the oﬃcial TREC evaluation program, it mapped
these graded judgements to binary judgements by treating
values > 0 as relevant and values ≤ 0 as not relevant.
3
http://boston.lti.cs.cmu.edu/Data/clueweb09/

298

late their best-case scenarios, and uniform probability for
all subtopics is assumed as [29, 13].
For ListMLE and SVMDIV, we utilize the same training
data generated by Algorithm 1 to train their model, and
also conduct 5-fold cross validation. ListMLE adopts the
relevance features summarized in Table 1. SVMDIV adopts
the representative word level features with diﬀerent importance criterion, as listed in their paper and released code
[36]. As described in above subsection, SVMDIV will rerank top-K retrieved documents returned by ListMLE. We
test K ∈ {30, 50, 100}, and ﬁnd it performs best at K = 30.
Therefore, the following results of SVMDIV are achieved
with K = 30.
For our approach, the learning rate η parameter in Algorithm 2 is chosen from 10−7 to 10−1 , and the best learning
rate is obtained based on the performance of validation set.

Table 1:
Relevance Features for learning on
ClueWeb09-B collection [21, 19].
Category Feature Description Total
Q-D
TF-IDF
5
Q-D
BM25
5
Q-D
QL.DIR
5
Q-D
MRF
10
D
PageRank
1
D
#Inlinks
1
D
#Outlinks
1

ListMLE. ListMLE is a plain learning-to-rank approach
without diversiﬁcation considerations, and is a representative listwise relevance approach in LTR ﬁeld [17].
SVMDIV. SVMDIV is a representative supervised approach for search result diversiﬁcation [36]. It proposes
to optimize subtopic coverage by maximizing word coverage. It formulates the learning problem and derives a training method based on structural SVMs. However, SVMDIV
only models diversity and discards the requirement of relevance. For fair performance comparison, we will ﬁrstly apply
ListMLE to do the initial ranking to capture relevance, and
then use SVMDIV to re-rank top-K retrieved documents to
capture diversity.
The above three diversity baselines: MMR, xQuAD and
PM-2, all require a prior relevance function to implement
their diversiﬁcation steps. In our experiment, we choose
ListMLE as the relevance function to implement them, and
denote them as: MMRlist , xQuADlist and PM-2list , respectively.
According to the diﬀerent ways in deﬁning the relational
function hS (Ri ) in section 4.2.1, our R-LTR diversiﬁcation
approach has three variants, denoted as R-LTRmin , R-LTRavg
and R-LTRmax , respectively.

5.2
5.2.1

Evaluation on Official Diversity Metrics

We now compare our approaches to the baseline methods
on search result diversiﬁcation. The results of performance
comparison are shown in Table 2, 3, and 4. We also present
the performance of top performing systems on CategoryB reported by TREC [7, 10, 9], which are just taken as
indicative references. The number in the parentheses are the
relative improvements compared with the baseline method
QL. Boldface indicates the highest scores among all runs.
From the results we an see that, our R-LTR outperform
the plain LTR approach without diversiﬁcation consideration, i.e. ListMLE, which can be viewed as a special case
of our approach. Speciﬁcally, the relative improvement of
R-LTRmin over ListMLE is up to 41.87%, 49.71%, 29.17%,
in terms of ERR-IA on WT2009, WT2010, and WT2011,
respectively. It indicates that our approach can tackle multicriteria ranking problem eﬀectively, with the consideration
of both content-based information and diversity relationship
among candidate objects.
Regarding the comparison among representative implicit
and explicit diversiﬁcation approaches, explicit methods (i.e.
xQuAD and PM-2) show better performance than the implicit method (i.e. MMR) in terms of all the evaluation
measures. MMR is the least eﬀective due to its simple
predeﬁned “marginal relevance”. The two explicit methods
achieve comparable performance: PM-2list wins on WT2010
and WT2011, while xQuADlist wins on WT2009, but their
overall performance diﬀerences are small.
Furthermore, our approach outperforms the state-of-theart explicit methods in terms of all the evaluation measures.
For example, with the evaluation of ERR-IA, the relative
improvement of R-LTRmin over the xQuADlist is up to
17.18%, 11.26%, 13.38%, on WT2009, WT2010, WT2011,
respectively, and the relative improvement of R-LTRmin over
the PM-2list is up to 18.31%, 10.65%, 10.59% on WT2009,
WT2010, WT2011, respectively. Although xQuADlist and
PM-2list all utilize the oﬃcial subtopics as explicit query
aspects to simulate their best-case scenarios, their performances are still much lower than our learning-based approaches, which indicates that there might be certain gap
between their heuristic predeﬁned utility functions and the
ﬁnal evaluation measures.
Comparing with the learning-based diversiﬁcation baseline method, our R-LTR approach also show better per-

5.1.4 Implementation
In our experiments, we use Indri toolkit (version 5.2)4
as the retrieval platform. For the test query set on each
dataset, we use a 5-fold cross validation with a ratio of 3:1:1,
for training, validation and testing. The ﬁnal test performance is reported as the average over all the folds.
For data preprocessing, we apply porter stemmer and
stopwords removing for both indexing and query processing. We then extract features for each dataset as follows.
For relevance, we use several standard features in LTR research [21], such as typical weighting models (e.g., TF-IDF,
BM25, LM), and term dependency model [19, 38], as summarized in Table 1, where Q-D means that the feature is
dependent on both query and document, and D means that
the feature only depends on the document. For all the Q-D
features, they are applied in ﬁve ﬁelds: body, anchor, title, URL and the whole document, resulting in 5 features in
total, respectively. Additionally, the MRF feature has two
types of values: ordered phrase and unordered phrase [19],
so the total feature number is 10.
For three baseline models: MMR, xQuAD and PM-2, they
all have a single parameter λ to tune. We perform a 5fold cross validation to train λ through optimizing ERR-IA.
Additionally, for xQuAD and PM-2, the oﬃcial subtopics
are used as a representation of taxonomy classes to simu4

Performance Comparison

http://lemurproject.org/indri

299

Table 2: Performance comparison of all methods in oﬃcial TREC diversity measures for WT2009.
Method
QL
ListMLE
MMRlist
xQuADlist
PM-2list
SVMDIV
R-LTRmin
R-LTRavg
R-LTRmax
TREC-Best

ERR-IA
0.1637
0.1913 (+16.86%)
0.2022 (+23.52%)
0.2316 (+41.48%)
0.2294 (+40.13%)
0.2408 (+47.10%)
0.2714 (+65.79%)
0.2671 (+63.16%)
0.2683 (+63.90%)
0.1922

α-NDCG
0.2691
0.3074 (+14.23%)
0.3083 (+14.57%)
0.3437 (+27.72%)
0.3369 (+25.20%)
0.3526 (+31.03%)
0.3915 (+45.48%)
0.3964 (+47.31%)
0.3933 (+46.15%)
0.3081

NRBP
0.1382
0.1681 (+21.64%)
0.1715 (+24.09%)
0.1956 (+41.53%)
0.1788 (+29.38%)
0.2073 (+50.00%)
0.2339 (+69.25%)
0.2268 (+64.11%)
0.2281 (+65.05%)
0.1617

Table 3: Performance comparison of all methods in oﬃcial TREC diversity measures for WT2010.
Method
QL
ListMLE
MMRlist
xQuADlist
PM-2list
SVMDIV
R-LTRmin
R-LTRavg
R-LTRmax
TREC-Best

ERR-IA
0.1980
0.2436 (+23.03%)
0.2735 (+38.13%)
0.3278 (+65.56%)
0.3296 (+66.46%)
0.3331 (+68.23%)
0.3647 (+84.19%)
0.3587 (+81.16%)
0.3639 (+83.79%)
0.2981

α-NDCG
0.3024
0.3755 (+24.17%)
0.4036 (+33.47%)
0.4445 (+46.99%)
0.4478 (+48.08%)
0.4593 (+51.88%)
0.4924 (+62.83%)
0.4781 (+58.10%)
0.4836 (+59.92%)
0.4178

formance than the SVMDIV approach. The relative improvement of R-LTRmin over the SVMDIV is up to 12.71%,
9.49%, 10.02%, in terms of ERR-IA on WT2009, WT2010,
WT2011, respectively. SVMDIV simply uses weighted word
coverage as a proxy for explicitly covering subtopics, while
our R-LTR approach directly models the generation probability of the diverse ranking based on the sequential ranking
formulation. Therefore, our R-LTR approach shows deeper
understanding and better formulation of diverse ranking,
and leads to better performance. We further conduct statistical tests on the results, which indicates that all these
improvements are statistically signiﬁcant (p-value < 0.01).
Among the R-LTR approaches, R-LTRmin obtains better performance than the other two variants especially on
WT2010 and WT2011 data collection, although their performance diﬀerence is small. It indicates that when deﬁning
the diversity relation between a document and a set of documents, the minimal distance would be a better choice.

NRBP
0.1549
0.1949 (+25.82%)
0.2252 (+45.38%)
0.2872 (+85.41%)
0.2901 (+87.28%)
0.2934 (+89.41%)
0.3293 (+112.59%)
0.3125 (+101.74%)
0.3218 (+107.74%)
0.2616

Table 5: The robustness of the performance of all
diversity methods in Win/Loss ratio
WT2009 WT2010 WT2011
Total
ListMLE
20/18
27/16
26/11
73/45
MMRlist
29/13
29/10
80/38
22/15
xQuADlist
28/11
31/12
31/12
90/35
PM-2list
32/12
32/11
90/38
26/15
SVMDIV
30/12
32/11
32/11
94/34
R-LTRmin
34/9
35/10
35/9
104/28
R-LTRavg
34/11
34/10
101/30
33/9
R-LTRmax
33/10
35/10
34/10
102/30

QL. Speciﬁcally, we deﬁne the robustness as the Win/Loss
ratio [36, 13] - the ratio of queries whose performance improves or hurts as compared with the original results from
QL in terms of of ERR-IA.
From results in Table 5, we ﬁnd that our R-LTR methods achieve best as compared with all the baseline methods, with the total Win/Loss ratio around 3.49. Among
the three variants of R-LTR methods, R-LTRmin performs
better than the others, with the Win/Loss ratio as 3.71.
Based on the robustness results, we can see that the performance of our R-LTR approach is more stable than all
the baseline methods. It demonstrates that the overall performance gains of our approach not only come from some
small subset of queries. In other words, the result diversiﬁcation for diﬀerent queries could be well addressed under
our approach.

5.2.2 Evaluation on Traditional Diversity Metrics.
Additionally, we also evaluate all the methods under traditional diversity measures, i.e. Precision-IA and Subtopics
Recall. The experimental results are shown in Figure 2 and
3. We can see that our approaches outperform all the baseline models on all the data collections in terms of both metrics, which is consistent with the evaluation results in Table
2, 3, and 4. It can further demonstrate the eﬀectiveness of
our approach on search result diversiﬁcation from diﬀerent
aspects. When comparing the three variants of our R-LTR
approaches, they all show similar performance and none obtains consistent better performance than the others under
these two measures.

5.4

Feature Importance Analysis

In this subsection, we analyze the relative importance of
the proposed diversity features. Table 6 shows an ordered
list of diversity features used in our R-LTRmin model according to the learned weights (average on three datasets). From
the results, we can see that the subtopic diversity Rij1 (topic)
is with the maximal weight, which is in accordance with

5.3 Robustness Analysis
In this section we analyze the robustness of these diversiﬁcation methods, i.e., whether the performance improvement
is consistent as compared with the basic relevance baseline

300

Table 4: Performance comparison of all methods in oﬃcial TREC diversity measures for WT2011.
Method
QL
ListMLE
MMRlist
xQuADlist
PM-2list
SVMDIV
R-LTRmin
R-LTRavg
R-LTRmax
TREC-Best

ERR-IA
0.3520
0.4172 (+18.52%)
0.4284 (+21.70%)
0.4753 (+35.03%)
0.4873 (+38.44%)
0.4898 (+39.15%)
0.5389 (+53.10%)
0.5276 (+49.89%)
0.5285 (+50.14%)
0.4380

α-NDCG
0.4531
0.5169 (+14.08%)
0.5302 (+17.02%)
0.5645 (+24.59%)
0.5786 (+27.70%)
0.5910 (+30.43%)
0.6297 (+38.98%)
0.6219 (+37.25%)
0.6223 (+37.34%)
0.5220

WT2009

WT2011

WT2010
R-LTR_max
R-LTR_avg
R-LTR_min

SVMDIV
PM-2_list
xQuAD_list
MMR_list
ListMLE

SVMDIV
PM-2_list
xQuAD_list
MMR_list
ListMLE

QL

0.08

R-LTR_max
R-LTR_avg
R-LTR_min

R-LTR_max
R-LTR_avg
R-LTR_min

SVMDIV
PM-2_list
xQuAD_list
MMR_list
ListMLE

0.06

NRBP
0.3123
0.3887 (+24.46%)
0.3913 (+25.30%)
0.4274 (+36.86%)
0.4318 (+38.26%)
0.4475 (+43.29%)
0.4982 (+59.53%)
0.4724 (+51.26%)
0.4741 (+51.81%)
0.4070

QL

0.1

0.12

0.14

0.16

0.12

0.14

QL

0.16

0.18

0.2

0.22

0.24

0.24 0.26 0.28 0.3 0.32 0.34 0.36 0.38 0.4 0.42

Figure 2: Performance comparison of all methods in Precision-IA for WT2009, WT2010, WT2011.
WT2009

QL

0.43

R-LTR_max
R-LTR_avg
R-LTR_min

R-LTR_max
R-LTR_avg
R-LTR_min
SVMDIV
PM-2_list
xQuAD_list
MMR_list
ListMLE

R-LTR_max
R-LTR_avg
R-LTR_min
SVMDIV
PM-2_list
xQuAD_list
MMR_list
ListMLE

0.39

WT2011

WT2010

SVMDIV
PM-2_list
xQuAD_list
MMR_list
ListMLE
QL

QL

0.47

0.51

0.55

0.59

0.63

0.52

0.56

0.6

0.64

0.68

0.72

0.76

0.64

0.68

0.72

0.76

0.8

0.84

0.88

Figure 3: Performance comparison of all methods in Subtopic Recall for WT2009, WT2010, WT2011.
to explore other useful features under our R-LTR framework
to further improve the performance of diverse ranking. We
will investigate this issue in future.

Table 6: Order list of diversity features with corresponding weight value.
feature
weight
Rij1 (topic)
3.71635
Rij3 (title)
1.53026
Rij4 (anchor) 1.34293
Rij2 (text)
0.98912
Rij5 (ODP)
0.52627
Rij6 (Link)
0.04683
Rij7 (URL)
0.01514

5.5

Running Time Analysis

We further study the eﬃciency of our approach and the
baseline models. All of the diversity methods associate with
a sequential selection process, which is time-consuming due
to the consideration of the dependency relations of document
pairs. While as discussed before, this type of algorithms all
have time complexity of O(n ∗ K), With a small K, the
prediction time is linear.
All the learning-based methods (i.e. ListMLE, SVMDIV
and R-LTR) need additional oﬄine training time due to the
supervised learning process. We compare the average training time of diﬀerent learning-based methods, and the result
is shown as following (unit: hour):

our intuition that diversity mainly lies in the rich semantic
information. Meanwhile, the title and anchor text diversity Rij3 (title) and Rij4 (anchor) also work well, since these
ﬁelds typically provide a precise summary of the content of
the document. Finally, The Link and URL based diversity
Rij6 (Link) and Rij7 (URL) seem to be the least important
features, which may be due to the sparsity of such types of
features in the data.
As a learning-based method, our model is ﬂexible to incorporate diﬀerent types of features for capturing both the
relevance and diversity. Therefore, it would be interesting

ListMLE (∼ 1.5h) ≺ SVMDIV (∼ 2h) ≺ R-LTR (∼ 3h)
We can observe that our approach takes longer but comparable oﬄine training time among diﬀerent learning-based
methods. Besides, in our experiments, we also found that
the three variants of our R-LTR approach are with nearly
the same training time. We will attempt to optimize our

301

code to provide much faster training speed via parallelization technique in the following work.

6. CONCLUSIONS
In this paper, we propose to solve the search result diversiﬁcation problem within a novel R-LTR framework. However, the speciﬁc deﬁnitions of ranking function and loss
function are challenging. Motivated by the top-down user
browsing behavior and the ubiquitous greedy approximation
for diverse ranking, we ﬁrstly deﬁne the ranking function as
the combination of relevance score and diversity score between the current item and those previously selected. Then
the loss function is deﬁned as the likelihood loss of ground
truth based on Plackett-Luce model, which can naturally
model the sequential generation of a diverse ranking list.
On this basis, we utilize stochastic gradient descent to conduct the unconstrained optimization. The prediction of a
diverse ranking list is then provided by iteratively maximizing the learned ranking function. Finally the experimental
results on public TREC data collections demonstrate the
eﬀectiveness and robustness of our approach.
The proposed R-LTR framework is quite general that can
be used in other applications, such as pseudo relevance feedback and topic distillation. Therefore, it would be interesting to apply our R-LTR framework in diﬀerent applications
in our future work.

7. ACKNOWLEDGEMENTS
This research work was funded by the 973 Program of
China under Grants No.2012CB316303 and No.2013CB329602,
863 program of China under Grants No.2012AA011003, National Natural Science Foundation of China under Grant
No.61232010 and No.61203298, and National Key Technology R&D Program under Grants No.2012BAH39B02 and
No.2012BAH46B04.

8. REFERENCES
[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong.
Diversifying search results. In Proceedings of the 2th ACM
WSDM, pages 5–14, 2009.
[2] C. Brandt, T. Joachims, Y. Yue, and J. Bank. Dynamic ranked
retrieval. In Proceedings of the 4th ACM WSDM, pages
247–256, 2011.
[3] J. Carbonell and J. Goldstein. The use of mmr, diversity-based
reranking for reordering documents and producing summaries.
In Proceedings of the 21st ACM SIGIR, pages 335–336, 1998.
[4] B. Carterette. An analysis of np-completeness in novelty and
diversity ranking. In Proceedings of the 2nd ICTIR, 2009.
[5] B. Carterette and P. Chandar. Probabilistic models of ranking
novel documents for faceted topic retrieval. In Proceedings of
the 18th ACM CIKM, pages 1287–1296, 2009.
[6] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected
reciprocal rank for graded relevance. In Proceedings of the 18th
ACM CIKM, pages 621–630, 2009.
[7] C. L. Clarke, N. Craswell, and I. Soboroﬀ. Overview of the trec
2009 web track. In TREC, 2009.
[8] C. L. Clarke, N. Craswell, I. Soboroﬀ, and A. Ashkan. A
comparative analysis of cascade measures for novelty and
diversity. In Proceedings of the 4th ACM WSDM, pages 75–84,
2011.
[9] C. L. Clarke, N. Craswell, I. Soboroﬀ, and E. M.Voorhees.
Overview of the trec 2011 web track. In TREC, 2011.
[10] C. L. Clarke, N. Craswell, I. Soboroﬀ, and G. V.Cormack.
Overview of the trec 2010 web track. In TREC, 2010.
[11] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova,
A. Ashkan, S. Büttcher, and I. MacKinnon. Novelty and
diversity in information retrieval evaluation. In Proceedings of
the 31st ACM SIGIR, pages 659–666, 2008.

302

[12] C. L. Clarke, M. Kolla, and O. Vechtomova. An eﬀectiveness
measure for ambiguous and underspeciﬁed queries. In
Proceedings of the 2nd ICTIR, pages 188–199, 2009.
[13] V. Dang and W. B. Croft. Diversity by proportionality: an
election-based approach to search result diversiﬁcation. In
Proceedings of the 35th ACM SIGIR, pages 65–74, 2012.
[14] S. Gollapudi and A. Sharma. An axiomatic approach for result
diversiﬁcation. In Proceedings of the 18th WWW, pages
381–390, 2009.
[15] J. He, V. Hollink, and A. de Vries. Combining implicit and
explicit topic representations for result diversiﬁcation. In
Proceedings of the 35th ACM SIGIR, pages 851–860, 2012.
[16] T. Hofmann. Probabilistic latent semantic indexing. In
Proceedings of the 22nd ACM SIGIR, pages 50–57, 1999.
[17] T.-Y. Liu. Learning to Rank for Information Retrieval.
Springer, 2011.
[18] J. I. Marden. Analyzing and Modeling Rank Data. Chapman
and Hall, 1995.
[19] D. Metzler and W. B. Croft. A markov random ﬁeld model for
term dependencies. In Proceedings of the 28th ACM SIGIR,
pages 472–479, 2005.
[20] G. Nemhauser, L. Wolsey, and M. Fisher. An analysis of
approximations for maximizing submodular set functions–i.
Mathematical Programming, 14(1):265–294, 1978.
[21] T. Qin, T.-Y. Liu, J. Xu, and H. Li. Letor: A benchmark
collection for research on learning to rank for information
retrieval. Inf. Retr., pages 346–374, 2010.
[22] T. Qin, T.-Y. Liu, X.-D. Zhang, D.-S. Wang, and H. Li. Global
ranking using continuous conditional random ﬁelds. In
Proceedings of the 22th NIPS, Vancouver, British Columbia,
Canada, December 8-11, 2008, pages 1281–1288, 2008.
[23] T. Qin, T.-Y. Liu, X.-D. Zhang, D.-S. Wang, W.-Y. Xiong, and
H. Li. Learning to rank relational objects and its application to
web search. In Proceedings of the 17th WWW, pages 407–416,
2008.
[24] F. Radlinski and S. Dumais. Improving personalized web search
using result diversiﬁcation. In Proceedings of the 29th ACM
SIGIR, 2006.
[25] F. Radlinski, R. Kleinberg, and T. Joachims. Learning diverse
rankings with multi-armed bandits. In Proceedings of the 25th
ICML, pages 784–791, 2008.
[26] D. Raﬁei, K. Bharat, and A. Shukla. Diversifying web search
results. In Proceedings of the 19th WWW, pages 781–790,
2010.
[27] K. Raman, T. Joachims, and P. Shivaswamy. Structured
learning of two-level dynamic rankings. In Proceedings of the
20th ACM CIKM, pages 291–296, 2011.
[28] K. Raman, P. Shivaswamy, and T. Joachims. Online learning to
diversify from implicit feedback. In Proceedings of the 18th
ACM SIGKDD, pages 705–713, 2012.
[29] R. L. Santos, C. Macdonald, and I. Ounis. Exploiting query
reformulations for web search result diversiﬁcation. In
Proceedings of the 19th WWW, pages 881–890, 2010.
[30] P. Shivaswamy and T. Joachims. Online structured prediction
via coactive learning. In ICML’12, 2012.
[31] A. Slivkins, F. Radlinski, and S. Gollapudi. Learning optimally
diverse rankings over large document collections. In
Proceedings of the 27th ICML, pages 983–990, 2010.
[32] S. Vargas, P. Castells, and D. Vallet. Explicit relevance models
in intent-oriented information retrieval diversiﬁcation. In
Proceedings of the 35th ACM SIGIR, pages 75–84, 2012.
[33] J. Wang and J. Zhu. Portfolio theory of information retrieval.
In Proceedings of the 32nd ACM SIGIR, pages 115–122, 2009.
[34] F. Xia, T.-Y. Liu, J. Wang, W. Zhang, and H. Li. Listwise
approach to learning to rank: theory and algorithm. In
Proceedings of the 25th ICML, pages 1192–1199, 2008.
[35] Y. Yue and C. Guestrin. Linear submodular bandits and their
application to diversiﬁed retrieval. In NIPS, pages 2483–2491,
2011.
[36] Y. Yue and T. Joachims. Predicting diverse subsets using
structural svms. In Proceedings of the 25th ICML, pages
1224–1231, 2008.
[37] C. X. Zhai, W. W. Cohen, and J. Laﬀerty. Beyond independent
relevance: methods and evaluation metrics for subtopic
retrieval. In Proc. of the 26th ACM SIGIR, pages 10–17, 2003.
[38] Y. Zhu, Y. Xue, J. Guo, Y. Lan, X. Cheng, and X. Yu.
Exploring and exploiting proximity statistic for information
retrieval model. In Proceedings of the 8th Asia Information
Retrieval Societies Conference, volume 7675 of Lecture Notes
in Computer Science, pages 1–13, 2012.

