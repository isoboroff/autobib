Using the Cross-Entropy Method to Re-Rank Search
Results
Oren Kurland

Haggai Roitman, Shay Hummel

Faculty of Industrial Engineering and
Management, Technion
Haifa 32000, Israel

IBM Research - Haifa
Haifa 31905, Israel

{haggai,shayh}@il.ibm.com

kurland@ie.technion.ac.il

ABSTRACT

2.

We present a novel unsupervised approach to re-ranking an
initially retrieved list. The approach is based on the Cross
Entropy method applied to permutations of the list, and relies on performance prediction. Using pseudo predictors we
establish a lower bound on the prediction quality that is required so as to have our approach significantly outperform
the original retrieval. Our experiments serve as a proof of
concept demonstrating the considerable potential of the proposed approach. A case in point, only a tiny fraction of the
huge space of permutations needs to be explored to attain
significant improvements over the original retrieval.

The Cross Entropy (CE) method [13] that is used in our
approach is a Monte Carlo framework for rare event estimation and combinatorial optimization. The CE method has
been previously applied in many domains such as machine
learning, simulation, networks, etc. [13]. To the best of our
knowledge, our work is the first to use the CE method in
the information retrieval domain.
Our approach relies on predicting the retrieval performance of permutations of a document list. Applying performance prediction to select one of two retrieved lists was
explored in some work [2, 6, 3, 11]. However, the conclusions
regarding the resultant effectiveness of using the proposed
predictors were inconclusive. In contrast, we do not present
a concrete predictor. Rather, we devise a pseudo predictor
that enables to control prediction quality, and accordingly
set a lower bound on the prediction quality required so as
to have our approach outperform the initial ranking.
Using a simulation study, a lower bound on the prediction
quality required for effective selective query expansion was
set [8]. While this work focused on performance prediction
over queries, our approach relies on prediction over rankings
for the same query. Furthermore, our approach is not committed to any ranking paradigm. In addition, rather than
use a simulation, we propose a novel pseudo predictor that
allows to control prediction quality.
Finally, we note that some list-wise learning to rank approaches [10] are also based on finding effective permutations of the same list, although not with the Cross Entropy
method that we employ. Permutations are explored during
the training phase and a ranker is induced. In contrast,
our approach employs optimization over permutations as an
unsupervised re-ranking mechanism.

Categories and Subject Descriptors: H.3.3 [Information Search
and Retrieval] Retrieval Models
General Terms: Algorithms, Experimentation
Keywords: Re-ranking, Optimization, Performance Prediction

1.

INTRODUCTION

We present a novel unsupervised approach to the challenge
of re-ranking a document list that was retrieved in response
to a query so as to improve retrieval effectiveness. The approach utilizes a Monte-Carlo-based optimization method,
named the Cross Entropy (CE) method [13], which is applied to permutations of the list. The approach relies on a
retrieval performance predictor that can be applied to any
ranking of the list.
Given the reliance on performance prediction, we present
a novel pseudo predictor that enables to fully control the
prediction quality. We use the pseudo predictor in our approach to set a lower bound on the prediction quality that is
needed so as to have our approach significantly outperform
the initial ranking. Further empirical evaluation provides a
proof of concept for our approach. Specifically, via the exploration of a tiny fraction of the huge space of all possible
permutations, our approach finds highly effective permutations. The retrieval effectiveness of these permutations is
substantially, and statistically significantly better, than that
of the original ranking of the list.

3.
3.1

RELATED WORK

FRAMEWORK
Problem Definition

Let Dqk denote the list of the k documents in a corpus
D that are the most highly ranked by some initial search
performed in response to query q. Let ΠDqk denote the set
of all k! possible permutations (rankings) of the documents
in Dqk . Let π ∈ ΠDqk denote a single permutation of Dqk
and let π(d) further denote the position (rank) of document
d (∈ Dqk ) in π. Let Q(π) denote the retrieval performance
(effectiveness) of the permutation π (∈ ΠDqk ).
The goal is to find a permutation π (∈ ΠDqk ) such that
Q(π) is maximized. Unfortunately, finding an optimal per-

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00.
http://dx.doi.org/10.1145/2600428.2609454.

839

Algorithm 1 Cross Entropy Re-ranking Optimization
b (π) , N, α, λ
1: input: πDk , Q
q
2: initialize:
 1
i 6= j
0
k−1 ,
3: P(i,j)
=
0,
i=j

mutation is NP-Complete [1]. In addition, with no prior
relevance judgement on the documents in Dqk , the true performance Q(π) for any given permutation π ∈ ΠDqk is unknown. Hence, the performance of any given permutation
can only be predicted, and the task becomes even more challenging.
b
Let Q(π)
denote the predicted performance of the permutation π (∈ ΠDqk ). Potential predictors may utilize any
available pre-retrieval features [9] (e.g., induced from the
query q and the corpus D), post-retrieval features (e.g., induced from the result list Dqk or the permutation π) or their
combination [4].

3.2

4: π∗ = πDk
q
5: t = 1
6: loop
7: Randomly draw N permutations πl ∈ ΠDk
q
b (π ∗ ) < maxl=1,...,N Q
b (πl ) then
8: if Q
b l)
9:
π ∗ = arg maxl=1,...,N Q(π
10: end if
b (πl )
11: Sort permutations πl according to Q
12: Let γt be the sample (1−α)-quantile of

An Optimization Approach

13:
14:

We next propose an optimization approach that effectively finds “promising” permutations which have the best
b
predicted performance according to a given predictor Q(π).
Since the resultant retrieval effectiveness of the optimization procedure depends on the prediction quality of the predictor employed, we empirically derive a lower bound for
the prediction quality of “effective” predictors. That is, if
a predictor with a prediction quality higher than the lower
bound is used in our approach then the approach is guaranteed to find — as determined based on the benchmarks we
have used — as a solution a permutation π ∗ whose retrieval
performance is better than that of the initial ranking.

3.2.1

15:
16:
17:
18:
19:
20:
21:
22:

using P t−1

the performances:
b (πl );l = 1, .., N
Q
for i = 1, . . . , k; j = 1, . . . , k do
PN
b (π )≥γ }
I {πl (dj )=πl (di )+1}I {Q
t
l
t
P(i,j)
= l=1
PN
b π ≥γ }
I Q
t
l=1 { ( l )
t−1
t
t
P(i,j)
=λP(i,j)
+(1−λ)P(i,j)

end for
if γt converged then
stop and return π ∗
else
t=t+1
end if
end loop

goal is to converge (via cross entropy minimization) to the
unknown probability space of optimal permutations, from
which optimal solutions can be generated [13].
On each iteration t, N random permutations are sampled
based on the last induced promising permutations probability space P t−1 . Next, the predicted performance of
each sampled permutation is calculated and the performance
of the current best performing permutation π ∗ is updated
accordingly. “New” promising permutations are then explored by first sorting the sampled permutations according to their predicted performance and then updating the
transition probabilities based on the top-⌈αN ⌉ performing
samples, whose minimum performance is γt . Given γt , each
t
transition probability P(i,j)
is induced according to the relative number of permutations out of the top-⌈αN ⌉ permutations (which have predicted performance equal to or higher
than γt ) that also ranked document di one position above
document dj . A fixed smoothing scheme, controlled by parameter λ, further allows to trade between the exploration
t−1
t
) of the
(given by P(i,j)
) and the exploitation (given by P(i,j)
algorithm.
The algorithm continuous until some convergence criteria is met. In this work we follow the convergence criteria suggested in [13] and stop the algorithm if the sample
(1 − α)-performance quantile γt does not change within several consecutive iterations.

Optimization using the Cross Entropy method

We propose a Monte-Carlo optimization approach to our
permutation-based re-ranking task. The approach, which
we term Cross Entropy Re-ranking Optimization (CERO),
uses the Cross Entropy (CE) method [13]. Within the CE
method, optimal solutions to hard problems (such as that
we try to solve) are modeled as rare events whose occurrence probability is effectively estimated [13]. Given such
estimates, optimal solutions can then be efficiently generated. Under a certain condition, the CE method is expected
to converge to the optimal solution [12].
We now describe our algorithm and provide its pseudo
code in Algorithm 1. The algorithm gets as an input the
b (π) and
initial ranked list πDqk , a performance predictor Q
several tuning parameters (mentioned below) that control
the learning rate and convergence of the algorithm. The algorithm iteratively explores permutations in ΠDqk using random sampling. To this end, the algorithm induces a probability space of “promising” permutations over ΠDqk using the
feedback it gets about the relative performance of permutations that were explored in previous iterations. It is easy to
show that for a given k, a unique bijection exists between
the permutation set ΠDqk and the set of all k! possible Hamiltonian paths in a complete graph with k nodes. Therefore,
random permutations can be efficiently drawn by sampling
Hamiltonian paths using a simple constrained random walk
t
method [13]. Let P(i,j)
denote the the probability for a single step transition between node i and node j in the graph,
which corresponds to the event π(dj ) = π(di ) + 1, i.e., document di is ranked in π one position before document dj .
With no prior knowledge on the permutations probability
space, the algorithm is initialized with the uniform probability (having the maximum entropy) and πDqk is considered as
the current best performing permutation. The algorithm’s

3.2.2

A criterion for effective prediction

The CERO algorithm is generic and can employ any performance predictor. Naturally, however, the prediction quality of the predictor has significant impact on the retrieval
effectiveness of the ranking produced by the algorithm.
Thus, we turn to devise a method for determining the
lower bound of prediction quality that will result in the
CERO algorithm outperforming the initial ranking of Dqk .
The bound is independent of a prediction approach.
Herein, we measure retrieval performance using average
precision (AP@k); i.e., Q(π) in our case is the AP of the per-

840

corpus

# of documents

queries

disks

GOV2
WT10G
TREC8

25,205,179
1,692,096
528,155

701-850
451-550
401-450

GOV2
WT10g
4&5-{CR}

obtained three initial lists, Dqk , composed of k = 100 documents, for each query. Mean average precision (MAP@k)
serves as the evaluation measure. Statistically significant
differences of performance are measured using the paired
t-test with a 95% confidence level.
Each initial list was re-ranked using the CERO algorithm
employed with pseudo AP predictors of varying prediction
quality levels. To control prediction quality, the pseudo
predictors were generated according to Eq. 2; the prediction quality was varied from ρ = 0.05 (worst predictor) to
ρ = 1.0 (best predictor). Following previous recommendations [13], the algorithm’s learning parameters were set as
follows: N = 1000, α = 0.01 and λ = 0.7.

Table 1: TREC data used for experiments.

mutation π. Following standard practice in work on queryperformance prediction [4], prediction quality is measured
by the Pearson correlation between the true AP of permub
tations (Q(π)) and their predicted performance (Q(π)).
To derive a lower bound on prediction quality, we next
present an approach for generating pseudo AP predictors,
whose prediction quality can be controlled. Following previous observations [5], we assume that true AP values follow
a normal distribution1 .
We first normalize the AP of the permutation π (∈ ΠDqk ):
Qnorm (π) =

Q(π) − EΠDk (AP )
q
q
;
V arΠDk (AP )

Eff ciency considerations. To implement CERO, we used
a parallelized version of the Cross Entropy method [7]. On
average, CERO converged in 21.32 iterations (with a 15.6
standard deviation). CERO explores at each iteration a
maximum of N = 1000 permutations. Thus, all together,
CERO considered only about 21k − 36k permutations out of
the 100! possible permutations of the initial list.

(1)

q

4.2

EΠDk (AP ) and V arΠDk (AP ) are the expectation and the
q

q

variance of the true AP values of the permutations in ΠDqk ,
respectively. The two statistics can be estimated using maximum likelihood estimation for normal distribution, by sampling a large enough random (uniform) sample of permutations in ΠDqk (e.g., N = 1000). Since AP follows a normal
distribution, we get that as k → ∞, for any permutation
π ∈ ΠDqk : Qnorm (π) ∼ N (0, 1) [5].
Proposition 1 defines a ρ-correlated pseudo AP predictor;
that is, a predictor with a ρ prediction quality (i.e., Pearson
correlation with true AP). The proof is quite straightforward
and is ommitted due to space considerations.

CERO’s effectiveness. We first study the potential of our
permutation-based optimization approach; specifically, in
finding highly effective permutations in the huge space (100!)
of permutations. To this end, we neturilize the effect of prediction quality by applying CERO with a “predictor” which
reports the true AP of the considered permutations (i.e.,
ρ = 1.0). The resultant MAP performance is presented in
Table 2. As reference comparisons we use the initial ranking and an optimal re-ranking where all relevant documents
from the initial list are positioned at the highest ranks.
We can see in Table 2 that, overall, CERO results in very
good approximations. Specifically, CERO’s MAP is at least
as 91% as good as that of the optimal MAP. Recall from
above that CERO explores only a tiny fraction of all permutations of the documents in the result list. It is worth noting
that an even better approximation may be obtained by finer
tuning of the algorithm (e.g., following [12]). We leave this
exploration for future work, and view the results presented
in Table 2 as a solid proof of concept for the optimization
approach we have employed.

Proposition 1. Given a query q, initial result list Dqk ,
and permutation π ∈ ΠDqk , a ρ-correlated pseudo AP predicbρ (π), is obtained as follows:
tor, denoted Q
bρ (π) = Qnorm (π)ρ +
Q

p

1 − ρ2 X

(2)

where Qnorm (π) is the normalized true AP value according
to Eq. 1 and X ∼ N (0, 1).

4.
4.1

Results

EVALUATION

The effect of prediction quality. In Table 3 we present
the effect of the prediction quality of the pseudo AP predictors used in CERO on its MAP retrieval performance.
Evidently, and as should be expected, the higher the prediction quality (ρ), the better the performance. Furthermore,
as from ρ = 0.3 CERO improves over the initial ranking for
all the retrieval methods and across all corpora; for ρ ≥ 0.35
the improvements are always statistically significant. With
higher prediction quality, the improvements over the initial
ranking become very substantial.

Setup

The TREC corpora and queries used for experiments are
specified in Table 1. Titles of TREC topics served for queries.
The Apache Lucene2 search library (version 4.3) was used for
the experiments. Documents and queries were processed using Lucene’s default analysis (i.e., tokenization, stemming,
stopwords, etc). For each query, 100 documents were retrieved using Lucene’s implementation, employed with default free-parameter values, of each of the following retrieval
methods: vector space model (TF-IDF), query-likelihood
(QL with Dirichlet smoothing) and Okapi BM25. Thus, we

5.

CONCLUSIONS AND FUTURE WORK

We presented a novel approach to re-ranking an initially
retrieved list. The approach is based on applying the Cross
Entropy optimization method [13] upon permutations of the
list. Query-performance predictors are used to evaluate the
performance of permutations. Empirical evaluation pro-

1

The assumption was further verified in our experiments
using the χ2 goodness-of-fit test. Details are omitted due to
space considerations.
2
http://lucene.apache.org

841

Initial
Optimal
CERO (ρ = 1.0)

BM25

GOV2
QL

TF-IDF

BM25

WT10G
QL

TF-IDF

BM25

TREC8
QL

TF-IDF

.151
.274
.251 (92%)

.172
.296
.275 (93%)

.137
.253
.232 (91%)

.156
.352
.320 (91%)

.164
.391
.367 (94%)

.158
.349
.322 (92%)

.198
.400
.367 (92%)

.206
.407
.373 (92%)

.190
.388
.356 (92%)

Table 2: The MAP of the initial retrieval, optimal re-ranking and the CERO algorithm when employed with
the true AP as the “predictor”. The percentages are with respect to Optimal.

Initial
ρ = 1.0
ρ = .95
ρ = .90
ρ = .85
ρ = .80
ρ = .75
ρ = .70
ρ = .65
ρ = .60
ρ = .55
ρ = .50
ρ = .45
ρ = .40
ρ = .35
ρ = .30
ρ = .25
ρ = .20
ρ = .15
ρ = .10
ρ = .05

BM25

GOV2
QL

TF-IDF

BM25

WT10G
QL

TF-IDF

BM25

TREC8
QL

TF-IDF

.151
.251 (66%)X
.249 (65%)X
.245 (62%)X
.242 (60%)X
.237 (57%)X
.232 (54%)X
.222 (47%)X
.222 (47%)X
.215 (43%)X
.207 (37%)X
.199 (32%)X
.192 (27%)X
.182 (20%)X
.170 (12%)X
.161 (6%)X
.154 (2%)
.143 (−5%)
.134 (−11%)
.120 (−20%)
.119 (−21%)

.172
.275 (60%)X
.272 (58%)X
.267 (56%)X
.262 (53%)X
.255 (49%)X
.248 (44%)X
.245 (42%)X
.231 (35%)X
.229 (33%)X
.219 (28%)X
.214 (24%)X
.202 (18%)X
.194 (13%)X
.180 (5%)X
.173 (1%)
.165 (−4%)
.150 (−13%)
.144 (−16%)
.141 (−18%)
.137 (−20%)

.137
.232 (69%)X
.231 (69%)X
.225 (64%)X
.225 (64%)X
.218 (59%)X
.212 (55%)X
.208 (52%)X
.203 (48%)X
.198 (45%)X
.194 (42%)X
.181 (32%)X
.173 (27%)X
.172 (25%)X
.157 (15%)X
.144 (5%)
.135 (−2%)
.125 (−8%)
.120 (−12%)
.117 (−14%)
.110 (−20%)

.156
.320 (105%)X
.320 (105%)X
.320 (104%)X
.319 (104%)X
.311 (99%)X
.309 (97%)X
.301 (93%)X
.301 (92%)X
.292 (87%)X
.290 (85%)X
.280 (79%)X
.260 (66%)X
.257 (64%)X
.211 (35%)X
.199 (27%)X
.172 (10%)X
.151 (−3%)
.134 (−14%)
.110 (−30%)
.79 (−50%)

.164
.367 (124%)X
.362 (121%)X
.361 (120%)X
.354 (115%)X
.355 (116%)X
.342 (108%)X
.345 (110%)X
.333 (103%)X
.328 (100%)X
.312 (90%)X
.303 (85%)X
.272 (66%)X
.258 (57%)X
.246 (50%)X
.222 (35%)X
.177 (8%)X
.139 (−15%)
.124 (−24%)
.119 (−28%)
.93 (−43%)

.158
.322 (104%)X
.321 (104%)X
.317 (101%)X
.317 (101%)X
.313 (99%)X
.309 (96%)X
.303 (93%)X
.296 (88%)X
.293 (86%)X
.285 (81%)X
.284 (81%)X
.260 (65%)X
.249 (58%)X
.225 (43%)X
.213 (35%)X
.172 (9%)X
.130 (−18%)
.112 (−29%)
.105 (−33%)
.103 (−35%)

.198
.367 (85%)X
.364 (83%)X
.361 (82%)X
.358 (81%)X
.354 (79%)X
.348 (76%)X
.342 (72%)X
.332 (67%)X
.325 (64%)X
.311 (57%)X
.303 (53%)X
.285 (44%)X
.267 (35%)X
.244 (23%)X
.221 (11%)X
.201 (1%)
.167 (−16%)
.145 (−27%)
.123 (−38%)
.115 (−42%)

.206
.373 (81%)X
.371 (80%)X
.370 (80%)X
.364 (77%)X
.362 (76%)X
.358 (74%)X
.345 (68%)X
.344 (67%)X
.332 (62%)X
.321 (56%)X
.310 (51%)X
.293 (42%)X
.268 (30%)X
.248 (21%)X
.227 (10%)X
.194 (−6%)
.159 (−22%)
.148 (−28%)
.133 (−35%)
.122 (−41%)

.190
.356 (87%)X
.356 (87%)X
.354 (86%)X
.348 (83%)X
.342 (80%)X
.339 (78%)X
.330 (73%)X
.322 (70%)X
.317 (67%)X
.304 (60%)X
.294 (54%)X
.278 (46%)X
.259 (36%)X
.237 (24%)X
.212 (12%)X
.179 (−6%)
.165 (−13%)
.139 (−27%)
.120 (−37%)
.109 (−43%)

Table 3: The MAP of the initial retrieval and of CERO when using different ρ-correlated pseudo AP predictors. X marks a statistically significant improvement over the initial ranking. Numbers in italics are lower
than those for the initial ranking. The reported percentages are with respect to the initial ranking.
vided a proof of concept for our approach. That is, the
optimization procedure finds highly effective permutations
by exploring only a tiny fraction of the space of all possible permutations. In addition, we devised novel pseudo
predictors that allow to carefully control prediction quality
and to infer the minimal prediction quality required for our
approach to (significantly) outperform the original ranking.
Our main plan for future work is devising query-performance
predictors that yield a prediction quality that is higher than
that we established as a lower bound for effective application of our approach. We note that almost all previously
proposed query-performance predictors [4] are not suited for
this task as they operate over different queries rather than
over different lists retrieved for the same query.

Acknowledgment
We would like to thank David Carmel for discussions on an
earlier version of this work. Oren Kurland’s work is supported in part by the Israel Science Foundation under grant
no. 433/12 and by a Google faculty research award.

6.

REFERENCES

[1] N. Alon. Ranking tournaments. SIAM Journal on
Discrete Mathematics, 20(1):137–142, 2006.
[2] G. Amati, C. Carpineto, and G. Romano. Query
difficulty, robustness, and selective application of
query expansion. In Proc. of ECIR, pages 127–137,
2004.
[3] N. Balasubramanian and J. Allan. Learning to select
rankers. In Proc. of SIGIR, pages 855–856, 2010.
[4] D. Carmel and E. Yom-Tov. Estimating the Query
Difficulty for Information Retrieval. Synthesis
Lectures on Information Concepts, Retrieval, and
Services. Morgan & Claypool Publishers, 2010.

842

[5] B. Carterette, J. Allan, and R. Sitaraman. Minimal
test collections for retrieval evaluation. In Proc. of
SIGIR, pages 268–275, 2006.
[6] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. A
language modeling framework for selective query
expansion. Technical Report IR-338, Center for
Intelligent Information Retrieval, University of
Massachusetts, 2004.
[7] G. E. Evans, J. M. Keith, and D. P. Kroese. Parallel
cross-entropy optimization. In Proc. of WSC, pages
2196–2202, 2007.
[8] C. Hauff and L. Azzopardi. When is query
performance prediction effective? In Proc. of SIGIR,
pages 829–830, 2009.
[9] C. Hauff, D. Hiemstra, and F. de Jong. A survey of
pre-retrieval query performance predictors. In Proc. of
CIKM, pages 1419–1420, 2008.
[10] T.-Y. Liu. Learning to Rank for Information Retrieval.
Springer, 2011.
[11] X. Liu and W. B. Croft. Experiments on retrieval of
optimal clusters. Technical Report IR-478, Center for
Intelligent Information Retrieval (CIIR), University of
Massachusetts, 2006.
[12] L. Margolin. On the convergence of the cross-entropy
method. Annals of Operations Research,
134(1):201–214, 2005.
[13] R. Y. Rubinstein and D. P. Kroese. The cross-entropy
method: a unified approach to combinatorial
optimization, Monte-Carlo simulation and machine
learning. Springer, 2004.

