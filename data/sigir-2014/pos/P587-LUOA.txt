Win-Win Search: Dual-Agent Stochastic Game in Session
Search
Jiyun Luo, Sicong Zhang, Hui Yang
Department of Computer Science, Georgetown University

{jl1749,sz303}@georgetown.edu, huiyang@cs.georgetown.edu
ABSTRACT
Session search is a complex search task that involves multiple search iterations triggered by query reformulations. We
observe a Markov chain in session search: user’s judgment
of retrieved documents in the previous search iteration affects user’s actions in the next iteration. We thus propose
to model session search as a dual-agent stochastic game:
the user agent and the search engine agent work together
to jointly maximize their long term rewards. The framework, which we term “win-win search”, is based on Partially
Observable Markov Decision Process. We mathematically
model dynamics in session search, including decision states,
query changes, clicks, and rewards, as a cooperative game
between the user and the search engine. The experiments on
TREC 2012 and 2013 Session datasets show a statistically
significant improvement over the state-of-the-art interactive
search and session search algorithms.

Figure 1: A Markov chain of decision states in session search. (S: decision states; q: queries; A: user
actions such as query changes; D: documents).
11, 17, 22, 24, 25, 30, 31]. Table 1 lists example information
needs and queries in session search.
We are often puzzled about what drives a user’s search in
a session and why they make certain moves. We observe that
sometimes the same user behavior, such as a drift from one
subtopic to another, can be explained by opposite reasons:
either the user is satisfied with the search results and moves
to another sub information need, or the user is not satisfied
with the search results and leaves the previous search path.
The complexity of users’ decision making patterns makes
session search quite challenging [4, 29].
Researchers have attempted to find out the causes of topic
drifting in session search. The causes under study include
personalization [29], task types [20, 24], and previous documents’ relevance [11]. A user study is usually needed to draw
conclusions about user intent. However, the focus of this
paper is not on identifying user intent. Instead, we simplify
the complexity of users’ decision states into a cross product of only two dimensions: whether previously retrieved
documents are relevant and whether the user would like to
explore the next sub information need. Our work differs
from existing work in that we consider that a session goes
through a series of hidden decision states, with which we
design a statistical retrieval model for session search. Our
emphasis is an effective retrieval model, not a user study to
identify the query intent.
The hidden decision states form a Markov chain in session
search. A Markov chain is a memoryless random process
where the next state depends only on the current state [18].
Figure 1 illustrates a Markov chain of hidden decision states
for TREC 2013 Session 9. In a session, a user’s judgment
of the retrieved documents in the previous iteration affects
or even decides the user’s actions in the next iteration. A
user’s actions could include clicks, query changes, reading
the documents, etc. The user’s gain, which we call reward,
is the amount of relevant information that he or she obtains
in the retrieved documents. The reward motives the later
user actions. If the decision states are known, we can use
a Markov Decision Process (MDP) to model the process.
However, in session search, users’ decision states are hidden.
We therefore model session search as a Partially Observable
Markov Decision Process (POMDP) [18].

Categories and Subject Descriptors
H.3.3 [Information Systems ]: Information Storage and
Retrieval—Information Search and Retrieval

Keywords
Dynamic Information Retrieval Modeling; POMDP; Stochastic Game; Session Search

1.

INTRODUCTION

Users often need a multi-query session to accomplish a
complex search task. A session usually starts with the user
writing a query, sending it to the search engine, receiving
a list of ranked documents ordered by decreasing relevance,
then examining the snippets, clicking on the interesting ones,
and spending more time reading them; we call one such sequence a “search iteration.” In the next iteration, the user
modifies the query or issues a new query to start the search
again. As a result, a series of search iterations form, which
include a series of queries q1 , ..., qn , a series of returned documents D1 , ..., Dn , and a series of clicks C1 , ..., Cn , some of
which are SAT clicks (satisfactory clicked documents [9]).
The session stops when the user’s information need is satisfied or the user abandons the search [6]. The information
retrieval (IR) task in this setting is called session search [7,
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’14, July 6–11, 2014, Gold Coast, Queensland, Australia.
Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00.
http://dx.doi.org/10.1145/2600428.2609629.

587

Table 1: Information needs and queries (examples are from TREC 2013 Session Track).
Session 2: Information Need
Session 2: Queries
You want to buy a scooter. So you’re interq1 =scooter brands
q2 =scooter brands reliable
q3 =scooter
q4 =scooter cheap
q5 =scooter review
q6 =scooter price
ested in learning more facts about scooters
including:what brands of scooters are out
q7 =scooter price
q8 =scooter stores
q9 =where to buy scooters
there? What brands of scooters are reliable? Which scooters are cheap? Which stores sell scooters? which stores sell the best scooters?
Session 9: Information Need
Session 9: Queries
You want to know more about old US coins. q1 =old us coins
q2 =collecting old us coins
q3 =selling old us coins
Relevant information to you includes value of old US coins, types of old US coins, old US silver dollar,
q4 =selling old “usa coins”
how to start collecting old US coins, how to sell old US coins and how to buy them, where to buy those coins.
Session 87: Information Need
Session 87: Queries
Suppose you’re planning a trip to the
q1 =best us destinations
q2 =distance new york boston
q3 =maps.bing.com
q4 =maps
q5 =bing maps
United States.You will be there for a
month and able to travel within a 150-mile
q6 =hartford tourism
q7 =bing maps
q8 =hartford visitors
radius of your destination.With that
q9 =hartford connecticut tourism
q10 =hartford boston travel
constraint, what are the best cities to
q11 =boston tourism
q12 =nyc tourism
q13 =philadelphia nyc distance
consider as possible destinations?
q14 =bing maps
q15 =philadelphia washington dc distance
q16 =bing maps
q17 =philadelphia tourism
q18 =washington dc tourism
q19 =philadelphia nyc travel q20 =philadelphia nyc train q21 =philadelphia nyc bus

In fact, not only the user, but also the search engine,
makes decisions in a Markov process. A search engine takes
in a user’s feedback and improves its retrieval algorithm iteration after iteration to achieve a better reward too. The
search engine actions could include term weighting, turning
on or turning off one or more of its search techniques, or adjusting parameters for the techniques. For instance, based
on the reward, the search engine can select p in deciding the
top p documents used in pseudo relevance feedback.
We propose to model session search as a dual-agent stochastic game. When there is more than one agent in a POMDP,
the POMDP becomes a stochastic game (SG). The two agents
in session search are the user agent and the search engine
agent. In contrast to most two-player scenarios such as chess
games in game theory, the two agents in session search are
not opponents to each other; instead, they cooperate: they
share the decision states and work together to jointly maximize their goals. We term the framework “win-win search”
for its efforts in ensuring that both agents arrive at a winwin situation. One may argue that in reality a commercial
search engine and a user may have different goals and that is
why some commercial search engines put their sponsors high
in the returned results. However, this paper focuses on the
win-win setting and assume a common interest – fulfilling
the information needs – for both agents.
The challenges of modeling session search as a stochastic
game lie in how to design and determine the decision states
and actions of each agent, how to observe their behaviors,
and how to measure the rewards and set the optimization
goals. We present the details in Sections 4 and 5. As a
retrieval framework, we pay more attention to the search
engine agent. When the search engine makes decisions, it
picks a decision that jointly optimizes the common interest.
We evaluate the win-win search framework on TREC 2012
& 2013 Session data. TREC (Text REtrieval Conference)
2010 - 2013 Session Tracks [20, 21] have spurred a great
deal of research in session search [3, 10, 11, 14, 24]. The
tracks provide interaction data within a session and aim to
retrieve relevant documents for the last query qn in the session. The interaction data include queries, top returned documents, user clicks, and other relevant information such as
dwell time. Document relevance is judged based on information need for the entire session, not just the last query. In
this paper, all examples are from TREC 2013. Our experiments show that the proposed framework achieves statisti-

cally significant improvements over state-of-the-art interactive search and session search algorithms.
The remainder of this paper is organized as follows: Section 2 presents the related work, Section 3 provides preliminaries for POMDP. Section 4 details the win-win search
framework, Section 5 elaborates the optimization, Section 6
evaluates the framework and Section 7 concludes the paper.

2.
2.1

RELATED WORK
Session search

Session search has attracted a great amount of research
from a variety of approaches [3, 11, 22, 25, 33]. They can be
grouped into log-based methods and content-based methods.
There is a large body of work using query logs to study
queries and sessions. Feild and Allan [8] proposed a taskaware model for query recommendation using random walk
over a term-query graph formed from logs. Song and He’s
work [27] on optimal rare query suggestion also used random
walk, with implicit feedback in logs. Wang et al. [30] utilized
the latent structural SVM to extract cross-session search
tasks from logs. Recent log-based approaches also appear in
the Web Search Click Data (WCSD) workshop series.1
Content-based methods directly study the content of the
query and the document. For instance, Raman et al. [25]
studied a particular case in session search where the search
topics are intrinsically diversified. Content-based session
search also include most research generated from the recent
TREC Session Tracks [20, 21]. Guan et al. [10] organized
phrase structure in queries within a session to improve retrieval effectiveness. Jiang et al. [14] proposed an adaptive
browsing model that handles novelty in session search. Jiang
and He [13] further analyzed the effects of past queries and
click-through data on whole-session search effectiveness.
Others study even more complex search – search across
multiple sessions [22, 24, 30]. Kotov et al. [22] proposed
methods for modeling and analyzing users’ search behaviors
in multiple sessions. Wang et al. [30] identified cross-session
search by investigating inter-query dependencies learned from
user behaviors.
Our approach is a content-based approach. However, it
uniquely differs from other approaches by taking a Markov
process point of view to study session search.
1
http://research.microsoft.com/en-us/um/people/
nickcr/wscd2014

588

2.2

Relevance feedback

Actions A is a discrete set of actions that an agent can
take. For instance, user actions include query changes, clicks,
and reading the returned documents or snippets.
Transition T is the state transition function T (si , a, sj ) =
P (sj |si , a). It is the probability of starting in state si , taking
action a, and ending in state sj . The sum over all actions
gives the total state transition probability between si and sj
T (si , sj ) = P (sj |si ); which is similar to the state transition
probability in the Hidden Markov Model (HMM) [1].
Reward r = R(s, a) is the immediate reward, also known
as reinforcement. It gives the expected immediate reward of
taking action a at state s.
Policy π describes the behaviors of an agent. A nonstationary policy is a sequence of mapping from states to
actions. π is usually optimized to decide how to moveP
around
in the state space to optimize the long term reward ∞
t=1 r.
Value function and Q-function Given a policy π at
time t, a value function V calculates the expected long term
∗
reward
Vπ,t
(s) = R(s, πt (s))+
P starting from state s∗ inductively:
γ s0 T (s, a = πt (s), s0 )Vπ,t+1
(s0 ), where the initial value
Vπ,t=1 (s) = R(s, a = πt=1 (s)), s is the current state, s0
is the next state, a = πt (s) is any valid action for s at t,
γ is a future discount factor. Usually an auxiliary function, called the Q-function,
P is used for a pair of (s, a):
Q∗ (st , a) = R(st , a) + γ a P (st |st+1 , a) maxa Q(st+1 , a),
where R(st , a) is the immediate reward at t, a is any valid
action at t + 1. Note that V ∗ (s) = maxa Q∗ (s, a).
Q-Learning Reinforcement learning (RL) algorithms provides solutions to MDPs [19]. The most influential RL algorithm is Q-learning. Given a Q-function and a starting
state s, the solution can be a greedy policy that at each
step, it takes theh action that maximizes the Q-function:
i
P
π ∗ (s) = arg maxa R(s, a) + γ a T (s, a, s0 )Q(s0 , a) , where

Session search is closely related to relevance feedback,
a traditional IR research field. Classic relevance feedback
methods include Rocchio [16], pseudo relevance feedback [2],
and implicit relevance feedback [27] based on user behaviors
such as clicks and dwell time. Recently, researchers have
investigated new forms of relevance feedback. Jin et al. [15]
employed a special type of click – “go to the next page” – as
relevance feedback to maximize retrieval effectiveness over
multi-page results. Zhang et al. [33] modeled query changes
between adjacent queries as relevance feedback to improve
retrieval accuracy in session search.
These relevance feedback approaches only considers oneway communication from the user to the search engine [15,
33]. On the contrary, this paper explicitly sets up a two-way
feedback channel where both parties transmit information.

2.3

MDP and POMDP in IR

Markov Decision Process (MDP) is an important topic
in Artificial Intelligence (AI). An MDP can be solved by a
family of reinforcement learning algorithms. Kaelbling et al.
[18] brought techniques from operational research to choose
the optimal actions in partially observable problems, and
designed algorithms for solving Partially Observable Markov
Decision Processes (POMDPs). IR researchers have just
begun showing interests in MDP and POMDP [11, 30, 32]
in finding solutions for IR problems.
Early work on interactive search modeling by Shen et al.
[26] used a Bayesian decision-theoretic framework, which is
closely related to the MDP approaches. The QCM model
proposed by Guan et al. [11] models session search as an
MDP and effectively improves the retrieval accuracy. However, [11] used queries as states while we use a set of welldesigned hidden decision states. In addition, we explicitly
model the stochastic game played between two agents, the
user and the search engine, while [11] focused on just the
search engine. Another difference is that we model a wide
range of actions including query changes, clicks, and document content while [11] only used query changes.
Yuan and Wang [32] applied POMDP for sequential selection of online advertisement recommendation. Their mathematical derivation shows that belief states of correlated ads
can be updated using a formula similar to collaborative filtering. Jin et al. [15] modeled Web search as a sequential
search for re-ranking documents in multi-page results. Their
hidden states are document relevance and the belief states
are given by a multivariate Gaussian distribution. They consider “ranking” as actions and “clicking-on-the-next-page” as
observations. In win-win search, we present a different set
of actions, observations, and messages between two agents.
The fundamental difference between our approach and theirs
is that we model the retrieval task as a dual-agent cooperative game while [15] uses a single agent.

3.

the base case maximizes R(s1 , a).
Partially Observable MDP (POMDP) When states
are unknown and can only be guessed through a probabilistic
distribution, an MDP becomes a POMDP [18]. POMDP is
represented by a tuple < S, A, T, Ω, O, B, R >, where S, A, R
are the same as in MDP. Since the states are unknown, the
transition function T models transitions between beliefs, not
transitions between states any more: T : B × A × B →
[0, 1]. Belief B is the set of beliefs defined over S, which
indicates the probability that an agent is at a state s. It
is also known as belief state. Observations Ω is a discrete
set of observations that an agent makes about the states. O
is the observation function which represents a probabilistic
distribution for making observation ω given action a and
landing in the next state s0 . A major difference between an
HMM and a POMDP is that POMDP considers actions and
rewards while HMM does not.

4.

THE WIN-WIN SEARCH FRAMEWORK

A session can be viewed as a Markov chain of evolving
states (Figure 1). Every time when a new query is issued,
both the user and the search engine transition into a new
state. In our setting, the two agents work together to achieve
a win-win goal.

PRELIMINARIES: MDP AND POMDP

Markov Decision Process provides the basics for the winwin search framework. An MDP is composed by agents,
states, actions, reward, policy, and transitions [19]. An agent
takes inputs from the environment and outputs actions. The
actions in turn influences the states of the environment. An
MDP can be represented by a tuple < S, A, T, R >:
States S is a discrete set of states. In session search, they
can be queries [11] or hidden decision states (Section 4.2).

4.1

Model Illustration

Figure 2 shows the proposed dual-agent SG, which is represented as a tuple < S, Au , Ase , Σu , Σse , Ωu , Ωse , O, B, T, R >.
S is the decision states that we will present in Section 4.2.

589

Table 2: Symbols in the dual-agent stochastic game.
Name
Symbol Meanings
State
S
the four hidden decision states in Figure 3
User action
Au
add query terms, remove query terms, keep query terms
Search engine action
Ase
increase/decrease/keep term weights, adjust search techniques, etc
Message from user to search engine
Σu
clicked and SAT clicked documents
Message from search engine to user
Σse
top k returned documents
User’s observation
Ωu
observations that the user makes from the world
Search engine’s observation
Ωse
observations that the search engine makes from the world and from the user
User reward
Ru
relevant information the user gains from reading the documents
Search engine reward
Rse
nDCG that the search gains by returning documents
Belief state
B
belief states generated from the belief updater and shared by both agents
2. t increases by one: t = t + 1. The user agent writes a
query q t and takes the tth user agent action atu , which
is a query change from the previous query.
3. (*) The search engine agent makes observations Ωse
from the world and updates its before-message-beliefstate bt•Σse based on O(S t , atu , Ωse ).
4. The search engine runs its optimization algorithm and
picks the best policy πse , which maximizes the joint
long term rewards for both agents. Following the policy, it takes actions atse . This is where the model performs retrieval.
5. Search engine action atse results in a set of documents
Dt , which are returned as message Σtse sent from the
search engine agent to the user agent through the world.
6. (*) The user agent receives message Σtse and observes
Ωu . If the user would like to stop the search, the
process ends. Otherwise, the user updates the aftermessage-belief-state btΣ•u based on O(S t , Σtse , Ωu ).
7. Based on the current beliefs, the user agent sends its
feedback messages Σtu to inform the search engine agent.
Σtu are clicks, some of which are SAT clicks. It contains
a set of documents Dclicked .
8. (*) The search engine agent observes Ωse from the
world and updates its after-message-belief-state btΣ•se
based on O(S t , Σtu , Ωse ).
9. The user agent picks a policy πu , which we don’t study
here, and continues to send out actions at+1
in the form
u
of query changes. The world moves into a new state
st+1 . t = t + 1. The process repeats from step 3.
Steps 3, 6, and 8 happen after making an observation from
the world or from the other agent. They then all involve a
belief update. In the remainder of this section, we present
the details of states (Section 4.2), actions (Section 4.3), observation functions (Section 4.4), and belief updates (Section
4.5) for win-win search.

Figure 2: Dual-agent stochastic game.

Au , Ase , Σu , and Σse are the actions. We divide the actions into two types: domain-level actions A, what an agent
acts on the world directly, and communication-level actions
Σ, also known as messages, which only go between the agents.
User actions Au are mainly query changes [11] while search
engine actions Ase are term weighting schemes and adjustments to search techniques. Both Σu and Σse are sets of
relevant documents, that an agent uses to inform the other
agent about what they consider as relevant. (Section 4.3)
Ω is the observation that an agent can draw from the world
or from the other agent. O is the observation function that
maps states and actions to observations: O : S × A → Ω or
O : S × Σ → Ω. Note that the actions can be domain-level
actions A or messages Σ, or a combination of both. (S. 4.4)
B is the set of belief states that shared by both agents.
The beliefs are updated every time when an observation happens. There are two types of belief: B•Σ , beliefs before the
messages and BΣ• , beliefs after the messages. (Section 4.5)
The reward function R is defined over B×A → R. It is the
amount of document relevance that an agent obtains from
the world. Rse is the nDCG score (normalized Discounted
Cumulative Gain [20]) that the search engine gains for the
documents it returns. Ru is the relevance that the user gains
from reading the documents. Our retrieval algorithm jointly
optimize both Ru and Rse . (Section 5)
Table 2 lists the symbols and their meanings in the dualagent SG. The two agents share the decision states and beliefs but differ in actions, messages, and policies. Although
they also make different observations, both contribute to the
belief updater; the difference is thus absorbed. As a retrieval
model, we only pay attention to the search engine policy
πse : B → A. The following describes their interactions in
the stochastic game:

4.2

States

We often observe that the same user behavior in session search may be motivated by different reasons. For instance, in TREC 2013 Session 2 (Table 1), a user searches
for “scooter brands” as q1 and finds that the 6th returned
document with title “Scooter Brands - The Scooter Review
- The Scooter Review” is relevant. The user clicks this document and reads it for 48 seconds, which we identify as a
SAT click since it lasts more than 30 seconds [12]. Next, the
user adds a new term ‘reliable’ into q1 to get q2 =“scooter
brands reliable”. We notice that ‘reliability’ does not appear in any previously retrieved documents D1 . It suggests
that the user is inspired to add ‘reliability’ from somewhere
else, such as from personal background knowledge or the in-

1. At search iteration t = 0, both agents begin in the
same initial state S 0 .

590

formation need. In this case, she finds relevant documents
from the previously retrieved documents but still decides to
explore other aspects about the search target.
On the contrary, in TREC 2013 Session 9 q1 (Table 1), another user searches for “old US coins” and also finds relevant
documents, such as a document about “... We buy collectible
U.S.A. coins for our existing coin collector clients...”. He
adds a new term ‘collecting’ to get the next query q2 “collecting old us coins”. After reducing the query terms and
document terms into their stemmed forms, the added term
‘collecting’ does appear in this document as we can see. It
suggests that the user selects a term from the retrieval results and hones into the specifics. In this case, he finds
relevant documents in the previously retrieved documents
and decides to exploit the same sub information need and
investigate it more.
We observe that even if both users show the same search
behavior, e.g. adding terms, the reasons vary: one is adding
the new search term because the original search results are
not satisfactory, while the other is because the user wants
to look into more specifics. This makes us realize that document relevance and users’ desire to explore are two independent dimensions in deciding how to form the next query.
Inspired by earlier research on user intent and task types
[24, 28] and our own observations, we propose four hidden
decision making states for session search. They are identified
based on two dimensions: 1) “relevant dimension” – whether
the user thinks the returned documents are relevant, and
2) “exploration dimension” – whether the user would like
to explore another subtopic. The two dimensions greatly
simplify the complexity of user modeling in session search.
The relatively small number of discrete states enables us to
proceed with POMDP and its optimization at low cost.
The cross-product of the two dimensions result in four
states: i) user finds a relevant document from the returned
documents and decides to explore the next sub information
need (relevant and exploration, e.g. scooter price → scooter
stores), ii) user finds relevant information and decides to
stay in the current sub information need to look into more
relevant information (relevant and exploitation, e.g. hartford visitors → hartford connecticut tourism), iii) user finds
out that the returned documents are not relevant and decides to stay and try out different expressions for the same
sub information need (non-relevant and exploitation, e.g.
philadelphia nyc travel → philadelphia nyc train), iv) user
finds out that documents are not relevant and decides to
give up and move on to another sub information need (nonrelevant and exploration, e.g. distance new york boston →
maps.bing.com).
Figure 3 shows the decision state diagram for win-win
search. The subscriptions stand for {RT = Relevantexploi
T ation, RR = RelevantexploRation, N RT = N onRelevant
exploiT ation, N RR = N onRelevantexploRation}. We insert a dummy starting query q0 before any real query and it
always goes to SN RR . The series of search iterations in a session move in the decision states from one to the next. A sequence of states can be time stamped and presented as st =
Sm , where t = 1, 2, ..., n and m = {RT, RR, N RT, N RR}.

4.3

Figure 3: States.

4.3.1

Domain-Level Actions

The domain-level actions Au and Ase represent the actions
directly performed on the world (document collection) by
the user agent and by the search engine agent, respectively.
The common user actions include writing a query, clicking
a document, SAT clicking a document, reading a snippet,
reading a document, changing a query, and eye-tracking the
documents. In this paper, we only study query changes and
clicks as user actions. However, the framework can be easily
adopted for other types of user actions.
Query changes ∆q [11] consist of added query terms +∆qt =
qt \qt−1 , removed query terms −∆qt = qt−1 \qt , and theme
terms qtheme = LongestCommon Subsequence(qt , qt−1 ). For
example, in Session 87 , given q19 =“philadelphia nyc travel”
and q20 =“philadelphia nyc train”, we obtain the following
query changes: qtheme = LCS(q19 , q20 ) = “philadelphia”,
−∆q20 = “travel”, and +∆q20 = “train”. All stopwords and
function words are removed.
The search engine domain-level actions Ase include increasing, decreasing, and maintaining the term weights, as
well as adjusting parameters in one or more search techniques. We present the details in Sections 5 and 6.

4.3.2

Communications-Level Actions (Messages)

The second type of actions are communication-level actions (messages) Σu and Σse . They are actions that only
performed between agents.
In our framework, the messages are essentially documents
that an agent thinks are relevant. Σu is the set of documents that the user sends out; we define them as the clicked
documents Dclicked . In TREC 2013 Session, 31% search
iterations contain SAT clicked documents. 23.9% sessions
contain 1 to 4 SAT clicked documents, and a few sessions,
for instance Sessions 45, 57 and 72, contain around 10 SAT
clicked documents. 88.7% SAT clicked documents appear in
the top 10 retrieved results.
Similarly, Σse is the set of documents that the search engine sends out. They are the top k returned documents
(k ranges from 0 to 55 in the TREC setting). They demonstrate what documents the search engine thinks are the most
relevant. In TREC 2013, 2.8% (10) search iterations return
less than 10 documents, 90.7% (322) return exactly 10, 5.1%
(18) return 10∼20, and 1.4% (5) return 20∼55 documents.

4.4

Observations

Section 4.1 illustrates the win-win search framework and
the interactions between agents. This section shows how we
calculate the observation functions.
The observation function O(sj , at , ωt ), defined as P (ωt |sj , at ),
is the probability of observing ωt ∈ Ω when agents take action at and land on state sj . The first type of observation

Actions

There are two types of actions in our framework, domainlevel actions and communications-level actions.

591

is related to relevance. In Section 4.1 Step 8, after the user
sends the message Σu (user clicks) out at Step 7, the search
engine updates its after-message-belief-state bΣ•se based on
its observation of user clicks. The observation function for
‘Relevant’ states is:

decision states) by:
O(st =Exploitation, au =∆qt , Σse =Dt−1 , ωt =Exploitation)
∝ P (st = Exploitation|ωt = Exploitation)
×P (ωt = Exploitation|∆qt , Dt−1 )
(6)
O(st =Exploration,au =∆qt , Σse =Dt−1 , ωt =Exploration)
∝ P (st = Exploration|ωt = Exploration)
×P (ωt = Exploration|∆qt , Dt−1 )

def

O(st =Rel, Σu , ωt =Rel) ==== P (ωt = Rel|st = Rel, Σu )
(1)
t =Rel,Σu )
It can be written as P (ωtP=Rel,s
.
By
taking
P
(s
=
t
(st =Rel,Σu )
Rel, Σu ) as a constant, we can approximate it by P (ωt =
Rel, st = Rel, Σu ) = P (st = Rel|ωt = Rel, Σu )P (ωt =
Rel, Σu ). Given that user clicks Σu are highly correlated
to ωt , we can approximate P (st = Rel|ωt = Rel, Σu ) by
P (st = Rel|ωt = Rel). Further, by taking P (Σ) as a constant, we have

(7)

The search engine can guess
the following intuition:


 Exploration
st is likely to be
Exploitation



O(st =Rel, Σu , ωt =Rel) ∝ P (st = Rel|ωt = Rel)P (ωt = Rel, Σu )
∝ P (st = Rel|ωt = Rel)P (ωt = Rel|Σu )
(2)
Similarly, we have
O(st =Non-Rel, Σu , ωt =Non-Rel)
∝ P (st = Non-Rel|ωt = Non-Rel)P (ωt = Non-Rel|Σu )
(3)
as well as O(st =Non-Rel, Σu , ωt =Rel) and O(st =Rel, Σu ,
ωt =Non-Rel).
Based on whether a SATClick exists or not, we calculate
the probability of the SG landing at the “Relevant” states
or the “Non-Relevant” states (the first dimension of hidden
decision states). At search iteration t, if the set of previously
returned documents leads to one or more SAT clicks, the
current state is likely to be relevant, otherwise non-relevant.
That is to say,
(
Relevant

st is likely to be
Non-Relevant

Based on this intuition, we calculate P (ωt = Rel|Σu ) and
P (ωt = Non-Rel|Σu ) as:

P (ωt = Non-Rel|Σu ) = P (@ SATClicks ∈

t−1
Dclicked
)

if (+∆qt 6= ∅ and +∆qt ∈
/ Dt−1 )
or (+∆qt = ∅ and −∆qt 6= ∅ )
if (+∆qt 6= ∅ and +∆qt ∈ Dt−1 )
or (+∆qt = ∅ and −∆qt = ∅ )

The idea is that given that Dt−1 is the message from search
engine and au = ∆q is the message from user, if added query
terms +∆q appear in Dt−1 , it is likely that the user stays at
the same sub information need from iteration t − 1 to t for
‘exploitation’. On the other hand, if the added terms +∆q
do not appear in Dt−1 , it is likely that the user moves to
the next sub information need from iteration t − 1 to t for
‘exploration’. In addition, if there is no added terms (+∆qt
is empty) but there are deleted terms ( −∆qt is not empty),
it is likely that the user goes to a broader topic to explore. If
+∆qt and −∆qt are both empty, it means there is no change
to the query, it is likely to fall into exploitation.
Hence, P (ωt |∆qt , Dt−1 ) can be calculated as:
P (ωt = Exploration|∆qt , Dt−1 ) =

if ∃ d ∈ Dt−1 and
d is SATClicked
otherwise.

t−1
)
P (ωt = Rel|Σu ) = P (∃ SATClicks ∈ Dclicked

the hidden states based on

(4)
(5)

The conditional probability of observations P (st = Rel|ωt =
Rel) and P (st = Non-Rel|ωt = Non-Rel) can be calculated
by maximum likelihood estimation (MLE). For instance,
of observed true relevant
, where “#
P (st = Rel|ω = Rel) = # #
of observed relevant
of observed true relevant” is the number of times where the
previously returned document set Dt−1 contain at least one
SAT clicks and those SAT clicked documents are indeed relevant documents in the ground truth. “# of observed relevant” is the number of times where Dt−1 contains at least
one SAT clicks. The ground truth of whether the SG lands
on a “Relevant” state is generated by documents whose relevance grades ≥ 3 (relevant to highly relevant). The relevance
are judged by NIST assessors [21].
The second type of observation is related to exploitation
vs. exploration. This corresponds to a combined observation at Step 3 and the previous Step 6 (Section 4.1), where
the SG update the before-message-belief-state b•Σse for a
user action au (query change) and a search engine message
Σse =Dt−1 , the top returned documents at the previous iteration. The search engine agent makes observations about
exploitation vs. exploration (the second dimension of hidden

592

P (+∆qt 6= ∅ ∧ +∆qt ∈
/ Dt−1 )
+P (+∆qt = ∅ ∧ −∆qt 6= ∅)
(8)
P (ωt = Exploitation|∆qt , Dt−1 ) = P (+∆qt 6= ∅ ∧ +∆qt ∈ Dt−1 )
+P (+∆qt = ∅ ∧ −∆qt = ∅)
(9)
where Dt−1 include all clicked documents and all snippets
that are ranked higher than the last clicked document at
iteration t − 1. User actions au include the current query
changes +∆qt and −∆qt . In fact, P (ωt |∆qt , Dt−1 ) needs to
be calculated for each specific case. For instance, P (ωt =
Exploration|a = ‘delete term’, ∆qt , Dt−1 ) =
# of observed true explorations due to deleting terms
. Here we only
# of observed explorations due to deleting terms
calculate for the actions with “deleted terms”. “# of observed explorations” is the number of observed explorations
suggesting that the user is likely to explore another subtopic
based on Eq. 8, while “# of observed true explorations” is
the number of observed explorations judged positive by human accessors in a ground truth. The annotations can be
found online.2
The conditional probability P (st = Exploitation|ωt =
of observed true exploitations
, where
Exploitation) is calculated as # #
of observed exploitations
“# of observed exploitations” is the number of observed exploitations suggesting that the user is likely to exploit the
same subtopic (based on Eq. 9), and “# of observed true exploitations” is the number of observed exploitations that are
judged positive in the ground truth. P (st = Exploration|ωt =
Exploration) is calculated in a similar way.

4.5

Belief Updates

At every search iteration the belief state b is updated
twice; once at Step 3, another at Step 8. It reflects the
interaction and cooperative game between the two agents.
2
The manual annotations for “exploration” transitions can
be found at www.cs.georgetown.edu/~huiyang/win-win.

A belief bt (si ) is defined as P (si |at , bt ). The initial belief
states can be calculated as: b0 (si = Sz ) = P (si = Sx )P (si =
Sy ), where x ∈ {R = Rel, N R = N on-Rel}, y ∈ {R =
exploRation, T = exploiT ation}, z is the cross-product of
x and y and zP∈ {RR, RT, N RR, N RT }. In addition, 0 ≤
b(si ) ≤ 1 and si b(si ) = 1.
The belief update function is bt+1 (sj ) = P (sj |ωt , at , bt )
by taking into account new observations ωt . It is updated
from iteration t to iteration t + 1:

Table 3: Dataset statistics.

def

bt+1 (sj ) ==== P (sj |ωt , at , bt )
P
O(sj , at , ωt ) si ∈S T (si , at , sj )bt (si )
=
P (ωt |at , bt )

TREC 2012
98
48
297
3.03
11
2.04

#Sessions
#Search topics
#Queries
Avg. session length
Max session length
Avg. #sessions per topic

(10)

where si and sj are two states, i, j ∈ {RR, RT, N RR, N RT }.
t indices the search iterations, and O(sj , at , ωt ) = P (ωt |sj , at )
is calculated based on Section
P 4.4. P (ωt |at , bt ) is the normalization factor to keep
si ∈S b(si ) = 1. For notation
simplicity, we will only use a to represent actions from now
on. However, it is worthy noting that actions can be both
domain-level actions a and messages Σ.
Transition probability T (si , at , sj ) is defined as P (sj |si , at , bt ).
#T ransition(s ,a ,s )
It is can be calculated as T (si , at , sj ) = #T ransition(sii ,att ,s∗j ) ,
where Transition (si , at , sj ) is the sum of all transitions that
starts at state si , takes action at , and lands at state sj .
T ransition (si , at , s∗) is the sum of all transitions that starts
at state si and lands at any state by action at .
Finally, taking O(sj , at , ωt ) = P (ωt |sj , at ), which also
equals to P (ωt |sj , at , bt ) when we consider beliefs, and T (si , at , sj )
= P (sj |si , at , bt ), the updated belief can be written as:

TREC 2013
87
49
442
5.08
21
1.78

The formula matches well with common search scenarios
where the user makes decisions about their next actions
based on the most relevant document(s) they examined in
the previous run of retrieval. Such a document we call it
maximum rewarding document(s). We use document with
the largest P (qt−1 |dt−1 ) as the maximum
Q rewarding document. P (qt−1 |dt−1 ) is calculated as 1− t∈qt−1 {1 − P (t|dt−1 )},
#(t,d

)

t−1
, #(t, dt−1 ) is the number of
where P (t|dt−1 ) = |dt−1
|
occurrences of term t in document dt−1 , and |dt−1 | is the
document length.
By optimizing both long term rewards for the user and
for the search engine, we learn the best policy π and use it
to predict the next action for the search engine. The joint
optimization for the dual-agent SG can be represented as:


ase = arg max Qse (b, a) + Qu (b, au )
a

(14)

where bt (si ) is P (si |at , bt ), whose initial value is b0 (si ).

where ase ∈ Ase at t = n and n is the number of search
iterations in a session, i.e., the session length.
In win-win search, Ase can include many search engine actions. One type of actions is adjusting a query’s term weight.
Assuming the query is reformulated from the previous query
by adding +∆q or deleting −∆q. That is to say, Ase = {increasing, decreasing, or keeping term weights}. The term
weights are increased or decreased by multiplying a factor.
We also use a range of search techniques/algorithms as action options for the search engine agent. They are reported
in Section 6. Based on Eq. 14, the win-win search framework picks the optimal search engine action.

5.

6.

bt+1 (sj ) =

P (ωt |sj , at , bt )

P

si ∈S

P (sj |si , at , bt )bt (si )

P (ωt |at , bt )
P
P (ωt |sj , at , bt ) si ∈S P (sj |si , at , bt )bt (si )
P
= P
sk ∈S P (ωt |sk , at )
si ∈S P (sk |si , at )bt (si )

(11)

JOINT OPTIMIZATION AND RETRIEVAL

After every search iteration, we decide the actions for the
search engine agent. We employ Q-learning [18] to find out
the optimal action. For all a ∈ Ase , we write the search engine’s Q-function, which represents the search engine agent’s
long term reward, as:

EVALUATION

We evaluate the proposed framework on TREC 2012 and
TREC 2013 Session Tracks [20, 21]. The session logs are
collected from users through a search system by the track
organizers. The topics, i.e., information need (Table 1), are
provided to users. The session logs record all URLs disX
played to the user, snippets, clicks, and dwell time. Table 3
0
Qse (b, a) = ρ(b, a)+γ
P (ω|b, au , Σse )P (ω|b, Σu ) max Qse (b , a)
shows the dataset statistics. The task is to retrieve a ranked
a
ω∈Ω
list of 2,000 documents for the last query in a session. Doc(12)
P
ument relevance is judged based on the whole-session relewhere the reward for a belief state b is ρ(b, a) = s∈S b(s)R(s, a).
vance. We use the official TREC evaluation metrics in our
P (ω|b, au , Σse ) corresponds to Eq. 8 and Eq. 9 and P (ω|b, Σu )
experiments. They include nDCG@10, nERR@10, nDCG,
0
corresponds to Eq. 4 and Eq. 5. b is the belief state upand MAP [21]. The ground truth relevant documents are
dated by Eq. 11.
provided by TREC.
In win-win search, we take into account both the search
The corpora used in our experiments are ClueWeb09 CatB
engine reward and the user reward. As in [11], we have Qu
(50 million English web pages crawled in 2009, used in TREC
calculated as the long term reward for the user agent:
2012), and ClueWeb12 CatB (50 million English web pages
P
crawled in 2012, used in TREC 2013). Documents with the
Qu (b, au ) = R(s,
P au ) + γ au T (st |st−1 , Dt−1 ) maxst−1 Qu (st−1 , au )
Waterloo spam scores [5] less than 70 are filtered out. All
= P (qt |d) + γ a P (qt |qt−1 , Dt−1 , a) maxDt−1 P (qt−1 |Dt−1 )
(13)
duplicated documents are removed.
which recursively calculates the reward starting from q1 and
We compare our system with the following systems: Lemur
continues with the policy until qt . P (qt |d) is the current
[23] (language modeling + Dirichlet smoothing), PRF (Pseudo
reward that the user gains through reading the documents.
Relevance Feedback in Lemur assuming the top 20 documaxDt−1 P (qt−1 |Dt−1 ) is the maximum of the past rewards.
ments are relevant), Rocchio (relevance feedback that as-

593

find out the optimal action that maximizes the joint long
term reward Qse (b, a) + Qu (b, au ) for both agents.
Table 4 shows the search accuracy of all systems under
comparison for TREC 2012 Session Track. We can see that
win-win search is better than most systems except QCM
SAT. It statistically significantly outperforms Rocchio by
20%, Lemur by 18.9%, and PRF by 41.8% in nDCG@10
(p-value<.05, one-side t-test). It also outperforms RocchioCLK, Rocchio-SAT and QCM+DUP, but the results are not
statistically significant. The trends for other evaluation metrics are similar to nDCG@10.
Table 5 shows the search accuracy of all systems for TREC
2013 Session Track. Since we only indexed ClueWeb12 CatB,
after spam reduction, many relevant CatA documents are
not included in the CatB collection. To evaluate the systems
fairly, we created a filtered ground truth which only consists
of relevant documents in CatB. The results are shown in
Table 5. We can see that win-win is the best run among all
systems. It shows statistically significant gain (p-value<.01,
one-sided t-test) over all other systems across all evaluation metrics. Particularly, the proposed approach achieves
a significant 54% improvement of nDCG@10 comparing to
QCM+DUP. The experimental results support that our approach is highly effective.

Table 4: Search accuracy on TREC 2012 Session (∗
indicates a statistical significant improvement over
Rocchio at p < 0.05 (t-test, one-sided)); ∗ indicates a
statistical significant improvement over QCM+DUP
at p < 0.05 (t-test, one-sided)).
Approach
Lemur
TREC median
TREC best
PRF
Rocchio
Rocchio-CLK
Rocchio-SAT
QCM+DUP
QCM SAT
Win-Win

nDCG@10
0.2474
0.2608
0.3221
0.2074
0.2446
0.2916†
0.2889
0.2742
0.3350∗†
0.2941†

nDCG
0.2627
0.2468
0.2865
0.2335
0.2714
0.2866
0.2836
0.2560
0.3054
0.2691

MAP
0.1274
0.1440
0.1559
0.1065
0.1281
0.1449
0.1467
0.1537†
0.1534†
0.1346

nERR@10
0.2857
0.2626
0.3595
0.2415
0.2950
0.3366
0.3254
0.3221
0.1534
0.3403

Table 5: Search accuracy on TREC 2013 Session (†
indicates a statistical significant improvement over
Rocchio at p < 0.01 (t-test, one-sided)); ∗ indicates a
statistical significant improvement over QCM+DUP
at p < 0.01 (t-test, one-sided)).
Approach
Lemur
TREC median
TREC best
PRF
Rocchio
Rocchio-CLK
Rocchio-SAT
QCM+DUP
QCM SAT
Win-Win

nDCG@10
0.1147
0.1531
0.1952
0.1061
0.1320
0.1315
0.1147
0.1316
0.1186
0.2026∗†

nDCG
0.1758
–
–
0.1701
0.1924
0.1929
0.1758
0.1929
0.1754
0.2609∗†

MAP
0.0926
–
–
0.0787
0.1060
0.1060
0.0926
0.1060
0.0939
0.1290∗†

nERR@10
0.1314
–
–
0.1245
0.1549
0.1546
0.1314
0.1547
0.1425
0.2328∗†

6.2

TREC Session tasks request for retrieval results for the
last query in a session. Theoretically, however, win-win
search can optimize at every search iteration throughout a
session. We hence compare our approach (the Win-Win run)
with the top returned documents provided by TREC (the
Original run) in terms of immediate search accuracy. We
define immediate search accuracy at i as an evaluation score
that measures search accuracy at search iteration i. The
evaluation scores used are nDCG@10 and nERR@10.
We report the averaged immediate search accuracy for all
sessions. It is worthy noting that session lengths vary. To
average across sessions with different lengths, we make all
sessions equals to the maximum session length in a dataset.
TREC 2012 and 2013 Session have different maximum session lengths; they are 11 and 21, respectively. When a session is shorter than the maximum session length, we use the
retrieval results from its own last iteration as the retrieval
results for iterations beyond its own last iteration. In addition, since TREC did not provide any retrieval results for
the last query, the Original runs has no value at the last
iteration.
Figures 4 and 5 plot the immediate search accuracy for
TREC 2012 & 2013 Session Tracks averaged over all sessions. We observe that win-win search’s immediate search
accuracy is statistically significantly better than the Original run at every iteration. In Figure 4, win-win outperforms
Original since iteration 2 in nDCG@10 and outperforms it
since iteration 3 in nERR@10. At the last iteration, winwin outperforms Original by a statistically significant 27.1%
in nDCG@10 (p-value<.05, one-sided t-test). We observe
similar trends in Figure 5. Another interesting finding is
that win-win search’s immediate search accuracy increases
while the number of search iterations increases. In Figure 4,
the nDCG@10 starts at 0.2145 at the first iteration and increases dramatically 37.1% to 0.2941 at the last iteration. It
suggests that by involving more search iterations, i.e., learning from more interactions between the user and the search

sumes the top 10 previous retrieved documents are relevant),
Rocchio-CLK (implicit relevance feedback that assumes only
previous clicked documents are relevant), Rocchio-SAT (implicit relevance feedback that assumes only previous SATclicked documents are relevant), QCM+DUP (the QCM approach proposed by [11]), and QCM SAT (a variation of
QCM by [33]). We choose Rocchio (a state-of-the-art interactive search algorithm) and QCM+DUP (a state-of-the-art
session search algorithm) as two baseline systems and all
other systems are compared against them. TREC median
and TREC best scores are also included for reference. Note
that TREC best are an aggregation from the best scores
of each individual submitted TREC runs; it is not a single
search system.

6.1

Immediate Search Accuracy

Search Accuracy

Our run, win-win, implements six retrieval technologies.
They are: (1) increasing weights of the added terms (+∆q)
by a factor of x={1.05, 1.10, 1.15, 1.20, 1.25, 1.5, 1.75 or 2};
(2) decreasing weights of the added terms by a factor of y
={ 0.5, 0.57, 0.67, 0.8, 0.83, 0.87, 0.9 or 0.95}; (3) the term
weighting scheme proposed in [11] with parameters α, β, δ,
γ set as 2.2, 1.8, 0.4, 0.92; (4) a PRF (Pseudo Relevance
Feedback) algorithm which assumes the top p retrieved documents are relevant while p ranges from 1 to 20; (5) an adhoc variation of win-win, which directly uses the last query in
a session to perform retrieval; and (6) a brute-force variation
of win-win, which combines all queries in a session, extracts
all unique query terms from them, and weights them equally.
Win-win examine 21 search engine action options in total to

594

Figure 7: Factual and Specific sessions.

Figure 4: TREC 2012 Im- Figure 5: TREC 2013 Immediate Search Accuracy. mediate Search Accuracy.

Figure 8: Factual and Amorphous sessions.
Figure 6: Long sessions (length >= 4). Transition
probabilities are listed with actions: Add (A), Remove (R), and Keep (K).
engine, win-win is able to monotonically improve its search
accuracy.

6.3

State Transitions

This experiment investigates how legitimate the proposed
states are in presenting the hidden mental states of users.
First, we use examples to demonstrate the state transitions in sessions. Session 87 (Table 1) is a long session with
21 queries. The chain of decision states identified for this
session based on techniques presented in Sections 4.2 and
4.4 is: SN RR (q1 =best us destination) → SRT (q2 =distance
new york boston) → SN RT → SN RR → SN RR → SRR →
SRR → SN RR → SRT → SRT → SRR (q11 =boston tourism)
→ SN RR (q12 =nyc tourism) → SN RR (q13 =philadelphia nyc
distance) → SN RR → SRT → SN RR → SRR → SN RT →
SN RT (q19 =philadelphia nyc travel) → SN RT (q20 =philadelphia
nyc train) → SN RT (q21 =philadelphia nyc bus). Our states
correctly suggests that the user is in the exploration states
(RR, NRR, NRR) from q11 to q13 , while he keeps changing queries to explore from city to city (boston, new york
city, and philadelphia). The user eventually finds the cities,
philadelphia and nyc, that fulfill the information need – “best
US destinations within a 150-mile radius”. During the last
3 queries, the user exploits the current subtopic (philadelphia and nyc) to find out more specifics on transportations
(travel, train, bus) about them. Our system correctly recognizes the last three states as exploitation states (NRT, NRT,
NRT). This example suggests that the proposed states are
able to reflect the real user decision states quite accurately.
Second, we examine state transition patterns in long sessions since they contain enough transitions for us to study.
Figure 6 plots state probabilities, state transition probabilities, and that under different user actions for long sessions
(sessions with 4 or more queries). The data are combined
from both TREC 2012 & 2013 Session Tracks. We notice
that NRR (non-relevant and exploration) is the most com-

595

mon state (42.4%). This reflects that a user spend may a
long time to explore while receiving non-relevant documents.
On the contrary, the RR state (relevant and exploration) is
the least common state (11.3%).Moreover, we see that state
transitions are not uniformly distributed. For instance, the
transition from NRT to both relevant states (RT and RR)
are very rare (in total 5.65%). In addition, we notice that
actions are related to state transition probabilities. There
are 90.8% transitions generated by adding terms and among
all the transitions with removing terms, 84.8% of them lead
to exploitation states (RT or NRT).
Third, we find that state probability distribution and state
probability transitions differ among different session types.
We plot the state probabilities and transition probabilities in
Figures 7 to 10 for four different TREC session types, which
were created along two aspects: search target (factual or
intellectual ) and goal quality (specific or amorphous). Suggested by [24], the difficulty levels of the session types usually are FactualSpecific < IntellectualSpecific < FactualAmorphous < IntellectualAmorphous. An interesting finding is
that as the session difficult level increases, the transition
probability from state NRT (non-relevant and exploitation)
to state RT (relevant and exploitation) becomes lower: FactualSpecific (0.25), IntellectualSpecific (0.2), FactualAmorphous (0.12), IntellectualAmorphous(0.1). It suggests that
the harder the task is, the greater the necessity to explore
rather than to exploit, when the user is not satisfied with
the current retrieval results. In addition, we observe that
Intellectual sessions (Figures 9 and 10) have a larger probability, 0.1548, to be in the RR (relevant and exploration)
state than the other session types (on average 0.1018).

7.

CONCLUSION

This paper presents a novel session search framework, winwin search, that uses a dual-agent stochastic game to model
the interactions between user and search engine. With a
careful design of states, actions, and observations, the new
framework is able to perform efficient optimization over a finite discrete set of options. The experiments on TREC Ses-

[3] B. Carterette, E. Kanoulas, and E. Yilmaz. Simulating simple
user behavior for system effectiveness evaluation. In CIKM ’11.
[4] M.-A. Cartright, R. W. White, and E. Horvitz. Intentions and
attention in exploratory health search. In SIGIR ’11.
[5] G. V. Cormack, M. D. Smucker, and C. L. Clarke. Efficient and
effective spam filtering and re-ranking for large web datasets.
Inf. Retr., 14(5), Oct. 2011.
[6] A. Diriye, R. White, G. Buscher, and S. Dumais. Leaving so
soon?: Understanding and predicting web search abandonment
rationales. In CIKM ’12.
[7] C. Eickhoff, K. Collins-Thompson, P. N. Bennett, and
S. Dumais. Personalizing atypical web search sessions. In
WSDM ’13.
[8] H. Feild and J. Allan. Task-aware query recommendation. In
SIGIR ’13.
[9] S. Fox, K. Karnawat, M. Mydland, S. Dumais, and T. White.
Evaluating implicit measures to improve web search. ACM
Trans. Inf. Syst., 23(2), Apr. 2005.
[10] D. Guan, H. Yang, and N. Goharian. Effective structured query
formulation for session search. In TREC ’12.
[11] D. Guan, S. Zhang, and H. Yang. Utilizing query change for
session search. In SIGIR ’13.
[12] Q. Guo and E. Agichtein. Ready to buy or just browsing?:
detecting web searcher goals from interaction data. In SIGIR
’10.
[13] J. Jiang and D. He. Different effects of click-through and past
queries on whole-session search performance. In TREC ’13.
[14] J. Jiang, D. He, and S. Han. Pitt at trec 2012 session track. In
TREC ’12.
[15] X. Jin, M. Sloan, and J. Wang. Interactive exploratory search
for multi page search results. In WWW ’13.
[16] T. Joachims. A probabilistic analysis of the rocchio algorithm
with tfidf for text categorization. 1997.
[17] R. Jones and K. L. Klinkner. Beyond the session timeout:
automatic hierarchical segmentation of search topics in query
logs. In CIKM ’08.
[18] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning
and acting in partially observable stochastic domains. Artificial
intelligence, 101(1):99–134, 1998.
[19] L. P. Kaelbling, M. L. Littman, and A. W. Moore.
Reinforcement learning: a survey. J. Artif. Int. Res., 4(1), May
1996.
[20] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and
M. Sanderson. Overview of the trec 2012 session track. In
TREC’12.
[21] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and
M. Sanderson. Overview of the trec 2013 session track. In
TREC’13.
[22] A. Kotov, P. N. Bennett, R. W. White, S. T. Dumais, and
J. Teevan. Modeling and analysis of cross-session search tasks.
In SIGIR ’11.
[23] Lemur Search Engine. http://www.lemurproject.org/.
[24] J. Liu and N. J. Belkin. Personalizing information retrieval for
multi-session tasks: The roles of task stage and task type. In
SIGIR ’10.
[25] K. Raman, P. N. Bennett, and K. Collins-Thompson. Toward
whole-session relevance: Exploring intrinsic diversity in web
search. In SIGIR ’13.
[26] X. Shen, B. Tan, and C. Zhai. Implicit user modeling for
personalized search. In CIKM ’05.
[27] Y. Song and L.-w. He. Optimal rare query suggestion with
implicit user feedback. In WWW ’10.
[28] A. R. Taylor, C. Cool, N. J. Belkin, and W. J. Amadio.
Relationships between categories of relevance criteria and stage
in task completion. Information Processing & Management,
43(4), 2007.
[29] J. Teevan, S. T. Dumais, and D. J. Liebling. To personalize or
not to personalize: Modeling queries with variation in user
intent. In SIGIR ’08.
[30] H. Wang, Y. Song, M.-W. Chang, X. He, R. W. White, and
W. Chu. Learning to extract cross-session search tasks. In
WWW ’13.
[31] R. W. White, I. Ruthven, J. M. Jose, and C. J. V. Rijsbergen.
Evaluating implicit feedback models using searcher simulations.
ACM Trans. Inf. Syst., 23(3), July 2005.
[32] S. Yuan and J. Wang. Sequential selection of correlated ads by
pomdps. In CIKM ’12.
[33] S. Zhang, D. Guan, and H. Yang. Query change as relevance
feedback in session search. In SIGIR ’13.

Figure 9: Intellectual and Specific sessions.

Figure 10: Intellectual and Amorphous sessions.
sion 2012 and 2013 datasets show that the proposed framework is highly effective for session search.
Session search is a complex IR task. The complexity
comes from the involvement of many more factors other
than just terms, queries and documents in most existing
retrieval algorithms. The factors include query reformulations, clicks, time spent to examine the documents, personalization, query intent, feedback, etc. Most existing work
on sessions and task-based search focuses on diving into one
aspect. Through significantly simplifying the factors, we realize the integration of all the factors in a unified framework.
For example, we simplify users’ decision states into only four
states, and discretize user actions and search engine actions
into a finite number of options. Such simplification is necessary in creating practical search systems.
This paper views the search engine as an autonomous
agent, that works together with user, another autonomous
agent, to collaborate on a shared task – fulfilling the information needs. This view assumes that the search engine is
more like a “decision engine”. Session search can be imagined as two agents exploring in a world full of information,
searching for the goal in a trial-and-error manner. Here we
assume a cooperative game between the two agents. However, as we mentioned in the introduction, the search engine
agent can of course choose a different goal. It will be very
interesting to see how to still satisfy the user to achieve winwin. We hope our work calls for future adventures in the
fields of POMDP in IR and game theory in IR.

8.

ACKNOWLEDGMENT

This research was supported by NSF grant CNS-1223825.
Any opinions, findings, conclusions, or recommendations expressed in this paper are of the authors, and do not necessarily reflect those of the sponsor.

9.

REFERENCES

[1] L. E. Baum and T. Petrie. Statistical inference for probabilistic
functions of finite state markov chains. The annals of
mathematical statistics, 37(6), 1966.
[2] G. Cao, J.-Y. Nie, J. Gao, and S. Robertson. Selecting good
expansion terms for pseudo-relevance feedback. In SIGIR ’08.

596

