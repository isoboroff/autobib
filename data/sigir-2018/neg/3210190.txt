Tutorial

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Knowledge Extraction and Inference from Text:
Shallow, Deep, and Everything in Between
Soumen Chakrabarti
IIT Bombay

3

ACM Reference Format:
Soumen Chakrabarti. 2018. Knowledge Extraction and Inference from Text:
Shallow, Deep, and Everything in Between. In SIGIR ’18: The 41st International ACM SIGIR Conference on Research & Development in Information
Retrieval, July 8–12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA,
4 pages. https://doi.org/10.1145/3209978.3210190

1

Before 2007 or so, interest in knowledge representation was limited
to researchers of symbolic AI, a section of semantic Web enthusiasts, and builders of question answering (QA) systems. Important
machine learning techniques were being invented for front-end
NLP such as POS tagging, chunking, and named entity recognition
(NER). But it was only after Wikipedia became among the largest
and most reputed repositories of semi-structured knowledge, and
Google purchased the Freebase knowledge graph (KG), that largescale analysis of the implicit and explicit links between knowledge
graphs and text corpora began to draw intense attention. With the
more recent triumph of deep learning, the pace of new developments has made it very tricky to keep track of best practices without
a birds-eye view of the field. Our goal is to provide a 10-year historical perspective that can guide a researcher or practitioner’s choice
of methods for a variety of extraction, inference and search tasks
that bring together text and knowledge representation for IR tasks.

ABSTRACT

Systems for structured knowledge extraction and inference have
made giant strides in the last decade. Starting from shallow linguistic tagging and coarse-grained recognition of named entities at
the resolution of people, places, organizations, and times, modern
systems link billions of pages of unstructured text with knowledge
graphs having hundreds of millions of entities belonging to tens of
thousands of types, and related by tens of thousands of relations.
Via deep learning, systems build continuous representations of
words, entities, types, and relations, and use these to continually
discover new facts to add to the knowledge graph, and support
search systems that go far beyond page-level “ten blue links”. We
will present a comprehensive catalog of the best practices in traditional and deep knowledge extraction, inference and search. We
will trace the development of diverse families of techniques, explore
their interrelationships, and point out various loose ends.

4

OBJECTIVES

After attending this tutorial, attendees will be able to:
• Read and critically understand papers on information extraction, type prediction and entity disambiguation for corpus
and queries, closed- and open-domain relation extraction,
and entity ranking and question answering using these building blocks.
• Trace the evolution from feature-driven to deep learning
paradigms for solving the above problems.
• Get some initial familiarity with how modern information
extraction and neural entity search and QA systems are
implemented.
• Gain critical insights into what remain difficult problems in
these areas.

Support material for attendees: Slides and other resources are
at https://sites.google.com/site/knowxtext/.

2

MOTIVATION AND RELEVANCE TO THE IR
COMMUNITY

TARGET AUDIENCE: INTRODUCTORY TO
INTERMEDIATE

We will target early-career academic researchers and industrial
practitioners. Attendees are expected to have some basic familiarity
with text indexing and corpus statistics (tokenization, typical heavytailed vocabularies, TFIDF). They are expected to be largely familiar
with undergrad statistics (probability, distributions, divergence).
Some elementary machine learning (clustering, regression and classification; logistic regression, support vector machine basics) will
also help but is not mandatory.

5

SCHEDULE AND CONTENT SUMMARY

Based on the experience of teaching a few graduate elective courses
and tutorials, we list below the important topics to be covered. As
we do this, we include key citations. All citations are listed at the end
of this proposal. Sections whose removal can shorten the tutorial
without drastic loss of continuity are color coded and marked with
a hollow bullet ‘◦’. The content will be collected into four 1.5-hour
lectures:
• Continuous embedding of words, entities, types and relations
• Morning break
• Fine typing and named entity disambiguation
• Lunch
• Closed-domain relation extraction and open information
extraction

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-5657-2/18/07.
https://doi.org/10.1145/3209978.3210190

1399

Tutorial

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

5.3

• Afternoon break
• Question answering using continuous corpus and knowledge
graph representations

5.1

Coarse NER resolves the types of named entity mentions only
a handful of very broad types. Studies [12] show that finer type
systems increase the accuracy of search applications. In this part
we will find out how to build systems that extend from coarse NER
tags to hundreds of fine types.
• FIGER [38], the system that popularized fine typing.
• Deep sequence models [56] for fine type tagging.
• Embedding types along with entities and contexts [20, 65, 70].
Multi-instance, multi-label models; mitigating label noise
[66] via neural attention networks.

Basics

Depending on the background of the audience, we need to cover
some subset of these topics to dive into the contents of this tutorial.
• Basic text modeling as sequence of tokens.
• Featurizing tokens, qgrams, word shapes, gazette features.
◦ Corpus and language models; topic models.
◦ (Text) classification: generative, conditional, discriminative.
• Word cooccurrence. Word embeddings [42, 49].
• Embedding-based comparison of query and passage [33, 55].
◦ Basics of feed-forward, recurrent, and memory networks.
In the interest of fitting the material in a day and keeping it coherent,
we will omit some closely related topics, on which references will
be made available:
◦ Coarse named entity recognition (NER)
◦ Coreference and record linkage

5.2

Annotating fine types in named entity
mentions

5.4 Disambiguating KG entities in
unstructured text
The natural next step after fine type determination of a named
entity mention is to refine it further to the canonical ID of an entity
registered in one or more knowledge bases or knowledge graphs
(KGs), unless there is evidence that the mention is that of an entity
outside the KG. This step is called named entity disambiguation
(NED) or entity linking, and is of great use in joining and aggregating evidence about an entity across multiple documents. NED
has seen sustained interest since around 2007, and performance
continues to improve even in 2017.
◦ Early systems: SemTag [14], Wikify [41].
◦ Local compatibility scoring, tree kernels [10].
• Leave-one-out coherence [13], phased coherence [43].
• Collective inference [32, 50]
• Context attention model [35], collective multi-focal attention
model [21].
• Using entity embeddings [67].
• Backprop through unrolled belief propagation [19].

Continuous knowledge representation

Recent search systems depend on both unstructured text corpora
and structured KGs. Word embeddings trained from unstructured
text [42, 49] gave a similarity-based bridge between syntactically
distinct words. It gradually became clear that embedding KG-side
artifacts (types, entities, relations) in a compatible manner can enable us to bridge corpus and KG information more effectively. This
led to a recent explosion of efforts to embed the KG and relate those
embeddings through specific algebraic structures. A major application is knowledge base completion (KBC), where a largely sound
but very incomplete KG is presented to the learning system, and it
has to infer high-precision additional relation tuples to materialize.
• The simplest structure [7] that a relation r is represented by
an embedding vector r and r (e 1 , e 2 ) holds if (to the extent
that) we can fit e 1 +rr ≈ e 2 . Unfortunately this ‘TransE’ model
does not support many-to-many relations.
• Another family of models [6, 29] fits a projection matrix M r
and a translation v r such that M r e 1 + v r ≈ M r e 2 .
• Another view of multi-relational inference is via a order-3
tensor T ∈ R E×E×R where E is the entity set and R is the
number of relations (i.e., relation types). [47] discussed how
to factorize T to jointly embed each entity in RK , as well as
give a matrix representation of each relation in RK ×K .
• A different approach is to embed not single entities but entity
→
K
pairs as a single vector e−−
1e 2 ∈ R , along with relations as
→
r ∈ RK , and then express Pr(r (e 1 , e 2 )) ∝ e−−
1e 2 · r [51].
◦ Combining the matrix and tensor factorization approaches
have shown benefits [28, 57].
• The above approaches cannot model asymmetry, antisymmetry, or transitivity, as is required by various relations.
Recently, more advanced methods such as Gaussian [23, 62],
holographic [48], order [61], and rectangle [58] embeddings
have been proposed to reduce or remove these limitations.
◦ The interplay between type and relation inference has been
further explored recently [27, 66].

5.5 Closed relation extraction
Another vital component of information extraction has been the
identification of canonical relations between pairs of entities mentioned in sentences.
◦ Brin [9] wrote a very early paper on bootstrapped relation
extraction using pattern-relationship duality. The idea was
to locate related entity pairs within a sentence in a corpus,
extract patterns from those sentences, filter and retain reliable patterns, then apply them on the corpus to extract
additional entity pairs.
• Agichtein and Gravano [1] further enhanced this paradigm
using an expectation maximization (EM) framework called
Snowball.
• As scalable NLP became commonplace, the simplistic patterns used earlier were replaced or supplemented by features
from dependency parse trees and path kernels [11] defined
between dependency paths.
• Suppose each sentence gives a perfectly reliable 0/1 judgment on whether a relation holds between two entities e 1 , e 2
that occur in it. Then a relation holds between them if at
least one gives evidence. Also, multiple relations can hold between a given (e 1 , e 2 ) pair. This leads to a multiple instance,

1400

Tutorial

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

multiple label (MIML) setting. Many papers propose distant
supervision schemes for relation extraction [2, 26, 44, 59].
• With the advent of deep learning, among the most successful
methods [60] has been to apply a convnet to a lexicalized
dependency path, pooling to an embedding vector r for the
relation, then combining r with entity embeddings e 1 ,ee 2 to
output a confidence in the relation tuple r (e 1 , e 2 ).
• We may also discuss other large knowledge harvesting systems such as NELL [45], DeepDive [71], and Google’s Knowledge Vault [15]. Some of this work combines source quality
and trustworthiness with extraction confidence to aggregate
evidence.

5.6

anticipate necessary or helpful extractions ahead of query
time. Therefore, query systems must be designed to find
evidence in a representation that combines [3, 30, 39, 52, 54,
64] structured KG with partly-structured corpus.
• Path Ranking [34], cascaded matrix operators [22], or Probabilistic Soft Logic [31], and composing embeddings along
relation paths [46] have also been explored.
• We will study the use of attention [63] and memory networks
[8, 36, 53] for question answering.

REFERENCES
[1] E. Agichtein and L. Gravano. Snowball: Extracting relations from large plaintext collections. In ICDL, pages 85–94, 2000. URL http://www.academia.edu/
download/31007490/cucs-033-99.pdf.
[2] G. Angeli, J. Tibshirani, J. Wu, and C. D. Manning. Combining distant and partial
supervision for relation extraction. In EMNLP Conference, pages 1556–1567, 2014.
URL http://www.anthology.aclweb.org/D/D14/D14-1164.pdf.
[3] K. Balog, L. Azzopardi, and M. de Rijke. A language modeling framework for
expert finding. Information Processing and Management, 45(1):1–19, 2009. ISSN
0306-4573. doi: http://dx.doi.org/10.1016/j.ipm.2008.06.003.
[4] M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. Open
information extraction from the Web. In M. M. Veloso, editor, IJCAI, pages
2670–2676, 2007. URL http://www.ijcai.org/papers07/Papers/IJCAI07-429.pdf.
[5] J. Berant, A. Chou, R. Frostig, and P. Liang. Semantic parsing on Freebase from
question-answer pairs. In EMNLP Conference, pages 1533–1544, 2013. URL
http://aclweb.org/anthology//D/D13/D13-1160.pdf.
[6] A. Bordes, J. Weston, R. Collobert, and Y. Bengio. Learning structured embeddings
of knowledge bases. In AAAI Conference, pages 301–306, 2011. URL http://www.
aaai.org/ocs/index.php/AAAI/AAAI11/paper/viewFile/3659/3898.
[7] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko.
Translating embeddings for modeling multi-relational data.
In NIPS
Conference, pages 2787–2795, 2013.
URL http://papers.nips.cc/paper/
5071-translating-embeddings-for-modeling-multi-relational-data.pdf.
[8] A. Bordes, N. Usunier, S. Chopra, and J. Weston. Large-scale simple question
answering with memory networks. arXiv preprint arXiv:1506.02075, 2015. URL
https://arxiv.org/pdf/1506.02075.pdf.
[9] S. Brin. Extracting patterns and relations from the World Wide Web. In P. Atzeni,
A. O. Mendelzon, and G. Mecca, editors, WebDB Workshop, volume 1590 of LNCS,
pages 172–183, Valencia, Spain, Mar. 1998. Springer. ISBN 3-540-65890-4. URL
http://ilpubs.stanford.edu:8090/421/1/1999-65.pdf.
[10] R. Bunescu and M. Pasca. Using encyclopedic knowledge for named entity
disambiguation. In EACL, pages 9–16, 2006. URL http://www.cs.utexas.edu/~ml/
papers/encyc-eacl-06.pdf.
[11] R. C. Bunescu and R. J. Mooney. A shortest path dependency kernel for relation
extraction. In EMNLP Conference, pages 724–731. ACL, 2005. doi: http://dx.doi.org/
10.3115/1220575.1220666. URL http://acl.ldc.upenn.edu/H/H05/H05-1091.pdf.
[12] S. Chakrabarti, S. Kasturi, B. Balakrishnan, G. Ramakrishnan, and R. Saraf. Compressed data structures for annotated web search. In WWW Conference, pages
121–130, 2012. ISBN 978-1-4503-1229-5. doi: 10.1145/2187836.2187854. URL
http://www.cse.iitb.ac.in/~soumen/doc/www2012/.
[13] S. Cucerzan. Large-scale named entity disambiguation based on Wikipedia
data. In EMNLP Conference, pages 708–716, 2007. URL http://www.aclweb.org/
anthology/D/D07/D07-1074.
[14] S. Dill et al. SemTag and Seeker: Bootstrapping the semantic Web via automated
semantic annotation. In WWW Conference, pages 178–186, 2003.
[15] X. Dong et al.
Knowledge vault: A web-scale approach to probabilistic knowledge fusion. In SIGKDD Conference, pages 601–610, 2014.
URL https://static.googleusercontent.com/media/research.google.com/en//pubs/
archive/45634.pdf.
[16] O. Etzioni, M. Cafarella, et al. Web-scale information extraction in KnowItAll. In
WWW Conference, New York, 2004. ACM. URL http://www.cs.washington.edu/
research/knowitall/papers/www-paper.pdf.
[17] O. Etzioni, A. Fader, J. Christensen, S. Soderland, and M. Mausam. Open information extraction: The second generation. In IJCAI, pages 3–10, 2011. URL
https://www.aaai.org/ocs/index.php/IJCAI/IJCAI11/paper/viewFile/3353/3408.
[18] A. Fader, S. Soderland, and O. Etzioni. Identifying relations for open information
extraction. In EMNLP Conference, pages 1535–1545, 2011. URL http://ml.cs.
washington.edu/www/media/papers/reverb_emnlp2011.pdf.
[19] O.-E. Ganea and T. Hofmann. Deep joint entity disambiguation with local neural
attention. arXiv preprint arXiv:1704.04920, 2017. URL https://arxiv.org/pdf/1704.
04920.pdf.
[20] D. Gillick, N. Lazic, K. Ganchev, J. Kirchner, and D. Huynh. Context-dependent
fine-grained entity type tagging. arXiv preprint arXiv:1412.1820, 2014. URL
https://arxiv.org/pdf/1412.1820.pdf.

Open entity instance and relation
extraction

A key limitation of closed relation extraction is that training instances are needed for each relation, and each relation needs to be
recognized and canonicalized (assigned an ID, rather than being
known purely as textual representations). This does not scale with
tens of thousands of useful relations connecting entities.
◦ Early advances toward ‘open’ relation extraction were made
for two specific relations: instance-of and subtype-of. The
KnowItAll system [16] was among the first to collect e ∈ t
or t 1 ⊂ t 2 pairs with very light supervision. Hearst patterns
[24] and HTML list extraction were two major devices used.
• Over the next few years, type extractions were extended to
general open relation extraction1 , termed open information
extraction or OpenIE [4, 17]. A common trick is to use linguistic triggers to “self-train” a fast relation recognizer, then
cluster and clean up textual relation expressions.
• Further refinements in extraction quality were obtained
by a number of follow-up systems. REVERB [18] enforces
verb particles in relation expressions. OLLIE [40] expend
the repertoire of extractions to support enabling conditions
(if-then), attribution (person said “. . . ”), relational nouns (Microsoft cofounder X) and multi-ary relations.
• A recent theme is to embed open and canonical relations
in the same space to enable search and inference across
both [22].

5.7

Search and inference over extracted
knowledge

Queries where structured knowledge can contribute additional
quality are quite common in commercial search logs [37]. As a
downstream consumer of knowledge extraction, several flavors of
search have been studied.
◦ In the first, KGs are extended and cleaned by mining incomplete KGs, possibly in conjunction with large text corpora (see previous sections). Then these extended KGs are
searched by translating entity-oriented natural language
queries into structured queries [5, 68, 69] and executing
these against the (augmented) KG.
• The second viewpoint, closer to the IR community, is that
no KG is perfect or close to complete, and that we will never
1 https://nlp.stanford.edu/software/openie.html,

http://openie.allenai.org/

1401

Tutorial

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

[21] A. Globerson, N. Lazic, S. Chakrabarti, A. Subramanya, M. Ringgaard, and
F. Pereira. Collective entity resolution with multi-focal attention. In ACL Conference, pages 621–631, 2016. URL https://www.aclweb.org/anthology/P/P16/
P16-1059.pdf.
[22] K. Guu, J. Miller, and P. Liang. Traversing knowledge graphs in vector space. In
EMNLP Conference, 2015. URL https://arxiv.org/pdf/1506.01094.pdf.
[23] S. He, K. Liu, G. Ji, and J. Zhao. Learning to represent knowledge graphs with
gaussian embedding. In CIKM, pages 623–632, 2015. URL http://or.nsfc.gov.cn/
bitstream/00001903-5/414021/1/1000014951686.pdf.
[24] M. Hearst. Automatic acquisition of hyponyms from large text corpora. In
International Conference on Computational Linguistics, volume 14, pages 539–545,
1992. URL http://www.aclweb.org/website/old_anthology/C/C92/C92-2082.pdf.
[25] J. Hoffart et al. Robust disambiguation of named entities in text. In EMNLP
Conference, pages 782–792, Edinburgh, Scotland, UK, July 2011. SIGDAT. URL
http://aclweb.org/anthology/D/D11/D11-1072.pdf.
[26] R. Hoffmann, C. Zhang, X. Ling, L. Zettlemoyer, and D. S. Weld. Knowledgebased weak supervision for information extraction of overlapping relations. In
ACL Conference, pages 541–550, 2011. URL http://anthology.aclweb.org/P/P11/
P11-1055.pdf.
[27] P. Jain, P. Kumar, Mausam, and S. Chakrabarti. Type-sensitive knowledge base
inference without explicit type supervision. In ACL Conference, 2018.
[28] P. Jain, S. Murty, Mausam, and S. Chakrabarti. Mitigating the effect of out-ofvocabulary entity pairs in matrix factorization for knowledge base inference. In
IJCAI, 2018. URL https://arxiv.org/abs/1706.00637.
[29] G. Ji, S. He, L. Xu, K. Liu, and J. Zhao. Knowledge graph embedding via dynamic
mapping matrix. In ACL Conference, pages 687–696, 2015. URL http://www.
aclweb.org/anthology/P/P15/P15-1067.pdf.
[30] M. Joshi, U. Sawant, and S. Chakrabarti. Knowledge graph and corpus driven
segmentation and answer inference for telegraphic entity-seeking queries. In
EMNLP Conference, pages 1104–1114, 2014. URL http://www.emnlp2014.org/
papers/pdf/EMNLP2014117.pdf. Download http://bit.ly/1OCKbVW.
[31] A. Kimmig, S. Bach, M. Broecheler, B. Huang, and L. Getoor. A short introduction
to probabilistic soft logic. In Proceedings of the NIPS Workshop on Probabilistic
Programming: Foundations and Applications, pages 1–4, 2012. URL https://lirias.
kuleuven.be/bitstream/123456789/369430/1/psl_pp12.pdf.
[32] S. Kulkarni, A. Singh, G. Ramakrishnan, and S. Chakrabarti. Collective annotation
of Wikipedia entities in Web text. In SIGKDD Conference, pages 457–466, 2009.
URL http://www.cse.iitb.ac.in/~soumen/doc/CSAW/.
[33] S. Kumar, S. Chakrabarti, and S. Roy. Earth mover distance pooling over siamese
lstms for automatic short answer grading. In IJCAI, pages 2046–2052, 2017. URL
http://static.ijcai.org/proceedings-2017/0284.pdf.
[34] N. Lao and W. W. Cohen. Relational retrieval using a combination of pathconstrained random walks. Machine Learning, 81(1):53–67, Oct. 2010. ISSN
0885-6125. doi: 10.1007/s10994-010-5205-8. URL http://dx.doi.org/10.1007/
s10994-010-5205-8.
[35] N. Lazic, A. Subramanya, M. Ringgaard, and F. Pereira. Plato: A selective context
model for entity resolution. TACL, 3:503–515, 2015. URL http://anthology.aclweb.
org/Q/Q15/Q15-1036.pdf.
[36] C. Liang, J. Berant, Q. Le, K. D. Forbus, and N. Lao. Neural symbolic machines:
Learning semantic parsers on Freebase with weak supervision. CoRR, 2016. URL
http://arxiv.org/abs/1611.00020.
[37] T. Lin, P. Pantel, M. Gamon, A. Kannan, and A. Fuxman. Active objects: Actions
for entity-centric search. In WWW Conference, pages 589–598. ACM, 2012. ISBN
978-1-4503-1229-5. doi: 10.1145/2187836.2187916. URL http://research.microsoft.
com/apps/pubs/default.aspx?id=161389.
[38] X. Ling and D. S. Weld. Fine-grained entity recognition. In AAAI, 2012. URL
http://xiaoling.github.io/pubs/ling-aaai12.pdf.
[39] C. Macdonald and I. Ounis. Learning models for ranking aggregates. In Advances
in Information Retrieval, volume 6611 of LNCS, pages 517–529. Springer, 2011.
URL http://www.dcs.gla.ac.uk/~craigm/publications/macdonald11learned.pdf.
[40] Mausam, M. Schmitz, R. Bart, S. Soderland, and O. Etzioni. Open language
learning for information extraction. In EMNLP Conference, 2012. URL https:
//homes.cs.washington.edu/~mausam/papers/emnlp12a.pdf.
[41] R. Mihalcea and A. Csomai. Wikify!: linking documents to encyclopedic knowledge. In CIKM, pages 233–242, 2007. URL http://portal.acm.org/citation.cfm?id=
1321440.1321475.
[42] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS Conference,
pages 3111–3119, 2013. URL https://goo.gl/x3DTzS.
[43] D. Milne and I. H. Witten. Learning to link with Wikipedia. In CIKM,
pages 509–518, 2008. URL http://www.cs.waikato.ac.nz/~dnk2/publications/
CIKM08-LearningToLinkWithWikipedia.pdf.
[44] B. Min, R. Grishman, L. Wan, C. Wang, and D. Gondek. Distant supervision for
relation extraction with an incomplete knowledge base. In NAACL Conference,
pages 777–782, 2013. URL http://www.anthology.aclweb.org/N/N13/N13-1095.
pdf.
[45] T. Mitchell et al. Never-ending learning. In AAAI Conference, 2015. URL http:
//www.cs.cmu.edu/~tom/pubs/NELL_aaai15.pdf.

[46] A. Neelakantan, B. Roth, and A. McCallum. Compositional vector space models
for knowledge base completion. In ACL Conference, 2015.
[47] M. Nickel.
Tensor factorization for relational learning.
PhD thesis,
Ludwig–Maximilians–Universität, München, 2013.
URL https://edoc.ub.
uni-muenchen.de/16056/1/Nickel_Maximilian.pdf.
[48] M. Nickel, L. Rosasco, T. A. Poggio, et al. Holographic embeddings of knowledge
graphs. In AAAI Conference, pages 1955–1961, 2016. URL https://arxiv.org/abs/
1510.04935.
[49] J. Pennington, R. Socher, and C. D. Manning. GloVe: Global vectors for word
representation. In EMNLP Conference, volume 14, pages 1532–1543, 2014. URL
http://www.emnlp2014.org/papers/pdf/EMNLP2014162.pdf.
[50] L. Ratinov, D. Roth, D. Downey, and M. Anderson. Local and global algorithms
for disambiguation to Wikipedia. In ACL Conference, ACL/HLT, pages 1375–1384,
Portland, Oregon, 2011. ISBN 978-1-932432-87-9. URL http://dl.acm.org/citation.
cfm?id=2002472.2002642.
[51] S. Riedel, L. Yao, A. McCallum, and B. M. Marlin. Relation extraction with matrix
factorization and universal schemas. In NAACL Conference, pages 74–84, 2013.
URL http://www.anthology.aclweb.org/N/N13/N13-1008.pdf.
[52] D. Savenkov and E. Agichtein. When a knowledge base is not enough: Question
answering over knowledge bases with external text data. In SIGIR Conference,
pages 235–244, 2016. URL https://dl.acm.org/citation.cfm?id=2911536.
[53] D. Savenkov and E. Agichtein. Evinets: Neural networks for combining evidence
signals for factoid question answering. In ACL Conference, volume 2, pages
299–304, 2017. URL http://aclweb.org/anthology/P17-2047.
[54] U. Sawant and S. Chakrabarti. Learning joint query interpretation and response
ranking. In WWW Conference, Brazil, 2013. URL http://arxiv.org/abs/1212.6193.
[55] A. Severyn and A. Moschitti. Learning to rank short text pairs with convolutional deep neural networks. In Proceedings of the 38th International ACM SIGIR
Conference on Research and Development in Information Retrieval, pages 373–382,
2015. URL http://disi.unitn.it/~severyn/papers/sigir-2015-long.pdf.
[56] S. Shimaoka, P. Stenetorp, K. Inui, and S. Riedel. An attentive neural architecture
for fine-grained entity type classification. arXiv preprint arXiv:1604.05525, 2016.
URL https://arxiv.org/pdf/1604.05525.pdf.
[57] S. Singh, T. Rocktäschel, and S. Riedel. Towards combined matrix and tensor
factorization for universal schema relation extraction. In Workshop on Vector
Space Modeling for Natural Language Processing, 2015.
[58] S. Subramanian and S. Chakrabarti. New embedded representations and evaluation protocols for inferring transitive relations. In SIGIR Conference, 2018. URL
https://gitlab.com/soumen.chakrabarti/rectangle.
[59] M. Surdeanu, J. Tibshirani, R. Nallapati, and C. D. Manning. Multi-instance
multi-label learning for relation extraction. In EMNLP Conference, pages 455–465,
2012. URL http://anthology.aclweb.org/D/D12/D12-1042.pdf.
[60] K. Toutanova, D. Chen, P. Pantel, H. Poon, P. Choudhury, and M. Gamon. Representing text for joint embedding of text and knowledge bases. In EMNLP
Conference, pages 1499–1509, 2015. URL https://www.aclweb.org/anthology/D/
D15/D15-1174.pdf.
[61] I. Vendrov, R. Kiros, S. Fidler, and R. Urtasun. Order-embeddings of images and
language. arXiv preprint arXiv:1511.06361, 2015. URL https://arxiv.org/pdf/1511.
06361.
[62] L. Vilnis and A. McCallum. Word representations via gaussian embedding. arXiv
preprint arXiv:1412.6623, 2014.
[63] C. Xiong, V. Zhong, and R. Socher. Dynamic coattention networks for question
answering. arXiv preprint arXiv:1611.01604, 2016. URL https://arxiv.org/pdf/1611.
01604.pdf.
[64] K. Xu, S. Reddy, Y. Feng, S. Huang, and D. Zhao. Question answering on Freebase
via relation extraction and textual evidence. arXiv preprint arXiv:1603.00957, 2016.
URL https://arxiv.org/pdf/1603.00957.pdf.
[65] Y. Yaghoobzadeh and H. Schütze. Corpus-level fine-grained entity typing using
contextual information. In EMNLP Conference, pages 715–725, 2015. URL http:
//aclweb.org/anthology/D15-1083.
[66] Y. Yaghoobzadeh, H. Adel, and H. Schütze. Noise mitigation for neural entity
typing and relation extraction. arXiv preprint arXiv:1612.07495, 2016. URL
https://arxiv.org/pdf/1612.07495.pdf.
[67] I. Yamada, H. Shindo, H. Takeda, and Y. Takefuji. Joint learning of the embedding of words and entities for named entity disambiguation. arXiv preprint
arXiv:1601.01343, 2016. URL https://arxiv.org/pdf/1601.01343.pdf.
[68] X. Yao and B. Van Durme. Information extraction over structured data: Question
answering with Freebase. In ACL Conference. ACL, 2014. URL http://www.cs.jhu.
edu/~xuchen/paper/yao-jacana-freebase-acl2014.pdf.
[69] S. W.-t. Yih, M.-W. Chang, X. He, and J. Gao. Semantic parsing via staged query
graph generation: Question answering with knowledge base. In ACL Conference,
pages 1321–1331, 2015. URL http://anthology.aclweb.org/P/P15/P15-1128.pdf.
[70] D. Yogatama, D. Gillick, and N. Lazic. Embedding methods for fine grained
entity type classification. In ACL Conference, pages 26–31, 2015. URL http:
//anthology.aclweb.org/P/P15/P15-2048.pdf.
[71] C. Zhang, C. Ré, M. J. Cafarella, J. Shin, F. Wang, and S. Wu. DeepDive: Declarative
knowledge base construction. SIGMOD record, 45(1):60–67, 2016.

1402

