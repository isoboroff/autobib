Session 5C: New Metrics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

How Well do Offline and Online Evaluation Metrics Measure
User Satisfaction in Web Image Search?
Fan Zhang

Ke Zhou∗

Yunqiu Shao

BNRist, DCST, Tsinghua University
Beijing, China
frankyzf94@gmail.com

University of Nottingham
Nottingham, U.K.
Ke.Zhou@nottingham.ac.uk

BNRist, DCST, Tsinghua University
Beijing, China
shaoyunqiu14@gmail.com

Cheng Luo

Min Zhang

Shaoping Ma

BNRist, DCST, Tsinghua University
Beijing, China
chengluo@tsinghua.edu.cn

BNRist, DCST, Tsinghua University
Beijing, China
z-m@tsinghua.edu.cn

BNRist, DCST, Tsinghua University
Beijing, China
msp@tsinghua.edu.cn

ABSTRACT

1

Comparing to general Web search engines, image search engines
present search results differently, with two-dimensional visual image panel for users to scroll and browse quickly. These differences
in result presentation can significantly impact the way that users
interact with search engines, and therefore affect existing methods
of search evaluation. Although different evaluation metrics have
been thoroughly studied in the general Web search environment,
how those offline and online metrics reflect user satisfaction in the
context of image search is an open question. To shed light on this,
we conduct a laboratory user study that collects both explicit user
satisfaction feedbacks as well as user behavior signals such as clicks.
Based on the combination of both externally assessed topical relevance and image quality judgments, offline image search metrics
can be better correlated with user satisfaction than merely using
topical relevance. We also demonstrate that existing offline Web
search metrics can be adapted to evaluate on a two-dimensional
presentation for image search. With respect to online metrics, we
find that those based on image click information significantly outperform offline metrics. To our knowledge, our work is the first to
thoroughly establish the relationship between different measures
and user satisfaction in image search.

With the rapid growth of multimedia contents, image search engines has become a popular and supplementary information sources
for searchers. Compared to that of the general (textual) Web search
engines, the presentation of image search results is quite different.
First of all, most image search engines display image snapshots
rather than document snippets so that users can directly see an
image preview of the result. Due to this, in addition to relevance,
other factors, such as image visual attractiveness, may affect users’
perceived satisfaction of image search results [15]. Secondly, image
search results are placed in a two-dimensional panel rather than
the one dimensional ordered ranking lists within the traditional
general Web search engine result pages (SERPs). This can affect
user’s examination behavior. Based on the analysis of eye gaze
first arrival time and examination duration, Xie et al. [54] observe
a middle-position bias of user’s examination behavior in image
search. This behavior does not conform to the traditional “Golden
Triangle” phenomena in general Web search. Thirdly, rather than
clicking the “next page” buttons to navigate for more results, image search engines generally load new pages (results) implicitly,
which means that users can view images across different pages by
scrolling up and down.
These differences between image search and general Web search
may have an impact on search evaluation, which sits at the center
of IR research. Carterette [4] formulates a conceptual framework to
interpret traditional model-based measures like DCG [24], RBP [40]
and ERR [5]. He argues that these measures are actually composed
of three underlying models, which are browsing model, document
utility model and utility accumulation model. However, these models may face challenges in image search scenarios: (1) A browsing
model describes how a user interacts with results in SERPs. To date,
the most well-developed browsing model is that users scan ranked
results one-by-one from top to bottom before they stop. However,
the middle-position bias observed by Xie et al. [54] indicates that
there exists a different browsing model in image search. (2) A document utility model represents how a user derives utility from
individual relevant documents. Relevance judgments from external
assessors are often collected to model document utility. Nevertheless, the user-perceived utility of an image result may be affected by
topical relevance as well as other factors. Similar phenomenon was
also observed by Geng et al. [15]. Their user study in comparing
image search results of Google and Bing suggests that topical relevance may not be the leading discriminating factor of Web image
search engines. (3) A utility accumulation model depicts how a
user accumulates utility in the course of browsing. Carterette [4]

KEYWORDS
Web image search; user satisfaction; evaluation metrics
ACM Reference Format:
Fan Zhang, Ke Zhou, Yunqiu Shao, Cheng Luo, Min Zhang, and Shaoping
Ma. 2018. How Well do Offline and Online Evaluation Metrics Measure
User Satisfaction in Web Image Search?. In SIGIR ’18: The 41st International
ACM SIGIR Conference on Research and Development in Information Retrieval,
July 8–12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages.
https://doi.org/10.1145/3209978.3210059
∗ Corresponding

author

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210059

615

INTRODUCTION

Session 5C: New Metrics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

2

introduces four utility accumulation models and classifies measures
into four distinct families. However, these models are all designed
for general Web search, not considering the two-dimensional result placement in image search. Similarly, online metrics, which
have been widely adopted for modern search engines, are also confronted with challenges in Web image search. The online metrics
are mostly based on user behavior. Due to the differences of user
behavior between general Web search and image search [3, 43], the
effective general Web search metrics may not be appropriate for
measuring image search.
There exist a number of studies on revealing the relationship
between different evaluation methods and user satisfaction [2, 7, 39,
45]. Recently, Chen et al. [7] meta-evaluate a series of existing online
and offline metrics to study how well they infer actual search user
satisfaction in different search scenarios. The results suggest that
offline metrics work better in homogeneous search environment
while online metrics are more consistent with user satisfaction in
heterogeneous search environment. Although different evaluation
metrics have been thoroughly studied in the context of general Web
search, as far as we know, no existing work sufficiently investigates
how well these metrics perform in Web image search scenarios.
Considering the differences between general Web search and image
search, whether these evaluation metrics are applicable for image
search remains an open research question.
Therefore, in this paper, we focus on the performances of different offline and online evaluation metrics on measuring user
satisfaction in the context of image search. Based on a laboratory
user study, we collect users’ explicit satisfaction feedbacks as well
as user behavior signals (such as click, hover, dwell time etc.) in
image search. Following the previous work [41], we also gather
separate judgments for topical relevance and image quality from external assessors hired by a crowdsourcing platform. By comparing
those two judgments, we find they are correlated but different. In
general, the offline metrics can achieve a better performance based
on a combination of those two judgments in the context of image
search. However, these two judgments can play different roles in
affecting user satisfaction for different search tasks (e.g. image quality is more important for locating tasks, compared to exploring
tasks). Compared with offline metrics, we find that online metrics
based on mouse click information correlate much better with user
satisfaction. Furthermore, our findings indicate that users’ click behavior is an essential and powerful signal to infer user satisfaction
in image search scenarios. To summarize, the main contributions
of this paper are as follows:

RELATED WORK

Of particular interest to our research is the extensive body of work
on (i) IR evaluation, (ii) image search and (iii) user satisfaction.

2.1

Offline and Online Evaluation

Search Engine evaluation has always been an important task in
IR, in both academic and industrial research. Both offline and online metrics are developed and widely used for general Web search
measurement. Traditional system-centric offline metrics are usually based on relevance judgments of query-document pairs from
external assessors, which mainly originated from Cranfield framework [10]. Typical offline metrics are widely used to measure the
quality of ranking algorithms [37], such as Precision, Recall and
Normalized Discounted Cumulative Gain (nDCG) [24]. Caterette [4]
develops a conceptual framework to interpret traditional offline
model-based measures like RBP [40], DCG [24], ERR [5] and AP
(Average Precision). These models are mainly based on the assumption that users examine the result list in a top-down manner. Besides these rank-based metrics, other aspects have been taken into
consideration as the discounting factor to develop offline metrics.
Time-Biased Gain (TBG) [48] uses time spent by the user as the
basis for discounting. U-Measure [12] looks at the text length. For
mobile search scenario, Luo at el. [36] consider the height of user
browsing trail as well as click necessity and develop Height-Biased
Gain (HBG). Image search engines show results differently from
general Web search engines [3] and users’ examination behavior
also differs [41, 54]. Beyond relevance, it has been found that image
attractiveness matters in image search [15, 41, 42]. To the best of
our knowledge, no existing offline evaluation metrics are designed
to adapt to the changes in image search.
Online evaluation is defined as the evaluation of a fully functioning system based on implicit measurement of real users’ experiences
of the system in a natural usage environment in [21]. Online evaluation approaches such as A/B tests [31], between-subject experimental designs, and interleaved comparisons [27], within-subject
experimental designs, as well as their variants [34, 47] are commonly used for evaluating system effectiveness. One of the biggest
challenges for online evaluation is to identify metrics that reflect
user satisfaction. Different from offline evaluation metrics with relevance judgments, online evaluation metrics are calculated based
on the users’ behavior logs collected from search engines to measure their performances. Early online metrics focus on click-based
methods like CTR (Click Through Rate) and the ranks of clicked
documents [26] as well as their extensions, e.g. UCTR [9], PLC [5].
Dwell time [55] is frequently used to improve over simple click
metrics including query dwell time, time to first click, the average
of click dwell time [25] and so on. Some learned click satisfaction
metrics integrate more features such as mouse movement, per-topic
reading time, to obtain more accurate estimates of document-level
user satisfaction [14, 19, 30]. Besides, a number of session-based
metrics have been proposed [18, 51]. Although online metrics have
been widely adopted for modern search engines, there are few
works looking at these metrics in image search scenarios.

• We thoroughly compare the performance of different offline
and online evaluation metrics and user satisfaction in the
context of image search. In general, we find online metrics
based on mouse click information align better with user
satisfaction than offline metrics based on the combination
of topical relevance and image quality judgments.
• We propose to incorporate image quality with topical relevance for offline evaluation in image search scenarios and
compare the difference between these two judgments.
• We adapt several existing web search offline evaluation metrics so that they can measure in a two-dimensional presentation in image search.
• We suggest that users’ click behavior is a good indicator to
infer user satisfaction in image search scenarios.

2.2

Image Search

Many image search engines have used traditional Web ranking algorithms to rank images, based on some (textual) meta information
of the images and content-based analysis. Furthermore, there are
also some works that attempt to improve the ranking results of
image search engines by incorporating visual diversification [50],

616

Session 5C: New Metrics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

re-ranking based on click data [23], or considering two dimensional
presentation position [8]. In our work, the image results we used
in the user study were provided by a popular commercial image
search engine, without any further processing. The commercial
image search engine may use all or a subset of the available features, and we treat it as a black-box. Launching in 2003 as part of
the Cross Language Evaluation Forum, ImageCLEF1 provides an
evaluation forum for the cross-language annotation and retrieval
of images. Sanderson [44] introduces evaluation measures used
in ImageCLEF. However, these measures follow those in general
Web search, which had been widely used in TREC and NTCIR. In
our work, we adapt several existing web search offline evaluation
metrics so that they can measure in a two-dimensional presentation in image search. We mainly aim to compare the performance
of different offline and online evaluation metrics given the image
results returned by the image search engines.

2.3

two stages. The first part is a laboratory user study, from which we
collect users’ explicit satisfaction feedbacks as well as user behavior
signals in Web image search environments. In the second stage,
we hired external assessors by a crowdsourcing platform to make
topical relevance and image quality judgments for image results.
An overview of the procedure is shown in Figure 1.

3.1

3.1.1 Participants. Considering image search engines are usually used by undergraduate students for their study or work, we
recruit 36 students (14 female and 22 male) to take part in our user
study via email, online forums and social networks. The ages of
participants range from 18 to 25. Various majors are included across
engineering, humanities, social sciences and arts. All the participants report that they are familiar with the search engines and have
experience in performing Web image search tasks. The participants
are informed that it takes about one and a half hour to complete
all the tasks without time limits imposed and they would be paid
about $25.

User Satisfaction

User satisfaction has been extensively discussed in the areas of consumer, marketing and psychology research since the mid-1970s [20]
and was first proposed by Su et al. [49] in information retrieval
system. In information retrieval, search satisfaction is defined as
the fulfillment of a user’s information need by Feild et al. [13]. Jones
et al. [28] stress the importance of user satisfaction and consider
it as the basic concept in IR system evaluation. Because of the importance of satisfaction, numerous research studies are around it.
Al-Maskari at el. [1] investigate factors influencing user satisfaction in information retrieval. Dan at el. [11] summarize 5 research
areas related to user satisfaction including search engine switching,
good and bad abandonment, query difficulty and performance, task
difficulty and predicting satisfaction.
Since satisfaction can be considered as the golden standard in
search performance evaluation, there exists a number of studies
on different evaluation methods and the correlation between these
methods and satisfaction [2, 7, 39, 45]. The study indicates that
relevance-based evaluation metrics, such as MAP and nDCG, may
not be perfectly correlated with user satisfaction by Scott at el. [22].
The relationship between relevance, usefulness, and satisfaction is
further studied by Mao at el. [38] and the work suggests that traditional system-centric evaluation metrics are not well aligned with
user satisfaction. Recently, Chen at el. [7] study the relationships
between user satisfaction and both offline and online evaluation
metrics in both homogeneous and heterogeneous search environment. The results suggest that offline metrics better align with user
satisfaction in homogeneous search while online metrics perform
more consistently with user satisfaction in heterogeneous environment. With the rapid growth of mobile search traffic as well as
changes in user behavior, there exist studies related to satisfaction
prediction and measurement in mobile search [16, 17, 32]. Relationships between good abandonment and satisfaction are frequently
discussed in this scenario [29, 52].
As for image search, the problem of how well different metrics
perform or align with user satisfaction is still an open question.
Therefore, we focus on the relationships between both offline and
online metrics and user satisfaction in Web image search scenarios.

3

User Study

We describe the details of our user study (first stage) in this section.

3.1.2 Tasks. According to the image search intent taxonomy
proposed by Xie et al. [53], all the image search tasks can be categorized into three intent categories, which are defined as follows:
• Exploring. Users want to learn something, confirm or compare information by browsing images.
• Entertaining. Users want to relax and kill time by freely
browsing the image search results.
• Locating. Users want to find images for further use. They
already have some requirements for these images.
Following this work [53] we design 12 image search tasks (4 tasks
for each category) that cover various image search intents. Table 1
shows examples of the tasks for those three categories.
Considering the different image search intent scenarios, we provide different requirements for different tasks. For the “Exploring”
tasks, the participants only need to verbally describe the information they have found or learnt. As shown in Table 1, the participants
are required to describe three pictures about Haikou City in an example of the “Exploring” tasks. However, for the “Entertaining”
tasks, the participants could freely browse the images related to the
topic without any further requirements. As for the “Locating” tasks,
we ask the participants to make some multimedia productions such
as a slide or a poster. In order to guarantee that the participants only
need to use images to complete their tasks, we provide a default
slide or poster with some necessary keywords and background.
For instance, in one of the “Locating” tasks, the participants are
required to make a slide to introduce the “protagonists of Harry
Potter". In the default slide we provide, we list the names of three
characters of Harry Potter so that the participants only needed to
find some pictures of the characters to complete the slide.
3.1.3 Experiment Procedure. As shown in Figure 1, after the
instruction and a training task that makes the participants familiar
with the experimental procedure, they are required to complete 12
Web image search tasks. For each task, we provide a detailed task
description (see Table 1) to simulate a realistic Web image search
scenario. Firstly, the participants should read the description and
repeat it in their own words to confirm that they have understood
the information need and requirement of the task. Then they would
be redirected to an experimental search system, the results of which

DATA COLLECTION

In this section, we describe the data collection procedure as well as
the dataset we use throughout this paper. The procedure consists of
1 http://www.imageclef.org/

617

Session 5C: New Metrics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

I. User Study

II. Data Annotation
Issued Queries
Query-Image Pairs:
Topical Relevance Judgments

1. Instruction & Training

2. Task Description
Reading & Rehearsal
3. Task Completion &
Interaction Information Logging

The First Ten
Rows of Images

Task Description
Feedback
Instruction

Offline
Metrics

Single Images:
Image Quality Judgments
Click Behaviors

Issued Queries &
Clicked Images

Hover Behaviors

4. Satisfaction Feedback
Query-level
Satisfaction
Feedback

User Behaviors

Online
Metrics

Dwell Time Behaviors

Figure 1: Data collection procedure. We collect user behavior logs and satisfaction feedbacks in I. User Study. With hired
external assessors, we gather topical relevance and image quality judgments in II. Data Annotation.
Table 1: Examples of search tasks and queries for different search intent categories.
Task Description
You just receive a job offer in Haikou City. You want to know more about this City (e.g.
streets, landscapes, buildings).
You want to browse some posters or photos of your favorite stars to kill time.
You want to make a slide about Harry Potter. You need some posters of Harry Potter
film to introduce the protagonists.

Example Query

Intent Category

Haikou

Exploring

Lebron James

Entertaining

Harry Potter

Locating

are provided by a popular commercial image search engine. They
could submit queries, scroll up and down, click on the results and
even download the full-size images, just like naturally using an
image search engine. Whenever the participants think that the
task is completed or it is difficult to find more useful information,
they could stop searching and click the finish button. After that,
the participants are required to provide satisfaction feedbacks. To
help the participants review the search process, all the queries and
clicked images are shown in the same order as when the participants
issued and clicked them. Finally we collecte a 5-point scaled querylevel satisfaction feedback with the instructions introduced by Liu
et al. [35]. Following previous work [7] and its simplicity, in this
preliminary work we focus on query-level satisfaction rather than
session-level satisfaction.

Please describe three pictures which are impressive
to you in words.
–
Please use images to complete the slide we provided for you.

Existing image search engines are mainly designed to optimize
the topical relevance, i.e., to what extent the topic of returned images matches that of the text query [46]. However, Geng et al. [15]
declare the importance of image attractiveness or quality for image
search. Therefore, to account for those factors, O’Hare et al. [41]
gather separate judgments for both topical relevance and image
quality, in order to measure these two main facets of relevance for
image results. Following this previous work [41], we also gather
separate judgments for topical relevance and image quality. The
criteria are based on the definition in [41] and modified according
to the annotation rules used by a popular commercial image search
engine. For topical relevance, unlike 3-point scaled judgment collected in [41], we use the following 4-point scale used by a popular
commercial image search engine:
• Irrelevant. The image fails to match the subject of the query
(e.g., the query is “Batman” while the main object in the
image is “Spider-Man”).
• Somewhat relevant. The image is only partially relevant to the
query. Specifically, the query contains two or more objects
while the image only depicts part of them (e.g., the query
is “Hillary Clinton and Donald Trump debating” while the
image only focuses on Trump).
• Fairly relevant. Although the objects are matched between
the query and the image, their modifiers are different (e.g.,
the query is “Red Ferrari” while the image is about “Black
Ferrari”).
• Highly relevant. Both the objects and their modifiers are
perfectly matched between the query and the image.
While for image quality, firstly we collected 5-point scaled judgments with similar criteria introduced in [41]:
• Bad. Extremely low quality, obviously watermarked, out of
focus, underexposed, badly framed images.
• Fair. Low-quality images with some technical flaws (slightly
blurred, small watermarked, slightly over/underexposed, incorrectly framed), which are not very appealing.

3.1.4 Experimental System. In our user study, the procedure
mentioned above is conducted on a 17-inch LCD monitor with a
resolution of 1366 × 768 pixels. The search system is displayed on a
Google Chrome browser, where we inject a customized JavaScript
plugin into search result pages to log users’ search behaviors including scrolling, hover, click, tab switching and mouse movement.
We also record queries issued by the participants and some information about the corresponding SERPs such as the position and
meta information of the returned image search results.

3.2

Task Requirement (Objective)

Data Annotation

After collecting users’ explicit satisfaction feedbacks as well as user
behavior signals in our user study, we further hire external assessors from a crowdsourcing platform to make judgments (second
stage) for all the top ten rows of image results shown in SERPs (the
experimental search system would load only ten rows of images for
each query by default). In the dataset, more than 80% of the images
clicked by the participants are from the first ten rows.

618

Session 5C: New Metrics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 2: Statistics of the dataset.

12

#participants
36

#sessions

#queries

379

1119

#results

0.5

79824

0
1
2
3

0.35

3
0.30

Image Quality

0.6

#tasks

0.4
0.3
0.2

• Good. Standard quality images without technical flaws (subject well framed, in focus, easily recognizable, not easily
perceived watermarked), low value for download or image
collections.
• Professional. Professional-quality images (flawless framing,
focus, lighting, not watermarked), which should also be
somewhat attractive/appealing.
• Exceptional. Very appealing images, showing both outstanding professional quality (photographic and/or editing techniques) and high artistic value.
From data analysis, since we find it is difficult for assessors to distinguish the difference between Exceptional and Professional quality,
we merge both grades into one. Finally, we obtain 4-point scaled
(Excellent, Good, Fair, Bad) quality judgments, which also makes
image quality judgment scale more comparable to one of the topical
relevance. From the instructions, we should note that unlike topical
relevance judgment, image quality judgment is query-independent.
To collect the topical relevance judgment, each time only one queryimage pair is shown to the assessors. We also provide the meta information of the image to help the assessor understand the context
of the image. However, an image quality judgment is made for a
single image and no other information is provided. All the judgments for topical relevance and image quality are made by three
assessors and the median is considered if there is a disagreement.
The Fleiss’s κ of topical relevance and image quality judgments
are 0.551 and 0.527 respectively, which are moderate agreements
according to [33]. It suggests that both topical relevance and image
quality could be assessed offline with an acceptable agreement in
the image search environments.

3.3

T opical Relevance

Image Quality

0.05

0

1

2

3

T opical Relevance

(b)

Figure 2: The marginal distributions and the joint distribution of topical relevance and image quality judgments. The
values from 0 to 3 mean respectively irrelevant/bad quality,
somewhat relevant/fair quality, fairly relevant/good quality,
highly relevant/excellent quality.
judgments using a simple heuristic method which gives precedence
to topical relevance, we first investigate the relationship between
both judgments. Note that topical relevance judgments are gathered
for all the query-image pairs while image quality judgments are
collected for all the single images. When we compare them based
on query-image pairs, the image quality scores of the same image
for different queries will be the same.
The marginal distributions of topical relevance (T R) and image
quality (IQ) are shown in Figure 2(a). We can observe a visible
difference between their distributions (Chi-Square test, χ 2 (3, N =
79824) = 9820.5, p < 0.001). For topical relevance, more than 60%
of image results are annotated as highly relevant (T R = 3). This
might not be surprising since image results in the top ten rows
returned by a commercial image search engine are usually topically
relevant to the query. On the other hand, regarding image quality,
fewer images are of excellent quality (IQ = 3) while more images
are of good quality (IQ = 2).
To further study the relationship between topical relevance and
image quality, we plot the heat map for the joint distribution of these
two judgments in Figure 2(b). We can see that T R and IQ are not
aligned well. Apart from the results which are both highly relevant
(T R = 3) and having excellent quality (IQ = 3), there are appreciable
results which are highly relevant while maintaining lower quality
(IQ < 3). We also compute the Pearson’s correlation coefficient r
and Linear Weighted Cohen’s κ between these two judgments. We
detect a weak positive correlation (r = 0.173, p < 0.001). Meanwhile, the computed κ is 0.190, which is only a slight agreement.
We manually examine the image results with high topical relevance
(T R = 3) and low image quality (IQ < 3), and find that although
most of these images match the related query perfectly, there are
some flaws such as the watermark in the images.
The above observations suggest that although topical relevance
and image quality are related to each other, they are definitely two
separate facets of the image results. Considering the importance of
these two facets, in the next section we compare their performance
when they are used for offline evaluation and attempt to combine
these two judgments.

Data Cleansing

EXPERIMENTAL RESULTS

In this section, we first compare the difference between topical
relevance and image quality judgments. Then we evaluate the performance of several offline metrics and online metrics in Section 4.2
and 4.3, respectively. Finally, in Section 4.4, we aim to obtain indepth insights into how offline and online metrics infer user satisfaction in different search intent scenarios.

4.1

0.15

0

(a)

After a careful inspection, we filter out 53 search sessions because
of technical problems in recording user behavior logs. For the remaining queries, through our user study and data annotation, we
collect users’ explicit satisfaction feedbacks, users’ search behaviors as well as topical relevance and image quality judgments from
external assessors. Table 2 shows some statistics of our dataset.
All the collected data used throughout this paper, including the
topical relevance and image quality judgments is available online
for academic research.2

4

0.20

1

0.10

0.1
0.0

0.25

2

Topical Relevance vs. Image Quality

4.2

As described in Section 3.2, we follow the previous work [41] and
gather separate judgments for topical relevance and image quality. However, unlike O’Hare et al [41] that directly combine both

Comparison Across Offline Metrics

With user satisfaction widely regarded as the golden standard in
user-centric search evaluation, in this paper, we use Pearson’s correlation coefficient to analyze which metrics can better reflect user
satisfaction based on the dataset described in Section 3.

2 https://drive.google.com/file/d/1ePk-oJprioGH-KS3emAnjxd2pXp3LrYO/

619

Session 5C: New Metrics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 3: Comparison of Pearson’s correlation coefficients between user satisfaction and offline metrics based on different relevance judgments (* indicates the correlation is significant at p < 0.001 level. † indicates the difference is significant at p < 0.001 level, comparing to the same metric based
on the combination judgments.)

CG
DCG@3r
DCG@5r
DCG@10r
RBP(0.1)
RBP(0.5)
RBP(0.8)
RBP(0.95)
ERR

Topical Relevance

Image Quality

Combination

0.230*†
0.201*†
0.217*†
0.231*†
0.151*†
0.166*†
0.192*†
0.234*†
0.140*†

0.302*†
0.276*†
0.290*†
0.302*†
0.187*†
0.222*†
0.245*†
0.273*†
0.180*†

0.359*
0.330*
0.346*
0.359*
0.254*
0.279*
0.306*
0.345*
0.240*

Table 4: Comparison of Pearson’s correlation coefficients between user satisfaction and offline metrics based on different examination sequences (* indicates the correlation is significant at p < 0.001 level.)

CG
DCG@10r
RBP(0.95)
ERR
MAX
AVG

Z-sequence

S-sequence

T-sequence

0.359*
0.359*
0.345*
0.240*
0.064
0.360*

0.359*
0.358*
0.346*
0.207*
0.064
0.360*

0.359*
0.358*
0.345*
0.215*
0.064
0.360*

Table 5: Comparison of Pearson’s correlation coefficients between user satisfaction and two-dimensional offline metrics
based on different row-integration methods (* indicates the
correlation is significant at p < 0.001 level.)

First, we compare different judgments for topical relevance and
image quality described in Section 4.1. We also employ a simple
heuristic method to create a combined relevance (CR), which is the
minimum of T R and IQ. Our intuition is that an image which is
useful to users should not only be highly topically relevant to the
query but also have high quality. Based on these three judgments
for the top ten rows of image results provided by the dataset, we
compute several widely-used typical offline metrics including CG,
DCG, RBP and ERR according to original rank orders of the image
results. Similar to the previous work [7], we investigate the effects
of the evaluation depth for DCG and the persistence parameter p for
RBP. Note that in image search engine result pages, the number of
images varies in different rows. Here we use “row” rather than “rank”
to measure the evaluation depth and DCG@3/5/10r means DCG
calculated at 3/5/10 rows, respectively. In addition, we normalize
all the metrics by the number of images across different queries.
The Pearson’s correlation coefficients between these metrics and
user satisfaction are shown in Table 3. From the results we can find
that metrics based on combined relevance have much higher correlation than that based on solely topical relevance or image quality
judgments. It suggests that both topical relevance and image quality play important roles in the measurement of general relevance
and corresponding satisfaction for image results. It is surprising
that metrics based on IQ perform better than that based on T R. We
assume that the reason is the imbalanced distribution of topical
relevance of image results, which makes topical relevance have
a poor discriminative power. Comparing different offline metrics,
from the perspective of DCG, we can see that DCG@10r correlates
user satisfaction slightly better than DCG@3r and DCG@5r. This
probably indicates that users tend to examine lots of images and
those images situated between 5th to 10th rows contribute to user
satisfaction. A similar phenomenon can be found for RBP, where
RBP(0.95) achieves the highest Pearson’s correlation coefficient
among all the RBP metrics.
Inspired by these findings, in the following experiments, we use
the combined relevance (CR) as relevance judgment to compute
four typical offline metrics, including CG, DCG@10r, RBP(0.95) and
ERR. We also compute two simple baseline metrics which are the
maximum (MAX) and average (AVG) of CR for all the image results.
Considering the conclusion that there is a middle-position bias
of user’s examination behavior in image search [54], we compare
three different examination sequences to compute metrics. Figure 3 presents an example how we obtain ’Z/S/T’ sequences from
a two-dimensional results placement. The Pearson’s correlation
coefficients between user satisfaction and metrics based on three
sequences are shown in Table 4 . We can see that there is little

CG
DCG@10r
RBP(0.95)
ERR
MAX
AVG

Row-SUM

Row-MAX

Row-AVG

0.284*
0.278*
0.282*
-0.013
0.229*
0.284*

0.301*
0.284*
0.295*
0.172*
0.064
0.301*

0.360*
0.357*
0.360*
0.319*
0.340*
0.360*

difference among different examination sequences. We also find
that CG and DCG@10r have nearly the same performance. Our
explanation for these results is that most of the image results, especially those in the top rows, have high CR scores. The combined
relevance is still not a good indicator of user’s perceived usefulness
for the image results.
1
6

2

3
7

4
8

5
9

10 11 12 13 14

‘Z’: (1,2,3,4,5, 6,7,8,9, 10,11,12,13,14)
‘S’: (1,2,3,4,5, 9,8,7,6, 10,11,12,13,14)
‘T’: (3,2,4,1,5, 7,8,6,9, 12,11,13,10,14)

Figure 3: An example of different examination sequences.
Besides, existing metrics are all designed for general Web search,
not considering the two-dimensional result placement in image
search. Given this situation, we adapt these metrics to the changes
in image search. For simplicity, we take image results in a row
as an integrated result with three different integration methods,
which are the sum, maximum and average of CR for the images
in the row. Then the two-dimensional offline metrics can be calculated like traditional metrics in the one dimensional ranking list
based SERPs. The Pearson’s correlation coefficients between user
satisfaction and these two-dimensional metrics are shown in Table 5. First we can see that the average row-integration method
performs better than the other two methods. Compared with results in Table 4, two-dimensional metrics of RBP(0.95), ERR and
MAX all maintain better correlations with user satisfaction while
CG, DCG@10r and AVG almost remain the same. We think it is
because RBP(0.95), ERR and MAX are more close to metrics like CG
or AVG in the two-dimensional situation where the gap between
the weights of different results is more narrow. In other words, the
improvement on the performance of metrics doesn’t benefit from
the row-integration methods.
Based on the combination of topical relevance and image quality
judgments, the Cumulative Gain (CG) and the average (AVG) of
combined relevance score have the highest correlations with user
satisfaction. On this account, it is difficult to design more powerful

620

Session 5C: New Metrics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 6: Online metrics and their descriptions.
Online Metric

Description

UCTR
QCTR
MaxRR/MinRR/MeanRR
PLC
MaxScroll
QueryDwellTime
SumClickDwell/AvgClickDwell
RTimeToFirstClick
RTimeToLastClick
DsatClickCount/DsatClickRatio

Whether there was a click or not in the session
Number of clicks in a session
Respectively maximum/minimum/mean reciprocal ranks (RR)
Number of clicks divided by the position of the lowest click position
Maximum of scroll distance
Dwell time of the query session
Respectively sum/average of click dwell time in a query.
Reciprocal of time delta between the start of search session and the first click
Reciprocal of time delta between the start of search session and the last click
Number/ratio of dissatisfied (DSAT) clicks within the session

Sources of Evidence

Table 7: Comparison of Pearson’s correlation coefficients between user satisfaction and Mouse-based/Dwell Time-based
online metrics (* indicates the correlation is significant at
p < 0.001 level.)
Mouse-based
UCTR
QCTR
MaxRR
MinRR
MeanRR
PLC
MaxScroll

Pearson’s r
0.551*
0.363*
0.280*
0.107*
0.217*
0.313*
-0.022

Dwell Time-based
QueryDwellTime
SumClickDwell
AvgClickDwell
RTimeToFirstClick
RTimeToLastClick
DsatClickCount
DsatClickRatio

Table 8: Comparison of Pearson’s correlation coefficients between user satisfaction and online metrics based on different user interaction signals (* indicates the correlation is significant at p < 0.001 level. † indicates the difference is significant at p < 0.001 level, comparing to the same metric based
on users’ click behavior.)

Pearson’s r
0.174*
0.288*
0.358*
0.362*
0.202*
0.360*
0.509*

UCTR
QCTR
MaxRR
MinRR
MeanRR
MaxRRow
MinRRow
MeanRRow
PLC
SumClickDwell
AvgClickDwell
RTimeToFirstClick
RTimeToLastClick
DsatClickCount
DsatClickRatio

offline metrics in image search based on existing topical relevance
and image quality judgments.

4.3

Mouse
Mouse
Mouse
Mouse
Mouse
Dwell
Dwell
Dwell
Dwell
Dwell

Comparison Across Online Metrics

Different from offline evaluation metrics with relevance judgments,
online metrics have been widely adopted for modern search engines
because such metrics are calculated based on the realistic users’
behavior logs collected from search engines to measure their performances. Following the previous research [7], the online metrics
we use in this work and their definitions are shown in Table 6. At
first, we compare the performance of Mouse-based metrics and
Dwell Time-based metrics. The Pearson’s correlation coefficients
between these metrics and user satisfaction are shown in Table 7.
The results reveal some interesting findings:
(1) In [7], most online metrics correlate with user satisfaction
negatively. However, in our work, except MaxScroll, online metrics have a positive correlation with user satisfaction. In general
Web search, the interactions signals which online metrics adopted
usually reflect search effort and high search effort can reduce user
satisfaction [6, 25]. On the contrary, in image search scenarios, it
takes users less effort to examine or click an image result. We note
that MaxScroll, which maintains a moderate negative correlation
(-0.519) with user satisfaction in general Web search [7], hardly
correlates (-0.022) with user satisfaction at all in image search. This
may also result from that the user-perceived effort of scrolling
decreases in image search.
(2) UCTR metric maintains the best positive correlation with user
satisfaction. It indicates that users’ click behavior is a commendable
signal to infer user satisfaction in image search environments. We
can also draw this conclusion from the result that SumClickDwell
and AvgClickDwell have positive correlations with user satisfaction,
rather than negative correlations in general Web search [7].
(3) DsatClickCount and DsatClickRatio surprisingly have positive correlations with user satisfaction. Here we define clicks with
a dwell time < 15s as dissatisfied clicks. We also test different
thresholds while the correlations are still positive. Only when the
thresholds are very small, the correlations are close to zero. Our
explanation is that there are few clicks, especially dissatisfied clicks,

Click-based

Hover-based

C&H-based

0.551*
0.363*
0.280*
0.107*
0.217*
0.457*
0.273*
0.400*
0.313*
0.288*
0.358*
0.362*
0.202*
0.360*
0.509*

0.166*†
0.066†
0.068†
0.037†
0.055†
0.129*†
0.098†
0.127*†
0.175*†
0.086†
0.107*†
0.102*†
0.124*†
0.063†
0.100*†

0.312*†
0.147*†
0.230*†
0.009†
0.112*†
0.353*†
0.120*†
0.249*†
0.309*
0.256*
0.307*†
0.020†
0.149*†
0.145*†
0.215*†

in image search. Since users can browse a snapshot of an image
result, they may be more careful to click an image. Therefore, all
the clicks are likely to be satisfied clicks.
(4) From previous analyses, we can reasonably assume that users’
clicking an image usually implies that the image is relevant to the
user. Now it is not hard to account for the positive correlations
between user satisfaction and metrics like MaxRR, MinRR, MeanRR
and PLC. If we take a clicked image result as a relevant result,
MaxRR, MinRR and MeanRR are close to Mean Reciprocal Rank
(MRR) and PLC will be regarded as approximately the precision of
the results.
In previous work [7], Chen et al. incorporate mouse hover information into existing online evaluation metrics, and show that
they can better align with user satisfaction than click-based online
metrics. Following their work, we compare the performance of
Click-based, Hover-based as well as Click and Hover-based (C&Hbased, clicks and hovers are both regarded as “clicks”) online metrics.
In addition, considering the two-dimensional result placement in
image search, we compute MaxRRow/MinRRow/MeanRRow, which
are maximum/minimum/mean reciprocal rows (rather than ranks)
of the clicks, to be compared with MaxRR/MinRR/MeanRR. The
Pearson’s correlation coefficients between user satisfaction and
these metrics are shown in Table 8.
We can see from the results that Click-based metrics significantly
outperform Hover-based and C&H-based metrics. We know that in
general Web search, various types of results such as instant answers
and verticals may contain sufficient information to satisfy the users,
which sometimes makes clicks unnecessary. In such case, hovers
can help capture more information than clicks [7]. However, in
image search, all the results are images, which makes hover behavior a noisy signal. In contrast, click behavior becomes an essential

621

Session 5C: New Metrics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

indicator of user satisfaction. Besides, row-based metrics (MaxRRow/MinRRow/MeanRRow) correlate better with user satisfaction
than rank-based metrics (MaxRR/MinRR/MeanRR). It may suggest
that the effort to examine images in the same row is not equal to
that in different rows.
In summary, based on the comparison of correlations between
user satisfaction and different online metrics in this section, we can
draw a conclusion that users’ click behavior is an important and
commendable signal to infer user satisfaction in image search scenarios. Furthermore, compared with the offline metrics introduced
in Section 4.2, UCTR has much better correlations with user satisfaction. It implies that in the context of image search, online metrics
based on mouse click information can be even more effective than
offline metrics, even without any relevance judgments.

4.4

5.0
4.5
4.0

satisfaction

3.5
3.0
2.5
2.0
1.5
1.0
1

2

3

4

5

6

7

8

9

10

11

12

task

Figure 4: Satisfaction distribution among different tasks: Exploring (task 1-4); Entertaining (task 5-8); Locating (task 912).

Offline Metrics vs. Online Metrics in
Different Search Intent Scenarios

In previous sections, we have established in general the correlations
between user satisfaction and different offline/online evaluation
metrics in the context of image search. For offline metrics, topical
relevance and image quality are of equal importance to depict the
relevance of image results. Based on the combination of these two
judgments, offline metrics correlate better with user satisfaction.
However, the performance of offline metrics is poorer than that
of some online metrics based on mouse click information. In this
section, we select out some best performing offline/online metrics
and take a deep insight into how these metrics infer user satisfaction in different search intent scenarios described in Section 3.1.2.
In the dataset described above, there are 373/287/459 queries for
Exploring/Entertaining/Locating categories respectively.
Firstly, we analyze the distribution of user satisfaction among
different tasks. The results are shown in Figure 4. From the results
we can observe that there are variances among satisfaction across
different tasks. For example, Entertaining tasks (task 5 to 8) usually
have higher satisfaction than Exploring (task 1 to 4) or Locating
(task 9 to 12) tasks. We also note that the Entertaining task 9 has
much lower satisfaction than other tasks. We think the reason is
that, for task 9, it is very difficult to collect images including both
dancers and wine glasses in order to design a poster for the dancing
party. Users issued about twice as many queries in task 9 as any
other task.
In three search intent scenarios, we compare Pearson’s correlation coefficients between user satisfaction and offline metrics based
on different relevance judgments. Here all the offline metrics are
calculated according to original ranking orders of the image results
since we do not find significant differences between metrics based
on different examination sequences. For the same reason, we do
not consider two-dimensional offline metrics as well. The results
are shown in Table 9. Although combined relevance (CR) performs
better overall, the performances of different judgments are quite different in three search intent scenarios. For Exploring tasks, offline
metrics based on topical relevance solely have the best performance.
Meanwhile, the correlations between user satisfaction and metrics
based on image quality are very low. It indicates that when users
perform exploratory tasks, they focus more on topical relevance
than image quality of image results. In Exploring search intent
scenario, the main requirement of users is to confirm or compare
information by browsing images. Even if there are some flaws in
a relevant image, users can obtain some useful information from
the image to fulfill their needs. Nevertheless, for Locating tasks,

the opposite is the case, where metrics based on topical relevance
perform far worse than those based on image quality or combined
relevance. This is in line with our expectation because users want
to find images for further use in Locating search intent scenario. For
example, in one of the Locating tasks, the participants are required
to make a slide about Harry Potter. To make it more elegant, the
participants may need to find some posters of Harry Potter films. In
spite that an image is highly relevant to the query “Harry Potter”,
users may not be satisfied with the image if it has some flaws like a
watermark. Given this reason, users focus more on image quality
than topical relevance in Locating search intent scenario. As for
Entertaining tasks, users are instructed to freely browse the image
results to relax. Sometimes they just want to look through several
photos of their favorite stars. There is also a situation where they
find the latest poster of the star and can not help saving this image. Therefore, in Entertaining search intent scenario, both highly
relevant images and high-quality images may make users more
satisfied, and metrics based on combined relevance correlate better
with user satisfaction than those based on solely topical relevance
or image quality.
We also investigate the performance of online metrics in different
search intent scenarios. Table 10 shows the results. In different
search intent scenarios, the relationship between the performances
of different online metrics remains consistent. However, compared
with Exploring or Entertaining search intent scenario, these online
metrics have significantly better correlations with user satisfaction
in Locating search intent scenario. Most of these metrics are based
on users’ mouse click and dwell time behaviors. For Locating tasks,
users usually have some specific requirements about images. When
they browse images in SERPs, they tend to be more careful in
clicking images. On the other hand, once they click an image and
want to download it for further use, it can take users more time to
stay on the landing (preview) page. In this case, users’ mouse click
and dwell time behaviors become more powerful signals to infer
user satisfaction.
To sum up, since image search users have different intents in
different search intent scenarios, what characteristics of the images
they focus on and how they behave vary tremendously across
different intents. In Exploring search intent scenario, users focus
more on topical relevance of images. However, for Locating tasks,
image quality becomes a more important factor to be considered

622

Session 5C: New Metrics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 9: Comparison of Pearson’s correlation coefficients between user satisfaction and offline metrics based on different
relevance judgments in different search intent scenarios (* indicates the correlation is significant at p < 0.001 level. † indicates
the difference is significant at p < 0.001 level, comparing to the same metric based on the combination judgments.)

CG
DCG@10r
RBP(0.95)
ERR

TR
0.222*
0.220*
0.214*
0.173*

Exploring
IQ
0.087†
0.113†
0.136
0.106

CR
0.201*
0.206*
0.204*
0.162

TR
0.171†
0.204*†
0.223*
0.187

Entertaining
IQ
0.314*
0.328*
0.297*
0.210*

Table 10: Comparison of Pearson’s correlation coefficients
between user satisfaction and online metrics in different
search intent scenarios (* indicates the correlation is significant at p < 0.001 level.)

UCTR
QCTR
MaxRRow
MinRRow
MeanRRow
PLC
MaxScroll
QDT
SumClickDwell
AvgClickDwell
RTimeToFirstClick
RTimeToLastClick
DsatClickCount
DsatClickRatio

Exploring

Entertaining

Locating

0.415*
0.276*
0.406*
0.178*
0.332*
0.219*
0.021
0.252*
0.280*
0.319*
0.298*
0.112
0.265*
0.338*

0.430*
0.264*
0.302*
0.165
0.253*
0.237*
-0.004
0.128
0.124
0.186
0.270*
0.122
0.271*
0.422*

0.632*
0.424*
0.493*
0.393*
0.470*
0.371*
-0.050
0.255*
0.356*
0.457*
0.378*
0.297*
0.420*
0.609*

TR
0.083†
0.096†
0.159*†
0.096†

Locating
IQ
0.328*
0.327*
0.328*
0.194*

CR
0.302*
0.312*
0.336*
0.227*

hover behavior brings some noise. Since clicking an image is likely
to imply that the image is useful to the user in image search and
it is easy for search engines to collect users’ click behavior, online
metrics based on mouse click can be as effective as offline metrics
in image search, even without any relevance judgments.
Our work is the first attempt to compare the performances of
offline and online evaluation metrics in image search scenarios.
The results provide insights for both evaluation metrics and user
satisfaction understanding in image search. Certainly, there are still
some limitations of our work which we would like to list as our
future work directions. The scale of laboratory-based dataset we
used in our work is relatively small compared to the commercial
search engine settings. Meanwhile, we only focus on query-level
satisfaction evaluation. The study of large-scale offline and online
metrics comparison and session-level evaluation metrics are left for
future work. In addition, given the unsatisfactory performance of
offline metrics based on topical relevance and image quality judgments, we would like to further study the appropriate assessments
for image search. Mao et al. [38] collect user-perceived usefulness
of search results and compare that with topical relevance judgment
by the external assessors. The study find that those two assessments are not aligned well. In our future work, we would like to
investigate the relationship between user-perceived usefulness and
different potential judgments that can result in more effective offline
evaluation for image search.

by users. Meanwhile, users’ mouse click and dwell time behaviors
are demonstrated to be more powerful to reflect user satisfaction
in Locating search intent scenario.

5

CR
0.333*
0.349*
0.340*
0.244*

CONCLUSIONS AND FUTURE WORK

Search engine evaluation is essential in both academic and industrial IR research. While the differences in results presentation between image search and general Web search engines may have an
impact on existing methods of search evaluation, the performances
of offline and online metrics in image search scenarios remain
under-investigated. In this paper, we regard user satisfaction as
the golden standard in search performance evaluation. Based on a
laboratory user study where we collect users’ explicit satisfaction
feedbacks and user behavior signals in image search scenarios as
well as topical relevance and image quality judgments gathered
from external assessors, we investigate how different offline and online evaluation metrics align with user satisfaction. Offline metrics
are computed based on relevance judgments. We find that topical
relevance and image quality are both essential factors to measure
the relevance of results in image search. However, these two judgments play different roles in different search intent scenarios. For
Exploring tasks that users intend to confirm or compare information about something, users focus more on topical relevance than
image quality of image results. On the contrary, in Locating search
intent scenario where users want to find images for further use,
they may take image quality as a more important facet than topical
relevance. In general, offline metrics based on the combination of
these two judgments correlate better with user satisfaction. Unfortunately, the combined relevance is still not a good indicator of
user’s perceived usefulness for the image results, which constrains
the performance of offline metrics in image search environments.
On the other hand, the online metric UCTR has significantly better performance than offline metrics in term of aligning with user
satisfaction. It suggests that click behavior is an essential signal in
image search, especially in Locating search intent scenario, while

ACKNOWLEDGMENTS
This work is supported by Natural Science Foundation of China
(Grant No. 61622208, 61732008, 61532011) and National Key Basic
Research Program (2015CB358700).

REFERENCES
[1] Azzah Al-Maskari and Mark Sanderson. 2010. A review of factors influencing user
satisfaction in information retrieval. Journal of the Association for Information
Science & Technology 61, 5 (2010), 859–868.
[2] Azzah Al-Maskari, Mark Sanderson, and Paul Clough. 2007. The relationship
between IR effectiveness measures and user satisfaction. In Proceedings of the
30th annual international ACM SIGIR conference on Research and development in
information retrieval. ACM, 773–774.
[3] Paul André, Edward Cutrell, Desney S. Tan, and Greg Smith. 2009. Designing
Novel Image Search Interfaces by Understanding Unique Characteristics and Usage.
Springer Berlin Heidelberg, Berlin, Heidelberg, 340–353. https://doi.org/10.1007/
978-3-642-03658-3_40
[4] Ben Carterette. 2011. System effectiveness, user models, and user utility: a
conceptual framework for investigation. In Proceedings of the 34th international
ACM SIGIR conference on Research and development in Information Retrieval. ACM,
903–912.
[5] Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. 2009. Expected
reciprocal rank for graded relevance. In Proceedings of the 18th ACM conference
on Information and knowledge management. ACM, 621–630.
[6] Ye Chen, Yiqun Liu, Ke Zhou, Meng Wang, Min Zhang, and Shaoping Ma. 2015.
Does vertical bring more satisfaction?: Predicting search satisfaction in a heterogeneous environment. In Proceedings of the 24th ACM International on Conference
on Information and Knowledge Management. ACM, 1581–1590.
[7] Ye Chen, Ke Zhou, Yiqun Liu, Min Zhang, and Shaoping Ma. 2017. Metaevaluation of Online and Offline Web Search Evaluation Metrics. In Proceedings
of the 40th International ACM SIGIR Conference on Research and Development in
Information Retrieval. ACM, 15–24.

623

Session 5C: New Metrics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

[35] Yiqun Liu, Ye Chen, Jinhui Tang, Jiashen Sun, Min Zhang, Shaoping Ma, and
Xuan Zhu. 2015. Different users, different opinions: Predicting search satisfaction
with mouse movement information. In Proceedings of the 38th International ACM
SIGIR Conference on Research and Development in Information Retrieval. ACM,
493–502.
[36] Cheng Luo, Yiqun Liu, Tetsuya Sakai, Fan Zhang, Min Zhang, and Shaoping Ma.
2017. Evaluating Mobile Search with Height-Biased Gain. In International ACM
SIGIR Conference on Research and Development in Information Retrieval. 435–444.
[37] Christopher D Manning, Prabhakar Raghavan, Hinrich Schütze, et al. 2008. Introduction to information retrieval. Vol. 1. Cambridge university press Cambridge.
[38] Jiaxin Mao, Yiqun Liu, Ke Zhou, Jian-Yun Nie, Jingtao Song, Min Zhang, Shaoping
Ma, Jiashen Sun, and Hengliang Luo. 2016. When does Relevance Mean Usefulness
and User Satisfaction in Web Search?. In Proceedings of the 39th International ACM
SIGIR conference on Research and Development in Information Retrieval. ACM,
463–472.
[39] Alistair Moffat, Paul Thomas, and Falk Scholer. 2013. Users versus models:
What observation tells us about effectiveness metrics. In Proceedings of the 22nd
ACM international conference on Information & Knowledge Management. ACM,
659–668.
[40] Alistair Moffat and Justin Zobel. 2008. Rank-biased precision for measurement
of retrieval effectiveness. ACM Transactions on Information Systems (TOIS) 27, 1
(2008), 2.
[41] Neil O’Hare, Paloma de Juan, Rossano Schifanella, Yunlong He, Dawei Yin, and Yi
Chang. 2016. Leveraging user interaction signals for web image search. In Proceedings of the 39th International ACM SIGIR conference on Research and Development
in Information Retrieval. ACM, 559–568.
[42] Jose San Pedro and Stefan Siersdorfer. 2009. Ranking and classifying attractiveness of photos in folksonomies. In International Conference on World Wide Web.
771–780.
[43] Hsiao Tieh Pu. 2013. A comparative analysis of web image and textual queries.
Online Information Review 29, 5 (2013), 457–467.
[44] Mark Sanderson. 2010. Performance measures used in image information retrieval.
In ImageCLEF. Springer, 81–94.
[45] Mark Sanderson, Monica Lestari Paramita, Paul Clough, and Evangelos Kanoulas.
2010. Do user preferences and evaluation measures line up?. In Proceedings
of the 33rd international ACM SIGIR conference on Research and development in
information retrieval. ACM, 555–562.
[46] Tefko Saracevic. 1975. Relevance: A review of and a framework for the thinking
on the notion in information science. Journal of the Association for Information
Science and Technology 26, 6 (1975), 321–343.
[47] Anne Schuth, Katja Hofmann, and Filip Radlinski. 2015. Predicting search satisfaction metrics with interleaved comparisons. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information
Retrieval. ACM, 463–472.
[48] Mark D Smucker and Charles LA Clarke. 2012. Time-based calibration of effectiveness measures. In Proceedings of the 35th international ACM SIGIR conference
on Research and development in information retrieval. ACM, 95–104.
[49] Louise T. Su. 1992. Evaluation measures for interactive information retrieval.
Pergamon Press, Inc. 503–516 pages.
[50] Reinier H van Leuken, Lluis Garcia, Ximena Olivares, and Roelof van Zwol.
2009. Visual diversification of image search results. In Proceedings of the 18th
international conference on World wide web. ACM, 341–350.
[51] Hongning Wang, Yang Song, Ming Wei Chang, Xiaodong He, Ahmed Hassan, and
Ryen W. White. 2014. Modeling action-level satisfaction for search task satisfaction
prediction. ACM. 123–132 pages.
[52] Kyle Williams, Julia Kiseleva, Aidan C. Crook, Imed Zitouni, Ahmed Hassan
Awadallah, and Madian Khabsa. 2016. Is This Your Final Answer?: Evaluating
the Effect of Answers on Good Abandonment in Mobile Search. In Proceedings
of the 39th International ACM SIGIR Conference on Research and Development in
Information Retrieval (SIGIR ’16). ACM, New York, NY, USA, 889–892. https:
//doi.org/10.1145/2911451.2914736
[53] Xiaohui Xie, Yiqun Liu, Maarten De Rijke, Jiyin He, Min Zhang, and Shaoping
Ma. 2018. Why People Search for Images using Web Search Engines. WSDM’18
(2018).
[54] Xiaohui Xie, Yiqun Liu, Xiaochuan Wang, Meng Wang, Zhijing Wu, Yingying
Wu, Min Zhang, and Shaoping Ma. 2017. Investigating Examination Behavior of
Image Search Users. In Proceedings of the 40th International ACM SIGIR Conference
on Research and Development in Information Retrieval (SIGIR ’17). ACM, New
York, NY, USA, 275–284. https://doi.org/10.1145/3077136.3080799
[55] Emine Yilmaz, Manisha Verma, Nick Craswell, Filip Radlinski, and Peter Bailey.
2014. Relevance and effort: An analysis of document utility. In Proceedings of the
23rd ACM International Conference on Conference on Information and Knowledge
Management. ACM, 91–100.

[8] Flavio Chierichetti, Ravi Kumar, and Prabhakar Raghavan. 2011. Optimizing
two-dimensional search results presentation. In Proceedings of the fourth ACM
international conference on Web search and data mining. ACM, 257–266.
[9] Aleksandr Chuklin, Pavel Serdyukov, and Maarten De Rijke. 2013. Click modelbased information retrieval metrics. In Proceedings of the 36th international ACM
SIGIR conference on Research and development in information retrieval. ACM,
493–502.
[10] Cyril Cleverdon and Michael Keen. 1966. Aslib Cranfield research project. Factors
determining the performance of indexing systems 2 (1966).
[11] Ovidiu Dan and Brian D Davison. 2016. Measuring and Predicting Search Engine
Users’ Satisfaction. ACM Computing Surveys (CSUR) 49, 1 (2016), 18.
[12] Zhicheng Dou and Zhicheng Dou. 2013. Summaries, ranked retrieval and sessions:
a unified framework for information access evaluation. In International ACM
SIGIR Conference on Research and Development in Information Retrieval. 473–482.
[13] Henry A Feild, James Allan, and Rosie Jones. 2010. Predicting searcher frustration.
In Proceedings of the 33rd international ACM SIGIR conference on Research and
development in information retrieval. ACM, 34–41.
[14] Steve Fox, Kuldeep Karnawat, Mark Mydland, Susan Dumais, and Thomas White.
2005. Evaluating implicit measures to improve web search. Acm Transactions on
Information Systems 23, 2 (2005), 147–168.
[15] Bo Geng, Linjun Yang, Chao Xu, Xian-Sheng Hua, and Shipeng Li. 2011. The role
of attractiveness in web image search. In Proceedings of the 19th ACM international
conference on Multimedia. ACM, 63–72.
[16] Qi Guo and Yang Song. 2016. Large-scale analysis of viewing behavior: Towards
measuring satisfaction with mobile proactive systems. In Proceedings of the 25th
ACM International on Conference on Information and Knowledge Management.
ACM, 579–588.
[17] Qi Guo, Shuai Yuan, and Eugene Agichtein. 2011. Detecting success in mobile
search from interaction. In International ACM SIGIR Conference on Research and
Development in Information Retrieval. 1229–1230.
[18] Ahmed Hassan, Rosie Jones, and Kristina Lisa Klinkner. 2010. Beyond DCG: user
behavior as a predictor of a successful search. In Proceedings of the third ACM
international conference on Web search and data mining. ACM, 221–230.
[19] Ahmed Hassan and Ryen W White. 2013. Personalized models of search satisfaction. In Proceedings of the 22nd ACM international conference on Information &
Knowledge Management. ACM, 2009–2018.
[20] Thorsten Hennig-Thurau and Alexander Klee. 1997. The impact of customer satisfaction and relationship quality on customer retention: A critical reassessment
and model development. Psychology & Marketing 14, 8 (1997), 737–764.
[21] Katja Hofmann, Lihong Li, Filip Radlinski, et al. 2016. Online evaluation for
information retrieval. Foundations and Trends® in Information Retrieval 10, 1
(2016), 1–117.
[22] Scott B Huffman and Michael Hochster. 2007. How well does result relevance
predict session satisfaction?. In Proceedings of the 30th annual international ACM
SIGIR conference on Research and development in information retrieval. ACM,
567–574.
[23] Vidit Jain and Manik Varma. 2011. Learning to re-rank: query-dependent image
re-ranking using click data. In Proceedings of the 20th international conference on
World wide web. ACM, 277–286.
[24] Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated gain-based evaluation
of IR techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (2002),
422–446.
[25] Jiepu Jiang, Ahmed Hassan Awadallah, Xiaolin Shi, and Ryen W. White. 2015.
Understanding and Predicting Graded Search Satisfaction. In Eighth ACM International Conference on Web Search and Data Mining. ACM, 57–66.
[26] Thorsten Joachims. 2002. Evaluating retrieval performance using clickthrough
data. Text Mining 57 (2002), 79–96.
[27] Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In
Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining. 133–142.
[28] Karen Sparck Jones. 1981. Information retrieval experiment. ButterworthHeinemann.
[29] Madian Khabsa, Aidan Crook, Ahmed Hassan Awadallah, Imed Zitouni, Tasos
Anastasakos, and Kyle Williams. 2016. Learning to Account for Good Abandonment in Search Success Metrics. In ACM International on Conference on Information and Knowledge Management. 1893–1896.
[30] Youngho Kim, Ahmed Hassan, Ryen W White, and Imed Zitouni. 2014. Comparing
client and server dwell time estimates for click-level satisfaction prediction.
In Proceedings of the 37th international ACM SIGIR conference on Research &
development in information retrieval. ACM, 895–898.
[31] Ron Kohavi, Roger Longbotham, Sommerfield Dan, and Randal M. Henne. 2009.
Controlled experiments on the web: survey and practical guide. Data Mining &
Knowledge Discovery 18, 1 (2009), 140–181.
[32] Dmitry Lagun, Chih Hung Hsieh, Dale Webster, and Vidhya Navalpakkam. 2014.
Towards better measurement of attention and satisfaction in mobile search. In
International ACM SIGIR Conference on Research & Development in Information
Retrieval. 113–122.
[33] J Richard Landis and Gary G Koch. 1977. The measurement of observer agreement
for categorical data. biometrics (1977), 159–174.
[34] Lihong Li, Jin Young Kim, and Imed Zitouni. 2015. Toward predicting the outcome
of an A/B experiment for search relevance. In Proceedings of the Eighth ACM
International Conference on Web Search and Data Mining. ACM, 37–46.

624

