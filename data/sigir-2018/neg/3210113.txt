Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Towards Distributed Pairwise Ranking using Implicit Feedback
Mohsan Jameel

University of Hildesheim
mohsan.jameel@ismll.de

Nicolas Schilling

Lars Schmidt-Thieme

University of Hildesheim
schilling@ismll.de

University of Hildesheim
schmidt-thieme@ismll.de

ABSTRACT
R (i |c) = |{j | ŷ(j |c, Θ) ≥ ŷ(i |c, Θ)}|

Learning with pairwise ranking methods for implicit feedback
datasets has shown promising results as compared to pointwise
ranking methods for recommendation tasks. However, there is limited effort in scaling the pairwise ranking methods in a large scale
distributed setting. In this paper we address the scalability aspect
of a pairwise ranking method using Factorization Machines in
distributed settings. Our proposed method is based on a block partitioning of the model parameters so that each distributed worker
runs stochastic gradient updates on an independent block. We developed a dynamic block creation and exchange strategy by utilizing
the frequency of occurrence of a feature in the local training data
of a worker. Empirical evidence on publicly available benchmark
datasets indicates that the proposed method scales better than the
static block based methods and outperforms competing state-ofthe-art methods.

(1)

ACM Reference Format:
Mohsan Jameel, Nicolas Schilling, and Lars Schmidt-Thieme. 2018. Towards
Distributed Pairwise Ranking using Implicit Feedback. In SIGIR ’18: The
41st International ACM SIGIR Conference on Research & Development in
Information Retrieval, July 8–12, 2018, Ann Arbor, MI, USA. ACM, New York,
NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210113

The pairwise ranking algorithms [10] have shown to outperform
pointwise ranking algorithms [5, 6] on the ranking task. The pairwise methods are well suited for implicit feedback as they learn a
pairwise loss over a set of observed positive feedback and a set of
unobserved feedback. However, scalability studies of the pairwise
methods for implicit feedback are limited, and generally focused
on shared memory systems [11].
In this paper we address the scalability of a pairwise ranking
algorithm for a large scale dataset. Specifically, we investigate the
viability of existing distributed Stochastic Gradient Optimization
(SGO) algorithms [3, 5], which were originally designed for a pointwise loss and are mostly limited to a single relational Matrix Factorization (MF). These methods block partition the rating matrix
and create static blocks of model parameters. However, the implicit
feedback dataset only contains observed positive examples and
unobserved examples are sampled. The static block partitioning
of model parameters limits the sample space of unobserved examples, which can therefore introduce a bias in the gradient updates.
We present a dynamic block partitioning and exchange strategy
for model parameters by utilizing the information about the frequency of occurrence of features in each local data partition. We
demonstrate the applicability of our algorithm by using a generic
framework based on Factorization Machines (FM) [9] to solve a
pairwise ranking problem including multiple entity relationships.

1

2

KEYWORDS
Recommender Systems, Distributed Pairwise Ranking, Implicit
Feedback

PERSONALIZED PAIRWISE RANKING
FROM IMPLICIT FEEDBACK
2.1 Pairwise Ranking from Implicit Feedback

INTRODUCTION

Personalized recommender systems are pivotal in enhancing customer’s online shopping experiences, due to their ability to make
personalized recommendations. These recommender systems learn
user behavior from past observations, which are captured either
through explicit user feedback, i.e. ratings or implicitly from user
interaction with the system, i.e. the user purchase history, movies
watched etc. In real world systems, it is usually inexpensive to
capture implicit feedback. The recommender systems generate a
ranked list of targets i, for each personalized request represented
through a given context c. The ranking function R (i |c) → N+ ,
defined in (1), generates an ordered ranking of the targets for each
context c ∈ C, where ŷ(i |c, Θ) is a model with model parameters
Θ.

A context may consist of a single entity like a user, or multiple
entities, i.e. a combination of a user and a resource, and can also
have additional relations. Suppose there are D entities in a context,
Q
the context set is defined as C = lD=1 Xl , where Xl is the set of
entities l. A set of targets T , for example movies watched or tags
used for annotation and the historical observations are captured in
a set S ⊂ C × T . 1 Given training data Ds ← {(c, i + , i − ) | (c, i + ) ∈
S ∧ (c, i − ) < S}, a pairwise ranking optimization problem is defined
X
as,
arg min
L(i + ≻c i − , Θ) + λ Θ ∥Θ∥ 22
(2)
Θ

where i +

(c,i +,i − ) ∈ Ds

≻c
defines a pairwise ranking, i.e. target i + is preferred
over i − for a given context c, and λ Θ ∈ R+ is the regularization
parameter for the L2 regularization of the model parameters. There
are many choices for the pairwise loss function L(i + ≻c i − , Θ), but
for this paper we used the Bayesian personalized ranking (BPR)

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210113

1 For

i−

example, in a tag recommender, the observation set S can be defined as S ⊆
X 1 × X 2 × T , where X 1 is a set of all users, X 2 is a set of all items and T is a set of
all tags.

973

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

achieve partial parallelism for a subset of the model parameters.
Therefore, we propose to partition the observation set on one of
the context entities Xu with the highest number of features i.e.
′
u = arg maxu ′ ∈ {1, ..., D } |Xu |. We used the famous ‘bin packing’
[7] algorithm to divide the observation dataset among workers.
First, we calculated the number of observations for each context,
then we assigned all the contexts with an element a ∈ Xu to a
worker such that the total number of observations at each worker
is approximately equal. This ensures that all the observations for
a context is available to one worker only. The cost of partitioning using ‘bin packing’ is proportional to random sharding. Since
the observation set at each worker is disjoint in Xu , the model
parameters corresponding to the entity Xu are not required to be
exchanged among workers. Thus we achieved a partial parallelism
in Xu .

[10] loss in (3), which is designed using a Bayesian modeling of D s
and has shown strong empirical results [10, 12].


L(i + ≻c i − , Θ) = − ln σ (ŷ(c, i + , Θ) − ŷ(c, i − , Θ))
(3)
BPR optimization can be applied to any recommender model. However, factorization models are the most popular among these models.
Factorization Machines (FM) [9] present a generic approach towards
solving recommender tasks and can mimic most of the factorization
models through feature engineering, which is why we will focus
on them. Let x ∈ RM be the feature vector, then the second order
expansion of FMs is given as,
ŷ(x) = w 0 + ⟨w, x⟩ +

M X
M
X

⟨vl , vj ⟩xl x j

(4)

l =1 j=l +1

where K ∈ N+ is the hyper-parameter defining the dimensionality
of factorization, Θ = {wo , w, V} are the model parameters where
w 0 ∈ R, w ∈ RM , V ∈ RM ×K , and ⟨·, ·⟩ is the dot/scalar product
of two vectors. The second term in (4) captures all two-way interactions between input and factorizes the interaction weight. The
feature vector x for personalized ranking is represented through
binary indicators as.
|X 1 |

|X D |

3.2

|T |

z
}|
{
z
}|
{ z
}|
{
x = (0, . . . 0, 1, 0, . . . 0, . . . , 0, . . . 0, 1, 0, . . . 0, 0, . . . 0, 1, 0, . . . 0) (5)
|
{z
}
C

3

DISTRIBUTED PAIRWISE RANKING (DPR)
OPTIMIZATION

The pairwise ranking algorithm optimizes a ranking loss between
pairs of observed and unobserved interactions. A simple way to
learn in a distributed setting is by using parallel stochastic gradient
descent [8]. The training data is randomly partitioned among workers. Each worker learns a separate model on each individual data
partition and a master node averages these models after each pass
over the data. The communication cost is high as each worker communicates a copy of the model parameters with a master worker. A
better way is to exploit parallelism in the problem structure, such
that every distributed worker updates an independent and disjoint
set of model parameters. Many training instances in a sparse dataset
are orthogonal, i.e. ⟨xi , xj ⟩ = 0, and can be scheduled in parallel
without any overlapping updates to the model parameters. But
finding all non-conflicting training pairs is computationally not
feasible. We use simple heuristics based on the problem structure to
overcome the computational complexity and apply it to a generic
Factorization Model i.e Factorization Machines (FM). The main
design consideration was building a dynamic block partitioning
scheme, which is driven by the frequency of occurrences of model
parameters on a given worker. Our approach differs from existing
approaches [3, 13] in two aspects, a) they target a pointwise method,
and b) create blocks through a static partitioning scheme.

3.1

Model Local Parallel

The second level of parallelism is achieved by partitioning the
model parameters. The data partitioning on entity Xu means we
require O (|Xu |) less communication. The remaining entities of
the context group are divided into P disjoint subsets Xiv ∩ Xjv =
, ∀i, j ∈ {1, . . . , P }|i , j, where Xiv ⊂ Xv and Xjv ⊂ Xv and
v ∈ {1, . . . , D} \ {u}. Similarly the target entity is also divided into
P subsets T1 , T2 · · · , TP . The partitioning of model parameters into
subsets allows each individual worker to work on a disjoint set of
training examples i.e. Xpu × Xpv × Tp ⊂ Sp . During the learning
process the model parameters corresponding to entities Xv and T
need to be exchanged between workers. It is important to create
these partitions intelligently so that each worker receives those
model parameters for which it has training examples.
Static Scheme

Relative Imbalance (RI%)

Delicious

V11
V12

80

V33

V21
V22
V13

V31
V32
V23

60
Static
Dynamic
40

20

0
0

10

20

30

40

Iterations

(a)

50

60

70

80

Dynamic Scheme
V11

V21

V31

V12

V22

V32

V33b
V23b

V13a

V13b

V33a

V23a

(b)

Figure 1: (a) Relative percentage of Imbalance (RI%) updates
across 16 blocks. A lower value of RI% shows better work
balance among workers. (b) The illustration of static and dynamic partitioning and exchange scheme with an example
using the three entities and three workers.
3.2.1 Static Block Partitioning. The existing methods [3] create
these subsets through the index division, i.e. let b = |Xv | then the
indices of the subset Xpv can be calculated as p(b/P ) to (p + 1)(b/P ).
These static blocks are than cyclically exchanged among workers
and require P D−1 rounds to complete an epoch. There are two major
short falls in this strategy. Firstly, although the model parameters
are evenly divided, the number of updates per block are not. We
tracked the number of updates per block Up in each iteration and
define the relative percentage of imbalance as (Umax −Umin )/Usum ,

Data Local Parallel

In distributed pointwise techniques, the observation set S is randomly sharded into P disjoint blocks i.e. Sp=1, ..., P . This technique
is not feasible for a pairwise loss optimization. Firstly, the unobserved instances are inferred from the observed set. Therefore the
observed set for a given context must be available in the same
data partition Sp . Secondly, using a smart partitioning we can

974

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

where Umax , Umin , and Usum are maximum, minimum and sum
across P blocks respectively. Fig(1) shows a high percentage of
imbalance updates across blocks for the static scheme. Secondly,
the static partitioning of the target entity restricts the sample space
to a small subset, i.e. |Tp | ≪ |T |.

have used average Area under the ROC curve (AUC) given in [10]
for a comparison of the prediction quality of these methods2 . DPR
and PSGD [14] are implemented in C++ using the Message Passing
Interface (MPI) 3 .We also compared against the state-of-the-art
pointwise ranking method WR-MF [5], which is based on a single
relational Matrix Factorization and is implemented in Apache Spark
4 . To illustrate different partitioning schemes presented in Section 3,
we denote DPR-dynamic for a dynamic block partitioning scheme
and DPR-static for a static block partitioning scheme. A cluster of
10 nodes is used for the evaluation purposes, where each node has
an Intel Xeon E5620 2.40GHz and 24 GB RAM, connected through
a Gigabit interconnect.
Table 1: The statistics and parameter for each dataset

3.2.2 Dynamic Block Partitioning. A more intuitive way is to
utilize the information about how likely a parameter will be updated
at the target worker. The frequency F of occurrence of a feature i
P
in the training set Sp , is calculated as Fi (p) = x∈Sp I (x i , 0) (I is
an indicator function that returns one if x i , 0, and zero otherwise)
provides a better guess about the likelihood of a parameter update.
We propose to use the empirical distribution over this frequency
to select the destination worker for a model parameter. Once the
blocks are created they are communicated to the target workers.
We employ this strategy every time a worker needs to communicate
his current share of the model parameters with other workers. Fig
(1) shows a comparison of the percentage of imbalance between
the static and dynamic scheme. It can be seen that the percentage
of imbalance is greatly reduced using the dynamic scheme. Each
worker uniformly distributes the model parameters of a target entity
T to the other workers, which helps in increasing the sample space
of unobserved examples. The dynamic blocks of model parameters
are created at every communication round as illustrated with an
example in Fig (1). A complete overview of our Distributed Pairwise
Ranking (DPR) algorithm is listed in Algorithm (1), where Exchange
represents exchange strategies explained above.

|X 1 |
|X 2 |
|X 3 |
|S|

4.2

2:
3:
4:
5:
6:
7:
8:
9:
10:

yahoo[2]
1 × 106
6.2 × 105
2.6 × 108

Delicious[4]
5.3 × 105
1.7 × 106
2.4 × 106
1.4 × 108

Comparison of Convergence

In the first set of experiments in Fig (2), the convergence speed of
DPR, WR-MF, and PSGD were compared on the Yahoo, netflix and
movielens datasets. The graph presents the relative difference to
the maximum AUC achieved among the competing models vs time
(seconds) in log-scale. On all the datasets, DPR-dynamic converges
fastest to the best AUC values of 0.993 and 0.974 on the Yahoo
and netflix datasets respectively, and clearly show superiority over
DPR-static. WR-MF took longer to reach the same AUC value on
Yahoo and movielens datasets, but it attains a lower AUC (0.968)
on netflix. The DPR-static was slowest to converge to similar AUC
values as DPR-dynamic and WR-MF. The DPR-dynamic achieved
faster convergence because of the dynamic blocking scheme, which
presents more diverse sample pairs as compare to DPR-static and
speeds up learning. PSGD on the other hand did not converge to a
better result on these datasets in a reasonable time.
The second set of experiments were conducted on the Delicious
dataset, which is originally an implicit feedback dataset and contains three entities. WR-MF is not applicable in this settings as
it is limited to the datasets containing relationships between two
entities. We compared DPR-dynamic with DPR-static and PSGD.
DPR-dynamic converges fastest to the best AUC value of 0.992,
whereas DPR-static converges to an AUC of 0.990 and took considerably more time. PSGD again did not converge to a good value of
AUC in a reasonable time. Overall, we observed that DPR-dynamic
works better than DPR-static.

Algorithm 1 Distributed Pairwise Ranking (DPR) Algorithm
1:

ml10m
ml20m
netflix[1]
7.1 × 104 1.3 × 105 4.8 × 105
1.0 × 104 2.7 × 104 1.7 × 104
1 × 107
2 × 107
9.9 × 107

w0 ← 0; w ← (0, . . . , 0); V ∼ N (0, ρ) ▷ where Θ = {w 0 , w, V}
▷ distributed workers P run in parallel
repeat
for round ∈ {1, . . . , D} do
p
for (c, i + , i − ) ∈ Xpu × Xpv × Tp × Tp ⊂ Ds do


∆ ← 1 − σ (ŷ(c, i + , Θ) − ŷ(c, i − , Θ))


∂ ŷ(c, i + , Θ) − ŷ(c, i − , Θ)
θ ← θ − η∆ ∂θ
Exchange(Θp , F , round)
until stopping criterion is met
Gather all model parameters Θ from P workers

4 EXPERIMENTS
4.1 Datasets and Evaluation
In this section, we empirically evaluate the proposed algorithm
DPR on five publicly available recommender systems benchmark
datasets listed in Table (1). The Delicious dataset is an implicit
feedback dataset, it contains tagging activities of users for resources.
The movielens, netflix and Yahoo datasets are originally 5-star
rating based datasets with entities being a user and a movie. They
are preprocessed into implicit feedback dataset by retaining tuples
with rating values ≥ 3. The Delicious dataset is preprocessed to
5 cores (i.e. each user, item and tag appears at least in 5 posts),
the others are preprocessed to 10 cores. We did a hyperparameter
search in grids, i.e K = {24 , · · · , 27 }, λ = {10−5 , · · · , 10−1 }, η =
{10−5 , · · · , 10−1 } and α = {10−2 , · · · , 10}. A test set is created by
removing a single post for each context from the training set. We

4.3

Scalability of DPR

In this section we present a scalability analysis of the DPR (DPRdynamic) algorithm. Fig (3) presents the learning curves by varying
the number of workers. The x-axis represents a log-scale of the
CPU time (in seconds) expanded by multiplying with the number of
workers. The y-axis represents relative difference to the maximum
AUC. A linear scalability in the number of workers is achieved, if
2 For

a detailed comparison of BPR and WR-MF on different evaluation measures in a
serial setting, we direct our readers to [10, 12].
3 MPICH version 3.2, https://www.mpich.org/.
4 Spark 2.2.0, https://spark.apache.org/docs/2.2.0/mllib-collaborative-filtering.html

975

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

yahoo

DPR-Dynamic
DPR-Static

0.07

WR-MF

PSGD

DPR-Dynamic
DPR-Static

0.05

PSGD

0.06
0.04
0.05
0.03

0.04
0.03

0.02

0.02
0.01

0.01

DPR-32

103

DPR-Dynamic
DPR-Static

WR-MF

0.07
0.06

DPR-Dynamic
DPR-Static

WR-MF

PSGD

0.10
0.05
0.08
0.04
0.06

0.03

0.04

0.02

0.02

DPR-32

0.03
0.04
0.02
0.02

0.01
0.00

0.00
104

102

ml20m, workers-16
PSGD

DPR-16

103

104

Delicious
101

ml10m, workers-16
0.12

DPR-4
DPR-8
0.06

0.00
102

0.05

DPR-2
DPR-4

DPR-16

DPR-32

0.04
0.03
0.02
0.01
0.00
103

0.01

104

time(s) x workers

0.00

0.00
100

101

101

time(s)

102

time(s)

Figure 3: The scalability analysis of DPR (DPR-dynamic) on
varying number of workers. The y-axis is the relative difference to the maximum AUC value. The x-axis, represents
time(sec) expanded by multiplying by the number of workers in log-scale.

Delicious, workers-32
DPR-dynamic

DPR-static

PSGD

relative to the max AUC

0.08

0.06

0.04

[2] Gideon Dror, Noam Koenigstein, Yehuda Koren, and Markus Weimer. 2012. The
Yahoo! Music Dataset and KDD-Cup ’11. In Proceedings of KDD Cup 2011 competition, San Diego, CA, USA, 2011. 8–18.
[3] Rainer Gemulla, Erik Nijkamp, Peter J. Haas, and Yannis Sismanis. 2011. Largescale matrix factorization with distributed stochastic gradient descent. In Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, San Diego, CA, USA, August 21-24, 2011. 69–77.
[4] Olaf Görlitz, Sergej Sizov, and Steffen Staab. 2008. PINTS: peer-to-peer infrastructure for tagging systems. In Proceedings of the 7th international conference on
Peer-to-peer systems, IPTPS’08, Tampa, FL, USA, February 25-26, 2008. 19.
[5] Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative filtering for implicit feedback datasets. In Data Mining, 2008. ICDM’08. Eighth IEEE International
Conference on. Ieee, 263–272.
[6] Noam Koenigstein and Ulrich Paquet. 2013. Xbox movies recommendations:
variational bayes matrix factorization with embedded feature selection. In Seventh
ACM Conference on Recommender Systems, RecSys ’13, Hong Kong, China, October
12-16, 2013. 129–136.
[7] John Levine and Frederick Ducatelle. 2004. Ant colony optimization and local
search for bin packing and cutting stock problems. Journal of the Operational
Research Society 55, 7 (2004), 705–716.
[8] Mu Li, Li Zhou, Zichao Yang, Aaron Li, Fei Xia, David G Andersen, and Alexander
Smola. 2013. Parameter server for distributed machine learning. In Big Learning
NIPS Workshop, Vol. 1.
[9] Steffen Rendle. 2012. Factorization Machines with libFM. ACM TIST 3, 3 (2012),
57:1–57:22.
[10] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.
2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. In Proceedings
of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI ’09,
Montreal, Quebec, Canada. 452–461.
[11] A. Murat Yagci, Tevfik Aytekin, and Fikret S. Gürgen. 2017. On Parallelizing SGD
for Pairwise Learning to Rank in Collaborative Filtering Recommender Systems.
In Proceedings of the Eleventh ACM Conference on Recommender Systems, RecSys
2017, Como, Italy, August 27-31, 2017. 37–41.
[12] Lu Yu, Ge Zhou, Chu-Xu Zhang, Junming Huang, Chuang Liu, and Zi-Ke Zhang.
2016. RankMBPR: Rank-Aware Mutual Bayesian Personalized Ranking for Item
Recommendation. In Web-Age Information Management - 17th International Conference, WAIM 2016, Nanchang, China, June 3-5, 2016, Proceedings, Part I. 244–256.
[13] Erheng Zhong, Yue Shi, Nathan Liu, and Suju Rajan. 2016. Scaling Factorization
Machines with Parameter Server. In Proceedings of the 25th ACM International on
Conference on Information and Knowledge Management (CIKM ’16). ACM, New
York, NY, USA, 1583–1592. DOI:https://doi.org/10.1145/2983323.2983364
[14] Martin Zinkevich, Markus Weimer, Alexander J. Smola, and Lihong Li. 2010.
Parallelized Stochastic Gradient Descent. In 24th Annual Conference on Neural
Information Processing Systems, Vancouver, British Columbia, Canada. 2595–2603.

0.02

0.00
102

103

time(s)

Figure 2: The convergence of different methods on recommender datasets. The y-axis is the relative difference to the
maximum AUC value. The x-axis, represents time(sec) in
log-scale.

the learning curves recorded by running on a varying number of
workers overlap. All the learning curves overlap, which shows the
linear scaling of the learning curves across the number of workers.

5

netflix

DPR-16

103

0.00

relative to the max AUC

WR-MF

DPR-4
DPR-8

0.04

relative to the max AUC

relative to the max AUC

0.08

netflix, workers-32

0.06

relative to the max AUC

yahoo, workers-32

CONCLUSION

In this paper we study the scalability of a pairwise ranking algorithm in distributed settings. We investigate the applicability
of distributed SGD techniques for a pointwise method on a pairwise method. The static block partitioning scheme employed by
pointwise methods was not useful for pairwise methods. We developed a dynamic block partitioning of model parameters and
experimentally show that it works better than the static scheme.
The experiments show that DPR outperforms WR-MR, which is a
distributed algorithm that optimizes a pointwise loss.
As a future work, it would be interesting to investigate the dynamic partitioning scheme for the exchange of parameters for other
model classes as well as in an asynchronous distributed algorithm.
This would require overlapping exchange and update steps of the
DPR algorithm.

REFERENCES
[1] Robert M. Bell and Yehuda Koren. 2007. Lessons from the Netflix prize challenge.
SIGKDD Explorations 9, 2 (2007), 75–79.

976

