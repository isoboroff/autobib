Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Towards Intent-Aware Contextual Music Recommendation:
Initial Experiments
Sergey Volokhin

Eugene Agichtein

Emory University
sergey.volokhin@emory.edu

Emory University
eugene.agichtein@emory.edu
Table 1: Music Listening Intents and Descriptions

ABSTRACT
While activity-aware music recommendation has been shown to improve the listener experience, we posit that modeling the listening
intent can further improve recommendation quality. In this paper,
we perform initial exploration of the dominant music listening intents associated with common activities, using music retrieved from
popular online music services. We show that these intents can be
approximated through audio features of the music itself, and potentially improve recommendation quality. Our initial results, based
on 10 common activities and 5 popular listening intents associated
with these activities, support our hypothesis, and open a promising
direction towards intent-aware contextual music recommendation.

Intent
Concentration
Distraction

Filtering backBlock out irritating or loud noise.
ground noise
Clear one’s mind and try to create something
Inspiration
or come up with a solution to a problem.
Mood and emoEvoking certain emotions or mood using music.
tion control
Motivation
Gathering strength and will to complete a task.
Relaxation
Become less anxious and tense, rest.

KEYWORDS
Contextual music recommendation; music listening intent
ACM Reference Format:
Sergey Volokhin and Eugene Agichtein. 2018. Towards Intent-Aware Contextual Music Recommendation: Initial Experiments. In SIGIR ’18: The 41st
International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8–12, 2018, Ann Arbor, MI, USA. ACM, Ann Arbor, MI,
USA, 4 pages.

1

Description
Sharpening attention and devoting it to task at
hand and avoid distraction.
Be distracted or entertained, not to be bored by
the current activity.

However, a recent study has shown this not to be the case: for the
same activity, different users often report different music listening
intents [13]. By showing that music listening intent is distinct from
the activity context, reference[13] indicated that further improvements to music recommendation may be possible.
Further empirical validation of the music listening intent is
needed, and more importantly, an exploration of whether the listening intent could be operationalized to improve recommendation
quality. Furthermore, user intent, especially in recommender systems, is notoriously difficult to automatically infer, and to incorporate into recommendation. To make this problem more tractable,
we propose to model the activity context, and the intent of the
listener jointly, by first using the (inferred or specified) activity
context to restrict the set of the most likely intents, and then use
the intent-specific model to prioritize the contextual music recommendations. Our contributions are threefold: First, we estimate
the empirical music listening intent distribution of music videos
posted on YouTube.com, one of the currently most popular music
sharing services (Section 2.1). Second, we perform initial experiments on representing music listening intent as audio features
using another popular music API, from Spotify.com (Section 2.2).
Third, we report initial results on using the trained audio intent
models to improve activity- and intent-aware music recommendation, and show promising improvements over activity-only music
recommendation (Section 3.2).

INTRODUCTION AND MOTIVATION

Millions of people increasingly rely on music recommendation and
streaming services for extended periods each day. The most popular
music services such as Pandora, Spotify or Last.fm, use, among other
algorithms, context aware recommendations, which often include
demographic context, user location, type of the device, time of
day, and others, to improve recommendation quality. Contextual
recommender systems (CARS) have been extensively studied [3,
4], and have been successfully incorporated into state-of-the-art
recommendation systems [11, 12].
In addition to context, other dimensions of recommendations
have been proposed, such as user emotion, knowledge, skills, or
psychological state[11, 12], which can be difficult to detect explicitly.
Therefore, previous research has primarily focused on representing
the activity as the main recommendation context (e.g.,[2, 7, 9]),
with the underlying assumption that it is sufficient to represent the
user context to match the user’s listening intent.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210154

2

METHODOLOGY

We begin by validating the survey-based results of the most popular
music listening intents for 10 most common activities described
in[13]. The most common intents and activities reported therein are
reproduced in Tables 1 and 2 respectively. In Section 2.1 we describe
our empirical validation of associating the common activities and
intents using a popular online music sharing site (YouTube.com):
we exploit the user-posted descriptions for the music videos to

1045

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA

Sergey Volokhin and Eugene Agichtein

Table 2: Activities
Cleaning
Shopping

Cooking
Showering

Driving
Studying

Eating
Walking

word embedding space. Specifically, we used pre-trained word2vec
model[10] provided by Google2 , which contains 300-dimensional
vectors to represent over three million words and phrases, to analyze that metadata and to compute semantic similarity between
descriptions and sets of keywords, corresponding to intents. We
have experimented with different thresholds of similarity, and decided that 0 was the most accurate one.
To augment the embeddings-based similarity, we also experimented with three other complementary approaches to match the
intents to user-provided video descriptions:
• Stemming: Instead of lemmatization described above, we
experimented with word stemming (using NLTK PorterStemmer). The results were similar, with the most popular posted
intents of 6 of the 11 activities matching those reported in
the survey of reference[13]
• Focusing on Verbs: As verbs are often strongly associated
with intent statements (e.g., “focus” or “concentrate”), we
experimented on focusing explicitly on the verbs contained
in the descriptions, using the TextBlob parser3 for part of
speech tagging, and calculated similarity of intent and description based on the verbs and verb phrases alone. Interestingly, the intents for the same 6 of the 11 activities agreed
with the survey data.
• Topic modeling-based matching: Finally, we experimented
with representing the video descriptions using LDA topics[5].
We used NLTK and gensim packages for python for LDA
modeling on lemmatized descriptions. We created a single aggregated corpus of descriptions for all activities, and trained
the LDA topic models with varying number of topics M, with
M=15 resulting in most meaningful and interpretable topics
in our data.

Exercising
Working

infer the contributor intent when posting music videos of playlists
for common activities (e.g., “exercise”). Then, in Section 2.2 we
use another popular online music service, Spotify.com, to identify
the audio features of music associated with the playlists generated for popular intents (e.g., “concentration”), and in turn to use
these to train intent-specific audio models. We then use the trained
models to re-rank the activity-specific music recommendation by
incorporating the pre-trained intent music models.

2.1

Using Online Statistics to Validate
Activity-Intent Association

To provide empirical validation of the dominant intents associated with common activities, we retrieved human-generated music
playlists from YouTube.com. We used the retrieved data to analyze
the posted video descriptions using word embeddings[10] to match
the description texts to each of the intents in Table 1. Our intuition
is that while many of the descriptions may not reflect the intent of
the listener, some of the music/video contributors would anticipate
the needs of the users, to attract listeners/viewers, and thus use
intent-oriented keywords in the description, such as “inspirational”
or “relaxing”. For this task, we used the popular YouTube Search
API1 to retrieve videos with the associated metadata, including
keywords and descriptions corresponding to intent in their names.
We used the following procedure to compute the match between
a potential listening intent and a video description:
(1) Activity-based video retrieval: Retrieve raw descriptions,
tags, and names of videos using the name of the activity (e.g.,
“exercise”) as the query, with additional filters designed to
restrict the results to only music playlists, e.g., including the
word ’music’ in the query, and specifying length of over 20
minutes.
(2) Filtered text: Remove spurious text like web-links and common phrases, such as asking to subscribe to a channel, to
follow the contributor on social media, and generic words
like ’music’, ’best’, ’video’, etc.
(3) Lemmatization: Tokenize and convert the remaining text
to the lemmatized form, using the WordNet lemmatizer[1].
(4) Expand Intent Representation: For our initial proof-ofconcept prototype, we manually developed a list of keywords
for each intent, and automatically expanded it using the
aforementioned word-embeddings, by selecting the 10 most
similar words in the embedding space for each starting keyword. The expansion words were also lemmatized to enable
more accurate match with words in video descriptions.
(5) Compute Intent-Description Similarity: Finally, for each
music video description we estimated which intents were
mentioned, as described below, and computed the distribution of the represented intents.
Intent Match Computation: We predicted up to top K (K=3) most
similar intent matches for each music video, using the cosine similarity between the video description and the intent description, in the

After experimenting with these methods of matching survey-based
intents in Table 1 to the actual descriptions of online videos, we
found that all methods produced similar intent distribution. Hence,
for the reported results described next, we chose the XXX method
for matching intents to videos.
Results: Figure 1 compares the distribution of top-3 most popular intents for some of the activities based on survey data from
[13], vs. the inferred intents in the YouTube data extracted using
the procedure described above. Interestingly, the top-3 most popular intents from both datasets match for 6 of the 10 activities
reported in reference[13]. For the remaining 4 activities, the empirical intent distribution matched at least one out of top-3 reported
intents, on average agreeing on 2.4 intents out of top-3. These results provide empirical support to self-reported dominant intents
in reference[13].
LDA did not provide any new results, but partially supported
hypotheses about the correlation between intents and activities
(some activities had most popular intents with them in the topic).
Discussion: We should note that user-provided video descriptions
are noisy, and it is almost impossible to filter out spurious text,
such as tracklists, or lyrics. Also, some of the music videos may
not be playlists or valid music videos, despite our filters. Another
potential source of noise is the YouTube playlist naming conventions. Contributors of the videos often try to name their videos
using generic and catchy words, in order to appeal to YouTube
2 Available

at https://code.google.com/archive/p/word2vec/

3 http://textblob.readthedocs.io/en/dev/

1 https://developers.google.com/apis-explorer/#p/youtube/v3/youtube.search.list

1046

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Towards Intent-Aware Contextual Music Recommendation: Initial Experiments

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA

Table 3: Summary of Audio Features from Spotify API
Feature Name
Acousticness
Danceability
Duration in ms
Energy
Instrumentalness
Key
Liveness
Loudness
Mode

Figure 1: Average distribution of intents for some activities from online descriptions (left diagonal shading) vs. selfreports (right diagonal shading).

Speechiness

Tempo

viewers and search algorithms, which means that popular words
are more likely to appear. For example, we were able to find only
5 videos total for the activity “commuting”, which apparently is
just not a popular music video topic, despite this activity being
somewhat popular from self-reported music listening (7t h out of
11 among all participants, according to [13]). We also were not
able to find matching videos for intents ’background filtering’ and
’distraction’. Therefore, we excluded the “Commuting” activity and
these 2 intents from further analysis.
Interestingly, the intent “relaxation” appeared with almost every
activity (9 out of 10), but was associated with only 4 out of 10
activities in the survey data reported in[13]. We conjecture that
this is due to the bias of YouTube contributors to describe videos
using common keywords such as “relaxation” to attract viewers.

2.2

Valence

We experimented with a variety of machine learning algorithms
for this task, including Logistic Regression and both fuzzy and hard
Clustering, but the best results were achieved using a Random Forest classifier (using sklearn implementation). The overall accuracy
of classification, trained and tested on user-generated playlists, was
0.56 on hold-out test data, which is significantly higher than an
expected random guess accuracy of about 0.2 (as we used the 5
available intents for the experiments). While the current intent
classification accuracy is modest, we are not aware of an existing
baseline for this task, and plan to further improve upon these initial
results in future work.

Learning Intent-Aware Audio Models

Having partially validated the set of dominant intents for the variety
of common activities, we explore whether we can represent intent
using audio features.
To do this, we used a free API from a popular online music service,
Spotify.com, which provides extensive search and music recommendation and metadata capabilities. Specifically, using the Spotify
Python API4 , we retrieved playlists for each intent in Table 1 and
activity in Table 2. We then extracted the available audio features,
also using a Spotify API5 . These audio features are summarized in
Table 3.
While Spotify does not disclose how those features are calculated, from our inspection, the interpretable features appear to be
accurate and intuitive. These features also are relatively comprehensive, and provide a good initial representation for audio-based
recommendation. Even more detailed song analysis with deeper
breakdown of each song is available from Spotify, and could be
explored in future work.
Using those features, we now attempt to learn an association
between listening intent and audio characteristics. We use the usercontributed playlists with intent words in the playlist titles to provide examples of intent-specific music to a training algorithm.
4 Available

Description
A confidence measure of whether the track is acoustic.
Describes how suitable a track is for dancing based
on a combination of musical elements.
The duration of the track in milliseconds.
Represents a perceptual measure of intensity and activity. Energetic tracks feel fast, loud, and noisy.
Predicts whether a track contains no vocals. Rap or
spoken word tracks are clearly ’vocal’.
The key the track is in. Integers map to pitches using
standard Pitch Class notation.
Detects the presence of an audience in the recording.
The overall average loudness of a track in dB.
Indicates the modality (major or minor) of a track.
Detects the presence of spoken words in a track. The
more exclusively speech-like the recording (e.g. talk
show, audio book, poetry), the closer to 1.0 the attribute value.
The overall estimated tempo of a track in beats per
minute (BPM).
Describes the musical positiveness. Tracks with high
valence sound more happy, while tracks with low
valence sound more negative, sad.

3

INTENT-AWARE RECOMMENDATION

We now describe our initial study of incorporating intent into
contextual music recommendation.

3.1

Experimental Setup

To create the initial set of playlists/tracks, we used the Spotify API
to generate multiple activity-specific playlists. We then re-ranked
the songs using the trained intent model, described above, in order
to generate a final intent-aware playlist for each activity, for the
intents identified to be dominant for this activity in the previous
section. We compared two methods of recommending music tracks:
(1) SPTF: the popular Spotify playlist generator, based on activity only: We retrieve the first playlist generated by Spotify,
using the activity keywords as query, considering this result
as a strong baseline for activity-specific music recommendation.
(2) AIR: Activity-aware Intent Recommendation (Our method):
using the top playlists retrieved by the Spotify API, our
method AIR re-ranks songs for each activity using pre-trained
intent models, as described above. Top 10 songs with highest

at https://github.com/plamere/spotipy

5 https://developer.spotify.com/web-api/object-model/#audio-features-object

1047

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA

Sergey Volokhin and Eugene Agichtein

Table 4: Correlation of the AIR and SPTF playlists for the
five most common activities vs. human judgments.
nMRR
Syst.
Activ.
Driving
Working
Exercising
Cleaning
Cooking

Kendall τ

• Italian Restaurant Music of Italy – Italian Tango
• Martin Landh – At Last
• Enzo – Silent Joy
The music pieces selected for this playlist are generally instrumental
with little to no vocals and slow tempo which could be described as
appropriate for a dinner or lunch activity. In fact, this playlist was
blindly rated by two independent human judges and had a high
correlation with human preferences (Tau = 0.63, nMRR = 0.7).
In summary, the initial results show the promise of the proposed approach. For activities Driving, Cooking and Eating, the
AIR intent-aware recommendation method had superior correlation with human preferences than a state of the art activity-only
recommendation. However, we emphasize that these are only initial
results, and much room remains for improvement.

τ -AP

AIR

SPTF

AIR

SPTF

AIR

SPTF

0.82
0.36
0.67
0.62
1.0

0.52
0.72
0.44
0.72
0.72

0.2
0.25
-0.15
-0.28
0.5

-0.19
-0.37
0.07
0.16
-0.54

0.04
0.06
-0.28
-0.32
0.5

-0.32
-0.46
-0.11
-0.07
-0.58

predicted scores for the activity-specific dominant intents
are selected as recommendations for each activity.

4

To evaluate the playlists, we conducted a user study by asking
participants to rate the two generated playlists. The automatically
generated playlists for each of the 10 activities, from both methods,
were pooled. The randomly re-shuffled pooled playlists were then
rated by one, two or three independent human judges (median
being 2) who were not the authors. The judges who ranked the
tracks from best to worst, judging by how they would fit the activity.
The rankings were averaged across the judges for each activity.
The quality of the playlists was evaluated using rank correlations
against the human ordering described above, using three metrics:

In this paper we provided empirical evidence that multiple music
listening intents are strongly associated with common physical
activities. Furthermore, these intents can be captured in available
audio features from popular music streaming service APIs, namely
Spotify.com. Finally, we showed promising results in improving the
recommendation quality by incorporating the audio characteristics
associated with the intents.
While our initial results are promising, many open directions
remain. Though we took the initial activity detection as a given,
incorporating potentially noisy output of activity detection into the
recommendation remains a challenge. Automatically associating
music intent with audio characteristics can be further improved,
enabling promising opportunities for contextual intent-aware music
recommendation.

• Kendallτ : We used the well-known Kendall rank correlation
coefficient or τ [8] to calculate how well do the human preferences correlate to the systems rankings. However, Kendallτ
is known not to distinguish between agreement high and
low in the rankings, thus we also considered other metrics.
• τ -AP metric is used to calculate relevance of recommendations [14].
• nMMR: We use a variation of Mean Reciprocal Rank[6] from
information retrieval, normalized against best possible ranking (which we call nMMR), to compare the playlists as a
whole compared to the human preferences.

3.2

REFERENCES
[1] 2010. Princeton University "About WordNet." WordNet. Princeton University.
(2010). https://wordnet.princeton.edu/
[2] 2014. Google Activity Recognition Client. (2014). goo.gl/V98vUs
[3] Sofiane Abbar, Mokrane Bouzeghoub, and Stéphane Lopez. 2009. Context-aware
recommender systems: A service-oriented approach. In VLDB PersDB workshop.
[4] Gediminas Adomavicius, Ramesh Sankaranarayanan, Shahana Sen, and Alexander Tuzhilin. 2005. Incorporating Contextual Information in Recommender
Systems Using a Multidimensional Approach. ACM Trans. Inf. Syst. 23, 1 (Jan.
2005), 103–145. https://doi.org/10.1145/1055709.1055714
[5] David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation.
Journal of machine Learning research 3, Jan (2003), 993–1022.
[6] Nick Craswell. 2009. Mean Reciprocal Rank. Springer US, Boston, MA. 1703–1703
pages. https://doi.org/10.1007/978-0-387-39940-9_488
[7] Ricardo Dias, Manuel J. Fonseca, and Ricardo Cunha. 2014. A User-centered
Music Recommendation Approach for Daily Activities. In CBRecSys@RecSys.
[8] M. G. KENDALL. 1938. A NEW MEASURE OF RANK CORRELATION. Biometrika
30, 1-2 (1938), 81–93. https://doi.org/10.1093/biomet/30.1-2.81
[9] Young-Seol Lee and Sung-Bae Cho. 2011. Activity Recognition Using Hierarchical
Hidden Markov Models on a Smartphone with 3D Accelerometer. In Proc. of HAIS
2011, Emilio Corchado, Marek Kurzyński, and Michał Woźniak (Eds.). 460–467.
https://doi.org/10.1007/978-3-642-21219-2_58
[10] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality. In
Advances in neural information processing systems. 3111–3119.
[11] Markus Schedl, Peter Knees, and Fabien Gouyon. 2017. New Paths in Music
Recommender Systems Research. In Proc. of RecSys (RecSys ’17). ACM, New York,
NY, USA, 392–393. http://doi.acm.org/10.1145/3109859.3109934
[12] Markus Schedl, Peter Knees, and Fabien Gouyon. 2017. New Paths in Music
Recommender Systems Research, Slides. (2017). Retrieved February 12, 2018
from http://www.cp.jku.at/tutorials/mrs_recsys_2017/slides.pdf
[13] Sergey Volokhin and Eugene Agichtein. 2018. Understanding Music Listening
Intents During Daily Activities with Implications for Contextual Music Recommendation. In Proc. of CHIIR.
[14] Emine Yilmaz, Javed A. Aslam, and Stephen Robertson. 2008. A New Rank
Correlation Coefficient for Information Retrieval. In Proc. of SIGIR. ACM, New
York, NY, USA, 587–594. https://doi.org/10.1145/1390334.1390435

Results and discussion

Table 4 reports the correlation of the playlists generated by our
method (AIR) and that of Spotify baseline (SPTF) for the top 5 most
popular activities, according to [13]. In three activities (Driving,
Working, Cooking) our approach is preferred to SPTF, while SPTF
is preferred for one activity, and for one there is no preference.
These results are promising, especially considering that the SPTF
playlists were human generated by an editor, while our method
re-ranks the tracks automatically.
An example of a playlist constructed by our AIR method for
the activity “Eating” is provided for illustration below. We used
9 Spotify’s playlists for training to recognized listening intents
associated with eating. Our automatically generated playlist for the
activity “Eating” contained the following songs:
•
•
•
•
•
•
•

CONCLUSIONS AND FUTURE WORK

Molly Kate Kestner – Footprints
Johannes Bornlof – Streams
Mia Strass – Warm Darkness
Ana Olgica – Sugarcane
Kylian Rebour – The World is Turning
Enzo – Message in a bottle
Peter Sandberg – Synesthesia

1048

