Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Translating Representations of Knowledge Graphs
with Neighbors
Chun-Chih Wang

Pu-Jen Cheng

National Taiwan University
r04922066@csie.ntu.edu.tw

National Taiwan University
pjcheng@csie.ntu.edu.tw

ABSTRACT

assume independence between triplets because entities are actually
connected. Therefore, context-aware models try to find dependent
properties between objects. For example, GAKE [4] proposes a general framework to integrate graphical structural information and
TransE-NMM [10] considers each entity as a mixture model of its
neighbors.
In this work, we propose a model called TransN, a novel approach
to capture more precise context information and to incorporate
neighbor information dynamically. Firstly, we apply effective neighbor selection to reduce the number of neighbors. It is computationally inefficient to consider all neighbors and some of them might
be noisy. Second, we try to encode neighborhood information with
context embeddings. It is infeasible to use only one vector to capture
both the meaning of the object and the context information at the
same time since an entity plays distinct roles when being itself or a
neighbor of others. Third, we further utilize attention mechanism
to focus on most influential nodes since different neighbors provide
different level of information.

Knowledge graph completion is a critical issue because many
applications benefit from their structural and rich resources. In
this paper, we propose a method named TransN, which considers the dependencies between triples and incorporates neighbor
information dynamically. In experiments, we evaluate our model
by link prediction and also conduct several qualitative analyses
to prove effectiveness. Experimental results show that our model
could integrate neighbor information effectively and outperform
state-of-the-art models.

KEYWORDS
Representation Learning; Knowledge Graph; Natural Language
Processing
ACM Reference Format:
Chun-Chih Wang and Pu-Jen Cheng. 2018. Translating Representations
of Knowledge Graphs with Neighbors. In SIGIR ’18: The 41st International
ACM SIGIR Conference on Research and Development in Information Retrieval,
July 8–12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages.
https://doi.org/10.1145/3209978.3210085

1

2 METHODOLOGY
2.1 Notations

INTRODUCTION

Let E denotes the set of entities and R denotes set of relations.
A knowledge graph is denoted by G, composing several correct
triplets (h, r , t), where h, t ∈ E and r ∈ R. We also create an inverse
of each relation, denoted by R −1 , i.e., R −1 = {r −1 |r ∈ R}. In this
way, a set of original triplets together with inverse triplets can
be denoted as K = {(h, r , t), (t, r −1 , h)|(h, r , t) ∈ G}, and the set
of neighbors of entity e is denoted by Ne = {(e ′, r ′ )|r ′ ∈ R ∪
R −1 , (e ′, r ′, e) ∈ K}.

A knowledge graph is a heterogeneous, multi-relational, and
directed graph comprises entities as nodes and relations as different
type of edges. Facts in knowledge graphs are represented as triplets,
(head, relation, tail), abbreviated as (h, r , t), where relation is the
relationship between head and tail entities. Although there are
many instances in knowledge graphs, they are usually far from
complete since they were constructed manually. Knowledge graph
completion aims at predicting missing relations between entities
based on existing triplets in the knowledge graphs.
Recently, a more efficient way is to represent each object (entities and relations) of knowledge graphs into a continuous vector
space [9]. These vectors will contain not only the meanings of the
objects themselves but also patterns between objects. Translationbased models, such as TransE [2], TransH [13], TransR [6] and
CTransR [6] define their own score functions and apply to a marginbased loss function. Results show that they have reached the stateof-the-art performance. However, it is unreasonable for them to

2.2

Neighbor Selection

To take advantage of neighborhood information, we need to
select effective neighbors before applying our model since not all
neighbors are useful and some of them might be noises. Furthermore, it is computationally inefficient to consider all neighbors. We
first assume that if an entity has more neighbors, it can obtain more
information from its neighbors. From this viewpoint, we select
different number of neighbors for each entity in order to consider
more neighbors for those who have more neighbors. In this way,
the number of selected neighbor of entity e can be defined as

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210085

|Ne |
+ θmin
(1)
N
where θmax and θmin are hyperparameters meaning maximum and
minimum number of neighbors to be selected, |Ne | is the number
of neighbors for entity e, and N = max(|Ne |).
θ e = (θmax − θmin ) ×

e

However, we still have no idea which neighbors are important before training. Hence, we come up with another idea to take training

917

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

process into account. For each epoch t, we derive θ et , which means
the number of neighbors to be considered for an entity e. At the
beginning of the training process, we tend to select all neighbors for
each entity so that we could know which neighbors are potentially
helpful. After a certain period of time, we gradually reduce the
number of selected neighbors to θ e the number calculated in the
previous step. In this way, we could obtain θ et by using exponential
decay to realize our idea,
θ et = δ (t) × θmax + (1 − δ (t)) × θ e

Unlike our model, TransE-NMM uses a kernel function to catch
the attention between relations. The most significant benefit of our
model is that even though two relations do not occur simultaneously
in the training data, our model is still able to find the correlation
between them.

2.4

Based on section 2.2 and 2.3, we can define our score function as
follows,
fr (h, t) = ∥ϑh,r + r − ϑt,r ∥ 22
(5)
Since knowledge graphs only contain positive examples, we
create negative examples by replacing either the head entity or
the tail entity, but not both, for each triplet. Formally, a set of
negative examples is defined as G ′ = {(h ′, r , t)|h ′ ∈ E, (h ′, r, t) <
G}∪{(h, r , t ′ )|t ′ ∈ E, (h, r , t ′ ) < G}. We use the “bern” strategy[13],
to reduce the probability of introducing false negatives. Then we
use the margin-based ranking loss as our objective function,
Õ
Õ
L=
[γ + fr (h, t) − fr (h ′, t ′ )]+

(2)

where δ (t) = exp(−µt) and µ is the decay factor. To prevent the
number from changing too fast or too slow, we directly set µ = 0.01
in our experiments.

2.3

Neighbor-based Representation

After obtaining effective neighbors for each entity, we will generate neighbor-based entity embedding based on the selected neighbors. In our model, each object, entity or relation, is represented by
two vectors, one is called object embedding while the other is called
context embedding. For example, for a triplet (h, r , t), its vectors are
h, hc , r, rc , t, tc ∈ Rn , the subscript c denotes context vectors. To be
more precise, the object embedding of an entity is used to represent
the meaning of the entity and the object embedding of a relation is
used to translate from head entity to tail entity. On the other hand,
the context embedding of an entity is to capture interactions between entities and the context embedding of a relation is to capture
correlations between relations.
We assume new neighbor-based representation is a linear combination from its object embedding and context embeddings of its
neighbors. Hence, for an entity e, its new neighbor-based representation when predicting relation r is defined as ϑe,r ,
ϑe,r = α e e + (1 − α e )

Õ

βr,r ′ ec′

(h,r,t )∈ G (h ′,r,t ′ )∈ G ′

where [x]+ ≜ max(0, x), γ is the margin size to separate positive
and negative triplets. In practice, we enforce constraints on the
norms of both object and context embeddings, i.e. ∀h, r , t, we have
∥h∥ 2 ≤ 1, ∥r∥ 2 ≤ 1, ∥t∥ 2 ≤ 1, ∥hc ∥ 2 ≤ 1, ∥rc ∥ 2 ≤ 1, ∥tc ∥ 2 ≤ 1.
The learning process of TransN is conducted using RMSprop [11]
optimizer. To avoid overfitting, we initialized the object embedding
with the results of TransE and initialize weight matrix as an identity
matrix. Context embeddings are initialized following the random
procedure stated in [5].

3 EXPERIMENTS
3.1 Datasets

(3)

In this work, we compare our model with baseline models by
link prediction [2]. Three common knowledge graphs are used,
WordNet [8], Freebase [1] and NELL [3]. WordNet is a large lexical
knowledge graph and Freebase comprises a huge amount of realworld facts. Last, NELL is a CMU project which accumulating many
beliefs by reading the web. Table 1 shows the statistics of each
dataset.

(e ′,r ′ )∈Ne (θ et )

where Ne (θ et ) is the subset of Ne which has θ et number of neighbors.
For each entity e, there is a corresponding α e to determine whether
the entity should weight more either on its own object embedding
or on its neighbors, and α e = σ (ae ), where σ is a sigmoid function
and ae ∈ R.
Even though we have already applied neighbor selection in the
previous section, the importance of the selected neighbors should
not be the identical. For example, when predicting whether two
students are classmates or not, it is more useful to know the schools
they study at than knowing their gender or nationality. In this
way, we introduce attention mechanism [7] to weight neighbors
differently. We use βr,r ′ to capture the correlation between two
relations r ans r ′ . If relation r ′ is helpful when predicting relation
r , then βr,r ′ should be larger. We also introduce a weight matrix
W ∈ Rn×n to capture the interaction between two vectors. β will
be obtained by multiplying context embeddings of two relations
with W followed by a softmax normalization. As a consequence,
βr,r ′ is defined as,
exp(w r,r ′ )
exp(rc Wrc′ )
βr,r ′ = Í
= Í
i exp(w r,r i )
i exp(rc Wri,c )
where rc is the context embedding of relation r .

Training Objective

Table 1: Data statistics.

3.2

Dataset

#Rel

#Ent

#Train

#Valid

#Test

WN18
FB15K
NELL186

18
1,345
186

40,493
14,951
14,463

141,422
483,142
31,134

5,000
50,000
5,000

5,000
59,071
5,000

Link Prediction

Link prediction is to predict the missing head or tail entity for a
correct triplet. During testing, we replace either head or tail entity
of each triplet by all entities in the knowledge graph. We then
calculate their scores based on fr (h, t) of the corrupted triplets and
rank them in ascending order. This is called “raw” setting. Because
a corrupted triplet may still exist in the knowledge which should be

(4)

918

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 2: Evaluation results on link prediction. Scores in “bold” and “underline” are the best and second best scores respectively.
Data Sets
Metric
Unstructured (Bordes et al. 2012)
SE (Bordes et al. 2011)
SME (linear) (Bordes et al. 2012)
SME (bilinear) (Bordes et al. 2012)
TransE (Bordes et al. 2013)
TransH (Wang et al. 2014)
TransR (Lin et al. 2015)
CTransR (Lin et al. 2015)
GAKE (Feng et al. 2016)
TransE-NMM (Nguyen et al. 2016)
TransN

WN18
Mean Rank
Hits@10
Raw Filt Raw Filt
315
304 35.3 38.2
1,011 985 68.5 80.5
545
533 65.1 74.1
526
509 54.7 61.3
263
251 75.4 89.2
401
388 73.0 82.3
238
225 79.8 92.0
231
218 79.4 92.3
259
249 84.2 95.0
142 134 86.2 97.1

FB15K
Mean Rank
Hits@10
Raw Filt Raw Filt
1,074 979 4.5
6.3
273 162 28.8 39.8
274 154 30.7 40.8
284 158 31.3 41.3
243 125 34.9 47.1
212
87 45.7 64.4
198
77 48.2 68.7
199
75 48.4 70.2
228 119 44.5 64.8
190 101 49.5 65.7
181
81 50.8 71.1

Table 3: Evaluation results on FB15K by mapping properties of relations. Scores in “bold” and “underline” are the best and
second best scores respectively.
Tasks
Relation Category
Unstructured (Bordes et al. 2012)
SE (Bordes et al. 2011)
SME (linear) (Bordes et al. 2012)
SME (bilinear) (Bordes et al. 2012)
TransE (Bordes et al. 2013)
TransH (Wang et al. 2014)
TransR (Lin et al. 2015)
CTransR (Lin et al. 2015)
TransE-NMM (Nguyen et al. 2016)
TransN

Prediction Head (HIT@10)
1-to-1 1-to-N N-to-1 N-to-N
34.5
2.5
6.1
6.6
35.6
62.6
17.2
37.5
35.1
53.7
19.0
40.3
30.9
69.6
19.9
38.6
43.7
65.7
18.2
47.2
66.8
87.6
28.7
64.5
78.8
89.2
34.1
69.2
81.5
89.0
34.7
71.2
59.9
87.1
25.0
48.5
82.7
95.3
36.1
70.3

considered to be a correct triplet, we get rid of this kind of corrupted
triplets to make sure there is only one answer before ranking. This
is called “filt” setting. Similar to previous works, we report two
evaluation metrics: mean rank of correct triplets(Mean Rank) and
the proportion of correct triplets ranked in top 10(Hits@10). A good
model should achieve lower mean rank and higher Hits@10.
In this task, we use WN18, FB15K to evaluate our performance.
Except for TransE-NMM, all previous models use the same datasets,
so we refer to their experimental results directly. For hyperparameters, we select margin size γ among {1, 2, 4}, the dimension of
both object embeddings and context embeddings n among {20, 50,
100}, learning rate λ among {0.001, 0.01, 0.1}. To avoid too much
parameter tuning, we directly set θmax and θmin to be 100 and 10
respectively. We take L 2 as dissimilarity, and train on each dataset
for 1000 rounds. Best parameters are obtained by monitoring the
performance on the valid datasets. The best configuration obtained
by valid dataset are: for WN18, γ = 1, λ = 0.01, n = 50; for FB15K,
γ = 2, λ = 0.01, n = 100.
Evaluation results of WN18 and FB15K are shown in Table 2.
Since TransE-NMM do not report its performance on these two
datasets, we implement it by ourselves and report its results. From
the results, we can observe that (1)TransN nearly outperforms all

Prediction Tail (HIT@10)
1-to-1 1-to-N N-to-1 N-to-N
34.3
4.2
1.9
6.6
34.9
14.6
68.3
41.3
32.7
14.9
61.6
43.3
28.2
13.1
76.0
41.8
43.7
19.7
66.7
50.0
65.5
39.8
83.3
67.2
79.2
37.4
90.4
72.1
80.8
38.6
90.1
73.8
59.1
36.3
93.2
53.7
81.1
40.2
94.8
74.6

the other models on WN18 and FB15K for both MeanRank and
Hits@10 which means neighbors can provide useful information;
(2)TransN achieves better results than both TransR and CTransR
even though we do not project entities into different spaces with
more complicated matrix multiplication; (3)Although GAKE uses
both neighbor and path information, TransN achieves better results,
which means translation-based models are better than graph-based
models on this task; (4)TransN outperforms baseline models more
significantly on WN18 than on FB15K, which means in a more
dense dataset, neighbors are more informative and could provide
more helpful information.
Table 3 shows the link prediction results of different mapping
properties of relations on FB15K. From the table, we can observe
TransN outperforms TransE, which proves that with the help of
neighbors, we can relieve the problems encountered by TransE.
Moreover, although we do not apply any complicated projection
method to allow each entity possessing distributed embeddings,
we could reach better results than TransH, TransR and CTransR.
Last, our model beats TransE-NMM on all types of relations, which
means with the introduction of context embeddings, we could utilize neighbor information more effectively.

919

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

(a)

(b)

or on neighbors, over three datasets. We can observe that most α are
larger than 0.5, which makes sense that new neighbor-based entity
embeddings still trust the entity itself more. If we take a closer look
at three datasets, FB15K has the biggest α overall while WN18 has
the smallest α, and NELL186 is in between. We can attribute the
results to the different properties of these datasets. From Table 1, the
density, #Ent/#Rel, of FB15K, WN18, NELL186 are 11.2, 77.8, 2274.6,
which have the same tendency of their α. This means for a more
dense dataset, neighbors can thus provide more useful information,
and α will become smaller.
The last parameter we would like to evaluate is β, which captures
the correlations between two relations. For a given relation r , we
multiply its context embeddings with all the other relations together
with the weight matrix W, and the relation with the largest β is the
most correlated one. Table 4 shows the results on three datasets.
From the table, we can find that the obtained relations are also
semantically correlated. For example, if we know the title of a given
film, it is possible for us to predict the film’s genre.

(c)

Figure 1: Visualization of the obtained embeddings on
NELL186. (a) Entity embeddings of TransE-NMM. (b) Object
embeddings of TransN. (c) Context embeddings of TransN.

4
(a) FB15K

(b) WN18

(c) NELL186

In this paper, we introduce a model called TransN, which represents each entity and relation with two vectors, one is object
embedding while the other is context embedding. We also propose
an approach to select effective neighbors to reduce noises. Experimental results on various kinds of knowledge graphs prove the
effectiveness of integrating neighbor information. In the future, we
will try to apply our idea on projection-based translation models.
Furthermore, we could also consider more high-order neighbors in
our model.

Figure 2: Distribution of α in three datasets. Each blue dot
represents an entity and the red line stands for α = 0.5.
Table 4: Examples of most correlated relations obtained
from β.

3.3

Relation

Top-1 neighboring relation

_hyponym
(WN18)

_instance_hyponym

netflix_title/netflix_genres
(FB15K)

netflix_genre/titles

wineryproduceswine
(NELL186)

beveragemadefrombeverage

CONCLUSIONS AND FUTURE WORK

REFERENCES
[1] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor.
2008. Freebase: A Collaboratively Created Graph Database for Structuring Human
Knowledge. In Proceedings of KDD. 1247–1250.
[2] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating Embeddings for Modeling Multi-relational
Data. In Proceedings of NIPS. 2787–2795.
[3] Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka, Jr., and Tom M. Mitchell. 2010. Toward an Architecture for Never-ending
Language Learning. In Proceedings of AAAI. 1306–1313.
[4] Jun Feng, Minlie Huang, Yang Yang, and Xiaoyan Zhu. 2016. GAKE: Graph Aware
Knowledge Embedding. In COLING. 641–651.
[5] Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training
deep feedforward neural networks. In Proceedings of AISTATS, Vol. 9. 249–256.
[6] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning
Entity and Relation Embeddings for Knowledge Graph Completion. In Proceedings
of AAAI. 2181–2187.
[7] Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective Approaches to Attention-based Neural Machine Translation. In Proceedings of
EMNLP. 1412–1421.
[8] George A. Miller. 1995. WordNet: A Lexical Database for English. Commun. ACM
38, 11 (1995), 39–41.
[9] Dat Quoc Nguyen. 2017. An overview of embedding models of entities and
relationships for knowledge base completion. (2017).
[10] Dat Quoc Nguyen, Kairit Sirts, Lizhen Qu, and Mark Johnson. 2016. Neighborhood
Mixture Model for Knowledge Base Completion. In Proceedings of SIGNLL. 40–50.
[11] T. Tieleman and G. Hinton. 2012. Lecture 6.5—RmsProp: Divide the gradient by
a running average of its recent magnitude. COURSERA: Neural Networks for
Machine Learning. (2012).
[12] L.J.P. van der Maaten and G.E. Hinton. 2008. Visualizing High-Dimensional Data
Using t-SNE. Journal of Machine Learning Research 9 (2008), 2579–2605.
[13] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge
Graph Embedding by Translating on Hyperplanes. In Proceedings of AAAI. 1112–
1119.

Qualitative Analysis

In this section, we conduct several deep analyses, including embedding visualization, parameter analysis, relation interpretation.
First, we show how good are the embeddings learned by TransN
and compare our model with TransE-NMM. We project our learned
embeddings to two-dimensional space by [12]. From Figure 1(a)(b),
TransN could cluster different kinds of animals or plants more effectively. That means TransN can capture more precise meaning
while keep different kinds of entities as far as possible. Furthermore, from Figure 1(b)(c), we can observe object embeddings and
context embeddings play different roles as we have mentioned. In
Figure 1(c), the context embeddings of animals are mixed together
but not intertwined with plants. It can justify our assumption that
object embeddings capture the actual meaning of the entities and
context embeddings try to interact with other entities.
Figure 2 shows the distribution of α, which is the parameter
controlling the neighbor-based embeddings to trust more on itself

920

