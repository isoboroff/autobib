Session 7B: Content & Semantics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

From Royals to Vegans: Characterizing Question Trolling on a
Community Question Answering Website
Ido Guy

Bracha Shapira

Ben-Gurion University of the Negev, Beer-Sheva, Israel
eBay Research, Netanya, Israel∗
idoguy@acm.org

Ben-Gurion University of the Negev, Beer-Sheva, Israel
bshapira@bgu.ac.il
its recent growth, the trolling phenomenon has been recognized as
a serious threat to online communities [45].
Early research on trolling behavior tended to be largely qualitative, generally involving case study analyses of a small number of
manually-identified trolls [12]. Such studies focused on exploring
trolling types [22], motivations [47], and response strategies [17].
In recent years, more quantitative studies of trolling have started to
emerge [13, 36, 37, 46, 48]. Due to the difficulty in creating a large
dataset of troll posts, most studies relied on a relatively modest set
of examples, typically identified by manual annotation or the use
of black lists of users who had been banned from the community.
However, a recent study [45] claims that such methods may often
“fail to capture what moderators and users consider trolling.”
In this work, we focus on trolling within one of the largest
community question answering (CQA) websites, Yahoo Answers
(YA). Trolling on YA can be performed both at the question level and
the answer level and in this work we study the former. We identify
troll questions based on the YA abuse report system, where any
user can report deviant behavior by another user [27]. We combine
both a user report and a subsequent deletion by a moderator in
order to label troll questions. This combination of reports from
the community and moderator actions serves as a good means for
identifying ground-truth trolling, while also allowing to work at a
scale and diversity larger than any prior work: we identified over
400,000 troll questions on YA between the years 2007 and 2014,
from over 200,000 different askers, across a wide variety of topics.
Previous work has mostly focused on trolling within forums,
news sites, or commenting platforms [12, 13, 37, 45, 48]. In such sites,
trolling is performed at the comment (post) level, usually as part of
a long thread, sometimes rich with other troll comments [37, 45],
and often aiming at the thread’s head article or a specific prior
comment. In contrast, question trolling is performed “from scratch”,
without an explicit context, which makes it different in nature and
potentially harder to detect: the position within the thread and the
content of previous posts often serve as key features for comment
trolling detection [37], yet such information is irrelevant in the case
of question trolling. To the best of our knowledge, this is the first
study to explore trolling at the question level.
In our analysis, we compare the set of troll questions to a set
of non-troll (“clean”) questions of the same size. We examine a
broad set of characteristics, including different types of metadata,
linguistic properties of the text of the question and its answers,
and past activity of the asker. Our findings reveal a handful of
distinguishing qualities. For example, despite their shorter life span,
troll questions receive more answers and votes than clean questions.
Based on these differences, we experiment with a basic classification
setting, reaching an accuracy of 85.2% for the task of distinguishing

ABSTRACT
The phenomenon of trolling has emerged as a widespread form
of abuse on news sites, online social networks, and other types of
social media. In this paper, we study a particular type of trolling, performed by asking a provocative question on a community questionanswering website. By combining user reports with subsequent
moderator deletions, we identify a set of over 400,000 troll questions
on Yahoo Answers, i.e., questions aimed to inflame, upset, and draw
attention from others on the community. This set of troll questions
spans a lengthy period of time and a diverse set of topical categories.
Our analysis reveals unique characteristics of troll questions when
compared to “regular” questions, with regards to their metadata,
text, and askers. A classifier built upon these features reaches an
accuracy of 85% over a balanced dataset. The answers’ text and
metadata, reflecting the community’s response to the question, are
found particularly productive for the classification task.
ACM Reference Format:
Ido Guy and Bracha Shapira. 2018. From Royals to Vegans: Characterizing Question Trolling on a Community Question Answering Website. In
Proceedings of The 41st International ACM SIGIR Conference on Research &
Development in Information Retrieval (SIGIR ’18). ACM, New York, NY, USA,
10 pages. https://doi.org/10.1145/3209978.3210058

1

INTRODUCTION

Social media is built upon interaction and sharing, yet its openness
also makes it vulnerable to abuse from its users. One form of such
abuse is trolling, which has become more common in recent years,
especially on news and social media sites. Herring et al. [25] defined a troll as an individual who baits and provokes other group
members, often with the result of drawing them into a fruitless
argument. A troll has also been identified as a person who engages
in negatively-marked online behavior [22], a user who initially
pretends to be a legitimate participant, but later attempts to disrupt the community [16], and a user who posts false or offensive
comments in online communities to fool others [31]. Trolls have
been characterized as trouble makers [45], bad guys [17], sadists [9],
and “creatures” who take pleasure in upsetting others [28]. With
∗ Part

of the research was conducted while working at Yahoo Research.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210058

835

Session 7B: Content & Semantics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

troll and clean questions over a balanced dataset. Textual features
are found particularly useful for classification, with the answers’
text being even more predictive than the text of the question itself.

2

forums, with nearly 12,000 users in total, they characterized the
evolution of antisocial behavior by individual users and developed
a classifier that predicted whether users would be banned based on
their overall activity. Their findings indicated that antisocial users
tend to focus their efforts on a small number of threads, are more
likely to post irrelevantly, and are more successful at garnering
responses from other users. A recent study of a forum on role playing games [45] identified trolls as users that were sanctioned by a
moderator with a ticket that matched a similar regular expression to
the one we use. Classification of both trolls and posts based on 147
posts from 120 users indicated that interaction features outperform
textual features.
Perhaps most closely related to our work is the study by Mihaylov
and Nakov [37], who developed a classifier for troll comments in
a Bulgarian news forum. They examined both a concept of “paid
trolls”, identified based on a leaked list of user names, who were
compensated for opinion manipulation, and “mentioned trolled”,
identified based on another comment accusing them using a form
of the word ‘troll’. Since such identification method is not accurate
enough, the authors used manual annotation to generate their
dataset of 578 comments from mentioned trolls. Metadata features
were found more useful than textual features, as they distinguished
replies, which were more prone to trolling, from head comments.
The literature on abusive behavior and particularly trolling on
CQA sites is sparse. To the best of our knowledge, no previous
study has focused on question trolling. Gazan [19] identified seven
content-bearing terms mostly associated with user-reported abusive
behavior on the Answerbag CQA site, with troll being one of the
seven. Question trolling is briefly mentioned in a study of religious
belief questions on YA [14], stating that “The sensitive nature of the
topic can leave posters open to the suspicion that their questions are
not genuine, and that they are simply trolling to provoke controversy.”
Additionally, question trolling can be tied with the broader topic of
question quality analysis and prediction [3]. Li et al. [33] stated that
question quality detection enables to find and recommend good
questions, but also to identify bad ones that hinder the CQA service.
Yet, as we will show, troll questions are not necessarily of low
quality. For example, they tend to have a large number of answers,
which is considered a characteristic of high-quality questions [33].
Kayes et al. [27] conducted a comprehensive study of content
abusers on YA based on the same abuse report system used in our
work. They examined general abuse, over both questions and answers, but did not inspect trolling specifically. They also developed
a classifier for abusive users, with users suspended by a moderator
as a ground truth and features derived from user activity and community feedback. The user’s history of reported posts was found
to be the most predictive feature. In our own result analysis, we tie
our findings to those reported by relevant prior work.

RELATED WORK

Research on abusive behavior online has become increasingly widespread. Early work examined online bad behavior (also sometimes
referred to as deviant, malicious, or offensive) in newsgroups [44]
and chat communities [50]. More recently, much of the work has
focused on news and social media websites. Li et al. [34] distinguished between spammers, trolls, and fanatics on a commenting
platform. Kumar [30] identified 5 types of malicious behavior by online reviewers: vandals, hoaxes, sockpuppets, trolls, and fraudulent.
Cyberbullying is a type of malicious behavior, which is “substantially different than trolling” [20]. It aims to repeatedly hurt, insult,
or harass another individual, typically a youngster [32, 53], and can
have a severe effect on the victim. Researchers have experimented
with a variety of methods to detect abusive behavior [34, 39, 40, 52].
Many of the methods applied machine learning [18, 29, 41, 49] and
some used features that bear similarity to those used in this work,
based on the content of the post or past activity of the user.
Trolling as a specific type of abusive behavior has received its
own attention in the literature. Early work attempted to define
and qualitatively characterize the phenomenon [25]. Studies of the
motivations behind trolling identified attention seeking [47], boredom [47, 51], venting [32], and amusement [9, 51] among the key
reasons. Researchers have also associated trolls with various personality traits, such as sadism [9]; aggression, deception, disruption,
and success [22]; or wickedness and nastiness [13]. A recent study,
however, demonstrated that “ordinary” people can also become
trolls, due to negative mood and exposure to trolling behavior by
others [11]. Researchers also explored reactions to trolling [17]
and assessed the impact on the community. They found that trolls
often succeed in drawing attention and disrupting the community [6, 9, 12, 48], although some argue that trolls also play a vital
social role and have an engaging impact on the community [12, 28].
Recent studies have put more focus on the detection of troll
behavior, mostly trying to identify trolling at the user level, based
on past activity and community response, rather than at the post
level as done in this work. Since troll posts are rather rare, many
studies used relatively small datasets, based on manual annotation
and/or user blacklists. Seah et al. [46] showed that domain-adapted
sentiment analysis can help improve the detection of trolls on a
Singapore forum using a dataset of 40 users. Cambria et al. [10] used
domain-specific ontologies to identify trolling in a dataset of 500
manually-annotated tweets. Dlala et al. [15] applied clustering on
the user graph to identify outliers suspicious as trolls. Siersdorfer et
al. [48] used a bag-of-words classifier to distinguish between trolls
and other users on YouTube and Slashdot based on the content and
ratings of their past comments. Another line of research ranked
the users according to their trustworthiness, with trolls being the
ones with the lowest scores [42, 56].
A few studies used moderator sanctions against users to identify
ground-truth trolling. Cheng et al. [12] explored a notion of antisocial behavior, which includes trolling, flaming, bullying, and harassment. In a rather large-scale longitudinal study of three online news

3

DATASET CREATION

Our study is based on Yahoo Answers, one of the oldest, largest,
and most diverse CQA websites [1, 27]. On YA, any user can ask a
question, which includes a mandatory title and an optional description. Users can also answer any question and vote (up or down) on
other answers (one answer per question and one vote per answer).
A best answer can be selected by the asker or, if the asker does not

836

Session 7B: Content & Semantics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: Troll-reported posts in our dataset.

provide such feedback, by the community using voting. Questions
on YA are spread across more than 1600 categories, in a taxonomy
of up to 3 levels, with 26 categories at the top level [1]. We refer
to the most general type of nodes in the hierarchy as “Top-Level
Categories” (TLCs) and to the most specific type of categories as
“Leaf-Level Categories” (LLCs).
Besides providing questions and answers, YA users may also
contribute by reporting abusive content. Reporters serve as an
intermediate layer in the YA moderation process, since these abuse
reports are verified by human inspectors (moderators). If the report
is found valid, the content is promptly deleted (97% of the validly
reported content is deleted within the same day [27]). Users can
report abusive content (questions or answers) by clicking an “abuse
report” flag sign embedded with the content and optionally writing
a text that elaborates the reasons. For our study, we considered
all English questions posted to YA in the years 2007-2014, with
their corresponding answers, votes, best answer selections, and
abuse reports. Overall, the complete dataset includes more than
200,000,000 questions with over 1,000,000,000 answers, and over
10,000,000 abuse reports.
Our first attempt to identify troll questions was by considering
questions on YA with at least one answer that includes the word
‘troll’ in different variants, as done in some of the previous studies [22, 37, 45]. In particular, we considered answers (in lower-case
form) that matched the regular expression ‘\btroll’, where ‘\b’ represents the word boundary position, filtering out words that include
but do not start with ‘troll’, such as ‘controlling’. Not using ‘\b’ at
the end of the regular expression allows capturing forms such as
‘trolls’, ‘trolling’, and even ‘trollolo’. A manual inspection by one of
the authors of 100 such questions, sampled uniformly at random
from the complete dataset, indicated that this method is rather far
from being accurate: only 53% of the questions were validated as
trolling. The reasons for the low accuracy mostly included referring to the trolling phenomenon in the answer without actually
accusing the question itself of trolling; referring to another answer
rather than the question; and teasing the asker using a false troll
accusation. Indeed, in their study of “mentioned trolls” on a news
forum, Mihaylov and Nakov stated that troll identification based
solely on a comment’s content might lead to “witch hunting”, as
users could be accused of being trolls unfairly [37].
To obtain higher accuracy for trolling identification, while maintaining a relatively large volume of identified posts, we set out to use
the YA’s abuse report system. We considered both an abuse report
by a YA user and a delete action by a moderator, since using any of
the two alone, as done in several previous studies [12, 19, 27, 34, 45],
might be insufficient: on the one hand, the abuse report functionality might be abused itself, for example, when disliking or disagreeing with a post, especially given that reporting is fast and easy,
unlimited by number, and not penalized for misuse [27]. On the
other hand, moderators themselves may misinterpret the intentions
of the alleged troll, hold slightly different definitions of trolling,
or fail to detect trolling altogether [45]. Following, we considered
questions that were reported for abuse with report text matching
the regular expression ‘\btroll’, which were subsequently deleted
by a YA moderator. In this case, manual inspection by the same
author of 100 questions, sampled uniformly at random from the
complete dataset, indicated that a high portion of 93% were troll

Deleted
Non-Deleted

Questions

Answers

419,925
312,031

91,218
176,604

questions. This gives a rough indication that the new method can
indeed capture troll questions much more accurately.
Overall, the abuse reports that matched our troll regular expression covered 999,778 questions and answers (henceforth referred
to as troll-reported posts). Table 1 shows that questions account for
the majority of the troll-reported posts (73.2%). Across all reported
posts, questions account for only 38.2%, indicating that troll abuse
is particularly common on questions. By contrast, in their study of
news and gaming forums, Cheng et al. [12] found that trolls were
more likely to reply to others’ posts than to start a new discussion.
Table 1 also indicates the portion of troll-reported questions
and answers that were deleted by a YA moderator. While most
questions (57.4%) were deleted, for answers only a minority (34.1%)
were deleted. From this point onward, we focus on troll-reported
questions that were deleted by a moderator and refer to this set
as troll questions. For comparison, we sampled uniformly at random from the complete dataset an identical number of questions
(419,925) that were reported for abuse and deleted by a moderator,
but their report text did not match the troll regular expression,
henceforth referred to as abuse questions. These represent all other
types of abuse, such as spam, impersonation, fraud, and privacy
invasion [27]. Additionally, we sampled uniformly at random an
identical number of questions from the complete dataset that were
not reported for abuse, henceforth referred to as clean questions.

4

TROLL QUESTION CHARACTERISTICS

In this section, we present an extensive comparison between troll
questions and both abuse questions and clean questions. We start
with comparing basic characteristics, such as creation time and
metadata of the question’s title, description, answers, and asker.
Following, we compare the categories of the questions based on the
YA taxonomy, allowing to gain insight into the topics that are more
(and less) prone to trolling. We then perform linguistic analysis that
reveals lexical and syntactic characteristics of the question’s three
textual fields: title, description, and answers. Finally, we inspect the
history of the questions asker’s activity.

4.1

Basic Characteristics

Overall, our troll question dataset includes 419,925 questions flagged
in 488,515 reports (max reports per question 48) by 38,695 different reporters over 208,685 different abusers. The abuser with most
reports was reported 503 times, for 75 different questions; the maximum number of unique questions reported per abuser is 192.
Our dataset includes 358,709 unique question titles. Of these,
94.26% occur once, while only 1.24% occur 5 times or more. The
most repeated troll question is the tricky “How do I ask a question
on Yahoo Answers?”, occurring 335 times by as many as 321 unique
user IDs. Table 2 lists 10 different examples of troll questions from
our dataset.

837

Session 7B: Content & Semantics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Troll questions

30%

21.7%
18.8%

20%
0%

up to 10 mins 10 mins - 1 hour 1 hour - 1 day

8.3% 6.7%

5.9% 7.7%

1 day - 1 week

over 1 week

Figure 1: Distribution of troll questions and abuse questions
by the difference between their report and creation time.
6.0%

15.0%

5.0%

14.5%

4.0%

2.0%

Troll

Abuse

Clean

0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23

13.5%
13.0%

Troll

% of questions with a best answer selected
Average (median) length of best answer in words
% of best answers of 100 words or more
Average (median) upvotes per question
Average (median) downvotes per question
% of questions with at least 1 (5) upvotes
% of questions with at least 1 (5) downvotes
Average (median) YA tenure in days
% of questions posted with YA tenure > 1 year
Average (median) point snapshot

Abuse

Monday Tuesday

Wed.

Clean
Thursday Friday

Troll

Abuse

Clean

10.8 (10)
58.4 (53)

9.9 (9)
56.3 (50)

9.6 (9)
52.1 (46)

81.5%
64.1 (23)
18.1%

83.5%
57.6 (22)
14.8%

81.3%
51.2 (28)
13.8%

6.6 (5)
12.6%
30.4 (15)
5.1%
0.37
0.23

5.9 (3)
21.6%
29.8 (14)
5.2%
0.33
0.17

4.7 (3)
14.4%
43.0 (21)
10.0%
0.28
0.12

20.9%
46.2 (21)
10.9%

16.8%
51.1 (22)
12.6%

37.1%
64.1 (35)
17.7%

9.4 (2)
7.5 (0)
3.1 (0)
5.4 (0)
4.9 (0)
2.2 (0)
60.8% (35.5%) 46.6% (26.4%) 38.7% (14.2%)
47.9% (24.0%) 38.1% (18.9%) 30.1% (10.2%)
140.5 (7)
11.7%
965.1 (91)

185.8 (19)
16.5%
1425.5 (95)

325.1 (127)
30.1%
1342.5 (119)

yet in our study their questions (both title and description) are
comparable by length to all other questions.
As shown in Table 3, the number of answers for troll questions
is higher than abuse questions, while both are higher than clean
questions, despite the fact that the question has been deleted by
a moderator at some point after posting. Kayes et al. found that
abusive users tend to attract more answers to their questions [27],
but apparently for trolls the numbers are especially high. This
becomes more evident when inspecting the medians: 5 versus 3 for
troll and abuse questions, respectively. In addition, the portion of
troll questions with no answers is especially low in comparison to
abuse questions and is even slightly lower than for clean questions,
despite the shorter life span. Overall, these findings indicate that
the “do not feed the troll” strategy [5, 23] does not hold, as users
get lured into answering troll questions. The length of the answers
tends to be shorter for troll and abuse questions compared to clean
questions. This may suggest that answerers of abusive questions
dedicate lower efforts to their answers. Very detailed answers, of
100 words or more, are especially rare for troll and abuse questions.
We also set out to explore the similarity between answers, both
for the same question and across questions of the same top-level
category (TLC). We conjectured that troll answers may be more
similar to each other, as they reflect the community’s response
to a provocation attempt. Indeed, as can be seen in Table 3, the
average cosine similarity, measured between the TF-IDF vectors
representing the answers1 , is higher for troll questions than for
clean questions and abuse questions, both when measured across
answers to the same question (“per question”) and when measured
across answers to questions within the same TLC (“per TLC”). This
supports our hypothesis that answers to troll questions are more
similar to each other within and across questions.
As could be expected, the portion of questions for which a best answer was selected is substantially lower for troll and abuse questions
than for clean questions, as their deletion blocks the opportunity to
select a best answer. Still, the portion of questions with best answer

14.0%

3.0%

Average (median) number of answers
% of questions with no answers
Average (median) answer length in words
% of answers of 100 words or more
Average answer similarity per question
Average answer similarity per TLC

Abuse questions

31.0%
24.1%

10%

Title

42.7%
33.1%

Descr

40%

% of questions with a description
Average (median) description length in words
% of questions with description of 100 words or more

Answers

50%

Average (median) title length in words
Average (median) title length in characters

Best ans

Atheists: You know you are going to hell right?
Is treating Elizabeth Windsor a waste of medical resources?
What sort of veggie side dish should I have with the deer I shot?
Did you know the definition of Kobe Bryant is Selfish Loser?
Does my Ramadan fast break if I throw my dad down the stairs?
Do cars have another purpose besides killing people?
Do you think Hitler would have accepted Obama into the Nazi Party?
Why do you watch wrestling when you know it is gay?
Is it true that evolutionists have negative IQs?
How to beat my child without leaving a bruise? 10 pts for best answer.

Votes

Table 3: Characteristics of troll, abuse, and clean questions.

Asker

Table 2: Example troll questions.

Saturday Sunday

Figure 2: Distribution of troll, abuse, and clean questions by
time-of-the-day (left) and day-of-the-week (right).
Figure 1 shows the distribution of troll questions and abuse questions by the difference between their creation time and report time.
Generally, reports occur rather fast, with over 50% of the questions
reported within an hour of their creation time. It can be seen that
very fast reports (within 10 minutes) are less frequent for trolling,
perhaps indicating a less obvious type of abuse.
Figure 2 depicts the distribution of questions by time-of-day and
day-of-week of their posting (adjusted to the asker’s timezone). It
can be observed that compared to clean questions, troll questions are
slightly more common during morning hours (6am to noon) and
on Fridays, whereas abuse questions are more common on night
hours (9pm to 4am) and the weekend.
Table 3 presents various properties of troll questions, as compared
to abuse questions and clean questions, which relate to the question’s
title, description, answers, best answer, votes, and asker. It can be
seen that the title of troll questions is longer than clean questions
and slightly longer than abuse questions, both in terms of words and
characters. In addition, the portion of troll questions with a description is similar to clean questions and slightly lower compared to
abuse questions. The length of the description, when exists, is higher
for troll questions than for clean questions by average, but lower
by median. This could be explained by a particularly high portion
of the troll question descriptions that include 100 words or more.
Previous work found that trolls tend to write shorter posts [45],

1 IDF

838

values were calculated based on all answers in the complete dataset.

Session 7B: Content & Semantics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 4: Top-level categories with highest (left) and lowest
(right) percentages of troll questions (‘%troll’), with the portion of questions of each TLC out of the total number of
questions in the complete dataset (‘%’).

selected is higher for troll than for abuse questions. When selected,
best answers are particularly short for troll questions.
The number of votes per question (across all its answers) – both
upvotes and downvotes – is substantially higher for abuse questions
in comparison to clean questions. This is demonstrated both in the
higher average and median and in higher portions of questions with
1 vote or more and 5 votes or more. For troll questions, the numbers
of upvotes and downvotes are even higher than for abuse questions,
partially due to the higher number of answers per question and
partially due to higher number of votes per answer (in the case of
upvotes). This gives another indication of the higher engagement
troll questions trigger [35]. The high number of upvotes may imply
support for answerers who confront the troll.
The last set of characteristics presented in Table 3 relates to the
question’s asker. The YA tenure, defined as the time span between
the date the user signed up and the date s/he posted the question,
is lower for troll questions than both abuse and, in particular, clean
questions. A related but different aspect, the point snapshot represents the number of points the asker has accumulated on YA at
the time of question posting. YA has an elaborate point-and-level
system that rewards for providing answers, especially best answers,
and penalizes for asking questions [2, 27]. It can be seen that askers
of troll questions tend to have lower number of points than askers
of abuse and clean questions.

4.2

TLC

Most Trolled TLCs
%

Society & Culture
Sports
Social Science
Food & Drink
Politics & Government
Pregnancy & Parenting
Pets
Travel
News & Events
Yahoo Products
Dining Out
Environment
Family & Relationships

7.52%
3.76%
1.84%
1.29%
4.55%
3.38%
2.26%
1.98%
0.51%
1.93%
0.19%
0.15%
11.15%

Least Trolled TLCs
%

%troll

TLC

0.44%
0.39%
0.38%
0.36%
0.34%
0.31%
0.28%
0.27%
0.22%
0.19%
0.19%
0.18%
0.18%

Consumer Electronics
Computers & Internet
Games & Recreation
Local Businesses
Education & Reference
Business & Finance
Home & Garden
Science & Mathematics
Cars & Transportation
Health
Entertainment & Music
Arts & Humanities
Beauty & Style

3.52%
6.19%
3.36%
0.28%
5.22%
3.05%
0.96%
5.14%
2.01%
8.04%
13.48%
3.23%
5.02%

%troll
0.02%
0.03%
0.03%
0.05%
0.05%
0.05%
0.07%
0.08%
0.10%
0.11%
0.12%
0.15%
0.15%

Table 5: 15 leaf-level categories with highest (left) and lowest (right) percentages of troll questions (‘%troll’), with the
portion of questions of each LLC out of the total number of
questions in the complete dataset (‘%’).

LLC

Most Trolled LLCs

Royalty
Vegetarian & Vegan
Ramadan
Gender & Women’s Studies
Magazines
Parenting
Aircraft
Other - Cultures & Groups
Basketball
Adolescent
Senior Citizens
Grade-Schooler
Wrestling
Yahoo Answers
Football (American)

Categories

One of the unique characteristics of YA is topic diversity, reflected
by its broad set of categories. Table 4 presents all 26 TLCs according
to the percentage of troll questions out of all questions in each. The
TLCs with highest portions demonstrate the high-level topics most
prone to trolling, among them are society, sports, food, politics,
parenting, and social science. On the other hand, TLCs of more
technical topics, such as electronics, computers, and video games,
have the lowest portions of question trolling. Harper et al. observed
two primary intents on CQA sites – informational and conversational – and showed that TLCs serve as a good means to distinguish
between the two [24]. Our findings indicate that the conversational
TLCs tend to have higher portions of trolling, while the TLCs with
lower rates of question trolling are informational in nature [21].
This makes sense as trolls are typically in pursue for discussion.
We also note that the most trolled TLCs are quite different than the
most abused TLCs; for instance, the TLC with the highest portion
of abuse questions is Travel, which is only 8th in trolling.
To gain a finer-grained understanding of the most trolled domains, we inspected the LLCs. We only considered LLCs with a
total of 100,000 questions or more. As can be seen in Table 5, these
reveal a more specific set of topics more prone to trolling. Royalty
(under the TLC Societly & Culture), which mostly focuses on the
British royal family, tops the list, with Vegetarian & Vegan (under
Food & Drink) at close second. The 2nd and 3rd examples in Table 2 demonstrate troll questions from these two LLCs, respectively.
While these two categories are especially prone to trolling (over 3%
of the questions), their general abuse level is not among the highest.
Also on the list of most trolled LLCs are specific types of sports
(basketball, wrestling, football), populations (adolescents, seniors,
grade schoolers), and Yahoo Answers itself. The least trolled topics

Least Trolled LLCs

%

%troll

LLC

0.05%
0.11%
0.24%
0.40%
0.06%
0.26%
0.05%
0.68%
0.42%
0.41%
0.06%
0.05%
0.50%
0.47%
0.40%

3.23%
3.01%
1.37%
1.37%
1.32%
0.92%
0.92%
0.91%
0.88%
0.81%
0.77%
0.71%
0.71%
0.65%
0.64%

Internet
Careers & Employment
Television
Music
Service - Yahoo Mail
Cell Phones & Plans
Service- Yahoo Messenger
PDAs & Handhelds
MySpace
Other - Yahoo Mail
External Mail (POP)
Fantasy Sports
Mathematics
Other - Yahoo Messenger
Standards & Testing

%

%troll

0.18%
0.09%
0.11%
0.28%
0.12%
1.14%
0.04%
0.18%
0.20%
0.12%
0.06%
0.19%
1.85%
0.11%
0.19%

0.000%
0.000%
0.000%
0.001%
0.008%
0.008%
0.008%
0.009%
0.009%
0.009%
0.010%
0.010%
0.010%
0.012%
0.012%

include internet and cellphones, music and television, alongside
career and mathematics. While some types of sports are commonly
trolled, fantasy sport rarely is. Also, while Yahoo Answers itself is
a trolled topic, other Yahoo products, such as Mail and Messenger,
have very low portions of troll questions.

4.3

Linguistic Analysis

In this section, we explore the linguistic properties of troll questions, as reflected in their title, description, and answers. We first
examine the use of question words, then perform a broader analysis of distinctive terms, and finally inspect parts of speech and
punctuations.
Table 6 depicts the use of question words. It can be seen that
titles of troll questions open much more frequently with ‘why’ in
comparison to clean questions. On the other hand, ‘what’ and ‘how’
questions are less frequent on troll questions, as well as the generally less common ‘where’ and ‘which’. Indeed, ‘why’ questions
are considered more open ended [26] and likely reflect the troll’s
need to prompt a discussion, while ‘how’ and ‘where’ often reflect
a more focused informational need [24]. In total, lower portions of
the troll titles are phrased as wh-questions (open with one of the
wh-words), while higher portions are phrased as yes/no questions

839

Session 7B: Content & Semantics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 6: Percentage of question titles starting with different wh-words and percentage of titles, descriptions, and answers phrased as wh (‘WH’) and yes/no (‘Y/N’) questions.

Table 7: Most distinctive terms for troll vs. clean (‘Troll’) and
for clean vs. troll (‘Clean’) titles, descriptions, and answers.
Unigrams

Why How What Who When Where Which WH
Troll
Abuse
Clean

11.64 7.55 6.96
7.55 6.65 7.24
4.45 10.58 13.05

1.43
1.78
1.47

0.92
0.78
0.96

0.85
1.71
1.92

0.65
1.03
1.22

Answers

Y/N

WH

Y/N

WH

Y/N

29.85 22.17
26.74 18.34
33.65 18.32

4.28
3.85
2.52

2.25
1.93
1.39

4.1
3.95
5.1

3.32
3.01
3.72

Troll
why
girls
women
people
atheists
so
white
black
gay
wife
men
think

his
thanks
he
any
she
help
her
please
they
can
why
anyone
you
know
him
want
their
need
women wondering
who
thank
girls
download

Answers

(start with ‘is’, ‘do’, ‘can’, etc. [55]). Yes/no questions may more often reflect a provocation intent by asking a rhetorical question (see
various examples in Table 2). For descriptions, both wh- and yes/no
questions are somewhat more common on troll questions (although
generally much less frequent than on titles), perhaps reflecting another attempt to tease or inflame. For answers, on the other hand,
both types of questions are less common on troll questions, possibly
indicating there is a less frequent need for a follow-up [8].
To gain insights beyond question words, we set out to examine which terms characterize troll questions in comparison to clean
questions and vice versa. To this end, we used Kullback-Leibler (KL)
divergence, which is a non-symmetric distance measure between
two given distributions [4]. Specifically, we calculated the terms
that contribute the most to the KL divergence between the language
model of the title, description, and answers of troll questions and the
language model of title, description, and answers of clean questions,
respectively.2 Table 7 lists the terms with the highest KL divergence.
Inspecting the titles, the most distinctive troll unigrams include
the question word ‘why’ at the top, followed by various population
segments (typically in a plural form), from ‘girls’ to ‘atheists’ and
from the general ‘people’ to ‘white’. Further down (beyond Table 7),
the list includes more population groups, such as ‘ladies’, ‘americans’, ‘jews’, ‘conservatives’, and ‘vegans’. The troll unigram list
also includes ‘gay’, which reflects a population segment, but is also
used as a pejorative in certain communities, and the word ‘think’,
which is prominently associated with conversational questions [21].
The most distinctive clean question unigrams include the question
words ‘what’, ‘how’, and ‘where’ and words that reflect help seeking, such as ‘can’, ‘anyone’, ‘help’, and ‘need’. Further down the list
(beyond Table 7) are many technological words, such as ‘laptop’,
‘phone’, ‘windows’, ‘installed’, ‘settings’, and ‘equation’.
At the top of the troll bigram list are ‘why do’ and ‘why are’, followed by several individual and population types, such as ‘my wife’
and ‘meat eaters’. Also on the list are ‘yahoo answers’ and ‘breaking
news’. The troll trigram list includes, among others, phrases that
aim to arouse discussion, such as ‘is it true’, ‘did you know’, and
‘10 pts answer’. The first two are examples for yes/no questions,
while the latter reflects an attempt to solicit answers using the bestanswer reward [2]. The clean bigrams and trigrams reflect help
requests and various informational or advice needs, such as ‘how
do’, ‘does anyone know’, ‘the name of’, and ‘is the best’.
Inspecting descriptions, the troll terms (unigrams, bigrams, and
trigrams) commonly reflect third-person language, sometimes describing a situation, such as ‘on her hip’ or ‘his feelings changed’.

Clean

Title

Description

Description

Title

troll
question
trolling
why
trolls
stupid
gay
you
questions
lol
reported
funny

Bigrams
Troll

Trigrams
Clean

what
why do
what is
how
why are
how to
can
is it
what are
help
my wife
can i
song
black people
is the
anyone
why is
how do
where
meat eaters
anyone know
good
yahoo answers
where can
download
would you
does anyone
ipod
not funny
i need
need
sex with
help with
name
breaking news
a good

can
good
luck
try
more
best
use
get
one
some
time
like

he was
her hip
she was
out of
year old
on her
a spanking
a girl
block it
on me
the balls
hand on
a troll
are you
this question
do you
a life
you are
why do
grow up
you’re a
would you
why are
don’t you

Troll

i want
on her hip
i need
in the balls
thank you
hand on her
i can
lost a bet
anyone know
block that move
does anyone his feelings changed
please help
afraid of falling
is there
her hand on
was wondering
of falling in
in advance
her right hand
any ideas
camp in january
wondering if
ex dated for
you can
if you
good luck
it is
you will
the best
to get
have to
a lot
i have
i would
help you

Clean

is it true
what is the
did you know
where can i
why do people
does anyone know
on yahoo answers
what are some
any girls answer
how do i
why do you
what are the
10 pts answer
how do you
is it wrong
the name of
do i ask
can i find
a nerd ever
what is a
in the balls
need help with
do you think
is the best

get a life
you are a
people like you
why do you
why are you
is a joke
you’re a troll
the 2 points
better to do
why don’t you
are you serious
what the hell

i want to
i have a
does anyone know
i was wondering
i need to
thanks in advance
what is the
would like to
was wondering if
where i can
please help me
would be great
help you re
site might help
do no longer
hope this helps
if you have
this site might
a lot of
might help you
be able to
you can get
you want to
make sure you

In addition, similarly to titles, troll distinctive terms include ‘why’
and population segments such as ‘women’ or ‘girls’. Also similarly
to titles, the clean question terms reflect help requests (such as i
want/need/have), with common use of courtesy words and gratitude expression such as ‘please’, ‘thanks’, ‘i was wondering’, or
‘would be great’.
Inspecting the lists beyond Table 7, the troll terms for both title
and description include adult words (‘sex’, ‘panties’, ‘naked’), profanity (‘in the balls’, ‘a spanking’, ‘butt’, and ‘penis’), and a variety
of verbs that express violence (‘slap’, ‘hit’, ‘kick’, ‘beat’, ‘rape’, and
‘kill’). These findings disagree with a previous study reporting that
trolls do not obviously use offensive language [45].
The language of the answers also poses unique characteristics.
The troll unigram list reflects common use of the ‘troll’ stem (yet,
as explained in Section 3, not a strong enough indicator of trolling
on its own). In addition, it includes ‘question’ and ‘questions’ that
indicate explicit reference to the trolled content; ‘you’, reflecting a
second person language towards the asker; the question word ‘why’,
which, as previously mentioned, is also a strong characteristic of
troll questions; negative adjectives, such as ‘stupid’, and further
down the list ‘sick’, ‘ignorant’, ‘idiot’, ‘racist’, ‘disgusting’, ‘dumb’,
and ‘pathetic’; and ‘lol’ and ‘funny’, which may reflect either a real
enjoyment or taunting the asker. The troll bigrams and trigrams
often reflect second person language and demonstrate common
responses to trolls, including criticizing, challenging, and mocking [23], e.g., ‘get a life’, ‘grow up’, and ‘are you serious’. Further

2 Comparing

the language model of troll questions to abuse questions yielded a rather
similar term list, which is not presented due to space limitations.

840

Session 7B: Content & Semantics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 8: Part-of-speech distribution (percentage of all tokens) across titles, descriptions, and answers.
Answers

Abuse

Clean

Troll

Abuse

Clean

Troll

Abuse

Clean

Common Nouns
Proper Nouns
Verbs
Adjectives
Adverbs
Pronouns
Determiners
Prepositions

18.7
5.8
17.8
6.0
5.9
9.9
6.2
7.3

18.5
8.1
15.3
5.6
5.0
8.2
5.3
6.5

20.9
5.4
16.3
5.6
5.0
8.6
6.2
7.4

16.1
3.0
18.3
5.4
6.8
12.8
6.4
8.4

17.3
4.7
17.0
5.8
6.4
10.5
6.4
8.1

16.5
2.5
18.1
5.4
7.1
11.5
6.7
8.3

16.4
3.4
17.6
6.2
7.2
10.7
7.2
8.3

16.5
4.1
17.1
6.4
7.0
10.2
7.0
8.0

17.3
3.5
16.8
6.3
6.8
9.8
7.2
8.6

Punctuations

12.0

14.0

13.4

10.5

11.4

10.4

12.2

12.5

11.7

Q

down beyond Table 7, the list includes many other such expressions,
e.g., ‘you must be’, ‘should be ashamed’, ‘tell your parents’, ‘I dare
you’, ‘you’re an idiot’, ‘is a joke’, ‘shame on you’, ‘go away troll’,
‘you need help’, ‘call the police’ ‘i feel sorry’, ‘you make me’, ‘what
is wrong’, ‘are you kidding’, and ‘made me laugh’. On the other
hand, the clean answer terms include actionable verbs used for
advice such as ‘try’, ‘use’, or ‘get’ [54]. They also reflect courtesy
towards the asker, e.g., ‘good luck’ or ‘hope this helps’.
To generalize from lexical terms to syntax, we set out to examine
part-of-speech (POS) tags. To this end, we used the NLTK [7] POS
tagger over the text of the title, description, and answers of troll,
abuse, and clean questions. Table 8 presents the distribution of the
main parts of speech. Overall, the emerged differences are not
immense. For titles – verbs, adverbs, and especially pronouns – are
more frequent on troll questions, at the expense of nouns. These
trends can also be observed for the answers, and to a lesser extent
for the descriptions, and emerge both in comparison to abuse and
to clean questions. Previous work reported that troll posts include
more prepositions [45], however we did not find evidence for this
in our data.
Punctuation marks are less common on troll titles, but slightly
more common on answers to troll questions than answers to clean
questions. More specifically, excessive punctuations, such as multiple consecutive exclamation marks, question marks, dots, and
capital letters are more common on troll text (especially titles). The
occurrence of ‘@’, which is often used for conversation, is also
more common on troll titles, descriptions, and answers. On the
other hand, the use of parentheses, quotes, and for answers also
commas and colons, is less frequent on troll questions.

4.4

A

%A Up(%) Down(%) LLC TLC %Best Up/A Down/A AbQ(%) TrQ(%)

All YA

Description

Troll

Troll
Abuse
Clean

11 9 40.8
23 48 59.4
16 22 49.5

27.8
41.1
34.3

27.1
40.7
34.2

5
10
14

4
6
8

6.7
7.6
10.0

0.59
0.68
0.35

0.71
0.60
0.29

63.2
66.4
28.5

28.3
18.0
3.4

LLC

Title

Table 9: Asker’s activity prior to question posting across the
entire YA site (‘All YA’) and within the LLC of the posted
question (‘LLC’). Statistics are calculated by median, aside
from those ending with ‘(%)’, which reflect the portion of
users with nonzero values.

Troll
Abuse
Clean

3
5
2

21.1
34.5
20.4

20.3
33.7
19.4

–
–
–

–
–
–

4.1
5.4
0

0.81
0.91
0

1.0
0.75
0

44.8
51.2
11.7

20.4
12.5
1.6

0
3
0

0
33.3
0

about 40%, compared to nearly 50% for clean askers and 60% for
abuse askers. The trend persists when it comes to votes – both
upvotes (‘Up(%)’) and downvotes (‘Down(%)’), without substantial
differences between the two – where the portion of troll askers
who voted is lower than clean askers, which is in itself lower than
abuse askers. As shown in Table 3, troll askers have the shortest YA
tenure, which may explain their lower contribution. That said, abuse
askers also have a substantially shorter tenure, yet their number of
contributions is higher. Cheng et al. [12] found that antisocial users
post 10 times more than regular users; this finding coincides with
our finding for generally-abusive users rather than for trolls. The
trend somewhat changes when examining the specific LLC: troll
askers have posted more questions within the same LLC than clean
askers (median of 3 versus 2) and a similar proportion of both have
answered and voted.
A substantial difference between abuse askers and clean askers is
in the number of categories (both ‘LLC’ and ‘TLC’) in which they
made a contribution (either a question or an answer): abusers are
focused on a lower number of categories even though they have
made more contributions. Troll askers are focused on an even lower
number of categories than abuse askers.
Overall, we see that troll askers have made fewer contributions,
focused on a narrower set of categories, of which one is usually the
troll category. The differences are especially notable in comparison
to abuse askers, who tend to have a particularly high number of
contributions at the time of asking the abuse question. While the
results can be affected by suspensions from the site, we believe they
also reflect behavioral patterns and indicate that troll askers tend to
frequently switch accounts [12, 18, 47].
Inspecting community feedback on contribution, troll askers receive lower portions of best answers (‘%Best’), even lower than
abuse askers. On the other hand, like abuse askers, troll askers receive a substantially higher number of votes per answer, both up
(‘Up/A’) and down (‘Down/A’), both across YA and within the same
LLC. While abuse askers receive more upvotes, troll askers collect
more downvotes. Overall, we see that troll askers receive a somewhat more negative feedback on their past contributions, reflected
in both a lower ratio of best answers and a higher ratio between
downvotes and upvotes. As for abuse history, the portion of users
with previously abused questions (reported and deleted, ‘AbQ(%)’)
is considerably higher for abuse askers and troll askers than for
clean askers. The differences are even more substantial within the
same LLC. The portion of abuse askers with a prior abuse question

User Activity

In this section, we further focus on the question askers and their
activity on YA, both across the site and within the specific LLC of the
question, prior to posting it. Table 9 presents the statistics for troll,
abuse, and clean questions. The first 7 columns refer to different
contributions from the user, while the other 5 reflect feedback
received from the community for these contributions.
Across YA, it can be seen that askers of troll questions (henceforth
referred to as troll askers) have posted fewer questions (‘Q’) and
provided fewer answers (‘A’) at the time of asking the troll question.
In contrast, abuse askers have posted more questions and answers
at asking time than clean askers. The proportion of answers out
of all contributions is different for troll askers (‘%A’): a median of

841

Session 7B: Content & Semantics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

is only slightly higher than the portion of troll askers, even though
they have posted substantially more questions. As for prior troll
questions (‘TrQ(%)’), the portion of troll askers with such history
is considerably higher compared to abuse askers. Overall, as could
be expected, we see that question abusers have some tendency to
repeat the action under the same account (and same LLC), and similarly some trolls have already posted a troll question with the same
account and under the same LLC. This may also explain the more
negative feedback they have received for their prior contributions.

Table 10: Performance of different classifiers for the troll vs.
clean, troll vs. abuse, and deleted vs. retained question classification tasks.

5

731,956 questions, of which 57.4% were labeled positive (see Table 1). In all three settings, we used 10-fold cross validation to train
and evaluate the classifiers. As our main evaluation metric we use
accuracy; alongside we also report the F1 score for the troll question
(positive) class. Results are presented in Table 10.
For the troll versus clean classification, the best results were
achieved by the gradient tree boosting classifier (with random forest at close second), reaching an accuracy of 85.2%. In their work
on news forums, Mihaylov and Nakov [37] reported a somewhat
lower result for their classifier of comments from “mentioned trolls”
(see Section 2) versus comments from non-trolls, with an accuracy
of 81.1% at highest. For the troll versus abuse classification, the
performance is substantially lower, indicating the distinction is
more challenging. In this case, the random forest classier attained
the best performance, reaching an accuracy of 79.9%. Finally, in
the deleted versus retained setting, the accuracy is lower than for
the balanced troll vs. clean setting, at 81.3%, achieved using gradient boosting. One reason for the lower performance can be the
difficulty in distinguishing troll questions from clean yet reported
questions compared to clean questions that have not been reported.
To better understand the importance of different features, we
trained the gradient boosting classifier for the troll questions versus
clean questions task with different feature subsets. Table 11 presents
the accuracy results. The first section examines the use of the different textual fields – title, description, and answers – when using
the combination of all feature types (metadata, n-grams, W2V, and
POSgrams). Using the title alone yielded a fairly high performance,
while the description was somewhat less productive. Interestingly,
answers yielded the highest performance of all three fields and even
higher than the combination of title and description. This indicates
that the community’s response to the troll questions is more helpful
for the classification task than the question itself. In other words,
the expression of outrage by the community is more troll-predictive
than the attempts to arouse it. One might think that this outcome
is the result of the answers’ metadata being particularly rich, as
they include the number of answers, best answer, and votes. Yet,
even when removing metadata features and inspecting text only
(n-grams, W2V, and POSgrams), the performance based on answers
was higher than based on title together with description, indicating
that the answers’ text is more revealing of the trolling class than
the text of the question. Using all three fields together yielded a
further performance enhancement. One thing to note is that the answers’ metadata and text only become available some time (usually
between a few minutes and a few hours [1]) after the question’s
posting. Therefore, if the classifier’s goal is predicting a troll question at posting time, the relevant performance to consider is when

Troll – Clean

SVM with RBF kernel
Logistic regression
Random forest
Gradient tree boosting

TROLL QUESTION CLASSIFICATION

Building on the observations and insights from the previous section,
we set out to design a classifier that would distinguish between troll
and clean questions. Our feature families map to the different types
of analysis presented in Section 4: (i) Title: the text and metadata
of the question’s title; (ii) Description: the text and metadata of the
question’s description; (iii) Answers: the text and metadata of the
question’s answers; (iv) User: the metadata of the asker and his/her
past contributions and respective community feedback across YA
and within the same LLC; (v) Category: the TLC and LLC of the
question; and (vi) Time: the time-of-day and day-of-week of the
question’s posting time. Within these feature families, metadata
features correspond to those presented in Table 3. For Answers,
they also include the best answer and the votes. Past contribution
and community feedback features were defined based on those
listed in Table 9. For textual features of the title, description, and
answers, we experimented with the following: (i) n-grams: number
of term occurrences in the text for n ∈ {1, 2, 3} (unigrams, bigrams,
and trigrams). We considered n-grams appearing at least k times in
the training set. We set k =5 to optimize average accuracy via 10fold cross validation over the training data following experiments
with k ∈ {1, 3, 5, 7, 10}; (ii) W2V: word embedding representation
of the text using Word2Vec [38]. Each term was represented as
a vector of 300 dimensions. We used the complete dataset (over
200M questions) to learn the W2V vectors based on the text of the
titles, descriptions, and answers, respectively for each of the three
fields; and (iii) POSgrams: number of occurrences of part-of-speech
unigrams, bigrams, and trigrams in the text. POSgrams with less
than k =5 occurrences in the training set were discarded.
We experimented with the following classifiers: logistic regression, SVM with RBF kernel, gradient tree boosting, and random
forest. We used the scikit-learn library [43] to run machine learning
algorithms and the NLTK suite [7] for natural language processing.
Like most previous work, we used a balanced dataset [12, 37, 45],
with troll questions as the positive examples and clean questions as
the negative examples. To also get a sense of how distinguishable
troll questions are from abuse questions, we examined a setting in
which the two serve for positive and negative examples, respectively. In both of these settings, the overall dataset included 839,850
questions. Additionally, to examine a more realistic setting, which
reflects a practical scenario, we used troll questions as positive examples and troll-reported questions that were not deleted by the
moderator as negative examples. This setting, referred to as deleted
versus retained, reflects an automation of the moderator role, where
questions reported for trolling by YA users are either validated as
troll questions or deemed clean. In this case, the dataset included

842

Troll – Abuse

Deleted – Retained

Accuracy

F1

Accuracy

F1

Accuracy

F1

77.8
83.5
84.6

78.9
83.8
84.1

66.1
76.0

65.3
75.5

72.4
79.9
80.2

71.9
78.9
80.6

85.2

85.6

81.3

81.0

79.9

78.3

79.6

78.1

Session 7B: Content & Semantics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 11: Accuracy when using (‘Only’) or removing (‘Exclude’) subsets of features while training the gradient boosting classifier for the troll vs. clean task.
Features

Only

Exclude

Title
Description
Answers
Title & Description
Answers Text: n -grams + W2V + P O S grams
Title & Description Text: n -grams + W2V + P O S grams
TDA = Title + Description + Answers

79.8
76.4
81.3
80.5
80.6
80.1
84.6

84.2
84.6
82.3
83.0
83.4
83.8
76.3

TDA: metadata
TDA: n -grams
TDA: W2V
TDA: POS grams
TDA: metadata + n -grams
TDA: metadata + W2V
TDA: n -grams + W2V
TDA: metadata + n -grams + W2V

75.6
80.6
81.5
71.4
81.7
82.7
83.1
84.7

84.6
83.4
83.1
85.2
82.6
82.6
77.8
76.7

User
Category
Time

74.1
64.6
53.7

84.7
85.0
85.2

All = TDA + User + Category + Time

85.2

−

least common. In a finer-grained inspection, two categories emerge
as having a particularly high portion of question trolling: royals
and vegetarians/vegans. In a study of trolling on a feminist forum,
Herring et al. state that non-mainstream online communities are
especially vulnerable to trolling, as they must balance inclusive
ideals against the need for protection and safety, a tension that
can be exploited by disruptive elements to generate intra-group
conflict [25].
Our analysis reveals unique characteristics of troll questions
across categories. Despite their shorter life span, they attract more
answers and more votes on these answers. Cheng et al. found that
trolls receive more replies than average users, suggesting that they
might be successful in luring others into fruitless, time-consuming
discussions [12]. While the troll questions tend to be longer, their
respective answers tend to be shorter. Answers to troll questions
also pose higher content similarity, both for the same question and
across questions. At the user level, askers of troll questions tend
to have a shorter tenure, fewer contributions across the site, lower
answering portion out of all contributions, and fewer categories
of activity. Their prior contributions also receive more negative
feedback: higher downvote portions, lower best answer rates, and
higher likelihood of being troll-reported.
In their recent study, Samory and Peserico [45] claimed that
“trolls (unlike most other abusers) hardly stand out in a conversation e.g. in terms of vocabulary.” Our analysis suggests, by contrast,
that troll questions pose linguistic differences compared to clean
questions. The use of both yes/no and ‘why’ questions is more common on troll questions, likely reflecting overly naive or rhetorical
questions [16, 22]. The language of troll questions (title and description) also includes more adult and profanity words, violent verbs,
pronouns, and references to population groups, and fewer expressions of help seeking and courtesy. Overall, troll questions share
more characteristics with conversational rather than informational
questions [21, 24]. The text of the answers to troll questions also
exhibits unique characteristics, including direct addressing of the
question or the asker; using a variety of negative adjectives, often
expressing irritation towards the asker; sparser use of actionable
advice verbs; and, similarly to the questions, frequent references to
populations and sporadic courtesy expressions.
A classifier built upon the above findings attained an accuracy
of up to 85.2% over a balanced dataset of troll and clean questions.
This result is higher than reported in a recent study of troll post
detection on a news forum, also using a balanced dataset [37]. In
our case, textual features demonstrated the most predictive power,
with the text of the answers being even more useful than that of the
question itself. Using word embedding led to some performance
gain. Future research may examine the use of neural networks
to capture deeper semantics and latent factors that can further
improve classification performance.
Our method for identifying ground-truth troll questions combines user reports and moderator actions. This supports high precision at large scale, but might miss on recall, as certain types
of troll questions might be “under-reported”, for example, due to
cultural differences among communities. Future work should examine complementary means for troll identification. Other directions
for future research include studying trolling at the answer level;
conducting a longitudinal study of the lifecycle of trolls on CQA

excluding Answers features, at 82.3% accuracy. Analogously, for
the deleted versus retained setting, the accuracy when excluding
Answers features, reflecting a moderator’s decision at question
posting time, is 79.1%.
The second section of Table 11 focuses on the performance of
the classifier when using all three textual fields together, using
different feature subsets. It can be seen that the lexical features (ngrams) yielded good performance. Using the W2V features yielded
a slightly higher performance than the n-grams. Combining both
led to a further performance enhancement. Using metadata features
only, without considering the text at all, yielded a fair performance,
albeit substantially lower than the text itself. The POSgram features
yielded a rather low result and furthermore their removal had no
impact on the overall performance. As reported in Section 4, the
differences in POS distribution for troll and clean questions were
rather small. Adding metadata features to either the n-gram or W2V
features (or both) yielded some performance gain.
Inspecting feature subsets beyond the textual fields (third section
of Table 11) indicates that user features yielded only moderate performance on their own, yet they did show some contribution to the
overall performance. Time features yielded very low performance
and their removal had no effect, while category features were also
not very productive.

6

CONCLUSIONS

While most of the studies on trolling have focused on forums or
news comment threads, with many attempting to detect trolls at
the user level, our work focuses on the question level, relying on a
large set of over 400,000 troll questions. These reflect an attempt to
inflame the community by asking a question, without the context of
a specific article, post, or comment. Working with Yahoo Answers,
a highly diverse CQA website, allowed us to examine question
trolling across a wide variety of topics. Our analysis shows that
categories such as society, sports, politics, and food are the most
prone to trolling, while on technological categories, such as electronics, computers, cellphones, and video games, trolling is the

843

Session 7B: Content & Semantics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

websites; and studying the reporters, who serve as the community’s
“gate keepers” from trolling.

[28] Ben Kirman, Conor Lineham, and Shaun Lawson. 2012. Exploring Mischief and
Mayhem in Social Computing or: How We Learned to Stop Worrying and Love
the Trolls. In Proc. of CHI EA. 121–130.
[29] April Kontostathis, Kelly Reynolds, Andy Garron, and Lynne Edwards. 2013.
Detecting Cyberbullying: Query Terms and Techniques. In Proc. of WebSci. 195–
204.
[30] Srijan Kumar. 2017. Characterization and Detection of Malicious Behavior on the
Web. Ph.D. Dissertation. University of Maryland, College Park.
[31] Jérôme Kunegis, Andreas Lommatzsch, and Christian Bauckhage. 2009. The
Slashdot Zoo: Mining a Social Network with Negative Edges. In Proc. of WWW.
741–750.
[32] So-Hyun Lee and Hee-Woong Kim. 2015. Why people post benevolent and
malicious comments online. Commun. ACM 58, 11 (2015), 74–79.
[33] Baichuan Li, Tan Jin, Michael R. Lyu, Irwin King, and Barley Mak. 2012. Analyzing
and Predicting Question Quality in Community Question Answering Services.
In Proc. of WWW Companion. 775–782.
[34] Tai Ching Li, Joobin Gharibshah, Evangelos E Papalexakis, and Michalis Faloutsos.
2017. TrollSpot: Detecting misbehavior in commenting platforms. In Proc. of
ASONAM.
[35] Lena Mamykina, Bella Manoim, Manas Mittal, George Hripcsak, and Björn Hartmann. 2011. Design Lessons from the Fastest Q&A Site in the West. In Proc. of
CHI. 2857–2866.
[36] Todor Mihaylov, Georgi Georgiev, and Preslav Nakov. 2015. Finding Opinion
Manipulation Trolls in News Community Forums. In Proc. of CoNLL. 310–314.
[37] Todor Mihaylov and Preslav Nakov. 2016. Hunting for troll comments in news
community forums. In Proc. of ACL. 399–405.
[38] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality. In
Proc. of NIPS. 3111–3119.
[39] S. Nadali, M. A. A. Murad, N. M. Sharef, A. Mustapha, and S. Shojaee. 2013. A
review of cyberbullying detection: An overview. In Proc. of ISDA. 325–330.
[40] Prankit Namdeo, RK Pateriya, and Sonika Shrivastava. 2017. A Review of Cyber
bullying Detection in Social Networking. In Proc. of ICICCT. 162–170.
[41] Chikashi Nobata, Joel Tetreault, Achint Thomas, Yashar Mehdad, and Yi Chang.
2016. Abusive Language Detection in Online User Content. In Proc. of WWW.
145–153.
[42] F. Javier Ortega, José A. Troyano, Fermin L. Cruz, Carlos G. Vallejo, and Fernando
Enriquez. 2012. Propagation of trust and distrust for the detection of trolls in a
social network. Computer Networks 56, 12 (2012), 2884–2895.
[43] Fabian Pedregosa et al. 2011. Scikit-learn: Machine Learning in Python. J. Mach.
Learn. Res. 12 (Nov. 2011), 2825–2830.
[44] David J Phillips. 1996. Defending the boundaries: Identifying and countering
threats in a Usenet newsgroup. The information society 12, 1 (1996), 39–62.
[45] Mattia Samory and Enoch Peserico. 2017. Sizing Up the Troll: A Quantitative
Characterization of Moderator-Identified Trolling in an Online Forum. In Proc. of
CHI. 6943–6947.
[46] Chun Wei Seah, Hai Leong Chieu, Kian Ming A Chai, Loo-Nin Teow, and Lee Wei
Yeong. 2015. Troll detection by domain-adapting sentiment analysis. In Proc. of
Fusion. 792–799.
[47] Pnina Shachaf and Noriko Hara. 2010. Beyond Vandalism: Wikipedia Trolls. J.
Inf. Sci. 36, 3 (2010), 357–370.
[48] Stefan Siersdorfer, Sergiu Chelaru, Jose San Pedro, Ismail Sengor Altingovde, and
Wolfgang Nejdl. 2014. Analyzing and Mining Comments and Comment Ratings
on the Social Web. ACM Trans. Web 8, 3 (2014).
[49] A. Squicciarini, S. Rajtmajer, Y. Liu, and C. Griffin. 2015. Identification and
Characterization of Cyberbullying Dynamics in an Online Social Network. In
Proc. of ASONAM. 280–285.
[50] John R Suler and Wende L Phillips. 1998. The bad boys of cyberspace: Deviant
behavior in a multimedia chat community. CyberPsychology & Behavior 1, 3
(1998), 275–294.
[51] Scott Thacker and Mark D Griffiths. 2012. An exploratory study of trolling in
online video gaming. IJCBPL 2, 4 (2012), 17–33.
[52] George Tsatsaronis, Filippos Karolos Ventirozos, and Iraklis Varlamis. 2017. Detecting Aggressive Behavior in Discussion Threads Using Text Mining. In Proc.
of CICLing.
[53] Heidi Vandebosch and Katrien Van Cleemput. 2008. Defining cyberbullying:
A qualitative research into the perceptions of youngsters. CyberPsychology &
Behavior 11, 4 (2008), 499–503.
[54] Ingmar Weber, Antti Ukkonen, and Aris Gionis. 2012. Answers, Not Links:
Extracting Tips from Yahoo! Answers to Address How-to Web Queries. In Proc.
of WSDM. 613–622.
[55] Ryen W. White, Matthew Richardson, and Wen-tau Yih. 2015. Questions vs.
Queries in Informational Search Tasks. In Proc. of WWW Companion. 135–136.
[56] Zhaoming Wu, Charu C. Aggarwal, and Jimeng Sun. 2016. The Troll-Trust Model
for Ranking in Signed Networks. In Proc. of WSDM. 447–456.

REFERENCES
[1] Lada A. Adamic, Jun Zhang, Eytan Bakshy, and Mark S. Ackerman. 2008. Knowledge Sharing and Yahoo Answers: Everyone Knows Something. In Proc. of WWW.
665–674.
[2] Eugene Agichtein, Carlos Castillo, Debora Donato, Aristides Gionis, and Gilad
Mishne. 2008. Finding High-quality Content in Social Media. In Proc. of WSDM.
183–194.
[3] Antoaneta Baltadzhieva and Grzegorz Chrupala. 2015. Question Quality in
Community Question Answering Forums: A Survey. SIGKDD Explor. Newsl. 17, 1
(2015), 8–13.
[4] Adam Berger and John Lafferty. 1999. Information Retrieval As Statistical Translation. In Proc. of SIGIR. 222–229.
[5] Kelly Bergstrom. 2011. “Don’t feed the troll”: Shutting down debate about
community expectations on Reddit.com. First Monday 16, 8 (2011).
[6] Amy Binns. 2012. DON’T FEED THE TROLLS! Managing troublemakers in
magazines’ online communities. Journalism Practice 6, 4 (2012), 547–562.
[7] Steven Bird. 2006. NLTK: The Natural Language Toolkit. In Proc. of COLING-ACL.
69–72.
[8] Pavel Braslavski, Denis Savenkov, Eugene Agichtein, and Alina Dubatovka. 2017.
What Do You Mean Exactly?: Analyzing Clarification Questions in CQA. In Proc.
of CHIIR. 345–348.
[9] Erin E Buckels, Paul D Trapnell, and Delroy L Paulhus. 2014. Trolls just want to
have fun. Personality and individual Differences 67 (2014), 97–102.
[10] Erik Cambria, Praphul Chandra, Avinash Sharma, and Amir Hussain. 2010. Do
not feel the trolls. Proc. of SDoW Workshop.
[11] Justin Cheng, Michael Bernstein, Cristian Danescu-Niculescu-Mizil, and Jure
Leskovec. 2017. Anyone can become a troll: Causes of trolling behavior in online
discussions. arXiv preprint abs/1702.01119 (2017).
[12] Justin Cheng, Cristian Danescu-Niculescu-Mizil, and Jure Leskovec. 2015. Antisocial Behavior in Online Discussion Communities. In Proc. of ICWSM. 61–70.
[13] Bryn Alexander Coles and Melanie West. 2016. Trolling the Trolls: Online Forum
Users Constructions of the Nature and Properties of Trolling. Comput. Hum.
Behav. 60, C (2016), 233–244.
[14] Sally Jo Cunningham and Annika Hinze. 2014. Social, religious information
behavior: An analysis of Yahoo! Answers queries about belief. Advances in the
Study of Information and Religion 4, 1 (2014).
[15] Imen Ouled Dlala, Dorra Attiaoui, Arnaud Martin, and Boutheina Ben Yaghlane.
2014. Trolls identification within an uncertain framework. In Proc. of ICTAI.
1011–1015.
[16] Judith S Donath. 1999. Identity and deception in the virtual community. Communities in cyberspace (1999), 29–59.
[17] Pnina Fichman and Madelyn Rose Sanfilippo. 2015. The bad boys and girls of
cyberspace: How gender and context impact perception of and reaction to trolling.
Social science computer review 33, 2 (2015), 163–180.
[18] Patxi Galán-García, José Gaviria de la Puerta, Carlos Laorden Gómez, Igor Santos,
and Pablo García Bringas. 2016. Supervised machine learning for the detection of
troll profiles in twitter social network: Application to a real case of cyberbullying.
Logic Journal of the IGPL 24, 1 (2016), 42–53.
[19] Rich Gazan. 2016. Seven Words You Can’t Say on Answerbag: Contested Terms
and Conflict in a Social Q&A Community. In Proc. of HT. 27–36.
[20] Maja Golf-Papez and Ekant Veer. 2017. Don’t feed the trolling: rethinking how
online trolling is being defined and combated. Journal of Marketing Management
(2017), 1–19.
[21] Ido Guy, Victor Makarenkov, Niva Hazon, Lior Rokach, and Bracha Shapira. 2018.
Identifying Informational vs. Conversational Questions on Community Question
Answering Archives. In Proc. of WSDM. 216–224.
[22] Claire Hardaker. 2010. Trolling in asynchronous computer-mediated communication: From user discussions to academic definitions. Journal of Politeness
Research 6 (2010), 215–242.
[23] Claire Hardaker. 2015. ‘I refuse to respond to this obvious troll’: an overview of
responses to (perceived) trolling. Corpora 10, 2 (2015), 201–229.
[24] F. Maxwell Harper, Daniel Moy, and Joseph A. Konstan. 2009. Facts or Friends?:
Distinguishing Informational and Conversational Questions in Social Q&A Sites.
In Proc. of CHI. 759–768.
[25] Susan Herring, Kirk Job-Sluder, Rebecca Scheckler, and Sasha Barab. 2002. Searching for safety online: Managing “trolling” in a feminist forum. The Information
Society 18, 5 (2002), 371–384.
[26] Jaakko Hintikka and Ilpo Halonen. 1995. Semantics and pragmatics for whyquestions. The Journal of Philosophy 92, 12 (1995), 636–657.
[27] Imrul Kayes, Nicolas Kourtellis, Daniele Quercia, Adriana Iamnitchi, and
Francesco Bonchi. 2015. The Social World of Content Abusers in Community
Question Answering. In Proc. of WWW. 570–580.

844

