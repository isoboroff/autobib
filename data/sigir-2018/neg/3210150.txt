Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

New Embedded Representations and Evaluation Protocols
for Inferring Transitive Relations
Sandeep Subramanian

Soumen Chakrabarti

IIT Bombay

IIT Bombay

A common recipe for inferring general relations between entities
is to fit suitable vectors to each of them, and to train a network to
input query vectors and predict presence or absence of the probed
relationship. A key question has been whether types merit a special
representation, different from the generic devices that represent
KG relations, because of their special properties. Two types may be
disjoint, overlapping, or one may contain the other. Containment
is transitive.
Compared to the vast array of entity-relation representations
available [2, 10, 12, 16, 19], few proposals exist [5, 17, 18] for representing types to satisfy their specific requirements. Of these, only
order embedding (OE) by Vendrov et al. [17] directly enforces transitivity by modeling it as elementwise vector dominance.
We make three contributions. First, we present a significant improvement to the OE loss objective. Second, we generalize OE to
rectangle embeddings for types: types and entities are represented
by (hyper-)rectangles and points respectively. Ideally, type rectangles contain subtype rectangles and entity instance points. Rather
than invoke established neural gadgets as black boxes, we introduce constraints and loss functions in a transparent manner, suited
to the geometric constraints induced by the task at hand. Third,
we remove a limitation in the training and evaluation protocol of
Vendrov et al. [17], and propose a sound alternative. Experiments
using synsets from the WordNet noun hierarchy (same as Vendrov
et al. [17]) show the benefits of our new formulations. Our code
will be available1 .

ABSTRACT
Beyond word embeddings, continuous representations of knowledge graph (KG) components, such as entities, types and relations,
are widely used for entity mention disambiguation, relation inference and deep question answering. Great strides have been made
in modeling general, asymmetric or antisymmetric KG relations
using Gaussian, holographic, and complex embeddings. None of
these directly enforce transitivity inherent in the is-instance-of and
is-subtype-of relations. A recent proposal, called order embedding
(OE), demands that the vector representing a subtype elementwise
dominates the vector representing a supertype. However, the manner in which such constraints are asserted and evaluated have some
limitations. In this short research note, we make three contributions
specific to representing and inferring transitive relations. First, we
propose and justify a significant improvement to the OE loss objective. Second, we propose a new representation of types as hyperrectangular regions, that generalize and improve on OE. Third, we
show that some current protocols to evaluate transitive relation
inference can be misleading, and offer a sound alternative. Rather
than use black-box deep learning modules off-the-shelf, we develop
our training networks using elementary geometric considerations.
ACM Reference Format:
Sandeep Subramanian and Soumen Chakrabarti. 2018. New Embedded Representations and Evaluation Protocols for Inferring Transitive Relations. In
SIGIR ’18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8–12, 2018, Ann Arbor, MI, USA. ACM,
New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210150

1

2

RELATED WORK

Words and entities2 are usually embedded as points or rays from
the origin [7, 13, 21]. It is well appreciated that relations need
more sophisticated representation [2, 10, 12, 16, 19], but types
seem to have fallen by the wayside, in relative terms. Vilnis and
McCallum [18] pioneered a Gaussian density representation for
words, to model hypernymy via the asymmetric KL divergence as
an inference gadget. Items x, y are represented by Gaussian densities дx , дy (with suitable mean and covariance parameters). If
x ≺ y we want low KL(дx ∥дy ). Normalized densities with unit
mass seem inappropriate for types with diverse population sizes.
Athiwaratkun and Wilson [1] have used a thresholded divergence
dγ (x, y) = max{0, KL(дx ∥дy ) − γ }. However, modeling asymmetry
does not, in itself, enforce transitivity. Neither is anti-symmetry
modeled. Jameel and Schockaert [5] proposed using subspaces to
represent types. They do not address type hierarchies or transitive containment. Recently, Nickel and Kiela [11] introduced an
elegant hyperbolic geometry to represent types, but moving away
from Euclidean space can complicate the use of such embeddings in

INTRODUCTION

Contemporary information extraction from text, relation inference
in knowledge graphs (KGs), and question answering (QA) are informed by continuous representations of words, entities, types and
relations. Faced with the query “Name scientists who played the violin,” and having collected candidate response entities, a QA system
will generally want to verify if a candidate is a scientist. Testing if
e ∈ t or t 1 ⊆ t 2 , where e is an entity and t, t 1 , t 2 are types, is therefore a critical requirement. Unlike Albert Einstein, lesser-known
candidates may not be registered in knowledge graphs, and we may
need to assign a confidence score of belongingness to a target type.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210150

1 https://gitlab.com/soumen.chakrabarti/rectangle
2 Also

1037

see wiki2vec https://github.com/idio/wiki2vec

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

downstream applications, in conjunction with conventional word
embeddings. Vendrov et al. [17] proposed a simpler mechanism:
embed each type t to vector u t ∈ RD , and, if t 1 ⊆ t 2 , then require
u t1 ≥ u t2 , where ≥ is elementwise. I.e., u t1 must dominate u t2 . OE
was found better at modeling hypernymy than Gaussian embeddings. In OE, types are open cones with infinite volume, which
complicates representing various intersections.

4

Despite its novelty and elegance, OE has some conceptual limitations. A type t with embedding u t is the infinite axis-aligned
open convex cone {pp : p ≥ u t } with its apex at u t . Thus, types
cannot “turn off” dimensions, all pairs of types intersect (although
the intersection may be unpopulated), and all types have the same
infinite measure, irrespective of their training population sizes.
We propose to represent each type by a hyper-rectangle (hereafter,
just ‘rectangle’), a natural generalization of OE cones. A rectangle
is convex, bounded and can have collapsed dimensions (i.e., with
zero width). Obviously, rectangles can be positioned to be disjoint,
and their sizes can give some indication of the number of known
instances of corresponding types. Containment of one rectangle
in another is transitive by construction, just like OE. Entities remain represented as points (or infinitesimal rectangles for uniform
notation).
Each type or entity x is represented by a base vector u x , as well
D , so that in dimension d, the
as a nonnegative width vector v x ∈ R+
rectangle has extent [u x,d , u x,d + v x,d ]. Informally, the rectangle
representing x is bounded by “lower left corner”uu x and “upper right
corner” u x + v x . For entities, v x ≡ 0 . For types, v x are regularized
with a L2 penalty. The rectangles are allowed to float around freely,
so u x are not regularized.

3 σ OE: OE WITH IMPROVED LOSS
OBJECTIVE
In what follows, we use the partial order x ≺ y to unify e ∈ t and
t 1 ⊆ t 2 for notational simplicity. If x ≺ y, OE required u x ≥ u y . OE
defines ℓ(x, y) = ∥ max{00,uu y −uu x }∥22 , which is 0 iff u y ≤ u x . Given
labeled positive instances x ≺ y and negative instances x ⊀ y, the
overall loss is theÕ
sum of two parts:
Õ
L+ =
ℓ+ (x, y) =
ℓ(x, y)
(1)
x ≺y

L− =

Õ

x ≺y

ℓ− (x, y) =

x ⊀y

Õ

max{0, α − ℓ(x, y)},

(2)

x ⊀y

where α is a tuned additive margin. The intuition is that when
x ⊀ y, we want ℓ(x, y) ≥ α. There are two limitations to the above
loss definitions. First, ∥ · · · ∥22 is too sensitive to outliers. This is
readily remedied by redefining ℓ(x, y) using L1 norm, as
D
Õ
∥ max{00,uu y − u x }∥1 =
[uy,d − u x,d ]+ ,
(3)

If x ∈ y or x ⊆ y, the rectangle representing x must be contained in the rectangle representing y. Let the violation in the dth
dimension be
δ + (x, y; d) = max{[uy,d − u x,d ]+ ,
(8)

d=1

[u x,d + v x,d − uy,d − vy,d ]+ }
Then the loss expressionÕ
for positive instances is
L+ =
max [δ + (x, y; d)]+ .

where [•]+ = max{0, •} is the hinge/ReLU operator. But the semantics of ℓ− are wrong: we are needlessly encouraging all dimensions
to violate dominance, whereas violation in just one dimension would
have been enough.

x ≺y d ∈[1, D]

d ∈[1, D]

[u
+ vy,d − u x,d − v x,d ]+ }
Õy,d
and L− =
min [δ − (x, y; d)]+ .

so that the loss is zero if dominance fails in at least one dimension.
To balance this L ∞ form in case of positive instances, we redefine
ℓ+ (x, y) = max [uy,d − u x,d ]+ ,
(5)

x ⊀y

d ∈[1, D]

d ∈[1, D]

(11)

As in σ OE, we can add margin, stiffness, and nonlinearity to rectangles, and get
δ + (x, y; d) = max{[uy,d + ∆ − u x,d ]+ ,
(12)

so that the loss is zero only if dominance holds in all dimensions.
The unbounded hinge losses above mean a few outliers can
hijack the aggregate losses L+ and L− . Moreover, the absence
of a SVM-like geometric margin (as distinct from the loss margin
α above) also complicates separating ≺ and ⊀ cases confidently.
Our final design introduces a nonlinearity (sigmoid function) to
normalize per-instance losses, additive margin ∆ and a standard
stiffness hyperparameter
 ψ.


d ∈[1, D]

(9)

This ensures that the loss is proportional to the largest violating
margin and that the loss is zero if the rectangle of x is contained in
the rectangle of y. Analogously, we define
δ − (x, y; d) = min{[u x,d − uy,d ]+ ,
(10)

Specifically, for x ⊀ y, loss should be zero if u x,d < uy,d for any
d ∈ [1, D]. Accordingly, we redefine
ℓ− (x, y) = min [u x,d − uy,d ]+ ,
(4)

ℓ+ (x, y) = σ ψ max [uy,d + ∆ − u x,d ]+ − 1/2
d ∈[1, D]


ℓ− (x, y) = σ ψ min [u x,d + ∆ − uy,d ]+ − 1/2.

RECTANGLE EMBEDDINGS

[u x,d + v x,d + ∆ − uy,d − vy,d ]+ }
δ − (x, y; d) = min{[u x,d + ∆ − uy,d ]+ ,

(13)

[uy,d + vy,d + ∆ − u x,d − v x,d ]+ }

Õ
Õ 
L+ =
ℓ+ (x, y) =
σ ψ max δ + (x, y; d)

(14)

x ≺y

(6)

L− =

Õ
x ⊀y

(7)

5

(Obviously the ‘−1/2’ terms are immaterial for optimization, but
bring the loss expression to zero when there are no constraint
violations.)

x ≺y

ℓ− (x, y) =

Õ
x ⊀y

d ∈[1, D]



σ ψ min δ − (x, y; d) .
d ∈[1, D]

(15)

TRAINING AND EVALUATION PROTOCOLS

Because the training and evaluation instances are tuple samples
from a single (partially observed) partial order, great care is needed

1038

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

in designing the training, development and testing folds. To use unambiguous short subscripts, we call them learn, dev and eval folds,
each with positive and negative instances L + , L − , D + , D − , E + , E − .
Let T be the raw set of tuples (x ∈ t or t 1 ⊆ t 2 ). The transitive
closure (TC) of T , denoted clo(T ), includes all tuples implied by T
via transitivity.

5.1

5.2

Clearly, evaluation results must be reported separately for instances
that cannot be trivially inferred via TC, where the algorithm needs
discover a suitable geometry from the combinatorial structure of
clo(T ) beyond mere reachability. To this end, we propose the following sanitized protocol.
(1) Sample positive learn fold L + ⊂ clo(T ).
(2) Negative learn fold L − of size |L + | is generated by repeating
as needed:
(a) Sample (u, v) ∈ L + uniformly.
(b) Perturb one of u or v to get (u ′, v ′ ).
(c) If (u ′, v ′ ) ∈ clo(T ), discard.
(3) Sample positive dev fold D + ⊂ (clo(T ) \ clo(L + )).
(4) Discard (u, v) from D + if u or v not found in L + ∪ L − (explained below).
(5) Sample positive eval fold E + ⊂ (clo(T ) \ (clo(L + ) ∪ clo(D + ))).
(6) Discard elements from E + using the same protocol used to
discard elements from D + .
(7) Generate negative dev and eval folds, D − and E − , using the
same protocol used to generate L − from L + .
An entity or type never encountered in the learn fold cannot be
embedded meaningfully (unless corpus information is harnessed,
see Section 7), so it is pointless to include in dev or eval folds
instances that contain such entities or types. Such sampled instances
are discarded. To fill folds up to desired sizes, we repeatedly sample
pairs until we can retain enough instances.

OE protocol

Vendrov et al. [17] followed this protocol:
(1) Compute clo(T ).
(2) Sample positive eval fold E + ⊂ clo(T ).
(3) Sample positive learn fold L + ⊂ clo(T )\E + .
(4) Sample positive dev fold D + ⊂ clo(T ) \ (E + ∪ L + ).
(5) Generate negative eval, learning and dev folds, E − , L − & D −
(see below).
(6) Return L + , L − , D + , D − , E + , E − .
A negative tuple is generated by taking a positive tuple (x, y) and
perturbing either of them randomly to (x, y ′ ) or (x ′, y), where x ′, y ′
are sampled uniformly at random. In OE negative folds were the
same size as positive folds.
The WordNet [8] hypernymy data set used by Vendrov et al. [17]
has |T | = 82115 and | clo(T )| = 838073. E + and D + , sampled from
clo(T ), had only 4000 tuples each. All remaining tuples were in the
learn fold. Vendrov et al. [17] freely admit that “the majority of test
set edges can be inferred simply by applying transitivity, giving
[them] a strong baseline.” They reported that the TC baseline gave
a 0/1 accuracy of 88.2%, Gaussian embeddings [18] was at 86.6%,
and OE at 90.6%.

6

Test fold F1

0.8
0.6
0.4
Order embedding (OE)
Transitive closure

0.0
0

200000

400000
|Train fold|

EXPERIMENTS

Data set: We prepare our data set similar to Vendrov et al. [17].
WordNet [8] gives 82115 (hypernym, hyponym) pairs which we
use as directed edges to construct our KG. The WordNet noun hierarchy is prepared by experts, and is also at the heart of other type
systems [9, 15] used in KG completion and information extraction.
We augment the KG by computing its transitive closure, which
increases the edge count to 838073. Then we use the two protocols
in Section 5 to create training, dev and test folds. The sanitized
protocol produces 679241 positive and 679241 negative training
instances, 4393 positive and 4393 negative dev instances, and 4316
positive and 4316 negative test instances. These sizes are close to
those of Vendrov et al. [17].

1.0

0.2

Sanitized OE protocol

Code and hyperparameter details: OE and our enhancements,
σ OE and rectangle embeddings, were coded in Tensorflow with
Adam’s optimizer. Hyperparameters, such as batch size (500), initial
learning rate (0.1), margin ∆ and stiffness ψ , were tuned using the
dev fold. Optimization was stopped if the loss on the dev fold did
not improve more than 0.1% for 20 consecutive epochs. All types
and entities were embedded to D = 50 dimensions.

600000

Figure 1: A large fraction of test instances can be inferred by
simply computing the transitive closure of the training fold
in the OE protocol.

Results: Vendrov et al. [17] reported only microaveraged 0/1
accuracy (‘Acc’). Here we also report average precision (AP), recall
(R), precision (P) and F1 score, thus covering both ranking and
set-retrieval objectives. AP and R-P curves are obtained by ordering
test instances by the raw score given to them by OE, σ OE, and
rectangle embeddings. Table 1 compares the three systems after
using the two sampling protocols to generate folds.

Instead of 0/1 accuracy, Figure 1 shows the more robust F1 score
on test instances achieved by transitive closure and OE, as the size
of training data is varied. Vendrov et al. [17] reported accuracy near
the right end of the scale, where OE has little to offer beyond TC.
In fact, OE does show significant lift beyond TC when training data
is scarce. As we shall see, even with ample training data, σ OE and
rectangle embeddings improve on OE.

1039

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

OE protocol
Sanitized OE protocol
OE
σ OE
Rect
OE
σ OE
Rect
Acc 0.922
0.921
0.926
0.574
0.742
0.767
AP
1
1
1
0.977
0.969
0.986
P
0.994
0.915
0.973
0.987
0.925
0.983
R
0.850
0.929
0.877
0.151
0.527
0.544
F1
0.916
0.922
0.923
0.262
0.671
0.700
Table 1: Performance of OE, σ OE, and rectangle embeddings
under the OE protocol and the sanitized protocol, on the
WordNet hypernymy relation.

REFERENCES
[1] Ben Athiwaratkun and Andrew Gordon Wilson. 2018. On Modeling Hierarchical
Data via Probabilistic Order Embeddings. In ICLR. https://openreview.net/foru
m?id=HJCXZQbAZ
[2] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational
data. In NIPS Conference. 2787–2795. http://papers.nips.cc/paper/5071-translati
ng-embeddings-for-modeling-multi-relational-data.pdf
[3] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning
to Rank: From Pairwise Approach to Listwise Approach. In ICML. 129–136.
http://www.machinelearning.org/proceedings/icml2007/papers/139.pdf
[4] Haw-Shiuan Chang, ZiYun Wang, Luke Vilnis, and Andrew McCallum. 2017. Unsupervised Hypernym Detection by Distributional Inclusion Vector Embedding.
arXiv preprint arXiv:1710.00880 (2017). https://arxiv.org/pdf /1710.00880.pdf
[5] Shoaib Jameel and Steven Schockaert. 2016. Entity embeddings with conceptual
subspaces as a basis for plausible reasoning. arXiv preprint arXiv:1602.05765
(2016).
[6] Xiao Ling and Daniel S Weld. 2012. Fine-Grained Entity Recognition.. In AAAI.
http://xiaoling.github.io/pubs/ling-aaai12.pdf
[7] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality. In
NIPS Conference. 3111–3119. https://goo.gl/x3DTzS
[8] G Miller, R Beckwith, C FellBaum, D Gross, K Miller, and R Tengi. 1993. Five
papers on WordNet. Princeton University. ftp://ftp.cogsci.princeton.edu/pu
b/wordnet/5papers.pdf
[9] Shikhar Murty, Patrick Verga, Luke Vilnis, and Andrew McCallum. 2017. Finer
Grained Entity Typing with TypeNet. arXiv preprint arXiv:1711.05795 (2017).
https://arxiv.org/pdf /1711.05795.pdf
[10] Maximillian Nickel. 2013. Tensor factorization for relational learning. Ph.D.
Dissertation. Ludwig–Maximilians–Universität, München. https://edoc.ub.un
i-muenchen.de/16056/1/NickelM aximilian.pdf
Poincaré embeddings for
[11] Maximillian Nickel and Douwe Kiela. 2017.
learning hierarchical representations. In NIPS Conference. 6341–6350.
http://papers.nips.cc/paper/7213-poincare-embeddings-for-learning-hie
rarchical-representations.pdf
[12] Maximilian Nickel, Lorenzo Rosasco, Tomaso A Poggio, et al. 2016. Holographic
Embeddings of Knowledge Graphs. In AAAI Conference. 1955–1961. https:
//arxiv.org/abs/1510.04935
[13] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. GloVe:
Global Vectors for Word Representation.. In EMNLP Conference, Vol. 14. 1532–
1543. http://www.emnlp2014.org/papers/pdf /EMNLP2014162.pdf
[14] Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and Sebastian Riedel. 2016. An
Attentive Neural Architecture for Fine-grained Entity Type Classification. arXiv
preprint arXiv:1604.05525 (2016). https://arxiv.org/pdf /1604.05525.pdf
[15] Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. YAGO: A Core
of Semantic Knowledge Unifying WordNet and Wikipedia. In WWW Conference.
ACM Press, 697–706. http://www2007.org/papers/paper391.pdf
[16] Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume
Bouchard. 2016. Complex embeddings for simple link prediction. In ICML. 2071–
2080. http://arxiv.org/abs/1606.06357
[17] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. 2015. Orderembeddings of images and language. arXiv preprint arXiv:1511.06361 (2015).
https://arxiv.org/pdf /1511.06361
[18] Luke Vilnis and Andrew McCallum. 2014. Word representations via gaussian
embedding. arXiv preprint arXiv:1412.6623 (2014).
[19] Qizhe Xie, Xuezhe Ma, Zihang Dai, and Eduard Hovy. 2017. An Interpretable
Knowledge Transfer Model for Knowledge Base Completion. arXiv preprint
arXiv:1704.05908 (2017). https://arxiv.org/pdf /1704.05908.pdf
[20] Yadollah Yaghoobzadeh and Hinrich Schütze. 2015. Corpus-level Fine-grained
Entity Typing Using Contextual Information. In EMNLP Conference. 715–725.
http://aclweb.org/anthology/D15-1083
[21] Ikuya Yamada, Hiroyuki Shindo, Hideaki Takeda, and Yoshiyasu Takefuji. 2017.
Learning Distributed Representations of Texts and Entities from Knowledge Base.
arXiv preprint arXiv:1705.02494 (2017). https://github.com/studio-ousia/ntee
[22] Josuke Yamane, Tomoya Takatani, Hitoshi Yamada, Makoto Miwa, and Yutaka
Sasaki. 2016. Distributional Hypernym Generation by Jointly Learning Clusters
and Projections. In COLING. 1871–1879. http://www.aclweb.org/antholog
y/C16-1176

It is immediately visible that absolute performance numbers are
very high under the original OE protocol, for reasons made clear
earlier. As soon as the OE protocol is replaced by the sanitized protocol, no system is given any credit for computing transitive closure.
The 0/1 accuracy of OE drops from 0.922 to 0.574. F1 score drops
even more drastically from 0.916 to 0.262. In contrast, σ OE and rectangle embeddings fare better overall, with rectangle embeddings
improving beyond σ OE.
1.0

Precision

0.9
0.8
0.7
OE
sigmaOE
Rect

0.6
0.5
0.0

0.2

0.4
0.6
Recall

0.8

1.0

Figure 2: Recall-precision profiles on WordNet.
Whereas σ OE and rectangle embeddings improve on OE at the
task of set retrieval, their ranking abilities are slightly different.
Figure 2 shows that σ OE is inferior at ranking to both OE and rectangle embeddings. Rectangle embeddings have the best precision
profile at low recall. Modifying our code to use ranking-oriented
loss functions [3] may address ranking applications better.

7

CONCLUDING REMARKS

Here we have addressed the problem of completing e ∈ t and
t 1 ⊆ t 2 relations starting from an incomplete KG, but without
corpus support. For out-of-vocabulary (not seen during training)
entities, mention contexts in a corpus are vital typing clues [6, 14,
20]. We plan to integrate context (word) embeddings with order
and rectangle embeddings. It would be of interest to see how our
refined loss objectives and testing protocols compare with other
corpus-based methods [4, 22].
Acknowledgment: Thanks to Aditya Kusupati and Anand Dhoot
for helpful discussions, and nVidia for a GPU grant.

1040

