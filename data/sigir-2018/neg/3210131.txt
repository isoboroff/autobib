Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Are we on the Right Track? An Examination of Information
Retrieval Methodologies
Enrique Amigó

National Distance
Education University
Spain
enrique@lsi.uned.es

Hui Fang

Stefano Mizzaro

University of Delaware
USA
hfang@udel.edu

University of Udine
Italy
mizzaro@uniud.it

new knowledge in IR is at least partly because it is not easy to
import scientific methodologies from other areas such as physics
or human sciences into IR; unlike in other engineering areas, the
unpredictability of human behavior makes it difficult to find general
laws that describe precise phenomena. (Note that in this paper
we focus on effectiveness rather than efficiency or scalability in
which the user is not involved.) Regarding social and psychological
researches, the need for effectiveness in systems makes futile the
production of general principles.
This situation makes us wonder whether the current practices
in IR research are on the “right" track toward discovery of new
knowledge about IR. The more general question here is: What are
the best methodologies (if any) that researchers should follow to
optimally advance the knowledge in IR research? To address this
question, in this paper, we first try to quantify the current methodological trends in IR research and categorize existing IR methodologies along two dimensions: (1) empirical vs. theoretical, and (2)
top-down vs. bottom-up. We then identify six desirable properties
and anaylze these four types of methodologies accordingly. The
analysis indicates that none of the methodologies can satisfy all the
desirable properties but they are complementary to each other. For
example, theoretical methodologies give theoretical foundations,
interpretability, and robustness across new scenarios, while empirical methodologies provide evidence about the relative effectiveness
of approaches in particular realistic scenarios, as well as providing
statistical significance when comparing systems to each other.
Furthermore, we categorized the 167 full papers published in
the SIGIR 2016 and 2017 as well as ICTIR 2017 conferences into
the proposed four categories, i.e., empirical bottom-up, empirical
top-down, theoretical bottom-up and theoretical bottom-down, and
found that, to certain extent, empirical bottom-up methods are the
most dominating methodology in the IR field, indicating a strong
bias toward empirical rather than theoretical work.
Motivated by the analysis results, we propose a general methodology for IR research that aims to leverage the strengths of both
theoretical and empirical methodologies.

ABSTRACT
The unpredictability of user behavior and the need for effectiveness make it difficult to define a suitable research methodology
for Information Retrieval (IR). In order to tackle this challenge,
we categorize existing IR methodologies along two dimensions:
(1) empirical vs. theoretical, and (2) top-down vs. bottom-up. The
strengths and drawbacks of the resulting categories are characterized according to 6 desirable aspects. The analysis suggests that
different methodologies are complementary and therefore, equally
necessary. The categorization of the 167 full papers published in
the last SIGIR (2016 and 2017) and ICTIR (2017) conferences suggest
that most of existing work is empirical bottom-up, suggesting lack
of some desirable aspects. With the hope of improving IR research
practice, we propose a general methodology for IR that integrates
the strengths of existing research methods.

CCS CONCEPTS
• Information systems → Information retrieval;

KEYWORDS
Axiomatics, Research methodologies
ACM Reference Format:
Enrique Amigó, Hui Fang, Stefano Mizzaro, and ChengXiang Zhai. 2018. Are
we on the Right Track? An Examination of Information Retrieval Methodologies. In SIGIR ’18: The 41st International ACM SIGIR Conference on Research
& Development in Information Retrieval, July 8–12, 2018, Ann Arbor, MI, USA.
ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210131

1

ChengXiang Zhai

University of Illinois at
Urbana-Champaign
USA
czhai@illinois.edu

INTRODUCTION

Most of Information Retrieval (IR) contributions follow a standard
structure: analysis of the state of the art, description of the approach,
and empirical evaluation over a certain data set. The large amount
of available annotated collections allows to improve models by trial
and error. However, this methodology does not match with the
standard scientific procedures: hypothesis statement, definition of
an experiment guided by the specific hypothesis, and result analysis.
As a consequence, the IR community tends to produce solutions
to a greater extent than knowledge. The slow progress in creating

2

CATEGORIZING IR METHODOLOGIES

IR methodologies can be categorized along two somewhat different dimensions. First, they can be categorized as either theoretical
or empirical approaches. Theoretical approaches are those derived
based on formal theory, while empirical approaches are those driven
by the data sets. This distinction corresponds with the why and how
categorization proposed by Fuhr [17]. Second, we can also distinguish bottom-up from top-down approaches. Bottom-up approaches
are supported by preexisting IR models and test cases sampled from
real scenarios, while top-down approaches are supported by general

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210131

997

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

An influencing example is the connection between pointwise
Mutual Information and neural language models found by several
authors Arora et al. [3], Levy and Goldberg [21], Melamud et al.
[26]. In addition, this methodological approach can simplify the
axiomatic analysis of existing approaches. For instance, Amigo et.
al. Amigó et al. [1] generalized into four models clustering extrinsic evaluation metrics. Every metric in each category shares the
same limitations and strengths in terms of formal constraints. But
note that not all generalizations are equally useful: the usefulness
depends on how many variables or assumptions are necessary to
particularize existing methods.

2.3
Figure 1: Categorizing methodologies

The third and most common methodology is the empirical validation of systems or hypothesis over a test cases sampled from
a scenario. These are the empirical bottom-up methodologies. In
principle, the most realistic procedure consists in employing users
in laboratory conditions. However, extensive studies in interactive
tracks of evaluation campaigns (e.g., iClef) have shown several limitations: human subjects are expensive and only a limited amount
of systems can be compared. In addition, previous work suggests
that users tend to adapt to applications, being difficult to infer differences between IR approaches. An alternative is the massive log
analysis. The disadvantage is that conclusions can be biased by the
employed systems. For instance, it has been proved that recommendation data sets are highly biased by the existing recommendation
services [4]. The third and popular alternative is building human
annotated data sets, that can be extensively reused for new systems
(e.g., TREC, CLEF, DUC, etc).

axioms and synthetic interpretability oriented test cases. The two
dimensions give us four combinations which we can then use to
categorize the IR methodologies in detail as shown in Figure 1.

2.1

Theoretical Top-Down (or Axiomatic)

The first category is theoretical top-down approach, which can also
be referred to as axiomatic approach. This kind of methodology
often starts with a set of axioms or formal constraints that an ideal
solution needs to satisfy and then uses the axioms to guide for the
search of the optimal solution. In fact, axiomatic analysis has been
shown to be effective for diagnosing deficiencies of basic retrieval
models and improving them [6, 14, 15], including particularly the
development of BM25+, an improvement of a long-standing state
of the art model (i.e., BM25) to fix its deficiency in document length
normalization [23]. This methodology is also quite more common
in evaluation metric proposals, given that meta-evaluation data sets
for empirical studies (biased by the evaluated approaches as well as
input data) are more difficult to generate than information access
data sets. The formal analysis of evaluation metrics recurs in the
literature since the 70s, but it has received an increasing interest in
the last five or ten years, when most of the papers have been published [1, 2, 7, 16, 24, 27, 31]. Moreover, the formalization attempts
of evaluation metrics have gone well beyond the IR field: several
results concern related areas like classification [31] or clustering
[1, 25], or even textual similarity, opinion mining, and so on.
We would like to point out that axioms (i.e., formal constraints)
and assumptions are different. Formal constraints are theoretical
properties that need to hold?. On the other hand, assumptions are
propositions that allow to derive a certain model, but its effectiveness is necessarily checked with empirical experiments. For
instance, generative models such as LDA [5] are derived from statistical assumptions, which are empirically validated.

2.2

Empirical Bottom-Up

2.4

Empirical Top-Down

The fourth category is the empirical top-down methodology, consisting on defining artificially particular cases in which systems
should behave in a certain manner. In the context of IR models,
it is quite common to develop synthetic data to compare systems
to each other. This methodology also appears in the context of
evaluation metric development. For instance, Golbus et al. [18] reported to what extent IR metrics are sensitive to document rankings
containing relevant documents but different grades of diversity.

3

ANALYZING IR METHODOLOGIES

What would an ideal IR methodology look like? We identify the
following six desirable properties.
• Universality: The methodology can be applied to different scenarios.
• Interpretability. The methodology and its results can be explained
to understand the nature of the problem.
• Discriminativeness. The methodology and its results allow to
conclude preferences between approaches.
• Derivability. The methodology can be used to derive new approaches.
• Quantitativity. The methodology can generate results for quantifying the performance.
• Representativity. The quantitative results reflect the actual statistical distribution of cases in a real scenario, allowing the use of
statistical significance tests.

Theoretical Bottom-Up (or Generalization)

The second category is theoretical bottom-up (or generalization)
approaches. This refers to the research studies that unify models
that have been already validated empirically or theoretically. The
result is a general method that can be particularized by means of
parameters or statistical assumptions. These include, for instance,
the definition of a general IR approach based on language models [37], or the generalization of IR metrics into a parameterizable
user behavior model [9].

998

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: Categorization of full papers in recent conferences.
Conference

Theo. T-D

Theo. B-U

Emp. T-D

Emp. B-U

SIGIR 2016
SIGIR 2017
ICTIR 2017

2 (≃3%)
2 (≃3%)
3 (≃10%)

5 (≃8%)
4 (≃5%)
5 (≃20%)

3 (≃5%)
1 (≃1%)
3 (≃10%)

62 (≃98%)
77 (≃100%)
26 (≃96%)

Let us analyze the previously discussed four categories based
on these desirable properties. The one that characterize theoretical methodologies is universality. Axioms and generalizations are
satisfied regardless the particular scenario in study, while empirical studies are scenario-dependent. This prevents data overfitting
which is common in empirical methodologies [20], and the bias of
using system pooling in the relevance annotation process [32].
Both theoretical methodologies also provide interpretability: Axioms explain the nature of approaches. Connecting models at formal
level (generalization) gives explanations of empirical improvements
previously reported. However, axiomatics (top-down) reports preference criteria between models (discriminativeness), while generalizations (bottom-up) gives information about formal connections
but not about the effectiveness of IR models. The counter part is that
generalization offers derivability, while axioms state a theoretical
allowed space of solutions, but not their definition itself.
Regarding empirical methodologies, their main contribution is
quantitativity. In addition, just like axiomatics, they give information about the relative suitability of models. The strength of
synthetic data (empirical top-down) regarding sampled test cases
(empirical bottom-up) is interpretability, given that the artificial test
cases are specifically designed to capture particular characteristics
of models. On the other hand, sampled test cases (empirical bottomup) comply representativeness respecting the original distribution
of test cases. This allows the use of statistical significance tests.
Figure 1 illustrates the strengths and drawbacks of methodologies according to these six properties. None of the four methodologies includes every desirable aspects. This suggest that the particularities of IR research requires the accumulation of evidence from
different methodologies.

4

Figure 2: Proposed general methodology
present a theoretical analysis of the ranking competition as a repeated game, and its minmax regret equilibrium. Finally there are
9 papers that present some kind of generalization or connection
between models [8, 10, 12, 28, 29, 33, 34, 36, 40].
We also categorized the 27 full papers published in ICTIR 2017 a
conference focused on theoretically grounded works. Consequently,
the percentages of theoretical and top-down researches is quite
higher. However, in sum, 92% of papers in the SIGIR and 74% in the
ICTIR last conference are exclusively based on empirical bottom-up
methodologies. Our main conclusion is obviously that the empirical bottom-up methodology is practically unanimous while other
methodologies are still infrequent, producing a gap in the community in terms of universality, interpretability, and derivability.

5

PROPOSED IR METHODOLOGY

According to our analysis, we propose the general methodology
described in Figure 2 consisting of:
(1) Define formal constraints or axioms to be satisfied by models.
(2) Generalize existing models into families.
(3) Study the formal properties of models and look for a correspondence between families and formal properties.
(4) Improve or derive new models according to the formal analysis.
(5) Define cases or synthetic data sets according to the formal
analysis.
(6) Evaluate the models empirically over standard data sets and
artificial data. Study the correspondence between empirical
results and formal properties.
(7) Revise the model to optimize the performance according to the
empirical results.
We did not find in the previous literature any single work covering all these stages, but some works are close. For instance, Fang
et al. [14], Fang and Zhai [15] defined formal constraints for IR functions. They analyzed and tuned several popular retrieval functions.
They confirmed the effectiveness of satisfying formal constraints
with empirical evaluation over both standard benchmarks and synthetic data designed according to the theoretical analysis. These
papers have been cited more than 180 times according to Google
Scholar. This suggests that, although these methodologies are not
commonly used, they are appreciated by the community. In the context of evaluation, Amigó et al. [1] defined four formal constraints

QUANTIFYING METHODOLOGY
DISTRIBUTION IN SIGIR

We make a photograph of the current trends in IR methodologies.
First we categorize the 140 full papers published in the proceedings
of the last two SIGIR conferences. Table 1 shows the results. Note
that some works cover several methodologies. All papers include
empirical bottom-up experiments. The exception is one paper by
Sakai [30] about the use of statistical significance in IR studies. Regarding empirical top-down methodologies, we found three papers
that use synthetic data sets [35, 38, 39], and the contribution by
Goodwin et al. [19] which explores the effectiveness of his approach
over specific manually defined cases. Regarding the axiomatic or
theoretical top-down approaches, we found only 4 papers. The
work by Zhang and Zhai [39] proposes a novel formulation of the
Interface Card model based on sequential decision theory; the paper by Dinçer et al. [13] considers formal properties of evaluation
metrics; Chierichetti et al. [11] evaluate formally the capability of
a generative model to capture the Zipf law; and Raifer et al. [29]

999

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

for extrinsic clustering metrics. Then they generalized existing metrics into four families and they studied their formal properties. In
addition to models, the constraint set itself is also a generalization
of other formal constraint sets from previous work. They observed
that different families satisfied different constraints. This work also
included the definition of synthetic data for checking the reliability
of formal constraints according to humans, and finally an empirical
evaluation over standard benchmark confirmed the positive effect
of satisfying formal constraints. This paper has been cited more
than 500 times according to Google Scholar. Another example is
the Lin’s work [22] which states an axiomatic based definition of
similarity which is particularized into similarity between ordinal
values, string similarity and word similarity. This work has been
cited 4538 times according to Google Scholar.
In our work (this short paper), we also tried to define axiomatically desirable properties, generalize research methodologies and
add some bottom-up empirical information from conference paper
statistics. We did not cover the empirical top-down methodology.

6

[12] Clebson C.A. de Sá, Marcos A. Gonçalves, Daniel X. Sousa, and Thiago Salles.
2016. Generalized BROOF-L2R: A General Framework for Learning to Rank
Based on Boosting and Random Forests. In Proc. of the 39th ACM SIGIR. 95–104.
[13] B. Taner Dinçer, Craig Macdonald, and Iadh Ounis. 2016. Risk-Sensitive Evaluation and Learning to Rank Using Multiple Baselines. In Proc. of the 39th ACM
SIGIR. 483–492.
[14] Hui Fang, Tao Tao, and Chengxiang Zhai. 2011. Diagnostic Evaluation of Information Retrieval Models. ACM Trans. Inf. Syst. 29, 2, Article 7 (April 2011),
7:1–7:42 pages.
[15] Hui Fang and ChengXiang Zhai. 2005. An exploration of axiomatic approaches
to information retrieval. In SIGIR ’05. 480–487.
[16] César Ferri, José Hernández-Orallo, and R. Modroiu. 2009. An experimental
comparison of performance measures for classification. Pattern Recognition
Letters 30, 1 (2009), 27–38.
[17] Norbert Fuhr. 2012. Salton Award Lecture Information Retrieval As Engineering
Science. ACM SIGIR Forum 46, 2 (Dec. 2012), 19–28.
[18] Peter B. Golbus, Javed A. Aslam, and Charles L. A. Clarke. 2013. Increasing
evaluation sensitivity to diversity. Information Retrieval J. 16, 4 (2013), 530–555.
[19] Bob Goodwin, Michael Hopcroft, Dan Luu, Alex Clemmer, Mihaela Curmei,
Sameh Elnikety, and Yuxiong He. 2017. BitFunnel: Revisiting Signatures for
Search. In Proc. of the 40th ACM SIGIR. 605–614.
[20] Markus Junker and Andreas Dengel. 2001. Preventing Overfitting in Learning
Text Patterns for Document Categorization. In Advances in Pattern Recognition —
ICAPR 2001. Springer Berlin Heidelberg, Berlin, Heidelberg, 137–146.
[21] Omer Levy and Yoav Goldberg. 2014. Neural Word Embedding As Implicit Matrix
Factorization. In Proc. of the 27th International Conference on Neural Information
Processing Systems. MIT Press, Cambridge, MA, USA, 2177–2185.
[22] Dekang Lin. 1998. An Information-Theoretic Definition of Similarity. In Proc. of
the Fifteenth International Conference on Machine Learning (ICML ’98). Morgan
Kaufmann Publishers Inc., San Francisco, CA, USA, 296–304.
[23] Yuanhua Lv and ChengXiang Zhai. 2011. Lower-bounding Term Frequency
Normalization. In Proc. of the 20th ACM CIKM. 7–16.
[24] Eddy Maddalena and Stefano Mizzaro. 2014. Axiometrics: Axioms of Information
Retrieval Effectiveness Metrics. In Proc. of the Sixth EVIA Workshop. National
Institute of Informatics, Tokyo, Japan, 17–24. ISBN: 978-4-86049-066-9.
[25] Marina Meila. 2003. Comparing clusterings. In Proc. of COLT 03.
[26] Oren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning
Generic Context Embedding with Bidirectional LSTM.. In CoNLL, Yoav Goldberg
and Stefan Riezler (Eds.). ACL, 51–61.
[27] Alistair Moffat. 2013. Seven Numeric Properties of Effectiveness Metrics. In
AIRS’13. 1–12.
[28] Fedor Nikolaev, Alexander Kotov, and Nikita Zhiltsov. 2016. Parameterized
Fielded Term Dependence Models for Ad-hoc Entity Retrieval from Knowledge
Graph. In Proc. of the 39th ACM SIGIR. 435–444.
[29] Nimrod Raifer, Fiana Raiber, Moshe Tennenholtz, and Oren Kurland. 2017. Information Retrieval Meets Game Theory: The Ranking Competition Between
Documents? Authors. In Proc. of the 40th ACM SIGIR. 465–474.
[30] Tetsuya Sakai. 2016. Statistical Significance, Power, and Sample Sizes: A Systematic Review of SIGIR and TOIS, 2006-2015. In Proc. of the 39th ACM SIGIR.
5–14.
[31] Fabrizio Sebastiani. 2015. An Axiomatically Derived Measure for the Evaluation
of Classification Algorithms. In Proc. of ICTIR 2015. 11–20.
[32] Alberto Tonon, Gianluca Demartini, and Philippe Cudré-Mauroux. 2015. Poolingbased continuous evaluation of information retrieval systems. Information Retrieval Journal 18, 5 (01 Oct 2015), 445–472.
[33] Jun Wang, Lantao Yu, Weinan Zhang, Yu Gong, Yinghui Xu, Benyou Wang, Peng
Zhang, and Dell Zhang. 2017. IRGAN: A Minimax Game for Unifying Generative
and Discriminative Information Retrieval Models. In Proc. of the 40th ACM SIGIR.
515–524.
[34] Xuanhui Wang, Michael Bendersky, Donald Metzler, and Marc Najork. 2016.
Learning to Rank with Selection Bias in Personal Search. In Proc. of the 39th ACM
SIGIR. 115–124.
[35] Qingyun Wu, Huazheng Wang, Quanquan Gu, and Hongning Wang. 2016. Contextual Bandits in a Collaborative Environment. In Proc. of the 39th ACM SIGIR.
529–538.
[36] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2016. Modeling
Document Novelty with Neural Tensor Network for Search Result Diversification.
In Proc. of the 39th ACM SIGIR. 395–404.
[37] ChengXiang Zhai. 2008. Statistical Language Models for Information Retrieval A
Critical Review. Found. Trends Inf. Retr. 2, 3 (March 2008), 137–213.
[38] Dell Zhang, Jun Wang, Emine Yilmaz, Xiaoling Wang, and Yuxin Zhou. 2016.
Bayesian Performance Comparison of Text Classifiers. In Proc. of the 39th ACM
SIGIR. 15–24.
[39] Yinan Zhang and Chengxiang Zhai. 2016. A Sequential Decision Formulation
of the Interface Card Model for Interactive IR. In Proc. of the 39th ACM SIGIR.
85–94.
[40] Zhiwei Zhang, Qifan Wang, Luo Si, and Jianfeng Gao. 2016. Learning for Efficient
Supervised Query Expansion via Two-stage Feature Selection. In Proc. of the 39th
ACM SIGIR. 265–274.

CONCLUSIONS

The analysis presented in this paper suggests that, unlike in other
areas, the particularities of IR researches lead to the need of accumulating evidence from complementary methodologies. The statistics
over recent conferences suggest that this is not the current trend.
Our analysis also shows the suitability of combining methodologies
in single works. Additionally, the proposed general methodology
for IR researches provides a tentative way of checking the completeness of contributions in reviewing processes. Our work is a
preliminary exploration of IR research methodologies. We hope it
will stimulate more work on this important topic so as to accelerate
discovery of new knowledge in IR.
Acknowledgments. This research was partially supported by the
Spanish Government (project Vemodalen TIN2015-71785-R).

REFERENCES
[1] Enrique Amigó, Julio Gonzalo, Javier Artiles, and Felisa Verdejo. 2009. A comparison of extrinsic clustering evaluation metrics based on formal constraints.
Information Retrieval 12, 4 (2009), 461–486.
[2] Enrique Amigó, Julio Gonzalo, and Felisa Verdejo. 2013. A General Evaluation
Measure for Document Organization Tasks. In Proc. of the 36th ACM SIGIR. New
York, NY, USA, 643–652.
[3] Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. 2016.
A Latent Variable Model Approach to PMI-based Word Embeddings. TACL 4
(2016), 385–399.
[4] Alejandro Bellogín, Pablo Castells, and Iván Cantador. 2017. Statistical biases in
Information Retrieval metrics for recommender systems. Information Retrieval
Journal 20, 6 (2017), 606–634.
[5] David M. Blei, Andrew Y. Ng, Michael I. Jordan, and John Lafferty. 2003. Latent
dirichlet allocation. Journal of Machine Learning Research 3 (2003).
[6] P. D. Bruza and T. W. C. Huibers. 1994. Investigating aboutness axioms using
information fields. In Proc. of the 17th ACM SIGIR. Springer-Verlag New York,
Inc., New York, NY, USA, 112–121.
[7] Luca Busin and Stefano Mizzaro. 2013. Axiometrics: An Axiomatic Approach to
Information Retrieval Effectiveness Metrics. In Proc. of ICTIR 2013. New York –
USA, 22–29.
[8] Rocío Cañamares and Pablo Castells. 2017. A Probabilistic Reformulation of
Memory-Based Collaborative Filtering: Implications on Popularity Biases. In
Proc. of the 40th ACM SIGIR. 215–224.
[9] Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. 2009. Expected
Reciprocal Rank for Graded Relevance. In Proc. of the 18th ACM CIKM. 621–630.
[10] Jingyuan Chen, Hanwang Zhang, Xiangnan He, Liqiang Nie, Wei Liu, and TatSeng Chua. 2017. Attentive Collaborative Filtering: Multimedia Recommendation
with Item- and Component-Level Attention. In Proc. of the 40th ACM SIGIR.
335–344.
[11] Flavio Chierichetti, Ravi Kumar, and Bo Pang. 2017. On the Power Laws of
Language: Word Frequency Distributions. In Proc. of the 40th ACM SIGIR. 385–
394.

1000

