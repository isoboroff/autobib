Session 6C: Knowledge Bases/Graphs

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

On Link Prediction in Knowledge Bases:
Max-K Criterion and Prediction Protocols
Jiajie Mei

Richong Zhang

BDBC and SKLSDE
Beihang University
jiajie.mei@buaa.edu.cn

BDBC and SKLSDE
Beihang University
zhangrc@act.buaa.edu.cn

Yongyi Mao

Ting Deng

School of Electrical Engineering and Computer Science
University of Ottawa
ymao@uottawa.ca

BDBC and SKLSDE
Beihang University
dengting@act.buaa.edu.cn
France). Although the existing KBs contain enormous amount of
such information, it is well known that significant amount of factual
knowledge is in fact missing. For example, not all facts about Paris,
about France, or about which city is the capital of which country
are collected in the KB. This has motivated intense research on
knowledge base completion, where the objective is to “fill in” the
missing information based on the current content of a KB.
Most commonly the KB completion problem is posed as link
prediction. Briefly explained, in a link prediction task (h, r , ?) for a
given entity h and a given relation r , the objective is to determine
which entity (or entities) t that can form a factual triple (h, r, t).
The current dominating methodology for KB link prediction is
via learning a KB embedding model [3, 5, 10–12, 15, 18, 20], in which
the entities and relations of the KB are represented as quantities in
some Euclidean space. Such a methodology, fundamentally based on
distributed representations [8], has not only proved to be effective
for KB link prediction, but also helped to improve our understanding
and engineering of knowledge representation.
Despite the success of various KB embedding models, we argue
that the evaluation criterion used in link prediction is inappropriate.
Specifically, when answering the link prediction task (h, r , ?), implicitly or explicitly the top-k criterion is used. That is, for a given
value of k, the predictive algorithm outputs k answers based on
ranking certain score computed for each entity. We argue that this
criterion is problematic when for different (h, r, ?) tasks, the number
of correct answers, or “answer multiplicity”, varies significantly. In
this case it is impossible to select a global cutoff that compromises
well across such a wide spectrum of answer multiplicities.
This paper presents a new evaluation criterion, which we refer
to as max-k. In this criterion, the predictive algorithm is asked to
give at most k answers for a prescribed k value. In other words,
the predictive algorithm is free to use a strategy or protocol to
output any number of answers, no greater than k. We adapt the
classical precision, recall and F1 metrics to this setting to evaluate
the answers provided by the predictive algorithm. We show that
when the predictive algorithm is an oracle knowing the correct
answers, then max-k is at least as good as top-k under these metrics.
We then present two protocols applying universally to all prediction models. The first is a sampling protocol which draws k entities
from the predictive distribution generated by the model, using distinct drawn entities as the answers. We show that if the model’s
predictive distribution reflects the oracle’s knowledge of the correct

ABSTRACT
Building knowledge base embedding models for link prediction has
achieved great success. We however argue that the conventional
top-k criterion used for evaluating the model performance is inappropriate. This paper introduces a new criterion, referred to as
max-k. Through theoretical analysis and experimental study, we
show that the top-k criterion is fundamentally inferior to max-k.
We also introduce two prediction protocols for the max-k criterion.
These protocols are strongly justified theoretically. Various insights
concerning the max-k criterion and the two protocols are obtained
through extensive experiments.

CCS CONCEPTS
• Computing methodologies → Reasoning about belief and
knowledge; Statistical relational learning; • Information systems
→ Top-k retrieval in databases;

KEYWORDS
Knowledge Base Embedding, Link Prediction, Evaluation Metric
ACM Reference Format:
Jiajie Mei, Richong Zhang, Yongyi Mao, and Ting Deng. 2018. On Link
Prediction in Knowledge Bases: Max-K Criterion and Prediction Protocols.
In SIGIR ’18: The 41st International ACM SIGIR Conference on Research and
Development in Information Retrieval, July 8–12, 2018, Ann Arbor, MI, USA.
ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3209978.3210029

1

INTRODUCTION

The emerging of large-scale knowledge bases (KBs) such as Freebase [2], Yago [16] and DBpedia [1] has stimulated the development
of novel information-retrieval applications with great commercial
or societal impact. Usually a KB is presented as a collection of
triples (h, r , t), where h and t are entities and r is a relation. For
example, a knowledge triple could be (Paris, IsCapitalOf,
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210029

755

Session 6C: Knowledge Bases/Graphs

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

д-score, i.e., д(h, r, t  ), for each candidate entity t  is computed and
ranked; the candidate entities giving the top-k highest scores are
declared as the answers. Here, k is a prescribed cutoff rank.
As the top-k criterion is completely based on the ranking of
candidate answers, conventional rank-based performance metrics
are usually used to evaluate the prediction models [4].
The definitions of these metrics depend on the availability of
another set of factual triples G  ⊂ G 0 \ G. Note that a standard
practice to obtain G  is to reserve a subset of the KB triples as
G  and use the remainder as G. In standard terminologies, G  is
regarded as the testing data, and the data in G is used for training
and possibly validation as well. Now we review these metrics.
The raw rank RNK(h, r, t) of a triple (h, r, t) ∈ G  is the position
of (h, r, t) in the list of {(h, r, t  ) : t  ∈ [N ]} sorted in descending order of д-score. For any given triple (h, r, t) ∈ G  , denote C(h, r , t) :=
{(h, r, t  ) ∈ G ∪ G  : t   t }. The filtered rank fRNK(h, r , t) is the
position of (h, r, t) in the list of {(h, r, t  ) : t  ∈ [N ]} \ C(h, r , t)
sorted in descending order of д-score.
For a given positive integer k, the top-k hit HIT@k and the filtered top-k hit fHIT@k are the fraction of triples (h, r, t) in G  for
which RNK(h, r, t) ≤ k and that for which fRNK(h, r , t) ≤ k, respectively. The mean rank RNK and the filtered mean rank fRNK
are respectively the average of RNK(h, r, t) and that of fRNK(h, r , t)
over all triples in G  . The mean reciprocal rank MRR and the filtered mean reciprocal rank fMRR are respectively the average of
RNK(h, r, t)−1 and that of fRNK(h, r, t)−1 over all triples in G  .
Among these metrics, the filtered metrics are meant to characterize the predictability of the algorithm. For the prediction algorithms
that are based on machine learning models, these metrics essentially measure the generalizability of the models. The raw metrics,
on the other hand, characterize the aggregated performance both
in terms of generalizability and in terms of memorization capability
(namely, how well the model “remembers” the training examples).

answers, the sampling protocol is optimal in an average sense. The
second is a deterministic greedy protocol, developed and theoretically justified via an asymptotic analysis of the sampling protocol.
Using these protocols, we carry out extensive experiments investigating the performance of some representative KB embedding
models over several popular datasets. We highlight the following
experimental findings. First, the precision, recall and F1 metrics
under the max-k criterion behave differently from the conventional
metrics used for the top-k criterion. We believe that these max-k
metrics provide a more balanced measure for model performances.
Additionally, we may use the sampling and greedy protocols on
the existing models and compare their performance against the
oracle max-k limits. This allows us to assess the performance gap
between these models and the oracle limits thereby assessing the
performance of the current art relative to the theoretical limits.
Finally, a comparison of the top-k and max-k oracle limits suggests
that the top-k criterion is fundamentally and significantly inferior.

2 LINK PREDICTION AND TOP-K
2.1 The Link Prediction Problem
Consider a knowledge base (KB) that contains N entities. We will,
for simplicity, identify the set of entities as the set of positive integers [N ] := {1, 2, . . . , N }. Let R denote the set of relations contained in the KB. Then the KB can be expressed as a collection of
triples (h, r , t) ∈ [N ] × R × [N ], where h and t are referred to as
the head and tail of the triple. Each triple here represents a factual knowledge item. The KB considered for link prediction is then
specified by the set of all triples in the KB, which we denote by G.
In practice, the set G of triples is far from being complete since
there are additional factual triples missing from G. Let G 0 denote
the complete collection of all factual triples involving the entities in
[N ] and the relations in R. The goal of a link prediction task (h, r, ?),
based on G, is to predict the set of all triples (h, r , t) ∈ G 0 \ G.
Here the link prediction problem is formulated as predicting the
tails. The problem may also be formulated as predicting the heads.
Although in our experiments we will consider both, we restrict our
discussion to tail prediction, to ease the presentation.
In essence, every link-prediction algorithm develops a score function д : [N ] × R × [N ] → R on the space of all possible triples
(factual or not) based on the information contained in G. For every
given triple (h, r, t), д(h, r , t) attempts to measure the likelihood
that the triple (h, r, t) is factual. Without loss of generality, we will
assume that a higher score indicates a higher (estimated) likelihood.
The score function д is built right upon the representation of
knowledge (triples). A large portion of link-prediction models try
to represent entities and relations as embeddings in some low dimensional space. An embedding model essentially constructs such
a score function д, which depends on h, r , t only through their embeddings. The differences between various KB embedding models
mainly lie in the parametrization of function д. In a separate section,
we will give a brief overview of some of these models.

2.2

3

PREVIOUS EMBEDDING MODELS

We now give a concise review of KB embedding models for link
prediction, where we focus on the models used in our experiments.
TransE [4] is one of the pioneering models for KB embedding,
generalizing an earlier model known as Unstructured Model [3],
and is extended to a sequence of later models, such as TransH [19]
and TransR [11]. Briefly, for each triple (h, r, t) in the KB, TransE
assumes that the embeddings h and r for the head and tail entities
and the embedding r for the relation satisfy h + r ≈ t. The score
function д(h, r, t) is defined as the L2 norm of h + r − t.
ComplEx [18] embeds entities and relations in complex space.
For each r , it assumes a score matrix Xr whose sign matrix is
partially observed. The entries correspond to factual and nonfactual
triples have the sign “1” and
 “−1”, respectively. The score function

д(h, r, t) = Re K ri hi t¯i models the entries of Xr (Re(·) is the
i=1

real part of a complex number and t¯i is the conjugate of ti ).
Analogy [12] imposes analogical properties on embeddings.
Each r is represented as a normal matrix Wr and for any r , r  ∈ R,
Wr and Wr  are constrained to statisfy commutativity property.
The score function is defined as д(h, r, t) = h Wr t.
ProjE [14] projects the embeddings WE of the entities onto
the vector f (h ⊕ r), calculating the scores of all candidate triples

Top-K Criterion and Conventional Metrics

In all link-prediction models presented to date, the top-k criterion
is implicitly or explicitly assumed for selecting the final answers.
Under this criterion, for a given link prediction task (h, r, ?), the

756

Session 6C: Knowledge Bases/Graphs

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

collectively. Here, f is a non-linearity and h⊕r = Dh h+Dr r+bc (Dh ,
Dr and bc are parameters).
The score vector d(h, r) is defined as


requires the cutoff rank value to be individualized for each link
prediction task.
In general, one expects that such a problem exists in wide classes
of query-answering and multi-label classification problems. From
a practical point view, the querier (i.e, the person who demands
the answers), not knowing the answers, may have in mind the
maximum number of answers that he would get. For example, in
question like “which books are in the Harry Potter Series?”, he may
expect up to 10 answers (rather than, for example, 50); in tasks like
“which countries are in Europe?”, he may expect up to 80 answers
(rather than 10, for example). That is, the querier may specify a
value k as the maximum number of answers demanded, and the
answering/prediction engine generates k ∗ answers according to the
д-score of the answers, where k ∗ is maximally k. Such a criterion
we call the max-k criterion.
Under the max-k criterion, we note that it is desirable for the
answering engine to automatically select the reliable answers that
provide a good balance between precision and recall. However,
having no ground-truth answers available, using what protocol to
generate answers based on д-scores becomes a central problem.

l WE f (h ⊕ r) + bp , where l is the sigmoid or soft-max function
and bp is a bias term. д(h, r , t) is the entry in d(h, r) with index t.
To date, ComplEx, Analogy and ProjE are among the state-ofthe-art KB embedding models. Other competitive models, including
PTransE [10] and ConvE [6] are not introduced due to page limit.

4 MAX-K CRITERION AND METRICS
4.1 From Top-K To Max-K
We now argue that the top-K criterion has severe limitations for
practical link prediction tasks.
To ease notation, we may often re-write the pair (h, r ) as x and
the triple (h, r , t) as (x, y). That is, x ∈ X := [N ] × R, and y ∈ [N ].
For each x ∈ X, let Y 0 (x) be the set of entities y forming a factual
triple (x, y) in G 0 . Similarly, let Y(x) (and resp. Y  (x)) be the set
of entities y forming a factual triple (x, y) in G (and resp. in G  ).
Obviously, Y(x) ∪ Y  (x) ⊆ Y 0 (x). We call the size of Y 0 (x), i.e.,
|Y 0 (x)|, the answer multiplicity of the link prediction task (x, ?).
For practical KBs, it can be reasoned that the answer multiplicity
varies significantly across different link prediction tasks. Note that
although the distribution of answer multiplicities in the ground
truth G 0 can not be directly observed, extrapolating from the observed data G allows us to estimate this distribution1 . The statistics
of several popular datasets given in Table 1 demonstrate the high
variability of the answer multiplicities2 .

4.2

Under the max-k criterion, natural metrics can be constructed to
evaluate the performance of a link-prediction model.
Let K := {x : (x, y) ∈ G  }, i.e., K is the set of all “keys” extracted
from the testing triples in G  . That is, each key x ∈ K defines a
single link prediction task (x, ?). Recall that the rank-based metrics
for the top-k criterion evaluate the prediction performance for
each testing triple. Under the max-k criterion, it is more natural to
evaluate the performance for each task (x, ?).
Let S(x; k) be the set of distinct entities in [N ], under the max-k
criterion, returned by a prediction algorithm for task (x, ?). The
conventional notions of precision, recall and F1 directly apply.
For a task (x, ?), the raw precision P@k is the fraction of entities in
S(x; k) that are contained in Y(x) ∪ Y  (x), and the filtered precision
fP@k is the fraction of entities in S(x; k) that are contained in Y  (x).
The raw recall R@k is the fraction of entities in Y(x) ∪ Y  (x) that
are contained in S(x; k), and the filtered recall fR@k is the fraction
of entities in Y  (x) that are contained in S(x; k).
Correspondingly, the raw F1 F1@k and filtered F1 fF1@k for task
(x, ?) are respectively defined as

Table 1: Statistics of Answer Multiplicity {|Y(x)| : (x, y) ∈ G}
Dataset
FB15K [4]
FB15K-237 [17]
WN18 [4]
WN18RR [7]
YAGO3-10 [13]

min
1
1
1
1
1

max
3,963
3,962
486
486
61,329

mean
3.68
3.73
1.66
1.69
5.52

stddev
24.11
24.22
5.17
4.73
101.35

Max-K Metrics

sum
1,066,284
579,300
292,884
179,738
2,168,080

The high variability of the answer multiplicities challenges the
choice of a cutoff rank in the top-k criterion. If a large cutoff is
selected, significantly many false positive answers will be produced
for the low-multiplicity prediction tasks. If a small cutoff is selected,
the answers for the high-multiplicity tasks will suffer from a poor
recall. Mostly, the cutoff rank k is chosen customarily as 10 (thereby
giving rise to the HIT@10 and fHIT@10 metrics). However, for
example, in the testing data of FB15K, we observe that there exist
approximately 21% of the link prediction tasks having answer multiplicities higher than 10. Besides nearly 39% of the link prediction
tasks possess answer multiplicities lower than 3. This, in practice,
would make the cutoff rank 10 a poor choice for both precision and
recall. In fact, it could be shown that any particular global choice
of the cutoff rank would face a similar dilemma, using which one
either suffers from a poor recall, or from a poor precision, or from
both. This is because for such high variability in answer multiplicities, simultaneously achieving good precision and recall necessarily

F1@k

:=

fF1@k

:=

2 · P@k · R@k
P@k + R@k
2 · fP@k · fR@k
fP@k + fR@k

Over the entire set of prediction tasks specified by K, each of
these measures can then be averaged. Among these metrics, it is
well-known that the F1 metrics can be viewed as an overall score
integrating both precision and recall. Note that all these measures
are equally applicable to the top-k criterion, in which case S(x; k)
is simply taken as the set of the highest ranked k answers.
To compare the max-k and top-k criteria, imagine an oracle who
knows the correct answers Y(x)∪Y  (x) and needs to select answers
for the max-k and top-k criteria respectively. For max-k, he would
select precisely all answers in Y(x) ∪ Y  (x) if |Y(x) ∪ Y  (x)| ≤ k

we assume that observed data G is obtained by randomly sampling the groundtruth data G 0 , then it can be shown the answer multiplicity distribution in G 0 has the
same shape as the distribution of | Y(x ) | but has magnified mean and variance.
2 In Table 1,“stddev” stands for standard deviation.

1 If

757

Session 6C: Knowledge Bases/Graphs

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

and select k answers in Y(x)∪Y  (x) if |Y(x)∪Y  (x)| > k. For top-k,
he would select k answers in Y(x)∪Y  (x) if |Y(x)∪Y  (x)| ≥ k, and
if |Y(x) ∪ Y  (x)| < k, he would select all answers in Y(x) ∪ Y  (x)
together with k − |Y(x) ∪ Y  (x)| additional answers in order to
make up k answers. We then have the following results.
Lemma 1. The raw precision, recall and F1 performances of the
oracle under the top-k and max-k criteria for task (x, ?) are as follows.
=

∗
R@k top

=

∗
F1@k top

=

∗
P@k max

=

∗
R@k max

=

∗
F1@k max

=


|Y(x) ∪ Y  (x)|
min
,1
k


k
min
,
1
|Y(x) ∪ Y  (x)|


2k
2|Y(x) ∪ Y  (x)|
min
,
|Y(x) ∪ Y  (x)| + k |Y(x) ∪ Y  (x)| + k
1


k
min
,
1
|Y(x) ∪ Y  (x)|


2k
,
1
min
|Y(x) ∪ Y  (x)| + k

0.07

0.06

0.06

0.05

0.05

0.04

0.04

0.03

0.03

0.02

0.02

0.01

0.01

0

10

20

30

40

0

50

0

10

20

30

40

50

Figure 1: Two example answer predicting distributions.
To fix ideas, consider the following toy example. Suppose that
N = 50 and the querier specifies k = 20 in the max-k criterion.
Consider the following two cases.
• Suppose that the obtained д-score is the predictive distribution in Figure 1 (left). In this case, the entities 1 to 10
have dominating probabilities. Then we desire that these 10
entities are selected as the answers.
• Suppose that the obtained д-score is the predictive distribution in Figure 1 (right). In this case, none of probability
values dominate but entities 1 to 40 each have significantly
larger probability than the remaining entities. Then we desire that the selected answers are 20 entities from the first
40 entities.

Note that the filtered metrics can be obtained similarly. From
these results, we see that for the same k, the oracle’s precision,
recall and F1 performances for max-k are all at least as good as
those for top-k. This should justify the use of max-k criterion.

Desiring such a behaviour, we propose the following sampling
protocol for max-k criterion.
Sampling Protocol
For each prediction task (x, ?), draw k answers i.i.d. from the
predictive distribution pY |X (·|x) given by the model, and use
the distinct drawn entities to form the answer set S(x; k).
Obviously how well such a protocol performs not only depends
on the protocol per se, but also depends on the predictive distribution used. To assess the quality of this protocol, we consider an
oracle setting in which the effect of the predictive distribution’s
incorrectness is removed from our analysis. Let pY∗ |X (·|x) be the oracle distribution, namely, it puts equal non-zero probability on each
entity in the correct answers Y(x)∪Y  (x) and puts zero probability
on other entities. Then the following result can be proved.

5 PREDICTION PROTOCOLS FOR MAX-K
Given the д-score of each entity, we now consider the problem of
designing appropriate protocols for selecting up to k answers.
First note that simply implementing the top-k criterion can be
regarded as a trivial protocol that also satisfies the max-k criterion.
We call this protocol the TopK protocol, which is defined below.
TopK Protocol
For each prediction task (x, ?), return k answers having the
highest д-scores to form the answer set S(x; k).
Introducing this protocol merely serves to evaluate the top-k
approach under the above defined max-k metrics. This enables us to
compare top-k and max-k in the same framework in experiments.
Now we turn to designing protocols that arrive at a good compromise between precision and recall, or to maximize the F1 metrics
if possible. Such an optimization problem appears difficult both
analytically and numerically. Nevertheless we will present two protocols. The first is a sampling protocol, motivated by an intuitive
understanding and supported by a theoretical justification. The
second is a deterministic greedy protocol, developed based on an
asymptotic analysis of the sampling protocol. These protocols are
capable of automatically dealing with diverse answer multiplicities
and generating a list of answers of varying lengths. Not only theoretically justified, these protocols are shown in a later section to
result in significantly improved F1 values over the TopK protocol.

5.1

0.08

0.07

0



∗
P@k top

0.08

Theorem 1. Using the oracle distribution pY∗ |X (·|x) as the predictive distribution in the sampling protocol, the expected precision,
recall, and F1 values all achieve those given in Lemma 1 for the max-k
criterion.
Similar results concerning the filtered metrics may also be obtained. At this point, we conclude that the sampling protocol for
the max-k criterion is at least optimal in an average sense.
5.1.1 From Score Functions to Probability Distributions. Many
link-prediction models are not probabilistic. That is, their д-score
functions do not have probabilistic interpretations. Hence, the sampling protocol does not apply directly to these models. We now
present a simple transformation, which converts the д-score function to the desired predictive distribution pY |X as follows.

Sampling Protocol

We first consider the case in which the д-score function д(x, y) is a
conditional distribution pY |X (y|x), indicating the probability that
an entity y is a correct answer for task (x, ?).

pY |X (y|x) :=

exp αд(x, y)

,
exp αд(x, y  )

y  ∈[N ]

758

Session 6C: Knowledge Bases/Graphs

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA



where α is a positive parameter one may tune to shape the distribution. Noting that this transformation merely exploits the widely
used soft-max function, we call it the soft-max transformation. It is
well-known that this transformation is monotonic in y. Namely, for
the same x, it preserves the rank of the д-score for each entity y.
Although there can be other ways to generate probability values
from scores, the soft-max transformation appears the most natural
and universal. Indeed in most models that output a probability
(mass) distribution, this transformation has been used.
Having this transformation ready, from here on, we will restrict
our discussion to the setting in which the predictive model outputs
a distribution pY |X (·|x) for each task (x, ?).

5.2

the shortened sequence contains k 1 −

i=1

5.3

Greedy Protocol

The proof in Theorem 2 motivates another prediction protocol.
We will assume that the condition pY |X (1|x) ≥ pY |X (2|x) ≥
. . . ≥ pY |X (N |x) is satisfied. Note that making this condition satisfied only requires sorting the probability values pY |X (y|x) for all
y ∈ [N ] and re-label the entities according their sorted order. The
greedy protocol is then given as follows.
Greedy Protocol
Find the value
of 
k in Theorem

 2; compute q as the nearest int
k

eger to k 1 −
pY |X (i |x) ; return S(x; k) as {1, 2, . . . , 
k + q}.
i=1

Theorem 2. Suppose that pY |X (·|x) is such that pY |X (1|x) ≥
k be the integer such that pY |X (
k |x) ≥
pY |X (2|x) ≥ . . . ≥ pY |X (N |x). Let 
1/k and pY |X (
k + 1|x) < 1/k. If no such 
k exists, let 
k = 0. Then the
size of the answer set S(x; k) obtained by sampling pY |X (·|x) satisfies

k

(1)

p

where −→ denotes convergence in probability, and the summation
term is taken as 0 when 
k = 0.
The proof of the theorem is somewhat technical, but since it
contains additional insight, we sketch its key ideas.
Proof Sketch: By the Weak Law of Large Number (WLLN), in the
large k limit, with probability approaching 1 that the sequence of k
entities drawn from pY |X (·|x) contains pY |X (1|x)k “1"s, pY |X (2|x)k
“2"s, . . . , pY |X (N |x)k “N "s. That is, as long as
pY |X (i |x)k ≥ 1,



also proved for this case.

Asymptotic Analysis of Sampling Protocol

k, as k −→ ∞,
pY |X (i |x) + 
i=1


i=1


pY |X (i |x) distinct en-

tities with probability approaching 1. As such, with probability
approaching
1, S(x;k) contains precisely entities 1, 2, . . . , 
k and


k

k 1−
pY |X (i |x) other distinct entities. Hence the theorem is

We now show an asymptotic property of the sampling protocol.
This property will be the foundation to establish the next protocol.
In the max-k criterion, we will now consider the setting where
k is asymptotically large. In this setting, we will investigate how
many answers will be generated by the sampling protocol and what
these answers are, using a given predictive distribution pY |X (·|x).

p

|S(x; k)| −→ k 1 −


k


From the proof of Theorem 2, for asymptotically large k, the
greedy and sampling protocol are equivalent. Since the sampling
protocol is optimal in an average sense, similar optimality statement
can be made about the greedy protocol for large k. With small
or modest k, the greedy protocol has the advantage of providing
deterministic results where the answers provided by the sampling
protocol are subject to uncertainty and statistical irregularities.
Besides, the greedy protocol in the max-k setting may be viewed
as implementing a “top-k” criterion with a self-determined k value,

k + q. The protocol is guaranteed to select 
k + q entities with the
highest probabilities. Though the sampling protocol implicitly aims
the same, its probabilistic nature gives no guarantee in this regard.

6

EXPERIMENT

We perform extensive experiments to study the max-k criterion and
the proposed protocols for various link prediction models and over
a range of datasets. Due to limited space, we only select a small
fraction of our results to illustrate the key findings.

(2)

Table 2: Statistics of Datasets

the sequence contains entity i with probability approaching 1. The
largest i satisfying Equation 2 can be shown to be 
k precisely.
Consider the case in which no i ∈ [N ] satisfies Equation (2), i.e.,

k = 0. In this case, again by WLLN, the probability that the sequence
contains two repeated entities approaches 0. Thus, the probability
that sequence contains precisely k distinct entities approaches 1,

Dataset
FB15K
FB15K-237
WN18
WN18RR
YAGO3-10

p

namely, |S(x; k)| −→ k and the theorem is proved for this case.
k > 0. Then entities 1, 2, . . . , 
Now suppose 
k are contained in
the drawn sequence with probability approaching 1. Thus S(x; k)
includes these entities almost surely. Remove entities 1, 2, . . . , 
k
from this sequence. The remaining
sequence
is
shortened
and
has



k

length arbitrarily close to k 1 −
pY |X (i |x) . By an argument

6.1

N
14,951
14,541
40,943
40,943
123,182

|R |
1,345
237
18
11
37

#train
483,142
272,115
141,442
86,835
1,079,040

#valid
50,000
17,535
5000
3,034
5,000

#test
59,071
20,466
5000
3,134
5,000

Experiment Setup

6.1.1 Datasets. The datasets used in our experiments include
FB15K [4], WN18 [4], FB15K-237 [17], WN18RR [7], and YAGO310 [13]. These datasets are popular in KB link-prediction research.
The FB15K dataset is filtered from Freebase, which contains general facts. The WN18 dataset is a subset of WordNet, containing
the lexical relational data between synsets. Recently, it has been
observed that these two datasets are populated with reciprocal

i=1

similar to that used in the first case, with vanishing probability
that the shortened sequence contains two repeated entities. Thus

759

Session 6C: Knowledge Bases/Graphs

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 3: Selected α value for each model on each dataset

of only very few (or even one) entities are very high, and all other
probability values are suppressed, including those for other correct
answers. As such, recall drops. This recall-vs-α behaviour, first
rising and then dropping, also depends on the value k as well as
the correctness of the predictive distribution, i.e., the model. Note
that the order of the colour-coded recall curves for TransE is quite
different from that for ComplEx and that for Analogy.
The combined effect of precision and recall then also gives a nonmonotonic behaviour of F1 performance with respect to α (bottom
row). It is interesting to note that for each of the examined models
and for each choice of α value, the F1 curve starts to flatten after
k = 10. This can be explained by the fact that F1 is the harmonic
mean of precision and recall: since precision stays constant with
respect to k, the increase of recall necessarily saturates the F1 value.

Model FB15K WN18 FB15K-237 WN18RR YAGO3-10
TransE
10
10
10
20
5
ComplEx
2
2
5
5
2
Analogy
2
2
5
5
2
triples3 [9], providing optimistic artifacts for link prediction. The
FB15K-237 [17] and WN18RR [7] datasets are subsets of FB15K and
WN18 respectively. They are obtained by further filtering FB15K
and WN18 and removing reciprocal triples. YAGO3-10 is filtered
from YAGO3, which extends YAGO using multilingual information
in Wikipedia. The statistics of these datasets are listed in Table 2.
6.1.2 Models. The prediction models used in the experiments
are TransE [4], ComplEx [18], Analogy [12], and ProjE [14]. Among
these models, only projE is a probabilistic model, which outputs a
predictive distribution for each link prediction task. For each model,
we use the code released in the public domain. Specifically, we use
the code released in [10] for TransE, the code released in [12] for
ComplEx and Analogy, and the code released in [14] for ProjE4 .
Default hyper-parameter settings of the code are used.

6.2

6.2.2 Selection of α. The remainder of the results are reported
for fixed choices of α. For each dataset and each model that is nonprobabilistic, we pick the α value in the examined choices given
above5 that maximizes fF1@10 in the above experiments. Though
such a choice of α may seem arbitrary, it reflects our interest in a
balanced measure of performance (hence the F1 measure rather than
precision or recall measure) and the predictability of the models
(hence a filtered metric rather than a raw metric). Besides, picking
10 as the value of k for optimizing fF1 is justified by the bottom-row
plots of Figure 2, where the fF1 curves start to flatten at around
k = 10, and further increasing k only increases fF1 marginally. This
is also supported by the conventional top-k metrics, where k = 10
is the most popular (e.g. in HIT@10).
The selected α values are given in Table 3. It can be seen that
the “best choices" of α values for ComplEx and Analogy are the
same. This correlates with the model similarity between ComplEx
and Analogy, and is also supported by the similar behaviours of
ComplEx and Analogy in Figure 2.

Results

6.2.1 Converting Scores to Distributions. Since the scoring functions in TransE, ComplEx and Analogy are not distributions, to
evaluate their behaviour under the proposed protocols for max-k,
we apply the soft-max transformation to the score functions. Tuning the scaling factor parameter α in the soft-max transformation
allows the resulting distribution to be shaped with different contrast and is expected to impact the performance of the prediction
algorithm.
Using the sampling protocol over FB15K-237 as an example,
Figure 2 shows the filtered precision, recall, and F1 performances
for these four models with α-settings in {0.2, 0.5, 1, 2, 5, 10, 20, 50}.
Note that under the sampling protocol, precision is independent of
the k value for max-k, equal to the sum of the predictive probability
values of the correct answers. Thus precision is only plotted against
changing α (top row). As expected, increasing α monotonically
increases precision. This is because larger α leads to higher contrast
in the resulting distribution, shaping the distribution farther away
from the uniform distribution. Such higher contrast shrinks the
set S(x; k) of selected answers to only contain highly confident
ones, thereby increasing precision. The recall performance however
exhibits a complex behaviour with respect to α (middle row). At
very low α, the resulting distribution is nearly uniform; sampling
from such a distribution puts little emphasis on the correct answers,
thereby resulting in very low recall. As α increases, the distribution
is gradually shaped to give good contrast, enabling it to distinguish
the correct answers from the wrong ones. This makes the output
answers include more correct answers and results in higher recall.
But as α keeps increasing and moves away from this regime, the
contrast in the distribution becomes too high. And the probabilities

6.2.3 Comparing Models Under Top-K Metrics and Max-K Metrics
. The four models are compared under the conventional metrics
(top-k hit, mean rank, and mean reciprocal rank) and the max-k
metrics (precision, recall and F1). Under the max-k criterion, the
models are evaluated under both the sampling protocol and the
greedy protocol. Due to page limit, only the filtered metrics are
reported (Table 4), with the winning values shown in bold.
It can be seen that the max-k metrics under the sampling protocol
behave in a trend very similar to those under the greedy protocol.
This is expected since the greedy protocol is derived as a limiting
case of the sampling protocol, under certain asymptotic assumption.
However, the conventional metrics behave quite differently from the
max-k metrics. Among the three examined conventional metrics,
it appears that only the fMRR metric follows a trend consistent
with the precision metrics under max-k criterion. But there isn’t
sufficient data to support a strong correlation between the two.
We believe that much of the distinction between the top-k and
the max-k metrics (for example, between fHIT@10 and fR@10)
lies in the difference between the definitions of the max-k and
top-k criteria: top-k demands strictly k answers, whereas max-k
allows any number of answers, up to k. As we will show in later

pair of triples (h, r, t ) and (t, r , h) are referred to as a reciprocal pair, if r and r 
indicate the same relation but with object and subject swapped. For example, (John,
isParentOf, Mary) and (Mary, isChildOf, John) are a reciprocal pair.
4 An error in the code of ProjE was noted by its author after the publication of the
paper, and we have fixed the error.
3A

precision results presented in Figure 2, one can see that the choices of α values
mark the important transitions of the performance curves.
5 From

760

Session 6C: Knowledge Bases/Graphs

TransE

ComplEx

0.12

0.10

0.08

0.08

0.08

0.06

0.06

0.06

fP

0.10

0.04

0.04

0.04

0.02

0.02

0.02

0

10

20

30

40

0.00

50

TransE

0.40

0.25
0.20

20

40

0.00

50

0.30
0.25
0.20

0.25
0.20
0.15

0.10

0.10

0.05

0.05
40

60

80

0.00

100

20

40

60

80

0.00

100

TransE

0.10

0.10

0.10

0.04
0.02

40

40

60

80

= 0.2
= 0.5
=1
=2
=5
= 10
= 20
= 50

0.06
0.04
0.02

100

0.00

60

80

100

0.08
fF1@k

fF1@k

0.08
= 0.2
= 0.5
=1
=2
=5
= 10
= 20
= 50

50

Analogy
0.12

20

20

ComplEx
0.12

0.06

40

k

0.12

0

0

k

0.08

30

0.05
0

k

0.00

20

= 0.2
= 0.5
=1
=2
=5
= 10
= 20
= 50

0.30

0.10

20

10

Analogy

0.35

0.15

0

0

0.40

= 0.2
= 0.5
=1
=2
=5
= 10
= 20
= 50

0.15

0.00

fF1@k

30

ComplEx

0.35

fR@k

0.30

10

0.40

= 0.2
= 0.5
=1
=2
=5
= 10
= 20
= 50

0.35

0

fR@k

0.00

Analogy

0.12

0.10

fP

fP

0.12

fR@k

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

0

20

40

k

60

80

= 0.2
= 0.5
=1
=2
=5
= 10
= 20
= 50

0.06
0.04
0.02

100

0.00

0

20

40

k

60

80

100

k

Figure 2: Impact of α on model performance (FB15K-237, sampling protocol)
Table 4: Comparing models under conventional and Max-K metrics
Dataset

FB15K

WN18

FB15K-237

WN18RR

YAGO3-10

Models
TransE
ComplEx
Analogy
ProjE
TransE
ComplEx
Analogy
ProjE
TransE
ComplEx
Analogy
ProjE
TransE
ComplEx
Analogy
ProjE
TransE
ComplEx
Analogy
ProjE

Conventional Metrics
fHIT@10 fRNK fMRR
73.46%
74
0.456
83.46%
119
0.716
83.89%
114
0.723
75.35%
78
0.588
94.55%
479
0.584
94.50%
737
0.942
94.57%
717
0.942
94.64%
298
0.820
40.50%
327
0.219
36.03%
525
0.206
36.90%
521
0.211
42.46%
231
0.249
43.44%
5805
0.191
41.48%
8358
0.390
41.29%
8252
0.391
47.69%
3675
0.367
30.32%
1904
0.151
43.65%
2920
0.266
43.42%
2506
0.257
64.20%
1027
0.470

Max-K Metrics (Sampling)
fP
fR@10 fF1@10
17.11% 26.81%
0.187
24.88% 37.68%
0.272
25.23% 38.01%
0.275
19.27% 46.95%
0.243
19.56% 36.78%
0.226
61.56% 83.49%
0.660
61.42% 83.53%
0.659
40.37% 70.01%
0.468
8.74%
15.16%
0.098
11.51% 13.35%
0.118
11.43% 13.54%
0.118
11.34% 24.45%
0.135
1.62%
2.27%
0.018
22.40% 33.73%
0.247
22.45% 34.08% 0.249
14.67% 33.51%
0.183
2.00%
9.87%
0.030
1.60%
8.76%
0.025
1.66%
8.83%
0.025
6.23% 22.38%
0.083

761

Max-K Metrics (Greedy)
fP
fR@10 fF1@10
16.53% 28.75%
0.195
24.14% 39.68%
0.278
24.51% 40.01%
0.281
16.95% 63.30%
0.239
22.32% 40.08%
0.273
61.90% 87.77%
0.676
61.78% 87.89%
0.675
31.71% 76.96%
0.417
8.31%
17.02%
0.102
11.18% 13.70%
0.118
11.20% 13.84%
0.119
10.27% 31.20%
0.136
1.53%
2.43%
0.018
22.23% 36.02%
0.257
22.36% 36.32%
0.259
13.34% 40.81%
0.190
2.36%
14.79%
0.040
1.93%
10.08%
0.031
2.12%
11.45%
0.034
6.99% 31.96% 0.102

Session 6C: Knowledge Bases/Graphs

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

TransE

TransE

TransE

1.0

1.0

0.8

0.8

0.8

0.6

0.6

0.6

P@k

R@k

F1@k

1.0

0.4

0.4
Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

0.2

0.0

0

20

40

60

80

0.4
Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

0.2

0.0

100

0

20

40

60

80

0.0

100

ComplEx

0.8

0.6

0.6

0.6

R@k

P@k
0.4

0.4
Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

0.2

40

60

80

0.0

0

20

40

60

k

80

0.0

100

Analogy

0.8

0.8

0.6

0.6

0.6

R@k

P@k

0.4
Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

0.2

60

80

0.0

0

20

40

60

80

0.0

100

20

40

60
k

ProjE

ProjE

ProjE

1.0

1.0

0.8

0.8

0.8

0.6

0.6

0.6

R@k

P@k

0.4

0.2

40

60

80

100

0.4
Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

0.2

100

80

F1@k

1.0

20

0

k

Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

100

Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

0.2

k

0.4

80

0.4
Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

0.2

100

60

F1@k

0.8

0

40

Analogy
1.0

0.0

20

Analogy
1.0

40

0

k

1.0

20

Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

0.2

k

0.4

100

0.4
Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

0.2

100

80

F1@k

0.8

0

60
k

ComplEx

0.8

0.0

40

ComplEx

1.0

20

20

k
1.0

0

0

k
1.0

0.0

Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

0.2

0.0

0

20

40

k

60
k

80

Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

0.2

100

0.0

0

20

40

60

80

100

k

Figure 3: Comparing various protocols and theoretical limits on FB15K-237
experiments, they induce very different behaviour for the same
predictive distribution. Besides, note that the conventional top-k
metrics are triple-based, namely, the basic performance measure
evaluates how likely a given triple (x, y) is factual, and such a measure is then averaged over all testing triples. By contrast the max-k
metrics are key-based or task-based, namely, the basic performance
measures the quality of the answers provided for a prediction task
(x, ?), then averaged over all tasks. This should also cause some
dataset-dependent difference.
Nonetheless, these results suggest that a model that performs
well under the top-k metrics does not necessarily perform well
under the max-k metrics. Take ProjE for example. Though it appears
as the best model on YAGO3-10 under both top-k metrics and max-k
metrics, these metrics give contradicting indications on WN18.

6.2.4 Comparing Sampling, Greedy, TopK Protocols and Theoretical Limits. We now study the performance of various models under
the max-k criterion. Specifically, we evaluate the performance of
a given model under the sampling, greedy and TopK protocols.
Additionally, we evaluate two notions of theoretical limit. One
notion is the oracle’s max-k performance, which includes the precision, recall, and F1 of the oracle under the max-k criterion, i.e.,
∗ , R@k ∗
∗
P@k max
max and F1@k max given in Lemma 1; these quantities
are labeled by MaxK_Oracle in the plots. The other notion is the
oracle’s top-k performance, which includes the precision, recall,
∗ , R@k ∗
and F1 of the oracle under the top-k criterion, i.e., P@k top
top
∗
and F1@k top given in Lemma 1; these quantities are labeled by
TopK_Oracle in the plots. Only raw metrics are reported.
Figure 3 contains the performance of TransE, ComplEx, Analogy and ProjE under the two protocols over FB15K-237, together
with the MaxK_Oracle and TopK_Oracle limits. Overall, for each

762

Session 6C: Knowledge Bases/Graphs

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

On FB15K, Performance of ComplEx

On FB15K, Performance of Analogy

On FB15K, Performance of ProjE

0.8

0.8

0.8

0.6

0.6

0.6

0.4

0.4
Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

0.2

0.0

F1@k

1.0

F1@k

1.0

F1@k

1.0

0

20

40

60

80

0.4
Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

0.2

0.0

100

0

20

40

60

80

0.0

100

0.6

0.4

0.4
Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

0.2

20

40

60

80

0.4
Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

0.2

0.0

100

0

20

40

k

60

80

0.0

100

On WN18RR, Performance of ComplEx

On WN18RR, Performance of Analogy

20

40

0.8

0.6

0.6

0.6

0.4

0.2

60

80

0.0

0

20

40

60

80

Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

0.2

0.0

100

0

20

40

60

80

k

k

k

On YAGO3-10, Performance of ComplEx

On YAGO3-10, Performance of Analogy

On YAGO3-10, Performance of ProjE

0.8

0.8

0.8

0.6

0.6

0.6

0.4
Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

0.2

20

40

60

80

0.4
Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

0.2

100

100

F1@k

1.0

F1@k

1.0

F1@k

1.0

0.4

100

0.4
Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

0.2

100

80

F1@k

0.8

F1@k

0.8

F1@k

1.0

Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

60

On WN18RR, Performance of ProjE

1.0

40

0

k

1.0

20

Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

0.2

k

0.4

100

F1@k

0.6
F1@k

0.6
F1@k

0.8

0

80

On WN18, Performance of ProjE

0.8

0.0

60
k

On WN18, Performance of Analogy

0.8

0

40

On WN18, Performance of ComplEx

1.0

0.0

20

k
1.0

0

0

k
1.0

0.0

Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

0.2

0.0

0

20

40

k

60
k

80

Sampling
Greedy
TopK
TopK Oracle
MaxK Oracle

0.2

100

0.0

0

20

40

60

80

100

k

Figure 4: Comparing F1@k performance and theoretical limits on other datasets
model, the performances of the sampling protocol and those of the
greedy protocol are close to each other. In some cases (e.g., for Analogy and ComplEx), their difference is negligible, confirming that
the greedy protocol is a reasonable approximation of the sampling
protocol. In other cases, the differences between the two protocols
are more visible (as for ProjE and TransE), we believe that this is
due to a compound effect of the correct answer’s distribution and
the model’s predictive distribution; when the two distributions do
not match well enough and/or when the contrast of the model’s
predictive distribution isn’t significant, such a phenomenon will
occur. To verify this, we compute two quantities for each model.
The first is the total variational distance (TVD) between the model’s
predictive distribution and the uniform distribution over the set of
all entities, averaged over all keys. The second is the total probability mass (TPM) of the correct answers under the model’s predictive

distribution, again averaged over all keys. The TVD quantity can be
seen as a measure of the predictive distribution’s contrastiveness,
and the TPM quantity can be regarded as a measure of the matching
between the model predictive distribution and the correct answer’s
distribution. The results (see table below) confirm our conjecture.
TransE ComplEx Analogy ProjE
TVD 0.9957
0.9993
0.9992
0.9561
TMP 0.5971
0.7608
0.7596
0.5137
Comparing the F1 curves of MaxK_Oracle and TopK_Oracle
in the plots in Figure 3 reveals that the max-k limit is significantly
higher than the top-k limit. With increasing k, the TopK_Oracle
curve decreases after a small value of k, whereas the MaxK_Oracle
curve continuously increases. This is due to the great advantage
of the max-k protocol in precision, which dominates its slight disadvantage in recall. If one believes in the F1 score as a balanced

763

Session 6C: Knowledge Bases/Graphs

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

measure of performance, this should convincingly indicate that the
top-k criterion is fundamentally inferior to the max-k criterion.
This is further comfirmed by comparing the F1 score of the
sampling and greedy protocols against that of the TopK_Oracle
limit. The F1 performances of all four models under these two
protocols exceed TopK_Oracle, the theoretical upper limit that all
possible models can ever achieve under the top-k criterion.
Besides, when applying the TopK protocol on the model’s predictive distribution, we see that its F1 score is only slightly lower than
the TopK_Oracle F1 curve and the gap between the two curves
diminishes with k. That is, with the top-k criterion, there is little
need to further improve the model, particularly at large k.

of China (No. 61772059, 61602023, 61421003). This paper is also
supported by the State Key Laboratory of Software Development
Environment of China and Beijing Advanced Innovation Center
for Big Data and Brain Computing.

REFERENCES
[1] Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak,
and Zachary Ives. 2007. Dbpedia: A nucleus for a web of open data. Springer.
[2] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor.
2008. Freebase: a collaboratively created graph database for structuring human
knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on
Management of data. ACM, 1247–1250.
[3] Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. 2014. A semantic matching energy function for learning with multi-relational data. Machine
Learning 94, 2 (2014), 233–259.
[4] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational
data. In Advances in Neural Information Processing Systems. 2787–2795.
[5] Antoine Bordes, Jason Weston, Ronan Collobert, and Yoshua Bengio. 2011. Learning structured embeddings of knowledge bases. In Conference on Artificial Intelligence.
[6] Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2017.
Convolutional 2D Knowledge Graph Embeddings. CoRR abs/1707.01476 (2017).
arXiv:1707.01476 http://arxiv.org/abs/1707.01476
[7] Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2017.
Convolutional 2D Knowledge Graph Embeddings. CoRR abs/1707.01476 (2017).
arXiv:1707.01476 http://arxiv.org/abs/1707.01476
[8] G. E. Hinton, J. L. McClelland, and D. E. Rumelhart. 1986. Parallel Distributed
Processing: Explorations in the Microstructure of Cognition, Vol. 1. MIT Press,
Cambridge, MA, USA, Chapter Distributed Representations, 77–109. http://dl.
acm.org/citation.cfm?id=104279.104287
[9] Rudolf Kadlec, Ondrej Bajgar, and Jan Kleindienst. 2017. Knowledge Base Completion: Baselines Strike Back. In Proceedings of the 2nd Workshop on Representation
Learning for NLP, Rep4NLP@ACL 2017, Vancouver, Canada, August 3, 2017. 69–74.
https://aclanthology.info/papers/W17-2609/w17-2609
[10] Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2015. Modeling relation paths for
representation learning of knowledge bases. Proceedings of the 2015 Conference
on Empirical Methods in Natural Language Processing, EMNLP 2015 (2015).
[11] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning
entity and relation embeddings for knowledge graph completion. In Proceedings
of AAAI.
[12] Hanxiao Liu, Yuexin Wu, and Yiming Yang. 2017. Analogical Inference for Multirelational Embeddings. In Proceedings of the 34th International Conference on
Machine Learning (Proceedings of Machine Learning Research), Doina Precup and
Yee Whye Teh (Eds.), Vol. 70. PMLR, International Convention Centre, Sydney,
Australia, 2168–2178.
[13] Farzaneh Mahdisoltani, Joanna Biega, and Fabian M. Suchanek. 2015. YAGO3: A
Knowledge Base from Multilingual Wikipedias. In CIDR 2015, Seventh Biennial
Conference on Innovative Data Systems Research, Asilomar, CA, USA, January 4-7,
2015, Online Proceedings. http://cidrdb.org/cidr2015/Papers/CIDR15_Paper1.pdf
[14] Baoxu Shi and Tim Weninger. 2017. ProjE: Embedding Projection for Knowledge
Graph Completion.
[15] Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. 2013. Reasoning with neural tensor networks for knowledge base completion. In Advances
in Neural Information Processing Systems. 926–934.
[16] Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of
semantic knowledge. In Proceedings of the 16th international conference on World
Wide Web. ACM, 697–706.
[17] Kristina Toutanova and Danqi Chen. 2015. Observed versus latent features
for knowledge base and text inference. In Proceedings of the 3rd Workshop on
Continuous Vector Space Models and their Compositionality. 57–66.
[18] Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, and Guillaume
Bouchard. 2016. Complex Embeddings for Simple Link Prediction. In Proceedings
of the 33nd International Conference on Machine Learning, ICML 2016, New York
City, NY, USA, June 19-24, 2016. 2071–2080. http://jmlr.org/proceedings/papers/
v48/trouillon16.html
[19] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge
graph embedding by translating on hyperplanes. In Proceedings of the TwentyEighth AAAI Conference on Artificial Intelligence. Citeseer, 1112–1119.
[20] Jianfeng Wen, Jianxin Li, Yongyi Mao, Shini Chen, and Richong Zhang. 2016. On
the Representation and Embedding of Knowledge Bases beyond Binary Relations.
In Proceedings of the Twenty-Fifth International Joint Conference on Artificial
Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016. 1300–1307. http:
//www.ijcai.org/Abstract/16/188

6.2.5 Comparing Models and Datasets Under The Max-K Criterion. Using a similar approach as that in Section 6.2.4, we now compare three state-of-the-art models, ComplEx, Analogy and ProjE,
and assess the datasets with respect to these models.
The plots in Figure 4 are in the same form as those in Figure 3,
except that we here only include the F1 metric. From the figure,
one can conclude that under the F1 metric, ComplEx and Analog
behave nearly identically. On FB15K, WN18 and WN18RR, they
both outperform ProjE by a visible margin. On YAGO3-10, the three
models have very similar performance. For each of the examined
dataset in Figure 4, when comparing the model performance against
the MaxK_Oracle limit, it is interesting to observe that on WN18,
there is only very small gap between the MaxK_Oracle curve
and the curves of these state-of-the-art models. That is, on this
dataset, the performance of the current state of the art has already
approached the ultimate limit. As such, we believe that on the
WN18 dataset, further improvement upon these models will be
very difficult, hence anticipating little progress in the coming years.
However, on FB15K, YAGO3-10 and particularly on WN18RR,
the performance of the three models is still far away from the
MaxK_Oracle limit. This suggests that there is still significant
room for developing innovative models to further improve the
performance on these datasets. One should not expect a model
to achieve the MaxK_Oracle limit. To what extent a model can
approach the MaxK_Oracle performance is in fact governed by
the learnability bound (intrinsically dictated by the structure and
size of the data), living somewhere below the MaxK_Oracle curve.

7

CONCLUDING REMARKS

Although the context of this work is link prediction in KBs, the
max-k criterion and the protocols introduced in this paper in general apply widely to information retrieval, multi-label classification
and many related areas. This criterion is practically motivated and
at the same time theoretically sound. The proposed sampling and
greedy protocols also have a theoretical foundation and a universal applicability. We anticipate that the max-k criterion and the
proposed protocols find many applications beyond link prediction
in KBs. Return to this context, we suggest that the conventional
metrics used for link prediction be discarded, and replaced by the
precision, recall and F1 metrics introduced herein.

ACKNOWLEDGMENTS
This work is supported partly by China 973 program (No.2014CB340
305, 2015CB358700), by the National Natural Science Foundation

764

