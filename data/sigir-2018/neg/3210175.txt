Demonstration Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Dynamic Composition of Question Answering
Pipelines with Frankenstein
Kuldeep Singh

Fraunhofer IAIS, Germany
kuldeep.singh@iais.fraunhofer.de

Ioanna Lytra

University of Bonn & Fraunhofer
IAIS, Germany
lytra@cs.uni-bonn.de

Akhilesh Vyas

Arun Sethupat Radhakrishna
University of Minnesota, USA
sethu021@umn.edu

Maria-Esther Vidal

Fraunhofer IAIS, Germany
akhilesh.vyas@iais.fraunhofer.de

TIB, Germany
maria.vidal@tib.eu

ABSTRACT

1

Question answering (QA) systems provide user-friendly interfaces
for retrieving answers from structured and unstructured data given
natural language questions. Several QA systems, as well as related
components, have been contributed by the industry and research
community in recent years. However, most of these efforts have
been performed independently from each other and with different focuses, and their synergies in the scope of QA have not been
addressed adequately. Frankenstein is a novel framework for
developing QA systems over knowledge bases by integrating existing state-of-the-art QA components performing different tasks. It
incorporates several reusable QA components, employs machine
learning techniques to predict best performing components and
QA pipelines for a given question, and generates static and dynamic executable QA pipelines. In this paper, we illustrate different
functionalities of Frankenstein for performing independent QA
component execution, QA component prediction, given an input
question as well as the static and dynamic composition of different
QA pipelines.

Question answering (QA) systems combine the expertise of information retrieval, natural language processing, and artificial intelligence to extract answer(s) for user-defined input questions from
the Web of data. Question answering over unstructured data has
been a field of continuous interest for the researchers in the last two
decades [11]. In recent years, question answering over structured
knowledge bases (e.g., DBpedia [1]) has also gained momentum.
This is evident from the fact that more than 62 QA systems have
been published since 2010, which use RDF1 knowledge bases to
find answers of natural language questions [10]. These QA systems
translate an input question into its corresponding formal representation (i.e., SPARQL query) and focus on factoid questions. In
this process, a QA system based on semantic parsing performs
many tasks such as named entity recognition (NER), named entity
disambiguation (NED), relation linking (RL), and query building
(QB). For instance, given the exemplary question “Name the municipality of Roberto Clemente Bridge”, in the first step of a QA
pipeline2 , the named entity recognition and disambiguation component identifies and disambiguates the entity (i.e., Roberto Clemente
Bridge) that appears in the question and links this entity to its corresponding DBpedia mention (i.e., dbr:Roberto_Clemente_Bridge3 ).
In the next step, the relation linker component identifies the natural language relation (i.e., "municipality of") and links it to the
corresponding DBpedia URI (i.e., dbo:municipality4 ). Finally, the
query builder component accepts the input from the previous steps
of a QA pipeline and constructs a SPARQL query that can extract
answers from DBpedia endpoints. Apart from the aforementioned
components, several other components such as natural language
processing components or components for answer generation may
be involved. Besides complete end-to-end QA systems, researchers
have developed various independent tools and components for
individual stages of a QA pipeline.
In this paper, we present a demonstration of Frankenstein
[10], a novel framework for developing QA systems over structured
data. In Frankenstein, state-of-the-art QA components can be
easily integrated, run, and evaluated. Frankenstein uses a formal
methodology named Qanary to integrate different QA components.

CCS CONCEPTS
• Computing methodologies → Natural language processing; Knowledge representation and reasoning;

KEYWORDS
Question Answering, Software Reusability, Semantic Web, Semantic
Search, QA Framework
ACM Reference Format:
Kuldeep Singh, Ioanna Lytra, Arun Sethupat Radhakrishna, Akhilesh Vyas,
and Maria-Esther Vidal. 2018. Dynamic Composition of Question Answering
Pipelines with Frankenstein. In SIGIR ’18: The 41st International ACM
SIGIR Conference on Research and Development in Information Retrieval,
July 8–12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages.
https://doi.org/10.1145/3209978.3210175

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-5657-2/18/07.
https://doi.org/10.1145/3209978.3210175

INTRODUCTION

1

Resource Description Framework (RDF) :https://www.w3.org/RDF/
Please note that a full QA pipeline is composed of all the necessary tasks to
transform a user-supplied textual question into a formal query.
3
The prefix dbr is bound to http://dbpedia.org/resource/
4
The prefix dbo is bound to http://dbpedia.org/ontology/
2

1313

Demonstration Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Qanary [3] is a formal methodology that incorporates standard RDF
technology to integrate heterogeneous components in a common
platform. It relies on an extensible vocabulary (qa [9]) to represent
all the concepts related to the QA process (e.g., question, answer,
the data generated by the components, etc.) in a homogeneous
way. Hence, components can communicate with each other using
a common interface and data format. Currently, Frankenstein
has 29 reusable components integrated into its architecture that
perform various QA tasks. Also, a prediction mechanism based on
machine learning techniques is employed in Frankenstein that
predicts best performing components for the input question based
on the type of question. Frankenstein greedily arranges these
components to form the optimum dynamic pipeline for each input
question given the question’s features. Unlike a monolithic QA system in which all the components are tightly coupled, Frankenstein
follows a modular architecture that allows easy interchangeability,
reusability, and composition of QA components.
The paper is structured as follows: the next section briefly describes Frankenstein architecture and its associated functionality.
Section 3 illustrates several use cases for the demonstration of
Frankenstein. Section 4 describes the relevance of the demonstration for the research community and, finally, conclusions are
discussed in Section 5.

2

components into a QA pipeline in order to extract the answer from
the corresponding knowledge base. Hence, instead of running all
380 viable combinations, Frankenstein just executes in most cases
single (sometimes up to three) QA pipelines. For the prediction
mechanism, Frankenstein builds individual classifiers for each
component in order to predict the performance of that component
based on the characteristics of the input question, determined by
a diverse feature set. To get the concrete representation of an input question, we extract several features such as question length,
question head-word (e.g., wh-word like who, which, what, etc.),
answer type (i.e., boolean, list, or number), and POS tags present
in the question. Researchers from the information retrieval and
natural language processing communities have empirically shown
that these features play an important role in question classification [2, 5, 8], therefore, we reuse these features in our prediction
mechanism to select best component per task for each question.

ARCHITECTURE AND FUNCTIONALITY
OF FRANKENSTEIN

Frankenstein has a modular architecture that integrates QA components as RESTful services in its platform. Currently, Frankenstein has 29 components integrated in its architecture that can
be combined to generate 380 distinct QA pipelines. These components currently present in the repository represent five QA tasks:
Named Entity Recognition (NER), Named Entity Disambiguation
(NED), Relation Linking (RL), Class Linking (CL), and Query Building (QB). Frankenstein open source code is in the repository
https://github.com/WDAqua/Frankenstein.
Furthermore, Frankenstein has three core functionalities namely:
(1) Independent QA Component Execution, (2) QA Component Prediction, and (3) Dynamic and Static QA Pipeline Composition. We
describe them briefly in the following paragraphs.
Independent QA Component Execution. Thanks to the modular and loosely coupled architecture of Frankenstein it is possible to run each QA component independently. That is, if a user
seeks to obtain the intermediate outputs at individual stages (e.g.,
NER, NED tasks) of a QA pipeline, she can just execute any of the
available components for these tasks in isolation. In addition, a user
can execute an end-to-end QA pipeline that comprises several of the
available QA components. Also, some of the available components
(e.g., the NER and NED components) can be used for other purposes
as well such as text analysis.

Static and Dynamic QA Pipeline Composition. Based on
the user preferences, Frankenstein QA pipelines can be executed
in two modes: static and dynamic. In a static pipeline, the user can
choose one component per task and run a complete pipeline. That
is, the user can either choose a random component per task or the
best performing component for each task. Using over than 3,000
questions, we have empirically determined the best QA components within Frankenstein in general but also for specific input
question features [10]. For a dynamic pipeline, the user can choose
components predicted by our prediction mechanism and arrange
these to get the answer(s) for the input question.

3

DEMONSTRATION OF USE CASE

We motivate our demonstration by considering various functionalities of Frankenstein discussed in the previous section. We
consider the following use cases:
Execution of Single QA Component. Using Frankenstein,
a user can execute a single component at a time rather than running
the complete pipeline. Consider, for instance, the question “Name
the municipality of Roberto Clemente Bridge”. If the user seeks to
identify the named entity in this question, she can choose any of
the 11 NER or 9 NED components. For example, if we choose to
run Babelfy5 NED component independently, the sample output of
Babelfy NED component is given below, which is annotated using
the qa vocabulary:
< tag:stardog:api:0 .13501646973289871 >
a < http: // www . wdaqua . eu / qa # AnnotationOfInstance > ;
oa:core / hasTarget _:bnode_5daa0259_7cbe_4e6c_8504 ;
oa:hasBody < http: // dbpedia . org / resource / Clemente >;
< http: // dbpedia . org / resource / Municipality >;
< http: // dbpedia . org / resource / Roberto_Clemente_Bridge >;
< http: // dbpedia . org / resource / Roberto_Clemente >;
oa:annotatedBy < http: // babelfy . org /;
oa:AnnotatedAt " 2017 -10 -02 T13:04:21 " ^^ xsd:dateTime .

QA Component Prediction. Frankenstein has 11 NER, 9
NED, 5 RL, 2 CL, and 2 QB components. For an input question,
it is not an optimum solution to execute all 380 pipelines to get
the answer. Hence, a heuristic approach is required to identify the
optimum pipeline per question. For this, we first predict the best
performing components per task and then greedily arrange these

Here, Babelfy could identify and disambiguate the correct named
entity (dbr:Roberto_Clemente_Bridge6 ), but it also provided three
other entities resulting into precision value 0.25. In this case, the
5
6

1314

http://babelfy.org/
The prefix dbr is bound to http://dbpedia.org/resource/

Demonstration Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

(b)

(a)

(c)

Figure 1: Frankenstein Demonstration: (a) Google search engine does not return any concrete answer for our exemplary question "Name the municipality of Roberto Clemente Bridge". (b) The user can type the question on the UI and run a QA pipeline
composed of the best performing components predicted by the prediction mechanism to get the answer. (c) Frankenstein
dynamic pipeline can get the right answer as can be seen at the screenshot of the Stardog triple store containing the outputs
of the QA components (i.e., SPARQL query, SPARQL answer), expressed using the qa vocabulary.
user can choose another component which correctly identifies the
correct result with precision 1.0. The sample output of Tag Me7
NED component is given below which identifies the named entity
present in the question with precision 1.0.
< tag:stardog:api:0 .13503696973286872 >
a < http: // www . wdaqua . eu / qa # AnnotationOfInstance > ;
oa:core / hasTarget _:bnode_5daa0259_7cbe_4e6c_8504 ;
oa:hasBody
< http: // dbpedia . org / resource / Roberto_Clemente_Bridge >;
oa:annotatedBy < http: // tagme . com /;
oa:AnnotatedAt " 2017 -10 -03 T13:09:24 " ^^ xsd:dateTime .

prediction mechanism predicted Tag Me for the named entity disambiguation task, RNLIWOD component for relation linking, and
NLIWOD component as the best query builder. As illustrated in the
previous section, Tag Me can correctly identify and disambiguate
the entity present in the question, RNLIWOD relation linker and
NLIWOD query builder also perform the corresponding tasks with
precision score 1.0. The sample output of the relation linker is:
< tag:stardog:api:0 .13503696973286872 >
a < http: // www . wdaqua . eu / qa # AnnotationOfRelation > ;
oa:core / hasTarget _:bnode_83fb54ce_bfde_4fa8_8622 ;
oa:hasBody
< http: // dbpedia . org / ontology / municipality >;
oa:annotatedBy < http: // rnliwod . com /;
oa:AnnotatedAt " 2017 -09 -28 T20:20:39 " ^^ xsd:dateTime .

The main advantage of Frankenstein is that it allows users to
identify where a particular component is failing and replace the
erroneous component with a QA component performing the same
task. Such flexibility can not be found in other QA frameworks [10].
Hence, in this use case, the user will also experience the flexibility
provided by the modular architecture of Frankenstein.

Similarly, the predicted query builder correctly formulates the
SPARQL query for the input question (when provided correct input
URLs for the identified entity and relation) as:

Predicting Best QA Component Per Task. We further demonstrate how Frankenstein prediction mechanism can help selecting
the best component per task. For the preparation of the training
data for classifiers, we have used two datasets (QALD8 and LCQuAD9 ) comprising over 3,000 natural language questions. We
have empirically evaluated our approach and detailed evaluation
results can be seen in our recent work [10]. For our exemplary
question “Name the municipality of Roberto Clemente Bridge”, our

< tag:stardog:api:0 .13503696973286872 >
a < http: // www . wdaqua . eu / qa # AnnotationOfAnswerSPARQL > ;
oa:core / hasTarget _: </ qanary_qa / URIAnswer >;
oa:hasBody
SELECT DISTINCT ? uri WHERE
{ < http: // dbpedia . org / resource / Roberto_Clemente_Bridge >
< http: // dbpedia . org / ontology / municipality > ? uri } " ;
oa:annotatedBy ␣ < http: // nliwod . com / >;
oa:AnnotatedAt ␣ " 2017 -10 -05 T18:19:18 " ^^ xsd:dateTime ␣ .

Executing a QA Pipeline. For our exemplary question “Name
the municipality of Roberto Clemente Bridge”, both Apple Siri10

7

https://services.d4science.org/web/tagme
8
https://qald.sebastianwalter.org/index.php?x=home&q=5
9
http://lc-quad.sda.tech/

10

1315

https://www.apple.com/ios/siri/ (last accessed: 2nd Feb 2018)

Demonstration Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

and Google search engine11 are not able to return the right answer. However, at least one Frankenstein pipeline can retrieve
the right answer which is, in this case, Pittsburgh, Pennsylvania
(dbr:Pittsburgh,_Pennsylvania). For this, the user can reuse the
predicted best QA components in the previous use case and run
the Frankenstein dynamic pipeline to extract the right answer.
Furthermore, Frankenstein has a UI where the user can type
a question and select the components to be included in the QA
pipeline. Besides running a complete QA pipeline, the user can
also run parts of a QA pipeline. For instance, AGDISTIS is a NED
component that accepts a natural language question and recognised
spots of entities in the question as input. To get spotted entities
in the question, an NER component can be executed along with
AGDISTIS named entity disambiguator [10].

this platform just by following simple configuration steps. Overall, Frankenstein promotes reusability of components and tools
performing different QA tasks by integrating them into a single
platform. Question Answering is a domain which is driven by different fields, consequently, it requires a collaborative approach to
achieve significant progress. Hence, by reusing infrastructure and
tools provided by Frankenstein, researchers can build QA systems in collaboration with a focus on individual stages of QA tasks,
and reuse components for other tasks from the Frankenstein
repository. We believe Frankenstein and its approach towards
promoting reusability for QA components will trigger discussion
among researchers to explore possibilities of reusability of research
and components developed by the information retrieval community.

ACKNOWLEDGMENT
4

This work has received funding from the EU H2020 R&I programme
for the Marie Skłodowska-Curie action WDAqua (GA No 642795).

RELEVANCE FOR THE RESEARCH
COMMUNITY

Frankenstein is part of our broader research agenda to build an
infrastructure for creating QA systems bringing together collaborative efforts rather developing new QA systems from scratch. Our
vision is initiated by the fact that so far the research community has
focused deeply on various QA tasks such as question classification,
named entity recognition and disambiguation, relation extraction
and has released many independent components and tools accomplishing these tasks. Combining these tools in a single platform
eventually leads to the development of modular and reusable QA
systems where researchers can reuse few components and focus on
building specific components for other tasks. OKBQA is a similar
effort that has marked its presence in SIGIR 2017 by organising
a successful workshop [4]. We have reused several components
from OKBQA [6] in Frankenstein. In Frankenstein use-cases,
researchers working in the domain of QA can foresee how to build
QA systems in collaboration. For the researcher who develops new
tools for information retrieval and natural language processing,
this work is equally interesting for exploring possibilities to reuse
their work in the context of QA.
Furthermore, question classification has attracted a significant
amount of attention by the information retrieval researchers in
recent years [2, 7]. To the best of our knowledge, for the first time
research results of question classification have been reused for
predicting the best performing components per task within a QA
framework, which focuses on developing QA systems for structured
data. Therefore, our work is also in the interest of the researchers
working in this domain.

5

REFERENCES
[1] Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak,
and Zachary G. Ives. 2007. DBpedia: A Nucleus for a Web of Open Data. In The
Semantic Web, 6th International Semantic Web Conference, 2nd Asian Semantic
Web Conference, ISWC 2007 + ASWC 2007, Busan, Korea, November 11-15, 2007.
[2] Phil Blunsom, Krystle Kocik, and James R. Curran. 2006. Question classification
with log-linear models. In SIGIR 2006: Proceedings of the 29th Annual International
ACM SIGIR Conference on Research and Development in Information Retrieval,
Seattle, Washington, USA, August 6-11, 2006. ACM, 615–616.
[3] Andreas Both, Dennis Diefenbach, Kuldeep Singh, Saeedeh Shekarpour, Didier
Cherix, and Christoph Lange. 2016. Qanary - A Methodology for VocabularyDriven Open Question Answering Systems. In The Semantic Web. Latest Advances
and New Domains - 13th International Conference, ESWC 2016, Heraklion, Crete,
Greece, May 29 - June 2, 2016, Proceedings. Springer, 625–641.
[4] Key-Sun Choi, Teruko Mitamura, Piek Vossen, Jin-Dong Kim, and AxelCyrille Ngonga Ngomo. 2017. SIGIR 2017 Workshop on Open Knowledge Base and
Question Answering (OKBQA2017). In Proceedings of the 40th International ACM
SIGIR Conference on Research and Development in Information Retrieval (SIGIR ’17).
ACM, New York, NY, USA, 1433–1434. https://doi.org/10.1145/3077136.3084372
[5] Zhiheng Huang, Marcus Thint, and Zengchang Qin. 2008. Question Classification
using Head Words and their Hypernyms. In 2008 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2008, Proceedings of the Conference, 25-27
October 2008, Honolulu, Hawaii, USA, A meeting of SIGDAT, a Special Interest
Group of the ACL. ACL, 927–936.
[6] Jin-Dong Kim, Christina Unger, Axel-Cyrille Ngonga Ngomo, André Freitas,
Young-gyun Hahm, Jiseong Kim, Sangha Nam, Gyu-Hyun Choi, Jeong-uk Kim,
Ricardo Usbeck, et al. 2017. OKBQA Framework for collaboration on developing
natural language question answering systems. (2017). http://sigir2017.okbqa.org/
papers/OKBQA2017_paper_9.pdf
[7] Babak Loni. 2011. A survey of state-of-the-art methods on question classification.
(2011). Literature Survey, Published on TU Delft Repository.
[8] Muhammad Saleem, Samaneh Nazari Dastjerdi, Ricardo Usbeck, and AxelCyrille Ngonga Ngomo. 2017. Question Answering Over Linked Data: What is
Difficult to Answer? What Affects the F scores?. In Joint Proceedings of BLINK2017:
2nd International Workshop on Benchmarking Linked Data and NLIWoD3: Natural Language Interfaces for the Web of Data co-located with 16th International
Semantic Web Conference (ISWC 2017), Vienna, Austria, October 21st - to - 22nd,
2017. CEUR-WS.org. http://ceur-ws.org/Vol-1932/paper-02.pdf
[9] Kuldeep Singh, Andreas Both, Dennis Diefenbach, and Saeedeh Shekarpour. 2016.
Towards a Message-Driven Vocabulary for Promoting the Interoperability of
Question Answering Systems. In Tenth IEEE International Conference on Semantic
Computing, ICSC 2016, Laguna Hills, CA, USA, February 4-6, 2016. IEEE Computer
Society, 386–389. https://doi.org/10.1109/ICSC.2016.59
[10] Kuldeep Singh, Arun Sethupat Radhakrishna, Andreas Both, Saeedeh Shekarpour,
Ioanna Lytra, Ricardo Usbeck, Akhilesh Vyas, Akmal Khikmatullaev, Dharmen
Punjani, Christoph Lange, Maria Esther Vidal, Jens Lehmann, and Sören Auer.
2018. Why Reinvent the Wheel–Let’s Build Question Answering Systems Together. In The Web Conference (WWW 2018).
[11] Ellen M. Voorhees. 2001. Question Answering in TREC. In Proceedings of the 2001
ACM CIKM International Conference on Information and Knowledge Management,
Atlanta, Georgia, USA, November 5-10, 2001. 535–537. https://doi.org/10.1145/
502585.502679

CONCLUSIONS

In this paper, we present several use cases to illustrate functionalities of the Frankenstein framework. Frankenstein is the first
framework that allows dynamic composition of QA pipelines by
considering the type of the input question and selecting the best
components per QA task for this question. The user can also run
individual QA components independently of other components
based on the user’s requirements as all components are integrated
as RESTful services in Frankenstein. The modular architecture
of Frankenstein allows developers to add more components to
11

https://www.google.de/ (last accessed: 2nd Feb 2018)

1316

