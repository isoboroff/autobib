Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Deep Domain Adaptation Based on Multi-layer Joint Kernelized
Distance
Sitong Mao, Xiao Shen, Fu-lai Chung
Department of Computing, Hong Kong Polytechnic University
Hong Kong, China
sitong.mao@connect.polyu.hk,xiao.shen@connect.polyu.hk,cskchung@comp.polyu.edu.hk

ABSTRACT

insufficient labels of the target data. Domain adaptation is one of
the fields that are associated with this problem.
Domain adaptation addresses the case that adapts a model trained
on source data to target data from different but related domains. The
importance of domain adaptation has been explored in a series of
applications, such as natural language processing, computational biology, computer vision and information retrieval. Under the setting
of domain adaptation, despite consisting of the same categories, the
source data and the target data are typically distributed differently
which is referred to as domain shift. For instance, in the scenario
of visual domain adaptation, the distribution can be substantially
affected by angle transformation, illumination, or occlusion. Therefore, many approaches to domain adaptation focus on bridging
different domains by reducing their distribution discrepancy.
To combat the performance degradation on target domain arising from domain shift, previous works have explored approaches
in various directions. The common approaches include feature
augmentation, feature transformation, and domain re-sampling. Although these methods have made prominent progress, their shallow
architectures prevent them from achieving more desirable performance. Recently, deep neural networks have been proved having
strong ability of learning transferable features, and this depicts the
potential of empowering domain adaptation with deep learning.
Features in deep neural networks eventually transit from general
to specific as the layer goes higher(deeper), which shows that parameters from lower-layers are general enough to suit both source
and target tasks, while the transferability gap between tasks grows
in higher(deeper) layers. Therefore, the network pre-trained on
source dataset is not likely to be discriminative enough when it is
applied to the target dataset directly. Inspired by such characteristic
of deep neural networks, some hierarchical approaches have been
proposed to boost the generalization performance by reducing the
domain discrepancy in higher layers [3] [4].
However, deep neural networks are hard to train when regularizer is integrated into the loss function, which play a role of
adversarial training, and there are too many parameters needed
to be finetuned very carefully. The key idea in this paper is to add
samples that are most likely to be correctly predicted from each
category to the source domain for more effective transfer. Such
process to control the transfer is carried out in an iterative finetuning manner. Furthermore, the process of selecting target data
is conducted for each category simultaneously, which makes the
proposed framework efficient and accurate.
In the next section, we review some existing works related to
domain adaptation. After that, our proposed method is introduced.
We then evaluate our approach and show the comparative results
before concluding the paper.

Domain adaptation refers to the learning scenario where a model
learned from the source data is applied on the target data which
have the same categories but different distributions. In information
retrieval, there exist application scenarios like cross domain recommendation characterized similarly. In this paper, by utilizing deep
features extracted from the deep networks, we proposed to compute
the multi-layer joint kernelized mean distance between the kth target data predicted as the ith category and all the source data of the
jth category dikj . Then, target data Tm that are most likely to belong
to the ith category can be found by calculating the relative distance
k /Í d k . By iteratively adding T to the training data, the finedii
m
j ij
tuned deep model can adapt on the target data progressively. Our
results demonstrate that the proposed method can achieve a better
performance compared to a number of state-of-the-art methods.

CCS CONCEPTS
• Computing methodologies → Object recognition; Transfer learning;
ACM Reference Format:
Sitong Mao, Xiao Shen, Fu-lai Chung. 2018. Deep Domain Adaptation Based
on Multi-layer Joint Kernelized Distance. In Proceedings of SIGIR ’18. ACM,
New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210155

1

INTRODUCTION

Machine learning has become an essential part of many tasks
recently, including information retrieval as in cross domain recommendation [1] and cross network influence maximization [2].
However, methods that work well rely on the assumption that the
training set and the test set are drawn from the same feature space
and the same distribution, which does not always happen in realistic settings. Hence, to obtain favorable performance on a target
dataset whose feature space or distribution is different from the
source data, one may need to recollect labeled training data manually and then retrain the models on it. However, it is prohibitively
expensive or even impossible to recollect the training data with
label information. Thus, it is important to develop methods that
can borrow prior knowledge to compensate for the unavailable or
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210155

1049

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA

2

RELATED WORK

In standard domain adaptation scenario, labels of target domain
are usually assumed unavailable during training. Daumé III [5] proposed a feature augmentation-based method that maps features to
an augmented space. Another direction is to learn a transformation
under which source and target distributions can be represented
closer [6]. One additional type of approach is making labeled source
instances that are most similar to data in the target domain carry
more weights [7]. Despite the appreciable improvement made by
these methods, they are still limited by the shallow architecture
which cannot effectively learn representative features and suppress
domain-specific variability.
Deep neural networks have recently gained much attention in
many applications for its distinctive power in learning more robust features that are invariant to the differences between tasks.
However, the distribution discrepancy between domains cannot
be effectively minimized in the higher layers. This indeed inspires
exploration of domain adaptation approaches based on deep neural
networks. Most previous works mainly consider the distribution
changes of the features P(X ), namely marginal distribution. Long et
al. [3] propose a Deep Adaptation Network (DAN) which incorporates multi-kernel maximum mean discrepancy (MK-MMD) of the
highest few layers as a regularizer in the CNN loss function. In this
approach, the MK-MMD is computed without considering the labels.
However, it is not clear that under what conditions, the approximately same marginal distributions (P S (T (X )) ≈ P T (T (X ))) can
imply similar conditional distributions (P S (Y |T (X )) ≈ P T (Y |T (X ))).
To address the shortcoming of merely relying on correcting the
domain shifts in marginal distributions, Gong et al. [8] aim to find
conditional transferable components that are invariant across different domains. In [4], Long et al. recently propose a “joint distribution
discrepancy” (JDD) metric, by bounding which together with the
source loss, the conditional distribution discrepancy can be reduced.
To compute JDD, the distribution over the class labels produced by
the pre-trained CNN model is used. However, as aforementioned,
the transferability decreases with CNN layer going higher, and this
may limit the predictive accuracy achieved and affect the transfer
learning performance.
In fact, the model to be proposed in the next section attempts to
alleviate the mis-prediction problem of target labels arising from
domain shifts. Quite different from previous approaches, we make
use of joint multi-layer kernelized distance to identify the target
samples that are most likely to be correctly predicted and utilize
them to carry out transfer learning. In this way, the domain distribution discrepancy can be reduced for more effective domain
adaptation. Such a process is proposed to carry out in rounds so
that a better control of transfer can be achieved. In the proposed
model, selecting correctly predicted target data is conducted for
each category at the same time, by solving which a more accurate and efficient performance than processing multiple categories
together can be obtained.

3

is the number of samples in the source domain, and Y S = {yis }, i =
1, 2, . . . , ns , yis ∈ {1, 2, . . . , c}, where c is the number of categories.
The number of examples in T is denoted as nt .
In the following, the “multi-layer joint distance” is firstly demonstrated. Then, the method of selecting data that are most likely
to be predicted correctly and the iterative finetuning process are
described.

3.1

Multi-layer Joint Distance

s ℓ , x t ℓ extracted
Given the source and target deep features x im
jm
from different layers ℓ = ℓ1 , . . . , ℓ2 of the pre-trained deep neural
network. Here m = 1, 2, . . . , c denotes the ground truth label of the
s ℓ denotes
source data and the predicted label of the target data. x im
the deep feature of the ith source data of the mth category extracted
t ℓ denotes the deep feature of the jth target
from layer ℓ, and x jm
data of the mth category extracted by layer ℓ. Many previous works
measure the discrepancy between two distributions by reflecting
the deep features in the Reproducing Kernel Hilbert Space Hk ,
where the inner product is defined as < ϕ(x), ϕ(x ′ ) >= k(x, x ′ ) and
k(·, ·) is the kernel function. In the proposed method, instead of
calculating the distance between all the target data and the source
data, we consider each target data as an individual distribution.
t and a source category
Then, the distance between a target data x jm
′
m = 1, . . . , c is defined as:
ℓ2
ℓ2
t
s
tℓ
sℓ
2
dk2 (x jm
, xm
ϕ ℓ (x jm
)] − E[⊗ℓ=ℓ
ϕ ℓ (xm
′ ) = ∥E[⊗
′ )]∥ ℓ . (1)
ℓ=ℓ1
Hk
1
ℓ2
Here, ⊗ℓ=ℓ
ϕ ℓ (x ℓ ) = ϕ ℓ1 (x ℓ1 )⊗· · ·⊗ϕ ℓ2 (x ℓ2 ) is the joint embedding
1
feature maps of different layers. And the inner product in this joint
embedding Reproducing Hilbert Space is defined as
ℓ2
ℓ2
< ⊗ℓ=ℓ
ϕ ℓ (x ℓ ), ⊗ℓ=ℓ
ϕ ℓ (x ′ℓ ) >=
1
1

ℓ2
Ö

k ℓ (x ℓ , x ′ℓ ).

(2)

ℓ=ℓ1

t and source category m ′ is
Then the distance between x jm
t
s
dk2 (x jm
, xm
′)

=

ℓ2
Ö
ℓ=ℓ1

tℓ
tℓ
k ℓ (x jm
, x jm
)+
s
nm
′

ℓ2
Õ Ö

1
s )2
(nm
′

i,k =1 ℓ=ℓ1

sℓ
sℓ
k ℓ (x im
′ , x km ′ )−

(3)

s

nm ′ ℓ2
Ö
2 Õ
tℓ
sℓ
k ℓ (x jm
, x im
′ ).
s
n
m ′ i=1 ℓ=ℓ1

3.2

Overall Adaptation Process

3.2.1 Selecting Data for Adaptation. Given target data predicted
t ℓ , j = 1, 2, . . . , nt , ℓ = ℓ , . . . , ℓ , the
to be the mth category x jm
1
2
m
t and the m ′ source category
distance between each target data x jm

t , x s ) can be computed by the method described previously.
dk2 (x jm
m′
t is defined as
Then, the relative distance between x jm

METHODOLOGY

In the standard settings of unsupervised domain adaptation, each
category has labeled source dataset S = {X S , Y S } and un-labeled
target data T = {X T }. Here X S = {x is }, i = 1, 2, . . . , ns , where ns

t
R(x jm
)=

1050

c d 2 (x t , x s )
Õ
k jm m
.
2 (x t , x s )
d
m=1 k jm m ′

(4)

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Deep Domain Adaptation Based on Multi-layer Joint Kernelized Distance
t will be close to
Intuitively, the correctly predicted target data x jm
s
s
′
t ) is, the more
xm and far from xm ′ , m , m . So the smaller R(x jm
t belongs to the mth category. Then in each category, we
likely x jm
select top k target data that are most likely to be correctly predicted
to add to the training set. Here, k could be selected according to the
amount of data in the test set. For larger test set, we could choose
larger k.

4.1

3.2.2 Iterative Implementation. The proposed method is an iterative design with the involved deep neural network (CNN in this
paper) fine-tuned with updated training data. At start, a pre-trained
network model M 0 is fine-tuned by the original labeled samples
from the source domain. The trained network is then used to classify
the unlabeled samples of the target domain. The label information
together with the deep neural network extracted features x s ℓ and
x t ℓ are fed to the multi-layer joint kernelized distance module to
compute the correctly predicted ranking for each individual category (label). Based on the ranking information, a portion of the
target samples are deemed as correctly labeled data, which can then
be added to the source samples to form the updated labeled training
set. At the same time, such auxiliary data are removed from the
unlabeled target sample set to prepare for the next round of model
training (fine-tuning) and correctly predicted target data selection.
This process will be repeated in rounds until there is no remaining target data for adding to the training dataset, or enough target
data have been incorporated. The integrated procedure has been
described by Algorithm 1.

4.1.1 Network Architecture. In the experiment discussed here,
we implement our method with AlexNet [9]. The AlexNet contains
eight layers with weights. The first five layers are convolutional
layers and the last three layers are fully-connected. This architecture requires constant size of inputs, so all the images are rescaled
to 227 × 227 × 3-dimensional before being fed to it. The number of
neurons in the fully-connected layer fc6 and fc7 are all 4096, and in
fc8 it is equal to the number of categories in the dataset.
4.1.2 Fine-tuning. The initial model M0 is fine-tuned on the
source domain from AlexNet pre-trained on ImageNet utilizing the
framework Caffe [10]. In the standard fine-tuning procedure, the
first three convolutional layers are frozen. The learning rate for
conv4 − f c7 is set to a small number to slightly tune the parameters
initialized from the pre-trained model. The learning rate for f c8
can be set larger, usually 10 times that of conv4 − f c7. We used the
stochastic gradient descent (SGD) update strategy with a momentum of 0.9. The learning rate is initialized from the range 0.0001 to
0.0003, and will be reduced by multiplying a factor γ at the order
of 10−3 gradually. To make the proposed approach directly compare with previous methods, we follow an unsupervised domain
adaptation protocol, in which all labeled source domain samples
are used for training the initial network, while labels of all target
examples are not provided [3]. To make sure that our approach is
feasible to implement, the parameters such as the initial learning
rates and the batch size are constant in each round.

Algorithm 1 Deep Domain Adaptation Based on MJKD
Input:
1: Source data S = {X S , Y S }
2: Source labels Y S = {y s }, s = 1, 2, . . . , c
3: Target data T = {X T }
4: Given pre-trained model M
Procedure:
5: Get the initial model M 0 by fine-tuning from M on S;
6: for p = 0; T , ϕ and not enough integrated data; p + + do
7:
Apply Mp on T ;
p
8:
Find correctly predicted target data {Ni } by using the RMJKD;
Ð p
p
9:
S = S {Ni }; T = T − {Ni };
10:
Update the network: Mp+1 = Finetune(Mp , S);
11: end for
Output:
12: The classification of target samples.

4

Experimental Setup

Office-31 dataset. The Office-31 dataset [6] contains images originated from three domains: Amazon, Webcam, and DSLR. These
three domains consist of same set of categories, i.e., 31 categories.
Amazon(A) contains images downloaded from online merchants1 .
These images are product shots at medium resolution typically
taken in an environment with studio lighting conditions without
redundant background. DSLR(D) consists of images that were captured with a digital SLR camera in realistic environments with natural lighting conditions. The images have high resolution and low
noise. Webcam(W) consists of images from a webcam. These images
are of low resolution, show significant noise, as well as white balance artifacts. These three domains in Office-31 database represents
several interesting visual domain shifts. Hence, we can evaluate the
proposed approach on 6 transfer tasks: W → D, W → A, D → W ,
D → A, A → W , and A → D.

4.2

Results

We compare our results with six existing methods: TCA [11],
GFK [12], LapCNN [13], DDC [14], DAN [3], and JAN, JAN-A [4].
The results obtained for the Office-31 dataset are summarized in
Table 1. Obviously, the proposed method achieves better accuracy
than the previous state-of-the-art methods in each transfer task
as highlighted in bold. The second best results are underlined. We
can observe that JAN which jointly reduces domain discrepancy
by considering conditional distribution performs better than DAN
which only relies on correcting marginal distribution shifts. The
proposed model which minimizes the prediction error of the target

EXPERIMENTS

In this section, we evaluate the proposed approach by employing
the convolutional neural networks (CNNs) for classification and
compare the results to some state-of-the-art methods in a wellknown domain adaptation dataset. More results in the context of
information retrieval will be reported in the online archive. To
investigate from which layer to extract the features for adaptation,
we consider features from different layers.

1 www.amazon.com

1051

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
Table 1: Comparison of accuracy on Office-31 dataset under conventional unsupervised domain adaptation settings.

Method

W→D

W→A

D→W

D→A

A→W

A→D

TCA
GFK
AlexNet
LapCNN
DDC
DAN
JAN
JAN-A

95.2
95.0
99.0
99.1
98.5
99.0
99.5
99.6

50.9
48.1
49.8
48.2
52.2
53.1
55.0
56.3

93.2
95.6
95.1
94.7
95.0
96.0
96.6
96.6

51.6
52.4
51.1
51.6
52.1
54.0
58.3
57.5

61.0
60.4
71.6
60.4
61.8
68.5
74.9
75.2

60.8
60.6
63.8
63.1
64.4
67.0
71.8
72.8

MJKD

99.8

56.4

97.0

59.3

77.0

73.7

labels can achieve distinctive performance, though there are mislabeled target data added to the training set in each round.

(a) Initial source(A) features

networks to adapt the training set from the source domain to the
target domain. Different from previous works, the proposed method
makes use of multi-layer joint distance to control the transfer in
an iterative manner. Such an approach can enhance the accuracy
of predicting the target labels and thereby effectively avoid potentially ineffective or negative transfer of mis-predicted samples.
Our results show that the proposed method can outperform the
state-of-the-art methods adopted in our experiments.

ACKNOWLEDGEMENT

(b) Initial target(D ) features

This work was supported by UGC GRF Project No. PolyU 152039/14E.

REFERENCES

(c) Final source(A) features

[1] Aleksandr Farseev, Ivan Samborskii, Andrey Filchenkov, and Tat-Seng Chua.
Cross-domain recommendation via clustering on multi-layer graphs. In SIGIR,
pages 195–204. ACM, 2017.
[2] Xiao Shen, Fu-lai Chung, and Sitong Mao. Leveraging cross-network information
for graph sparsification in influence maximization. SIGIR ’17, pages 801–804.
ACM.
[3] Mingsheng Long and Jianmin Wang. Learning transferable features with deep
adaptation networks. CoRR, abs/1502.02791, 1:2, 2015.
[4] Mingsheng Long, Jianmin Wang, and Michael I Jordan. Deep transfer learning
with joint adaptation networks. ICML, 2017.
[5] Hal Daumé III. Frustratingly easy domain adaptation. arXiv preprint
arXiv:0907.1815, 2009.
[6] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual
category models to new domains. Computer Vision–ECCV 2010, pages 213–226,
2010.
[7] Boqing Gong, Kristen Grauman, and Fei Sha. Connecting the dots with landmarks:
Discriminatively learning domain-invariant features for unsupervised domain
adaptation. In ICML, pages 222–230, 2013.
[8] Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and
Bernhard Schölkopf. Domain adaptation with conditional transferable components. In ICML, pages 2839–2848, 2016.
[9] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification
with deep convolutional neural networks. In Advances in Neural Information
Processing Systems, pages 1097–1105, 2012.
[10] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long,
Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional
architecture for fast feature embedding. In ACM International Conference on
Multimedia, pages 675–678. ACM, 2014.
[11] Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang. Domain adaptation via transfer component analysis. IEEE Transactions on Neural Networks,
22(2):199–210, 2011.
[12] Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic flow kernel for
unsupervised domain adaptation. In CVPR, pages 2066–2073. IEEE, 2012.
[13] Jason Weston, Frédéric Ratle, Hossein Mobahi, and Ronan Collobert. Deep
learning via semi-supervised embedding. In Neural Networks: Tricks of the Trade,
pages 639–655. Springer, 2012.
[14] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell.
Deep domain confusion: Maximizing for domain invariance. arXiv preprint
arXiv:1412.3474, 2014.

(d) Final target(D ) features

Figure 1: Top 2 images are t-SNE embeddings of deep features extracted by the initial model for task A→D. Bottom 2 images are
t-SNE embeddings of features extracted the model learned by the
proposed approach.

4.3

Qualitative Analysis

To illustrate how the proposed approach makes the network
more discriminative on target domain by reducing the discrepancy,
we plot the t-SNE embeddings of deep features extracted from the
initial model and the model obtained from the last iteration. Let
us take the task A→D as an example. The top 2 plots in Figure 1
show the source and target deep features extracted by the initial
network while the bottom 2 plots show the features extracted by
the final model learned by the proposed approach. We plot the
features from ten categories for each domain. Each class is marked
with a number as shown in the figures. We can observe that the distribution discrepancy between source features and target features
are reduced.

5

CONCLUSION

In this paper, we present a new domain adaptation approach
which can be applied to different IR tasks by employing deep neural

1052

