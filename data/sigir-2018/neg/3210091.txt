Short Research Papers II

SIGIRâ€™18, July 8-12, 2018, Ann Arbor, MI, USA

Who is the Mr. Right for Your Brand? â€“ Discovering Brand Key
Assets via Multi-modal Asset-aware Projection
Yang Liu

Tobey H. Ko

Zhonglei Gu

Department of Computer Science
Hong Kong Baptist University
Hong Kong SAR, China
IRACE, HKBU, Shenzhen, China
csygliu@comp.hkbu.edu.hk

Department of Industrial and
Manufacturing
Systems Engineering
The University of Hong Kong
Hong Kong SAR, China
tobeyko@hku.hk

Department of Computer Science
Hong Kong Baptist University
Hong Kong SAR, China
cszlgu@comp.hkbu.edu.hk

ABSTRACT

1

Following the rising prominence of online social networks, we
observe an emerging trend for brands to adopt influencer marketing, embracing key opinion leaders (KOLs) to reach potential customers (PCs) online. Owing to the growing strategic
importance of these brand key assets, this paper presents a
novel feature extraction method named Multi-modal Assetaware Projection (M2 A2 P) to learn a discriminative subspace
from the high-dimensional multi-modal social media data for
effective brand key asset discovery. By formulating a new
asset-aware discriminative information preserving criterion,
M2 A2 P differentiates with the existing multi-model feature
extraction algorithms in two pivotal aspects: 1) We consider
brandâ€™s highly imbalanced class interest steering towards the
KOLs and PCs over the irrelevant users; 2) We consider a
common observation that a user is not exclusive to a single class (e.g. a KOL can also be a PC). Experiments on
a real-world apparel brand key asset dataset validate the
effectiveness of the proposed method.

More and more brands today embrace influencer marketing
to deliver brand messages over social networks. As it is impossible to reach every consumer in the market, brands are
targeting the key asset - potential customers (PCs) whose
taste match what the brand delivers - to maximize their
profitability. The brandâ€™s major collaborators in influencer
marketing are the key opinion leaders (KOLs), who are another brand key asset that bridges the brand with its PCs.
Given that the effectiveness of the influencer marketing campaign relies on selecting the right key assets, brands are not
hesitant in investing on high caliber specialists capable of
accurately identifying the key assets from an ocean of other
irrelevant users (IUs). Despite its relatively high accuracy,
one major drawback of manual identification is its scalability.
As the number of users grows, the time and money required
to conduct manual identification is likely to explode. The eminent need for more cost effective methods for brand key asset
discovery thus prompts for the proposal of computational
methods for PC and KOL discovery [5, 9].
The brand key asset discovery task is exceptionally challenging due to the complexity (i.e. high-dimensionality) and
the diversity (i.e. multiple modalities) of the social media data.
To address the challenges, we propose a novel method dubbed
Multi-modal Asset-aware Projection (M2 A2 P) to learn compact and discriminative features from high-dimensional multimodal social media data. M2 A2 P differentiates from the
state-of-the-art multi-modal/multi-view feature extraction
algorithms [3, 4, 11] by considering two pivotal characteristics of the brand key asset discovery task: 1) As brands are
generally more interested in KOLs and PCs than IUs, the
imbalanced interests on different classes are modeled; 2) By
observing that a brandâ€™s KOL can simultaneously be a PC,
signaling a correlation between these two groups of users, the
non-exclusivity of brand key assets is explored. As shown in
Figure 1, the goal of M2 A2 P is to learn ğ‘€ transformation
matrices W1 (for Modality 1), . . . , Wğ‘€ (for Modality ğ‘€ ) to
project the high-dimensional multi-modal data to a common
subspace, where the data within the KOL/PC class are expected to be close to each other while the data from different
classes are expected to be faraway from each other. Since
the brand key assets, i.e., KOLs and PCs, correlate with
each other in some extent, M2 A2 P tolerates the overlapping
between KOLs and PCs in the learned subspace.

KEYWORDS
Key Opinion Leader, Potential Customer, Brand Key Asset
Discovery, Multi-modal Asset-aware Projection
ACM Reference Format:
Yang Liu, Tobey H. Ko, and Zhonglei Gu. 2018. Who is the
Mr. Right for Your Brand? â€“ Discovering Brand Key Assets via Multi-modal Asset-aware Projection. In SIGIR â€™18: The
41st International ACM SIGIR Conference on Research and
Development in Information Retrieval, July 8â€“12, 2018, Ann
Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https:
//doi.org/10.1145/3209978.3210091

Permission to make digital or hard copies of all or part of this work
for personal or classroom use is granted without fee provided that
copies are not made or distributed for profit or commercial advantage
and that copies bear this notice and the full citation on the first
page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee. Request permissions
from permissions@acm.org.
SIGIR â€™18, July 8â€“12, 2018, Ann Arbor, MI, USA
Â© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210091

1113

INTRODUCTION

Short Research Papers II

SIGIRâ€™18, July 8-12, 2018, Ann Arbor, MI, USA

where ğ‘¡ğ‘Ÿ(Â·) denotes the trace operation, and
âŸ¨ï¸€
âŸ©ï¸€
ğ´ğ‘–ğ‘— = 1{lğ‘– = lğ‘— } Ã— lğ‘– /||lğ‘– ||, lğ‘— /||lğ‘— || Ã— ğ‘ğ‘–ğ‘— ,
(3)
(ï¸€
âŸ¨ï¸€
âŸ©ï¸€)ï¸€
ğµğ‘–ğ‘— = 1{lğ‘– Ì¸= lğ‘— } Ã— 1 âˆ’ lğ‘– /||lğ‘– ||, lğ‘— /||lğ‘— || Ã— ğ‘ğ‘–ğ‘— ,
(4)
where 1{Â·} is the indicator function
which
equals
one
if
the
arâŸ¨ï¸€ âŸ©ï¸€
gument holds and zero otherwise, Â·, Â· denotes the inner product operation, lğ‘– /||lğ‘– || is the normalized label vector of the
âˆ‘ï¸€
2
ğ‘–-th sample1 , and ğ‘ğ‘–ğ‘— = ğ‘€
ğ‘š=1 ğ‘’ğ‘¥ğ‘(âˆ’||xğ‘–ğ‘š âˆ’xğ‘—ğ‘š || /2ğœğ‘š )/ğ‘€
indicates the closeness of two samples
âˆ‘ï¸€ in the original space,
2
where ğœğ‘š is empirically set by ğœğ‘š = ğ‘›
ğ‘–=1 ||xğ‘–ğ‘š âˆ’ xğ‘–ğ‘šğ¾ || /ğ‘›
with xğ‘–ğ‘šğ¾ being the ğ¾-th nearest neighbor of xğ‘–ğ‘š .
In Eq. (3), we have ğ´ğ‘–ğ‘— = 0 if two samples have different
labels, or both of them are from the class of IUs. This indicates
that Eq. (1) attempts to minimize the asset-aware intraclass scatter for only KOLs and PCs. By doing so, we focus
on bringing together the data samples within the classes of
more interests. Meanwhile, Eq. (2) maximizes the asset-aware
inter-class scatter for different classes. The more distinct two
label vectors from each other, the more effort we put on to
differentiate them in the learned subspace. By using the inner
product to measure the similarity between label vectors, the
correlation between different labels can be well captured [6].
Moreover, if two samples from different classes are close to
each other in the original space, it is relatively hard to classify
them. Therefore, we put more effort on differentiating them in
the learned subspace, which is achieved by incorporating the
closeness indicator ğ‘ğ‘–ğ‘— in ğµğ‘–ğ‘— . Eqs. (1)-(2) could be rewritten
as:
W = arg min ğ‘¡ğ‘Ÿ(Wğ‘‡ Qğ´ W),
(5)

Figure 1: Illustration of the idea of Multi-modal
Asset-aware Projection (M2 A2 P).

2

MULTI-MODAL ASSET-AWARE
PROJECTION

Let ğ’³ be the set of data samples with ğ‘€ modalities: ğ’³ =
{(x11 , ..., x1ğ‘€ ), ..., (xğ‘›1 , ..., xğ‘›ğ‘€ )}, where xğ‘–ğ‘š âˆˆ Rğ·ğ‘š (ğ‘– =
1, ..., ğ‘›, ğ‘š = 1, ..., ğ‘€ ) denotes the feature representation
of the ğ‘–-th data sample (i.e., the ğ‘–-th user) in the ğ‘š-th
modality, ğ‘› denotes the number of data samples in the set,
ğ‘€ denotes the number of modalities, and ğ·ğ‘š denotes the
original dimension of the ğ‘š-th modality. To represent the
data in ğ‘€ modalities, we define X1 = [x11 , ..., xğ‘›1 ], ..., Xğ‘€ =
[x1ğ‘€ , ..., xğ‘›ğ‘€ ]. We further define a label set â„’ with two labels:
{ğ¾ğ‘‚ğ¿, ğ‘ƒ ğ¶}. The label vector associated with the ğ‘–-th user
is represented by a 2-dimensional binary vector lğ‘– , where
lğ‘– = [1, 1]ğ‘‡ if the ğ‘–-th user is both a KOL and a PC for the
brand; lğ‘– = [1, 0]ğ‘‡ if a KOL but not a PC; lğ‘– = [0, 1]ğ‘‡ if a
PC but not a KOL; and lğ‘– = [0, 0]ğ‘‡ if an IU.
Given the above training dataset, M2 A2 P aims to learn
ğ‘€ transformation matrices Wğ‘š âˆˆ Rğ·ğ‘š Ã—ğ‘‘ (ğ‘š = 1, ..., ğ‘€ ),
which are capable of projecting the multi-modal data to
a common subspace ğ’µ = Rğ‘‘ , where both the asset-aware
discrimination and the modality consistency of the dataset
could be preserved.

2.1

W

W = arg max ğ‘¡ğ‘Ÿ(Wğ‘‡ Qğµ W),

(6)

W

where
â¡

W

ğ‘‡
X1 Lğ¼ğ‘›ğ‘‘
11 X1
â¢
.
â¢
.
â£
.
ğ‘‡
Xğ‘€ Lğ¼ğ‘›ğ‘‘
ğ‘€ 1 X1

=
Â·Â·Â·
..
.
Â·Â·Â·

ğ‘‡ ğ‘‡
[W1ğ‘‡ , W2ğ‘‡ , Â· Â·â¤Â· , Wğ‘€
]
ğ‘‡
X1 Lğ¼ğ‘›ğ‘‘
1ğ‘€ Xğ‘€
â¥
.
â¥,
.
â¦
.
ğ¼ğ‘›ğ‘‘
ğ‘‡
Xğ‘€ Lğ‘€ ğ‘€ Xğ‘€

and

Qğ¼ğ‘›ğ‘‘

=

in which ğ¼ğ‘›ğ‘‘ = ğ´, ğµ, and

Lğ¼ğ‘›ğ‘‘
block of the matrix
ğ‘šğ‘œ (ğ‘š, ğ‘œ = 1, ...,â¡ğ‘€ ) is the (ğ‘š, ğ‘œ)-th
â¤
ğ¼ğ‘›ğ‘‘

L

, i.e., L

ğ¼ğ‘›ğ‘‘

=

â¢
â¢
â£

Lğ¼ğ‘›ğ‘‘
11
.
.
.
ğ¼ğ‘›ğ‘‘
Lğ‘€ 1

Â·Â·Â·
..
.
Â·Â·Â·

Lğ¼ğ‘›ğ‘‘
1ğ‘€
. â¥
.
. â¥
. â¦

Here Lğ¼ğ‘›ğ‘‘ is a Lapla-

Lğ¼ğ‘›ğ‘‘
ğ‘€ğ‘€

cian matrix defined as Lğ¼ğ‘›ğ‘‘ = Diagğ¼ğ‘›ğ‘‘ âˆ’ Adjğ¼ğ‘›ğ‘‘ , where
Adjğ¼ğ‘›ğ‘‘ = 1ğ‘€ Ã—ğ‘€ âŠ— Ind (Ind = A, B; A = [ğ´ğ‘–ğ‘— ]ğ‘›Ã—ğ‘› and
B = [ğµğ‘–ğ‘— ]ğ‘›Ã—ğ‘› ) is an ğ‘›ğ‘€ Ã— ğ‘›ğ‘€ matrix, with 1ğ‘€ Ã—ğ‘€ denoting the ğ‘€ Ã— ğ‘€ matrix with all entries being 1 and
the symbol âŠ— denoting the Kronecker product operation
on two matrices,
Diagğ¼ğ‘›ğ‘‘ is a diagonal matrix defined as
âˆ‘ï¸€ğ‘›ğ‘€
ğ¼ğ‘›ğ‘‘
(ğ·ğ‘–ğ‘ğ‘” )ğ‘–ğ‘– = ğ‘—=1 (ğ´ğ‘‘ğ‘— ğ¼ğ‘›ğ‘‘ )ğ‘–ğ‘— (ğ‘– = 1, ..., ğ‘›ğ‘€ ).

Asset-aware Discrimination
Preserving

To take the imbalanced interests on different classes and
the key asset correlation into consideration, we propose the
following objective functions for asset-aware discrimination
preserving:

2.2

Modality Consistency Preserving

ğ‘€
ğ‘›
(ï¸ âˆ‘ï¸
âˆ‘ï¸

)ï¸ In multi-modal learning, various modalities actually characğ‘‡
ğ‘‡
min ğ‘¡ğ‘Ÿ
ğ´ğ‘–ğ‘— (Wğ‘š
xğ‘–ğ‘š âˆ’Wğ‘œğ‘‡ xğ‘—ğ‘œ )(Wğ‘š
xğ‘–ğ‘š âˆ’Wğ‘œğ‘‡ xğ‘—ğ‘œ )ğ‘‡ ,
terize the same data items. As a result, the consistency among
ğ‘š,ğ‘œ=1 ğ‘–,ğ‘—=1
multiple modalities should be taken into consideration.
(1)
Assume that the representation of the ğ‘œ-th modality can
ğ‘€
ğ‘›
(ï¸ âˆ‘ï¸
)ï¸ be transformed from that of the ğ‘š-th modality by a ğ· Ã—ğ·
âˆ‘ï¸
ğ‘œ
ğ‘š
ğ‘‡
ğ‘‡
ğ‘‡
ğ‘‡
ğ‘‡
ğµğ‘–ğ‘— (Wğ‘š xğ‘–ğ‘š âˆ’Wğ‘œ xğ‘—ğ‘œ )(Wğ‘š xğ‘–ğ‘š âˆ’Wğ‘œ xğ‘—ğ‘œ ) ,
max ğ‘¡ğ‘Ÿ
matrix T, i.e., Xğ‘œ = TXğ‘š (ğ‘š, ğ‘œ = 1, ..., ğ‘€ ). Then Wğ‘š and
ğ‘š,ğ‘œ=1 ğ‘–,ğ‘—=1

(2)

1

1114

We define lğ‘– /||lğ‘– || = [0, 0]ğ‘‡ if lğ‘– = [0, 0]ğ‘‡ .

Short Research Papers II

SIGIRâ€™18, July 8-12, 2018, Ann Arbor, MI, USA

Wğ‘œ should follow the same relationship, i.e., Wğ‘œ = TWğ‘š .
According to the Representer Theorem [8], the columns of
Wğ‘š and Wğ‘œ can be represented by the linear combination
of {x1ğ‘š , ..., xğ‘›ğ‘š } and {x1ğ‘œ , ..., xğ‘›ğ‘œ }, respectively, i.e.,
Wğ‘š = Xğ‘š Î“ğ‘š , Wğ‘œ = Xğ‘œ Î“ğ‘œ .

(7)

Then we have Xğ‘œ Î“ğ‘œ = Wğ‘œ = TWğ‘š = TXğ‘š Î“ğ‘š = Xğ‘œ Î“ğ‘š ,
which indicates Î“ğ‘œ = Î“ğ‘š , or at least Î“ğ‘œ and Î“ğ‘š are similar
(i.e., the columns of (Î“ğ‘œ âˆ’ Î“ğ‘š ) are in the nullspace of Xğ‘œ
and that of Xğ‘š ). To preserve the modality consistency, we
propose the following objective function:
min

ğ‘€
âˆ‘ï¸

||Î“ğ‘š âˆ’ Î“ğ‘œ ||2ğ¹ .

Figure 2: Performance comparison of different feature extraction methods on the apparel brand key
asset dataset.

a 256-D LBP feature vector for each published media; iii)
Textual description: 1 modality of a 373-D TF-IDF feature
vector summarizing the text of userâ€™s full name, bio, and captions of userâ€™s 20 latest published media; and iv) Statistical
information: 1 modality of a 5-D feature vector encapsulating
statistics of userâ€™s published media, followers, followings, as
well as an aggregated number of comments and likes in the
userâ€™s 20 latest published media.

(8)

ğ‘š,ğ‘œ=1

Based on Eq. (7), we rewrite Eq. (8) as follows:
W = arg min ğ‘¡ğ‘Ÿ(Wğ‘‡ CW),

(9)

W

â¡

where C =

â¢
â¢
â£

(ğ‘€ âˆ’ 1)Cğ‘‡
1 C1
.
.
.
âˆ’Cğ‘‡
ğ‘€ C1

Â·Â·Â·
..
.
Â·Â·Â·

â¤
âˆ’Cğ‘‡
1 Cğ‘€
â¥
.
â¥,
.
â¦
.
ğ‘‡
(ğ‘€ âˆ’ 1)Cğ‘€ Cğ‘€

in which

Cğ‘š = (Xğ‘‡ğ‘š Xğ‘š )â€  Xğ‘‡ğ‘š (ğ‘š = 1, Â· Â· Â· , ğ‘€ ) and (Â·)â€  denotes the
Moore-Penrose pseudoinverse.

2.3

3.1

Multi-modal Asset-aware Projection

To preserve both the asset-aware discrimination and the
modality consistency, we integrate the aforementioned three
objectives in Eqs. (5), (6), and (9) to form a unified objective
function of the proposed M2 A2 P:
(ï¸‚
)ï¸‚
W ğ‘‡ Qğµ W
(ï¸€
)ï¸€
,
(10)
W = arg max ğ‘¡ğ‘Ÿ
Wğ‘‡ Qğ´ + ğ›¼C W
W
Wğ‘‡ W=Iğ‘‘

where ğ›¼ âˆˆ [0, +âˆ) is the trade-off parameter to balance
the weight of the asset-aware discrimination and that of the
modality consistency. The constraint Wğ‘‡ W = Iğ‘‘ is introduced to remove the scaling factor, where Iğ‘‘ denotes the
ğ‘‘-dimensional identity matrix. The optimal W that maximizes the objective function in Eq. (10) is composed of
the normalized eigenvectors corresponding to the ğ‘‘ largest
eigenvalues of the following eigen-decomposition problem:
(ï¸€
)ï¸€
Qğµ w = ğœ† Qğ´ + ğ›¼C w.
(11)

3

Quantitative Evaluation

We compare M2 A2 P with PCA [2], LDA [1], multi-label LDA
(MLDA) [10], multi-emotion similarity preserving embedding
(ME-SPE) [6], multi-view discriminant analysis (MvDA) [4],
and multi-view feature learning (MVFL) [11]. We also include the original 1, 914-dimensional data as the baseline.
For PCA, LDA, MLDA, and ME-SPE, we first concatenate
the original features of 10 modalities into a 1, 914-D feature
vector, and then map it to a low-dimensional subspace. For
MvDA, MVFL, and the proposed M2 A2 P, we first map the
original features of 10 modalities to a common subspace, and
then concatenate the reduced representations of these 10
modalities into a single feature vector. For LDA, MvDA, and
MVFL, we transform the labels from [1, 1]ğ‘‡ , [1, 0]ğ‘‡ , [0, 1]ğ‘‡ ,
and [0, 0]ğ‘‡ to 3, 2, 1, and 0, respectively.
We use two groups of criteria for performance evaluation.
i) The label-based metrics: Macro/Micro average precision
and F1 score [12]. The ğ¾-nearest neighbor (ğ¾-NN) classifier
is used for classification after feature extraction. ii) The
example-based metrics: average precision, Hamming loss, oneerror, and ranking loss [12]. The multi-label ğ¾-NN classifier is
used for classification. We set ğ¾ = 10 in our experiments. We
utilize 10% of samples from the dataset for training, and the
rest 90% for testing. We use 10-fold cross validation strategy
to select the parameters for all methods. The experiment is
repeated 10 times on randomly selected training/test sets
to obtain the average as the final result. Figure 2 shows
the results of all the algorithms. By jointly learning the
structure of multiple modalities under the criteria of assetaware discrimination preserving and modality consistency
preserving, M2 A2 P achieves the best performance.
We then examine the individual contribution of Assetaware discrimination preserving (referred to as A) and modality Consistency preserving (referred to as C) in M2 A2 P by
adjusting the values of the parameter ğ›¼ (= 0, 1, 108 ). The
performance of M2 A2 P with different ğ›¼s is shown in Table 1.

EXPERIMENTS

In this section, we validate the effectiveness of M2 A2 P on a
real-world apparel brand key asset dataset2 , which is composed of 8, 426 users, including 1, 451 KOLs, 3, 284 PCs, and
4, 320 IUs, where 629 users are labeled as both KOL and
PC. For each user, the original feature dimension is 1, 914,
consisting of 10 modalities with 4 types of information: i)
User profile: 2 modalities of features extracted from the userâ€™s
profile image â€“ a 128-D color histogram feature vector and
a 256-D LBP feature vector; ii) User-generated content: 6
modalities of features popped from the userâ€™s latest 3 published media: a 128-D color histogram feature vector and
2

The dataset is available at: https://goo.gl/ZZ3T9a (feature set) and
https://goo.gl/D2M1EB (label set).

1115

Short Research Papers II

SIGIRâ€™18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: The performance (average precision and ranking loss) of M2 A2 P with different settings of ğ›¼ (= 0, 1, 108 ).

Aver. Precision
Ranking Loss

ğ›¼ = 0 (A)

ğ›¼ = 1 (A, C)

ğ›¼ = 108 (C)

0.875
0.111

0.892
0.095

0.880
0.107

The dominant components with different ğ›¼s are listed in
the bracket of the corresponding cell. We can observe that
the performance of M2 A2 P is comparable to some existing
feature extraction methods even if we only consider a single
component, which validates the rationality of preserving the
asset-aware discrimination and modality consistency when
learning the subspace from high-dimensional multi-modal social media data for brand key asset discovery. By preserving
both components simultaneously, the performance of M2 A2 P
is further improved, which shows the superiority of jointly optimizing the asset-aware discrimination and modality
consistency in a unified framework.

3.2

Figure 3: Three typical examples from the dataset
and the identification results of different feature extraction methods on these examples.
good performance on a real-world apparel brand key asset
dataset. For the future work, we are particularly interested
in incorporating the domain knowledge from different brands
for delivering brand-specific key asset discovery.

Qualitative Analysis

We qualitatively analyze the effectiveness of the proposed
method with several typical users in our dataset. Figure 3
presents the multi-modal information of three typical users,
including (from left to right) their statistical information,
profile images, latest published media, and text highlights.
Here users 1-3 are annotated as KOL, IU, and KOL+PC,
respectively. The identification results of different feature
extraction methods are shown in the last column of Figure 3
âˆš
using different colors (with
and Ã— indicating the correct
and incorrect identification, respectively).
User 1 is a typical KOL for the brand. The massive volume
of followers and media likes together with the high quality
media of herself showcasing different styles of fashion-forward
apparel make her a simple identification task for all the
algorithms. User 2 can easily be identified as irrelevant user
in the manual process due to his low relevance to the brand,
but was wrongly identified by half of the algorithms. One
possible reason might be related to the high occurrence of
the term â€œtravelâ€ in the userâ€™s text highlight, which coincides
with that of many of the brandâ€™s KOLs. M2 A2 Pâ€™s correct
identification of user 3 over all other algorithms shows the
robustness of our method. At a glance, user 3 does not engage
in much interaction with its followers despite a high follower
count, which lead all the other algorithms misidentify the
user. Yet, with her text highlight shown high relevance with
the apparel brand, M2 A2 P was able to uncover the hidden
correlation between different user-generated contents, and
correctly identify her in the experiment.

4

ACKNOWLEDGMENTS
This work was supported in part by the National Natural
Science Foundation of China under Grant 61503317, the
General Research Fund from the Research Grant Council of
Hong Kong under Project RGC/HKBU12202417, and the
Science and Technology Research and Development Fund of
Shenzhen with Project Code JCYJ20170307161544087.

REFERENCES
[1] R. A. Fisher. 1936. The Use of Multiple Measurements in Taxonomic Problems. Ann. Eugen. 7 (1936), 179â€“188.
[2] H. Hotelling. 1933. Analysis of a complex of statistical variables
into principal components. J. Educ. Psychol. 24, 6 (1933), 417â€“
441.
[3] F. Huang, Y. Cheng, C. Jin, Y. Zhang, and T. Zhang. 2017.
Deep Multimodal Embedding Model for Fine-grained Sketchbased Image Retrieval. In Proc. 40th SIGIR. 929â€“932.
[4] M. Kan, S. Shan, H. Zhang, S. Lao, and X. Chen. 2016. MultiView Discriminant Analysis. IEEE Trans. Pattern Anal. Mach.
Intell. 38, 1 (2016), 188â€“194.
[5] Y. Liu, Z. Gu, T. H. Ko, and J. Liu. 2017. Brand Key Asset
Discovery via Cluster-wise Biased Discriminant Projection. In
Proc. IEEE/WIC/ACM Int. Conf. Web Intell. 284â€“290.
[6] Y. Liu, Y. Liu, Y. Zhao, and K.A. Hua. 2015. What Strikes
the Strings of Your Heart? â€“ Feature Mining for Music Emotion
Analysis. IEEE Trans. Affect. Comput. 6, 3 (2015), 247â€“260.
[7] Y. Lu, K. Jerath, and P. V. Singh. 2013. The Emergence of
Opinion Leaders in a Networked Online Community: A Dyadic
Model with Time Dynamics and a Heuristic for Fast Estimation.
Management Science 59, 8 (2013), 1783â€“1799.
[8] B. SchoÌˆlkopf and A. J. Smola. 2001. Learning with Kernels:
Support Vector Machines, Regularization, Optimization, and
Beyond. MIT Press, Cambridge, MA, USA.
[9] H. Sharara, L. Getoor, and M. Norton. 2011. Active Surveying: A
Probabilistic Approach for Identifying Key Opinion Leaders. In
Proc. 22nd IJCAI. 1485â€“1490.
[10] H. Wang, C. Ding, and H. Huang. 2010. Multi-label Linear
Discriminant Analysis. In Proc. 11th ECCV. 126â€“139.
[11] J. Xu, J. Han, and F. Nie. 2017. Multi-view Feature Learning with
Discriminative Regularization. In Proc. 26th IJCAI. 3161â€“3167.
[12] M.-L. Zhang and Z.-H. Zhou. 2014. A Review on Multi-Label
Learning Algorithms. IEEE Trans. Knowl. Data Eng. 26, 8
(2014), 1819â€“1837.

CONCLUSION

In this paper, we presented Multi-modal Asset-aware Projection (M2 A2 P) to learn a discriminative subspace from the
high-dimensional multi-modal social media data for brand
key asset discovery. By capturing both the asset-aware discrimination and the modality consistency, M2 A2 P achieved

1116

