Session 4D: Recommender Systems - Methods

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Learning Contextual Bandits in a Non-stationary Environment
Qingyun Wu, Naveen Iyer, Hongning Wang
Department of Computer Science, University of Virginia
Charlottesville, VA, USA
{qw2ky,nki2kd,hw5x}@virginia.edu

ABSTRACT
Multi-armed bandit algorithms have become a reference solution
for handling the explore/exploit dilemma in recommender systems,
and many other important real-world problems, such as display
advertisement. However, such algorithms usually assume a stationary reward distribution, which hardly holds in practice as users’
preferences are dynamic. This inevitably costs a recommender system consistent suboptimal performance. In this paper, we consider
the situation where the underlying distribution of reward remains
unchanged over (possibly short) epochs and shifts at unknown
time instants. In accordance, we propose a contextual bandit algorithm that detects possible changes of environment based on
its reward estimation confidence and updates its arm selection
strategy respectively. Rigorous upper regret bound analysis of the
proposed algorithm demonstrates its learning effectiveness in such
a non-trivial environment. Extensive empirical evaluations on both
synthetic and real-world datasets for recommendation confirm its
practical utility in a changing environment.

CCS CONCEPTS
• Information systems → Recommender systems; • Theory
of computation → Online learning algorithms; Regret bounds;
• Computing methodologies → Sequential decision making;

KEYWORDS
Non-stationary Bandit; Recommender Systems; Regret Analysis
ACM Reference Format:
Qingyun Wu, Naveen Iyer, Hongning Wang. 2018. Learning Contextual
Bandits in a Non-stationary Environment. In SIGIR ’18: The 41st International
ACM SIGIR Conference on Research & Development in Information Retrieval,
July 8–12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages.
https://doi.org/10.1145/3209978.3210051

1

INTRODUCTION

Multi-armed bandit algorithms provide a principled solution to the
explore/exploit dilemma [2, 3, 14], which exists in many important
real-world applications such as display advertisement [21], recommender systems [18], and online learning to rank [27]. Intuitively,
bandit algorithms adaptively designate a small amount of traffic to
collect user feedback in each round while improving their model
estimation quality on the fly. In recent years, contextual bandit
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210051

495

algorithms [9, 17, 18] have gained increasing attention due to their
capability of leveraging contextual information to deliver better
personalized online services. They assume the expected reward of
each action is determined by a conjecture of unknown bandit parameters and given context, which give them advantages when the
space of recommendation is large but the rewards are interrelated.
Most existing stochastic contextual bandit algorithms assume a
fixed yet unknown reward mapping function [9, 11, 18, 20, 25]. In
practice, this translates to the assumption that users’ preferences
remain static over time. However, this assumption rarely holds in
reality as users’ preferences can be influenced by various internal
or external factors [7]. For example, when a sports season ends
after a championship, seasonal fans might jump over to following a
different sport and not have much interest in the off-season. More
importantly, such changes are often not observable to the learners.
If a learning algorithm fails to model or recognize the possible
changes of the environment, it would constantly make suboptimal
choices, e.g., keep making out-of-date recommendations to users.
In this work, moving beyond a restrictive stationary environment
assumption, we study a more sophisticate but realistic environment
setting where the reward mapping function becomes stochastic
over time. More specifically, we focus on the setting where there
are abrupt changes in terms of user preferences (e.g., user interest
in a recommender system) and those changes are not observable
to the learner beforehand. Between consecutive change points, the
reward distribution remains stationary yet unknown, i.e., piecewise
stationary. Under such a non-stationary environment assumption,
we propose a two-level hierarchical bandit algorithm, which automatically detects and adapts to changes in the environment by
maintaining a suite of contextual bandit models during identified
stationary periods based on its interactions with the environment.
At the lower level of our hierarchical bandit algorithm, a set of
contextual bandit models, referred to as slave bandits, are maintained to estimate the reward distribution in the current environment (i.e., a particular user) since the last detected change point.
At the upper level, a master bandit model monitors the ‘badness’
of each slave bandit by examining whether its reward prediction
error exceeds its confidence bound. If the environment has not
changed, i.e., being stationary since the last change, the probability
of observing a large residual from a bandit model learned from
that environment is bounded [1, 9]. Thus the ‘badness’ of slave
bandit models reflects possible changes of the environment. Once a
change is detected with high confidence, the master bandit discards
the out-of-date slave bandits and creates new ones to fit the new
environment. Consequentially, the active slave bandit models form
an admissible arm set for the master bandit to choose from. At each
time, the master bandit algorithm chooses one of the active slave
bandits to interact with the user, based on its estimated ‘badness’,
and distributes user feedback to all active slave bandit models attached with this user for model updating. The master bandit model

Session 4D: Recommender Systems - Methods

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA

Qingyun Wu, Naveen Iyer, Hongning Wang

maintains its estimation confidence of the ‘badness’ of those slave
bandits so as to recognize the out-of-date ones as early as possible.
We rigorously prove the upper regret
√ bound of our non-stationary
contextual bandit algorithm is O(ΓT S max log S max ), in which ΓT
is the total number of ground-truth environment changes up to
time T and S max is the longest stationary period till time T . This
arguably is the lowest upper regret bound any bandit algorithm can
achieve in such a non-stationary environment without further assumptions. Specifically, the best one can do in such an environment
is to discard the old model and estimate a new one at each true
change point, as no assumption about the change should be made.
This leads to the same upper regret bound achieved in our algorithm. However, as the change points are unknown to the algorithm
ahead of time, any early or late detection of the changes can only
result in an increased regret. More importantly, we prove that if an
algorithm fails to model the changes a linear regret is inevitable.
Extensive empirical evaluations on both a synthetic dataset and
three real-world datasets for content recommendation confirmed
the improved utility of the proposed algorithm, compared with both
state-of-the-art stationary and non-stationary bandit algorithms.

contextual bandit problems. Their algorithm comprises a hypothesis test for linearity followed by a decision to use either the learnt
linear contextual bandit model or a context-free bandit model. But
this algorithm still assumes a stationary environment, i.e., neither
the ground-truth linear model nor unknown models are changing over time. Liu et al. [8] proposed to use cumulative sum and
Page-Hinkley Test to detect √
sudden changes in the environment.
An upper regret bound of O( ΓT T logT ) is proved for one of their
proposed algorithms. However, this work is limited to a simplified
Bernoulli bandit environment. Recently, Luo et al [22] studied the
non-stationary bandit problem and proposed several bandit algorithms with statistical tests to adapt to changes in the environment.
They analyzed various notions of regret including interval regret,
switching regret, and dynamic regret. Hariri et al. [15] proposed
a contextual Thompson sampling algorithm with a change detection module, which involves iteratively applying a combination
of cumulative sum charts and bootstrapping to capture potential
changes of user preference in interactive recommendation. But no
theoretical analysis is provided about this proposed algorithm.

2

We develop a contextual bandit algorithm for a non-stationary environment, where the algorithm automatically detects the changes in
the environment and maintains a suite of contextual bandit models
for each detected stationary period. In the following discussions,
we will first describe the notations and our assumptions about the
non-stationary environment, then carefully illustrate our developed
algorithm and corresponding regret analysis.

3
RELATED WORK

Multi-armed bandit algorithms [3, 4, 9, 11, 18, 20] have been extensively studied in literature. However, most of the stochastic bandit
algorithms assume the reward pertaining to an arm is determined
by an unknown but fixed reward distribution or a context mapping
function. This limits the algorithms to a stationary environment
assumption, which is restrictive considering the non-stationary
nature of many real-world applications of bandit algorithms.
There are some existing works studying the non-stationary bandit problems. A typical non-stationary environment setting is the
abruptly changing environment, or piecewise stationary environment, in which the environment undergoes abrupt changes at unknown time points but remains stationary between two consecutive
change points. To deal with such an environment, Hartland et al.
[16] proposed the γ −Restart algorithm, in which a discount factor γ
is introduced to exponentially decay the effect of past observations.
Garivier and Moulines [10] proposed a discounted-UCB algorithm,
which is similar to the γ −Restart algorithm in discounting the historical observations. They also proposed a sliding window UCB
algorithm, where only observations inside a sliding window are
used to update the bandit model. Yu and Mannor [26] proposed
a windowed mean-shift detection algorithm to detect the potential abrupt changes
in the environment. An upper regret bound of

O ΓT log(T ) is proved for the proposed algorithm, in which ΓT is
the number of ground-truth changes up to time T . However, they
assume that at each iteration, the agent can query a subset of arms
for additional observations. Slivkins and Upfal [23] considered a
continuously changing environment, in which the expected reward
of each arm follows Brownian motion. They proposed a UCB-like
algorithm, which considers the volatility of each arm in such an
environment. The algorithm restarts in a predefined schedule to
account for the change of reward distribution.
Most existing solutions for non-stationary bandit problems focus on context-free scenarios, which cannot utilize the available
contextual information for reward modeling. Ghosh et al. proposed
an algorithm in [13] to deal with environment misspecification in

METHODOLOGY

3.1

Problem Setting and Formulation

In a multi-armed bandit problem, a learner takes turns to interact
with the environment, such as a user or a group of users in a
recommender system, with a goal of maximizing its accumulated
reward collected from the environment over time T . At round t,
the learner makes a choice at among a finite, but possibly large,
number of arms, i.e., at ∈ A = {a 1 , a 2 , . . . , a K }, and gets the
corresponding reward r at , such as a user clicks on a recommended
item. In a contextual bandit setting, each arm a is associated with
a feature vector xa ∈ Rd (∥xa ∥2 ≤ 1 without loss of generality)
summarizing the side-information about it at a particular time point.
The reward of each arm is assumed to be governed by a conjecture
of unknown bandit parameter θ ∈ Rd (∥θ ∥2 ≤ 1 without loss
of generality), which characterizes the environment. This can be
specified by a reward mapping function fθ : r at = fθ (xat ). In a
stationary environment, θ is constant over time.
In a non-stationary environment, the reward distribution over
arms varies over time because of the changes in the environment’s
bandit parameter θ . In this paper, we consider abrupt changes in
the environment [10, 15, 16], i.e., the ground-truth parameter θ
changes arbitrarily at arbitrary time, but remains constant between
any two consecutive change points:
r 0 , r 1 ,· · ·, r tc1 −1 , r tc1 , r tc1 +1 ,· · ·, r tc2 −1 ,· · · ,r tc Γ , r tc Γ +1 ,· · ·, rT
|
{z
} |
{z
}
|
{z
}
distribute by f θc

0

distribute by f θc

where the change points

1

ΓT −1
{tc j }j=1

distribute by f θc

Γ−1

of the underlying reward dis-

ΓT −1
tribution and the corresponding bandit parameters {θc j }j=0
are

496

Session 4D: Recommender Systems - Methods

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Learning Contextual Bandits in a Non-stationary Environment

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
Master Model

Slave Model 1

Lower-Level: The selected
slave model selects the
best arm from the arm
Arm 2
Arm 1
pool based on the upper
Estimated Reward Estimated Reward
confidence bound of its
estimated reward.
Probability

Reward

− xaTt θ t∗c | > ∆ j
j

(2)

Remark 1. The above assumption is general and mild to satisfy in
many practical scenarios, since it only requires a portion of the arms
to have recognizable change in their expected rewards. For example, a
user may change his/her preference in sports news but not in political
news. The arms that do not satisfy Eq (2) can be considered as having
small deviations in the generic reward assumption made in Eq (1). We
will later prove our bandit solution remains its regret scaling in the
presence of such small deviation.

3.2

Reward

...

...

Slave Model N
Badness

.....
Arm K

Estimated Reward

Reward

Figure 1: Illustration of dLinUCB. The master bandit model
maintains the ‘badness’ estimation of slave models over
time to detect changes in the environment. At each round,
the most promising slave model is chosen to interact with
the environment; and the acquired feedback is shared across
all admissible slave models for model update.

Assumption 1. For any two consecutive change points tc j and
tc j+1 in the environment, there exists ∆ j > 0, such that when t ≥ tc j+1
at least ρ (0 < ρ ≤ 1) portion of all the arms satisfy,
j+1

Badness

.....

(1)

in which ηt is Gaussian noise drawn from N (0, σ 2 ), and the superscript ∗ in θ t∗ means it is the ground-truth bandit parameter in
the environment. In addition, we impose the following assumption
about the non-stationary environment, which guarantees the detectability of the changes, and reflects our insight how to detect
them on the fly,

|xaTt θ t∗c

Slave Model 2

Badness

Probability

r t = fθ t (xat ) = xaTt θ t∗ + ηt

Upper-Level: The master
model selects one of the
slave models based on the
lower confidence bound
of their estimated ‘badness’.

Probability

unknown to the learner. We only assume there are at most ΓT − 1
change points in the environment up to time T , with ΓT ≪ T .
To simplify the discussion, linear structure in fθu (xat ) is postulated, but it can be readily extended to more complicated dependency structures, such as generalized linear models [9], without
changing the design of our algorithm. Specifically, we have,

our proposed algorithm can be readily adapted to any other choices
of the slave model. This claim is also supported by our later regret
analysis. As a result, we name our algorithm as Dynamic Linear
Bandit with Upper Confidence Bound, or dLinUCB in short.
In the following, we first briefly describe our chosen slave model
LinUCB. Then we formally define the concept of ‘badness’, based
on which we design the strategy for creating and discarding slave
bandit models. Lastly, we explain how dLinUCB selects the most
promising slave model from the admissible model set. The detailed
description of dLinUCB is provided in Algorithm 1.
Slave bandit model: LinUCB. Each slave LinUCB model maintains all historical observations that the master model has assigned
to it. Based on the assigned observations, a slave model m gets an
estimate of user preference θˆt (m) = At−1 (m)bt (m) [18], in which
Í
At (m) = λI + i ∈Im, t xai xaTi , I is a d × d identity matrix, λ is the
Í
coefficient for L2 regularization; bt (m) = i ∈Im, t xai r ai , and Im,t
is an index set recording when the observations are assigned to the
slave model m up to time t. According to [1], with a high probability 1 − δ 1 the expected reward estimation error of model m is
upper bounded: |rˆat (m) − E[r at ]| ≤ Bt (m, a), in which Bt (m, a) =
q
√ 
| Im, t |
σ 2 d ln(1 + λδ
) + λ ∥xa ∥A−1 (m) . Based on the upper confi1
t
dence bound principle [3], a slave model m takes an action using
the following arm selection strategy (i.e., line 6 in Algorithm 1):

at (m) = arg max xT θˆt (m) + Bt (m, a)
(3)

Dynamic Linear UCB

Based on the above assumption about a non-stationary environment, in any stationary period between two consecutive change
points, the reward estimation error of a contextual bandit model
trained on the observations collected from that period should be
bounded with a high probability [1, 6]. Otherwise, the model’s
consistent wrong predictions can only come from the change of
environment. Based on this insight, we can evaluate whether the
stationary assumption holds by monitoring a bandit model’s reward
prediction quality over time. To reduce variance in the prediction error from one bandit model, we ensemble a set of models, by creating
and abandoning them on the fly.
Specifically, we propose a hierarchical bandit algorithm, in which
a master multi-armed bandit model operates over a set of slave contextual bandit models to interact with the changing environment.
The master model monitors the slave models’ reward estimation
error over time, which is referred to as ‘badness’ in this paper, to
evaluate whether a slave model is admissible for the current environment. Based on the estimated ‘badness’ of each slave model,
the master model dynamically discards out-of-date slave models
or creates new ones. At each round t, the master model selects a
slave model with the smallest lower confidence bound (LCB) of
‘badness’ to interact with the environment, i.e., the most promising
slave model. The obtained observation (xat , r at ) is shared across
all admissible slave models to update their model parameters. The
process is illustrated in Figure 1.
Any contextual bandit algorithm [9, 18, 20, 25] can serve as our
slave model. Due to the simplified linear reward assumption made
in Eq (1), we choose LinUCB [18] for the purpose in this paper; but

a ∈A

a

Slave model creation and abandonment. For each slave bandit
model m, we define a binary random variable ei (m) to indicate
whether the slave model m’s prediction error at time i exceeds its
confidence bound,

ei (m) := 1 |rˆi (m) − r i (m)| > Bi (m, ai ) + ϵ
(4)
√
−1
−1
where ϵ = 2σ erf (δ 1 − 1) and erf (·) is the inverse of Gauss
error function. ϵ represents the high probability bound of Gaussian
noise in the received feedback.
According to Eq (7) in Theorem 3.1, if the environment stays
stationary since the slave model m has been created, we have
P(ei (m) = 1) ≤ δ 1 , where δ 1 ∈ (0, 1) is a hyper-parameter in

497

Session 4D: Recommender Systems - Methods

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA

Qingyun Wu, Naveen Iyer, Hongning Wang

Algorithm 1 Dynamic Linear UCB (dLinUCB)

Eq (5) provides a tight bound to detect changes in the environment. If the environment is unchanged, within a sliding window
the estimation error made by an up-to-date slave model should
not exceed the right-hand side of Eq (5) with a high probability.
Otherwise, the stationary hypothesis has to be rejected and thus
the slave model m should be discarded. Accordingly, if none of the
slave models in the admissible bandit set satisfy this condition, a
new slave bandit model should be created for this new environment. Specifically, the master bandit model controls the slave model
creation and abandonment in the following way.
• Model abandonment: when the slave model m’s estimated ‘badness’
exceeds
q its upper confidence bound defined in Eq (5), i.e., êt (m) >

1:
2:

3:
4:

5:
6:

7:
8:
9:
10:

11:
12:

13:
14:

Inputs: λ > 0, τ > 0, δ 1 , δ 2 ∈ (0, 1), δ˜1 ∈ [0, δ 1 ]
Initialize: Maintain a set of slave models Mt with M1 =
{m 1 }, initialize m 1 : A1 (m 1 ) = λI, b1 (m 1 ) = 0, θˆ1 (m 1 ) = 0; and
initialize the ‘badness’ statistics of it: ê 1 (m 1 ) = 0, d 1 (m 1 ) = 0
for t = 1 to T do
Choose a slave model
√ from the active
 slave model set m̃t =
arg minm ∈Mt êt (m) − ln τ × dt (m)
Observe candidate arm pool At , with xa ∈ Rd for ∀a ∈ At

Take action at = arg maxa ∈At xaT θˆt (m̃t ) + Bt (m̃t , a) , in
which Bt (m̃t , a) is defined in Eq (3)
Observe payoff r at
Set CreatNewFlag = True
for m ∈ Mt do
et (m) = 1{|√rˆt (m) − r t | > Bt (m, a) + ϵ }, where rˆt (m) =
xaTt θˆt (m) and ϵ = 2σ erf−1 (δ 1 − 1)
if et (m) = 0 then
Update slave model: At +1 (m) = At (m) + xat xaTt ,
bt +1 (m) = bt (m) + xat r t , θˆt +1 = At−1
+1 (m)bt +1 (m)
end if
τ̃ (m) = min{t −tm , τ }, where tmÍis when m was created
Update ‘badness’ êt (m) =

15:

q

t
i =t −τ̃

e i (m)
,
τ̃ (m)

ln(1/δ )

δ 1 + 2τ̃ (m)2 , it will be discarded and removed from the admissible
slave model set. This corresponds to line 18-20 in Algorithm 1.
• Model creation: When no slave model’s estimated ‘badness’ is
within its expected
q confidence bound, i.e., no slave model satisfies
ln(1/δ 2 )
êt (m) ≤ δ˜1 +
, a new slave model will be created. δ˜1 ∈
2τ̃ (m)

[0, δ 1 ] is a parameter to control the sensitivity of dLinUCB, which
affects the number of maintained slave models. When δ˜1 = δ 1 , the
threshold of creating and abandoning a slave model matches and
the algorithm only maintains one admissible slave model. When
δ˜1 < δ 1 multiple slave models will be maintained. The intuition is
that an environment change is very likely to happen when all active
slave models face a high risk of being out-of-date (although they
have not been abandoned yet). This corresponds to line 8, 16-17,
and 22-26 in Algorithm 1.
Slave model selection and update. At each round, the master
bandit model selects one active slave bandit model to interact with
the environment, and updates all active slave models with the acquired feedback accordingly. As we mentioned before, with the
model abandonment mechanism every active slave model is guaranteed to be admissible for taking acceptable actions; but they are
associated with different levels of risk of being out of date. A welldesigned model selection strategy can further reduce the overall
regret, by minimizing this risk. Intuitively, when facing a changing environment, one should prefer a slave model with the lowest
empirical error in the most recent period.
The uncertainty in assessing each slave model’s ‘badness’ introduces another explore-exploit dilemma, when choosing the active
slave models. Essentially, we prefer a slave model of lower ‘badness’
with a higher confidence. We realize this criterion by selecting a
slave model according to its Lower Confidence Bound (LCB) of the
estimated ‘badness.’ This corresponds to line 4 in Algorithm 1.
Once the feedback (xat , r t ) is obtained from the environment
on the selected arm at , the master algorithm can not only update
the selected slave model but also all other active ones for both of
their ‘badness’ estimation and model parameters (line 11-13 and
line 15 in Algorithm 1 accordingly). This would reduce the sample
complexity in each slave model’s estimation. However, at this stage,
it is important to differentiate those “about to be out-of-date” models
from the “up-to-date” ones, as any unnecessary model update blurs
the boundary between them. As a result, we only update the perfect
slave models, i.e., those whose ‘badness’ is still zero at this round of
interaction; and later we will prove this updating strategy is helpful
to decrease the chance of late detection.

dt (m) =

ln 1/δ 2
2τ̃ (m)

if êt (m) < δ˜1 + dt (m) then
Set CreatNewFlag = False
else if êt (m) ≥ δ 1 + dt (m) then
Discard slave model m: Mt +1 = Mt − m
end if
end for
if CreateNewFlag or Mt = ∅ then
Create a new slave model mt : Mt +1 = Mt + mt
Initialize mt : At (mt ) = λI, bt (mt ) = 0, θˆt (mt ) = 0
Initialize ‘badness’ statistics of mt : êt (mt ) = 0, dt (mt ) =

16:
17:
18:
19:
20:
21:
22:
23:
24:
25:

0
26:
27:

end if
end for

Bi (m, a). Therefore, if we observe a sequence of consistent prediction errors from the slave model m, it strongly suggests a change of
environment, so that this slave model should be abandoned from
the admissible set. Moreover, we introduce a size-τ sliding window
to only accumulate the most recent observations when estimating
the expected error in slave model m. The benefit of sliding window
design will be discussed
with more details later in Section 3.3.
Í
t

e i (m)

(m)
We define êt (m) := i =t −τ̃τ̃ (m)
, which estimates the ‘badness’
of slave model m within the most recent period τ̃ to time t, i.e.,
τ̃ (m) = min{t − tm , τ }, in which tm is when model m was created.
Combining the concentration inequality in Theorem 7.2 (provided
in the appendix), we have the assertion that if in the period [t −
τ̃ (m), t] the stationary hypothesis is true, for any given δ 1 ∈ (0, 1)
and δ 2 ∈ (0, 1), with a probability at least 1 − δ 2 , the expected
‘badness’ of slave model m satisfies,

s
ê t (m) ≤ E[e t (m)] +

ln(1/δ 2 )
≤ δ1 +
2τ̃ (m)

s

ln(1/δ 2 )
2τ̃ (m)

(5)

498

Session 4D: Recommender Systems - Methods

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Learning Contextual Bandits in a Non-stationary Environment

3.3

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
3.3, we have pe ≤ δ 2 . Combining the property of binomial distribution B(Sc j , pe ) and Chebyshev’s concentration inequality, we have

Regret Analysis

In this section, we provide a detailed regret analysis of our proposed
dLinUCB algorithm. We focus on the accumulated pseudo regret,
which is formally defined as,
R(T ) =

T
Õ
t =1

E[r at∗ ] − E[r at ]



kc j ≤ 2Sc j δ 2 with probability at least 1− 2S1−δδ2 . Hence, with a probcj 2
sc
ÍT
k
S
max
ability (1−δ 3 )×(1−δ 1 )
, we have R early ≤ Γj=1
kc j R Lin ( k j ) ≤
cj
ÍΓT
1 ) ≤ 2δ Γ S R ( 1 ). Considering the calcu2S
δ
R
(
c
2
2
T
T
Lin
Lin
j
j=1
2δ
2δ

(6)

2

where at∗ is the best arm to select according to the oracle of this
problem, and at is the arm selected by the algorithm to be evaluated.
It is easy to prove that if a bandit algorithm does not model the
change of environment, it would suffer from a linearly increasing
regret: An optimal arm in the previous stationary period may become sub-optimal after the change; but the algorithm that does not
model environment change will constantly choose this sub-optimal
arm until its estimated reward falls behind the other arms’. This
leads to a linearly increasing regret in each new stationary period.
Next, we first characterize the confidence bound of reward estimation in a linear bandit model in Theorem 3.1. Then we prove the
upper regret bound of two variants of our dLinUCB algorithm in
Theorem 3.2 and Theorem 3.5. More detailed proofs are provided
in the appendix.
Theorem 3.1. For a linear bandit model m specified in Algorithm
1, if the underlying environment is stationary, for any δ 1 ∈ (0, 1) we
have the following inequality with probability at least 1 − δ 1 ,
|rˆt (m) − r t | ≤ Bt (m, a) + ϵ
(7)
q
| It (m) |
where Bt (m, a) = α t ∥xat ∥A−1 with α t = σ 2 d ln(1 + λδ
)+
1
t −1
√ 
√
−1
λ , ϵ = 2σ erf (δ 1 −1), σ is the standard deviation of the Gaussian
noise in reward feedback, and erf(·) is the Gauss error function.

Lemma 3.3 (Bound the probability of early detection). For
δ 2 ∈ (0, 1) and any slave model in Algorithm 1,
pe = P[êt (m) > δ 1 + dt (m)|stationary in past τ̃ (m) rounds] ≤ δ 2 .

Denote R Lin (S) as the upper regret bound of a linear bandit model
within a stationary
on Theorem 3.1, one can prove
q period S. Based
 q
√ 
S
S ) + λ [1]. In
2
that R Lin (S) ≤ dS log(λ + d ) σ d log(1 + λδ
1
the following, we provide an upper regret bound analysis for the
basic version of dLinUCB, in which the size of the admissible slave
models is restricted to one (i.e., by setting δ˜1 = δ 1 ).
√
Theorem 3.2. When Assumption 1 is satisfied with ∆ ≥ 2 λ + 2ϵ,
if δ 1 and τ in Algorithm 1 are set according to Lemma 3.4, and δ 2 is
δ2
set to δ 2 ≤ 2S1max , with probability at least (1 − δ 1 )(1 − δ 2 )(1 − 1−δ
),
2
the accumulated regret of dLinUCB satisfies,
R(T ) ≤ 2ΓT R Lin (S max ) + ΓT (τ +

4
)
1 − δ2

2

lation of R Lin , when δ 2 ≤ 2S1max we have R early ≤ ΓT R Lin (S max )
with a probability at least (1 − δ 2 )(1 − δ 1 ). This upper bounds the
additional regret from any possible early detection, and maintains
it in the same order as the slave model’s.
Step 3. Define k̃c j as the number of interactions where the environment has changed (comparing to θc∗j ) but the change is not
detected by the algorithm. The additional regret from this late detection can be bounded by 2k̃c j (i.e., the maximum regret in each
round of interaction). Define pd as the probability of detection after
the change happens, we have P(k̃c j = k) = (1 − pd )k −1pd , i.e., a
Geometric distribution. According to Lemma 3.4, pd ≥ 1 −δ 2 . Based
on the property of Geometric distribution G(pd ) and Chebyshev’s
2 with probability 1 − δ 2 . If we
inequality, we have k̃c j ≤ 1−δ
1−δ 2
2
consider the case where the change point locates inside the sliding
window τ , we may have at most another τ delay after each change
point. Therefore, the additional regret from late detection can be
4 , which is not directly related to
bounded by R late ≤ ΓT τ + 1−δ
2
the length of any stationary period.
Combining the above three steps concludes the proof.
□

The intuition behind Lemma 3.3 is that when the environment
is stationary, the ‘badness’ of a slave model m should be small and
bounded according to Eq (5).
Lemma 3.4 (Bound the probability of late detection). When
the magnitude
of change in the environment in Assumption 1 satisfies
√
∆ > 2 λ√+ 2ϵ, and the shortest stationary period length Smin satisfies
√
S min > 2ρλ (∆−2 λ −2ϵ), for any δ 2 ∈ (0, 1), if δ 1 and τ in Algorithm
√
√
2 ln δ2

λ
2
1 are set to δ 1 ≤ 1− ρ1 1− 2S min
(∆−2
λ−2ϵ)
and
τ
≥
,
ρ
(ρ(1−δ 1 )−δ 1 )2
for any slave model m in Algorithm 1, we have,

pd = P êt (m) > δ 1 + dt (m)| changed within past τ̃ (m) ≥ 1 − δ 2 .

(8)
The intuition behind Lemma 3.4 is that when the environment
has changed, with a high probability that Eq (7) will not be satisfied
in an out-of-date slave model. It means that we will accumulate
larger badness from this slave model. In both Lemma 3.3 and 3.4, δ 2
is a parameter controlling the confidence of the ‘badenss’ estimation
in Chernoff Bound; and therefore an input to the algorithm.

where S max is the length of the longest stationary period up to T .
Proof. Step 1: If change points can be perfectly detected, the
Í T −1
regret of dLinUCB can be bounded by Γj=0
R Lin (Sc j ). However,
additional regret may accumulate if early or late detection happens.
In the following two steps, we will bound the possible additional
regret from early detection, denoted as R early , and that from late
detection, denoted as R late .
Step 2: Define kc j as the number of early detection within this
stationary period [tc j , tc j+1 ], with Sc j = tc j+1 − tc j . Define pe as
the probability of early detection in the stationary period, we
S 
S −k
have P(kc j = k) = c j pek (1 − pe ) c j . According to Lemma

Remark 2 (How the environment assumption affects dLinUCB). 1. The magnitude of environment change ∆ affects whether a
change is detectable by our algorithm. However, we need to emphasize
that when ∆ is very small, the additional regret from re-using an
out-of-date slave model is also small. In this case, a similar scale of
regret bound can still be achieved, which will be briefly proved in
Appendix and empirically studied in Section 4.1. 2. We require the

k

499

Session 4D: Recommender Systems - Methods

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA

Qingyun Wu, Naveen Iyer, Hongning Wang

√
√
shortest stationary period length S min > max{ 2ρλ (∆ − 2 λ − 2ϵ), τ },
which guarantees there are enough observations accumulated in a
slave model to make an informed model selection. 3. The portion of
changed arms ρ will affect the probability of achieving
our derived
√
√

λ
1
regret bound, as we require δ 1 ≤ 1 − ρ 1 − 2S min ρ (∆ − 2 λ − 2ϵ) .
ρ also interacts with S min and τ : when ρ is small, more observations
are needed for a slave model to detect the changes. The effect of ρ and
S min will also be studied in our empirical evaluations.

chosen model m̃, this added regret increases much slower than that
resulted from any slave model (i.e., lnT v.s., R Lin (T )); and thus maintaining multiple slave models is always beneficial. Besides,
the order of
√
upper regret bound of dLinUCB in both cases is O(ΓT S max log S max ),
which is the best upper regret bound a bandit algorithm can achieve
in such a non-stationary environment [10], and it matches the lower
bound up to a ΓT log S max factor.
Remark 4 (Generalization of dLinUCB). Our theoretical analysis confirms that any contextual bandit algorithm can be used as the
slave model in dLinUCB, as long as the its reward estimation error is
bounded with a high probability, which corresponds Bt (m, a) in Eq
(7). The overall regret of dLinUCB will only be a factor of the actual
number of changes in the environment, which is arguably inevitable
without further assumptions about the environment.

Theorem 3.2 indicates with our model update and abandonment
mechanism, each slave model in dLinUCB is ‘admissible’ in terms
of upper regret bound. In the following, we further prove that
maintaining multiple slave models and selecting them according to
their LCB of ‘badness’ can further improve the regret bound.
Theorem 3.5. Under the same condition as specified in Theorem
δ2
3.2, with probability at least (1 − δ 1 )(1 − δ 2 )(1 − 1−δ
), the expected
2
accumulated regret of dLinUCB up to time T can be bounded by,
R(T ) ≤

ΓÕ
T −1
j=0

+

R Lin (Sc j ) + 2ΓT (τ +

ΓÕ
T −1
j=0

8

Õ
m ∈M,m,mc∗ j

4
)
1 − δ2

ln Sc j
д

m,mc∗ j

+ (1 +

4

(9)

π2
)дm,mc∗
j
3

in which mc∗j is the best slave model among all the active ones in the
stationary period [tc j , tc j+1 ] according to the oracle, and дm,mc∗ is
j
difference between the accumulated expected reward from the selected
model m and that from mc∗j in the period [tc j , tc j+1 ] .

4.1

ΓÕ
T −1
j=0



G ∗ (Sc j ) − Gmc∗ (Sc j ) + Gmc∗ (Sc j ) − G(Sc j ) (10)
j
j

The first term of Eq(10) can be bounded based on Theorem 3.2. Define Ñc j (m) as the number of times a slave model m is selected when
Ítc j +1
it is not the best in [tc j , tc j +1 ]: Ñc j (m) = t =t
1{m̃t = m, mc∗j ,
cj
Í
m}, we have Gmc∗ (Sc j ) − G(Sc j ) ≤ m ∈M дm,mc∗ E[Ñc j (m)]. In
j

Experiments on synthetic datasets

In simulation, we generate a size-K (K = 1000) arm pool A, in
which each arm a is associated with a d-dimensional feature vector
xa ∈ Rd with ∥xa ∥2 ≤ 1. Similarly, we create the ground-truth
bandit parameter θ ∗ ∈ Rd with ∥θ ∗ ∥2 ≤ 1, which is not disclosed to
the learners. Each dimension of xa and θ ∗ is drawn from a uniform
distribution U (0, 1). At each round t, only a subset of arms in A
are disclosed to the learner for selection, e.g., randomly sample
10 arms from A without replacement. The ground-truth reward
r a is corrupted by Gaussian noise η ∼ N (0, σ 2 ) before being fed
back to the learner. The standard deviation of Gaussian noise σ is
set to 0.05 by default. To make the comparison fair, at each round
t, the same set of arms are presented to all the algorithms being
evaluated. To simulate an abruptly changing environment, after
every S rounds, we randomize θ ∗ with respect to the constraint
|xaT θ t∗c − xaT θ t∗c | > ∆ j for ρ proportion of arms in A. We set λ

Proof. Define the optimal expected cumulative reward in the
stationary period [tc j , tc j +1 ] according to the oracle as G ∗ (Sc j ) and
the expected accumulative reward in dLinUCB as G(Sc j ). Gmc∗ (Sc j )
j
is the expected cumulative reward from mc∗j . The accumulated
regret of dLinUCB can be written as,
R(T ) =

EVALUATIONS

We performed extensive empirical evaluations of dLinUCB against
several related baseline algorithms, including: 1) the state-of-the-art
contextual bandit algorithm LinUCB [18]; 2) adaptive Thompson
Sampling algorithm [15] (named as adTS) which has a change
detection module; 3) windowed mean-shift detection algorithm
[26] (named as WMDUCB1), which is a UCB1-type algorithm with
a change detection module ; and 4) Meta-Bandit algorithm [16],
which switches between two UCB1 models.

j

j+1

to 0.1, S to 800 and ∆ to 0.9 by default.
Under this simulation setting, all algorithms are executed to
5000 iterations and the parameter τ in dLinUCB is set to 200. Accumulated regret defined in Eq (6) is used to evaluate different
algorithms and is reported in Figure 2. The bad performance of
LinUCB illustrates the necessity of modeling the non-stationarity
of the environment – its regret only converges in the first stationary period, and it suffers from an almost linearly increasing regret,
which is expected according to our theoretical analysis in Section
3.3. adTS is able to detect and react to the changes in the environment, but it is slow in doing so and therefore suffers from a linear
regret at the beginning of each stationary period before converging.
dLinUCB, on the other hand, can quickly identify the changes and
create corresponding slave models to capture the new reward distributions, which makes the regret of dLinUCB converge much faster
in each detected stationary period. In Figure 2 we use the black

j

Lemma 3.6, we provide the bound of E[Ñc j (m)]. Substituting the
above conclusions into Eq (10) finishes the proof.
□
Lemma 3.6. The model selection strategy in Algorithm 1 guarantees,
8 ln Sc j
π2
E[Ñc j (m)] ≤ 2
+1+
(11)
3
дm,m ∗
cj

Remark 3 (regret comparison of dLinUCB with one slave
model and multiple slave models). By maintaining multiple
admissible slave models and selecting one according to the LCB of
‘badness’ when interacting with the environment, dLinUCB achieves a
regret reduction in the first part of Eq (9). Although there is additional
regret introduced by switching between the best model m∗ and the

500

Session 4D: Recommender Systems - Methods

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Learning Contextual Bandits in a Non-stationary Environment

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA

Table 1: Accumulated regret with different noise level σ , environment change ∆ and stationary period length S.
(σ , ∆, S)
dLinUCB
adTS
LinUCB
Meta-Bandit
WMDUCB1

(0.1, 0.9, 800)
87.46 ± 3.61
360.75± 39.59
436.84 ± 40.23
1822.31± 80.67
2219.36± 142.16

(0.05, 0.9, 800)
65.94± 2.30
249.63 ± 27.26
386.10±21.88
1340.01±29.94
1652.99± 21.33

(0.01, 0.9, 800)
54.07± 3.95
207.95± 22.28
347.19± 14.95
1354.03 ± 22.29
1635.35 ± 73.96

400

350

300

dLinUCB
adTS
LinUCB

4.2

Actual Changes
Detected Changes

Regret

200

150

100

50

1000

2000

3000

4000

(0.01, 0.1, 800)
46.12 ± 4.63
177.55.±20.36
226.87± 32.15
1402.63 ± 24.85
1506.55 ± 41.52

(0.01, 0.9, 400)
111.72 ± 4.87
412.55 ± 14.53
405.82 ± 33.38
1388.81 ± 115.91
1691.75 ± 48.09

Experiments on Yahoo! Today Module

We compared all the algorithms on the large-scale clickstream
dataset made available by the Yahoo Webscope program. This
dataset contains 45,811,883 user visits to Yahoo Today Module in a
ten-day period in May 2009. For each visit, both the user and each
of the 10 candidate articles are associated with a feature vector of
six dimensions (including a constant bias term) [18]. In the news
recommendation problem, it is generally believed that users’ interests on news articles change over time; and it is confirmed in this
large-scale dataset by our quantitative analysis. To illustrate our
observations, we randomly sampled 5 articles and reported their
real-time click-through-rate (CTR) in Figure 3 (c), where each point
is the average CTR over 2000 observations. Clearly, there are dramatic changes in those articles’ popularity over time. For example,
article 1’s CTR kept decreasing after its debut, then increased in
the next two days, and dropped eventually. Any recommendation
algorithm failing to recognize such changes would suffer from a
sub-optimal recommendation quality over time.
The unbiased offline evaluation protocol proposed in [19] is used
to compare different algorithms. CTR is used as the performance
metric of all bandit algorithms. Following the same evaluation principle used in [18], we normalized the resulting CTR from different
algorithms by the corresponding logged random strategy’s CTR.
We tested two different settings on this dataset based on where to
place the bandit model for reward estimation.
The first setting is to build bandit models for users, i.e., attaching θ on the user side to learn users’ preferences over articles. We
included a non-personalized variant and a personalized variant of
all the contextual bandit algorithms. In the non-personalized variant, the bandit parameters are shared across all users, and thus the
detected changes are synchronized across users. We name the resulting algorithms as uniform-LinUCB, uniform-adTS, and uniformdLinUCB. In the personalized variant, each individual user is associated with an independent bandit parameter θu , and the change
is only about him/herself. Since this dataset does not provide user
identities, we followed [25] to cluster users into N user groups and
assume those in the same group share the same bandit parameter. We name the resulting algorithms as N-LinUCB, N-adTS and
N-dLinUCB. To make the comparison more competitive, we also
include a recently introduced collaborative bandit algorithm CLUB
[12], which combines collaborative filtering with bandit learning.
From Figure 3 (a), we can find that both the personalized and nonpersonalized variants of dLinUCB achieved significant improvement compared with all baselines. It is worth noticing that uniformdLinUCB obtained around 50% improvement against uniform-LinUCB,
15% against N-LinUCB, and 25% against CLUB. Clearly assuming all
the users share the same preference over the recommendation candidates is very restrictive, which is confirmed by the improved performance from the personalized version over the non-personalized

250

0
0

(0.01, 0.5, 800)
44.94 ± 2.90
189.07±18.39
264.87± 21.53
1329.51 ± 18.93
1464.11 ± 89.16

5000

Iteration

Figure 2: Results from simulation.

and blue vertical lines to indicate the actual change points and the
detected ones by dLinUCB respectively. It is clear that dLinUCB
detects the changes almost immediately every time. WMDUCB1
and Meta-Bandit are also compared, but since they are context-free
bandits, they performed much worse than the above contextual
bandits. To improve visibility of the result, we exclude them from
Figure 2 and instead report their performance in Table 1.
As proved in our regret analysis, dLinUCB’s performance depends the magnitude of change ∆ between two consecutive stationary periods, the Gaussian noise σ in the feedback, and the length S
of stationary period. In order to investigate how these factors affect
dLinUCB, we varied these three factors in simulation. We ran all the
algorithms for 10 times and report the mean and standard deviation
of obtained regret in Table 1. In all of our environment settings,
dLinUCB consistently achieved the best performance against all
baselines. In particular, we can notice that the length S of stationary period plays an important role in affecting dLinUCB’s regret
(and also in adTS). This is expected from our regret analysis: since
T is fixed, a smaller S leads to a larger ΓT , which linearly scales
dLinUCB’s regret in Eq (8) and (9). A smaller noise level σ leads
to reduced regret in dLinUCB, as it makes the change detection
easier. Last but not least, the magnitude of change ∆ does not affect
dLinUCB: when ∆ is large, the change is easy to detect; when ∆ is
small, the difference between two consecutive reward distributions
is small, and thus the added regret from an out-of-date slave model
is also small. Again the context-free algorithms WMDUCB1 and
Meta-Bandit performed much worse than those contextual bandit
algorithms in all the experiments.
In addition, we also studied the effect of ρ in dLinUCB by varying
ρ from 0.0 to 1.0. dLinUCB achieved the lowest regret when ρ = 0,
since the environment becomes stationary. When ρ > 0: dLinUCB
achieves the best regret (with regret of 54.07 ± 3.95) when ρ = 1.0,
however as ρ becomes smaller the regret is not affected too much
(with regret of 57.59 ± 3.44). These results further validate our
theoretical regret analysis and unveil the nature of dLinUCB in a
piecewise stationary environment.

501

Session 4D: Recommender Systems - Methods

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA

Qingyun Wu, Naveen Iyer, Hongning Wang
0.09

1.8

1.5

Article
Article
Article
Article
Article

0.08
1.4

1.7

0.07

1.2

1.1

0.06
1.6

CTR

CTR-Ratio

CTR-Ratio

1.3

1
2
3
4
5

0.05

0.04

1.5

1.0

0.03

uniform-dLinUCB
uniform-adTS
uniform-LinUCB

0.9

0.8
May01

May02

May03

May04

May05

N-dLinUCB
N-adTS
N-LinUCB
May06

May07

CLUB
WMDUCB1

May08

May09

May10

dLinUCB
LinUCB
adTS

1.4

May03

May04

May05

May06

May07

May08

May09

May10

0.02

0.01
May01

(a) Bandit models on the user side

May03

May05

May07

May09

Time

Time

Time

(b) Bandit models on the article side

(c) Detected changes on sample articles

Figure 3: Performance comparison in Yahoo! Today Module.
version of all bandit algorithms. Because dLinUCB maintains multiple slave models concurrently, each slave model is able to cover
preference in a subgroup of users, i.e., achieving personalization
automatically. We looked into those created slave models and found
they closely correlated with the similarity between user features in
different groups created by [25], although such external grouping
was not disclosed to uniform-dLinUCB. Although adTS and WMDUCB1 can also detect changes, its slow detection and reaction to
the changes made it even worse than LinUCB on this dataset. MetaBandit is sensitive to its hyper-parameters and performed similarly
to WMDUCB1, so that we excluded it from this comparison.
The second setting is to build bandit models for each article, i.e.,
attaching θ on the article side to learn its popularity over time.
Based on our quantitative analysis in the data set, we found that
articles with short lifespans tend to have constant popularity. To
emphasize the non-stationarity in this problem, we removed articles which existed less than 18 hours, and report the resulting
performance in Figure 3 (b). We can find that dLinUCB performed
comparably to LinUCB at the beginning, while the adTS baselines
failed to recognize the popularity of those articles from the beginning, as the popularity of most articles did not change immediately.
In the second half of this period, however, we can clearly realize the
improvement from dLinUCB. To understand what kind of changes
dLinUCB recognized in this data set, we plot the detected changes
of five randomly selected articles in Figure 3 (c), in which dotted vertical lines are our detected change points on corresponding articles.
As we can find in most articles the critical changes of ground-truth
CTR can be accurately recognized. For example, article 1 and article
2 at around May 4, and article 3 at around May 5. Unfortunately, we
do not have any detailed information about these articles to verify
the changes; otherwise it would be interesting to correspond these
detected changes to real-world events. In Figure 3 (b), we excluded
the context-free bandit algorithms because they performed much
worse and complicate the plots.

4.3

TF-IDF feature vector to represent it. Then we used PCA to reduce
the dimensionality of the feature vectors and retained the first 25
principle components to construct the context vectors, i.e., d = 25.
We fixed the size of candidate arm pool to K = 25; for a particular
user u, we randomly picked one item from his/her nonzero reward
items, and randomly picked the other 24 from those zero reward
items. We followed [16] to simulate a non-stationary environment:
we ordered observations chronologically inside each user, and built
a single hybrid user by merging different users. Hence, the boundary between two consecutive batches of observations from two
original users is treated as the preference change of the hybrid user.
Normalized rewards on these two datasets are reported in Figure 4 (a) & (b). dLinUCB outperformed both LinUCB and adTS on
LastFM. As Delicious is a much sparser dataset, both adTS and
dLinUCB are worse than LinUCB at the beginning; but as more
observations become available, they quickly catch up. Since the distribution of items in these two datasets are highly skewed [5], which
makes the observations for each item very sparse, the context-free
bandits performed very poorly on these two datasets. We therefore
chose to exclude the context-free bandit algorithms from all the
comparisons on these two datasets in our result report.
Each slave model created for this hybrid user can be understood
as serving for a sub-population of users. We qualitatively studied
those created slave models to investigate what kind of stationarity
they have captured. On the LastFM dataset, each user is associated
with a list of tags he/she gave to the artists. The tags are usually
descriptive and reflect users’ preference on music genres or artist
styles. In each slave model, we use all the tags from the users
being served by this model to generate a word cloud. Figure 5
are four representative groups identified on LastFM, which clearly
correspond to four different music genres – rock music, metal
music, pop music and hip-hop music. dLinUCB recognizes those
meaningful clusters purely from user click feedback.
The way we simulate the non-stationary environment on these
two datasets makes it possible for us to assess how well dLinUCB
detects the changes. To ensure result visibility, we decide to report
results obtained from user groups (otherwise there will be too many
change points to plot). We first clustered all users in both of datasets
into user groups according to their social network structure using
spectral clustering [5]. Then we selected the top 10 user groups
according to the number of observations to create the hybrid user.
We created a semi-oracle algorithm named as OracleLinUCB, which
knows where the boundary is in the environment and resets LinUCB
at each change point. The normalized rewards from these two
datasets are reported in Figure 4 (c) & (d), in which the vertical lines
are the actual change points in the environment and the detected

Experiments on LastFM & Delicious

The LastFM dataset is extracted from the music streaming service
Last.fm, and the Delicious dataset is extracted from the social bookmark sharing service Delicious. They were made availalbe on the
HetRec 2011 workshop. The LastFM dataset contains 1892 users
and 17632 items (artists). We treat the ‘listened artists’ in each user
as positive feedback. The Delicious dataset contains 1861 users and
69226 items (URLs). We treat the bookmarked URLs in each user as
positive feedback. Following the settings in [5], we pre-processed
these two datasets in order to fit them into the contextual bandit
setting. Firstly, we used all tags associated with an item to create a

502

Session 4D: Recommender Systems - Methods

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

7.5

7.0

6.5

6.0

5.5

5.0
0

10000

20000

30000

40000

50000

60000

70000

80000

1.6

1.4

1.2

1.0

dLinUCB
adTS
LinUCB

0.8

0.6
0

20000

40000

time

(a) Normalized reward on LastFM

60000

80000

100000

time

14
12
10
8
6
dLinUCB
adTS
LinUCB

4
2
0

5000

10000

OracleLinUCB
actuall changes
detected changes

15000

20000

time

(b) Normalized reward on Delicious

(c) Cluster detection on LastFM

Normalized accumulated reward

1.8

dLinUCB
adTS
LinUCB

8.0

Normalized Accumulated Payoff

Normalized Accumulated Payoff

8.5

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
Normalized accumulated reward

Learning Contextual Bandits in a Non-stationary Environment

2.0

1.5

1.0

0.5

0.0
0

dLinUCB
adTS
LinUCB

2000

4000

6000

8000

OracleLinUCB
actuall changes
detected changes

10000

12000

time

(d) Cluster detection on Delicious

Figure 4: Performance comparison in LastFM & Delicious.
when serving for multiple users, dLinUCB treats them as identical or totally independent. As existing works have shed light on
collaborative bandit learning [11, 24, 25], it is meaningful to study
non-stationary bandits in a collaborative environment. Last but
not least, currently the master bandit model in dLinUCB does not
utilize the available context information for ‘badness’ estimation. It
is necessary to incorporate such information to improve the change
detection accuracy, which would lead to a further reduced regret.

6

ACKNOWLEDGMENTS

We thank the anonymous reviewers for their insightful comments.
This work was supported in part by National Science Foundation
Grant IIS-1553568 and IIS-1618948.

REFERENCES
[1] Yasin Abbasi-yadkori, Dávid Pál, and Csaba Szepesvári. 2011. Improved Algorithms for Linear Stochastic Bandits. In NIPS. 2312–2320.
[2] Peter Auer. 2002. Using Confidence Bounds for Exploitation-Exploration Tradeoffs. Journal of Machine Learning Research 3 (2002), 397–422.
[3] Peter Auer, Nicolò Cesa-Bianchi, and Paul Fischer. 2002. Finite-time Analysis of
the Multiarmed Bandit Problem. Mach. Learn. 47, 2-3 (May 2002), 235–256.
[4] P. Auer, N. Cesa-Bianchi, Y. Freund, and Robert E. Schapire. 1995. Gambling in
a rigged casino: The adversarial multi-armed bandit problem. In Foundations of
Computer Science, 1995. Proceedings., 36th Annual Symposium on. 322–331.
[5] Nicolò Cesa-Bianchi, Claudio Gentile, and Giovanni Zappella. 2013. A Gang of
Bandits. In Pro. NIPS (2013).
[6] Wei Chu, Lihong Li, Lev Reyzin, and Robert E Schapire. 2011. Contextual bandits
with linear payoff functions. In AISTATS’11. 208–214.
[7] Robert B Cialdini and Melanie R Trost. 1998. Social influence: Social norms,
conformity and compliance. (1998).
[8] Joohyun Lee Fang Liu and Ness Shroff. 2018. A Change-Detection based Framework for Piecewise-stationary Multi-Armed Bandit Problem (AAAI’18).
[9] Sarah Filippi, Olivier Cappe, Aurélien Garivier, and Csaba Szepesvári. 2010. Parametric bandits: The generalized linear case. In NIPS. 586–594.
[10] AurÃľlien Garivier and Eric Moulines. On Upper-Confidence Bound Policies for
Non-stationary Bandit Problems. In arXiv preprint arXiv:0805.3415 (2008).
[11] Claudio Gentile, Shuai Li, Purushottam Kar, Alexandros Karatzoglou, Giovanni
Zappella, and Evans Etrue. 2017. On Context-Dependent Clustering of Bandits.
In ICML’17. 1253–1262.
[12] Claudio Gentile, Shuai Li, and Giovanni Zappella. 2014. Online Clustering of
Bandits. In ICML’14. 757–765.
[13] Avishek Ghosh, Sayak Ray Chowdhury, and Aditya Gopalan. 2017. Misspecified
Linear Bandits. CoRR abs/1704.06880 (2017).
[14] John C Gittins. 1979. Bandit processes and dynamic allocation indices. Journal
of the Royal Statistical Society. Series B (Methodological) (1979), 148–177.
[15] Negar Hariri, Bamshad Mobasher, and Robin Burke. Adapting to User Preference
Changes in Interactive Recommendation.
[16] Cedric Hartland, Sylvain Gelly, Nicolas Baskiotis, Olivier Teytaud, and Michele
Sebag. 2006. Multi-armed Bandit, Dynamic Environments and Meta-Bandits.
(Nov. 2006). https://hal.archives-ouvertes.fr/hal-00113668
[17] John Langford and Tong Zhang. 2008. The epoch-greedy algorithm for multiarmed bandits with side information. In NIPS. 817–824.
[18] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. 2010. A contextualbandit approach to personalized news article recommendation. In Proceedings of
19th WWW. ACM, 661–670.
[19] Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. 2011. Unbiased offline
evaluation of contextual-bandit-based news article recommendation algorithms.
In Proceedings of 4th WSDM. ACM, 297–306.

Figure 5: Word cloud of tags from four identified user groups
in dLinUCB on LastFM dataset.
points by dLinUCB. Since OracleLinUCB knows where the change
is ahead of time, its performance can be seen as optimal. On LastMF,
the observations are denser per user group, so that dLinUCB can
almost always correctly identify the changes and achieve quite close
performance to this oracle. But on Delicious, the sparse observations
make it much harder for change detection; and more early and late
detection happened in dLinUCB.

5

CONCLUSIONS & FUTURE WORK

In this paper, we develop a contextual bandit model dLinUCB for a
piecewise stationary environment, which is very common in many
important real-world applications but insufficiently investigated in
existing works. By maintaining multiple contextual bandit models
and tracking their reward estimation quality over time, dLinUCB
adaptively updates its strategy for interacting
√ with a changing
environment. We rigorously prove an O(ΓT ST ln ST ) upper regret bound, which is arguably the tightest upper regret bound any
algorithm can achieve in such an environment without further assumption about the environment. Extensive experimentation in
simulation and three real-world datasets verified the effectiveness
and the reliability of our proposed method.
As our future work, we are interested in extending dLinUCB
to a continuously changing environment, such as Brownian motion, where reasonable approximation has to be made as a model
becomes out of date right after it has been created. Right now,

503

Session 4D: Recommender Systems - Methods

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA

Qingyun Wu, Naveen Iyer, Hongning Wang
≥P( |xiT θˆi − xiT θ t∗c − η i | ≤ B̃ i (m, a i ) + ϵ )

[20] Shuai Li, Alexandros Karatzoglou, and Claudio Gentile. Collaborative Filtering
Bandits. In Proceedings of the 39th International ACM SIGIR.
[21] Wei Li, Xuerui Wang, Ruofei Zhang, Ying Cui, Jianchang Mao, and Rong Jin.
2010. Exploitation and exploration in a performance based contextual advertising
system. In Proceedings of 16th SIGKDD. ACM, 27–36.
[22] Haipeng Luo, Alekh Agarwal, and John Langford. 2017. Efficient Contextual
Bandits in Non-stationary Worlds. arXiv preprint arXiv:1708.01799 (2017).
[23] Alex Slivkins and Eli Upfal. 2008. Adapting to a Changing Environment: the
Brownian Restless Bandits, In COLT08’. 343–354.
[24] Huazheng Wang, Qingyun Wu, and Hongning Wang. 2017. Factorization Bandits
for Interactive Recommendation.. In AAAI. 2695–2702.
[25] Qingyun Wu, Huazheng Wang, Quanquan Gu, and Hongning Wang. 2016. Contextual Bandits in a Collaborative Environment. In Proceedings of the 39th International ACM SIGIR. ACM, 529–538.
[26] Jia Yuan Yu and Shie Mannor. 2009. Piecewise-stationary Bandit Problems with
Side Observations. In Proceedings of the 26th ICML (ICML ’09). 1177–1184.
[27] Yisong Yue and Thorsten Joachims. 2009. Interactively optimizing information
retrieval systems as a dueling bandits problem. In Proceedings of 26th ICML. ACM,
1201–1208.

j

× P( |xiT θ t∗c − xiT θ i∗ | > B̃ i (m, a i ) + B i (m, a i ) + 2ϵ )
j

According to Theorem 7.1, we have P |xiTθˆi − xiTθ t∗c − ηi | ≤
j

B̃i (m, ai ) +ϵ ≥ 1 −δ 1 . Define Uc j as the upper bound of B̃i (m, ai ) +
Btc (m, ai ) + 2ϵ. If the change gap ∆c j satisfies ∆c j ≥ Uc j , we have
P(ei (m) = 1) ≥ ρ(1 − δ 1 ).
Next, we will prove that ∆c j ≥ Uc j can be achieved by a properly
set δ 1 . Similar as the proof in Step 2 of Theorem 3.2, where we bound
kc , we have with a high probability that B̃i (m, ai )+Btc j (m, ai )+2ϵ ≤
√
√

2ϵ + 2 λ + √2 Sc j ρ 1 − ρ(1 − δ 1 ) = Uc j . When ∆c j > 2 λ + 2ϵ,
λ
√
√
√
√

λ
S min > 2ρλ (∆c j −2 λ−2ϵ) and δ 1 ≤ 1− ρ1 (1− 2S min
ρ ∆c j −2 λ−2ϵ) ,
∆c j > Uc j can be achieved.
Eq (12) indicates when the environment has changed for a slave
model m, with a high probability of ei (m) = 1 and slave model
m will not be updated, which avoids possible contamination in m.
According to the concentration inequality in Theorem 7.2, with a
probability at least 1 − δ 2 , we have,

7 APPENDIX
7.1 Additional Theorems
If the training instances {(xi , r i )}i ∈Im, t in a linear bandit model
come from multiple distributions/environments, we separate the
training instances in Im,t into two sets Hm,t and H̃m,t so that instances from Hm,t are from the target stationary distribution, while
instances in H̃m,t are not. In this case, we provide the confidence
bound for the reward estimation in Theorem 7.1.

t
Õ
i =t −τ

Ñc j (m) ≤ l +

 −2c 2 E(W )2 
n
n(b − a)2

Proof of Lemma 3.3. According to Chernoff Bound, we have
ln(1/δ )
P(êt (m) ≤ δ 1 + 2τ̃ (m)2 ) ≥ 1 − δ 2 , which concludes the proof. □
Proof of Lemma 3.4. At time i ≥ tc j+1 , which means the environment has already changed from θc∗j to θc∗j+1 , we have,
− η i ) + (xiT θ t∗c

− xiT θ i∗ )|

τ
1
ln
2 δ2

2
δ

∞ Õ
t −1 Õ
t −1
Õ
t =tc j s=1 s i =l

1{êsi (m) − dsi (m) ≤ ês (mc∗j ) − ds (mc∗j )}

ê i (mc∗ j ) ≥ E[e(mc∗ j )] + d i (mc∗ j )

(14)

ê i (m) ≤ E[e(m)] − d i (m)
E[e i (m)] − 2d i (m) ≤ E[e(mc∗ j )]

(15)
(16)

Intuitively, Eq (14), (15) and (16) correspond to the following three
cases respectively. First, the expected ‘badness’ of the optimal slave
model mc∗j is substantially over-estimated. Second, the expected
‘badness’ of the slave model m is substantially under-estimated.
Third, the expected ‘badness’ of the two slave models mc∗j and m
are very close to each other.
According to Theorem 7.2, we have P êi (mc∗j ) ≥ E[e(mc∗j )] +


di (mc∗j ) ≤ δ 2 and P êi (m) ≤ E[e(m)] − di (m) ≤ δ 2 . For si ≥ l, we
∗
have E[e(m)] − E[e(mc j )] − 2di (m) ≥ 0, which means the case in Eq
(16) almost never occurs. Substituting the probability for Eq (14),
(15) and (16) into Eq (13), and when δ 2 = t −4 ,

Proof sketch of Theorem 3.1 and Theorem 7.1. The proof of
Eq (7) in Theorem 3.1 and Theorem 7.1 are mainly based on the
proof of Theorem 2 in [1] and the concentration property of Gaussian noise.
□

− xiT θ t∗c
j

r

(13)


2
in which l = (8 log Sc j )/дm,m
. êsi (m) − dsi (m) ≤ ês (mc∗j ) −
cj
ds (mc∗j ) implies that at least one of the three following inequalities
must hold,

Proof of Theorems and Lemmas

=P(|(xiT θˆi

τ
1
ln
≥ ρ(1 − δ 1 )τ −
2 δ2

2

Theorem 7.2 (Chernoff Bound). Let Z 1 , Z 2 , ...,ÍZ n be random
variables on R such that a ≤ Z i ≤ b. Define Wn = ni=1 Z i , for all
c > 0 we have,

P(e i (m) = 1) = P(|r̂ i − r i | > B i (m, a i ) + ϵ )

i =t −τ

r

Proof of Lemma 3.6. Under the model selection strategy in line
4 of Algorithm 1, using similar proof techniques as in Theorem 1
of [3], we have

Comparing B̃t (m, a) with Bt (m, a), we can see that when the
reward deviation of an arm (the 1 − ρ portion of arms that do
not q
satisfy Eq (1) in Assumption 1) is small with (1 − ρ)∆small ≤

|Im, t | 
2
σ d ln(1 + λδ
) / H̃m,t , the same confidence bound scaling
1
can be achieved.

7.2

e i (m)] −

2 ln

m, t

P(|Wn − E(Wn )| > cE(Wn )) ≤ 2 exp

t
Õ

2
With simple rewriting, we have when τ ≥ (ρ(1−δ )−δ
2 , ρ(1−δ 1 )τ −
1
1)
q
q
τ ln 1 ≥ δ τ + τ ln 1 , which means that with a probability
1
2
2
δ2
δ
Ít 2
q
i =t −τ e i
at least 1 − δ 2 , êt (m) =
≥ δ 1 + 2τ1 ln δ1
□
τ

Theorem 7.1 (LinUCB with contamination). In LinUCB with
a contaminated instance set H̃m,t , with probability at least 1 − δ 1 ,
we have |rˆt (m) − E[r t ]| ≤ B̃t , where B̃t (m, a) = α̃ t ∥xat ∥A−1 , α̃ t =
t −1
q
Í
|Im, t | √
σ 2 d ln(1 + λδ
)+ λ+Ct (m), and Ct = i ∈ H̃ (xai (θ i∗ −θ t∗c )).
1

e i (m) ≥ E[

∞ Õ
t −1 Õ
t −1

 Õ
π2
2
2
Ñc j (m) ≤ (8 ln S c j )/дm,m
2t −4 ≤ (8 ln S c j )/дm,m
∗ +
∗ +1+
cj
cj
3
t =1 s =1
s i =l

(12)

> B i (m, a i ) + ϵ )

which finishes the proof.

504

□

