Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Convolution-based Memory Network
for Aspect-based Sentiment Analysis
Chuang Fan*

Qinghong Gao*

Jiachen Du

Shenzhen Graduate School,
Harbin Institute of Technology

Shenzhen Graduate School,
Harbin Institute of Technology

Shenzhen Graduate School,
Harbin Institute of Technology

Lin Gui

Ruifeng Xu†

Kam-Fai Wong

Aston University

Shenzhen Graduate School,
Harbin Institute of Technology
xuruifeng@hit.edu.cn

The Chinese University of Hong
Kong

ABSTRACT

However, these features are always hard to be designed and transferred among different domains. Recently, many works attempted
to apply neural networks to aspect-based sentiment analysis. Li
et al. applied Adaptive Recursive Neural Network to this task by
propagating the sentiment labels of words towards target through
the syntactic structure of text [4]. Tang et al. used two Recurrent
Neural Networks with Long-Short Term Memory (LSTM) to separately represent text and targets, and then classified the sentiment by adopting a feed-forward network [5]. Neural memory network is known as one solution to explicitly modeling the context
in sentences[6, 7]. Memory Network is a kind of neural network
which explicitly stores the context of words in external memory
and uses neural attention mechanism to determine which part is
retrieved and fed into downstream components. Tang et al. proposed a deep memory network which used multiple computational
layers over external memory [8]. This model explicitly captures the
importance of each context word when inferring the sentiment polarity of a given target. By using a non-linear attention mechanism
to weight learned sequence features from networks, Chen et al. proposed a method to pick up important information in memory and
predicts the final sentiment [9].
Existing memory-network based models have shown expressive
performance, however, these models only take word-level information into consideration and lack the capacity to model the complicated expressions which consist of multiple words [10]. To overcome this problem, we propose a novel model named Convolutionbased Memory Network. The proposed model is based on memory network and inspired by convolutional operation, which explicitly model both words and multi-words information in the sentence. By simultaneously storing the context information into a
fixed-size window, the proposed memory network is able to capture long-distance dependency. By learning both the words and
multiple words representations, this model is expected to improve
the aspect-based sentiment analysis. The proposed model is evaluated on three datasets: two ones are from SemEval2014 Task4 and
the other is tweet dataset. Experimental results show that the proposed model outperforms the state-of-the-art baselines.

Memory networks have shown expressive performance on aspect
based sentiment analysis. However, ordinary memory networks
only capture word-level information and lack the capacity for modeling complicated expressions which consist of multiple words. Targeting this problem, we propose a novel convolutional memory
network which incorporates an attention mechanism. This model
sequentially computes the weights of multiple memory units corresponding to multi-words. This model may capture both words and
multi-words expressions in sentences for aspect-based sentiment
analysis. Experimental results show that the proposed model outperforms the state-of-the-art baselines.

KEYWORDS
sentiment analysis; memory network; convolutional operation
ACM Reference Format:
Chuang Fan, Qinghong Gao, Jiachen Du, Lin Gui, Ruifeng Xu, and Kam-Fai
Wong. 2018. Convolution-based Memory Network for Aspect-based Sentiment Analysis. In SIGIR ’18: The 41st International ACM SIGIR Conference on
Research & Development in Information Retrieval, July 8–12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/
3209978.3210115

1 INTRODUCTION
Aspect-based sentiment analysis aims to extract the aspects of given
target entities and the sentiment expressed towards each aspect
[1]. Different from ordinary sentiment analysis [2], aspect-based
sentiment analysis models need leverage information from both
text and target. Traditional feature-engineering based models separately extract the features of text and target for classification [3].
*

These two authors contribute equally to this work.
Corresponding author.

†

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07…$15.00
https://doi.org/10.1145/3209978.3210115

2 PROPOSED MODEL
Given a sentence S = {s 1 , s 2 , . . . , sn } and the target word a, the
goal of our model is to predict the sentiment polarity towards a.

1161

Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Weighted
Sum
Concat

content position

s2

m2

…

i-th slot
2t+1 slot

m3

s4

m4

s5

m5

1-st slot

s6

m6

i-th slot

Hop h
…

…
…

Attention
softmax

Sum

…

s3

…

Bi-GRU

Hop 2

…

…

Sample
S

Target a

…

Hop 1

…

Bi-GRU

Prediction

…

m1

…

s1

Prediction

1-st slot

Memory Units

…

2t+1 slot
Target a
(a)

(b)

Concat

Figure 1: (a): A single hop version of our model. (b): A multiple hop version of our model.
We map each word into a low dimensional embedding space V =
{v 1 , v 2 , . . . , vn } by GloVe[11] and extract word sequence features
by Bidirectional Recurrent Neural Networks with Gated Recurrent
Unit (Bi-GRU). The extracted features are stored in content memory. Additionally, the position information is stored in position
memory. The content memory and position memory are concatenated into memory units M = {m 1 , m 2 , ..., mn }. Then, the memory
units M from different are combined attentions for classification.
Inspired by convolutional operation, we introduce a novel attention mechanism that models the complicated expressions of multiple words.
The architecture of Convolutional Memory Network (Conv-Memnet)
is shown in Figure 1.

2.1 Single-hop Convolution-based Memory
Network

position i is computed by
i∑
+t

дi = w T (

(m j ⊕ T ))

(1)

j =i−t

exp(дi )
i ′ exp(дi ′ )

αi = ∑

(2)

where w is the parameter for computing attention signal and ⊕ is
the concatenate operation. The embedding of target word a (when
target is multi-words, take average of these words’ embeddings)
is denoted by T . α i is the attention signal showing importance of
memory units i and its surrounding chunk of size 2t + 1. Note that
we obtain the attention for each position rather than each word.
Hence, there are 2t + 1 prediction output vectors, namely, o j :
oj = T +

n
∑

α i · mi +j

(j = −t, . . . , 0, . . . , t)

(3)

i =1

Multi-words level features are known useful to identify the sentiment of a given target. Meanwhile, negations and emotion transitions are always context sensitive. However, the existing memory
networks [8, 9] based on single memory slot, have the difficulties
to represent the multi-word expressions. To solve this problem, we
jointly use the information contained a continuous subsequence of
memory units which correlate to multi-words in sentence. For the
reason of simplicity, the continuous subsequence of memory units
in our model is named as “memory chunk”.
To jointly process multiple memory units, we propose a new architecture that simultaneously model the memory chunk, which
is inspired by convolutional operation. To capture the key part of
sentence corresponding to a given target, the attention model is
applied to combine memory chunks for classification. When calculating the weight of attention for each memory chunk associated
with position i, we select previous t and succeeding t memory units
of position i corresponding to the multi-words centered on word
si . This procedure can be seen as a simulation to convolutional operation with window size of 2t + 1 running through the whole
memory sequence. For the first and the last word in a sentence, we
use zero vector to pad, where v 1−t = . . . = v 0 = vn +1 = . . . =
vn +t = ⃗0. The attention signal for memory chunk centered on

where o j is the j-th memory of sentence of a given target a. Moreover, the embedding of target is added to the output vectors to
enhance the correlations with target a. Then, the 2t + 1 vectors
are concatenated as o = o −t ⊕ o −t +1 . . . ot for the prediction by a
so f tmax function:
ô = so f tmax(Wm · o)

(4)

Where Wm is a transition matrix for computing the conditional
probability distribution.The model is trained by minimizing the
cross entropy:
∑
∑
L = (x,y ) ∈D c ∈C y c loд f c (x; θ )
(5)
Where D is the collection of training data and the C is the sentiment
category of sample. y ∈ R |C | is a one hot vector. f (x; θ ) is the
predicted sentiment distribution of the model.

2.2 Multi-hop Convolution-based Memory
Network
As discussed, before Single-hop Conv-Memnet is not enough to
represent the complicated multi-word expressions. Considering that
multi-layers computational models generally have better ability to

1162

Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: Experimental performance
Method
SVM
TD-LSTM
MemNet
RAM
Conv-Memnet

Laptop

Restaurant

Tweet

ACC

Macro-F1

ACC

Macro-F1

ACC

Macro-F1

0.7049
0.7183
0.7033
0.7449
0.7637

NA
0.6843
0.6409
0.7135
0.7210

0.8016
0.7800
0.7816
0.8023
0.7826

NA
0.6673
0.6583
0.7080
0.6838

0.6340
0.6662
0.6850
0.6936
0.7211

0.6330
0.6401
0.6691
0.6730
0.7080

learn data representations, the single-hop model is extended which
consists of multiple hops. The network is stacked as follows:

The proposed model are evaluated on three datasets. The first two
datasets are from SemEval2014 [12] which are the reviews for restaurant and laptop domain, respectively. The third one is collected
from tweeter by Li et al. [4]. The original dataset contains the fourth
category-conflict, which means that a sentence expresses both positive and negative opinion towards a given target. Thus, we preprocess the data as the same as Chen et al. [9].
The 300-dimmension word vectors pre-trained by GloVe[11] is
employed, the dimensionality of the Bi-GRU is set to 300 and the
number of layers in Bi-GRU is set to 5. We adopt Accuracy and
Macro-averaged F-measure as the evaluation metrics.

laptop and tweet datasets while RAM achieves the best performance on restaurant dataset. It is observed that SVM model achieved
lower performance. MemNet achieved nearly equal performance
compared with TD-LSTM on Restaurant dataset and outperforms
TD-LSTM on Tweet dataset, but performs worse on Laptop dataset.
The experimental results showed that both the attention mechanism in memory network and long distance dependency in LSTM
are effective to aspect-based sentiment analysis. It is also shown
that RAM outperforms the previous baselines on all datasets.
Experimental results show that our model outperforms MemNet
by 2.55% in F-measure. It becauses that MemNet adopts multiple
attentions in order to improve the attention results, but lacks the
capacity for modeling context information. Compared with MemNet, our model extract word sequence features by Bi-GRU and introduce a novel attention mechanism to capture complicated features consist of multiple words to predict sentiment. As shown in
Table 1, the proposed model achieves better overall performance
than RAM. RAM lacks the capacity of modeling complicated expressions consist of multiple word and updates each attention with
considering all memory units even though there is no more useful
information can be read from the memory. As described before,
our model overcomes these shortcomings by updating each attention more rely on its previous and succeeding memory units. In
addition, our model control the size of the window to avoid to capture too much noise information that irrelevant to current memory
unit.

3.2 Baselines

3.4 Effects of Multiple hops

• For the first hop, the target vector T in equation (1) is the
embedding of aspect word.
• For hop h (> 1) , there are 2t + 1 inputs since the previ1 h−1
h−1 }. Here,
ous layer has 2t + 1 outputs {oh−
−t , o −t +1 , . . . , o t
equation (1) is re-defined as follows:
∑ t
h−1
дi = w T ( ij +
(6)
=i−t (m j ⊕ o j−i ))
• In the last layer, the concatenation of 2t + 1 prediction vectors form the final prediction vector.

3 EXPERIMENTS AND EVALUATION
3.1 Experimental Setting

The proposed model is compared to the following competitive baselines:

Table 2: Performance vs. number of hops

(1) SVM: A SVM classifier using surface features, lexicon features and parse features [3].
(2) TD-LSTM: Use a forward LSTM and a backward LSTM to
capture the context information for classification [5].
(3) MemNet: A memory network based method which captures
more accurate attention information on word embeddings
to generate answer [8].
(4) RAM: Recurrent attention network on memory pays multiple attentions on the memory and combines memory units
from different attentions non-linearly [9].

No. of Hops
Hop 1
Hop 2
Hop 3
Hop 4
Hop 5
Hop 6

Laptop
0.6775
0.7000
0.7210
0.6869
0.6694
0.6655

Restaurant
0.6689
0.6752
0.6838
0.6741
0.6406
0.6226

Tweet
0.6735
0.6767
0.7080
0.6948
0.6795
0.6631

We conduct the experiments with the number of hops from 1 to
6 with the fixed window size of 3 to evaluate the effects of multiple
hops in this task. Generally speaking, the more hops are stacked,
the more complicated the model is. The experimental results are
listed in Table 2. It is shown that with the increasing number of

3.3 Experimental Results
The experimental results are listed in Table 1. It is shown that our
model consistently outperforms all compared baselines on both

1163

Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 4: Examples of multi-words with high attention

hops, the performances achieved are higher that single hop. The
model achieves the highest performance when the number of hops
is set to 3. When the number of hops is larger than 3, the performance decreases due to overfitting. As such, we choose 3 hops in
our final model.

Positive
not be disappointed
is not hard
not hard to
is never disappointing

3.5 Effects of Window size
We further conducted the experiments to evaluate the effects of
window size. Here, the the window size of {1, 3, 5, 7} are evaluated,
respectively, with the number of hops fix to 3. The achieved performances are listed in Table 3. It is observed that the model with
the window size of 3 achieves the top performance. If the window
size is set to 1, the model can only capture the word level information. On the contrary, if the window size is too large, the model
will capture too much noise information that is irrelevant or even
counterproductive to the current memory unit in context.

4 CONCLUSIONS
In this paper, we propose a novel Convolution-based Memory Network for determining the sentiment polarity of the opinion target.
This model extracts word sequence features by adopting Bi-GRU.
Inspired by convolutional operation, the new attention mechanism
is designed to effectively model the complicated expressions consist of multi-words. By controlling the size of convolutional window, our model is able to store the context into different memory
slots to capture context information in proper sequence. In addition, to make the model more expressive, we extend the single-hop
model to deep architecture which consists of multiple hops. The
experimental results show that the proposed method outperforms
the competitive state-of-the-art baselines. In the future, we will explore the more effective ways to model the position information
for aspect-based sentiment analysis.

Table 3: Performance vs. window size of convolutional operations.
No. of window size
k=1
k=3
k=5
k=7

Laptop
0.6745
0.7210
0.6755
0.6605

Restaurant
0.6663
0.6838
0.6829
0.6450

Negative
did not enjoy
not good for
do not like
can not work

Tweet
0.6649
0.7080
0.7013
0.6972

ACKNOWLEDGMENTS
This work was supported by the the National Key research and
Development Program of China 2017YFB0802204, National Natural science Foundation of China U1636103, 61632011, Shenzhen
Foundational Research Funding20170307150024907,Key Technologies Research and Development Program of Shenzhen JSGG 2017081
7140856618, Innovate UK (Grant No. 103652) and EU-H2020 (Grant
No.794196).

3.6 Case Study
It is enlightening to analyze whether the model really focuses on
the multi-words which describe the sentiment given an aspect. Figure 2 shows the representation of how the model focuses on multiwords in different hops. The sentence in Figure 2 is I can’t say
enough of how satisfied I am with their product and help aftermarket. The figure shows that the proposed model can correctly pays
the highest attention weights on multi-words can’t say enough of
how satisfied I am with a given target product and help aftermarket,
which produces the positive sentiment. Moreover, the distribution
changes of attention weights in different hops can be observed directly. With the stacking of hops, our model gradually pays more
attention to correct multi-words. This figure shows that our model
can effectively identify the important multi-words responding to
a given target.

REFERENCES

Figure 2: The changes in each hop of attention
In addition, we count the attention information of all three datasets.
In order to get the most influential multi-words, we set up the size
of convolutional operation of 3 and count the sum of 3 adjacent
words’ attention scores. The multiple attention scores sum in each
sentence are sorted to show the most representative multi-words
which are beneficial to identify the sentiment of given target. These
multi-words are listed in Table 4. This clearly shows that our model
effectively capture the information of multi-words.

1164

[1] Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In KDD,
pages 168–177, 2004.
[2] Yoon Kim. Convolutional neural networks for sentiment classification. In
EMNLP, pages 1746–1751, 2014.
[3] Svetlana Kiritchenko, Xiaodan Zhu, Colin. Cherry, and Saif Mohammad. Nrccanada-2014: Detecting aspects and sentiment in customer reviews. In SemEval,
pages 437–442, 2014.
[4] Dong Li, Furu Wei, Duyu Tang Chuanqi Tan, Ming Zhou, and Ke Xu. Adaptive
recursive neural network for target-dependent twitter sentiment classification.
In ACL, pages 49–54, 2014.
[5] Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu. Target-dependent sentiment classification with long short term memory. CoRR, abs/1512.01100, 2015.
[6] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory
networks. In NIPS, pages 2440–2448, 2015.
[7] Lin Gui, Jiannan Hu, Yulan He, Ruifeng Xu, Qin Lu, and Jiachen Du. A question
answering approach to emotion cause extraction. In EMNLP, pages 1593–1602,
2017.
[8] Duyu Tang, Bing Qin, and Ting Liu. Aspect level sentiment classification with
deep memory network. In EMNLP, pages 214–224, 2016.
[9] Peng Chen, Zhongqian Sun, Lidong Bing, and Wei Yang. Recurrent attention
network on memory for aspect sentiment analysis. In EMNLP, pages 452–461,
2017.
[10] Cheng Li and Qiaozhu Mei. Deep memory networks for attitude identification.
In WSDM, pages 671–680, 2017.
[11] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global
vectors for word representation. In EMNLP, pages 1532–1543, 2014.
[12] Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. Semeval-2014 task 4: Aspect based sentiment analysis. In SemEval, pages 27–35, 2014.

