Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Review Sentiment–Guided Scalable Deep Recommender System
Dongmin Hyun1 Chanyoung Park1 Min-Chul Yang2 Ilhyeon Song2 Jung-Tae Lee2 Hwanjo Yu1∗
1 Dept.

of Computer Science and Engineering, POSTECH, Pohang, South Korea
2 NAVER Corporation, Seongnam, South Korea
{dm.hyun,pcy1302,hwanjoyu}@postech.ac.kr,{minchul.yang,ilhyeon.song,jungtae.lee}@navercorp.com

ABSTRACT

1

Existing review-aware recommendation methods represent users
(or items) through the concatenation of the reviews written by (or
for) them, and depend entirely on convolutional neural networks
(CNNs) to extract meaningful features for modeling users (or items).
However, understanding reviews based only on the raw words of reviews is challenging because of the inherent ambiguity contained in
them originated from the users’ different tendency in writing. Moreover, it is inefficient in time and memory to model users/items by
the concatenation of their associated reviews owing to considerably
large inputs to CNNs. In this work, we present a scalable reviewaware recommendation method, called SentiRec, that is guided to
incorporate the sentiments of reviews when modeling the users
and the items. SentiRec is a two-step approach composed of the
first step that includes the encoding of each review into a fixed-size
review vector that is trained to embody the sentiment of the review, followed by the second step that generates recommendations
based on the vector-encoded reviews. Through our experiments,
we show that SentiRec not only outperforms the existing reviewaware methods, but also drastically reduces the training time and
the memory usage. We also conduct a qualitative evaluation on the
vector-encoded reviews trained by SentiRec to demonstrate that
the overall sentiments are indeed encoded therein.

Among various recommendation techniques, the most successful
approach is collaborative filtering (CF) [4]; it recommends items to
a user based on previous ratings of other users whose tastes are
similar to the target user. However, this in turn implies that the
performance of CF will suffer without a sufficient amount of ratings
previously given by users, which is common in reality.
To compensate for the sparsity of the user–item rating data,
side information related to users and items, such as user social
network [8], user review documents [11–14], and item affinity network [9] has been actively leveraged. In this work, we specifically
focus on user review-aware recommendation. User reviews are particularly useful for alleviating the sparsity of user ratings, because
the reviews not only embody a user’s intention behind the ratings,
but also contain conspicuous item properties. That is to say, if reviews are fully exploited, we can build recommender systems even
with few ratings provided, which naturally alleviates the sparsity
of user–item rating data.
To extract meaningful features from review documents, deep
learning-based approaches have been recently proposed [1, 13].
More specifically, convolutional neural network (CNN)-based recommendation methods have gained attention [3, 11, 14] thanks to
the capability of CNNs to capture general contextual features from
documents. DeepCoNN [14] adopts two CNNs, where one of them
models users through reviews written by the users, while the other
models items through reviews written for the items. Building upon
DeepCoNN, Seo et al. propose D-Attn [11] that further adopts the
dual local and global attention mechanism on the CNNs, which
endow the recommender systems with interpretability regarding
the reviews that are used for modeling users and items.
Despite their state-of-the-art performance, they are limited in
that users and items are modeled by the reviews consisting of raw
words. However, each user has different tendency in writing a review and thus words contain an inherent ambiguity, which makes
it hard to precisely understand the user’s intent. As a concrete
example, let’s assume that two different users provided reviews
that contain the following identical sentence: “... I like the laptop...
”. Whereas a tolerant user would use the word “like” to describe
an adequate laptop, a critical user would not use it unless he is
completely satisfied with the laptop. However, the previous reviewaware methods simply aggregate all the associated reviews and
feed them to CNNs expecting the CNNs to automatically extract
meaningful features for modeling users and items, which does not
suffice for precisely modeling the users and items. This phenomenon compounds when users provided only a few reviews, i.e.,
cold-start [10], which is common in reality. Moreover, as the existing approaches model each user/item by the concatenation of all
the words from every associated review, the size of input for CNNs

CCS CONCEPTS
• Information systems → Recommender systems;

KEYWORDS
Recommender System, Deep learning, Sentiment analysis
ACM Reference Format:
Dongmin Hyun1 Chanyoung Park1 Min-Chul Yang2 Ilhyeon Song2 Jung-Tae
Lee2 Hwanjo Yu1 . 2018. Review Sentiment–Guided Scalable Deep Recommender System. In SIGIR ’18: The 41st International ACM SIGIR Conference
on Research Development in Information Retrieval, July 8–12, 2018, Ann
Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/
3209978.3210111
∗ Corresponding

author

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210111

965

INTRODUCTION

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

𝑟𝑢,𝑖

𝑟𝑢,𝑖
FC 2
Concatenation

FC 1

users and items. Note that we assume every rating accompanies a
review. Given a review document du ,i ∈ Rk ×T on item i written
by user u, where k and T denote the latent dimensionality of a
word and the average number of words contained in each review,
repectively, all the reviews written by user u are concatenated to
k ×( |NuI |T ) , N I denoting a
a single user document matrix DU
u ∈ R
u
set of items rated by user u. Likewise, an item document matrix
U
for item i is represented as DiI ∈ Rk ×( |Ni |T ) , where NiU denotes a
U
set of users that rated item i. Then, Du and DiI are independently
fed into two parallel CNNs; one for users and the other for items.
After convolutions and max-pooling layers, these CNNs are jointly
combined in the last hidden layer to predict the rating of item i
given by user u, i.e., ru ,i . The architecture is shown in Figure 1a.
As mentioned previously, raw words contained in reviews contain an inherent ambiguity owing to users’ tendency in writing, and
thus entirely depending on the CNNs to extract features that are
useful for modeling users and items are prone to error. Moreover,
I
the size of the user/item document matrix DU
u /Di is considerably
large making the above methods impractical. Therefore, from the
following section, we introduce our two-step approach to overcome
the above limitations.

𝑟𝑢,𝑖

𝑚

𝑙

𝒇𝒖,𝒊

𝑝

𝑛

Dot product

𝒉𝑈
𝑢

𝒉𝐼𝑖

⋯

⋯

Max pooling
⋯

⋯

⋯

𝐶𝑜𝑛𝑣𝑜𝑙𝑡𝑢𝑡𝑖𝑜𝑛 𝐹𝑖𝑙𝑡𝑒𝑟𝑠

𝐶𝑜𝑛𝑣𝑜𝑙𝑡𝑢𝑡𝑖𝑜𝑛 𝐹𝑖𝑙𝑡𝑒𝑟𝑠

𝑾1𝑁𝑒𝑡1 ⋯ 𝑾𝑛𝑁𝑒𝑡1

⋯

𝒅𝒖,𝒊
𝑇

⋯

⋯

⋯
𝐼

𝒅𝑢,|𝑁𝑢 |
𝑇

⋯

𝒅𝒖,𝒊
𝑇

𝑘

⋯
𝐼

𝒅𝑢,|𝑁𝑢 |
𝑇

(a) The underlying architecture of baseline methods

⋯

𝒅𝒖,𝒊
𝑇

𝑝

𝑝
𝑾1𝑁𝑒𝑡 𝑈 ⋯ 𝑾𝑁𝑒𝑡 𝑈 𝑾1𝑁𝑒𝑡 𝐼 ⋯ 𝑾𝑁𝑒𝑡 𝐼
2

𝑘

2

⋯

𝑙

2

2

𝑙

⋯

𝐼

𝑈

𝒇𝑢,𝑖 𝒇𝑢,|𝑁𝑢 | 𝒇𝑢,𝑖 𝒇|𝑁𝑖

|,𝑖

Step 1
Step 2
(b) Architecture of SentiRec

Figure 1: Comparisons of architectures between the baseline
methods vs. SentiRec1 .
becomes considerably large, which makes the above approaches
practically not feasible in the real-world applications.
In this paper, to overcome the above limitations of the existing
methods, we propose a novel sentiment guided review-aware recommendation method, called SentiRec. The core idea is to leverage
the overall sentiments of reviews that are represented as ratings that
accompany the reviews. In our previous example, if we have a prior
knowledge that the tolerant user gave a 3-star rating to the laptop
while the critical user gave a 5-star rating, we will be able to more
accurately understand the review, which in turn enables us to better
model users and items.
Our proposed method consists of two steps. In the first step,
instead of representing a review by the concatenation of its constituent raw words as in the previous methods, we encode each
review into a fixed-size review vector that is guided to embody the
sentiment information of the review. More precisely, we regard a
rating that accompanies a review as a summarization of the overall
sentiment of a user on an item, and train a CNN that is designed to
predict the rating given the review as input, after which a fixed-size
vector for the review is obtained by taking the output of the last
hidden layer. The second step resembles the training process of
DeepCoNN and D-Attn, but is distinguished in that users/items
in SentiRec are represented by the concatenation of their associated
fixed-size review vectors, rather than raw words. The advantages
of SentiRec compared with the previous methods are: 1) we obtain
more accurate representations for reviews by incorporating users’
overall sentiments on items into reviews, which removes the possible ambiguity contained in the reviews. This in turn results in a
better understanding of the reviews, and leads to more accurate
representations for users and items resulting in an improved recommendation accuracy. Moreover, 2) we drastically reduce the size
of the input, which gives us scalability in terms of the training time
and the memory usage. Our experiments show that SentiRec outperforms the state-of-the-art baselines, while being considerably
more efficient. Moreover, we perform a qualitative evaluation on
the review vectors trained by SentiRec to ascertain that the overall
sentiments are indeed encoded in the vectors.

2

3

METHOD: SentiRec

Step 1: Incorporating Review Sentiments. The goal of this
step is to encode each review du ,i into a fixed-size review vector fu ,i ∈ Rl , such that the overall sentiment of user u on item
i is incorporated. We employ a CNN among various methods [2]
to leverage the ratings that accompany the reviews. More precisely, we build a CNN, named Net1 , to predict the rating ru ,i ∈ R
accompanying a review given by user u on item i using the review document du ,i . Formally, Net1 performs convolution operations on du ,i using f -th convolution filter Wf ∈ Rk ×j with the
f
window size set to j to extract the contextual features c t from

,i
the document: c t = σ (Wf ∗ du(:,(t:t+j-1))
+ b f ) where ∗ is the convolution operator, b f ∈ R is a bias term for the f -th convolution filter, and σ is a non-linear function such as ReLU. By applying the f -th filter on the entire text du ,i , we obtain a feature
f
f f
f
f
map cNet1 = [c 1 , c 2 , ..., c t , ..., cT −j+1 ]. Then, max-pooling is applied on the feature map to find the most important feature. i.e.,
f
f
ĉ Net1 = max(cNet1 ). Finally, with n different convolution filters, followed by a fully connected layer (FC: Rn → Rl ), we obtain a vector
1 , ĉ 2 , ..., ĉ n ]) ∈ Rl , which is passed to a fully
fu ,i = FC([ĉ Net
Net1
Net1
1
connected layer whose output is the predicted rating ru ,i . Note
that fu ,i generated by Net1 can be regarded as a fixed-size review
vector that incorporates the overall sentiment of user u on item i
contained in the review document du ,i . The architecture of Net1
is illustrated in Figure 1b (left).
f

Step 2: Generating Recommendations. Having trained Net1
for all the provided reviews, we now proceed to generate the
actual recommendations, i.e., rating prediction. Aiming at predicting the rating ru ,i that user u will give on item i, we merge
all the review vectors of reviews written by user u denoted by
u ,i
l × |NuI | , and the reviews written for item i
FU
u = [f ]i ∈NuI ∈ R

BACKGROUND

In this section, we explain how the existing review-aware recommendation methods, i.e., DeepCoNN [14] and D-Attn [11], represent
1T

can be different for each review as T is the average number of words contained in
each review.

966

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: Data statistics.
Datasets
# of users
# of items
# of reviews
Avg. # words / review
Avg. # reviews / user
Avg. # reviews / item
Density

Office
4,905
2,418
53,258
168.8
8.5
17.2
0.45%

Grocery
14,681
8,712
151,254
109.9
7.9
3.3
0.12%

Clothing
39,387
23,022
276,677
69.2
5.0
8.6
0.03%

user–item ratings (1 to 5). We remove users having fewer than five
ratings. Table 1 summarizes the detailed statistics of the datasets.
Baselines.
• MF [4]: A model-based CF method that projects users and items
into low-dimensional vectors solely based on user–item ratings.
• DeepCoNN [14]: A CNN-based review-aware recommendation
method that models users and items by their associated reviews.
• D-Attn [11]: An extension of DeepCoNN that further employs
global and local attentions.
Since DeepCoNN and D-Attn have surpassed other review-aware
recommendation methods, such as CTR [12], HFT [6], CDL [13],
ConvMF [3], we omit them for brevity.
Evaluation Protocol and Metric. We divide each user’s ratings
into training/validation/test sets in a 80%/10%/10% split. We also
evaluate all methods on cold-start setting, where we only test on
users with fewer than five ratings in the training dataset. All of
the hyperparameters are tuned on the validation set by grid search.
The best performing parameters for SentiRec are: j = 4, n = 256,
l = 50, p = 5 and m = 10. As for D-Attn, we employ the best
parameters that are reported in the paper [11]. As our focus is
on recommendations in terms of rating prediction, we employ
the mean squared error (MSE), a metric that has been commonly
used for evaluating the performance of user rating prediction on
the Amazon datasets [6, 9]. Note that we fix the seed for random
initialization of all the methods.

Sports
35,598
18,351
296,337
99.9
6.2
12.0
0.05%

denoted by FiI = [fu ,i ]u ∈N I ∈ Rl × |Ni | . Then we introduce two
i
parallel CNNs; one for modeling users (NetU
2 ), and the other for
modeling items (Net2I ). Their network structures are equivalent
to that of Net1 , while they only differ in the way the convolution
operations are performed. To be precise, as the respective inputs
I
U
I
FU
u and Fi to Net2 and Net2 are concatenated review vectors, the
order in which feature vectors are placed is not semantically meaningful; unlike concatenated raw words. Therefore, the window size
I
of the convolution filters of NetU
2 and Net2 should be fixed to 1
to extract features from each review independently; unlike Net1
whose window size is set to j for extracting the contextual features
from a review document concatenated with raw words. It is imporI
tant to note that we also update the inputs FU
u and Fi during the
model training to make the review vectors adapt to the recommenU
dation task. NetU
2 performs convolution operations on Fu using p
convolution filters, which is followed by a max-pooling layer. After
a fully connected layer (FC: Rp → Rm ), we obtain the final output
U
I
m
m
of NetU
2 given by hu ∈ R , and we similarly obtain hi ∈ R
I
from Net2 . Finally, we calculate a dot product [11] between two
I
U
I
vectors hU
u and hi , and obtain the predicted rating ru ,i = ⟨hu , hi ⟩.
We note that only the reviews used to train Net1 are used to train
I
U
I
NetU
2 and Net2 . The architecture of Net2 and Net2 is illustrated
in Figure 1b (right).
U

Table 2: Test performance in terms of MSE. (Imprv. denotes
the improvement of SentiRec vs. the best competitor.)

Office

All
Cold-start
All
Cold-start
All
Cold-start
All
Cold-start

Clothing
Sports

4.1

MF
0.854
1.039
1.026
1.093
1.209
1.241
0.957
1.014

Method
DCNN D-Attn
0.801
0.784
0.981
0.956
1.023
0.988
1.091
1.064
1.175
1.164
1.213
1.214
0.944
0.902
0.994
0.966

SentiRec
0.763
0.910
0.977
1.044
1.120
1.164
0.879
0.939

Imprv.
2.70%
4.81%
1.14%
1.91%
3.76%
4.04%
2.46%
2.72%

Performance Analysis

Table 2 shows the test performance of all the methods in terms of
MSE. We have the following observations: 1) From the comparisons
between MF with the rest, we verify the benefit of leveraging reviews as side information for recommendations. 2) We observe that
DeepCoNN is outperformed by D-Attn, which extends DeepCoNN
by adopting the attention mechanism. This verifies that both local
and global attentions help CNNs to better understand the reviews.
Note that we show this comparison here, as it is overlooked by
D-Attn [11]. 3) SentiRec shows the best performance among all
the baselines. This verifies that encoding the overall sentiments
of reviews into fixed-size vectors indeed help us remove possible
ambiguity contained in the reviews, which eventually facilitates
to more accurately model users and items. 4) The performance
improvement of SentiRec is consistently larger under the cold-start
setting. This implies that step 1 of SentiRec successfully learns the
general representations of reviews by sharing a network throughout all the reviews, which enables step 2 of SentiRec to model
users and items even with a few reviews provided.

EXPERIMENTS

Datasets. We evaluate our proposed method on four real-world
datasets extracted from Amazon.com by McAuley et al. [7]: Office
Products, Clothing Shoes & Jewelry, Grocery & Gourmet Food,
and Sports & Outdoors. All the reviews in the datasets accompany
2 Note

Setting

Grocery

Summary. In step 1, we encode each review into a fixed-size
review vector that incorporates the overall sentiment of a user
on an item, and leverage this vector-encoded review for modeling
users and items in step 2. We postulate that the exploitation of the
overall sentiment of a user helps us remove the possible ambiguity
contained in reviews, and gives us a high-quality representation
of each review. Moreover, by representing users and items by the
concatenation of their associated fixed-size review vectors rather
than raw words, SentiRec obtains scalability; the size of the inputs
I
to NetU
2 and Net2 is drastically reduced compared with that of
the previous review-aware methods. More precisely, the size of
the input for user u is reduced from O(k × |NiU |T ) to O(l × |NiU |)
yielding us at least T times of reduction in terms of the input size2 ,
which is significant considering that T , i.e., the average number of
words in a review, is usually large (Refer to Table 1).

4

Dataset

that in our experiments, k = 100 is twice as large as l = 50.

967

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

5
4
3
2

Table 3: Training time until convergence in seconds. (Numbers in brackets: num. required epochs until convergence.)
Method
(a) D-Attn
(b) SentiRec-Step 1
(c) SentiRec-Step 2
Ratio = (a / (b +c))

Office
9,018 (6)
546 (28)
60 (15)
14.9

Datasets
Grocery
Clothing
12,762 (3) 28,494 (3)
3,303 (23) 3,104 (20)
75 (5)
75 (3)
3.8
9.0

Sports
51,900 (5)
6,867 (21)
150 (5)
7.4

(a) Associated ratings – Step 1 (left) and Step 2 (right)

Figure 2: t-SNE visualizations of vector-encoded reviews.
general sentiments among each of the higher rating groups (4 and
5) tend to agree among themselves; users and items that give and
receive ratings of 5 (Figure 2a (left)) tend to give and receive high
ratings in general (Figure 2b (left)). From the above analysis, we
can ascertain that the superior performance of SentiRec is indeed
derived from the general sentiments encoded in the review vectors.

Table 4: GPU memory usage (MB)
Method
(a) D-Attn
(b) SentiRec-Step 1
(c) SentiRec-Step 2
Ratio = (a / (b +c))

4.2

Office
5,069
835
659
3.4

Datasets
Grocery Clothing
5,227
5,211
1,353
1,361
887
1,081
2.3
2.1

Sports
5,671
1,361
1,167
2.2

5

Scalability Analysis

CONCLUSIONS

We presented SentiRec, a review-aware scalable recommendation
method that is guided by the sentiments of reviews. In order to
remove the possible ambiguity contained in reviews, we leverage
users’ overall sentiments on items expressed through the ratings,
and encode the reviews into fixed-size vectors. Then, we model
users/items by merging their associated reviews by using the vectorencoded reviews, which gives us significant reduction in the training time and the memory usage, followed by two parallel CNNs to
generate recommendations. We demonstrate that SentiRec is both
effective and efficient compared with the state-of-the-art baselines.

Training time. Table 3 shows the the training time comparisons
between SentiRec and D-Attn, the best performing baseline. As SentiRec is composed of two steps, we report the training time for each
of them. We observe that SentiRec trains at most 14.9 times faster
than D-Attn. Such improvement is derived from the reduced size
of input reviews that are encoded into fixed-size vectors in step 1.
Memory usage. Table 4 shows the GPU memory usage comparisons between SentiRec and D-Attn. We observe at most 3.4 times of
improvement of SentiRec over D-Attn. Again, such improvements
are due to the reduced input size.
From the performance analysis in Section 4.1 and the scalability analyses in Section 4.2, we verify that SentiRec is a scalable
recommendation method that is practical in reality, which even
outperforms the state-of-the-art baselines in terms of the recommendation accuracy. It is worth mentioning that the actual training
time and the memory usage of SentiRec are shorter and smaller,
respectively, because step 1 can be performed off-line in advance.

4.3

(b) Averaged ratings – Step 1 (left) and Step 2 (right)

ACKNOWLEDGMENTS
This research was supported by 1) the MSIT, Korea (IITP-20182011-1-00783), 2) Basic Science Research Program through the NRF
funded by the MSIT (NRF-2017M3C4A7063570), and 3) the NRF
grant funded by the MSIT (NRF-2016R1E1A1A01942642).

REFERENCES
[1] Trapit Bansal, David Belanger, and Andrew McCallum. 2016. Ask the gru: Multitask learning for deep text recommendations. In RecSys. ACM.
[2] David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation.
Journal of machine Learning research (2003).
[3] Donghyun Kim, Chanyoung Park, Jinoh Oh, Sungyoung Lee, and Hwanjo Yu.
2016. Convolutional matrix factorization for document context-aware recommendation. In Proceedings of the 10th ACM RecSys. ACM, 233–240.
[4] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. Computer 42, 8 (2009).
[5] Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.
Journal of machine learning research (2008).
[6] Julian McAuley and Jure Leskovec. 2013. Hidden factors and hidden topics:
understanding rating dimensions with review text. In RecSys. ACM.
[7] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel.
2015. Image-based recommendations on styles and substitutes. In SIGIR. ACM.
[8] Chanyoung Park, Donghyun Kim, Jinoh Oh, and Hwanjo Yu. 2016. Trecso:
Enhancing top-k recommendation with social information. In WWW. 89–90.
[9] Chanyoung Park, Donghyun Kim, Jinoh Oh, and Hwanjo Yu. 2017. Do AlsoViewed Products Help User Rating Prediction?. In WWW.
[10] Andrew I Schein, Alexandrin Popescul, Lyle H Ungar, and David M Pennock.
2002. Methods and metrics for cold-start recommendations. In SIGIR. ACM.
[11] Sungyong Seo, Jing Huang, Hao Yang, and Yan Liu. 2017. Interpretable convolutional neural networks with dual local and global attention for review rating
prediction. In RecSys. ACM.
[12] Chong Wang and David M Blei. 2011. Collaborative topic modeling for recommending scientific articles. In SIGKDD. ACM.
[13] Hao Wang, Naiyan Wang, and Dit-Yan Yeung. 2015. Collaborative deep learning
for recommender systems. In SIGKDD. ACM.
[14] Lei Zheng, Vahid Noroozi, and Philip S Yu. 2017. Joint deep modeling of users
and items using reviews for recommendation. In WSDM. ACM.

Qualitative Analysis

In this section, we aim to qualitatively demonstrate that the overall
sentiments of reviews are indeed encoded in the review vectors after
training SentiRec. To this end, we perform t-SNE [5] visualizations
on the review vectors fu ,i obtained after training step 1 and step 2
of SentiRec. More precisely, each review vector fu ,i is projected to a
point, and it is colored once by its associated rating ru ,i , and another
time by the average of the ratings given by all users that rated the
item i and the ratings given to all items rated by the user u. i.e.,
Í
Í
0.5×(1/|NiU | u ∈N U ru ,i +1/|NuI | i ∈NuI ru ,i ). The latter one is to
i
determine the general sentiment of user u and item i each associated
with fu ,i . Figure 2a (left) shows that the review vectors are clearly
grouped into their corresponding ratings, verifying that step 1 is
correctly trained to reflect the ratings as expected. On the other
hand, whereas we observe from Figure 2a (right) that the reviews
with similar ratings are still grouped together even after training
step 2, the general sentiments are also revealed in the reviews as shown
in Figure 2b (right). Specifically, when compared with Figure 2b
(left), we can clearly see certain trends that the reviews belonging
to ratings 1, 2 and 3 are rearranged among themselves according
to the general sentiments of the reviews. Such rearrangements are
mainly exposed among lower rating gruops (1, 2 and 3), because the

968

