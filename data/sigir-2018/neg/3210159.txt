Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

K-plet Recurrent Neural Networks for Sequential
Recommendation
Xiang Lin

Shuzi Niu

University of Chinese Academy of Sciences
linxiangblcu@gmail.com

Institute of Software, Chinese Academy of Sciences
shuzi@iscas.ac.cn

Yiqiao Wang

Yucheng Li

University of Chinese Academy of Sciences
wangyiqiao15@mails.ucas.ac.cn

Institute of Software, Chinese Academy of Sciences
yucheng@iscas.ac.cn

ABSTRACT

1

Recurrent Neural Networks have been successful in learning meaningful representations from sequence data, such as text and speech.
However, recurrent neural networks attempt to model only the
overall structure of each sequence independently, which is unsuitable for recommendations. In recommendation system, an optimal
model should not only capture the global structure, but also the
localized relationships. This poses a great challenge in the application of recurrent neural networks to the sequence prediction
problem. To tackle this challenge, we incorporate the neighbor
sequences into recurrent neural networks to help detect local relationships. Thus we propose a K-plet Recurrent Neural Network
(Kr Network for short) to accommodate multiple sequences jointly,
and then introduce two ways to model their interactions between
sequences. Experimental results on benchmark datasets show that
our proposed architecture Kr Network outperforms state-of-the-art
baseline methods in terms of generalization, short-term and long
term prediction accuracy.

Recurrent Neural Networks are supposed to be able to capture
sequential pattern successfully from sequence data, such as speech
and text [10, 17, 21]. Recurrent networks retain a state that can
represent information from an arbitrarily long context window [10].
So it can capture time dependencies in each sequence globally.
Sequential recommendation is solved by recurrent neural network as follows: At each time step, it takes one item of the user’s
sequence in a sequential order. Then, it encodes this item’s information in the context for the next step. Finally, it utilizes the current
item information and the context information at last step to predict
the next item. Through this process, recurrent neural networks only
learn the global information from each sequence alone. However,
neither the global method (latent factor model) or the local approach (neighborhood approach) performs the best [9]. This poses
a great challenge in the application of recurrent neural network to
sequential recommendation.
To tackle this challenge, we introduce neighborhood model into
recurrent neural networks to capture both the local and global information at the same time. Therefore we propose a novel network
structure, k-plet recurrent neural network (inspired by triplet network [7]), to accommodate the query sequence and its k neighbor
sequences jointly. Kr network (short for k-plet recurrent neural
network) is comprised of k + 1 instances of the same recurrent
neural networks.
There are various ways to model the interaction between the
query sequence and its neighbor sequences in the Kr network. The
first and intuitive way is to introduce the interaction into the loss
function, denoted as KrNN-L. Specifically, we model the similarity
between the query sequence and its neighbors, and employ this similarity as the regularization of the original loss function, e.g. cross
entropy loss. We optimize this similarity regularized loss to learn
the network parameters. A deeper interaction is built in the prediction function of the query sequence, and we denote this method as
KrNN-P. In light of k-NN classification, we predict the next item
distribution of the query sequence as the weighted sum of probabilistic outputs of the query sequence and its neighbors. Weights
are estimated as the similarity between the query sequence and its
neighbors, such as the Euclidean distance. Experimental results on
benchmark datasets MovieLens-1M and Amazon Movies show that
our proposed Kr Network outperforms state-of-the-art baselines
but needs more time for training compared with traditional RNNs.
Our main contribution lies in the following two aspects:

CCS CONCEPTS
• Computer systems organization → Embedded systems; Redundancy; Robotics; • Networks → Network reliability;

KEYWORDS
Sequence prediction; Similarity Regularization; K-plet Network;
Sequential Recommendation
ACM Reference Format:
Xiang Lin, Shuzi Niu, Yiqiao Wang, and Yucheng Li. 2018. K-plet Recurrent
Neural Networks for Sequential Recommendation. In SIGIR ’18: 41st International ACM SIGIR Conference on Research and Development in Information
Retrieval, July 8-12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA,
4 pages. https://doi.org/10.1145/3209978.3210159

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210159

1057

INTRODUCTION

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

SIGIR’18, July 8–12, 2018, Ann Arbor, MI, USA

Xiang Lin, Shuzi Niu, Yiqiao Wang, and Yucheng Li
time stamp, i.e. |stu | > 1; for sequential recommendation in MovieLens and Amazon Product dataset, there is usually one item per time
m .
stamp such as |stu | = 1. The history for all users is S = {S ui }i=1
Vanilla RNN was first used to predict the next tracks users
wanted to play in the blog [1]. Then, variants of RNNs were comprehensively studied for sequential recommendation task [2]. GRU
based RNN was used for session-based recommendation with two
types of ranking loss functions: BPR [12] and their first devised
TOP1 [5]. DREAM [19] utilized pooling to summarize the basket of
embedded items and then fed into vanilla RNN to solve next basket recommendation. All these studies consider RNN as a function
fθ (·) of a user sequence, for example LSTM [6] as Eq.( 1), where
θ = {Wi ,Wf ,WC ,Wo , bi , bf , bC , bo }.

(1) We propose a K-plet recurrent neural network architecture,
referred to as Kr Networks, to capture both the local and
global information in the sequences for better recommendation.
(2) We introduce two distinguished ways to model the interaction between the query sequence and its neighbors in Kr
Network.

2

BACKGROUND

In this section, we first review some related work on recommendation algorithms. Then we formalize the sequence prediction problem in sequential recommendation with recurrent neural networks.
Finally, we introduce some related network structures in deep metric learning.

2.1

i tu = σ (Wi · [hut−1 , x tu ] + bi

ftu = σ (Wf · [hut−1 , x tu ] + bf )

Recommendation Algorithms

Ctu = ftu ∗ Ct −1 + i tu ∗ tanh(WC · [ht −1 , x tu ] + bC )
hut = σ (Wo · [hut−1 , x tu ] + bo ) ∗ tanh(Ctu )

Recommendation algorithms aim at matching the user interests
with the item set. According to different views on users’ historical behavior, recommendation algorithms mainly falls into two
categories: general and sequential recommendation.
General Recommendation. Users’ historical behaviors, such
as rating and consuming an item, are represented as a set by ignoring the temporal factor. Matrix factorization techniques are
popular and effective methods to capture the general interests of
users. Among these, Bayesian Personalized Rank (BPRMF [13]) is
famous for its pairwise learning strategy to solve the factorization
problem [12].
Sequential Recommendation. Different from general recommendation, the sequential pattern in the user’s rating or buying
behavior is of the main concern in the sequential recommendation.
Existing sequential recommendation algorithms mainly include
the Markov Chain and Recurrent neural network based methods.
Markov Chain models were first introduced into web page recommendation [22]. In most Markov Chain models, sequence data is
transformed into transition graph. Factorized Personalized Markov
Chain [14] is such a method that is combined with matrix factorization in order to handle sparsity. To improve the performance further,
a Markov Decision Process based recommendation method was
proposed [15]. Hierarchical Representation Model combined the
last action information with the general user interest to model user
representations for next basket recommendation [18]. For these
Markov Models, it is difficult to model the long-range dependence.
Thus recurrent neural network based models appear to overcome
this problem, which will be discussed in the following section.

2.2

(1)

CRNN [8] learned a RNN fθ u for each user u, shared Wi and
Wo among all users. Thus CRNN is prone to overfit for its overparameterization. Whatever, RNN and its variants are only good
at capturing the overall structure in the sequence data. Thus their
performances are not meant to be optimal.

3 K−PLET RECURRENT NEURAL NETWORK
Here we introduce K-plet Recurrent Neural Network, i.e. Kr Network. Kr Network is composed of k +1 instances of recurrent neural
q
networks: one for the query sequence S ⪯t , the other for the neighq
borhood sequences N⪯t . When we use Kr network to predict what
will be the next item at time step t + 1 with the training instance
q
q
S ⪯t ∪ N⪯t , the following two questions are of the most concerned:
• How to define the sequence similarity to determine the neighbor sequences in Kr Network?
• How to model the interaction between the query sequence
and its neighbors in the sequence prediction task?
We first briefly describe the framework of K-plet Recurrent Neural
Networks (Kr Network for short, a.k.a KrNN), then two different
sequence interaction modeling methods are proposed, referred to
as KrNN-L and KrNN-P separately.

3.1

Framework

Given the query sequence, we determine its neighbor sequences by
k nearest neighbor search based on the sequence similarity. Then
we feed one RNN with the query sequence, and use the other k
RNNs for its neighbors. Parameters are shared among k + 1 RNNs.
Finally, we model the RNN interaction at the loss layer or the output
layer, denoted as KrNN-L and KrNN-P repectively. Thus Kr Network
is built.

Recurrent Neural Networks for Sequential
Recommendation

In light of the success of RNN in sequence modeling, many RNN
based approaches reduce the recommendation task to the sequence
prediction problem. Suppose there are m users denoted as U =
{u 1 , . . . , um } and n items A = {a 1 , . . . , an }. For each user u, the
purchase or rating behavior is formalized as a sequence S u⪯t =
(s 1u , . . . , stu ), stu ⊆ A. A subsequence of user u from timestamp t 0
u
to t is represented as S [t
= (stu0 , . . . , stu ). For next basket recom0,t ]
mendation, there are usually more than one items in stu for each

3.2

KrNN-L

It is well known that two instances with similar inputs are supposed
to have similar labels. For the sequence prediction task, we reverse
this assumption, and suppose two context sequences to be similar if
their next item predictions are the same. Given the query sequence
q
i
S ⪯t , we extract its similar sequence as S[max
from each
(0,t −l ),t ]
i

1058

i

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

K-plet Recurrent Neural Networks for Sequential Recommendation

SIGIR’18, July 8–12, 2018, Ann Arbor, MI, USA

q

user sequence S i if st +1 = sti i +1 and t ≥ ti . l is the context window,
which are set as 200 in our experiments. We use k-NN search to
q
q
find k neighbor sequences of S ⪯t , denoted as N⪯t .
We introduce a special kind of graph regularization [16, 20] at the
loss layer to add local information to the recurrent neural network,
namely KrNN-L. The underlying assumption is that two users’
context sequences with the same next item share similar latent
representations. Following this assumption, the local consistency
respecting the item similarity can be preserved with the addition
of Lsr . Thus we incorporate the sequence similarity into the loss
function, and obtain the similarity regularized objective function
as Eq.( 2), where λ is the parameter controls the trade-off between
item prediction error and similarity regularization.
Õ
q
i
2
L = Ler r or +λLsr = Ler r or +λ
∥h(S ⪯t )−h(S[max
(0,t −l ),t ] )∥2
i

q

i ∈N⪯t

i

(2)
This similarity regularized loss function is optimized through Adagrad [3].h(.) is the output of hidden layer. In our experiments, the
prediction error Ler r or is Cross Entropy loss.

3.3

KrNN-P

Different from the sequence similarity definition used in KrNNq
L, KrNN-P employs the neighbor sequences N⪯t to play a role
q
in the prediction function of the query sequence S ⪯t . So here
we utilize another sequence similarity based on Euclidean disq
i
tance between S ⪯t and S[max
(t ≥ ti ) denoted as γq,i =
(0,t −l ),t ]
1
q
i
∥v(S⪯t )−v(S[max
(0, t

i

i −l ), ti

) ∥+C
]

i

, where v(S) is the item frequency vec-

tor of sequence S and C is a constant. Based on the sequence similarity γq,i , we use k-NN search to find k neighbor sequences of
q
q
S ⪯t , denoted as N⪯t .
In light of k-NN classification, we propose a weighted sum of
the network outputs of the query sequence and its neighbors as the
final output as follows:
Õ
q
q
q
i
F (S ⪯t , N⪯t ) = fθ (S ⪯t )+
γq,i fθ (S[max
(0,t −l ),t ] )
i
S[max
(0, t

q

i −l ), t i ]

i

i

∈N⪯t

EXPERIMENTAL RESULTS

To explore the performance of our Kr Network, we conduct comprehensive experiments on two benchmark datasets for sequential
recommendation.

4.1

4.2

Quantitative Analysis

All the performance differences between Kr Network and baselines
are statistically significant with p-value < 0.01 for paired t-test.
Short-term Prediction As shown in Table 1, KrNN-L and KrNNP outperform other recommendation algorithms in short term prediction. For example, sps@10 of KrNN-L is 34% higher than the
best general recommendation UKNN on MovieLens 1M and 1.7
times higher on Amazonmovies. Compared with MC on MovieLens
and Amazonmovies, the sps@10 of KrNN-P improvement is 53.5%
and 39.9%. For KrNN-L, sps@10 is 5% and 97% better than RNN on
MovieLens and Amazonmovies seperately. Compared with MrRNN,
KrNN-L improves sps@10 by 23% on Amazonmovies.
Long-term Prediction As shown in Table 1, RNN based method
outperform others. KrNN-L model achieve better performance than
others recommendation algorithms on both datasets. On the Amazonmovies, the ndcg of KrNN-L is 38% higher than best general
methods UKNN. Compared with MC methods on the Movielens
1M, KrNN’s recall@10 is 50% higher and the precision@10 is 44.8%
higher. Compared with RNN model, KrNN-L achieves better performance on both the four metrics especially on Amazonmovies.

(3)
where fθ (·) is the output of each recurrent neural network and F (·)
is the final output of the query sequence. We refer to this method
as KrNN-P. In the experiments, we optimize Cross Entropy loss
through Adagrad.

4

remove the users with the number of ratings less than 50. Finally
there are 3, 241 users and 24, 355 items, and 451, 804 ratings for our
experiments.
Baselines. As we introduce in background, we use three kinds
of baselines:
(1) General recommendation methods: POP, UKNN, and BPRMF [13].
(2) Markov Chain based sequential recommendation methods:
MC [2] and FPMC [14].
(3) RNN based sequential recommendation methods: RNN [2]
and manifold regularized output of the recurrent neural network denoted as MrRNN[11]. Our Kr Network: KrNN-L and
KrNN-P.
Evaluation Metric. We employ three kinds of metrics: (1)Short
term prediction: sps. A short term prediction aims to predict items
which will be bought or rated at next time [2]. (2)Long term prediction: precision, recall, ndcg and F1-score. (3)Generalization:
user_coverage. We use the fraction of users who received at least
one correct recommendation. All those metrics are cut off at 10
(@10) in the experiments.
Evaluation Protocol.For each user in each dataset, we transform his or her historical behavior into a rating or consuming
sequence according to time stamps. Each dataset is split into train,
validation, and test set by a proportion 8:1:1 according to the number of users. We use 10-fold cross validation to tune the parameter
and all the performances reported in our quantitative analysis section is averaged on test sets of these 10 folds.
Parameter Tuning.
We choose the best parameter setting over the 10-fold validation
set. (1) BPRMF and FPMC: the latent dimension is set 32 for both,
the learning rate is 0.05 and 0.2 respectively, the sampling bias is 200
for both, σ is 1 and 0.01 respectively. (2) UKNN: the neighborhood
size is 80. (3)RNN, MrRNN, KrNN-P and KrNN-L: one LSTM layer
with 50 hidden units, the learning rate is set as 0.1 and we all use
the adagrad for optimization. The sample size is 7 and the trade-off
parameter is 0.5.

Experimental Setting

Datasets. We use two benchmark recommendation datasets: Movielens 1M and Amazon Product Data [4] with category Movies and
TV, denoted as amazonmovies in our experiments. (1) Movielens
1M: This dataset is a subset of the Movielens dataset with 6, 040
users, 3, 706 movies and 1, 000, 209 ratings. (2) Amazonmovies:
In the category of Movies and TV in Amazon Product Data, we

1059

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

SIGIR’18, July 8–12, 2018, Ann Arbor, MI, USA

Xiang Lin, Shuzi Niu, Yiqiao Wang, and Yucheng Li

Table 1: Performance Comparison on Movielens 1M and
Amazonmovies

sequences jointly. We introduce two distinguished ways to model
the interaction between the query sequence and its neighbors. Experimental results on benchmark datasets show that our proposed
methods outperform others. In future, we will explore to make
K-NN search more efficient for GPU.

(a) Movielens 1M

Methods

s@10

r@10

p@10

F1

n@10

u@10

POP
UKNN
BPRMF
MC
FPMC
RNN
MrRNN
KrNN-P
KrNN-L

5.02
21.80
1.40
19.1
15.4
27.88
28.62
29.32
29.29

3.91
14.34
1.05
5.14
4.68
7.56
7.52
7.43
7.70

23.60
19.42
6.99
22.30
21.20
30.78
30.78
30.37
32.28

6.71
16.50
1.83
8.35
7.67
12.14
12.09
11.94
12.43

24.50
20.10
7.90
23.90
22.28
32.29
32.17
31.98
33.92

70.77
82.25
42.30
77.94
76.57
87.66
86.50
87.71
88.45

6

The research was supported by National Natural Science Foundation of China under Grant No.61602451.

REFERENCES
[1] Erik Bernhardsson. 2014.
Recurrent Neural Networks for
Collaborative Filtering.
https://erikbern.com/2014/06/28/
recurrent-neural-networks-for-collaborative-filtering.html.
[2] Robin Devooght and Hugues Bersini. 2016. Collaborative Filtering with Recurrent
Neural Networks. CoRR abs/1608.07400 (2016).
[3] John C Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning
Research 12 (2011), 2121–2159.
[4] Ruining He and Julian McAuley. 2016. Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering. In Proceedings
of the 25th International Conference on World Wide Web (WWW ’16). International World Wide Web Conferences Steering Committee, Republic and Canton
of Geneva, Switzerland, 507–517. https://doi.org/10.1145/2872427.2883037
[5] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.
2015. Session-based Recommendations with Recurrent Neural Networks. CoRR
abs/1511.06939 (2015).
[6] Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long Short-Term Memory.
Neural Computation 9, 8 (1997), 1735–1780.
[7] Elad Hoffer and Nir Ailon. 2014. Deep metric learning using Triplet network.
(2014), 84–92.
[8] Young Jun Ko, Lucas Maystre, and Matthias Grossglauser. 2016. Collaborative
Recurrent Neural Networks for Dynamic Recommender Systems. In Journal of
Machine Learning Research: Workshop and Conference Proceedings, Vol. 63.
[9] Yehuda Koren. 2008. Factorization Meets the Neighborhood: A Multifaceted
Collaborative Filtering Model. In Proceedings of KDD 2008. 426–434.
[10] Zachary Chase Lipton. 2015. A Critical Review of Recurrent Neural Networks for
Sequence Learning. CoRR abs/1506.00019 (2015). http://arxiv.org/abs/1506.00019
[11] Shuzi Niu and Rongzhi Zhang. 2017. Collaborative Sequence Prediction for
Sequential Recommender. In Proceedings of the 2017 ACM CIKM (CIKM ’17).
ACM, New York, NY, USA, 2239–2242.
[12] Steffen Rendle and Christoph Freudenthaler. 2014. Improving Pairwise Learning
for Item Recommendation from Implicit Feedback. In Proceedings of WSDM 2014.
[13] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidtthieme.
2009. BPR: Bayesian personalized ranking from implicit feedback. (2009), 452–
461.
[14] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factorizing Personalized Markov Chains for Next-basket Recommendation. In Proceedings
of WWW2010. 811–820.
[15] Guy Shani, David Heckerman, and Ronen I. Brafman. 2005. An MDP-Based
Recommender System. J. Mach. Learn. Res. 6 (Dec. 2005), 1265–1295.
[16] Alexander J Smola and Risi Kondor. 2003. Kernels and Regularization on Graphs.
computational learning theory (2003), 144–158.
[17] Ilya Sutskever, Oriol Vinyals, and Q Le. 2014. Sequence to sequence learning
with neural networks. (2014), 3104–3112.
[18] Pengfei Wang, Jiafeng Guo, Yanyan Lan, Jun Xu, Shengxian Wan, and Xueqi
Cheng. 2015. Learning Hierarchical Representation Model for NextBasket Recommendation. In Proceedings of SIGIR 2015. 403–412.
[19] Feng Yu, Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. 2016. A Dynamic
Recurrent Model for Next Basket Recommendation. In Proceedings of SIGIR 2016.
729–732.
[20] Xiao Yu, Xiang Ren, Quanquan Gu, Yizhou Sun, and Jiawei Han. [n. d.]. Collaborative Filtering with Entity Similarity Regularization in Heterogeneous Information
Networks.
[21] Matthew D Zeiler, Marcaurelio Ranzato, Rajat Monga, Mark Z Mao, Ke Yang, Q
Le, Patrick Nguyen, A Senior, Vincent Vanhoucke, Jeffrey Dean, et al. 2013. On
rectified linear units for speech processing. (2013), 3517–3521.
[22] Andrew Zimdars, David Maxwell Chickering, and Christopher Meek. 2001. Using
Temporal Data for Making Recommendations. In Proceedings of UAI 2001.

(b) Amazonmovies

Methods

s@10

r@10

p@10

F1

n@10

u@10

POP
UKNN
BPRMF
MC
FPMC
RNN
MrRNN
KrNN-P
KrNN-L

0.65
1.70
0.38
2.63
2.20
2.28
3.65
3.68
4.50

0.78
1.09
0.31
1.42
0.83
1.09
1.10
1.20
1.40

4.98
4.62
1.99
7.74
4.76
6.49
6.47
6.71
8.28

1.35
1.76
0.53
2.39
1.41
1.86
1.88
2.03
2.40

5.08
4.86
2.22
7.91
5.04
6.77
6.90
6.97
8.75

31.38
42.73
16.30
39.39
28.10
33.99
36.54
35.64
40.62

Table 2: Efficiency Comparison on MovieLens
Methods

K=1

K=3

K=5

K=7

K=15

KrNN-L
KrNN-P

658.00
1253.55

815.95
1360.75

967.96
1423.07

1186.22
1456.00

1810.80
1551.21

Generalization As shown in Table 1, for generalization performance, KrNN-P and KrNN-L achieves better performance than
others on movielens 1M, KrNN-L achieves nearly the same generalization performance as the user_coverage@10 is 5% lower than
UKNN, but it is 7% higher than UKNN on Amazonmovies. Compared with other baselines, KrNN’s advantage is clear, KrNN-L’s
user_coverage@10 is 13.5% higher than MC and 11% higher than
MrRNN on the MovieLens 1M.

4.3

Efficiency Analysis

Here we explore the sample size effect on the efficiency of KrNN-L,
KrNN-P and compare the training time (seconds) per epoch between
them on MovieLens 1M. The time of orignal RNN is 613.58s.
Through Table 2, we can see the sample size effect on the efficiency of KrNN is large. But k needs not to be large. We’ve proved
that only k = 1 is enough to achieve better performance in the two
datasets in our experiments. All the training time reported here is
based on one Nvidia K40m GPU.

5

ACKNOWLEDGMENTS

CONCLUSION

In this paper, We propose Kr Networks to capture both the local
and global information in the sequences for better recommendation, which accommodate the query sequence and its k neighbor

1060

