Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Reducing Variance in Gradient Bandit Algorithm using
Antithetic Variates Method
Sihao Yu, Jun Xu∗ , Yanyan Lan, Jiafeng Guo, Xueqi Cheng
1

University of Chinese Academy of Sciences, Beijing, China
2 CAS Key Lab of Network Data Science and Technology,
Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China
yusihao@software.ict.ac.cn,{junxu,lanyanyan,guojiafeng,cxq}@ict.ac.cn

ABSTRACT

1

Policy gradient, which makes use of Monte Carlo method to get
an unbiased estimation of the parameter gradients, has been widely
used in reinforcement learning. One key issue in policy gradient
is reducing the variance of the estimation. From the viewpoint
of statistics, policy gradient with baseline, a successful variance
reduction method for policy gradient, directly applies the control
variates method, a traditional variance reduction technique used in
Monte Carlo, to policy gradient. One problem with control variates
method is that the quality of estimation heavily depends on the
choice of the control variates. To address the issue and inspired by
the antithetic variates method for variance reduction, we propose
to combine the antithetic variates method with traditional policy
gradient for the multi-armed bandit problem. Furthermore, we
achieve a new policy gradient algorithm called Antithetic-Arm
Bandit (AAB). In AAB, the gradient is estimated through coordinate
ascent where at each iteration gradient of the target arm is estimated
through: 1) constructing a sequence of arms which is approximately
monotonic in terms of estimated gradients, 2) sampling a pair of
antithetic arms over the sequence, and 3) re-estimating the target
gradient based on the sampled pair. Theoretical analysis proved
that AAB achieved an unbiased and variance reduced estimation.
Experimental results based on a multi-armed bandit task showed
that AAB can achieve state-of-the-art performances.

Reinforcement learning, including the Multi-Armed Bandit(MAB) [7]
and Markov Decision Process(MDP) [5], have been successfully
used in variant machine learning applications recently. Among the
algorithms that solve the reinforcement learning problems, policy
gradient [13] has shown its advantages in effectiveness in highdimensional/continuous action spaces, fast convergence rate, and
handling stochastic policies etc. Roughly speaking, policy gradient
relies upon optimizing parametrized policies (a distribution over
the agent actions) with respect to the expected return (long-term
cumulative reward) by gradient ascent.
To calculate the parameter gradients at each optimization iteration, policy gradient algorithms such as REINFORCE [12, 13]
usually adopt the Monte Carlo method [9] to estimate the expectation of the gradient. The gradient estimated by the Monte Carlo
method is unbiased but usually has large variance, which hurts
the efficiency and effectiveness of the traditional policy gradient
algorithm. How to reduce the variance of the estimated gradient
becomes a key issue in policy gradient algorithms.
A number of research has been conducted to reduce the gradient
variance in policy gradient. For example, Policy gradient with baseline [3] is commonly used in real reinforcement learning tasks. In
the method, a baseline variate, which is designed as the averaged
rewards of the history steps, is first designed. Then, the real reward
of the action minus the baseline is used as the reward for the gradient estimation and parameter updating. It also shows that variance
of the newly estimated gradients is reduced while its expectation is
identical to that of the traditional policy gradient. More methods on
variance reduction for policy gradient please referred to [1, 8, 11]
From the viewpoint of statistics, the policy gradient with baseline is a direct application of control variates method [2], a variance
reduction approach in Monte Carlo Method, to improve the traditional policy gradient. The baselines are implementations of the
control variates in the reinforcement learning environment. In general, designing reliable control variates is critical for the success
of control variates method. Inappropriate setting of the control
variates may result in the raising of variance and hurt the estimation. When applied to reinforcement learning, though the policy
gradient with baseline heuristically constructs the baselines, which
is far away from the ideal control variates, it is difficult to achieve
its optimal effect.
To get rid of this problem, antithetic variates method [4] is proposed. Every time antithetic variates method draws a pair of antithetic samples for the estimation. Since one antithetic sample in
the pair is easily derived from another, the auxiliary functions (e.g.,
the control variates) is not a mandatory anymore.

KEYWORDS
Policy gradient; Antithetic variates; Coordinate gradient
ACM Reference Format:
Sihao Yu, Jun Xu∗ , Yanyan Lan, Jiafeng Guo, Xueqi Cheng. 2018. Reducing
Variance in Gradient Bandit Algorithm using Antithetic Variates Method.
In SIGIR ’18: The 41st International ACM SIGIR Conference on Research &
Development in Information Retrieval, July 8–12, 2018, Ann Arbor, MI, USA.
ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210068

∗

Corresponding author: Jun Xu

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210068

885

INTRODUCTION

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA
Sihao Yu, Jun Xu∗ , Yanyan Lan, Jiafeng Guo, Xueqi Cheng

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA

Í
sample
where n ≥ 1, the = samples x i ∼ πt , and η = n1 ni=1 q(x i )(1a=x i −
πt (a)). The equation above shows that η is an unbiased estimation
∂E[R t ]
for the gradient of R t (i.e., E[η] = ∂H (a)
). Thus, one effective way
t
to the accuracy of the estimation is to reduce the variance V[η].

Based on the observation, we propose a novel policy gradient
method which uses antithetic variates to improve policy gradient
for MAB. The proposed method, referred to as Antithetic-Arm
Bandit (AAB), estimates the parameter gradients through sampling
a pair of antithetic arms at each time. To achieve this, AAB adopts
the coordinate ascent framework for the optimization where each
coordinate corresponds an arm. At each iteration, the arms are
sorted according to their estimated gradients. After that, a pair of
antithetic arms are sampled on the basis of the sorted arms. The
gradient of the target arm (determined with another sampling) is
then re-estimated and updated.
Theoretical analysis showed that the gradients calculated with
AAB was an unbiased estimation and the variance of the estimation
was effectively reduced with high confidence.
Experiments were conducted to show the effectiveness of the
proposed AAB. The experimental results based on an MAB task
showed that AAB outperformed the baseline of traditional policy
gradient and achieved comparable performances with the policy
gradient with baseline.

2

2.2

BACKGROUND: VARIANCE REDUCTION IN
POLICY GRADIENT

This section introduces the formulation of variance reduction methods in the policy gradient for the multi-armed bandit problem.

2.1

Control variates method for policy gradient

Policy gradient with baseline is an effective method to reduce the
variance of η. In the k-armed bandit problem, it calculates the gradiÍ
ent of Ht (a) as n1 ni=1 [q(x i ) · (1a=x i − πt (a)) − R · (1a=x i − πt (a))],
where R is the averaged rewards of the history samplings.
From the viewpoint of statistics, policy gradient with baseline is
an application of control variates in policy gradient. The original
control variates method can be described as follows: considering the
estimation θ of an integral of function f (x), Monte Carlo method
Í
directly uses n1 ni=1 (ξ i ), where ξ i ∼ U (0, 1) and n ≥ 1, as an
estimation of θ , which is an obviously unbiased estimation.
The control variates method, on the other hand, estimates the
integral with:
∫ 1
∫ 1
∫ 1
θ=
f (x)dx =
[f (x) − cд(x)]dx + c
д(x)dx
0
0
0
(3)
∫ 1
=
[f (x) − cд(x)]dx + C,
0

∫1
where c and C = c 0 д(x)dx are constants. Define a new random variable ζ = f (ξ ) − cд(ξ ) + C, where ξ ∼ U (0, 1) is a random
number uniformly distributed over the interval (0, 1). It is simple to calculate that E[ζ ] = E[f (ξ )], and V[ζ ] ≤ V[f (ξ )] when
Corr [f (ξ ), д(ξ )] ≥ 0, thus, ζ could be a better unbiased estimation
with reduced variance for the integral.
In policy gradient with baseline, R ·(1a=x i −πt (a)) corresponds to
cд(x), R·E[(1a=x i −πt (a))] = 0 corresponds to C, and Corr [R(1a=x i −
πt (a)), q(x i )(1a=x i − πt (a))] ≥ 0. Thus, policy gradient with baseline can be considered as an application of control variates method
to policy gradient.

Gradient bandit algorithm

Suppose we are facing repeatedly with a choice among k different
actions. After each choice, we receive a numerical reward chosen
from a stationary probability distribution that depends on the selected action. Each action has an expected reward, called value. The
objective is to maximize the expectation of total reward over some
time periods. There are two targets in the game, that is, finding
the best action which has the largest value and maximizing the
accumulative reward in limited time periods.
Policy gradient aims to learn a numerical preference Ht (a) for
each action a = {1, 2, · · · k}, to calculate the policy πt (a) on time
step t. Denote the action selected on time t as At , the corresponding
reward as R t , and the value of selecting an action a as q ∗ (a) =
E[R t |At = a]. The policy πt (a distribution over the actions) is
defined as the softmax over the preferences Ht :
exp{Ht (a)}
πt (a) = Pr (At = a) = Í
.
(1)
k
0
0
a =1 exp{H t (a )}

2.3

Antithetic Variates Method

In statistics, antithetic variates method is another approach to reducing the variance of Monte Carlo method. Still consider the problem
∫1
of estimating θ = 0 f (x)dx. Antithetic variates method adopts the
stratified sampling strategy [10]. The sampler chooses a fixed set of
numbers 0 = α 0 < α 1 < · · · < α n = 1 and defines a new estimation
ζ
n
Õ
ζ =
(α j − α j−1 )f [α j−1 + (α j − α j−1 )ξ j ],
(4)

The policy in Equation (1) is used to play the bandit game, and
Í
the expected reward at time step t is E[R t ] = a πt (a)q ∗ (a). In
∂E[R t ]
principle, the gradient of the expected reward ∂H (a)
is used to
t
update the preferences of the actions. However, it is difficult to
calculate E[R t ] and its partial gradient to preference because q ∗ (a)
is unknown. Monte-Carlo method is used to estimate the gradient.
The basic idea is the system gets a sample q(a) as a reward when
action a has been issued. Thus, the gradient is estimated as:
∂E[R t ] Õ
=
πt (b)q ∗ (b)(1a=b − πt (a))
∂Ht (a)
b
" n
#
(2)
1Õ
sample
= E
q(x i )(1a=x i − πt (a)) = E [η]
n i=1

j=1

where the ξ j ∈ U (0, 1)(j = 1, · · · , n). It can be shown that ζ is an
Í
unbiased estimation for θ and V(ζ ) ≤ V( n1 ni=1 f (ξ i )). Specifically, antithetic variates method constructs antithetic pair among
the variables f [α j−1 + (α j − α j−1 )ξ j ] (i.e.,Cov[η 1 , η 2 ] < 0 where
η j = f [α j−1 + (α j −α j−1 )ξ j ], j = 1, 2, . . . , n) to get the smaller V(ζ ).

3

OUR APPROACH: ANTITHETIC-ARM BANDIT

This section proposes Antithetic-Arm Bandit (AAB), a novel variance reduction policy gradient algorithm for multi-armed bandit,
on the basis of antithetic variates method.

886

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Reducing Variance in Gradient Bandit Algorithm using Antithetic Variates Method

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA

Algorithm 1 Antithetic-Armed Bandit (AAB)

Algorithm 2 Sort function σ

Input: Action (arm) set A = {1, · · · , k }, number of iterations T,
stratified parameter m, learning rate λ
1: H t ← 0
2: for t = 1 to T do
n
o

Input: Sorted index I, solved policy p, random variable ξ
Output: action a
1: for a = 1 to length of p do
2:
if ξ − p(I(a)) < 0 then
3:
return I(a)
4:
end if
5: end for

k

3:
4:
5:
6:

7:

8:
9:
10:
11:
12:
13:
14:
15:

3.1

exp H (a)
πt ← Í 0 exp tH (a 0 )
{Equation (1)}
t
a
a=1
Sample an arm d(∈ A according to πt
0
a =d
∀a ∈ A, p(a) ←
π t (a)
otherwise
(1−π t (d))
∀a ∈ A, I(a) ←Index of action a after descent sorting arms
according to p
Í
1
α← m
i=1 πt (I(i))+ 2 πt (I(m+1)){Heuristics for calculating
α}
Sampling ξ ∼ U (0, 1)
ξ 1 ← αξ , ξ 2 ← 1 − (1 − α)ξ
a 1 ← σ (I,p,ξ 1 ), a 2 ← σ (I,p,ξ 2 ){Algorithm 2}
дd ← q(d)(1 − πt (d))
д1 ← q(a 1 )(−πt (a 1 )); д2 ← q(a 2 )(−πt (a 2 ))
ζd ← πt (d)дd + (1 − πt (d))(αд1 + (1 − α)д2 ) {Equation (5)}
Ht (d) ← Ht (d) + λζa
end for

sampled from A \ {d }, and the two antithetic random variables д1
and д2 are constructed. It can be shown that д1 and д2 have high
probability to be antithetic, because the arms were sorted. Finally
the gradient for the chosen arm d is calculated and the policy is
updated. The iteration is repeated until converge.
Intuitively, AAB constructs a monotonic compound function
Gd f πt σ which makes Cov[д1 , д2 ] < 0 because Cov[ξ 1 , ξ 2 ] < 0. In
the next section, we show that AAB makes an unbiased estimation
of the gradient and reduces the variance of the estimated gradients.

3.2

Antithetic-Arm Bandit

Theorem 3.1. ∀t = 1, 2, · · · , and ∀d ∈ A = {1, 2, . . . , k}, the
expectation of ζd in Equation (5) satisfies E[ζd ] = E[Gd f πt (ξ )],
where ξ ∼ U (0, 1).

Suppose an k-armed bandit problem where the set of actions are A =
{1, 2, · · · , k } and each action a ∈ A represents the a-th arm of the
bandit. AAB updates the gradient of different arms with coordinate
ascent. Given an arm a, the estimation of gradient proposed by
Monte Carlo method for a is
n
1Õ
q(x i )(1a=x i − πt (a))
η=
n i=1

Proof. Denote pi j as the probability of choosing the actions
i and j at the same time after action d being chosen. And we set
pi j = 0 if i = d, j = d or i > j. We have
E[ζd ] =πt (d)Gd (d) + (1 − πt (d))

where x i ∼ πt , πt is the current policy at time step t, and n is the
number of samplings.
Denote the random sampled arm according to current policy πt
as f πt (ξ ) where ξ ∼ U (0, 1). Also denote the function for calculating
the gradient as G. Thus, after taking an action x i = f πt (ξ ), the
gradient of the d-th arm can be calculated as Gd (x i ) = 1d=x i −
πt (d). At each iteration, AAB samples three arms: the first sampling
chooses a target action d ∼ πt and makes d to be the arm to update;
the second and the third sample two antithetic arms for calculating
the gradients for d. Specifically, given three sampled actions, the
estimation of gradient for arm d can be calculated as:
ζd = πt (d)Gd (d) + (1 − πt (d))αGd f πt σ (αξ )
+ (1 − πt (d))(1 − α)Gd f πt σ (1 − (1 − α)ξ )

Theoretical analysis

AAB makes an unbiased estimation of the gradient, as shown in the
following Theorem 3.1 because Gd f πt (ξ ) is an unbiased estimation
obviously when ξ ∼ U (0, 1).

=πt (d)Gd (d) + (1 − πt (d))
=πt (d)Gd (d) +

Õ

k Õ
k
Õ
i=1 j=1

[αpi j Gd (i) + (1 − α)pi j Gd (j)]

k Õ
k
k
Õ
Õ
( αpi j +
(1 − α)p ji )Gd (i)
i=1 j=1

j=1

πt (i)Gd (i)

i ∈A\{d }

=

Õ

πt (i)Gd (i) = E[Gd f πt (ξ )]

i ∈ {A}


AAB uses stratified sampling as the traditional antithetic variates
method do. Given α ∈ (0, 1), stratified sampling has lower variance
than the primal Monte Carlo method. In coordinate ascent, the
stratified sampling estimates the gradient of the arm d as:
ηd = πt (d)Gd (d) + (1 − πt (d))αGd f πt (πt (d) + αξ 1 )
(6)
+ (1 − πt (d))(1 − α)Gd f πt (πt (d) + α + (1 − α)ξ 2 )

(5)

where the random variable ξ ∼ U (0, 1), σ is the permutation function that sorts the arms so that the second and the third sampled
arms are antithetic, and α ∈ (0, 1) is the parameter which is heuristiÍ
1
cally set as m
i=1 πt (I(i))+ 2 πt (I(m+1)) where m (named stratified
parameter) represents the position of stratifying.
Algorithm 1 shows the AAB process and Algorithm 2 shows the
function for sorting the arms.
At time step t, given the current policy πt , an action d is sampled
which corresponds the arm to update. Then the algorithm sort the
actions (arms) with σ . After that, a pair of antithetic actions are

where ξ 1 , ξ 2 ∼ U (0, 1) and Cov[ξ 1 , ξ 2 ] = 0.
Moreover, it can be shown that the estimation of AAB has a
smaller variance than stratified sampling, as shown in Theorem 3.2.
Theorem 3.2. ∀t = 1, 2, · · · , and ∀d ∈ A = {1, 2, . . . , k}, the
variance of ζd in Equation (5) and ηd in Equation (6) satisfies V[ζd ] ≤
V[ηd ]

887

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA
Sihao Yu, Jun Xu∗ , Yanyan Lan, Jiafeng Guo, Xueqi Cheng

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA

80

60

0.004

50

Variance

% Optimal action

PGB
AAB

0.005

70

40
30

0.002

AAB
PGB
AABwithBase
PGBwithBase

20
10
0

100000

200000
300000
Steps

400000

0.003

0.001
0.000

500000

0

100000

200000
300000
Steps

400000

500000

Figure 1: Performance curves of different methods.

Figure 2: The variance of PGB and AAB.

Proof. Let θ 1 = αGd f πt σ (αξ ), θ 2 = (1−α)Gd f πt σ (1−(1−α)ξ ),
θ 3 = αGd f πt (πt (d) + αξ 1 ), and θ 4 = (1 − α)Gd f πt (πt (d) + α + (1 −
α)ξ 2 ) where ξ , ξ 1 , ξ 2 ∼ U (0, 1).
V[ζd ] = (1 − πt (d))2 (V[θ 1 ] + V[θ 2 ] + Cov[θ 1 , θ 2 ])
V[ηd ] = (1 − πt (d))2 (V[θ 3 ] + V[θ 4 ] + Cov[θ 3 , θ 4 ])
∵ θ 1 and θ 3 are I.I.D., and θ 2 and θ 4 are I.I.D.
∴ V[θ 1 ] = V[θ 3 ], V[θ 2 ] = V[θ 4 ]
∵ Cov[θ 1 , θ 2 ] ≤ 0, Cov[θ 3 , θ 4 ] = 0
∴ V[ζd ] ≤ V[ηd ]


with baseline which can be viewed as a control variates method
in statistics, AAB resorts to the antithetic variants method for the
task. Algorithms were proposed to conduct the estimation and the
theoretical analysis showed that the gradients estimated by AAB
are unbiased and the variance is smaller than that of by the conventional Monte Carlo methods. Experimental results also showed
that AAB can achieve the state-of-the-art performances.

4

6

This work was funded by the National Key R&D Program of China
under Grants No. 2016QY02D0405, the 973 Program of China under
Grant No. 2014CB340401, the National Natural Science Foundation
of China (NSFC) under Grants No. 61773362, 61425016, 61472401,
61722211, and 20180290, and the Youth Innovation Promotion Association CAS under Grants No. 20144310, and 2016102.

EXPERIMENTS

We conducted experiments to test the proposed AAB algorithm.
Following the practices in [6], “one real competitor” in [6] was used
as our experiments. As for the reward in the k-armed bandit, the
reward distributions were set to Bernoulli and the expected rewards
1 and p = 0.4, i =
of actions were set as p1 = 0.5, p2 = 0.5 − 10k
i
3, . . . , k. The number of arms k was set to k = 20. The parameter
m for calculating α in AAB was set as m = 4.
Figure (1) shows the performance curves of different methods
in terms of the ratio of choosing the optimal action. From the
results, we can see that our approaches (AAB and AABwithBase1 )
performed better than the baseline method of policy gradient (PGB),
and have similar performance to the method of policy gradient with
baseline (PGBwithBase). Note that PGB and PGBwithBase only
update the first of 3 sampled arms at each iteration. From the results
we can see that 1) AAB can effectively reduce the variance, making
it outperform PGB; 2) both AAB and policy gradient with baseline
can effectively reduce the gradient variance, leading to similar
performances; and 3) combining AAB with policy gradient with
baseline (AABwithBase) can marginally improve the performances.
We also tested the variance curves of AAB and PGB, as shown in
Figure (2). The variances of the estimated gradient by AAB are in
general smaller than that of by PGB in all of the iterations, showing
the effectiveness of AAB in reducing the gradient variance.

5

ACKNOWLEDGMENTS

REFERENCES
[1] Jonathan Baxter and Peter L Bartlett. 2001. Infinite-horizon policy-gradient
estimation. Journal of Artificial Intelligence Research 15, 1 (2001), 319–350.
[2] E.C.FIELLER and H.O.HARTLEY. 1954. SAMPLING WITH CONTROL VARIABLES. Biometrika 41, 3/4 (1954), 494–501.
[3] Evan Greensmith, Peter L Bartlett, and Jonathan Baxter. 2004. Variance reduction
techniques for gradient estimates in reinforcement learning. Journal of Machine
Learning Research 5, Nov (2004), 1471–1530.
[4] J. M. Hammersley and K. W. Morton. 1956. A new monte carlo technique:
Antithetic variates. Mathematical Proceedings of the Cambridge Philosophical
Society 52, 3 (1956), 449–475.
[5] Ronald A Howard. 1960. Dynamic programming and markov processes. (1960).
[6] Zohar Karnin, Tomer Koren, and Oren Somekh. 2013. Almost optimal exploration
in multi-armed bandits. In Proceedings of the 30th International Conference on
Machine Learning (ICML-13). 1238–1246.
[7] Michael N Katehakis and Arthur F Veinott Jr. 1987. The multi-armed bandit
problem: decomposition and computation. Mathematics of Operations Research
12, 2 (1987), 262–268.
[8] Vijay R Konda and John N Tsitsiklis. 2000. Actor-critic algorithms. In Advances
in neural information processing systems. 1008–1014.
[9] Nicholas Metropolis and Stanislaw Ulam. 1949. The monte carlo method. Journal
of the American statistical association 44, 247 (1949), 335–341.
[10] Jerzy Neyman. 1934. On the Two Different Aspects of the Representative Method:
The Method of Stratified Sampling and the Method of Purposive Selection. Journal
of the Royal Statistical Society 97, 4 (1934), 558–625.
[11] R. S Sutton. 1999. Policy Gradient Methods for Reinforcement Learning with
Function Approximation. Submitted to Advances in Neural Information Processing
Systems 12 (1999), 1057–1063.
[12] R. J. Williams. 1988. Towards a theory of reinforcement-learning connectionist
systems. Issues in Education 4, 1 (1988), 1–94.
[13] Ronald J Williams. 1992. Simple statistical gradient-following algorithms for
connectionist reinforcement learning. Machine learning 8, 3-4 (1992), 229–256.

CONCLUSION

In this paper, we propose a novel variance reduction method for policy gradient in multi-armed bandit problem, called Antithetic-Arm
Bandit (AAB). Compared with existing method of policy gradient
1 Note

that AAB can be combined with the policy gradient with baseline for further
reducing the variance.

888

