Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Toward an Interactive Patent Retrieval Framework
based on Distributed Representations
Walid Shalaby

Wlodek Zadrozny

University of North Carolina at Charlotte
Charlotte, NC
wshalaby@uncc.edu

University of North Carolina at Charlotte
Charlotte, NC
wzadrozn@uncc.edu

ABSTRACT

In this paper, we present a novel interactive framework for PR
based on distributed representations of concepts and entities identified in patents text. Offline, we jointly learn the embeddings of
words, concepts, patent documents, and patent classes in the same
semantic space. We then use the learned embeddings to generate
multiple vector-based representations of the topic patent query and
its prior art candidates. Given a topic patent, we find its prior art
through two steps: 1) candidate generation through keyword search,
favoring recall, and 2) candidate reranking through an ensemble
of semantic similarities computed from the vector representations,
favoring precision. Empirical evaluation of this automated retrieval
scheme on the CLEF-IP 2010 dataset shows its efficacy over keyword
search where we get 4.6% improvement in recall@100.
We also propose an effective query reformulation and term
weighting mechanism based on interactive relevance feedback. We
model term weighting as a supervised feature selection problem
where term weights are assigned based on how good each term is at
discriminating the relevant vs. irrelevant candidates obtained from
user feedback. Our interaction mechanism is more practical and
realistic than the one proposed by Golestan Far et al. [4]. We ask the
user to annotate hits in the top n results as relevant/irrelevant, while
in [4] the user is restricted to annotate only relevant candidates
which might appear very deep in the candidates list.
We simulate this interactive term weighting mechanism to demonstrate its effectiveness over the best performer in the CLEF-IP 2010
competition; PATATRAS [5]. Simulation results show that we can
outperform PATATRAS with only 1 annotated candidate regardless
of whether it is relevant or not. It is worth mentioning that similar
results have been presented in Golestan Far et al. [4], but with
restricting the user to annotate 1 relevant candidate which again
might require the user to navigate through several candidates1 .

We present a novel interactive framework for patent retrieval leveraging distributed representations of concepts and entities extracted
from the patents text. We propose a simple and practical interactive
relevance feedback mechanism where the user is asked to annotate relevant/irrelevant results from the top n hits. We then utilize
this feedback for query reformulation and term weighting where
weights are assigned based on how good each term is at discriminating the relevant vs. irrelevant candidates. First, we demonstrate
the efficacy of the distributed representations on the CLEF-IP 2010
dataset where we achieve significant improvement of 4.6% in recall
over the keyword search baseline. Second, we simulate interactivity to demonstrate the efficacy of our interactive term weighting
mechanism. Simulation results show that we can achieve significant
improvement in recall from one interaction iteration outperforming
previous semantic and interactive patent retrieval methods.

KEYWORDS
Interactive Information Retrieval; Concept and Entity Embeddings
ACM Reference Format:
Walid Shalaby and Wlodek Zadrozny. 2018. Toward an Interactive Patent
Retrieval Framework based on Distributed Representations. In SIGIR ’18:
The 41st International ACM SIGIR Conference on Research & Development in
Information Retrieval, July 8–12, 2018, Ann Arbor, MI, USA. ACM, New York,
NY, USA, Article 4, 4 pages. https://doi.org/10.1145/3209978.3210106

1

INTRODUCTION & BACKGROUND

Patent Retrieval (PR) is the pillar of almost all patent analysis tasks.
PR is a challenging task as patents are multi-page, multi-modal,
multi-language, semi-structured, and metadata rich documents. On
another hand, patent queries can be a complete multi-page patent
application. These unique features make traditional IR methods
used for Web or ad hoc search inappropriate or at least of limited
applicability to PR [12]. PR methods are either keyword-based
[7, 9, 13] or semantic-based [3, 6, 8]. Because neither methods has
acceptable performance, few interactive methods were proposed
to better discriminate relevant vs. irrelevant terms based on user
feedback [4].

2 PREPROCESSING & OFFLINE OPERATIONS
2.1 The Search Index
As shown in Figure 1, automated vector-based retrieval starts by
searching for an initial set of N candidates. For this purpose, we
build a search index of the target candidate patents collection using
Apache Solr. For each candidate patent, we index its Id, title, abstract,
description, claims, and IPC classification codes. During candidate
set generation, we use title, abstract, description, and claims of the
topic patent and search all candidate fields except the IPC codes.
We give equal weight to all the fields during search.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210106

1 Golestan

Far et al. [4] figure 4 shows that ~750 of 1281 test queries have the 1s t
relevant candidate among the top 10 (~59% chance)

957

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Figure 1: Automated reranking of prior art candidates using multiple scoring functions based on embeddings of words, concepts, patent documents, and patent classes.

2.2

where s is the context window size. Here, ti is the target token
which would be either a word or a concept mention, and ti+j is a
surrounding context word or concept mention.
2.3.2 Patent Documents Vectors. We learn unique vectors for
each patent document with the objective to maximize the ability
of predicting words/concepts appearing in the document given the
patent vector. Therefore, contexts are generated as pairs of (t j ,pidi )
where t j is a term (word/concept) appearing in a target patent
document pi whose Id is pidi in the candidates collection C. Under
this representation, our training objective would be maximizing:
|C |
1 X X
Lp =
log p(t j |pidi )
(2)
|C | i=1 t ∈p

Text Conceptualization

In the context of text mining, one flavor of text conceptualization
works by extracting basic level concepts2 (BLC) from the input text
by identifying mentions of those concepts and mapping them to
entries in target knowledge base. In this work, our concept space is
defined by all Wikipedia article titles. We perform conceptualization
by moving sliding windows of different sizes on the input patent
text. Each window of size n will produce n-gram tokens which are
then matched to a Wikipedia concept (article title) and replaced by
unique Id.
Conceptualization has two main advantages: 1) concepts with
different surface forms would be mapped to a single unique canonical form (e.g., Solar cell, Photovoltaic cell, PV cell), and 2) concept
mentions of arbitrary length would be mapped to unique Ids and
therefore a single vector would be learned for each concept rather
than each word of the concept expression. This is important for
concepts whose meaning is different from the compositional semantics of its individual words (e.g., rare earth element). As shown
in Figure 1, the output of text conceptualization is the union of the
Bag-of-Words (BoW) and identified concept mentions (BLC) in the
input patent text.

2.3

j

i

2.3.3 Patent Class Vectors. We learn unique vectors for each
patent class. Patent classes are important in patent retrieval as they
are assigned according to the patent technical features. Therefore,
they can be used for soft filtering; to limit the scope of search to few
class codes rather than searching through irrelevant technological
fields. Our objective is to maximize the ability of predicting terms
appearing in all the patents that belong to a target class given the
class vector. Therefore, contexts are generated as pairs of (t j ,c)
where t j is a term (word/concept) appearing in a given patent
document pi which c is one of its class codes CLSpi . Under this
representation, our training objective would be maximizing:
|C |
1 X X X
Lc =
log p(t j |c)
(3)
|C | i=1 t ∈p

Learning Distributed Representations

Our framework adapts skip-gram [10], the popular local context
window method, to jointly learn vector representations (embeddings) of words, concepts, patent documents, and patent classes
in the same semantic space. By embedding all these structures in
one space, we could measure the similarity between pairs of words,
concepts, documents, and classes and between combinations of
them using a proper similarity measure (e.g., cosine).
2.3.1 Word & Concept Vectors. We utilize the candidate patents
collection as the input corpus. After all concept mentions are identified using text conceptualization, we use the skip-gram model to
jointly learn the embeddings of both words and concepts. Formally,
given a patent corpus of V words w 1 ,w 2 , ...,wV . We iterate over the
corpus identifying words and concept mentions and thus generating a sequence of T tokens t 1 ,t 2 , ...tT where T < V (as multi-word
concepts will be counted as one token). Afterwards we train the
skip-gram aiming to maximize:
T
X
1X
Lt =
log p(ti+j |ti )
(1)
T i=1 −s ≤j ≤s,j,0

j

i

c ∈C LSpi

During training, we train the embedding model to jointly maximize
L = Lt + Lp + Lc which is estimated using the softmax function.
As the patents vocabulary is typically full of jargon and user
defined concepts, we start with a pretrained concept embeddings
model which utilizes Wikipedia [11]. Pretraining is intended to help
our training focus more on optimizing the representations of new
terms, patent documents, and classes rather than optimizing for
the entire vocabulary from scratch.

3

AUTOMATED VECTOR-BASED RETRIEVAL

Figure 1 shows the process of automated retrieval using our framework. At a high level, given a topic patent, we retrieve an initial set
of N candidates using keyword search from the Solr index. Then
we create a vector representation for the topic patent and each candidate from the words and concept mentions in their corresponding
text through conceptualization. We also generate another two vectors for each candidate through embedding lookup; one for the

2 By

concepts/entities we mean single or multiword expressions which denote an idea,
object, or event along with its characteristics

958

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Figure 2: Interactive QRE. Weights are based on terms ability to discriminate relevant/irrelevant candidates annotated by user.
candidate patent document and one for its class. After generating
all the vectors, we compute similarity scores between the topic
vector and each of the three vectors of each candidate. This will
generate three scores which are then combined with the keyword
search score to obtain the overall relevancy score which is used to
rank the N candidates. Below, we describe these steps in detail.

3.1

removal, or reweighting of relevant/irrelevant terms could significantly boost the performance of PR. However, automated QRE fails
to fully identify the significance of each term motivating the need
for interactive QRE. Our framework embraces interactive relevance
feedback for QRE. Inspired by [2], we model term weighting as
a supervised feature selection problem where term weights are
assigned based on how good each term is at discriminating the
relevant vs. irrelevant candidates obtained from user feedback.
Figure 2 shows the process of interactive QRE. Our mechanism
is similar to the technology assisted review protocol [1]. After candidate reranking, the user is asked to annotate the top n candidates
as either relevant or irrelevant to the topic patent. We then employ the chi-square statistic for term weighting considering the
topic patent + the annotated relevant candidates as the +ve samples, while the annotated irrelevant ones as the -ve samples. Then,
we create a modified ublc for the topic patent considering only
those terms ti in the topic patent and any of the annotated relevant candidates along with their chi-square weights w i such that
P
ublc = Ti=1 w i ∗ lookup(ti ). The modified ublc is used to compute
the ensemble scores and rerank the candidates. This process is
repeated until the user is satisfied with the results.
We argue that our proposed user interaction mechanism is more
practical than Golestan Far et al. [4]. In [4] the user is required to
annotate the relevant results only. However, this might be impractical as in patent retrieval it is usually expected that many relevant
results appear late in the result set and therefore the user effort
would be proportional to rankings of these relevant results. Our
mechanism, alternatively, doesn’t require the user to dig deep in the
candidates list as we require the annotations of the top n candidates,
therefore the user effort is proportional to n and independent from
the relevant hits rankings. On the other hand, our proposed scheme
exploits both relevant and irrelevant hits as the user go through the
candidates list. In case of no relevant candidates in the top n, we
can still use the topic query as a relevant hit and apply chi-square
weighting. In case of no irrelevant candidates in the top n, we can
fall back to our normalized tf weighting expanding the topic patent
terms with other terms from the annotated relevant ones.

Vector Generation

In this step we generate continuous vectors for the topic patent and
each of its prior art candidates. The BLC vector (vblc ) is created
from the weighted sum of the embeddings of all words and concepts
in the patent text. We use the normalized term frequency (tf ) as the
initial term weight. Formally, given a patent whose text contains
P
set of terms T , then vblc = Ti=1 w i ∗ lookup(ti ) where ti is a word
or a concept whose normalized tf is w i and lookup(.) retrieves the
vector of its input from the learned embedding space.
We generate two other vectors for each candidate patent. First,
the PID vector which corresponds to the vector learned for the
whole patent document. It is obtained by vpid = lookup(pid ). Second, the CLS vector which corresponds to the vector of that patent
class, and is obtained by vcl s = lookup(cls).

3.2

Candidate Scoring and Reranking

As mentioned earlier, the initial prior art candidates are obtained
by keyword search. After generating the vectors of the topic patent
and its candidates, we compute multiple semantic similarity scores
which are then combined to produce the final relevancy score of
each candidate to the topic patent. All scores utilize the cosine
measure between pairs of vectors (u,v) as cos (u, v) = | |u|u | . | v|v| | . In
all below scores, ublc is the BLC vector of the topic patent, and v is
one of the vectors of a prior art candidate.
3.2.1 BLC Score. It is computed as sblc = cos (ublc , vblc ). It captures the fine-grained similarities between the two BLC vectors.
3.2.2 PID Score. It is computed as spid = cos (ublc , vpid ). It
captures the coarse-grained similarities between the topic BLC
vector and the whole candidate document.
3.2.3 CLS Score. It is computed as scl s = cos (ublc , vcl s ). It
captures the similarity between the topic BLC vector and the highlevel technical features of the candidate embedded in its class vector.
3.2.4 Ensemble Scoring. Finally, we combine the three scores
with the normalized keyword search score (skw ) through weighted
sum to produce the final relevancy score of each candidate as s =
α ∗ sblc + β ∗ spid + γ ∗ scl s + δ ∗ skw , where α + β + γ + δ = 1 and
are tuned empirically.

4

5

PERFORMANCE EVALUATION

We evaluate our framework on the CLEF-IP 2010 benchmark dataset3
which contains ~2.6 million patent documents. Similar to [4], we
considered only 1286 queries (topic patents) which has at least one
relevant document whose title, abstract, description, and claims in
English. During keyword search, we set the number of initial candidates N to 1000. To make our results comparable to previous studies
[4, 5], we then perform IPC filtering during keyword search; keeping only candidates that share at least one IPC class with the topic

INTERACTIVE RELEVANCE FEEDBACK

As we will show in the evaluation section and indicated by previous
studies [4, 12], query reformulation (QRE) by means of expansion,

3 http://www.ifs.tuwien.ac.at/~clef-ip/

959

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: Vector-based retrieval with interaction.

hits. Importantly, our system gives much better results in terms of
both Recall and Mean Average Precision (MAP) scores than [4].
To better demonstrate the significance of our interaction and
term weighting mechanism, we performed an experiment where we
simulated the user annotating the top n candidates (n = {1,2,3,4,5,10})
from vector-based reranking and then iterate over the new ranked
list multiple iterations (iter# = {1,2,3}). Figure 3 shows how recall
improves with increasing n and iter#. As we can notice, significant
improvement is achieved after iter#1 with diminishing return as
we iterate. We think this is because the diversity of vocabulary in
the pool of candidate terms for chi-square weighting decreases as
we iterate on the top hits causing weights to stabilize after 1 or 2
iterations. When n is relatively large (5 or 10), the diversity keeps
increasing and thus the magnitude of improvements is relatively
higher with more iterations. We can also notice that user effort is
proportional to n making our interaction mechanism more practical.

Recall@100
Keyword baseline

41.9

PATATRAS [5]

46.7

Vector-based reranking
top 1 annotated
top 5 annotated
top 10 annotated

46.7
47.3
48.1
49.3

Table 2: Performance of our vector-based reranking when
coupled with the interactive mechanism of Golestan Far
et al. [4] (r is number of user annotated relevant candidates).

Golestan Far et al.⋆ [4]
Ours
⋆ Results

MAP
r=1
r=3

Recall@100
r=1
r=3

28.8
30.5

47.9
51.2

36.9
52.9

6

54.7
60.5

CONCLUSION

In this paper, we presented a novel interactive framework for patent
retrieval. Our framework is generic and can accept non-patent
queries as well. We support human-in-the-loop through soliciting
user feedback with reasonable effort. Under the hood, we utilize chisquare statistic to learn proper term weights and subsequently perform QRE to promote more relevant results and demote irrelevant
ones. The proposed framework efficiently computes multiple similarity scores which captures semantic similarities at different levels
(words, concepts, documents, and categories). Empirical results
show superior performance of our system compared to previous
fully automated keyword, semantic, and interactive methods.

from [4] considering τ = 0

REFERENCES
[1] Gordon V Cormack and Maura R Grossman. 2014. Evaluation of machine-learning
protocols for technology-assisted review in electronic discovery. In Proceedings
of the 37th international ACM SIGIR conference on Research & development in
information retrieval. ACM, 153–162.
[2] Franca Debole and Fabrizio Sebastiani. 2004. Supervised term weighting for
automated text categorization. In Text mining and its applications. Springer,
81–97.
[3] Atsushi Fujii. 2007. Enhancing patent retrieval by citation analysis. In Proceedings
of the 30th annual international ACM SIGIR conference on Research and development
in information retrieval. ACM, 793–794.
[4] Mona Golestan Far, Scott Sanne, Mohamed Reda Bouadjenek, Gabriela Ferraro,
and David Hawking. 2015. On Term Selection Techniques for Patent Prior Art
Search. In Proceedings of the 38th International ACM SIGIR Conference on Research
and Development in Information Retrieval. ACM, 803–806.
[5] Patrice Lopez and Laurent Romary. 2009. Multiple Retrieval Models and Regression Models for Prior Art Search. In CLEF 2009 Workshop. 18p.
[6] Walid Magdy and Gareth JF Jones. 2011. A study on query expansion methods for
patent retrieval. In Proceedings of the 4th workshop on Patent information retrieval.
ACM, 19–24.
[7] Walid Magdy, Johannes Leveling, and Gareth JF Jones. 2009. Exploring structured
documents and query formulation techniques for patent retrieval. In Multilingual
Information Access Evaluation I. Text Retrieval Experiments. Springer, 410–417.
[8] Parvaz Mahdabi, Shima Gerani, Jimmy Xiangji Huang, and Fabio Crestani. 2013.
Leveraging conceptual lexicon: query disambiguation using proximity information for patent retrieval. In Proceedings of the 36th international ACM SIGIR
conference on Research and development in information retrieval. ACM, 113–122.
[9] Parvaz Mahdabi, Mostafa Keikha, Shima Gerani, Monica Landoni, and Fabio
Crestani. 2011. Building queries for prior-art search. Springer.
[10] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality. In
Advances in neural information processing systems. 3111–3119.
[11] Walid Shalaby and Wlodek Zadrozny. 2017. Learning Concept Embeddings for
Efficient Bag-of-Concepts Densification. arXiv preprint arXiv:1702.03342 (2017).
[12] Walid Shalaby and Wlodek Zadrozny. 2017. Patent Retrieval: A Literature Review.
arXiv preprint arXiv:1701.00324 (2017).
[13] Xiaoibng Xue and W Bruce Croft. 2009. Transforming patents into prior-art
queries. In Proceedings of the 32nd international ACM SIGIR conference on Research
and development in information retrieval. ACM, 808–809.

Figure 3: Recall of simulated interaction when varying the
number of annotated hits n & interaction iterations (Iter#).
query. We experimentally set α = 0.2, β = 0.4,γ = 0.125,δ = 0.275.
We simulate user interactions by automatically annotating the top n
candidates from the vector-based reranking using the true relevance
judgments.
Table 1 shows the performance of our system compared to PATATRAS [5], a patent retrieval system with significant preprocessing4
and sophisticated use of patent metadata. As we can see, the automated vector-based reranking achieves equal performance to
PATATRAS and improves recall by 4.6% compared to the keyword
baseline demonstrating the usefulness of the learned distributed
representations. Interactive QRE improves performance even more;
we can outperform PATATRAS performance if the user annotates
the first result from automated reranking as relevant or irrelevant.
Table 2 shows the results of our vector-based reranking compared to [4]. For fair comparison, we report the results considering
the user annotating relevant candidates only as in [4]. Generally,
we get more improvements as the user annotates more relevant
4 PATATRAS

extracts some relevant patents from citations in the topic patent description using regex. This preprocessing step contributes up to 8% of their recall.

960

