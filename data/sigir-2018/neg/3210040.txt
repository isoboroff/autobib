Session 7B: Content & Semantics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Identify Shifts of Word Semantics through Bayesian Surprise
Zhuofeng Wu

Cheng Li

Zhe Zhao

Zhejiang University
wuzhuofeng@zju.edu.cn

University of Michigan
lichengz@umich.edu

University of Michigan
zhezhao@umich.edu

Fei Wu

Qiaozhu Mei

Zhejiang University
wufei@cs.zju.edu.cn

University of Michigan
qmei@umich.edu

ABSTRACT

1

Much work has been done recently on learning word embeddings
from large corpora, which attempts to find the coordinates of words
in a static and high dimensional semantic space. In reality, such
corpora often span a sufficiently long time period, during which
the meanings of many words may have changed. The co-evolution
of word meanings may also result in a distortion of the semantic
space, making these static embeddings unable to accurately represent the dynamics of semantics. In this paper, we present a novel
computational method to capture such changes and to model the
evolution of word semantics.
Distinct from existing approaches that learn word embeddings
independently from time periods and then align them, our method
explicitly establishes the stable topological structure of word semantics and identifies the surprising changes in the semantic space over
time through a principled statistical method. Empirical experiments
on large-scale real-world corpora demonstrate the effectiveness of
the proposed approach, which outperforms the state-of-the-art by
a large margin.

1 Language

INTRODUCTION

is a mirror of reality ([26], p. 6): environment, society,
culture, technology. When reality changes, language changes. Reality is a function of language [15]. Language influences our view of
the world and influences social structures. Failing to adjust to the
changes of language, fails to adapt to the reality.
If Vannevar Bush were still alive and read this paper, he should
be surprised about how differently the word “retrieval” means compared to his time. Would he still name his imaginary machine
“memex” if he knew the modern meanings of “memory?” Consider
Marty McFly coming to the future and searching for “an apple
store”; consider him asking about “weeds” on Reddit; consider inviting a 13th-century knight and a 16th-century scholar to a “bachelor”
party. Wait, did you just say “party?”
Thanks to the fast evolution of technology and the prevalence
of social media, meanings of words have been changing more and
more rapidly. What is your first impression of “deep?” “CNN ?” “At?”
How to detect and adapt to these changes is a critical challenge of
natural language processing, which largely affects the performance
of downstream tasks such as Web search, information filtering,
personalized recommendation, and dialog system.
Measuring frequency is easy; measuring meanings is hard. Many
existing studies have been done to analyze the change of semantics
(i.e., meanings) of certain words (e.g., [5, 19, 39]), but the need of
careful linguistic investigations makes them hard to scale. Recent
successes of large-scale word representation learning have inspired
researchers to rethink about detecting semantic shifts. Comparing
to traditional methods based on qualitative analysis, these methods
can efficiently compute the embeddings of words in vector spaces,
so that their meanings can be analyzed in a quantitative way.
Multiple methods have been proposed recently that build upon
word embedding methods (such as the Word2Vec [30]). Instead
of focusing on a small number of target words, these methods
massively compute and rank the differences of the embeddings of
many words across different time periods (e.g., [14, 20, 21]). Words
whose embedding vectors diverge significantly are suspected to
have a change of semantics. As a result, these methods do retrieve
the words that have truly changed their meanings, but they also
introduce a large number of false positives. Extensive manual work
has to be done to scan through the words that have passed the filter
in order to identify the true changes.
How to identify shifts of word semantics with both a high recall and a high precision? First, to answer the question what has
changed, it is important to consider what is consistent. In fact, one

CCS CONCEPTS
• Computing methodologies → Lexical semantics; • Information systems → Language models; Similarity measures;

KEYWORDS
Word Embeddings; Semantic Shifts; Bayesian Surprise
ACM Reference Format:
Zhuofeng Wu, Cheng Li, Zhe Zhao, Fei Wu, and Qiaozhu Mei. 2018. Identify
Shifts of Word Semantics through Bayesian Surprise. In SIGIR ’18: The 41st
International ACM SIGIR Conference on Research Development in Information
Retrieval, July 8–12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA,
10 pages. https://doi.org/10.1145/3209978.3210040

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210040

1 This

825

work was done when the first author was visiting the University of Michigan.

Session 7B: Content & Semantics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

2.2

critical problem of the existing approaches is the lack of a fixed
frame of reference, in particular, a semantic space that is reasonably
stable over time so that the shifts of words in that space can be
detected. If the semantic space itself changes, so do most of the
words in the space. As a result, many “semantic” shifts identified by
existing methods are artifacts of the distortion of the space. Second,
to claim that a word’s meaning has changed, one has to be certain
about its old meanings. In other words, to identify true shifts of
semantics, one should to be able to distinguish real changes from
noises. If there is a high uncertainty about a word in the semantic
space, a “shift” might have been a random walk.
In this paper, we present a novel design of methods to accurately
identify shifts of word semantics, which successfully addresses the
two critical problems above. Instead of attempting to align the word
embeddings to obtain the same frame of reference, we represent
word semantics in a topological space that is robust to distortions
over time. We propose a novel and effective method to detect shifts
in this space, which is based on a principled statistical model called
the Bayesian surprise. The method can successfully distinguish
true changes of word semantics from noises and uncertainty, which
significantly increases the precision without sacrificing the recall.
We present carefully designed experiments on four large scale,
real world data sets. The proposed method is evaluated in two
modes, to detect synthetic changes and real changes, respectively. It
achieves near-perfect performance on detecting synthetic changes,
outperforming all competing methods by large margins, as well as
clearly better performance on detecting real changes.

2

RELATED WORK

The key innovation of our work is to compute a topological representation of the semantic space and to identify shifts in that space
through Bayesian surprise. There are four lines of literature that
are closely related to our work: word embeddings, semantic shift
detection, Bayesian surprise and dynamic topic models.

2.1

Semantic Shift Detection

There have been several studies on detecting semantic changes over
time. Most of them learn separate embeddings for different time
periods and identify the changes by comparing these embeddings.
In order to compare word vectors from different periods, one has
to ensure that the vectors are aligned into one unified coordinate
system. Kim et al. [20] adopted the Skip-gram model [29, 30] to
train embeddings from individual time periods, and they initialized
the word vectors in subsequent years with the word vectors obtained from the previous years. They identified semantic shifts by
comparing the cosine similarity of the embedding vectors for the
same words in different years. Kulkarni et al. [21] addressed the
embedding alignment problem by learning an alignment function,
through a Linear Regression (LR) or a Local Linear Regression (LLR).
Hamilton et al. [14] proposed a slightly different approach. They
used orthogonal Procrustes to align the embeddings among different periods, which preserves the cosine similarities by the largest
extent. Different from these studies, we represent word semantics
in a topological space instead of a vector space, which is robust to
distortions and avoids the hassle of aligning embeddings.
Other work also tried to avoid the alignment problem in various
ways. Gulordava and Baroni [12] used the cosine product of context
vectors to measure the distributional similarity of words in the
Google Books N-gram Corpus between two periods. Brigadir et
al. [7] compared the nearest neighbors of a word from different time
periods and scored semantic shifts using Jaccard distance. Similarly,
Hamilton et al. [13] defined semantic changes by measuring local
neighborhoods. They compared the nearest neighbors from three
periods and defined a cosine-similarity function between two sets.
Our work differs from the above studies by considering all historical
periods to construct and update the topological semantic space and
to quantify semantic shifts.
Frermann and Lapata proposed a dynamic Bayesian model [10]
to track the change of word meanings. They represented a word
by a set of senses, with each sense being a probability distribution
over words, through which one can directly observe the change of
these senses. Such an approach makes a strong assumption on the
generation of word senses, and it cannot quantify the changes of
the induced senses. In our work, we explicitly quantify the surprise
introduced by the new data, without making assumptions on the
numbers and distributions of word senses.
Comparing to all existing work on semantic shift detection, the
biggest difference of our method is the use of Bayesian surprise to
formally model and quantify shifts in a topological semantic space.

Word Embeddings

Word embeddings essentially compute the coordinates of words in
a low-dimensional vector space. Bengio et al. [4] proposed a neural
language model, which is optimized through stochastic gradient
descent and outperforms traditional n-gram models. Mikolov et
al. [29, 30] proposed Word2Vec, which adopts optimization methods such as negative sampling and abstract stochastic gradient
descent [35]. Their model greatly reduces the training time and
effectively improves the quality of the embedding vectors. Tang et
al. [37] proposed the LINE, a network embedding method that is able
to preserve both the local and global network structures. When
applied to a word co-occurrence network, network embedding
methods such as LINE and DeepWalk [33] learn word embeddings.
A recent study by Levy et al. [24] reveals that the performance
gain of word embeddings are due to certain system design choices
and hyper-parameter optimization. The essence of word embeddings is related to the Singular Value Decomposition (SVD) of the
pointwise mutual information (PMI) matrix [9].
The first step of our method learns word embeddings from individual time periods, and any reasonable word embedding method
can be adopted for this purpose.

2.3

Bayesian Surprise

Bayesian surprise is a formal statistical method proposed by Itti and
Baldi [17], which explicitly measures the influence of a new data
point on the background data. Studies show that Bayesian surprise
well predicts human attentions [17]. Bayesian surprise has been
successfully applied to various tasks, such as computer vision [11],
neuro-imaging [31], and landmark detection [34].
Bayesian surprise was first applied to text analysis by Louis [25],
for the task of text summarization. She used Bayesian surprise to
compute a surprise value for each word type in the input and then
extracted the sentences with highest surprise values.

826

Session 7B: Content & Semantics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

We present the first work that applies Bayesian surprise to quantify the shifts of word meanings.

2.4

should be robust to the noises in the semantic space, noises that
are either introduced due to the sparsity of the observations or due
to the uncertainty of the meanings.
To model the stability and the changes of word semantics, we focus on a topological representation of the semantic space (instead of
a vector space), which is robust to distortions over time. Changes of
the topology are identified through a principled statistical method,
the Bayesian surprise, so that they are robust to uncertainty and
random noises. The proposed method will be described in details
in the following section.

Dynamic Topic Models

To model the evolution and dynamics of latent topics, Mei and
Zhai extracted topics from individual time periods and connected
similar topics in different periods [27]. Blei and Lafferty proposed
the Dynamic Topic Model (DTM) [6] which models the evolution
of topic-word distributions over time. Wang and McCallum [40]
adopted a different angle to model topic trends. They treated the
word co-occurrences and documents’ time stamps as observations
of the latent topics. None of these methods quantifies the change
of individual terms and thus cannot be applied to our problem.

3

4

METHOD TO DETECT SEMANTIC SHIFTS

We propose a carefully designed method that establishes the topological representations of word semantics over time and uses Bayesian
surprise to identify significant shifts of word semantics.

PROBLEM FORMULATION

We aim to detect words that present significant changes in their
meanings within a certain time period (or an epoch). Let us consider
a longitudinal corpus C that is divided into n epochs {Ct }, where
t ∈ {1, 2, ..., n} indicates a particular time period. Let Φt (V ) denote
the representation of the semantics of all words in the vocabulary
V at time t, and ϕ t (v) the representation of the meaning of word
v ∈ V . We are interested in measuring the deviation of meaning of
v during the period t, that is, ∆(ϕ t −1 (v), ϕ t (v)). Words with a high
∆(ϕ t −1 (v), ϕ t (v)) are considered as shifted during time period t.
In general, assuming that most words do not have a meaning
change in a single epoch, ∆(ϕ t −1 (v), ϕ t (v)) should be close to zero
in most cases. The fundamental question here is how to establish a good representation of ϕ t (v) and how to make ϕ t (v) and
ϕ t −1 (v) comparable so that ∆(ϕ t −1 (v), ϕ t (v)) can be calculated. In
practice, many existing approaches tend to model ϕ t (v) as a vector in a high-dimensional space of word semantics (i.e., Φt (V ))
which is learned from Ct through word embedding methods such
as Word2Vec. All these approaches face two critical problems: 1)
the word embedding ϕ t (v) learned from a single epoch of data Ct
might not sufficiently represent the meanings of v, and 2) the semantic spaces learned independently from two epochs, e.g., Φt −1 (V )
and Φt (V ), might not align accurately. The first problem makes
the computed ∆(ϕ t −1 (v), ϕ t (v)) vulnerable to biases and random
noises, and the second problem leads to many false positives as a
high ∆(ϕ t −1 (v), ϕ t (v)) may be an artifact of the misalignment of
the semantic spaces. As a result, many heuristics have to be used
to align/augment the embeddings learned from individual epochs.
Without loss of generality, we may consider Φt (V ) as a function
of {C 1 , C 2 , ..., Ct } instead of just Ct , and ϕ t (v) as a function of
Φt (V ) instead of just v. That says, the semantic space of language
does not only depend on the current time period but should also
reflect the historical usage of words; and the meaning of one word
is not simply indicated by its own position in the space but should
also depend on how it relates to other words.
This general formulation enables us to formally consider the
relativity in the evolution of a semantic space over time, namely:
what remains still and what moves? Indeed, if word semantics is
represented with a vector space, it is critical to make sure that
there is no distortion of its dimensions over time, so that words
whose meanings did not change will remain in the same positions.
Moreover, if the meaning of a word has truly shifted, that change

4.1

Design Rationale

Our method builds upon the insight that a topological structure is
an effective representation of the stability of the semantic space
and Bayesian surprise is a powerful tool to identify significant
changes. Shifts identified through Bayesian surprise in a topological semantic space are robust to distortions and noises introduced
by the partition of the corpus into multiple epochs. To better understand the rationale of our design, it is useful to think about the
limitations of alternative approaches:
• Word embeddings obtain vector representations of word semantics. If the coordinates of a word in the learned vector
space changes, the meaning of the word may have changed.
However, a change of the absolute coordinates of a word
isn’t necessarily caused by a shift of meaning. Any distortion of the vector space, such as a rotation, a permutation, a
change (increase, decrease) of dimensionality, a merger or a
split of certain dimensions, would have resulted in an apparent change of the coordinates of the same word. Therefore,
heuristics are used to control the distortion of the vector
spaces over time, such as post hoc alignment of two vector spaces, using one embedding to initialize/regularize the
learning of the next embedding, imposing strong priors of
how the vector spaces evolve, etc. These heuristics can control the distortions to some extent, but their effectiveness in
practice is usually limited. For example, it is hard to establish
a perfect alignment of two embeddings, and even a small
misalignment would accumulate over multiple epochs. A
topological space has a fundamental advantage over vector
spaces, as it is robust to such distortions over time.
• Unlike word embeddings, a topological space does not define the semantics of a word by numerical coordinates of
its position, but by its relative position to other words. If
we represent semantics with a manifold (a special type of
topological space), the meaning of a word can be interpreted
by the nearest neighbors of the word in that space. While
embedding vectors of words are vulnerable to noises and
distortions, their local and global topology tend to sustain.
• We found that almost all existing approaches identify word
semantic shifts through comparing only two consecutive
epochs, which often suffers from a high risk of noise and

827

Session 7B: Content & Semantics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

4.4

uncertainty. We specifically adopt Bayesian surprise in our
design to distinguish systematic shifts from noises and insufficient observations.

4.2

Method Overview

Our proposed method for detecting shifts of word semantics consists of three components: embedding learning, manifold construction, and change detection.
Like existing approaches, we first divide the corpus into subcorpora according to the time period each document was published
in. Once the partition is done, we train word embeddings on each
sub-corpus independently.
Unlike existing approaches, once the embeddings are trained, we
do not attempt to compare them or align them. Instead we construct
a topological semantic space (i.e., a semantic manifold) based on
these embeddings.
Whenever a new time period is processed, the neighborhood
of each word in the semantic manifold is updated: new neighbors
added, existing neighbors reinforced, and outdated neighbors weakened.
By comparing the neighborhood of a word before and after the
update, the change of its semantics can be measured. Note that this
approach is different from comparing the two consecutive time
periods, and all previous time periods will be taken into consideration. To stand out from this measurement, the neighborhood of the
word must have sufficient observations in history and must present
a surprising change in the current period. We use the Bayesian
Surprise [16], a formal method that measures the Bayesian definition of surprise, to achieve this goal. Bayesian surprise provides
an intuitive way to quantify the significance of shifts of a word’s
meaning with respect to its historical meanings.

4.3

Constructing Semantic Manifold

Having trained word embeddings Φt (V ) for each time period Ct , we
can construct and update the topological space of word semantics.
Unless specifically regularized or aligned, the embeddings learned
from different epochs are unlikely to share exactly the same dimensions. The topological space instead is likely to remain the same.
That is, if two words were close (in meaning) in the semantic space
of t and their meanings did not change in t + 1, they should still be
close in the semantic space of t + 1.
While other types of topological spaces can be used here too,
we decide to construct a manifold of word semantics, or a semantic
manifold. A manifold is a specialized topological space where the
neighborhood of each point resembles a Euclidean space [23]. This
is a neat property for our purpose, as we can use the Euclidean distance of the embedding vectors of two words to determine whether
they should be in the neighborhood of a manifold.
Following the common practice of manifold learning [23], we
approximate a semantic manifold by constructing a K-NearestNeighbor (KNN) graph, G = (V , E), where V is the set of words
and E is the set of directed edges between the words, with each
edge e = (u, v), e ∈ E indicating that word v is among the top K
words with the closest semantics to word u in at least one time
period. We define the k-nearest-neighbors of a word v at epoch
t in embedding space as: kN N (ϕ t (v)). Then the nearest semantic
networks at epoch t can be constructed as follows:
Et = {(u, v), ∀v ∈ kN N (ϕ t (u)), ∀u ∈ Vt }

(1)

Precisely calculating k-nearest neighbors for each words has a
complexity of O(|V | 2p) (p as the dimensionality of word embeddings), which is extremely expensive when the vocabulary size |V |
is colossal. We use the algorithm LargeVis 2 from Tang et al. [36]
to compute an approximate KNN graph. Similar to the definition of
k-nearest neighbors, we define the approximate nearest neighbors
of a word v at epoch t in embedding space as: aN N (ϕ t (v)).
Starting from the first time period, the semantic manifold is updated by augmenting Vt and Et into the current graph G. That is,
V = V ∪ Vt and E = E ∪ Et . Note that in a semantic manifold,
the distance between two neighbors should be updated over time.
That says, in the corresponding semantic graph, the weights of
edges should also be updated. In practice, the edge weights could
be updated based on certain functions of the actual Euclidean distance of the embedding vectors. For simplicity, in this paper we
just augment the weight of (u, v) by one if word v appears in the
neighborhood of u in one more time period.

Learning Word Embeddings

We adopt a commonly used embedding method called LINE [37].
LINE learns node representations of a network by optimizing an
objective function that attempts to preserve both the first-order
proximity and the second-order proximity. When applied to a simple word co-occurrence network where nodes are words and edges
are weighted by the numbers of times two words co-occur in the
same documents, LINE outputs the embedding of words.
The first-order proximity is determined by the co-occurrences
of two words in the same context. The second-order proximity
brings together two words that both co-occur with the same set of
words, even though they themselves might not have co-occurred
in the same context. LINE learns two types of embeddings separately by preserving either the first- or the second-order proximity,
and the two embeddings are finally concatenated to form the final
representation of each word.
In theory, any reasonable word embedding method can be used
here. We select LINE as it does present advantages over some other
methods such as the Skip-gram [30], due to the specific treatments
to two types of proximities [37]. Readers are encouraged to explore
other embedding methods and replicate our results.

4.5

Detecting Shift with Bayesian Surprise

The semantic manifold constructed and updated over time represents the stability of the semantic space, which is robust to the
misalignment and distortion of word embeddings learned from individual epochs. With the semantic manifold, identifying changes
of word meanings becomes much easier. Intuitively, one may simply
compare the neighbors of a word in the current epoch (aN N (ϕ t (v)))
with the neighbors of v in the semantic manifold as of t − 1. Indeed,
this can be done simply by comparing the two sets, e.g., through the
Jaccard distance. However, this simple approach, although better
2 https://github.com/lferry007/LargeVis

828

Session 7B: Content & Semantics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

than directly calculating the difference of two embedding vectors,
may still suffer from two problems: 1) the observations of a word are
not sufficient to support a systematic shift of its meaning; 2) there
is a high variance of the neighbors among the past time periods.
Indeed, when a word is not frequently used or when it is very
new, even though the nearest neighbors may have diverged a lot,
the difference may have been an artifact of data sparsity and there
is no sufficient evidence of a semantic shift. Similarly, if a word
does not have a stable set of neighbors among past time periods, it
indicates the meaning of that word has been uncertain, and a new
set of neighbors should not be considered as a semantic shift. Both
problems call for a method that accounts for the statistical significance of a change, which is beyond the capability of calculating
the difference of two sets or two vectors. We address this problem
with a principled statistical method, the Bayesian surprise. Below
we first formally define Bayesian surprise, as proposed by Itti and
Baldi [16]. Then we present how to use Bayesian Surprise to detect
semantic changes of words.
Bayesian Surprise: According to the Bayesian Theorem [1], an observer has a prior probability P(H ) associated with each hypothesis
H ∈ H , where H is the space of all hypotheses based on the background knowledge of the observer. Given newly observed data (in
our case, the sub-corpus of a new epoch) Ct +1 , the posterior probability of a single hypothesis can be defined according to Bayesian
P (C t +1 |H )P (H )
Theorem: P(H |Ct +1 ) =
. The surprise carried by the
P (C t +1 )
new observation Ct +1 to the hypothesis space H is defined as the
difference between the prior distribution and posterior distribution
of the hypotheses. Naturally, we use the Kullback-Leibler (KL) divergence [22] to compute the difference between two distributions:
S(Ct +1 , H ) = KL(P(H |Ct +1 ), P(H ))
∫
P(H |Ct +1 )
.
=
P(H |Ct +1 )loд
P(H )
H

Then we can obtain the posterior distribution given by new
observation Ct +1 on word v:
P(Hv |Ct +1 ) = Dir (x; α 1 + β 1 , α 2 + β 2 , ..., α |V | + β |V | ). (5)
At last, we are able to calculate the surprise score on word v as:
Score(v) = S(Ct +1 , Hv ) = KL(P(Hv |Ct +1 ), P(Hv ))
(6)
When calculating the KL divergence, we use add-one smoothing
for the Dirichlet distributions, which means the initial value of αu
is 1 at time zero. In practice, we follow settings given by Itti and
Baldi [3], as well as the formulas from [32] to calculate the Bayesian
surprise between two Dirichlet distributions. That is,
P(Hv |Ct +1 )
P(Hv )
H
∫ v
Dir (x, α + β)
=
Dir (x, α + β) log
dx
Dir (x, α)
∫
= (log Dir (x, α + β) − log Dir (x, α))Dir (x, α + β)dx

Score(v) =

∫

P(Hv |Ct +1 ) log

|V |
Õ
©
log Γ(α i + βi )
­log Γ(α 0 + β 0 ) −
i=1
Õ«
+ (α i + βi − 1) log x i − log Γ(α 0 )

=

+

∫

|V |
Õ

log Γ(α i ) −

i=1

Õ
ª
(βi − 1) log x i ® Dir (x, α + β)dx,
¬

(7)
Í |V |
Í |V |
where α 0 = i=1 α i and β 0 = i=1 βi .
∫
Because Dir (x, α +β) is a probability function,
∫ we have: Dir (x, α +
β)dx = 1. Besides, the geometric mean Dir (x, α + β) log x i dx
equals ψ (α i + βi ) − ψ (α 0 + β 0 ), where ψ is the digamma function.
Then we get:

(2)

In our setting, we denote Hv as the set of all the hypotheses
encoding background information of word v. We want to calculate
the amount of surprise that is attached to the word v when a new
epoch’s data Ct +1 arrives. In particular, we use a multinomial distribution to model the likelihood P(Ct +1 |Hv ). We adopt Dirichlet distribution as our prior distribution, as it is the conjugate prior of the
multinomial distribution. Denote Dirichlet distribution as Dir (x; α),
where α = [α 1 , α 2 , ..., α |V | ], |V | is the size of vocabulary, αu ∈V
counts the appearances of word u in the neighborhood of word v in
history (i.e., αu = |{t ′ ∈ [t] | u ∈ aN N (ϕ t ′ (v))}|, [t] = {1, . . . , t }).
Formally,
P(Hv ) = Dir (x; α 1 , α 2 , ..., α |V | )
Í |V |
|V |
Γ(
αi ) Ö
(3)
= Î i=1
x iα i −1 .
|V |
i=1 Γ(α i ) i=1

|V |
|V |
Õ
Γ(α 0 + β 0 ) Õ
Γ(α i )
log
βi (ψ (α i ) − ψ (α 0 ))
+
+
Γ(α 0 )
Γ(α i + βi ) i=1
i=1
(8)
We can further simplify this equation as follows to avoid direct
calculation of Γ(x):

Score(v) = log

Score(v) =

α 0 +β
Õ0 −1

log j −

j=α 0

|V | α i +β
Õ
Õi −1
i=1

j=α i

log j +

|V |
Õ

βi (ψ (α i ) − ψ (α 0 ))

i=1

(9)
Words with a high score by Equation 9 are considered to be
shifted in semantics during the current time period.

5

When the data of the new epoch Ct +1 (or new observations) is
given, we can construct a new KNN graph G t +1 . Let Et +1 (v) be the
set of edges starting from word v in the network G t +1 . For word
u ∈ V , we define βu as the denotation of whether word u appears
in the nearest neighbor set of word v in G t +1 :
(
1, (v, u) ∈ Et +1 (v)
βu =
(4)
0, else

EXPERIMENTS

In this section, we present empirical experiments that compare the
proposed method with various baselines and alternative methods
on four public data sets.

5.1

Competing Methods

We compare the proposed method against three types of baselines, two representing how the task is done in previous studies,

829

Session 7B: Content & Semantics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

k-nearest neighbors aN Nt (v) and aN Nt +1 (v) of word v from two
epochs, we can evaluate the difference between the two sets through
the Jaccard Distance:

by computing the changes of the embeddings of words, and one
representing an alternative version of the proposed method. They
include (i) a method [20] that uses the embeddings trained in previous time period as the initialization of the current period, (ii) a
method [21] that independently learns embeddings from different
periods and aligns them thereafter, and (iii) a method that uses the
Jaccard Distance to quantify the topological structure change.
Init: this method uses the previous epoch’s embedding as the initialization for the current epoch’s embedding, then trains the embeddings with LI N E [37], and then uses the Euclidean distance
between the embedding vectors of a word in two epochs to quantify the change, that is, Score(v) = ∥ϕ t +1 (v) − ϕ t (v)∥. In practice,
we have explored both the Euclidean distance and the Cosine similarity as the scoring function and did not observe much difference.
We therefore use the Euclidean distance for all methods.
LR: this method trains word embeddings independently for each
time period, and then aligns them through Linear Regression [21].
An important prerequisite for this method is that most words did not
change. Given this prerequisite, one can align the embedding spaces
into one unified coordinate system and characterize the change of
meanings. Suppose that we have trained two embeddings Φt (V ) and
Φt +1 (V ), we seek to learn a linear transformation LRt →t +1 (v) ∈
Rp×p that maps a word v from ϕ t (v) to ϕ t +1 (v) by solving the
following optimization problem:
Õ
LR = arдmin
∥ϕ t (v)LR − ϕ t +1 (v)∥22 .
(10)
t →t +1

LR

Ñ
aN Nt +1 (v) | − | aN Nt (v) aN Nt +1 (v) |
Ð
.
| aN Nt (v) aN Nt +1 (v) |
(14)
The difference between aNN+Jaccard and our proposed method,
denoted as aNN+Bayesian, is that the former does not use Bayesian
surprise to measure the shift.
Actually, some heuristics are general and can be combined. For
example, one can always initialize the embeddings of the current
epoch with the embeddings of the previous epoch. One can also use
other embedding learning algorithms (such as Word2Vec) instead
of LINE. This creates more baselines such as LINE+Init+aNN+Jaccard or even variations of the proposed method such as LINE+Init+
aNN+Bayesian. We explore several combinations in Table 1.

Score(v) =

5.2

DBLP Abstract
The DBLP Abstract dataset consists of more than 3.07 million
papers, 2.55 million of which have abstracts4 . It spans 64 years from
1954 to 2017. We divide the corpus into 64 snapshots.

(11)

LLR: this method is similar to LR, but uses Local Linear Regression
as its alignment function [21]. Unlike LR that learns a transformation function using all words, LLR only considers nearest neighbors. Denote the k-nearest neighbors of a word v at epoch t in
embedding space as: kN N (ϕ t (v)). Having trained two embeddings
Φt (V ) and Φt +1 (V ), we seek to learn a local linear transformation
LLRt →t +1 (v) ∈ Rp×p that maps a word v from ϕ t (v) to ϕ t +1 (v)
by solving the following optimization problem:
Õ
LLR(v) = arдmin
∥ϕ t (w)LLR − ϕ t +1 (w)∥22 (12)
LLR

ACMDL
The ACMDL dataset consists of full text papers of ACM publications. It contains nearly 197 thousand conference papers and nearly
77 thousand journal papers. This data spans 61 years from 1951 to
2011. We divide the corpus into 61 separate snapshots.
The Google Books Ngram Dataset
The Google Books Ngram dataset5 consists of more than 5 million digitized books in eight languages over five countries and
nearly 1010 observed words, in forms of n-grams (n ∈ [1, 5]) with
term frequency and document frequency. Following the sampling
procedure described in [21], we sample 200 million 5-grams from
English Fictions and consider the years from 1800 to 2004. We divide
the data into 41 snapshots by every 5 years.

w ∈k N N (ϕ t (v))

Then the displacement is defined as:
Score(v) = ∥ϕ t (v)LLR(v) − ϕ t +1 (v)∥.

Data Sets

ACM Abstract
The ACM Abstract dataset consists of more than 2.38 million
papers, 1.67 million of which have abstracts3 . It spans 66 years from
1951 to 2016. We divide the corpus into 66 snapshots.

Denote ϕ t′ (v) = ϕ t (v)LRt →t +1 (v) the transformation of ϕ t (v). For
some words, the difference between ϕ t′ (v) and ϕ t +1 (v) could be
very large, and it is likely that the meanings of these words have
shifted. So we can define the change as:

t →t +1

Ð

We choose four large-scale, real-world data sets, ACM Abstract
[38], DBLP Abstract [38], ACMDL Papers [8] and Google Books Ngrams [28]. These data sets represent formal academic writings and
professional books, which are commonly used in existing studies
to analyze semantic shifts [14, 21, 41].

v ∈V

Score(v) = ∥ϕ t (v)LRt →t +1 (v) − ϕ t +1 (v)∥.

| aN Nt (v)

(13)

Apparently, all Init, LR, and LLR make the assumption that the
dimensionality of the semantic space is a constant over time, which
might not hold in reality. By comparing to these methods, we can
understand the benefits of representing the semantics space as a
manifold instead of a vector space.
aNN+Jaccard: this method uses a topological representation of
the semantics space and uses the Jaccard distance [2] to identify
the change, which is similar to the method used in [7, 13]. Like
the proposed method, it first uses LINE to train embeddings for
each epoch and then calculates approximate k-nearest neighbors
for each word v in embedding space. Suppose we have approximate

5.3

Ground Truth

To quantitatively evaluate the performance of proposed method
and the competing methods, we will need the ground truth of
words that have truly changed their meanings. Unfortunately, such
ground-truth is hard to obtain and does not exist on all data sets.
3 This
4 This

data is from: https://aminer.org/citation, we use ACM-V8 in our experiment.
data is from: https://aminer.org/citation, we use DBLP-V10 in our experiment.

5 This data is available at: http://storage.googleapis.com/books/ngrams/books/datasetsv2.html

830

Session 7B: Content & Semantics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: Performance of competing methods: LINE+Init+aNN+Bayesian achieves top performance.
ACM Abstract Dataset
Recall
MAP
98.75
62.44⋄⋆⋆⋆
⋆⋆⋆
⋆⋆⋆
19.75⋄⋄⋄
6.99⋄⋄⋄
⋆⋆⋆
⋆⋆⋆
39.75⋄⋄⋄
18.78⋄⋄⋄
⋆⋆⋆
⋆⋆⋆
57.00⋄⋄⋄
21.80⋄⋄⋄
96.75
52.90⋄
⋆⋆⋆
⋆⋆⋆
77.00⋄⋄⋄
36.02⋄⋄⋄
98.00
58.34
⋆
⋆⋆
93.50⋄⋄
47.38⋄⋄⋄
97.00
54.51

Method
LINE+Init
word2vec+LLR
word2vec+LR
LINE+LLR
LINE+LR
LINE+Init+LLR
LINE+Init+LR
LINE+aNN+Jaccard
LINE+Init+aNN+Jaccard
Proposed methods
LINE+aNN+Bayesian
LINE+Init+aNN+Bayesian

⋆
92.50⋄⋄
94.25⋄

DBLP Abstract Dataset
Recall
MAP
99.75
65.01
⋆⋆⋆
⋆⋆⋆
23.50⋄⋄⋄
5.26⋄⋄⋄
⋆⋆⋆
⋆⋆⋆
48.50⋄⋄⋄
22.47⋄⋄⋄
⋆⋆⋆
⋆⋆⋆
69.00⋄⋄⋄
31.89⋄⋄⋄
98.75
58.64⋄
⋆⋆⋆
⋆⋆⋆
80.00⋄⋄⋄
39.66⋄⋄⋄
99.75
64.89
⋆⋆
85.75⋄⋆
48.78⋄⋄
98.75
60.47

⋆⋆⋆
87.54⋄⋄⋄
⋆⋆⋆
91.82⋄⋄⋄

⋆⋆⋆
96.70⋄⋄⋄
⋆⋆⋆
97.97⋄⋄⋄

98.00⋄
98.00

ACMDL Dataset
Recall
MAP
98.25
60.52⋆⋆⋆
⋆⋆⋆
⋆⋆⋆
17.50⋄⋄⋄
5.04⋄⋄⋄
⋆⋆⋆
⋆⋆⋆
58.75⋄⋄⋄
27.49⋄⋄⋄
⋆⋆⋆
⋆⋆⋆
4.25⋄⋄⋄
1.83⋄⋄⋄
⋆
91.25⋄⋄
48.72⋄⋄⋄
⋆⋆⋆
⋆⋆⋆
8.75⋄⋄⋄
2.40⋄⋄⋄
97.75
57.43
⋆⋆⋆
⋆⋆⋆
79.00⋄⋄⋄
32.61⋄⋄⋄
95.50
49.50

94.50⋄
97.75⋆

⋆⋆⋆
89.41⋄⋄⋄
⋆⋆⋆
93.98⋄⋄⋄

Google Books Dataset
Recall
MAP
75.00
51.23
⋆⋆⋆
⋆⋆⋆
4.50⋄⋄⋄
0.48⋄⋄⋄
⋆⋆⋆
⋆⋆
31.75⋄⋄⋄
14.88⋄⋄⋄
⋆⋆
⋆⋆
57.25⋄⋄⋄
27.77⋄⋄⋄
72.50
42.35⋄⋄
⋆
63.25⋄⋄⋄
34.11⋄⋄⋄
76.00
52.31
⋆
68.00⋄⋄
37.63⋄⋄
73.00
44.69
⋆⋆⋆
96.50⋄⋄⋄
⋆⋆⋆
99.00⋄⋄⋄

⋆⋆⋆
96.17⋄⋄⋄
⋆⋆⋆
98.28⋄⋄⋄

All numbers are in percentage. “⋆(⋆⋆, ⋆⋆⋆)′′ indicates the result is significant over LINE+Init+aNN+Jaccard (in general the best among all non-Bayesian, manifold-based models)
according to paired t-test [42] at level 0.05(0.01,0.001). “⋄(⋄⋄,⋄⋄⋄)′′ indicates the result is significant over LINE+Init+LR according to paired t-test at level 0.05(0.01,0.001).

Kulkarni el at. [21] created a reference list R of 21 words that
have changed meanings with a very high probability during the
time frame of the Google book data, which has been used as a
benchmark [12, 18, 20, 41]. We adopt the same evaluation procedure
and compared the competing methods.
Each algorithm outputs a ranked list of words based on their
scores or ranks of possible shifts in any 5-year period on the Google
books data set. Denote such a ranked list by L, we evaluate the
Precision@p with respect to the reference list R, where p is the
number of top words for evaluation in the result list L.
We plot the 5 representative methods in Figure 1. From Figure 1,
we can see that our proposed algorithm LINE+Init+ann+Bayesian
can effectively identify shifts of word semantics, which clearly outperforms other baselines in most of the cases.
Note that in this test scenario, even the precision of the best
method does not exceed 40%. This is not surprising as the 21 words
is by no means a comprehensive set of words with semantic changes.
These methods may have correctly identified other words, but they
are just not included in the small reference list. We show some of
these words found by our methods in Section 6.2.

5.4

The reference list R of 21 words is insufficient to obtain statistical
significance, and such a list does not exist for other data sets. To
better compare the methods, Kulkarni et al. [21] proposed a neat
procedure. They created a synthetic ground truth to a data set in
which real words are injected with clear linguistic shifts.
Following their procedure, we create a synthetic ground truth
for each data set. We randomly sample 200 pairs of words with
similar frequency from certain time periods (in Google Books 19502000, in ACMDL 1991-2010, in DBLP abstract 1991-2016, in ACM
abstract 1991-2015) with two conditions: 1) their frequencies in the
entire data set are larger than 2,000, and 2) their frequencies in the
current epoch and several previous epochs are larger than 200. The
two restrictions are set to ensure that the words being selected are
relatively stable and less noisy.
After obtaining the pairs of words, we artificially swap the paired
words in a randomly selected time period. In this way, we know
that the meanings of these words have significantly changed in the
corresponding time period. Naturally, a good algorithm should be
able to identify such words in that period and and rank them high.
To compare the performance of different models, in each period
we require each model to rank 50 words by the score of semantic
shifts. Based on the synthetic ground truth, we calculate the Recall
and the Mean Average Precision (MAP) of each method.
The overall performance of all competing models are shown in
Table 1, it is clear that our proposed methods LINE+aNN+Bayesian
and LINE+Init+aNN+Bayesian both outperform all the other methods in MAP, without a sacrifice of Recall. LINE+Init+aNN+Bayesian
achieves near-perfect MAP and Recall on all data sets, while the
best performing competing methods obtain a MAP lower than 80%.
Comparing between the word embedding methods, the methods
using LINE+Init to learn the embeddings perform better than the
methods using LINE alone or using Word2Vec, even if the vectors
are aligned later or a semantic manifold is built. This is possibly
due to the fact that using last period’s embedding to initialize the
current period not only aligns the embedding space but also reduces
noises in a single epoch. In addition, models using LINE perform
better than models using Word2Vec.

0.40
0.35
0.30

Precision

0.25
0.20
0.15
0.10
LINE+Init
Word2vec+LR
LINE+Init+LR
LINE+Init+aNN+Bayesian
LINE+Init+aNN+Jaccard

0.05
0.00
0

20

40

60

p in Precision@p

80

Synthetic Ground Truth

100

Figure 1: Performance on identifying ground truth.

831

Session 7B: Content & Semantics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

0.5
LINE
LINE+Init

LINE
LINE+Init

0.8

Jaccard Distance

Percentage

0.4

0.3

0.2

0.1

0.0

0.7

0.6

0.5

0.4

[0, 0.4]

(0.4, 0.5] (0.5, 0.6] (0.6, 0.7] (0.7, 0.8] (0.8, 0.9] (0.9, 1.0]

0

Jaccard Distance Range

(a) Percentage of each Jaccard Distance interval

2000

4000

6000

Frequency

8000

10000

12000

(b) Average Jaccard Distance of frequency

Figure 2: The Jaccard Distance distribution of two embedding coordinate systems trained with same parameters independently.

6

Comparing different versions of alignment-based models, we
find that the Linear Regression model performs better than Local
Linear Regression model in general. The reason might once again
be the existence of a large number of noisy points.
It is a little surprising that applying initialization before training embedding also improves the performance of manifold-based
methods, as a topological structure does not need the alignment of
embeddings. Again, our hypothesis is that a good initialization of
embedding learning not only aligns the embedding space but also
reduces noise and uncertainty in a single epoch.
Comparing methods that use semantic manifolds, we can see a
significant improvement introduced by Bayesian surprise. Comparing to Jaccard distance, Bayesian surprise takes all the historical
nearest neighbors into consideration and is robust to uncertainty
and noise. When Bayesian surprise is used, the difference between
using Init+ or not (4.57% in MAP on ACMDL) is much smaller than
when Jaccard is used (16.89% in MAP on ACMDL). This supports
our hypothesis that initializing word embeddings using previous
epochs is reducing the noise and uncertainty in a single epoch.
More exploration of this phenomenon is deferred to Section 6.1.

6.1

MAP

0.7

0.5

0.3
LINE+Init+Bayesian
LINE+Bayesian
LINE+Init+Jaccard
LINE+Jaccard
100

200

300

#samples(*Million)

400

The Influence of Noisy Points

In Section 5.4, we find that nearly all models trained with Init+
perform better than those with random initialization of embeddings.
This also holds for manifold-based models, which in theory should
not depend on the initialization. We hypothesize that this is because
Init+ can help us reduce noisy points, which are words that appear
infrequently and likely generate false positives. In order to verify
our hypothesis, we first find a way to quantify the stability of
embedding models with and without Init+.
We focus on two methods: LINE and LINE+Init, which only differ
by the initialization. We train both models twice, and then measure
the dissimilarity of the nearest neighbors of every word between
the first and the second run. Intuitively, if a model is stable enough,
the neighbors of a word in two runs should not change much. From
Figure 2(a), we can see that LINE+Init has more words with smaller
dissimilarity of neighbors, measured by the Jaccard Distance. This
indicates that Init+ indeed improves the stability of a model. Figure 2(b) shows that words with low frequency are more likely to
have a high dissimilarity of neighbors between two runs. These
infrequent words could easily produce noises for models without
Init+. Thus a general suggestion is to adopt Init+. Another solution
could be to filter out infrequent words beforehand.
Another possible way to reduce noise is trying to train embeddings sufficiently. To verify this, we vary the hyper-parameter “samples” in the LINE model, which controls the number of examples for
training. We show the results on Synthetic ACMDL data set, as the
same conclusion is observed on all four data sets. As Figure 3 shows,
as samples increase, the advantage of models with Init+ shrinks.
This empirically proves that with more sufficient training, we are
less likely to suffer from the noises.

0.9

0.1

DISCUSSIONS

The overall results show that our proposed methods clearly outperform baselines on detecting words with semantic shifts. Let us turn
our attention to some specific behaviors of the models.

500

Figure 3: Performance of using different number of training
samples in Synthetic ACMDL

832

Session 7B: Content & Semantics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 2: Sample of words detected by LINE+Init+ann+Bayesian on four data sets.

ACM Abstract
ACM Abstract
DBLP Abstract
DBLP Abstract
DBLP Abstract
DBLP Abstract
DBLP Abstract
ACMDL
ACMDL
ACMDL
Google Books
Google Books
Google Books

6.2

Word
cloud
deep
cloud
deep
spark
app
pooling
guest
cloud
soft
cell
gay
web

rank
1
1
1
1
1
5
8
5
9
20
1
1
3

year
2009
2014
2009
2013
2015
2011
2015
2003
2009
1991
2000
1985
2000

Past neighbors
surface, volumetric, terrain, subsurface, fiduciary
deeper, insight, understand, dig, deeply
clouds, surface, brdf, ice, subsurface
deeper, propagation, ld, shallow, integration
php, engine, perl, commercial, labview
viterbi, ml, sam, turbo, me
combing, weighting, grouping, fusion, pyramid
editor, editors, writer, newsletter, presenter
clouds, earth, map, sphere, terrain
practical, on, pushed, turned, highlighted
cabin, tent, cubicle, locker, trailer
lively, cheerful, courteous, sensitive, prosperous
maze, galaxy, peninsula, network, northern

What Our Method Learns

shallow

To gain a better understanding of the proposed method, we qualitatively analyzed some words to which new meanings are attached.
Table 2 shows results from our LINE+Init+ann+Bayesian method
on four data sets. Some words are singled out because that they
have truly acquired new meanings. For example, the word cloud
acquired a new sense “networks of computing equipment” in 2009
due to the development of cloud service. The same thing happens
to words web, gay, cell and guest.
Interestingly, another type of words found out by our model have
not necessarily changed meaning on its own, but are serving as a
part of a new concept. For example, the word deep, together with
word learning, constitutes a new concept in Computer Science.
Similar things happened for words soft, spark and pooling.
In Figures 4 and 5, we present two visualizations of how certain
words have shifted in the semantic manifolds. The shifts clearly
present differences in the local neighborhoods, which are captured
by Bayesian surprise.

7

Present neighbors
grid, ubiquitious, percasive, desktop, compute
deeper, neural, multilayer, perceptron, bp
clouds, grid, computing, virtualized, ubiquitous
multilayer, neural, spiking, multilayered, recurrent
apache, hadoop, mapreduce, opencl, standalone
iphone, android, apps, smartphone, microsoft
pyramid, discriminative, boosted, feature, encoding
daemon, host, linux, unix, servlet
hosting, services, enterprise, infrastructure, virtualization
ware, tracking, dynamic, hard, modeling
phone, mobile, telephone, wallet, card
lesbian, lesbians, urban, homosexual, adults
site, website, releases, internet, quest

deeper

integration
propagation

id

spiking

multilayered
recurrent

autoencoder

multilayer

neural

perception

)( )

)(

ensembles

concatenated

)( )

codes

)(

ldpc

rateless

punctured

Figure 4: Shifts of “deep” and “convolutional” in the semantic manifold (DBLP Abstract).

CONCLUSIONS AND FUTURE WORK

We have introduced a novel method to detect shifts of word semantics over time. Our method represents word semantics in a
manifold space, which is robust to distortions and does not require
alignments of embeddings. Bayesian surprise is applied to detect
changes in the topological space, which significantly outperforms
a complete set of baselines on four real world data sets with both
real and synthetic ground truth. Through the experiments, we also
obtained a comprehensive comparison among all existing methods.
One future direction of this work will focus on analyzing which
particular new meanings are added to the words. Moreover, generalizing our method to other network tasks is also a promising
direction, e.g., detecting a user’s interest change in social network
enables us to better recommend new topics and new friends.

fabric

purest
substance

costume
profile

magnitude
roadway

vastness
trough

pathway

murky
benches

site
website

Acknowledgment

internet

This work was supported in part by the National Science Foundation
under grant numbers 1054199, 1633370, 1131500 and 1620319, and in
part by the NSFC(61625107), and Key program of Zhejiang Province
(2015C01027).

solar
releases

Figure 5: A shift of the word “web” in the semantic manifold
(Google Books).

833

Session 7B: Content & Semantics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

REFERENCES

[23] John Lee. 2010. Introduction to topological manifolds. Vol. 940. Springer Science
& Business Media.
[24] Omer Levy, Yoav Goldberg, and Ido Dagan. 2015. Improving Distributional
Similarity With Lessons Learned From Word Embeddings. Transactions of the
Association for Computational Linguistics (2015).
[25] Annie P Louis. 2014. A Bayesian Method to Incorporate Background Knowledge During Automatic Text Summarization. Association for Computational
Linguistics.
[26] Aloysius P Martinich. 1984. Communication and reference. Walter de Gruyter.
[27] Qiaozhu Mei and ChengXiang Zhai. 2005. Discovering evolutionary theme
patterns from text: an exploration of temporal text mining. In Proceedings of the
eleventh ACM SIGKDD international conference on Knowledge discovery in data
mining. ACM, 198–207.
[28] Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser Aiden, Adrian Veres,
Matthew K Gray, Joseph P Pickett, Dale Hoiberg, Dan Clancy, Peter Norvig,
Jon Orwant, et al. 2011. Quantitative analysis of culture using millions of digitized books. science 331, 6014 (2011), 176–182.
[29] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector Space. arXiv preprint arXiv:1301.3781
(2013).
[30] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed Representations of Words and Phrases and Their Compositionality.
In Advances in neural information processing systems. 3111–3119.
[31] Dirk Ostwald, Bernhard Spitzer, Matthias Guggenmos, Timo T Schmidt, Stefan J
Kiebel, and Felix Blankenburg. 2012. Evidence for neural encoding of Bayesian
surprise in human somatosensation. Neuroimage 62, 1 (2012), 177–188.
[32] William D Penny. 2001. Kullback-liebler Divergences of Normal, Gamma, Dirichlet and Wishart densities. Wellcome Department of Cognitive Neurology (2001).
[33] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online Learning of Social Representations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. 701–710.
[34] Ananth Ranganathan and Frank Dellaert. 2009. Bayesian surprise and landmark
detection. In Robotics and Automation, 2009. ICRA’09. IEEE International Conference
on. IEEE, 2017–2023.
[35] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. 2011. Hogwild:
A Lock-free Approach to Parallelizing Stochastic Gradient Descent. In Advances
in neural information processing systems. 693–701.
[36] Jian Tang, Jingzhou Liu, Ming Zhang, and Qiaozhu Mei. 2016. Visualizing Largescale and High-dimensional Data. In Proceedings of the 25th International Conference on World Wide Web. 287–297.
[37] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei.
2015. LINE: Large-scale Information Network Embedding. In Proceedings of the
24th International Conference on World Wide Web. 1067–1077.
[38] Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. 2008. Arnetminer: extraction and mining of academic social networks. In Proceedings of
the 14th ACM SIGKDD international conference on Knowledge discovery and data
mining. ACM, 990–998.
[39] Elizabeth Closs Traugott. 1989. On the rise of epistemic meanings in English: An
example of subjectification in semantic change. Language (1989), 31–55.
[40] Xuerui Wang and Andrew McCallum. 2006. Topics over time: a non-Markov
continuous-time model of topical trends. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery and data mining. ACM, 424–433.
[41] Derry Tanti Wijaya and Reyyan Yeniterzi. 2011. Understanding Semantic Change
of Words Over Centuries. In Proceedings of the 2011 international workshop on
DETecting and Exploiting Cultural diversiTy on the social web. 35–40.
[42] Yiming Yang and Xin Liu. 1999. A re-examination of text categorization methods.
In Proceedings of the 22nd annual international ACM SIGIR conference on Research
and development in information retrieval. ACM, 42–49.

[1] [n. d.]. Preemptive. https://en.wikipedia.org/wiki/Bayes%27_theorem.
[2] [n. d.]. Preemptive. https://en.wikipedia.org/wiki/Jaccard_index.
[3] Pierre Baldi and Laurent Itti. 2010. Of Bits and Wows: a Bayesian Theory of
Surprise With Applications to Attention. Neural Networks (2010).
[4] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. 2003.
A Neural Probabilistic Language Model. Journal of machine learning research
(2003).
[5] Andreas Blank. 1999. Why do new meanings occur? A cognitive typology of the
motivations for lexical semantic change. Historical semantics and cognition 13
(1999), 6.
[6] David M Blei and John D Lafferty. 2006. Dynamic topic models. In Proceedings of
the 23rd international conference on Machine learning. ACM, 113–120.
[7] Igor Brigadir, Derek Greene, and Pádraig Cunningham. 2015. Analyzing Discourse
Communities With Distributional Semantic Models. In Proceedings of the ACM
Web Science Conference. 27.
[8] Yinlin Chen and Edward A Fox. 2014. Using ACM DL paper metadata as an
auxiliary source for building educational collections. In Proceedings of the 14th
ACM/IEEE-CS Joint Conference on Digital Libraries. IEEE Press, 137–140.
[9] Kenneth Ward Church and Patrick Hanks. 1990. Word Association Norms, Mutual
Information, and Lexicography. Computational linguistics (1990).
[10] Lea Frermann and Mirella Lapata. 2016. A Bayesian Model of Diachronic Meaning
Change. Transactions of the Association for Computational Linguistics (2016).
[11] Ioannis Gkioulekas, Georgios Evangelopoulos, and Petros Maragos. 2010. Spatial
Bayesian surprise for image saliency and quality assessment. In Image Processing
(ICIP), 2010 17th IEEE International Conference on. IEEE, 1081–1084.
[12] Kristina Gulordava and Marco Baroni. 2011. A Distributional Similarity Approach
to The Detection of Semantic Change in The Google Books Ngram Corpus.
In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural
Language Semantics. 67–71.
[13] William L Hamilton, Jure Leskovec, and Dan Jurafsky. 2016. Cultural Shift or
Linguistic Drift? Comparing Two Computational Measures of Semantic Change.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing, Vol. 2016.
2116.
[14] William L Hamilton, Jure Leskovec, and Dan Jurafsky. 2016. Diachronic Word
Embeddings Reveal Statistical Laws of Semantic Change. In Proceedings of the
54th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), Vol. 1. 1489–1501.
[15] Barbara S Held and Edward Pols. 1985. The confusion about epistemology and
âĂĲepistemologyâĂİâĂŤand what to do about it. Family process 24, 4 (1985),
509–517.
[16] Laurent Itti and Pierre Baldi. 2009. Bayesian Surprise Attracts Human Attention.
Vision research (2009).
[17] Laurent Itti and Pierre F Baldi. 2006. Bayesian surprise attracts human attention.
In Advances in neural information processing systems. 547–554.
[18] Adam Jatowt and Kevin Duh. 2014. A Framework for Analyzing Semantic Change
of Words Across Time. In Proceedings of the 14th ACM/IEEE-CS Joint Conference
on Digital Libraries. 229–238.
[19] Margarita Kay. 1979. Lexemic change and semantic shift in disease names. Culture,
medicine and psychiatry 3, 1 (1979), 73–94.
[20] Yoon Kim, Yi-I Chiu, Kentaro Hanaki, Darshan Hegde, and Slav Petrov. 2014.
Temporal Analysis of Language through Neural Language Models. ACL 2014
(2014), 61.
[21] Vivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, and Steven Skiena. 2015. Statistically Significant Detection of Linguistic Change. In Proceedings of the 24th
International Conference on World Wide Web. 625–635.
[22] Solomon Kullback. 1997. Information Theory and Statistics. Courier Corporation.

834

