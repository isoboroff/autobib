Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

A Test Collection for Evaluating Legal Case Law Search
Daniel Locke

Guido Zuccon

Queensland University of Technology
Brisbane, Queensland
daniel.locke@hdr.qut.edu.au

Queensland University of Technology
Brisbane, Queensland
g.zuccon@qut.edu.au
Statistic Locke et al. [11] Koniaris et al. [8] Koniaris et al. [9]
Documents
63,916
63,742
3,890
Queries
100
330
330
Assessments
2645
105,036
unknown
Purpose query generation
diversification
diversification

ABSTRACT
Test collection based evaluation represents the standard of evaluation for information retrieval systems. Legal IR, more specifically
case law retrieval, has no such standard test collection for evaluation. In this paper, we present a test collection for use in evaluating
case law search, being the retrieval of judicial decisions relevant
to a particular legal question. The collection is made available at
ielab.io/caselaw.

Table 1: Collections previously created for use in case law
retrieval.
Source
Courtlistener
Austlii

Table 2: Number of documents available in publicly assessable collections, as recorded on 2 February 2018.

ACM Reference Format:
Daniel Locke and Guido Zuccon. 2018. A Test Collection for Evaluating
Legal Case Law Search. In SIGIR ’18: The 41st International ACM SIGIR
Conference on Research & Development in Information Retrieval, July 8–
12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https:
//doi.org/10.1145/3209978.3210161

1

60,000 documents, do not accurately represent the number of documents that a lawyer may search on. This is representative of, as
the collections are indeed composed of, only the decisions of one
court. By way of comparison, the current total number of legal
decisions in common publicly accessible legal search systems are
listed in Table 2: these are orders of magnitude larger than the
collections of Locke et al. and Koniaris et al.. For this reason, as we
describe below, we use a much greater amount of documents in our
collection. This number of documents, we believe, is of a realistic
size of what lawyers may search over.
The collections of Locke et al. [11] and Koniaris et al. [8] are also
not appropriate for case law search use in so far as the assessments
are concerned. Locke et al. [11] pooled methods prior to assessment, thus ensuring that there were judgments for all methods they
considered. However, with an average of roughly 26 assessments
per topic, this limited number of assessments is not particularly
appropriate for a recall orientated task such as case law retrieval
[7], nor is it likely to be applicable to methods that were not pooled.
Such a limited number of assessments per topic is representative of
the extensive work required to assess documents.
Koniaris et al.’s [8] collection has a large number of assessments.
But, these assessments were not manually gathered: they assumed
as relevant a subset of the highest ranked documents from LDA
topic models.
Turtle [18] details two collections created for internal use at
Westlaw,1 the latter of which containing 410,000 documents, is a
more realistic representation of the number of documents that a
lawyer may search on.
While Turtle’s collection is not publicly available, the methods
they describe for the creation of their collection are pertinent, as
we describe in Section 3.3. They used expert searchers, all of whom
were attorneys, to produce the best query for a topic. Each searcher
was allowed to issue as many queries as they liked in the formulation stage. This differs from the method used in Locke et al. [11]
where queries were pre-determined and then poooled. As we describe below, we take a combination of these methods and those

INTRODUCTION

Test collection based evaluation represents the standard of evaluation for information retrieval (IR) systems. Case law search is the
retrieval of judicial decisions relevant to particular legal question.
Lawyers must be able to find case law that applies to a client’s
situation. The amount of case law increases rapidly each year. Even
in 1962, Wilson [19] noted the large material lawyers must search
on and, with over 25,000 decisions being published each year, its
ever growing nature. Despite this, there is little research in the area.
Case law retrieval is therefore an important area of research.
As previous authors have noted [8], there is no standard test
collection for the evaluation of systems for this task. There are
several previous small collections, such as the work of Locke et
al. [11] and Koniaris et al. [8, 9]. These collections are, however,
task specific and, as we describe below, are limited in so far as their
assessments are concerned.
This paper presents a test collection for evaluating IR methods
to improve case law retrieval for lawyers or the general public,
thereby addressing the problem in the field. The collection comprises 3,597,230 documents, 12 topics and associated relevance assessments, as well as tags for relevant portions of documents.

2

Decisions available
4,009,726
841,686

RELATED WORK

In Table 1, we outline the available collections for case law retrieval.
Locke et al. [11] and Koniaris et al. [8] both created collections
based on an underlying similar document base. Both, with around
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210161

1A

1261

popular commercial case law search system.

Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Statistic
Docs
Stem occurences
Avg. docs judged rel. per query
Avg. docs judged nonrel. per query
Topics

used by Locke et al. [11], and use manually attained assessments
from lawyers and paralegals to gather relevance assessments.
Apart from these collections, with respect to legal collections
that focus on question answering, there are the collections of Do
et al. [4], which contains 1,105 documents, and of Penas et al. [15],
which contains 10,000 documents. These both focus on the related,
but distinct, task of retrieval in a civil law system, where legislative
instruments take greater importance than judicial decisions. Somewhat more closely related is FIRE [13] which concerned finding
cited cases in a legal decision. Nontheless, none of these collections
properly align with the task that this collection is concerned with.

Number
4,029,354
8,924,819,817
68
144
12

Table 3: Statistics of our collection.

3 THE CASE LAW COLLECTION
3.1 Corpus
Our collection is comprised of 3,597,2302 decisions downloaded
from www.courtlistener.com/api. It contains all American judicial
decisions available as at 7 November 2017.
Each decision contains a unique ID, the case name, the date on
which it was filed, a list of unique IDs of other decisions that cite
the particular decision, and an HTML version of the text of the
decision. We make available the list of document ID’s constituting
every document in our collection, as well as a script for creating the
documents, and for their indexing and mapping into Elasticsearch.3

3.2

Topics

Figure 1: Interface created for assessing documents. Label A
shows the list of pooled documents, and whether the documents have been assessed. Label B shows the current document being assessed. Lable C shows the dropdown menu
enabling assessors to select the appropriate relevance level.

Our collection contains 12 topics. We are currently in the process of
expanding our collection to a total of 38 topics. Each topic consists
of a question presented to the United States Supreme Court in
an appeal. We randomly selected each decision from among the
decisions of the 2016 term of the Supreme Court that were used by
Locke et al. [11] to create the topics in their collection. We did this
so that we could use the queries created for assessing documents
in that collection as queries to be issued to pool documents for
assessment for our topics. As an example of a topic in our collection:
“Whether the Federal Arbitration Act preempts a
state-law contract rule that singles out arbitration
by requiring a power of attorney to expressly refer
to arbitration agreements before the attorney-in-fact
can bind her principal to an arbitration agreement.”
To create a topic, from a question presented to the USSC, we
took the smallest amount of text necessary that was understandable
without reference to outside information. Where one of the decisions presented multiple questions we took the first. And, where
the question presented contained further information such as introductory paragraphs, we removed that information so long as
the question as presented would enable an assessor to adequately
identify relevant documents without it. This was done so as not to
burden the assessors with irrelevant information.

3.3

We developed an interface that provided each assessor with a
list of relevant documents, pre-issued queries as described below,
and a search interface to enable assessors to issue further queries.
We present a view of that interface in Figure 1.
We adopt a similar method used to create the internal Westlaw
collections detailed in Turtle’s work [18]. We asked assessors to
assess a minimum of 200 documents, or to continue until they think
they have found all relevant documents. We did not, like UQV100
[2], ask assessors to estimate the number of relevant documents
for the reason that our assessors are expert assessors and that it is
difficult to estimate this number a-priori for this search task.
Assessors were given a guideline for assessing, with four relevance levels: (i) not relevant; (ii) background; (iii) explanatory; and
(iv) on point. We believe these levels are accurate representations of
the worth of legal decisions as to a question of law. This is similar to
the relevance levels used in Turtle’s work [18]. However, we further
distinguished relevant documents between those that are of utility
to those that add little value. Apart from the definitions employed
by Turtle, we further define background as where a decision discusses some information relevant to the topic, including where the
information is relevant to the broader area of law, i.e. a discussion
of jurisdiction, not in the context of the specific topic legislative
provision; and, explanatory as where a decision discusses at length
information relevant to the topic, providing “substantial and valuable information”, including where the information is relevant to
the broader area of law, i.e. a discussion of jurisdiction, not in the
context of the specific topic legislative provision.

Relevance Assessments and Queries

Relevance assessments were manually conducted by two admitted
lawyers and a paralegal. These assessors were familiar with the
case law search task and performed related activities on a weekly
basis. We have a total of 2572 assessments over 12 topics.
2 There are 4,029,354 documents in the collection however, only 3,597,230 have
text searcheable in the HTML field that we extracted.
3 https://www.elastic.co/

1262

Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

We pooled assessments at a depth of 15. This is lower than the
pooling at depth 20 undertaken in the assessment of the collection
of Turtle [18]. However, as Turtle notes, result sets that retrieve
extensive number of documents may be far larger than what “most
users are willing to browse.” As with Turtle, the choice of our low
cutoff reflects our preference to the task being one of interactive
retrieval; we are looking for multiple queries to be iteratively refined
(as described below). Finally, this choice was also made for the
reason that we did not want to overburden assessors with many
results for each query, and for both time and financial reasons.
We do not take the list of documents that cite a document as
relevant. This is because a legal decision may cite previous decisions
for a number of different topics. Accordingly, we cannot take a
citation as relevant for a particular topic without having reviewed
it. This can be contrasted with collections involving systematic
reviews, where citations are reflective of relevance [16].
For each topic, an initial group of queries were evaluated, and
pooled documents presented to assessors. These queries comprised
the topic description, as well as two sentences and paragraphs from
the decision that the topic was based on, and queries based on these
sentences and paragraphs, taken from the work of Locke et al. [11].
We also asked assessors to generate queries, as they would in the
course of such a task normally, in order to meet a minimum number
of relevance assessments per topic. Assessors were allowed to use
best match queries4 or boolean queries similar to those used in a
commercial system.5 Again, the results were pooled and assessors
were to assess all documents that were added to the pool.
For the purposes of evaluation, it is possible to use graded relevance, as per the levels we provide. Alternatively, one can use binary
relevance. To use binary relevance, we recommend that explanatory
and on-point documents are combined to constitute the relevant
documents for a topic. This is because many documents that have
a relevance level of background merely address the broader area of
law, providing little more than a single commonly cited sentence.

3.4

Figure 2: Tagging of relevant text in a document. The red
highlighted text indicates the tagged portion of a document.
r

P @5 P @10 P @100 M AP R@10 R@100 M RR

- 0.7273 0.6455 0.5682

0.1764 0.3469 0.2403

0.4813 0.7601

kli-best
idf-best
plm-best

9 0.7727 0.6545 0.5591
7 0.7727 0.6545 0.5455
9 0.7727 0.6455 0.5500

0.1773 0.3503 0.2395
0.1795 0.3399 0.2228
0.1773 0.3475 0.2358

0.4834 0.7980
0.4886 0.7903
0.4834 0.7903

kli-mid
idf-mid
plm-mid

5 0.6364 0.5636 0.4636
5 0.6364 0.5909 0.5000
5 0.5000 0.5364 0.4773

0.1709 0.2928 0.1834
0.1677 0.3162 0.2139
0.1559 0.2689 0.1911

0.4482 0.7143
0.4746 0.7294
0.4312 0.6386

base-fire
idf-fire
plm-fire

- 0.6030 0.3417 0.2211
5 0.6281 0.3628 0.2362
5 0.6131 0.3548 0.2367

0.0374 0.3637 0.4422
0.0390 0.3902 0.4724
0.0385 0.3859 0.4734

0.7487 0.7017
0.7809 0.7193
0.7709 0.7097

klip-airs 8 0.7000 0.4420 0.3120
idfp-airs 9 0.6900 0.4480 0.3130
plmp-airs 6 0.7000 0.4460 0.3190

0.0576 0.4213 0.4354
0.0568 0.4166 0.4357
0.0570 0.4252 0.4393

0.6848 0.7671
0.6829 0.7660
0.6863 0.7640

Table 4: Comparison of methods implemented by Locke et
al. [11] (AIRS) on our collection and on the collections used
in their paper and in the FIRE IrLED prior decision retrieval
task [13].
their implementation is publicly available,6 and because they are the
only methods that were tested using both the FIRE collection [10,
13] and Locke et al.’s collection [11]. Thus, the empirical results
would allow us to study the effectiveness of their methods and
determine whether the relative difference in performance across
methods is generalisable to our collection.
We evaluated the methods from Locke et al. [11], for proportions
of the topic question. We used proportions 1/|D| through to D, where
D was the number of terms in the topic question. The methods used
to score and select term were Parsimonious Language Model, Inverse Document Frequency and Kullback-Liebler Informativenesss.
Each query was then evaluated as a best-match query in ElasticSearch.7 We used the same parameters for BM25. We did not tune
λ with respect to its use in the expectation-maximisation algorithm
of PLM; we set it at 0.5.
We report the results of these evaluations in Table 4. We report
both the best performing proportion for our collection (e.g. idf-best),
and the best performing proportion for the Locke et al. [11] (AIRS)
collection (e.g. idfp-airs), where that proportion was selected to
maximise MAP. We also report, the middle proportion for both our
collection (e.g. idf-mid) and that used in FIRE (e.g. idf-fire) (taking

Relevance Tags

Assessors were also asked to tag relevant portions of documents
that they assessed. For example, we show in Figure 2 how a portion
of text that is relevant to a topic might be tagged.
We supply these tags as an absolute start-end position in the
HTML. We have a total of 948 tags. The average tag length is 202
words. The average intersection between the words in a topic keywords and within a tag was computed using the Jaccard Coefficient.
This value is 0.0342, which highlights the difficulty posed by the
case law retrieval task, with relevant passages containing little overlap with a topic’s keywords. We describe potential uses for these
tags in Section 4.

4

P @1

Topic

POTENTIAL USES

The obvious use of our collection is the evaluation of information
retrieval system effectiveness tailored to the case law search task.
To further demonstrate its use, we evaluate the methods of Locke
et al. [11] using our collection. We selected those methods because

4 Using

6 Locke et al. made the code available at https://github.com/ielab/
ussc-caselaw-collection
7 Elasticsearch version 6.0.0.

an Elasticsearch best match query.
Boolean search feature matched that of LexisNexis https://www.lexisnexis.
com/help/global/US/en_US/gh_terms.asp.
5 The

1263

Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

half of the original query text). The results highlight similar trends
as were present in Locke et al.’s evaluations in FIRE and AIRS.
IDF outperforms the PLM and KLI methods for most measures.
Interestingly, while Locke et al.’s evaluation in AIRS found that PLM
outperformed KLI based reduction methods, KLI appears to be more
comparable, and even outperforms PLM based reduction in most
measures on our collection. That being said, the difference is not
particularly large. Aside from the traditional use of our collection
to assess search system effectiveness, we present the following
potential uses.

4.1

We also make the interface available for others to contribute
relevance assessments at ielab.io/caselaw. Necessarily, this brings
about issues of ensuring the quality of assessments, typical of crowdsourced assessments settings [1]. To this end, we will ensure quality
through user registration and approval, as well as evaluation of time
spent assessing [12], multiple assessments per document-query pair,
and reassessment of sampled document-query pairs by one of our
expert assessors.
Acknowledgements. Guido Zuccon is the recipient of an Australian
Research Council DECRA Research Fellowship (DE180101579) and a Google
Faculty Research Award.

Network effect on ranking

REFERENCES

As the extracted documents that comprise our collection provide a
list of other documents in the collection that cite a decision, one
possible application to investigate is the effect of networks on document ranking. Tapper [17] has suggested the application of citations
to the ranking of documents in legal case search. Fowler [5] has
shown that citation networks of United States Supreme Court decisions mimic those of citation networks in other domains. Despite
this, to the best of our knowledge, only the FLEXICON [6] system,
which considers citations in a vector-space ranking model, and
Koniaris et al. [8] use citation networks for ranking. The former
does not discuss effectiveness, and the latter considers only the
effect on diversifying results.

4.2

[1] Omar Alonso, Catherine C Marshall, and Marc A Najork. 2013. A human-centered
framework for ensuring reliability on crowdsourced labeling tasks. In First AAAI
Conference on Human Computation and Crowdsourcing.
[2] Peter Bailey, Alistair Moffat, Falk Scholer, and Paul Thomas. 2016. UQV100: A
test collection with query variability. In Proceedings of the 39th International ACM
SIGIR conference on Research and Development in Information Retrieval. ACM,
725–728.
[3] Jody J Daniels and Edwina L Rissland. 1997. What you saw is what you want:
Using cases to seed information retrieval. In International Conference on CaseBased Reasoning. Springer, 325–336.
[4] Phong-Khac Do, Huy-Tien Nguyen, Chien-Xuan Tran, Minh-Tien Nguyen, and
Minh-Le Nguyen. 2017. Legal Question Answering using Ranking SVM and Deep
Convolutional Neural Network. arXiv preprint arXiv:1703.05320 (2017).
[5] James H Fowler, Timothy R Johnson, James F Spriggs, Sangick Jeon, and Paul J
Wahlbeck. 2007. Network Analysis and the Law: Measuring the Legal Importance
of Precedents at the US Supreme Court. Political Analysis (2007), 324–346.
[6] Dephne Gelbart and JC Smith. 1991. Beyond boolean search: FLEXICON, a legal
tex-based intelligent system. In Proceedings of the 3rd international conference on
Artificial intelligence and law. ACM, 225–234.
[7] Kevin Gerson. 1999. Evaluating Legal Information Retrieval Systems: How do the
Ranked-retrieval Methods of WESTLAW and LEXIS Measure Up? Legal Reference
Services Quarterly 17, 4 (1999), 53–67.
[8] Marios Koniaris, Ioannis Anagnostopoulos, and Yannis Vassiliou. 2016. Multidimension Diversification in Legal Information Retrieval. In International Conference on Web Information Systems Engineering (WISE’16). Springer, 174–189.
[9] Marios Koniaris, Ioannis Anagnostopoulos, and Yannis Vassiliou. 2017. Evaluation
of Diversification Techniques for Legal Information Retrieval. Algorithms 10, 1
(2017), 22.
[10] Daniel Locke and Guido Zuccon. 2017. Automatic cited decision retrieval: Working notes of Ielab for FIRE Legal Track Precedence Retrieval Task. In Working
notes of FIRE 2017-Forum for Information Retrieval Evaluation (CEUR Workshop
Proceedings).
[11] Daniel Locke, Guido Zuccon, and Harrisen Scells. 2017. Automatic Query Generation from Legal Texts for Case Law Retrieval. Springer International Publishing,
181–193.
[12] E. Maddalena, M. Basaldella, D. De Nart, D. Degl’Innocenti, S. Mizzaro, and G.
Demartini. 2016. Crowdsourcing relevance assessments: The unexpected benefits
of limiting the time to judge. In HCOMP’16.
[13] Arpan Mandal, Kripabandhu Ghosh, Arnab Bhattacharya, Arindam Pal, and Saptarshi Ghosh. 2017. Overview of the FIRE 2017 track: Information Retrieval from
Legal Documents (IRLeD). In Working notes of FIRE 2017-Forum for Information
Retrieval Evaluation (CEUR Workshop Proceedings).
[14] Bhaskar Mitra and Nick Craswell. 2017. Neural Models for Information Retrieval.
arXiv preprint arXiv:1705.01509 (2017).
[15] Anselmo Peñas, Pamela Forner, Richard Sutcliffe, Álvaro Rodrigo, Corina Forăscu,
Iñaki Alegria, Danilo Giampiccolo, Nicolas Moreau, and Petya Osenova. 2010.
Overview of ResPubliQA 2009: Question Answering Evaluation Over European
Legislation. Multilingual information access evaluation I. text retrieval experiments
(2010), 174–196.
[16] Harrisen Scells, Guido Zuccon, Bevan Koopman, Anthony Deacon, Leif Azzopardi,
and Shlomo Geva. 2017. A test collection for evaluating retrieval of studies for
inclusion in systematic reviews. In Proceedings of the 40th International ACM
SIGIR Conference on Research and Development in Information Retrieval. ACM,
1237–1240.
[17] Colin Tapper. 1974. Legal Information Retrieval by Computer: Applications and
Implications. McGill Law Journal 20 (1974), 26.
[18] Howard Turtle. 1994. Natural Language vs. Boolean Query Evaluation: A Comparison of Retrieval Performance. In Proceedings of the 17th Annual International
ACM SIGIR Conference on Research and Development in Information Retrieval.
212–220.
[19] Robert A Wilson. 1962. Computer retrieval of case law. Sw. LJ 16 (1962), 409.

Use of tags

We foresee several potential uses for the tags that we provide. One
potential use is their relevance to focused retrieval. Daniels and
Rissland [3] discuss retrieval of documents at a paragraph level
rather than at a document level, focusing on presenting to users
relevant paragraphs within a document. Given the tagging of relevant portions within documents, our collection may be of utility
for evaluating methods for presenting to users relevant portions.
One further potential use of the tags we provide is for learning
to rank or neural retrieval models. As Mitra and Craswell [14] identify, where the retrieval of long documents is concerned, there is
a problem with regard to large amounts of irrelevant surrounding
text. Learning to rank focuses on reranking based on the top-k
documents returned by a ranking function. Tags identifying relevant portions of documents remove this irrelevant surrounding text.
Similarly, a potential use lies in applying the tags to intra-document
weighting of portions to learning to rank or to neural network models for information retrieval, in the context of document ranking.
It is envisioned, that perhaps, the ranking function could discount
the worth of part of the document that is not within the tag, or
increase the worth of the document that falls within the tag.

5

CONCLUSION AND FUTURE WORK

We provide a test collection for evaluating case law retrieval. The
collection contains approximately 4 million decisions — 2 orders
of magnitude more documents than previous collections; 12 topics
taken from questions presented to the United States Supreme Court;
and 2572 relevance assessments — with many more assessments per
topic than existing collections, as well as being graded rather than
binary. We are currently extending our collection to a total of 38
topics, adhering to the assessment requirements set out above. We
are also extending the collection by means of making it available
to further users to provide relevance assessments.

1264

