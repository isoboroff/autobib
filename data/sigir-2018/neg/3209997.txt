Session 2D: Conversational Systems

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Dialogue Act Recognition via CRF-Attentive Structured Network
Zheqian Chen

Rongqin Yang

Zhou Zhao

State Key Lab of CAD & CG
Zhejiang University, Hangzhou
zheqianchen@gmail.com

State Key Lab of CAD & CG
Zhejiang University, Hangzhou
rongqin.yrq@gmail.com

College of Computer Science
Zhejiang University, Hangzhou
zhaozhou@zju.edu.cn

Deng Cai

Xiaofei He

State Key Lab of CAD & CG
Alibaba-Zhejiang University Joint
Institute of Frontier Technologies
dengcai@cad.zju.edu.cn

Fabu Inc.
State Key Lab of CAD & CG
xiaofeihe@fabu.ai

ABSTRACT
Dialogue Act Recognition (DAR) is a challenging problem in dialogue interpretation, which aims to associate semantic labels to utterances and characterize the speaker’s intention. Currently, many
existing approaches formulate the DAR problem ranging from multiclassification to structured prediction, which suffer from handcrafted feature extensions and attentive contextual dependencies.
In this paper, we tackle the problem of DAR from the viewpoint of
extending richer Conditional Random Field (CRF) structured dependencies without abandoning end-to-end training. We incorporate
hierarchical semantic inference with memory mechanism on the
utterance modeling at multiple levels. We then utilize the structured
attention network on the linear-chain CRF to dynamically separate the utterances into cliques. The extensive experiments on two
primary benchmark datasets Switchboard Dialogue Act (SWDA)
and Meeting Recorder Dialogue Act (MRDA) datasets show that
our method achieves better performance than other state-of-the-art
solutions to the problem.

Speaker

Utterance

A
B
A
B
A
B
A
B
A
B

Hi, long time no see.
Hi, how are you?
What are you doing these days?
I’m busying writing my paper.
I heard that the deadline is coming.
Yeah.
You need to make a push.
Sure, that’s why I am so busy now.
I can’t bother you for too long, goodbye.
See you later.

DA Label
Greeting
Greeting
Question
Answer
Statement
Backchannel
Opinion
Agreement
Farewell
Farewell

Table 1: A snippet of a conversation sample. Each utterance
has related dialogue act label.

1

INTRODUCTION

Dialogue Act Recognition (DAR) is an essential problem in modeling and detecting conversation structure. The goal of DAR is to
attach semantic labels to each utterance in a conversation and recognize the speaker’s intention, which can be regarded as a sequence
labeling task. Many applications have benefited from the use of automatic dialogue act recognition such as dialogue systems, machine
translation, automatic speech recognition, topic identification and
talking avatars [20] [14]. One of the primary applications of DAR is
to support task-oriented conversation agent system. Knowing the
past utterances of DA can help ease the prediction of the current DA
state, thus help to narrow the range of utterance generation topics
for the current turn. For instance, the "Greeting" and "Farewell" acts
are often followed with another same type utterances, the "Answer"
act often responds to the former "Question" type utterance. Thus if
we can correctly recognize the current dialogue act, we can easily
predict the following utterance act and generate a corresponding response. Table 1 demonstrates a snippet of the kind of conversation
structure in which we are interested.
The essential problem of DAR lies in predicting the utterance’s
act by referring to contextual utterances with act labels. Most of the
existing models adopt handcrafted features and formulate the DAR
as a multi-classification problem. However, these methods which
adopt feature engineering process and multi-classification algorithms reveal fatal weakness from two aspects: First, they are labor

CCS CONCEPTS
• Human-centered computing → HCI theory, concepts and
models; Natural language interfaces; Interaction design theory, concepts and paradigms; Contextual design;

KEYWORDS
Dialogue Act Recognition, Conditional Random Field, Structured
Attention Network
ACM Reference Format:
Zheqian Chen, Rongqin Yang, Zhou Zhao, Deng Cai, and Xiaofei He. 2018.
Dialogue Act Recognition via CRF-Attentive Structured Network. In SIGIR
’18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8–12, 2018, Ann Arbor, MI, USA. 10 pages.
https://doi.org/10.1145/3209978.3209997
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3209997

225

Session 2D: Conversational Systems

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

2

intensive and cannot scale up well across different datasets. Furthermore, they abandon the useful correlation information among contextual utterances. Typical multi-classification algorithms like SVM,
Naive Bayes [13] [2] [38] can not account for the contextual dependencies and classify the DA label in isolation. It is evident that during a conversation, the speaker’s intent is influenced by the former
utterance such as the previous "Greeting" and "Farewell" examples.
To tackle these two problems, some works have turned to structured prediction algorithm along with deep learning tactics such
as DRLM-Conditional [17], LSTM-Softmax [20] and RCNN [19].
However, most of them fail to utilize the empirical effectiveness
of attention in the structured graphical networks and rely completely on the hidden layers of the network, which may cause the
structural bias. A further limitation is that although these works
claim they have considered the contextual correlations, in fact they
view the whole conversation as a flat sequence and neglect the dual
dependencies in the utterance level and act level [4] [16] [30]. Until
now, the achieved performances in DAR field are still far behind
human annotator’s accuracy.
In this paper, we present the problem of DAR from the viewpoint
of extending richer CRF-attentive structured dependencies along
with deep neural network. For simplicity, we call the framework
as CRF-ASN (CRF-Attentive Structured Network). The main idea
is inspired by the long range contextual capturing method proposed by Liu et al. [28] and the structural attention proposed by
Kim et al. [21]. Specifically, we utilize the hierarchical semantic
inference integrated with memory mechanism on the utterance
modeling. The memory mechanism is adopted to enable the model
to look beyond localized features and have access to the entire sequence. The hierarchical semantic modeling learns different levels
of granularity including word level, utterance level and conversation level. We then develop internal structured attention network
on the linear-chain conditional random field (CRF) to specify structural dependencies in a soft manner. This approach generalizes
the soft-selection attention on the structural CRF dependencies
and takes into account the contextual dependencies on the nearing
utterances. It is notable that the whole process is differentiable thus
can be trained in an end-to-end manner.
The main contributions of this paper are as follows:

CRF-ATTENTIVE STRUCTURED NETWORK

In this section, we study the problem of dialogue act recognition
from the viewpoint of extending rich CRF-attentive structured
dependencies. We first present the hierarchical semantic inference
with memory mechanism from three levels: word level, utterance
level and conversation level. We then develop structured graphical
attention to the linear chain conditional random field to fully utilize
the contextual dependencies.

2.1

The problem

Before presenting the problem, we first introduce some basic mathematical notions and terminologies for dialogue act recognition.
Formally, we assume the input is in the form of sequence pairs:
D = (C 1 , C 2 , ..., C N ) with Y = (Y1 , Y2 , ..., YM ). Cn is the input of
the n-th conversation in dataset D and Ym is the m-th targeted
dialogue act type. Each conversation Ci is composed of a sequence
of utterances which denoted as Ci = (U1 , U2 , ..., U j ) with aligned
act types (y1 , y2 , ..., y j ). We have each dialogue act type assigned to
utterance u j → y j and each associated y j ∈ Y denoted the possible
dialogue act belongs to Y act types. Again each utterance consists
of a sequence of diverse words Ui = (w 1 , w 2 , ..., w t ).
Most of the previous models do not leverage the implicit and intrinsic dependencies among dialogue act and utterances. They just
consider a conversation as a flat structure with an extremely long
chain of words. However, such a construction suffers vanishing
gradient problem as the extremely long words become impractical in the neural network back-propagation training process. To
alleviate this problem, we consider the conversation to be a hierarchical structure composed of three level encoders: first encode each
word in a fine-grained manner, and the second encoder operates
at the utterance level, the last encoder encodes each utterance in
the conversation level. Each encoder is based on the previous one
thus can make sure the output of the previous one can capture
the dependencies across the conversation. Apart from hierarchical
neural encoders, we also integrate external memory to allow the
model to have unrestricted access to the whole sequence rather
than localized features as in RNNs.
Naturally the dialogue act recognition problem can be regarded
as a sequence labeling task which can be assigned dialogue act
through multi-classification method or the structured prediction
algorithms. In our formulation, we adopt the linear chain conditional random field (CRF) along with hierarchical attentive encoders
for the structured prediction. Instead of labeling each utterance
in isolation, structured prediction models such as HMM, CRF can
better capture the contextual dependencies among utterances. In
our model, we define the structured attention model as being an
extended attention model which provides an alternative approach
for incorporating the machinery of structural inference directly
into our neural network.

• Unlike the previous studies, we study dialogue act recognition from the viewpoint of extending rich CRF-attentive
structured dependencies. The proposed CRF structural attention on the DAR problem provides an alternative approach to
encode the internal utterance inference with dialogue acts.
• We propose the hierarchical deep recurrent neural network
with memory enhanced mechanism to adequately model
the utterance semantic representations. The proposed framework can be trained end-to-end from scratch and can be
easily extended across different dialogue tasks.
• We conduct extensive experiments on two popular datasets
SWDA and MRDA to show that our method outperform
several state-of-the-art solutions on the problem. It is worth
noting that our approach has achieved nearly close to the
human annotator’s performance on SWDA within 2% gap,
which is very convincing.

2.2

Hierarchical Semantic Network

Due to the hierarchical nature of conversations, our proposed model
is constructed at multiple levels of granularity, e.g. word level, utterance level and conversation level. The representation of a conversation can be composed by each utterance u j , and each u j can
be obtained by combining the representations of constituent words

226

Session 2D: Conversational Systems

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Figure 1: The Overview of learning memory enhanced hierarchical conversation representation architecture. The memory
hop is set to 1. First concatenate the rich word embedding and obtain the original utterance representation ut from the basic
BiGRU. The hidden state ht represents the contextual encoding which cares the former and the latter utterance dependencies.
After summarizing hierarchical memory enhanced output ot and the original utterance ut , we get the final representation ut
denoted in a bold form.
w t . Taking inspiration from Memory Networks and incorporate
so-called memory hops, we adopt the memory enhanced contextual
representations in order to have unrestricted access to the whole
sequence rather than localized features as the former recurrent
neural network. After capturing the fine-grained utterances, we
assign each dialogue act label via CRF-based structural labeling
on the final layer. Here we include the memory enhanced hierarchical representation in Figure 1 to depict the conversation level
representation.
As illustrated in Figure 1, the hierarchical semantic network can
be divided into three parts: (1) fine grained initial embedding layer.
(2) memory enhanced contextual representation layer. (3) Dialogue
act labeling layer. The second part can be further broken down into
three main components: (a) the input memory m 1:t which takes
in the output from the word embedding layer. (b) the contextual
attention which takes the consideration of the former utterance
and the latter one. (c) the output memory c 1:t which is obtained
from the input memory connected with the attention mechanism.
The weights are determined by measuring the similarity between
the input memory and the current utterance input.
Fine Grained Initial Embedding: For a given conversation, each
utterance u j is encoded by a fine grained embedding layer. We first
try to utilize the rich lexical factors and linguistic properties to
enhance the word representation. For each word token w t in each
utterance, we initialized the word embedding using pretrained embeddings such as Word2vec or Glove. Furthermore, in order to tackle
the out-of-vocabulary (OOV) problem, we adopt the character-level
word embedding via CNN to combine with pretrained word level
embeddings. We also extend the lexical factors via POS tag and

NER tag to enhance the utterance understanding. The obtained four
factors are concatenated to form a rich lexical representation as:
ek = fembed (w t , c t , pos, ner )

(1)

Since we consider the bidirectional GRU to encode the representation of each utterance, we concatenate the outputs from the
forward and backward GRU hidden representations at the time
step. For each utterance Ut which consists a sequence of words
w 1 , w 2 , ..., w j , the original semantic representation is as follows:
Ut = f BI GRU (ek )

(2)

The diagram is illustrated in Figure 2. Here we utilize fembed
and fGRU to represent the word level embedding function and
utterance level encoder in our hierarchical model. After obtained
the original semantic representations on each utterance, we later
apply the memory enhanced contextual layer to further explore the
correlations between utterances.
Memory Enhanced Contextual Representation: Every utterance in a conversation is encoded with Ut = f BI GRU (ek ), where
f BI GRU (·) is the encoding function via Bi-GRU to map the input
words into a vector Ut ∈ Rd . The original sequence utterances
are denoted as {U1 , U2 , ..., Ut }. While this original semantic representation can be the input component in the context of memory
network. In order to tackle the drawback of insensitivity to temporal information between memory cells, we adopt the approach
in injecting temporal signal into the memory using a contextual
recurrent encoding:
Ct = tanh(Wm−1Ct −1 + Wm+1Ct +1 + bm )

227

(3)

Session 2D: Conversational Systems

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

2.3

Figure 2: An illustration of the fine grained initial utterance
embedding. Here we use four kinds of lexical information
to model the utterance sequence.

where Wm−1 , Wm+1 , bm are learnable parameters to adjust the
contextual state Ct .
It is a remarkable fact that the new sequence Ct can be seen as the
contextual integrated representations which take consider of the
former utterance and the latter one. The injected temporal signal
can further explore the contextual influence on the current input
utterance. We thus can make use of this obtained Ct to represent
another Ut which cares more about the context influence.
For the current input utterance Ut , in memory networks, the
input is required to be in the same space as the input memory.
Here we adopt the popular attention mechanism in the memory by
measuring the relevance between current input utterance Ut and
the contextual new representation Ct . The relevance is measured
with a softmax function:
p j,i = so f tmax (UtT Ct )

(4)

Once the attention weights have been computed, the output
memory can be used to generate the final output of the memory
layer in the form of a weighted sum over the attention and the
input utterance:
X
ot =
p j,i Ct
(5)
i

The output allows the model to have unrestricted access to elements in previous steps as opposed to a single hidden state Ut in
recurrent neural networks. Thereby we can effectively detect the
long-range dependencies among utterances in a conversation.
To further extend the complex reasoning over multiple supporting facts from memory, we adopt a stacking operation which stacks
hops between the original utterance semantic representation Ut
and the kth output hop ot to be the input to the (k + 1)th hop:
Utk+1 = okt + Utk

Structured CRF-Attention Network

Traditional attention networks have proven to be an effective approach for embedding categorical inference within a deep neural
network. However, In DAR problem, we need to further explore the
structural dependencies among utterances and dialogue acts. As
we see, utterances in a conversation are not existed independently.
The latter utterance may be the responding answer to the former
question, or that the chunk of utterances are in the same act type.
Here we consider generalizing selection to types of chunks selecting attention, and propose the structured attention to dynamicly
segment the utterances distribution within networks. Such a structured attention can be interpreted as using soft-segmentation that
considers all possible structures over the utterance input. Specifically, we draw the Figure 3 to illustrate how the structured attention
network work to soft-segment the utterances with each other.
In our paper, we formulate the DAR as a sequence labeling problem. It is a natural choice to assign a label to each element in the
sequence via linear chain CRF, which enable us to model dependencies among labels. Here we do not directly apply the original linear
chain CRF to the learned utterance. Although the dependencies
among utterances have been captured by the former hierarchical
semantic networks, we still need to further explore the dialogue
act dependencies in the label level. For dialogue act sequence labeling problem, greedily predicting the dialogue act at each time-step
might not optimal the solution. Instead, it is better to look into the
correlations in both utterance level and the dialogue act level in
order to jointly decode the best chain of dialogue acts.
Formally, let U = {U1 , U2 , ..., Un } represent a sequence of utter
ance inputs, let y = y1 , y2 , ..., yn be the corresponding dialogue
act sequence. Variable z are discrete latent variables {z 1 , z 2 , ..., zn }
with zi ∈ {0, 1}. The aim of the structured attention is to incorporate
structural dependencies across the conversation C. For instance,
given a conversation C, we explicitly cluster and seperate the utterance structure by looking into the dialogue act types with latent
variables z. Here we assume the utterances in the conversation as
an undirected graph structure with n vertices. The CRF is parameterized with clique potentials θc (zc ) ∈ R, indicating the subset of
z give by clique c. Figure 4 shows the structured attention model
for segmentation and clustering in conditional random field. As we
can see, in a conversation, the utterances accompany with dialogue
acts are segmented into different modules according to the latent
variable z.
Under this definition, the attention probability is defined as
P
p(z|U , y; θ ) = so f tmax ( C θc (zC )). For symmetry, we use the softmax in a general sense, i.e. so f tmax (д(z)) = z1 exp(д(z)), where
P
z = z ′ exp(д(z ′ )) is the implied recognition function. Here θ
comes from the former memory enhanced deep model over utterances U and corresponding dialogue acts y.
The conversation C over the utterances and dialogue acts is
defined as expectation:
X
C = Ez∼p (z |U ,y ) [f (U , z)] =
Ez∼p (zC |U ,y ) [fC (U , zC )] (7)

(6)

c

where Utk +1 encodes not only information at the current step (U jk ),

where we assume the annotation function f factors into f (U , y, z) =
P
C fC (U , y, zC ). The annotation function is defined to simply return the selected hidden state. The conversation C can be interpreted as an dialogue act aware attentive conversation as taking

but also relevant knowledge from the contextual memory (okt ). Note
that in the scope of this work, we limit the number of hops to 1 to
ease the computational cost.

228

Session 2D: Conversational Systems

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Figure 3: The diagram explictly model the selection of contiguous subsequences. Here we apply structrued attention network
to dynamically measure the relevance between utterances and make a soft-segmentation. For utterances with similar dialogue
act, the model will cluster them together; otherwise, the model will seperate them into two modules.
marginal distribution p(zi |U , y) can be calculated efficiently in linear time via the forward-backward algorithm. These marginals
further allow us to implicitly sum over the linear chain conditional
random field. We refer to this type of attention layer as a structural
attention layer , where we can explicitly look into the undirected
graphical CRF structure to find which utterances are in the same
chunk or in isolation.
Here we define the node potentials with a unary CRF setting:
Figure 4: A linear-chain structured attention model for segmentation in conditional random field.

θ i (j) =

n
X
j=1

the expectation of the annotation function with respect to a latent
variable z ∼ p, where p is parameterized to be function of utterances
U and dialogue acts y.
The expectation is a linear combination of the input representation and represents how much attention will be focused on each
utterance according to the dialogue act sequence. Concretely, we
P
define our annotation function to be f (U , y, z) = ni=1 fi (U , y, zi ),
where fi (U , y, zi ) = 1 {zi = 1} (Ui , yi ). The explicit expectation is
then:
n
X
Ez1, ...,zn [f (U , y, z)] =
p(zi = 1|U , y)(Ui , yi )
(8)

UiT (y j )

(10)

where for each utterance we summarize the possible dialogue act to
perform sequential reasoning. Given the potential, we compute the
structural marginals p(z 1 , ..., zn |u, y) using the forward-backward
algorithm, which is then used to compute the final probability of
predicting the sequence of dialogue acts as:
p(y1 , y2 , ..., yn , U1 , U2 , ..., Un ; θ )
Qn Pn
T
j=1 j=1 Ui (y j )
= P Qn Pn
T
y
j=1 j=1 Ui (y j )

i=1

2.4

The expectation is a linear combination of the input representations where the scalar is between [0, 1] and represents how much
attention should be focused on each input. We can model the structural dependencies distribution over the latent z with a linear chain
CRF with n states:
n−1
X
p(z 1 , ..., zn |U , y) = so f tmax (
θ i , θ i+1 (zi , zi+1 ))
(9)

(11)

End-to-End Training

We adopt the maximum likelihood training estimation to learn the
CRF-attentive structured parameters. Given the training set D with
(U , Y ) conversation pairs, the log likelihood can be written as:
L=

N
X

log p(Yi |Ui , Θ)

(12)

i=1

i=1

where we denote the Θ as the set of parameters within neural
networks from hierarchical layers: word embedding layer, memory
enhanced utterance modeling layer, CRF-attentive structured layer.

where θ zi ,zi +1 is the pairwise potential. Notice that the utterance U
and the dialogue act sequence y are both obtained from downstream
learned representation through hierarchical semantic network. The

229

Session 2D: Conversational Systems

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Dataset
SwDA
MRDA

We define the objective function in training process:
Θ L(Θ) = L(Θ) + λ ∥Θ∥ 22

min

(13)

where ρ is the learning rate and дt is the sub-gradient at time t.
Notice that one of our contributions is to apply CRF structural
attention as the final layer of deep models. The whole model can
be trained in an end-to-end manner. Here we consider the standard
Viterbi algorithm for computing the distribution p(z 1 , ..., zn |U , y; θ ).
For testing, we adopt Viterbi algorithm [41] to obtain the optimal
sequence by using dynamic programming techniques. The testing
procedure can be written as:
y ∈Y

3

Validation
112(22K)
11(15K)

Testing
19(4K)
11(15K)

and tune the parameters, we depart the original training dataset
into two parts, one for training and the other small part used to be
the validation set. We list the detailed statistics of the two datasets
in table 2.

3.2

Implemental Details

We preprocess each utterance using the library of nltk and exploit
the popular pretrained word embedding Glove with 100 dimensional vectors [33]. The size of char-level embedding is also set as
100-dimensional and is obtained by CNN filters under the instruction of Kim [22]. The Gated Recurrent Unit [8] which is variant
from LSTM [15] is employed throughout our model. We also refer
to Zhu et al. [52] to take modifications on GRU gating mechanism.
We adopt the AdaDelta [46] optimizer for training with an initial
learning rate of 0.005. We also apply dropout [37]between layers
with a dropout rate of 0.2. For the memory network enhanced reasoning, we set the number of hops as 1 to preliminary learn the
contextual dependencies among utterances. We do not set too many
hops as increasing the number of GRU layers reduced the accuracy
of the model. Early stopping is also used on the validation set with a
patience of 5 epochs. Conversations with the same number of utterances were grouped together into mini-batches, and each utterance
in a mini-batch was padded to the maximum length for that batch.
The maximum batch-size allowed was 48. During training, we set
the moving averages of all weights as the exponential decay rate
of 0.999. The whole training process takes approximately 14 hours
on a single 1080Ti GPU. All the hyper-parameters were selected
by tuning one hyper-parameter at a time while keeping the others
fixed.
Here we use the Accuracy metric to access the quality of the
predicted dialogue acts. Given the conversation C = {U1 , U2 , ..., Un }

with its ground-truth dialogue acts Y = y1 , y2 , ..., yn , we denote
the predicted dialogue actsby a as:

(15)

EXPERIMENTS

In this section, we conduct several experiments on two public DA
datasets SwDA and MRDA, and show the effectiveness of our approach CRF-ASN for dialogue act recognition.

3.1

Training
1003(173K)
51(76K)

Table 2: |C | is the number of Dialogue Act classes, |V | is the
vocabulary size. Training, Validation and Testing indicate
the number of conversations (number of utterances) in the
respective splits.

λ > 0 is a hyper-parameter to trade-off the training loss and regularization. By using SGD optimization with the diagonal variant of
AdaGrad, at time step t, the parameter Θ is updated as follows:
ρ
Θt = Θt −1 − q
дt
(14)
Pt
2
д
i=1 i

y ′ = arg max p(y|U , Θ)

|C | |V |
42 19K
5
10K

Data Preparation

We evaluate the performance of our method on two benchmark DA
datasets: Switchboard Dialogue Act Corpus (SwDA) and The ICSI
Meeting Recorder Dialogue Act Corpus (MRDA). These two datasets
have been widely used to conduct the dialogue act recognition or
the dialogue act classification tasks by several prior studies.
• SwDA: Switchboard Dialogue Act Corpus is a large handlabeled dataset of 1155 conversations from the Switchboard
corpus of human-to-human telephone speech. Each conversation involved two randomly selected strangers who had
been charged with talking informally about one of several
self-selected topics. For each utterance, together with a variety of automatic and semiautomatic tools, the tag set distinguishes 42 mutually exclusive utterance types via DAMSL
taxonomy.
• MRDA: The ICSI Meeting Recorder Dialogue Act Corpus
consists of hand-annotated dialog act, adjacency pair, and
hotspot labels for the 75 meetings in the ICSI meeting corpus. The MRDA scheme provides several class-maps and
corresponding scripts for grouping related tags together into
smaller number of DAs. In this work we use the most widely
used class-map that groups all tags into 5 DAs, i.e., Disruption (D), BackChannel (B), FloorGrabber (F), Question (Q),
Statement (S).
In our experiments, we follow the instructions of Kumar et
al. [23] and Lee et al. [25] to perform the pre-processing steps.
For two datasets, we performed pre-processing steps in order to
filter out the noise and some informal nature of utterances. We
first strip the exclamations and commas, and then we convert the
characters into lower-case. Notice that for SwDA, we only get the
training and testing datasets. In order to smooth the training step

Accuracy =

n
Y
1 X
(1 −
1[ai , yi ])
|U |
i=1

(16)

Ui ∈U

3.3

Performance Comparisons

We compare our proposed method with other several state-of-theart baselines for the problem of dialogue act recognition as follows:
• Bi-LSTM-CRF [23] method builds a hierarchical bidirectional LSTM as a base unit and the conditional random field
as the top layer to do the dialogue act recognition task.
• DRLM-Conditional [18] method combines postive aspects
of neural network architectures with probabilistic graphical
models. The model combines a recurrent neural network

230

Session 2D: Conversational Systems

Model
Human annotator
Ours (CRF-ASN)
Bi-LSTM-CRF
DRLM-Conditional
LSTM-Softmax
RCNN
CRF
HMM
SVM

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Accuracy(%)
84.0
80.8
79.2
77.0
75.8
73.9
71.7
71.0
70.6

Model
Full CRF-ASN
Simple CRF
Simple SVM
Simple Word Embedding
Simple Context state
Simple Memory Network
Simple Utterance Embedding

Accuracy(%)
80.8
79.5
77.8
78.7
79.1
78.3
79.0

Table 5: Component ablations on SwDA dataset

Table 3: Comparing Accuracy of our method (CRF-ASN)
with other methods in the literature on SwDA dataset.

Model
Ours (CRF-ASN)
Bi-LSTM-CRF
LSTM-Softmax
CRF
SVM

the testing evaluation. The experiments reveal some interesting
points:
• The results show that our proposed model CRF-ASN obviously outperforms the state-of-the-art baselines on both
SwDA and MRDA datasets. Numerically, Our model improves the DAR accuracy over Bi-LSTM-CRF by 1.6% and
0.5% on SwDA and MRDA respectively, which is very convincing to prove the superiority of our model.
• The deep neural networks outperform the other featurebased models. We can see the last three non-deep models
obtain worse performance than the top five deep-based methods. This suggests that the performance of dialogue act recognition can be improved significantly with discriminative
deep neural networks, either in convolutional neural network or the recurrent neural network.
• Apart from deep learning tactics, the problem formulations
are also critical to the DAR problem. We see structured prediction approaches eg. CRF-ASN, Bi-LSTM-CRF obtain better
results than multi-classification eg. LSTM-Softmax. What’s
more, under the same text encoding situation, the CRF-based
model achieves much better results than the SVM-based
method. Which can fully prove the superiority of the structured prediction formulation. We also notice that CRF is
better than HMM when adopted to the DAR task.
• The major differences between our proposed model CRFASN and the strong baseline BI-LSTM-CRF lie in two aspects:
First we adopt a more fine grained manner to encode the
utterances and utilize the memory enhanced mechanism to
compute the contextual dependencies. Second we employ
an adapted structured attention network on the CRF layer,
rather than directly apply the original CRF on the utterances.
These two modifications are essential and improve the performance significantly.

Accuracy(%)
91.4
90.9
86.8
83.9
81.8

Table 4: Comparing Accuracy of our method (CRF-ASN)
with other methods in the literature on the MRDA dataset.

•

•

•

•
•

language model with a latent variable model over shallow
discourse structure.
LSTM-Softmax [20] method applies a deep LSTM structure
to classify dialogue acts via softmax operation. The authors
claim that the word embeddings, dropout, weight decay and
number of LSTM layers all have large effect on the final
performance.
RCNN [4] method composes both sentence model and discourse model to extend beyond the single sentence. The
authors propose hierarchical CNN on sentence model and
RNN on the contextual discourses.
HMM [38] method treats the discourse structure of a conversation as a hidden Markov model and the individual dialogue
acts as observations emanating from the model states.
CRF Simple baseline which applies the text encoding and
CRF-based structure prediction on the DAR problem.
SVM Simple baseline which applies the text encoding and
multi-classification algorithm on the DAR problem.

Among them, The former four approaches eg. Bi-LSTM-CRF, DRLMConditional, LSTM-Softmax, RCNN all adopt the deep neural network model in order to better capture the utterances semantic
representations. The latter three methods (HMM, CRF, SVM) just
employ the simple feature selection on the text processing. About
half of the baselines including Bi-LSTM-CRF, DRLM-Conditional,
HMM, CRF consider the graphical structured prediction while the
others eg. RCNN, LSTM-Softmax, SVM just adopt the traditional
multi-classification algorithms.
Table 3 and Table 4 respectively show the experimental Accuracy
results of the methods on the SwDA and MRDA datasets. Note that
some of the experimental results are directly extracted from Kumar
et al. [24]. The hyper-parameters and parameters which achieve
the best performance on the validation set are chosen to conduct

3.4

Ablation Results

We respectively evaluate the individual contribution of the proposed
module in our model. We conduct thorough ablation experiments
on the SwDA dataset, which are recorded in the table 5. To make
it fair, we only modify one module at a time and fix the other
components to be in the same settings.
• We replace the proposed structured CRF-attention layer to
simple CRF, the results show structured CRF-attention layer
results in major improvement in the accuracy, approximately
over 1.3% absolute points. We further replace the structure
prediction formulation to multi-classification on SVM, the

231

Session 2D: Conversational Systems

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

BLEU-4 Score with diverse dialogue acts
fo

0.26

90
qw
80

0.24

qyd

Avg BLEU-4 Score

70
qy
60
sd
50
ad
40
h
30

0.22

0.20

0.18

aa

Original-CVAE
Bi-LSTM-CRF-CVAE
CRF-ASN-CVAE

20
b

0.16

10
sv

1

0
fo

qw

qyd

qy

sd

ad

h

aa

b

sv

3

4
5
Distinct dialogue acts

6

7

8

Figure 6: BLEU-4 score vs. the number of distinct reference
dialog acts. We use three types of dialogue generation model
CVAE to do the comparisons.

Figure 5: Confusion heatmap of CRF-ASN model for the
SwDA dataset. There are totally 10 DA labels, where the row
denotes the true label and the column denotes the predicted
label.

results drop dramatically, which illustrate the benefit of considering structural dependencies among utterances.
• We replace the fine-grained word Ew , Ec , POS, N ER to the
simple Glove vector. The results suggest that fine grained
word embedding is useful to represent a text. We also adapt
the context state Ct to only care its neighbor utterances. The
result is not satisfying, which conveys us that the basic text
understanding is critical in the semantic representations.
• We replace the memory network to directly apply CRF layer
to the utterance layer. We also conduct a comparing experiment which plus the original utterance to memory enhanced output. The two results show the designed hierarchical memory-enhanced components are helpful in the utterance understanding and modeling the contextual influence.

3.5

2

3.6

Dialogue Generation Application

In order to prove the superiority of our dialogue act recognition
performance, we apply our method to a recently proposed model
called CVAE [47] to assist open-domain dialogue generation task.
CVAE is a novel framework based on conditional variational autoencoders and generate dialogue responses under the condition of
learning a distribution over potential conversational intents. Here
we conduct the comparisons with three modifications of CVAE: (1)
Original CVAE, (2) CVAE with BI-LSTM-CRF and (3) CVAE with
CRF-ASN. The dataset we conduct on is the switchboard dialogue
corpus. Figure 6 indicates the BLEU-4 precision/recall vs. the number of distinct reference dialog acts. The comparisons reveal some
interesting points:
• Compared with the original CVAE model, Bi-LSTM-CRF enhanced CVAE and CRF-ASN enhanced CVAE achieve better
performance obviously. We notice that the original CVAE
only support fewer dialogue acts compared with the other
two models. It proves that dialogue generation performance
can get consistent improvements with the help of instant
dialogue act recognition.
• As the dialogue act types increase, the dialogue generation
performances first boost to the optimal and then gradually
fall down. Both of the Bi-LSTM-CRF and our CRF-ASN get
the optimal performance when the dialogue act types reach
to 6. This indicates that in most of the cases, the dialogue
acts major lie in the top 5∼6 types of dialogue act.
• Compared with Bi-LSTM-CRF-CVAE, our proposed CRFASN integrated CVAE consistently outperforms in a various
number of dialogue acts. It demonstrates that our proposed
method is significantly superior to the Bi-LSTM-CRF which
does not consider the structural dependencies in the DAR
task.

Visualization

Figure 5 shows the confusion heatmap of our proposed CRF-ASN
model for the SwDA dataset. Each element in the heatmap denotes the rate that the predicted label is the same to the true label.
We can see from the diagonal, the <sd,sd> <b,b> pairs achieve
the most satisfying matching score while <qyd, qyd> is much
worse than other pairs. This can be explained that the sd (statement) and b(acknowledge) have clearly self-identification while
qyd(Declarative Yes-No-Question) is easier to be mistakenly recognized. We can see that <qyd,qy> which represents (Declarative YesNo-Question,Yes-No-Question) is indeed hard to recognize since
their dialogue types are too similar to each other. For another reason,
we notice that due to the bias of the ground truth, there are some
cases that we predict the dialogue act correctly while the ground
truth is wrong. For some reason, classifying so many fine-grained
dialogue act labels is not easy for human annotators, besides the
human-subjectivity occupies an important role in recognizing the
dialogue act.

232

Session 2D: Conversational Systems

4

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

4.2

RELATED WORK

In this section, we briefly review some related work on dialogue
act recognition and attention network.

4.1

Attention Network

Attention mechanism has become an essential component in text
understanding in recent years. Since the first work proposed by
Bahdanau et al. [3] that adopt the attention mechanism in neural machine translation, attention mechanism based neural networks have
become a major trend in diverse text researching field, such as in
machine comprehension [7] [9] [32], machine translation [29] [10],
abstract summarization [35] [1], question answering [44] [48], text
classification [42] [50] [45], recommendation systems [49] and so
on. The principle of attention mechanism is to select the most
pertinent piece of information, rather than using all available information, a large part of it being irrelevant to compute the neural
response.
In our work, we propose the CRF-attentive structured network in
order to encode the internal utterance inference with dialogue acts.
The structured attention is a more general attention mechanism
which take account of the graphical dependencies and allow for
extending attention beyond the standard soft-selection approach.
The most similar work to our model is proposed by Kim et al. [21].
Kim et al. also experiment with two different classes of structured
attention networks: subsequence selection and syntactic selection.
However, the objectives of these two networks aims to segment the
structure dependencies, which are quite different from our DAR
task. In DAR task we care more on the dialogue act influences on the
overall conversation structure, thus the former structured attention
may not be suitable for our problem.

Dialogue Act Recognition

The main task of dialogue act recognition is to assign an act label
to each utterance in a conversation. Most of the existing work can
be categorized as following two groups.
Regarding the DAR as a multi-classification problem. Reithinger et al. [34] deal with the dialogue act classification using a
statistically based language model. Webb et al. [43] apply diverse
intra-utterance features involving word n-gram cue phrases to understand the utterance and do the classification. Geertzen et al. [12]
propose a multidimensional approach to distinguish and annotate
units in dialogue act segmentation and classification. Chen et al. [6]
had an empirical investigation of sparse log-linear models for improved dialogue act classification. Milajevs et al. [31] investigate a
series of compositional distributional semantic models to dialogue
act classification.
Regarding the DAR as a sequence labeling problem. Stolcke et al. [38] treat the discourse structure of a conversation as a
hidden Markov model and the individual dialogue acts as observations emanating from the model states. Tavafi et al. [40] study
the effectiveness of supervised learning algorithms SVM-HMM for
DA modeling across a comprehensive set of conversations. Similar
to the SVM-HMM, Surendran et al. [39] also use a combination
of linear support vector machines and hidden markov models for
dialog act tagging in the HCRC MapTask corpus. Boyer et al. [5]
also applied HMM to discover internal dialogue strategies inherent
in the structure of the sequenced dialogue acts. Galley et al. [11]
use skip-chain conditional random field to model non-local pragmatic dependencies between paired utterances. Zimmermann et
al. [53] investigate the use of conditional random fields for joint
segmentation and classification of dialog acts exploiting both word
and prosodic features.
Recently, approaches based on deep learning methods improved
many state-of-the-art techniques in NLP including DAR accuracy
on open-domain conversations [19] [51] [17] [24] [26]. Kumar et
al. [24] build a hierarchical encoder with CRF to learn multiple levels
of utterance and act dependencies. Khanpour et al. [20] design a
deep neural network model that benefits from pre-trained word
embeddings combined with a variation of the RNN structure for the
DA classification task. Ji et al. [17] also investigated the performance
of using standard RNN and CNN on DA classification and got the
cutting edge results on the MRDA corpus using CNN. Lee et al. [26]
proposes a model based on CNNs and RNNs that incorporates
preceding short texts as context to classify current DAs. Zhou
et al. [51] combine heterogeneous information with conditional
random fields for Chinese dialogue act recognition.
Unlike the previous studies, we formulate the problem from the
viewpoint of integrating contextual dependencies in both utterance level and the act label level. We not only consider the fine
grained multi-level semantic representations, but also integrate
the structured attention network to further capture the structure
designpendencies in the CRF layer.

5

CONCLUSION

In this paper, we formulate the problem of dialogue act recognition
from the viewpoint of capturing hierarchical rich utterance representations and generalize richer CRF attentive graphical structural
dependencies without abandoning end-to-end training. We propose
the CRF-Attentive Structured Network (CRF-ASN) for the problem.
We implement the model in two steps. We first encode the rich
semantic representation on the utterance level by incorporating
hierarchical granularity and memory enhanced inference mechanism. The learned utterance representation can capture long term
dependencies across the conversation. We next adopt the internal
structured attention network to compute the dialogue act influence and specify structural dependencies in a soft manner. This
approach enable the soft-selection attention on the structural CRF
dependencies and take account of the contextual influence on the
nearing utterances. We demonstrate the efficacy of our method
using the well-known public datasets SwDA and MRDA. The extensive experiments demonstrate that our model can achieve better
performance than several state-of-the-art solutions to the problem.

6

ACKNOWLEDGE

This work was supported in part by the National Nature Science
Foundation of China (Grant Nos: 61751307 and Nos: 61602405), in
part by the grant ZJU Research 083650 of the ZJUI Research Program
from Zhejiang University and in part by the National Youth Topnotch Talent Support Program. The experiments are supported by
Chengwei Yao in the Experiment Center of the College of Computer
Science and Technology, Zhejiang University.

233

Session 2D: Conversational Systems

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

REFERENCES

[28] Fei Liu, Timothy Baldwin, and Trevor Cohn. 2017. Capturing Long-range Contextual Dependencies with Memory-enhanced Conditional Random Fields. (2017).
[29] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. [n. d.]. Effective
Approaches to Attention-based Neural Machine Translation. ([n. d.]).
[30] Xuezhe Ma and Eduard Hovy. [n. d.]. End-to-end Sequence Labeling via Bidirectional LSTM-CNNs-CRF. ([n. d.]).
[31] Dmitrijs Milajevs and Matthew Purver. 2014. Investigating the contribution of
distributional semantic information for dialogue act classification. In Proceedings
of the 2nd Workshop on Continuous Vector Space Models and their Compositionality
(CVSC). 40–47.
[32] Boyuan Pan, Hao Li, Zhou Zhao, Bin Cao, Deng Cai, and Xiaofei He. 2017.
MEMEN: Multi-layer Embedding with Memory Networks for Machine Comprehension. (2017).
[33] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove:
Global vectors for word representation. In Proceedings of the 2014 conference on
empirical methods in natural language processing (EMNLP). 1532–1543.
[34] Norbert Reithinger and Martin Klesen. 1997. Dialogue act classification using
language models.. In EuroSpeech.
[35] Alexander M Rush, SEAS Harvard, Sumit Chopra, and Jason Weston. [n. d.]. A
Neural Attention Model for Sentence Summarization. ([n. d.]).
[36] Riccardo Serafin, Barbara Di Eugenio, and Michael Glass. 2003. Latent Semantic
Analysis for dialogue act classification. In Proceedings of the 2003 Conference
of the North American Chapter of the Association for Computational Linguistics
on Human Language Technology: companion volume of the Proceedings of HLTNAACL 2003–short papers-Volume 2. Association for Computational Linguistics,
94–96.
[37] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from
overfitting. Journal of machine learning research 15, 1 (2014), 1929–1958.
[38] Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates,
Daniel Jurafsky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykema, and Marie
Meteer. 2006. Dialogue act modeling for automatic tagging and recognition of
conversational speech. Dialogue 26, 3 (2006).
[39] Dinoj Surendran and Gina-Anne Levow. 2006. Dialog act tagging with support
vector machines and hidden Markov models.. In Interspeech.
[40] Maryam Tavafi, Yashar Mehdad, Shafiq Joty, Giuseppe Carenini, and Raymond
Ng. [n. d.]. Dialogue Act Recognition in Synchronous and Asynchronous Conversations. ([n. d.]).
[41] A. Viterbi. 1967. Error bounds for convolutional codes and an asymptotically
optimum decoding algorithm. IEEE Trans.informat.theory 13, 2 (1967), 260–269.
[42] Linlin Wang, Zhu Cao, Gerard de Melo, and Zhiyuan Liu. 2016. Relation Classification via Multi-Level Attention CNNs.. In ACL (1).
[43] Nick Webb, Mark Hepple, and Yorick Wilks. 2005. Dialogue act classification
based on intra-utterance features. In Proceedings of the AAAI Workshop on Spoken
Language Understanding, Vol. 4. 5.
[44] Hongyang Xue, Zhou Zhao, and Deng Cai. 2017. Unifying the Video and Question
Attentions for Open-Ended Video Question Answering. IEEE Transactions on
Image Processing 26 (2017), 5656–5666.
[45] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J Smola, and Eduard H Hovy. 2016. Hierarchical Attention Networks for Document Classification..
In HLT-NAACL. 1480–1489.
[46] Matthew D Zeiler. [n. d.]. ADADELTA: AN ADAPTIVE LEARNING RATE
METHOD. ([n. d.]).
[47] Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi. 2017. Learning Discourse-level
Diversity for Neural Dialog Models using Conditional Variational Autoencoders.
In ACL.
[48] Zhou Zhao, Hanqing Lu, Vincent Wenchen Zheng, Deng Cai, Xiaofei He, and
Yueting Zhuang. 2017. Community-Based Question Answering via Asymmetric
Multi-Faceted Ranking Network Learning. In AAAI.
[49] Zhou Zhao, Qifan Yang, Hanqing Lu, Tim Weninger, Deng Cai, Xiaofei He, and
Yueting Zhuang. 2018. Social-Aware Movie Recommendation via Multimodal
Network Learning. IEEE Transactions on Multimedia 20 (2018), 430–440.
[50] Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen Li, Hongwei Hao, and Bo
Xu. 2016. Attention-based bidirectional long short-term memory networks for
relation classification. In Proceedings of the 54th Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Papers), Vol. 2. 207–212.
[51] Yucan Zhou, Qinghua Hu, Jie Liu, and Yuan Jia. 2015. Combining heterogeneous
deep neural networks with conditional random fields for Chinese dialogue act
recognition. Neurocomputing 168 (2015), 408–417.
[52] Yu Zhu, Hao Li, Yikang Liao, Beidou Wang, Ziyu Guan, Haifeng Liu, and Deng
Cai. 2017. What to Do Next: Modeling User Behaviors by Time-LSTM. In IJCAI.
[53] Matthias Zimmermann. 2009. Joint segmentation and classification of dialog acts
using conditional random fields. In Tenth Annual Conference of the International
Speech Communication Association.

[1] Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A convolutional attention network for extreme summarization of source code. In International
Conference on Machine Learning. 2091–2100.
[2] Jeremy Ang, Yang Liu, and Elizabeth Shriberg. 2005. Automatic Dialog Act
Segmentation and Classification in Multiparty Meetings. In ICASSP.
[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural Machine
Translation by Jointly Learning to Align and Translate. Computer Science (2014).
[4] Phil Blunsom and Nal Kalchbrenner. 2013. Recurrent Convolutional Neural
Networks for Discourse Compositionality. In Proceedings of the 2013 Workshop
on Continuous Vector Space Models and their Compositionality. Proceedings of the
2013 Workshop on Continuous Vector Space Models and their Compositionality.
[5] Kristy Elizabeth Boyer, Eunyoung Ha, Michael D Wallis, Robert Phillips, Mladen A
Vouk, and James C Lester. 2009. Discovering Tutorial Dialogue Strategies with
Hidden Markov Models.. In AIED. 141–148.
[6] Yun-Nung Chen, William Yang Wang, and Alexander I Rudnicky. 2013. An
empirical investigation of sparse log-linear models for improved dialogue act
classification. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE
International Conference on. IEEE, 8317–8321.
[7] Zheqian Chen, Rongqin Yang, Bin Cao, Zhou Zhao, Deng Cai, and Xiaofei He.
2017. Smarnet: Teaching Machines to Read and Comprehend Like Human. (2017).
[8] Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the Properties of Neural Machine Translation: Encoder–Decoder
Approaches. Syntax, Semantics and Structure in Statistical Translation (2014), 103.
[9] Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William W Cohen, and Ruslan
Salakhutdinov. [n. d.]. Gated-Attention Readers for Text Comprehension. ([n.
d.]).
[10] Orhan Firat, Kyunghyun Cho, and Yoshua Bengio. 2016. Multi-Way, Multilingual
Neural Machine Translation with a Shared Attention Mechanism. In Proceedings
of NAACL-HLT. 866–875.
[11] Michel Galley. 2006. A skip-chain conditional random field for ranking meeting
utterances by importance. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,
364–372.
[12] Jeroen Geertzen, Volha Petukhova, and Harry Bunt. 2007. A multidimensional
approach to utterance segmentation and dialogue act classification. In Proceedings
of the 8th SIGdial Workshop on Discourse and Dialogue, Antwerp. 140–149.
[13] Sergio Grau, Emilio Sanchis, Maria Jose Castro, and David Vilar. 2004. Dialogue act classification using a Bayesian approach. In 9th Conference Speech and
Computer.
[14] Ryuichiro Higashinaka, Kenji Imamura, Toyomi Meguro, Chiaki Miyazaki, Nozomi Kobayashi, Hiroaki Sugiyama, Toru Hirano, Toshiro Makino, and Yoshihiro
Matsuo. 2014. Towards an open-domain conversational system fully based on
natural language processing.. In COLING. 928–939.
[15] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural
computation 9, 8 (1997), 1735–1780.
[16] Zhiheng Huang, Wei Xu, and Kai Yu. [n. d.]. Bidirectional LSTM-CRF Models for
Sequence Tagging. ([n. d.]).
[17] Yangfeng Ji, Gholamreza Haffari, and Jacob Eisenstein. 2016. A Latent Variable
Recurrent Neural Network for Discourse-Driven Language Models. In HLTNAACL.
[18] Yangfeng Ji, Gholamreza Haffari, and Jacob Eisenstein. 2016. A Latent Variable Recurrent Neural Network for Discourse Relation Language Models. In Proceedings
of NAACL-HLT. 332–342.
[19] Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent Convolutional Neural
Networks for Discourse Compositionality. ACL 2013 (2013), 119.
[20] Hamed Khanpour, Nishitha Guntakandla, and Rodney Nielsen. 2016. Dialogue
Act Classification in Domain-Independent Conversations Using a Deep Recurrent
Neural Network. In COLING.
[21] Yoon Kim, Carl Denton, and Luong Hoang Alexander M Rush. [n. d.]. Structured
Attention Networks. ([n. d.]).
[22] Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. 2016. CharacterAware Neural Language Models.. In AAAI. 2741–2749.
[23] Harshit Kumar, Arvind Agarwal, Riddhiman Dasgupta, Sachindra Joshi, and
Arun Kumar. 2017. Dialogue Act Sequence Labeling using Hierarchical encoder
with CRF. (2017).
[24] Harshit Kumar, Arvind Agarwal, Riddhiman Dasgupta, Sachindra Joshi, and
Arun Kumar. 2017. Dialogue Act Sequence Labeling using Hierarchical encoder
with CRF. CoRR abs/1709.04250 (2017).
[25] Ji Young Lee and Franck Dernoncourt. 2016. Sequential Short-Text Classification
with Recurrent and Convolutional Neural Networks. In HLT-NAACL.
[26] Ji Young Lee and Franck Dernoncourt. 2016. Sequential Short-Text Classification
with Recurrent and Convolutional Neural Networks. In Proceedings of NAACLHLT. 515–520.
[27] Piroska Lendvai and Jeroen Geertzen. 2007. Token-based chunking of turninternal dialogue act sequences. In Proceedings of the 8th SIGDIAL Workshop on
Discourse and Dialogue. 174–181.

234

