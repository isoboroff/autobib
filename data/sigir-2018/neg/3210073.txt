Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Comparing Two Binned Probability Distributions
for Information Access Evaluation
Tetsuya Sakai
Waseda University, Tokyo, Japan
tetsuyasakai@acm.org

ABSTRACT

rather than a point estimate. In fact, even in traditional information
retrieval, Maddalena et al. [6] have proposed to compute a distribution of nDCG scores instead of a point estimate, by preserving and
utilising the different relevance assessments from different users.
The present study concerns tasks such as the Dialogue Breakdown
Detection Challenge [2] and the NTCIR-14 STC-3 Dialogue Quality
Subtask [8], where both the ground truth data and the system’s
output are represented as a distribution of users’ opinions over bins
on a non-nominal scale. We first point out that popular bin-by-bin
measures such as Jensen-Shannon divergence and Sum of Squared
Errors [5] are clearly not adequate for such tasks, and that cross-bin
measures [7] should be used. While bin-by-bin measures simply
accumulate the errors in each bin, cross-bin measures utilise the
notion of the distance between bins. Through experiments using
artificial distributions as well as real ones from a dialogue evaluation task, we demonstrate that two cross-bin measures, namely,
the Normalised Match Distance (NMD) and the Root Symmetric Normalised Order-aware Divergence (RSNOD), are indeed substantially
different from the bin-by-bin measures. Furthermore, RSNOD lies
between the popular bin-by-bin measures and NMD in terms of
how it behaves. As these two cross-bin measures behave somewhat
differently, we recommend using both of them in evaluation tasks
such as the ones mentioned above.

Some modern information access tasks such as natural language
dialogue tasks are difficult to evaluate, for often there is no such
thing as the ground truth: different users may have different opinions about the system’s output. A few task designs for dialogue
evaluation have been implemented and/or proposed recently, where
both the ground truth data and the system’s output are represented
as a distribution of users’ votes over bins on a non-nominal scale.
The present study first points out that popular bin-by-bin measures
such as Jensen-Shannon divergence and Sum of Squared Errors are
clearly not adequate for such tasks, and that cross-bin measures
should be used. Through experiments using artificial distributions
as well as real ones from a dialogue evaluation task, we demonstrate that two cross-bin measures, namely, the Normalised Match
Distance (NMD; a special case of the Earth Mover’s Distance) and
the Root Symmetric Normalised Order-aware Divergence (RSNOD),
are indeed substantially different from the bin-by-bin measures.
Furthermore, RSNOD lies between the popular bin-by-bin measures
and NMD in terms of how it behaves. We recommend using both
of these measures in the aforementioned type of evaluation tasks.

KEYWORDS
dialogue evaluation; earth mover’s distance; evaluation measures;
order-aware divergence; Wasserstein distance

2

ACM Reference Format:
Tetsuya Sakai. 2018. Comparing Two Binned Probability Distributions for
Information Access Evaluation. In SIGIR ’18: The 41st International ACM
SIGIR Conference on Research and Development in Information Retrieval,
July 8–12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages.
https://doi.org/10.1145/3209978.3210073

1

PRIOR ART

In the Dialogue Breakdown Detection Challenge (DBDC) [2], participating systems are given human-machine dialogues as input.
Then, for each machine utterance, the systems are required to output an estimated probability distribution over three bins, namely,
B (breakdown—which means that the system utterance is too inappropriate for the user to continue a meaningful conversation), PB
(possible breakdown), and NB (not a breakdown). Note that the bins
are ordinal, not nominal: for example, B is clearly “closer” to PB than
to NB. Thirty assessors were hired to build the gold distribution
over the three categories. DBDC currently uses, along with point
estimate measures, Jensen-Shannon divergence and Mean Squared
Error as the official evaluation measures. However, these bin-bybin measures cannot handle non-nominal bins properly. Perhaps
to compensate for this, DBDC also computes the above measures
after collapsing the three bins into two (e.g., by merging B and
PB). However, a more elegant solution would be to utilise cross-bin
measures [7], as we shall clarify in Section 3.
In the NTCIR-14 STC-3 Dialogue Quality subtask (STC-3 DQ),
participating systems will be given a human-human, customerhelpdesk dialogue as input [8]; the systems are required to estimate
the distribution of the assessors’ ratings on that dialogue, in terms
of customer satisfaction, task accomplishment, and so on [8], given
a set of possible ratings (e.g., customer satisfaction on a 7-point

INTRODUCTION

Information access tasks have diversified; some of these tasks lack
groud truth data. For example, if the system is required to estimate the user satisfaction score given a human-machine dialogue,
it would be difficult to devise one ground truth score, for different
people would rate the same dialogue differently. Moreover, if the
system is aware of the above fact, it would be more useful if the
system could output an estimated distribution of the users’ ratings
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210073

1073

Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

scale). STC-3 DQ plans to build a gold distribution for each dialogue
in a way similar to DBDC.
Sakai [8] observed that the probability bins for the STC-3 DQ
are similar to those of DBDC in that they are non-nominal, and
that neither Jensen-Shannon divergence nor Mean Squared Error is
adequate for comparing the system and gold distributions. Hence,
he proposed a cross-bin measure called the Symmetric Normalised
Order-aware Divergence (SNOD) for the DQ subtask. However, he
did not evaluate any of these measures. Moreover, he did not discuss
a simple but important cross-bin measure called Match Distance [9],
an instance of the Earth Mover’s Distance, a.k.a. Wasserstein or
Mallows Distance, which has proven to be highly useful in the
context of computer vision and machine learning applications [4, 9].

3

MEASURES
Figure 1: Examples to show the limitations of bin-by-bin
measures (for L = 3).

We now formalise some measures for comparing two binned probability distributions, and explain why bin-by-bin measures are not
adequate for our purpose. Let A denote a given set of non-nominal
bins, e.g., A = {B, PB, NB} for DBDC, and let L = |A|. Let p(i)
(i = 1, . . . , L) denote the system’s estimated probability for Bin i,
Í
so that i ∈A p(i) = 1. Similarly, let p ∗ (i) denote the corresponding
Í
true probability, where i ∈A p ∗ (i) = 1. We also use the following
vector notations: p = (p(1), . . . , p(L)), p ∗ = (p ∗ (1), . . . , p ∗ (L)).

3.1

where the Kullback-Leibler divergence (KLD) for a pair of probability distributions p1 , p2 is given by:
Õ
p1 (i)
.
(6)
KLD(p1 ||p2 ) =
p1 (i) log2
p2 (i)
i s.t. p1 (i)>0

Note that JSD is symmetric (while KLD is not), but that it is not
normalised. For the above example, the JSD for X is 0.0390 while
that for Y is 0.0490: again, X is considered better.
Figure 1 provides some examples (with L = 3 for simplicity)
where all of the aforementioned bin-by-bin measures “fail.” The
bars shown in blue represent the gold probability distribution, and
the red lines represent the estimated distribution; however, since
the present study discusses symmetric measures only, the gold and
system distributions may be swapped. It can be observed that, while
Case (a) is better than Case (b) in that the estimated bin is closer to
the true bin, none of the above measures can reflect this difference;
similarly, while Case (c) is better than Case (d) in that it is less
skewed towards the left, none of the above measures can reflect
this difference either. It is clear that this problem arises from the
simple summation across the bins: there is no notion of distance
between bins.

Bin-by-Bin Measures

Variational Distance (VD) [5], or Sum of Absolute Errors, forms the
basis of the popular Mean Absolute Error:
Õ
VD(p, p ∗ ) =
|p(i) − p ∗ (i)| .
(1)
i ∈A

However, measures based on VD are not good for our purpose.
Consider a case with L = 5, where p ∗ = (0.2, 0.2, 0.2, 0.2, 0.2).
It is clear that System X that returns p = (0.3, 0.3, 0.2, 0.1, 0.1)
and Y that returns p = (0.3, 0.3, 0.2, 0.1, 0.1) will receive the same
score (VD = 0.4), even though X , which returned a relatively flat
distribution, should be considered better [8]. In this study, we shall
use a normalised version of VD to ensure the [0, 1] range:
VD(p, p ∗ )
.
(2)
2
Sum of Squares (SS), which forms the basis of Root Mean Squared
Error, can handle this particular problem of VD:
Õ
SS(p, p ∗ ) =
(p(i) − p ∗ (i))2 .
(3)
NVD(p, p ∗ ) =

3.2

There is an existing measure that can look across bins, called Match
Distance (MD) [9]. This is a special case of Earth Mover’s Distance
where the probabilities add up to one and the number of bins are
Í
Í
a given [7]. Let cp(i) = ik =1 p(k), and cp∗ (i) = ik =1 p ∗ (k). MD
is just the sum of absolute errors computed from the cumulative
probability distributions:
Õ
MD(p, p ∗ ) =
|cp(i) − cp∗ (i)| .
(7)

i ∈A

Let us consider a root normalised version:
r
SS(p, p ∗ )
∗
RNSS(p, p ) =
.
(4)
2
For the above example, the RNSS for X is 0.1414, while that for Y
is 0.1732: X is considered better. Chai and Draxler [1] discuss the
advantages of Root Mean Squared Error (which is similar to RNSS)
over Mean Absolute Error (which is similar to NVD).
Jensen-Shannon Divergence (JSD) is also free from the above
problem of VD. Let p M (i) = (p(i) + p ∗ (i))/2 for i = 1, . . . , L. JSD is
defined as [5]:
JSD(p, p ∗ ) =

KLD(p||p M ) + KLD(p ∗ ||p M )
,
2

Cross-Bin Measures

i ∈A

In the present study, we consider the following normalised version:
MD(p, p ∗ )
.
(8)
L−1
To see why L − 1 is an appropriate normalisation factor, the reader
should consider the worst case shown in Figure 1(b). Figure 1 shows
that NMD correctly prefers Case (a) over (b), and prefers (c) over
(d), despite its simplicity.
NMD(p, p ∗ ) =

(5)

1074

Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: Ranking pairs of distributions: Kendall’s τ with
95%CIs.

Sakai [8] proposed a measure that considers the distance between
a pair of bins more explicitly than NMD does. First, a distanceweighted sum of squares (DW) is defined for each bin:
Õ
DW (i) =
|i − j |(p(j) − p ∗ (j))2 .
(9)

L = 3 bins (60 distributions; 1,770 pairs of distributions)
RNSS
JSD
NMD
RSNOD
NVD
.947
.939
.773
.864
[.942, .951] [.932, .947] [.759, .786] [.855, .874]
RNSS
.899
.753
.859
[.892, .907] [.740, .767] [.850, .869]
JSD
.734
.873
[.719, .750] [.864, .882]
NMD
.819
[.807, .831]
L = 7 bins (140 distributions; 9,730 pairs of distributions)
RNSS
JSD
NMD
RSNOD
NVD
.845
.888
.593
.803
[.842, .849] [.885, .890] [.585, .601] [.798, .808]
RNSS
.808
.602
.815
[.804, .813] [.594, .610] [.810, .820]
JSD
.568
.801
[.559, .576] [.797, .806]
NMD
.643
[.635, .650]

j ∈A

B∗

Let
=
> 0}, that is, the set of bins where the gold
probabilities are positive. Order-Aware Divergence (OD) is the DW
averaged over these non-empty gold bins:
1 Õ
DW (i) .
(10)
OD(p||p ∗ ) = ∗
|B |
∗
{i |p ∗ (i)

i ∈B

Similarly, let B = {i |p(i) > 0}. Just as the symmetric JSD is obtained
from KLD, Symmetric OD can be defined by swapping the system
and gold distributions:
OD(p||p ∗ ) + OD(p ∗ ||p)
.
(11)
2
In this study, we define the Root Symmetric Normalised OD:
r
SOD(p, p ∗ )
∗
.
(12)
RSNOD(p, p ) =
L−1
While Sakai did not take the square root, we prefer compensate for
the fact that DW is a form of sum of squares: taking the square root
boosts the absolute scores and stretches the lower end of the scale
in the [0, 1] range. From Figure 1, it is clear that RSNOD rates (a)
higher than (b), and (c) higher than (d), just like NMD does.
SOD(p, p ∗ ) =

4

20 that cover only one bin. Hence, for L = 3, we created 20 ∗ 3 = 60
different distributions in total, which gives us 60 ∗ 59 = 1, 770 pairs
of distributions to compute the measures and thereby rank the pairs.
Similarly, for L = 5, we created 20 ∗ 5 = 100 distributions (4,950
pairs); for L = 7, we created 20 ∗ 7 = 140 distributions (9,730 pairs).
Due to lack of space, however, we will not discuss L = 5 any further.
Table 1 shows the kendall’s τ values with 95% confidence intervals: they represent the similarity between each pair of measures
in terms of how they rank the different pairs of artificial distributions. The absolute τ values are not so meaningful here, since our
randomly generated distributions are probably not representative
of real gold and system distributions; what matters is the relative
behaviour of τ across different pairs of measures. A trend that is
consistent across the different L’s is that the τ ’s concerning the
three bin-by-bin measures and RSNOD (e.g., .859-.947 for L = 3)
are higher than the τ between NMD and RSNOD (e.g., .819 for
L = 3), which in turn is higher than the τ ’s between NMD and binby-bin measures (e.g., .734-.773 for L = 3). That is, NMD behaves
substantially differently from the bin-by-bin measures, and RSNOD
lies somewhere in between NMD and the bin-by-bin measures (and
closer to the latter) in terms of how it ranks different pairs of binned
distributions.

COMPARING THE MEASURES

This section compares the aforementioned bin-by-bin measures,
namely, NVD, RNSS, and JSD, and the cross-bin measures, namely,
NMD and RSNOD. While the advantage of NMD and RSNOD over
the bin-by-bin measures for the purpose of comparing two probability distributions over non-nominal bins is already clear, we seek
to quantify the relationships among the five measures. Section 4.1
reports on an experiment with artificial distributions. Using artificial distributions does offer an advantage: if we just use real gold
and system distributions from a real task, we cannot consider any
other forms of distributions not covered by that particular task. In
experiments with artifical distributions, we can consider a wide
variety of distributions and their combinations. To complement the
above, Section 4.2 reports on an experiment with real distributions
from the latest DBDC task [2].

4.1

Artificial Distributions

To quantify the differences across the five measures, we can compute the measures for various pairs of probability distributions,
rank the pairs by each measure, and compute Kendall’s τ . A high
τ implies that the two measures behave similarly. To this end, we
generated artificial probability distributions by random selection
from a given set of bins, assuming that we have 30 votes (i.e., assessors) as in DBDC. We considered L = 3, 5, 7: for example, L = 3
corresponds to the situation with DBDC where we have B, PB, and
NB as the bins [2]; L = 7 is applicable if the assessors are asked for
(say) a dialogue satisfaction rating on a 7-point scale. We ensured
that we have some distributions with zero-probability bins: for example, for L = 3, we randomly created 20 distributions that cover
all three bins, and another 20 that cover only two bins, and another

4.2

Real Distributions

We also utilised real data from the latest DBDC task [2]: the task
provides a gold distribution over L = 3 bins (NB, PB, B) for a total
of 2,000 system utterances. In addition, we obtained two runs from
a top performing team in the English subtask [3]; hereafter, the
team’s least and most effective runs according to the official results
are referred to as System 1 and System 2, respectively. Thus, we
have 2,000 estimated distributions from each of these systems.
Table 2(I) shows the Kendall’s τ results in a way similar to Table 1:
these new results are based on ranking the 2,000 utterances by each
measure, computed by comparing System 2’s distribution with the

1075

Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

can be observed that the trend is similar to the results with τ ’s: the
bin-by-bin measures and RSNOD are similar to each other (95.398.2% agreement); NMD and RSNOD are less similar (90.8%); and
NMD is even less similar to the bin-by-bin measures (89.4-89.9%).
Of the 2000 − 1816 = 184 disagreeements between NMD and
RSNOD, the delta for NMD was zero (i.e., Systems 1 and 2 were
considered equivalent) for eight utterances, and hence 177 were
“true” disagreements. Figure 2 visualises these instances: it can be
observed that although NMD and RSNOD disagreed in terms of the
sign of the delta, the magnitute of the delta is less than 0.08 in every
case. Thus, these are not serious disagreements. Figure 3 zooms in
on an utterance indicated in Figure 2: for this utterance, RSNOD
says that System 1 is better, while NMD says that System 2 is better.
Even for us humans, it may be difficult to say which is correct; it
may not matter. Hence, our recommendation for evaluation tasks
based on comparing binned distributions would be to use both
NMD and RSNOD, but not the bin-by-bin measures.
Finally, we conducted a randomisation test for the difference
between Systems 1 and 2 for every measure. All five measures
agree that System 2 is better, and the p-values for NVD, RNSS, JSD,
NMD, RSNOD are 0.182, 0.140, 0.139, 0.149, 0.143. Hence NMD and
RSNOD appear to be similar in terms of discriminative power as
well, although a more extensive investigation is required.

Table 2: Comparing the measures with the 2,000 utterances
from the DBDC3 data.
(I) Kendall’s τ with 95%CIs (System 2)
RNSS
JSD
NMD
NVD
.960
.900
.799
[.959, .962] [.892, .908] [.790, .807]
RNSS
.912
.795
[.904, .920] [.786, .804]
JSD
.792
[.781, .802]
NMD
-

RSNOD
.946
[.943, .949]
.960
[.958, .962]
.915
[.907, .922]
.832
[.825, .840]
(II) Preference agreement for comparing Systems 1 and 2
RNSS
JSD
NMD
RSNOD
NVD
96.1%
95.3%
89.4%
95.5%
RNSS
97.8%
89.4%
98.2%
JSD
89.9%
98.2%
NMD
90.8%

5

CONCLUSIONS AND FUTURE WORK

We clarified the advantages of cross-bin measures over the popular
bin-by-bin measures for the purpose of comparing binned distributions in an evaluation task. Our future work includes further
investigation into the differences between NMD and RSNOD in
terms of discriminative power and agreement with human perception. The data used in our experiments are publicly available for
replication by other researchers1 .
Figure 2: 177 true disagreements (out of 2,000 utterances) between NMD and RSNOD regarding Systems 1 and 2.

REFERENCES
[1] T. Chai and R.R. Draxler. 2014. Root Mean Square Error (RMSE) or Mean Absolute
Error (MAE)? – Arguments against avoiding RMSE in the Literature. Geoscientific
Model Development 7 (2014), 1247–1250.
[2] Ryuichiro Higashinaka, Kotaro Funakoshi, Michimasa Inaba, Yuiko Tsunomori,
Tetsuro Takahashi, and Nobuhiro Kaji. 2017. Overview of Dialogue Breakdown
Detection Challenge 3. In Proceedings of Dialog System Technology Challenge 6
(DSTC6) Workshop.
[3] Sosuke Kato and Tetsuya Sakai. 2017. RSL17BD at DBDC3: Computing Utterance
Similarities based on Term Frequency and Word Embedding Vectors. In Proceedings of DSTC6. http://workshop.colips.org/dstc6/papers/track3_paper13_kato.pdf
[4] Elizaveta Levina and Peter Bickel. 2001. The Earth Mover’s Distance is the
Mallows Distance: Some Insights from Statistics. In Proceedings of ICCV 2001.
251–256.
[5] Jianhua Lin. 1991. Divergence Measures Based on the Shannon Entropy. IEEE
Transactions on Information Theory 37, 1 (1991), 145–151.
[6] Eddy Maddalena, Kevin Roitero, Gianluca Demartini, and Stefano Mizzaro. 2017.
Considering Assessor Agreement in IR Evaluation. In Proceedings of ACM ICTIR
2017. 75–82.
[7] Yossi Rubner, Carlo Tomasi, and Leonidas J. Guibas. 2000. The Earth Mover’s
Distance as a Metric for Image Retrieval. International Journal of Computer Vision
40, 2 (2000), 99–121.
[8] Tetsuya Sakai. 2017. Towards Automatic Evaluation of Multi-Turn Dialogues: A
Task Design that Leverages Inherently Subjective Annotations. In Proceedings of
EVIA 2017. 24–30.
[9] Michael Werman, Shmuel Peleg, and Azriel Rosenfeld. 1985. A Distance Metric for
Multidimensional Histograms. Computer Vision, Graphics, and Image Processing
32 (1985), 328–336.

Figure 3: DBDC probability distributions of Gold, System 1,
and System 2 for Utterance CIC0259_12.
gold one. (Similar results with System 1 are omitted due to lack of
space.) It is clear that even with real distributions from a dialogue
task, the aforementioned observation (Section 4.1) still holds.
Table 2(II) compares the five measures in a different way. For each
of the 2,000 utterances, the score delta between Systems 1 and 2 is
computed for each measure; then we count the preference agreement
between two measures. For example, if both NMD and RSNOD agree
that System 1’s distribution for a particular utterance is better, that
is an agreement. Between these two cross-bin measures, there were
1,816 agreements (1816/2000 = 90.8% as shown in Table 2(II)). It

1 http://waseda.box.com/sigir2018short

1076

