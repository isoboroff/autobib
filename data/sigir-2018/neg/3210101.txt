Short Research Papers I

SIGIR‚Äô18, July 8-12, 2018, Ann Arbor, MI, USA

Parameterizing Kterm Hashing
Dominik Wurzer

Yumeng Qin‚àó

School of Information Management
Wuhan University
dominik@wurzer.com

School of Information Management
Wuhan University
yumeng.qin@whu.edu.cn

ABSTRACT

the degree of novelty. We believe that uniform kterm weights are
sub-optimal for tasks like FSD, as kterms like {the, is, 23} would
carry the same weight as the kterm {downtown, earthquake, LA}.
By intuition, the latter one appears to be more helpful to discover
new events in data streams. We propose to abandon the principle
of uniform kterm importance and instead place weights on kterms
to boost detection effectiveness. Learning weights for all kterms
is impractical because their number is high1 . Instead of directly
learning weights for each kterm, we learn weights for surrogate
clusters. These clusters group kterms based on common characteristics, which allows associating a kterm‚Äôs importance with the
weight corresponding to its nearest cluster. Our experiments in
Section 4 show that parameterized Kterm Hashing can significantly
outperform uniformly weighted Kterm Hashing for FSD.

Kterm Hashing provides an innovative approach to novelty detection on massive data streams. Previous research focused on maximizing the efficiency of Kterm Hashing and succeeded in scaling
First Story Detection to Twitter-size data stream without sacrificing detection accuracy. In this paper, we focus on improving the
effectiveness of Kterm Hashing. Traditionally, all kterms are considered as equally important when calculating a document‚Äôs degree of
novelty with respect to the past. We believe that certain kterms are
more important than others and hypothesize that uniform kterm
weights are sub-optimal for determining novelty in data streams.
To validate our hypothesis, we parameterize Kterm Hashing by
assigning weights to kterms based on their characteristics. Our
experiments apply Kterm Hashing in a First Story Detection setting and reveal that parameterized Kterm Hashing can surpass
state-of-the-art detection accuracy and significantly outperform
the uniformly weighted approach.

1.1

KEYWORDS
Kterm Hashing; Novelty Detection; First Story Detection
ACM Reference Format:
Dominik Wurzer and Yumeng Qin. 2018. Parameterizing Kterm Hashing .
In SIGIR ‚Äô18: The 41st International ACM SIGIR Conference on Research and
Development in Information Retrieval, July 8‚Äì12, 2018, Ann Arbor, MI, USA.
ACM, New York, NY, USA, ?? pages. https://doi.org/10.1145/3209978.3210101

1

Related Work

First Story Detection (FSD) describes the research task of monitoring a stream of documents with the intent of identifying those
documents that speak about previously unknown events first [4].
FSD systems detect new events using a fixed thresholding strategy.
This requires the computation of a novelty score for each document.
Documents whose novelty score exceeds the detection threshold
are considered to speak about new events [3]. The traditional approach to FSD [1,2,3,15] calculates the novelty of a document by
its distance to its nearest neighbour. This is known to be the most
effective approach in FSD [3] but also among the slowest, as the
time complexity depends on the number of comparisons made.

INTRODUCTION
Kterm Hashing [18] offers a new approach for novelty computation
without ‚Äúdocument-level‚Äù comparisons. Instead, Kterm Hashing
constructs a single representation of the past - the memory - and
compares new documents to it. This provides a single point of comparison which results in a higher efficiency than document-level
comparison strategies [14]. Kterms are compound terms of length
k, based on all terms that appear in a document. By kterm length
(k), we refer to the number of compound terms. Upon arrival of
a new document from the stream, Kterm Hashing exhaustively
forms all kterms up to length k. Novelty is computed by the ratio of unseen kterms with respect to the memory and the number
of kterms formed. Newly encountered kterms are subsequently
made persistent in the memory for future calculations. The single
point of comparison shifts the time complexity from the number
of encountered documents to the number of kterms per document,
i.e. the binominal coefficient of document and kterm length. The
original publication [18] determines the membership of kterms in
the memory by hashing them onto a fixed sized Bloom Filter [5]

Kterm Hashing [18] provides an innovative approach to novelty
detection. When applied to streaming tasks, like First Story Detection (FSD), it exceeds the efficiency of state-of-the-art algorithms
by several orders of magnitude, without sacrificing effectiveness.
Kterm Hashing forms compound terms, called kterms, from all
unique terms in a document. The document‚Äôs degree of novelty
is computed by the number of unseen kterms in proportion to
the document length. When Kterm Hashing was first introduced,
all kterms were considered as equally important for quantifying
‚àó Corresponding

Author

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ‚Äô18, July 8‚Äì12, 2018, Ann Arbor, MI, USA
¬© 2018 Copyright held by the owner/author(s). Publication rights licensed to the
Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210101

1 binominal

945

coefficient of document length and kterm length

Short Research Papers I

SIGIR‚Äô18, July 8-12, 2018, Ann Arbor, MI, USA

to ensure fast look-ups in constant time and space [14]. They report FSD accuracy on par with UMass [3] and LSH-FSD [15], while
processing data streams up to 2 orders of magnitude faster. Our
experiments reveal that detection effectiveness can be increased by
distinguishing between different levels of kterm importance.

of kterm categories and how we associate them with the documents
in the training set. Each kterm category is a cluster represented by
a centroid vector. In Line 1, we initialize the centroid matrix (CENTROID) to hold random values for each of the 60 kterm features.
We then construct a dense kterm matrix (KTERM) by exhaustively
forming kterms form all documents in the training set and extract
the kterm feature values for each of them (Line 2). Lines 3 to 8 apply
K-mean clustering [22] to group the kterms into 120 categories. This
requires computing the similarities (SIMS 1 ) of the kterm vectors
with all centroid vectors using the dot product between their matrices. Kterms are assigned to their nearest cluster by identifying the
highest similarity value in each matrix column (Line 5). This turns
the similarity matrix (SIMS 1 ) into a sparse matrix that associates
kterms with their corresponding category (cluster). The clusters
are then re-computed by updating their centroid vector based on
the average of the kterm vectors assigned to it (Line 6 - 7). K-mean
clustering ensures that kterms with similar feature characteristics
are likely to end up in the same cluster. We determined the number
of iterations and categories empirically and found that fewer categories reduce the detection accuracy, while more categories only
result in marginally better accuracy. The training set for the subsequent learning procedure is formed by the dot product between the
document matrix and SIMS 1 , the matrix associating kterms with
their corresponding kterm category (Line 9).

Our approach of computing a document‚Äôs novelty based on the
novelty of weighted terms is distantly related to the concept applied by the IDF-FSD system [10]. Their novelty estimation relies
on the sum of term frequencies and inverse document frequencies
with respect to previously encountered documents. Both features
are commonly used by various Information Retrieval applications.
In addition to FSD, Kterm Hashing was also applied to Rumour
Detection [13] to significantly improve detection accuracy.

2

METHOD

Kterm Hashing [18] estimates novelty based on the fraction of unseen kterms in a document. Instead of considering all kterms as
equal, we want to distinguish them based on their importance for
the novelty computation.
The number of kterms spawned by a document depends on the
document length and kterm length. We choose kterms of length 1
to 3, which were found to perform best for FSD [18]. A document
with 10 unique terms2 spawns 175 unique kterms for length 1 to
3. Heaps Law [21] states that the vocabulary size grows without
bound. Since kterms are formed by exhaustively compounding
terms, their number grows faster than the collection vocabulary.
The streaming nature of FSD renders individual weights for kterms
infeasible. To mitigate this problem, we shift from individual kterm
weights to weights for kterm categories.

Algorithm 1 : kterm categories
Input:
DOCS [ documentID √ó ktermID ]
KTERM [ ktermID √ó featureID]
CENTROID [centroidID √ó featureID]
SIMS [centroidID √ó ktermID]
Output:
TRAININGSET [documentID √ó centroidID]

kterm feature
inverse document
frequency
term frequency
document frequency
entity
part of speech
spelling
numbers

Description
idf of kterm
components
tf of kterm components
df of kterm components
4 different entities
4 different POS tags
ratio of correctly spelled words
presence and frequency
of numbers
twitter specific
hashtags and usernames
kterm length
number of compound terms
Table 1: Each kterm feature category provides several features based on the number of occurrences, sum, min, max
and average of feature values

2.1

1:
2:
3:
4:
5:
6:
7:
8:
9:

2.2

Parameterizing Kterm Hashing

The previous section grouped kterms into categories and assigned
the documents in the training set to them. Before learning optimal
kterm category weights, we divide the training set into 2 classes,
‚Äúfirst stories‚Äù and ‚Äúfollow-ups‚Äù, and apply feature scaling to ensure
all feature values are within the same range. The kterm category
weights are optimized by a Support Vector Machine (SVM) classifier [23], which we found to perform best. The SVM uses a radial
2
basis function kernel (e (‚àíŒ≥ ‚àó |u‚àív | ) ) with gamma (Œ≥ ) of 0.15 and a
convergence tolerance of 0.1. Note that FSD data sets are highly

Forming kterm Categories

When detecting new events, we weigh each kterm based on the
weight of its category. These categories enclose kterms that share
similar characteristics. Table 1, lists 9 feature categories of which
we form 60 kterm features. Algorithm 1 describes the construction
2 average

CENTROID ‚Üê random(kterm features)
KTERM ‚Üê kterm features (DOCS)
for iteration in {1...100} do
SIMS1 ‚Üê CENTROID ‚Ä¢ KTERMT
SIMS1 ‚Üê colmax(SIMS1 )
SIMS2 ‚Üê uni f orm(SIMS1 )
CENTROID ‚Üê SIMS2 ‚Ä¢ KTERM
end
TRAININGSET ‚Üê DOCS ‚Ä¢ SIMST1

length of a tweet

946

Short Research Papers I

SIGIR‚Äô18, July 8-12, 2018, Ann Arbor, MI, USA

3.2

imbalanced because every first story is associated with several
follow-ups. Our experiment section (Section 4) compares strategies
to counteract this imbalance.
N (dn ) =

√ï
kt ‚ààd n

Œ± kt



 

|dn | ‚àí1 1 : kt<Mn‚àí1
0 : kt ‚ààMn‚àí1
|kt |

(1)

At run-time, we incorporate our kterm category weight (Œ± kt ) into
the equation of Kterm Hashing (Equation 1). The novelty of document dn , is based on the kterm category weights (Œ± kt ) of its kterms
(kt ‚àà dn ) if they are new with respect to the memory of the past
Mn‚àí1 .

3

3.3

Improving the accuracy of Kterm Hashing
for FSD

Table 2 compares the effectiveness of parameterized Kterm Hashing
- dubbed pkterm hashing - with traditional Kterm Hashing on the
‚ÄúCross-Twitter‚Äù data set, which was also used by the original paper
[18]. Following the original publication, we determine the kterm
length parameter for traditional Kterm Hashing using grid search.
For this experiment all systems make use of the 500 topics from
the ‚ÄúLarge-scale Twitter Corpus‚Äù as training data for parameter
optimization. In contrast to our expectations, Table 2 shows only a
marginally better detection cost for pkterm hashing. Deeper analysis of the training procedure revealed that the potential of pkterm
hashing is limited by the class imbalance of the training data.

EXPERIMENTS

In a streaming setting, like FSD, documents arrive on a continual
basis one at a time [19]. We require our approach to compute a
novelty score for each document in a single-pass over the data.
To evaluate the accuracy of our parameterized version of Kterm
Hashing, we compare it to the traditional approach on two massive
Twitter FSD data sets.

3.1

Evaluation Metric

As is common in Topic Detection and Tracking [4], we evaluate
FSD accuracy by the normalized Topic Weighted Minimum Detection Cost, dubbed Cmin . The detection cost Cmin provides a linear
combination of miss and false alarm probabilities, which allows
comparing different methods based on a single value metric [2]. We
make use of the standard TDT evaluation procedure [4] with the
official TDT3 evaluation scripts using the standard settings.

Data Set

The first data set, ‚ÄúCross-Twitter‚Äù3 , was also used in the original
Kterm Hashing paper [18]. It consists of 27 topics and 115,000 tweets
ordered by their publication time-stamp. Additionally, we use the
500 topics and 150,000 tweets of the ‚ÄúLarge-scale Twitter Corpus‚Äù
[11]. Both data sets are frequently used to evaluate the performance
of FSD systems [9, 11, 12, 15, 18, 19].

Counteracting class imbalance by class weights:
In FSD, each first story (detection target) is usually followed by
several follow-ups, creating a class imbalance for the training data
of the learning algorithm. Classical sampling methods [8] harm
the detection accuracy because the imbalance exceeds 1:1,000. We
address this imbalance by placing a class weight (Equation 2) on
the training set [7]. This increases the importance of the ‚Äúdetection
target‚Äù class when learning kterm category weights using a Support Vector Machine. Table 2 shows that class weights successfully
improve the detection accuracy of parameterized Kterm Hashing
by 1.3% in comparison with traditional uniformly weighted kterms.

3 The

Cross Project is a joint venture between the University of Edinburgh and the
University of Glasgow, http://demeter.inf.ed.ac.uk/cross/

Kterm Hashing Cmin Difference (%)
traditional
0.8021
pkterm hashing
0.7996
-0.31%
pkterm hashing +
class weight
0.7896
-1.31%
pkterm hashing +
class weight +
0.7822
-2.48%
skip evaluation
Table 2: Effectiveness of different variants of Kterm Hashing
on the ‚ÄúCross-Twitter‚Äù data set.

Œ¥cl assA (

#instances in class A
‚àó 0.3)
#instances in all classes

(2)

Increasing effectiveness by Skip Evaluation:
Machine learning algorithms tend to produce better results when
exposed to more training data [17]. Skip Evaluation is a frequently
applied methods in the Topic Detection and Tracking (TDT) program [4] to increase the number of topics in a data set without
annotating new topics [1, 20]. Skip Evaluation iterates a certain
number of passes over each topic. At each pass, the first story
(detection target) of each topic is removed (skipped), making the
first follow up document the new detection target. This doubles
the number of detection targets and reduces the number of followups by one per pass. We limited Skip Evaluation to 10 rounds on
the training set to prevent small scale topics from vanishing. Skip
Evaluation by itself does not resolve the class-imbalance of the
training set, which requires additional class weights. Note that Skip
Evaluation is only carried out on the training set to determine optimal parameter weights and not on the test set for Kterm Hashing.
Table 2, shows that parameterized Kterm Hashing benefits from
the increased training data size, as it outperforms the traditional
approach by 2.48%. Although the improvement might appear minor,

Kterm Hashing
Cmin
Difference (%)
traditional
0.7721
pkterm hashing
0.7689
-0.31%
pkterm hashing +
class weight
0.7456
-3.43%
pkterm hashing +
class weight +
0.7378*
-4.44%
skip evaluation
Table 3: Effectiveness of different variants of Kterm Hashing on the ‚ÄúLarge-scale Twitter Corpus‚Äù data set. Asterisk (*)
indicates statistically significant differences (p < 0.05).

947

Short Research Papers I

SIGIR‚Äô18, July 8-12, 2018, Ann Arbor, MI, USA

feature:
idf
tf
df
entity
POS spelling Twitter length
relative impact
on detection
-4.15% -2.69% -3.72% -5.23% -3.54%
-4.22%
-2.55%
-4.92%
cost (Cmin ):
Table 4: Features ablation: impact on performance when removing a feature group.
idf : inverse document frequency; tf : term frequency; df : document frequency; POS: part of speech;

we want to point out that detection accuracy surpasses the reported
effectiveness [18] of the UMass FSD system [3], which is considered
to be the state-of-the-art in terms of detection accuracy [14].

can significantly outperform the traditional approach for FSD. We
also demonstrated that parameterized Kterm Hashing in conjunction with class weights and sufficient training data, can outperform
state-of-the-art FSD systems in terms of detection accuracy.

We repeated the experiments on the ‚ÄúLarge-scale Twitter Corpus‚Äù,
as seen in Table 3. Unfortunately, ‚ÄúCross-Twitter‚Äù provides insufficient training examples (27 topics) to serve as a training set. Therefore, we randomly split the 500 topics of the ‚ÄúLarge-scale Twitter
Corpus‚Äù to create a training and test set with 250 topics each. Table
3 confirms the findings of increased detection accuracy of parameterized Kterm Hashing in conjunction with class weights and skip
evaluation. Following the higher number of topics, the difference
in detection cost reaches statistical significance (p < 0.05).

3.4

REFERENCES
[1] Allan J., Carbonell J., Doddington G., Yamron J., and Yang Y. Topic Detection and
Tracking Pilot Study Final Report. In Proceedings of the DARPA Broadcast News
Transcription and Understanding Workshop, 1998.
[2] Allan J., Lavrenko V. and Jin H. First story detection in TDT is hard. In Proceedings
of the ninth international conference on Information and knowledge management.
ACM, 2000
[3] Allan J., Lavrenko V., Malin D., and Swan R. Detections, bounds, and timelines:
UMass and TDT-3. In Proceedings of TDT Workshop. 2000
[4] James Allan. Topic detection and tracking: event-based information organization.
Kluwer Academic Publishers. 2002
[5] Bloom B. H. (1970). Space/time trade-offs in hash coding with allowable errors.
Communications of the ACM
[6] Cataldi M., Caro L. D., and Schifanella C. (2010). Emerging topic detection on
Twitter based on temporal and social terms evaluation. In Proceedings of the 10th
International Workshop on Multimedia Data Mining. ACM.
[7] C. Elkan,‚ÄúThe Foundations of Cost-Sensitive Learning‚Äù Proc. In‚Äôl Joint Conf. Artificial Intelligence, 2001.
[8] He H, Garcia E A. Learning from Imbalanced Data[J]. IEEE Transactions on
Knowledge and Data Engineering, 2009
[9] Guille A, Favre C. Event detection, tracking, and visualization in Twitter: a mentionanomaly-based approach[J]. SIGIR, 2015.
[10] Margarita Karkali Francois Rousseau Alexandros Ntoulas Michalis Vazirgiannis.
Efficient Online Novelty Detection in News Streams. Web Information Systems
Engineering - WISE 2013
[11] McMinn A J, Jose J M. Real-Time Entity-Based Event Detection for Twitter[C].
cross language evaluation forum, 2015: 65-77.
[12] Moran S., McCreadie R., Macdonald C. and Ounis I. Enhancing First Story Detection using Word Embeddings. Proceedings of the 39th International Conference
on Research and Development in Information Retrieval. SIGIR 2016.
[13] Yumeng Qin, Dominik Wurzer and Cunchen Tang. ‚ÄúPredicting future rumours‚Äù,
Chinese Journal of Electronics. 2018.
[14] Yumeng Qin, Dominik Wurzer, Victor Lavrenko and Cunchen Tang. ‚ÄúCounteracting Novelty Decay in First Story Detection.‚Äù In ECIR - European Conference
on Information Retrieval. AI.NI Springer. (2017)
[15] Sasa Petrovic. Real-time Event Detection in Massive Streams. PhD Thesis, 2012
[16] Phuvipadawat S. & Murata T. Breaking news detection and tracking in Twitter.
In Proceedings of IEEE on WI and IAT, 2010.
[17] Tsang I W, Kwok J T, Cheung P, et al. Core Vector Machines: Fast SVM Training
on Very Large Data Sets[J]. Journal of Machine Learning Research, 2005
[18] Dominik Wurzer, Victor Lavrenko, Miles Osborne. Twitter-scale New Event
Detection via K-term Hashing. In the Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP, 2015
[19] Dominik Wurzer, Victor Lavrenko, Miles Osborne. Tracking unbounded Topic
Streams. In the Proceedings of the 53rd Annual Meeting of the Association for
Computational Linguistics, ACL, 2015b
[20] Yang Y., Pierce T. and Carbonell J. A study of retrospective and on-line event
detection. In Proceedings of the 21st annual international conference on Research
and development in information retrieval. SIGIR, 1998
[21] Leo Egghe. Untangling Herdan‚Äôs law and Heaps‚Äôlaw: Mathematical and informetric arguments. Journal of the American Society for Information Science and
Technology (2007)
[22] MacQueen J. B. Some Methods for classification and Analysis of Multivariate
Observations. Proceedings of 5th Berkeley Symposium on Mathematical Statistics
and Probability. 1. University of California Press. (1967)
[23] B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training algorithm for optimal margin
classifiers. In D. Haussle; Proceedings of the Annual Conference on Computational
Learning Theory, pages 144-152, Pittsburgh, PA, July 1992. ACM.

Feature Analysis

The previous experiments confirmed that uniform kterm weights
are sub-optimal for FSD. Next, we analyze the impact of certain
kterm features on the detection accuracy of parameterized Kterm
Hashing. Kterms are weighed based on the weight of their category.
When analyzing features, we focused on the kterm features that
determine the kterm category, instead of analyzing the category
weight itself. To analyze a feature‚Äôs impact on the detection cost,
we apply feature ablation. Feature ablation measures the relative
change in detection cost when applying all but one feature [13].
In our case, feature ablation measures the impact on Cmin by excluding feature when forming kterm categories, as seen in Table 4.
The table reveals that features based on entities, inverse document
frequency and Part of Speech are particularly useful kterm features.
To our surprise, Twitter specific features, like hashtags, appear to
have a minor impact on detection cost. We further investigated
the impact of hashtags on Kterm Hashing by removing them from
the training and test set and measure a 3.68% relative reduction
in detection cost. This is interesting as several approaches [6,16]
for First Story Detection rely on hashtags. Manually inspection of
annotated topics revealed that the majority (> 60%) of hashtags
does not occur in the first story, but in the follow-ups and 6 out of
27 Cross-Twitter topics don‚Äôt contain any hashtags. Since hashtags
are often previously unseen terms, they spawn a high number of unseen kterms, which increase the novelty of followups and decreases
detection accuracy.

4

CONCLUSION

Traditional Kterm Hashing considers all kterms as equally important when calculating novelty on data streams. We showed that
uniform kterm weights are sub-optimal for FSD on two separate
data sets. Instead of placing individual weights on kterms, we group
them into categories and learn optimal weight settings for them.
Our experiments demonstrated how parameterized Kterm Hashing

948

