Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Quantitative Information Extraction From Social Data
Omar Alonso

Thibault Sellam

Microsoft
omalonso@microsoft.com

Columbia University
tsellam@cs.columbia.edu
which numbers are relevant to a wider audience. How much quantitative information is available on a social network like Twitter?
Can we extract useful numerical data that is relevant? We perform
experiments to quantify numerical data in a number of topics and
analyze the findings in the context of the event at hand.
In this paper we propose the problem of annotating a topic with
quantities and how such quantities can be extracted. As part of our
research, we use the notion of a quantfrag, that is, a small portion
of text that contains quantitative data.
Information extraction (IE) is the task of automatically extracting structured information as records from documents in different
domains [1]. We are interested in detecting relevant quantitative
information, that is, numbers and their associated context. Previous
work have focused on extracting dates and named entities, building knowledge bases [5], and analyzing disasters through social
media [4]. Augmenting tables with semantic information about
quantities is presented in [3]. Quantitative data has received limited attention in information retrieval and even in NLP, there is
little published research on quantities in language understanding.
Recent work by Roy et al. [6] describe steps for reasoning about
quantities and present the novel task of quantity entailment.

ABSTRACT
Social data is a rich data source for identifying trends and topics of
interest based on user activity. Social data also provides opportunities to collect numerical data about events like elections, sport
games, disasters or economic news. We propose the problem of
identifying relevant quantitative information from social data as
annotations for a topic. We investigate how to extract quantitative
information and perform a number of experiments and analysis
with Twitter data.
ACM Reference format:
Omar Alonso and Thibault Sellam. 2018. Quantitative Information Extraction From Social Data . In Proceedings of The 41st International ACM SIGIR
Conference on Research and Development in Information Retrieval, Ann Arbor,
MI, USA, July 8–12, 2018 (SIGIR ’18), 4 pages.
https://doi.org/10.1145/3209978.3210133

1

INTRODUCTION

Quantitative information can be very useful for augmenting the
summary of an event or for adding more context to a topic in the
form of annotations. For example, the outcome of the 2016 US
elections can be described with two numbers: 304 and 227, which
represent the number of electoral votes for Trump and Clinton. Two
more numbers for the same event are also relevant: 66M and 63M,
in this case the number of popular votes (in millions) for Clinton
and Trump respectively. While the overall topic is the surprise
victory of the Republican candidate, news articles and web pages
describing the event contain such numbers to quantify the result.
We are interested in recognizing fragments of text that report
“measures”, extracting the corresponding quantity, and capturing
a description of what is being measured. These annotations can
then be used for a wide range of applications like search, question
answering, passage retrieval, unsupervised learning, or information visualization. We believe that extracting relevant quantitative
information from social data can be a beneficial annotation to a
topic and useful in search scenarios where numerical data can help
with context, summaries, or query suggestions.
Identifying numbers in text is somewhat straightforward but
extracting relevant quantities is a challenging task. The noisy characteristics of social data makes this task complex but, at the same
time, there is potential to utilize human sensing at scale to signal

2

DETECTING QUANTITATIVE
INFORMATION

To understand the problem space, we look at how quantitative
information is reported in a news article compared to Twitter using
Greece’s financial crisis as example.
An article 1 from a reputable source describes the event using
different numerical data in several paragraphs. A couple of examples
are: As voters trudged off to polling stations, many cash
machines ... Athens had run out of money, even though
depositors are only allowed to withdraw e60 ($66) a
day and Last month Greece defaulted on a e1.6 billion
pile of loans to the IMF. In Twitter, due to space limitations,
numerical data is usually condensed like the following post from
@sbancel on June 28, 2015: Wow! #Greece. ATM withdrawals
limited to 60 euros per day. #Banks closed for at least
a week.
In a typical newswire article, the author has room for expanding
on details about the story so there is the possibility of reporting
more than one figure. Because of the nature of microblogging, a
tweet post is likely to contain the quantity that the user feels is
more relevant. If quantitative information is available on a social
network, how reliable and significant it is for the topic or event? We
hypothesize that the difference between quantitative information
reported in an article by an editor or journalist from regular users
is the notion of literacy. That is, users may feel more comfortable

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210133

1 https://www.economist.com/news/europe/21657003-greek-voters-\

have-rejected-austerity-eu-may-think-they-have-rejected-europe-no-what

1005

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

posting about numbers that they can interpret and communicate
in a discussion.
In our work, the unit of information is the quantfrag, a small piece
of text that mentions one or more quantitative properties of an event.
For the news article, we are interested in the fragment Last month
Greece defaulted on a e1.6 billion pile of loans to
the IMF, that mentions a quantity (1.6 billion e) and a description
(Greece’s default to the IMF). In the tweet example, the quantfrag is
ATM withdrawals limited to 60 euros per day, that contains
just the text necessary to understand the amount of the quantity
(e60) and the property being measured (the daily withdrawal limit
for cash dispensers). A few more quantfrag examples from the
article published in the Economist:
• on July 20th Athens is due to repay e3.5 billion
on a maturing bond issue to the ECB
• depositors are only allowed to withdraw e60 a day
• with more than 95% of the vote counted, "No" was
ahead by 61.3%

specific data sets. We use the extractor presented in [7], which
consists of an event configuration, that describes which hashtags
to query for a given window of time. To avoid robots and other
spammy accounts, we collect tweets from trusted users only [2].
We handpicked topics that cover a few categories; some of them
are unique and others are recurring. For each topic, we provide
the time interval that covers the event the best possible way. For
recurring events that span a few hours (e.g., State of the Union
Address, Superbowl, etc.) we include tweets from the following day
to make sure we cover more data. For each topic, we use a single
hashtag in the extraction. While there are other hashtags that are
also used for the topic during the same time period, we believe that
the data collected for such event is a representative sample. Table 1
shows a summary of the topics covered.
The column “filtered tweets” contains the number of tweets after
duplicate and near-duplicate removal from the set of trusted users.
The column “quantitative extracted” represents the raw number
of tweets that our extractor detected as numeric information and
“quantfrag extracted” means the raw number of quantfrags detected.
By looking at percentages we can observe that most of the topics
usually contain less than 10% of quantfrags with the exception of
natural disasters (hurricanes and fires) or an unexpected tragedy
(Las Vegas shooting) that tend to be higher but still less than 20%.
The topics that belong to politics do contain more quantfrag information, which is somewhat expected as political information
does reference numbers related to the economy. Entertainment and
sport tend to be at the lower end of the spectrum.
The next step in our analysis is to rank the quantfrags extracted
using two different strategies. The first one is to rank the quantfrag
by number of retweets (RTs) associated to the tweet that the quantfrag belongs to. In the second case, we rank by term frequency (TF),
where “term” is basically the quantity. The rationale is as follows.
RTs give us a lot of behavioral signal from Twitter users so we can
think of it as the “literacy” rank, that is, quantitative information
that users can understand with little context. The TF rank identifies
the most frequent quantfrag, the most repeated quantity.
Table 2 presents the top-10 quantfrags ranked by RTs and TF
for the US Elections. Due to space limitations some of the text
(e.g., hashtags) have been omitted. We can observe that the left
column tends to have richer numerical information that includes
some comparisons. In both rankers, we can see how the event is
unfolding: RTs tend to give the big picture while TF is more specific
about certain items. TF is calling the right numbers but because
many states carry the same number of electoral votes, the grouping
is ambiguous and needs the state information for a good separation
(e.g., 6 electoral votes in Nevada or Iowa?). Both rankers have the
magical number 270 on positions 1 and 2 and both also identify the
reference to the Simpsons.
How useful are the quantfrags and how can they be evaluated?
There is no equivalent to the web archive in Twitter and moments do
not go back in time, making the evaluation challenging. Wikipedia
usually have an associated article with quite a bit of content making
it an unfair comparison. Still, we can measure how precise our
literacy ranker is using precision at rank k, where P@k = #(relevant
items at k)/k, using Wikipedia as a ground truth, which is very
strict as articles are constantly being updated. We mark the entry
as relevant if the topic or quantfrag has a mention (sentence) in

As voters trudged off to polling stations, many cash machines
in central Athens had run out of money,
even though depositors are only allowed to withdraw 60 Euros
NNS

VBP

NP
S

RB

VBN

TO

VB

NNS

NP
ADVP
VP

CD

VP
VP

S

VP

Figure 1: Example of parse tree on which we apply rules to
detect quantfrags. In this case, we seek the second highest
clause (S) which contains a cardinal number (CD).
As we can observe, extracting only numbers is not sufficient
as we need the full fragment to understand what the quantity describes, which is different from traditional NLP quantity extractors.
The challenge is to extract a fragment long enough that contains
the description of the measure and also adequate so no additional
information is included.
One approach to build a quantfrag detector is to use rules, based
on language grammar for extracting clauses which contain cardinal
numbers, as illustrated in Figure 1. We use an internal Part-OfSpeech tagger trained on Twitter data only as a building block.
In practice, many other challenges arise. For instance, some
quantities are implicit or hidden in a temporal expression (e.g., “a
week long bank holiday”) and others are valid, but are not relevant
to the main event (e.g., “Mr Tsipras says he had two priorities”).
Our focus is not on the best extractor but the type and quality
of quantfrags that we can collect and how they can be used as
annotations to enrich a corpus. Extracting quantities from text is
only a part of the problem and many challenges arise from curating,
mining, and exploiting those pieces of information.

3

EXPERIMENTS AND RESULTS

We conduct experiments to measure how much quantitative information is available in Twitter for an event or topic. We have access
to the Twitter firehose, which allow us to go back in time and select

1006

Short Research Papers I

Topic

Hashtag

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Time period

#filtered
#quantitative #quantfrag %quantitative %quantfrag
tweets
extracted
extracted
extracted
extracted
Tax reform
taxreform
2-Nov–23-Dec 2017
3244
270
173
8%
5%
US Elections
electionnight
8–9 Nov 2016
12601
2023
1136
16%
9%
State of the Union
sotu
28–29 Jan 2014
3551
460
275
13%
8%
sotu
20–21 Jan 2015
5815
838
541
14%
9%
sotu
12–13 Jan 2016
7520
738
436
10%
6%
sotu
30–31 Jan 2018
7302
694
402
10%
6%
Tubbs fire
napafire
8–31 Oct 2017
337
68
44
20%
13%
sonomafires
8–31 Oct 2017
96
18
18
19%
19%
Hurricane Irma
irma
30-Aug–30-Sep 2017
19482
4210
2384
22%
12%
Hurricane Harvey
harvey
17-Aug–2-Sep 2017
17274
3567
2152
21%
12%
Las Vegas shooting lasvegas
1–2 Oct 2017
3429
836
491
24%
14%
The Force Awakens starwars
10–21 Dec 2015
8400
925
482
11%
6%
Rogue One
starwars
14–25 Dec 2016
1683
131
70
8%
4%
The Last Jedi
starwars
9–20 Dec 2017
3392
350
173
10%
5%
Academy Awards
oscars
26–27 Feb 2016
22661
2167
930
10%
4%
oscars
28–29 Feb 2017
25794
2844
1143
11%
4%
NFL championship superbowl
4–5 Feb 2018
10220
1181
597
12%
6%
superbowl
5–6 Feb 2017
13327
2488
929
19%
7%
superbowl
7–8 Feb 2017
5207
1338
389
26%
7%
Table 1: Quantitative and quantfrag extraction from Twitter for a set of topics expressed by hashtags for a specific time interval. The topics are grouped by the following categories: politics (Tax reform, Elections, SOTU), natural disasters (Tubbs
fire, Hurricanes Irma and Harvey) and unexpected events (Las Vegas shooting), Entertainment (Oscars, StarWars), and Sports
(Superbowl).

Rank by RT
Donald Trump edges nearer to the magical target of 270
#ExitPoll: Top candidate quality: Can Bring Change - Trump wins
82%
Electoral vote count - Trump leads Clinton 216 to 202
Electoral vote count - Trump leads Clinton 139 to 97

Rank by TF
Trump will win Iowa’s 6 electoral votes; Clinton will win Nevada’s
6 electoral votes
Donald Trump edges nearer to the magical target of 270
US Presidential Election Results 2016
Trump will win Wisconsin’s 10 electoral votes; Clinton secures 10
electoral votes with Maryland
Trump will win Alaska’s 3 electoral votes
The Simpsons predict President Trump 16 years ago
Trump will win Florida’s 29 electoral votes

Florida results with 91% in - Trump leads Clinton 48.9% to 48.1%
National popular vote - Trump leads Clinton 49% to 46.6%
This is how the political map of the US is looking after 42 of 50
states
National popular vote - with 20% in, Trump leads Clinton 50.9% to Trump wins North Carolina’s 15 electoral votes
45.4%
Reminder that The Simpsons predicted this 16 years ago
Trump wins Alabama’s 9 electoral votes
Donald Trump wins the 2016 election
Hillary Clinton wins Virginia’s 13 electoral votes
Table 2: Top-10 quantfrags ranked by retweets (RTs) and term frequency (TF). Quantitative information is presented in bold.

the Wikipedia article2 . The use of “or” is because the event is still
unfolding so percentages and partial counts are not final yet. With
the exception of the Simpsons’ reference and exit polls, all entries
are relevant giving a P@10 = 0.8. Table 3 present evaluation results
for the other topics.

When performing the evaluation, we analyze the quantfrags
that we marked as not relevant and conclude that they are all relevant within the event but much less afterwards. For example, in
the case of natural disasters or other emergencies, phone numbers score high (People in distress from Harvey can call
the following numbers 281-464-4851...; Those looking for
friends & loved ones following the shooting should call
1-866-535-5654). Quantfrags that belong to jokes are also present

2 https://en.wikipedia.org/wiki/United_States_presidential_election,_2016

1007

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Topic
Tax reform

P@5 sample of quantfrags
0.8 corporate tax rate will be 21%; single mother a 70% tax cut
Tubbs fire
0.8 killed at least 11 people and destroyed 1,500 homes; 30–50 mph
winds expected till midday
Harvey
0.8 category 4 hurricane with maximum sustained winds of 130 mph;
once in 500 year flood
Irma
0.8 category 5 hurricane with 175 mph
winds; three hurricanes simultaneously in the Atlantic
Superbowl 2018 0.8 NBC going dark for 30 seconds;
Only two QBs have ever beaten
Tom Brady
SOTU 2018
0.6 allocated $700 billion for military;
PolitiFact 498 times
Oscars 2017
0.8 At age 98 , her story continues to
inspire; Jackie Chan has been in
films since the 1960’s
The Last Jedi
0.6 highest rated at 96% with 83 reviews; $200 million-plus opening
weekend
Las Vegas
0.8 50+ dead , 200+ injured; 14 in critical condition one suspect down
Table 3: Precision evaluation for topics.

Timestamp Property
Quant.
5/5/18
Brady now has 4 300 yard games
300
5/5/18
Eagles win over Patriots 41-33
41-33
5/5/18
Ratings were down 3% from last year 3
Table 4: Superbowl topic annotations based on quantfrags.

1977, the year of the first episode in the series, that is reused across
different movies: 40 years ago in theaters not so far, far
away (2017), StarWars first premiered in 1977 (2016), and
When no theater wanted to show the movie in 1977 (2015).

4

SEARCH AND RETRIEVAL

So far, we have defined quantfrags and presented evidence that
they have the potential to be useful in IR applications. How can
we utilize a collection of quantfrags? Can they be employed for
supporting queries, such as “what are the records from Superbowl
2018”? To answer such questions, we need more data than plain
quantfrags and, instead, use them as annotations for retrieval. We
introduce a couple of annotations to support such queries:
(1) Normalization. Normalization is necessary to allow simple
content-based queries. The idea is to parse the quantfrags,
and extract tuples of the form <timestamp, property,
quantity, unit>. We give a few examples in Table 4.
(2) Context. In some cases, it is necessary to capture the wider
context from which the quantfrag was extracted (e.g., the
full sentence, or paragraph). The wider context helps users
understand the description of the measure, and it provides
flexibility for automated tasks such as search or clustering.

(Jennifer Lawrence is only 23 and she already has 3 Oscar
nominations. When I was 23 I had 3 prison penpals; NBC
going dark for 30 seconds was the best Super Bowl ad so
far). Interestingly enough, the 30 seconds outage is documented
in Wikipedia3 .
Based on this empirical data and analyzing more quantfrags, we
make an attempt to categorize them according to our perceived utility for future applications into three main categories: facts, transient,
and temporal reference. Facts are the numbers that represent the
outcome of an event or hard evidence (e.g., 270 electoral votes, 175
mph, etc.). Transient quantfrags are usually emergency numbers,
text numbers for donations, and numerical data from jokes. Finally,
temporal reference is an interesting case on its own as quantitative
information expressed as a year temporal expression is common
across all topics. We named it temporal reference because the presence of such quantfrag has the explicit goal of making, in most
cases, a reference to the past.
We now examine temporal references in more detail for recurring
events. In the case of SOTU, each speech references different years.
For example: This speech could have been given in 1950 or
1960 (2018), Seven years ago, we made the single biggest
investment in clean energy in our history (2016), Since
2010, America has put more people back to work than
Europe, Japan, and all advanced economies combined (2015),
and Until 1934, the SOTU Address was delivered every
December (2014). For StarWars, there is a common reference to

5

CONCLUSION AND FUTURE WORK

This paper introduced the problem of extracting quantities that can
be later be used as annotations in IR applications. We performed
a number of experiments using the Twitter firehose, analyzed the
data, and presented examples of quantfrags. We shed some light
into the kind of quantitative information that is available in social
network data and provided a simple categorization of such annotations. There is more work to do in terms of quantfrag ranking
and evaluation. Improvements on the quantity extraction using
better machine learning models is part of future work as well as
evaluating the quality of the annotations.

REFERENCES
[1] P. M. Andersen, P. J. Hayes, S. P. Weinstein, A. K. Huettner, L. M. Schmandt, and
I. B. Nirenburg. Automatic Extraction of Facts from Press Releases to Generate
News Stories. In Proc. of ANLP, pages 170–177, 1992.
[2] M. Hentschel, O. Alonso, S. Counts, and V. Kandylas. Finding Users we Trust:
Scaling up Verified Twitter Users Using their Communication Patterns. In Proc. of
ICWSM, 2014.
[3] Y. Ibrahim, M. Riedewald, and G. Weikum. Making Sense of Entities and Quantities
in Web Tables. In Proc. of CIKM, pages 1703–1712, 2016.
[4] M. Imran, C. Castillo, F. Diaz, and S. Vieweg. Processing Social Media Messages
in Mass Emergency: A Survey. ACM Comput. Surv., 47(4):67:1–67:38, 2015.
[5] D. Pighin, M. Cornolti, E. Alfonseca, and K. Filippova. Modelling Events through
Memory-based, Open-IE Patterns for Abstractive Summarization. In Proc. of ACL,
pages 892–901, 2014.
[6] S. Roy, T. Vieira, and D. Roth. Reasoning about Quantities in Natural Language.
TACL, 3:1–13, 2015.
[7] T. Sellam and O. Alonso. Raimond: Quantitative Data Extraction from Twitter to
Describe Events. In Proc. of ICWE, pages 251–268, 2015.

3 https://en.wikipedia.org/wiki/Super_Bowl_LII

1008

Unit
yards
score
%

