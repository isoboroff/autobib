Session 4C: Medical & Legal IR

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Seed-driven Document Ranking for Systematic Reviews
in Evidence-Based Medicine
Grace E. Lee and Aixin Sun
School of Computer Science and Engineering, Nanyang Technological University, Singapore
leee0020@e.ntu.edu.sg;axsun@ntu.edu.sg

ABSTRACT

Digital libraries
Clinical question

Systematic review (SR) in evidence-based medicine is a literature
review which provides a conclusion to a specific clinical question.
To assure credible and reproducible conclusions, SRs are conducted
by well-defined steps. One of the key steps, the screening step, is to
identify relevant documents from a pool of candidate documents.
Typically about 2000 candidate documents will be retrieved from
databases using keyword queries for a SR. From which, about 20
relevant documents are manually identified by SR experts, based
on detailed relevance conditions or eligibility criteria. Recent studies show that document ranking, or screening prioritization, is a
promising way to improve the manual screening process. In this
paper, we propose a seed-driven document ranking (SDR) model for
effective screening, with the assumption that one relevant document is known, i.e., the seed document. Based on a detailed analysis
of characteristics of relevant documents, SDR represents documents
using ‘bag of clinical terms’, rather than the commonly used bag
of words. More importantly, we propose a method to estimate
the importance of the clinical terms based on their distribution in
candidate documents. On benchmark dataset released by CLEF’17
eHealth Task 2, we show that the proposed SDR outperforms stateof-the-art solutions. Interestingly, we also observe that ranking
based on word embedding representation of documents well complements SDR. The best ranking is achieved by combining the
relevances estimated by SDR and by word embedding. Additionally,
we report results of simulating the manual screening process with
SDR.

Relevance conditions

Keyword query retrieval

Candidate documents
Manual screening

Systematic review

Relevant documents

Figure 1: Overview of steps in conducting SRs.

1

INTRODUCTION

Systematic review (SR) in evidence-based medicine is a literature
survey which provides a conclusion or answer to a specific clinical question. An example clinical question1 is: Can laparoscopy (a
diagnostic test) assess the resectability of pancreatic cancer? Studies
reported in research papers relevant to the question are identified
and combined to draw the overall conclusion. Systematic reviews
also show evidence of the derived conclusion and appraisal results
of the relevant literature. For the better understanding of the task,
we describe the steps and challenges in conducting SRs in evidencebased medicine.
Overview of Systematic Review. A SR is a literature survey
which presents the up-to-date conclusion or answer to a clinical
question based on relevant studies. Since the conclusions in SRs are
considered as gold standard, SRs are strictly conducted by following
systematic steps.2
The first step is to formulate a clinical question (or SR topic) that
needs to be answered. To determine a clinical question, SR experts
carefully check clinical literature and existing SRs. At the end of this
step, the specific clinical question is defined and SR experts become
knowing one or two relevant documents for the defined clinical
question. Relevance conditions, namely eligibility criteria in SR
community, are also defined in this step. Relevance conditions are
initially defined by using multiple key clinical terms often following
the PICO model3 . The conditions are then further elaborated from
the terms and finally contain details to assess diverse aspects of
documents.
Once a clinical question and relevance conditions are set, the
next step is to search potential relevant documents from databases
e.g., PubMed, using keyword queries. The aim of the retrieval step is
to collect all candidate documents which are possibly relevant to SR.
To achieve high recall, various combinations of keyword queries
are formulated for retrieval. The list of keyword queries are often
published together with SR to ensure reproducibility. The collection

CCS CONCEPTS
• Information systems → Similarity measures;

KEYWORDS
Document Ranking, Systematic Reviews, Seed Document
ACM Reference Format:
Grace E. Lee and Aixin Sun. 2018. Seed-driven Document Ranking for
Systematic Reviews in Evidence-Based Medicine. In SIGIR ’18: The 41st International ACM SIGIR Conference on Research & Development in Information
Retrieval, July 8–12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA,
10 pages. https://doi.org/10.1145/3209978.3209994
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3209994

1 The

example clinical question is simplified from its original version for readers to
grasp the concept of systematic reviews. The original clinical question is stated in [2].
2 http://training.cochrane.org/
3 P: Patient or Problem, I: Intervention, Prognostic Factor, or Exposure, C: Comparison,
O: Outcome. More details at: http://guides.mclibrary.duke.edu/ebm/pico

455

Session 4C: Medical & Legal IR

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: Percentage of relevant documents in candidate documents for 5 randomly selected SRs in CLEF 17 eHealth Task
2 dataset; and min, max, and median in the dataset.
Sample SRs

#Rel Docs

#Candidate Docs

% of Rel Docs

S R1
S R2
S R3
S R4
S R5

16
48
3
6
46

1,911
10,872
1,573
2,065
5,971

0.83%
0.44%
0.19%
0.29%
0.77%

Min percentage
Max percentage
Median percentage

1
18
15

12,705
114
2,074

0.008%
15.79%
0.72%

Though many approaches have been proposed to improve the
costly screening step, manual screening remains necessary to find
all relevant documents in candidate documents, concluded in a
recent survey on improving screening process using text mining
techniques [19]. Besides, the survey suggests that ranking candidate
documents, or screening prioritization, is safe for use in practice to
assist manual screening.
Screening prioritization aims to rank candidate documents so
that it enables SR experts to screen the relevant documents as early
as possible. With the same goal, a new task named “Technologically Assisted Reviews in Empirical Medicine” was introduced in
CLEF2017 eHealth competition.5 In this study, we use the dataset
released by this task.
Overview of Seed-driven Document Ranking. To define a clinical question and relevance conditions, SR experts carefully go
through literature and existing SRs. It is natural that SR experts
know the existence of one or two relevant documents after defining
the clinical question, i.e., before the screening process. In our proposed solution, we assume that one relevant document is known,
which we call the seed document. Now, the research question becomes: “Given a relevant document as a seed, how to rank the candidate documents such that the (remaining) relevant documents appear
at the top of the ranking?”
At first glance, this is a problem of query by example document.
A common approach is to identify key phrases from the example
document as queries to rank documents. However, the relevance
conditions defined in SR cover diverse aspects of the documents,
and their details e.g., gender/age of subjects in clinical trial and
randomized experiment design. To this end, we conduct a thorough
analysis of relevant documents and irrelevant documents in candidate documents. Based on the analysis we propose to represent
the seed document and candidate documents using ‘bag of clinical
terms’. The clinical terms are identified by using external thesaurus
i.e., UMLS medical thesaurus6 . Note that, this is different from common approaches in query by example where the key phrases are
identified based on statistical measures.
More importantly, we propose a method to estimate the weight of
a term by leveraging high similarities between relevant documents
in SRs. Relevant documents have higher intra-similarities than irrelevant documents, because all the relevant documents must meet
all the relevance conditions. Therefore, we estimate the importance
of a term based on its distribution in the candidate documents and
their similarities to the seed document. In this regard, the proposed
weighting scheme finds relevant documents without spending considerable effort identifying explicit relevance conditions. In SDR, we
adopt and modify the query likelihood retrieval model to estimate
the relevance.
Recently, there are many studies on using distributed representation of words, or word embeddings, for different tasks in Information Retrieval. In our setting, a document can be easily represented
based on the embeddings of its contained words. Using cosine similarity, one can get a ranking towards the seed document. In our
experiments, we evaluate different document representations including bag of clinical terms, bag of words, and word embeddings,

of candidate documents is usually large, with low precision and
high recall. Recently, there are studies aiming to increase precision
while maintaining recall using PICO [20].
The process of identifying relevant documents from candidate
documents is called screening in SR. In this process, a document is
evaluated regarding many aspects defined in the relevance conditions, such as patient information, and experimental design. Each
aspect in the relevance conditions contains elaborated details including medical history, symptoms, and ages of patients, to name a few.
According to the explicit relevance conditions, if a document satisfies all of the conditions, it is a relevant document. Documents with
partial or non satisfaction are labeled as irrelevant documents. The
screening process is performed in two steps: abstract screening and
full text screening. Abstract screening is to judge relevance based
on title and abstract of a candidate document. Full text screening is
then to confirm relevance based on the entire text of the document.
Table 1 lists the percentage of relevant and candidate documents
for five sample SRs taken from the CLEF 17 eHealth Task 2 dataset.4
The minimum, maximum, and median percentage of 50 SRs in the
dataset are also reported. Typically, fewer than 1% of candidate
documents are relevant.
The last step in SR is to assess qualities of relevant documents.
Since it is conducted on only relevant documents, this step is relatively less resource-intensive. Target information such as patient
information, experiment methodologies, and results, are collected
and synthesized by SR experts to reach an overall conclusion of the
clinical question. Improving the process of collecting data in this
step is also an active research area [9, 17, 24].
Challenges in Screening. The screening process is one of the
most expensive steps in conducting SRs. Multiple SR experts need
to manually and thoroughly examine every candidate document
to find all relevant documents. As reported in Table 1, fewer than
one percent of candidate documents are determined as relevant
in most SRs in the CLEF17 dataset. In another study [10, 21], it is
reported that the number of relevant documents accounts for only
1.2 percent of the candidate documents on average in 93 SRs.
4 According to the task overview [10], the candidate documents in CLEF17 eHealth
Task 2 were re-retrieved by the organizers from MEDLINE database, based on the
published keyword queries. The number of relevant documents is slightly smaller
than that were actually used in the SRs since relevant documents (and more candidate
documents) may come from other databases. In this study, we ignore one SR in the
CLEF17 dataset which has zero relevant document.

5 https://sites.google.com/site/clefehealth2017/task-2
6 https://en.wikipedia.org/wiki/Unified_Medical_Language_System

456

Session 4C: Medical & Legal IR

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

and different retrieval models including BM25 and query likelihood
model. Our experimental results show that SDR outperforms all
baseline methods and beat the best performance (without using
relevance feedback) reported in CLEF 2017. More interestingly, we
observe that relevance estimated based on word embeddings well
complements SDR. The best ranking is achieved by combining the
relevances estimated by SDR and by word embeddings. We also
simulate the manual screening process and it demonstrates the efficacy of the proposed approach when multiple relevant documents
are known.

relevance feedback in terms of average precision was achieved by
utilizing keyword queries [1]. In spite of the best result, average
precision is less than 0.2, indicating the difficulty of this task. With
relevance feedback, the best performance was achieved by [6]. The
authors adopted continuous active learning (CAL) in SRs, and CAL
was developed for technologically assisted reviews (TAR) such as
e-discovery in legal area [5, 15]. Their result shows that CAL can
improve the screening process of systematic reviews in medical
domain as well.
Query by Document. Information retrieval with a document as
query (query document) has been studied as general and domainspecific problems. As a general retrieval problem, given web page as
query document, finding similar blog posts or recommending news
articles have been explored [16, 28]. In domain-specific area, patent
search or academic literature search are example tasks [11, 13].
For both problems, existing studies try to reduce terms in query
document by identifying key phrases or concepts, because a query
document is long and contains noise.
In [28], the authors first extract candidate phrases from query
document and then score candidate phrases in multiple ways, e.g.,
using statistical information or using external knowledge base. A
query document is represented with two components in [26]. One is
to capture an overall characteristic of the document and the other is
a representation of key phrases in document. The authors claim that
utilizing both key phrases and a general representation of query
document can improve performance since they complement each
other. Several studies tackled the problem of query document as
query suggestion or query diversification in patent search [11, 12].
Likewise, key concepts in query document are identified while considering different aspects of documents. By suggesting the identified
key concepts and their related concepts as new query candidates,
it allows users to re-issue query keywords.
Different from the aforementioned problems, relevance in systematic reviews depends on relevance conditions and the conditions
explicitly evaluate multiple aspects of documents. The aspects critical to the relevance can be from key phrases to small details in documents. For instance, gender/age of subjects in clinical experiment
and randomized experiment design are often relevance conditions
in SRs. This information are neither frequently appeared terms nor
key phrases in clinical documents. In this work, we propose an
approach to leverage domain-specific characteristics of systematic
reviews to estimate relevance of terms in seed document.

Our Contributions. In summary, we make the following contributions in this work:
• We propose a new approach SDR for screening prioritization,
to assist conducting systematic reviews, with an assumption
that SR experts know one relevant document.
• We conducted a detailed analysis of relevant documents in SRs
and propose to use ‘bag of clinical terms’ to represent documents. Above all, we estimate the importance of the clinical
terms based on their distribution in candidate documents and
the similarities to the seed documents.
• Through extensive experiments, we demonstrate the effectiveness of SDR against baseline methods. We provide a comprehensive analysis of the ranking results and simulate the manual
screening process where more relevant documents are identified through the screening process.

2

RELATED WORK

We present existing approaches for improving the screening process.
Then we briefly overview the studies on query by documents.
Provision of Efficient Screening Process. To assist the manual
screening process for greater efficiency, mainly four approaches
were explored: (i) reducing the number of documents to be manually screened, (ii) reducing the number of SR experts needed for
screening, (iii) increasing the rate of manual screening process, and
(iv) prioritizing documents to be screened [19].
The approaches for reducing the number of documents often
adopt a supervised approach and train a classifier. Classifiers focus on predicting true negative documents, i.e., documents not to
be screened, in this extremely imbalanced setting. An evaluation
measure called Work Saved over Sampling (WSS) was proposed
in [3] for estimating how many documents can be excluded from
the screening process. To train classifiers, many machine learning
algorithms were examined including SVM and LDA [8, 25]. Similarly, the approaches to reduce the number of SR experts develop
classifiers, and each trained classifier is considered as an SR expert.
The third approach aims to provide tools to speed up the manual screening process. For example, visualizing clusters of similar
documents help SR experts screen them fast. The last approach is
screening prioritization, or document ranking. It is reported that
the ranking approach is the one ready to be applied in practice [19].
Recently, CLEF2017 eHealth Task 2 was organized as a new
task with the aim of screening prioritization [10]. In this task, two
types of approaches were examined: (i) document ranking with
no relevance feedback, and (ii) ranking with relevance feedback
such as active learning. The best performance for ranking without

3

SDR: SEED-DRIVEN DOCUMENT RANKING

Given a seed document ds and a set of candidate documents C =
{d 1 , d 2 , . . . , d |C | }, ds < C, our task of seed-driven document ranking
is to rank the documents in C such that the relevant documents are
ranked at top positions. To achieve this, we first conduct an analysis
on the characteristics of relevant documents in SRs, in Section 3.1.
Based on the observations, we present document representation
scheme and term weighting method for ranking in Section 3.2.

3.1

Observations on Relevant Documents

Shown in Table 1, systematic reviews have very skewed data distribution in terms of relevant documents among candidate documents.
More specifically, only about one percent of candidate documents

457

Session 4C: Medical & Legal IR

Intra-similarity

0.4

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Avg of relevant docs
Avg of irrelevant docs

BOW

Irrelevant docs

0.3

Relevant docs

(0.0, 0.2)
[0.6, 0.8)

0.2
0.1

[0.2, 0.4)
[0.8, 1.0]

[0.4, 0.6)

BOC

0.0

0.0

SRs

0.2

0.4

0.6

0.8

1.0

(Number of terms in range)/(Total number of terms)

Figure 2: Intra-similarity among relevant documents and irrelevant documents in SRs.

Figure 3: Distribution of clinical terms (in BOC) and words
(in BOW) over the five ranges of normalized document frequency in relevant documents (best viewed in color)

are relevant. Understanding the characteristics of the relevant documents is therefore the key to fully utilize the seed document and
achieve the best ranking. Through exploration of the data, we make
the following two observations.

they are structured with several key clinical terms following the
PICO model3 as a basis. Relevance conditions are then further elaborated and explicitly specified to cover more details, mostly defined
in clinical terms. As relevant documents must fully satisfy all the
relevance conditions, in terms of clinical terms relevant documents
for a given SR are expected to share high commonality each other.
Without loss of generality, we consider a word or a phrase to
be a clinical term if it matches an entry in the Unified Medical
Language System (UMLS)6 . There exist many off-the-shelf tools
to identify clinical terms in UMLS from medical documents, e.g.,
NCBO BioPortal API7 and QuickUMLS [23].
Now, a relevant document can be represented as a ‘bag-of-clinical
terms (BOC)’. We evaluate the distribution of clinical terms in
relevant documents, using ‘bag-of-words (BOW)’ with stopword
removal as reference. More specifically, we are interested in the
ratio of clinical terms (resp. words in BOW representation) that are
shared among relevant documents. To this end, we compute the
normalized document frequency (DocFreq) of each clinical term
(resp. word), and bin them into five ranges: (0.0, 0.2), [0.2, 0.4), [0.4,
0.6), [0.6, 0.8), [0.8 1.0]. A clinical term has a normalized DocFreq of
1.0 if it appears in every relevant document in a given SR, and 0.8
if it appears in 80% of relevant documents of the SR. Figure 3 plots
the ratio of the clinical terms/words in each of the five ranges.
Observe that the vast majority of words in BOW representation are in the range of (0.0, 0.2) and very few are in the ranges of
[0.6, 0.8) and [0.8, 1.0], if even noticeable. However, represented in
BOC, a larger portion of clinical terms appears in higher DocFreq
ranges. This comparison suggests that, ‘bag-of-clinical terms’ representation is much more effective in finding commonality between
relevant documents in a SR.

Observation 1. For a given SR, its relevant documents share
higher pair-wise similarity than that of irrelevant documents.
In systematic reviews, relevance judgment depends on the relevance conditions defined when formulating the clinical question.
Recall that every SR has different relevance conditions and the
conditions assess multiple aspects of a candidate document to determine its relevance. However, regardless of specific criteria in
relevance conditions, ‘relevance’ carries the universal meaning in
all SRs: if a document meets all relevance conditions, its label is
positive (relevant); if a document satisfies partial conditions or none,
its label is negative (irrelevant).
Positive labels on relevant documents ensure that they fully
satisfy the relevance conditions. Therefore, relevant documents
for a given SR share high commonalities in terms of relevance
conditions, suggesting that they share great similarities among
them. On the other hand, all candidate documents are retrieved
based on keyword queries derived from the clinical question and
relevance conditions. Therefore, every document in the candidate
set is relevant to the relevance conditions to some degree. To verify
to what extent the relevant documents are more similar to each
other, we conduct a simple test.
We calculated the average intra-similarity of relevant documents
for each SR in the CLEF17 dataset. Specifically, we calculated the
pair-wise cosine similarity of relevant documents using tf-idf representation. We under sample irrelevant documents to the same
number of relevant documents at random for each SR, and calculate
their pair-wise similarity. We repeat this process for 10 times to
estimate the intra-similarity of irrelevant documents. Plotted in
Figure 2, on the 50 SRs in CLEF17 dataset, the intra-similarity of
relevant documents and that of irrelevant documents are 0.266 and
0.109, respectively. This result suggests that: (i) relevant documents
are indeed similar with each other, and (ii) it is challenging to fully
rely on cosine similarity with tf-idf representation to identify the
remaining relevant documents with a seed document.

Summary. Through the analysis of candidate documents and relevant documents, we show that the relevant documents share higher
pair-wise similarity and bag-of-clinical terms representation is more
effective in identifying commonality among relevant documents.
Based on the observations, with reference to the seed document, we
design a term weighting scheme to give more weight to the clinical
terms that are likely to be shared among all relevant documents.

Observation 2. Relevant documents for a given SR share high
commonality in terms of clinical terms.
Relevance conditions in SRs are defined in the first step of conducting SRs. As the initial step of defining relevance conditions,

7 https://bioportal.bioontology.org/

458

Session 4C: Medical & Legal IR

3.2

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Weighting Scheme and Document Ranking

When multiple seed documents become available, e.g., during
the actual screening process, ds can be expanded to include additional relevant document ds ′ to form a new query and re-rank
the remaining candidate documents. We also note that the proposed weight scheme and ranking function can be easily adopted
to ‘bag-of-words’ representation by treating each word as a term.

Based on Observation 2, ‘bag-of-clinical terms’ (BOC) representation is more effective in capturing the characteristics of relevant
documents. From now onwards, we use BOC to represent both
seed document and all candidate documents. For simplicity, we use
‘term’ to refer a ‘clinical term’.
Observation 1 states that relevant documents share higher pairwise similarity. The terms that make relevant documents close
to each other and distant from irrelevant documents should be
promoted to have a higher weight. Without knowing all relevant
documents (which is the ultimate goal), we estimate the weight of
a term in seed document by measuring to what extent it separates
the documents similar to the seed document and those dissimilar.
Let ti ∈ ds be a term in seed document ds . Let D ti ⊆ C denote the
subset of candidate documents where ti appears in these documents,
and D ti ⊆ C denote the subset of candidate documents where ti
does not appear. The conditions, D ti ∪ D ti = C and D ti ∩ D ti = ∅,
always hold. The weight of term ti in ds , denoted by φ(ti , ds ), is
estimated by Equation 1.
!
δ (D ti , ds )
φ(ti , ds ) = ln 1 +
(1)
δ (D ti , ds )

4

EXPERIMENT SETUP

We design two sets of experiments to evaluate the performance of
SDR and alternative ranking models.
In the first set of experiments, we assume a single seed document is available for each SR, and compare the quality of candidate
document ranking with different baseline ranking models. Because
using different seed documents from the same SR may deliver different rankings, we average the performances obtained by using each
relevant document as the seed document to derive the final ranking
performance.
In the second set of experiments, we simulate the screening
process conducted by SR experts in an iterative manner. Starting
with the ranking obtained by using a single seed document, we
assume SR experts manually screen a batch of top k documents
to determine their relevance. The relevant documents identified
in the k documents are used to expand the seed document as a
new query and get a new ranking. The process iterates with the
screening of another batch of top-k documents. This simulation
aims to evaluate the effectiveness of SDR with multiple relevant
documents available. Next, we detail the dataset, baseline methods,
and evaluation metrics. The experimental results are reported and
discussed in Section 5.

In Equation 1, δ (D ∗ , ds ) is the average similarity between ds and
all documents in D ∗ , defined as follows.
1 Õ
δ (D ∗ , ds ) =
sim(d j , ds )
(2)
|D ∗ |
d j ∈D ∗

We measure the similarity between d j and ds by cosine similarity
of their tf-idf term vector representations. Note that, Equation 1
allows an infrequent term in the candidate collection to have a
high weight if the documents where the term is appeared are very
similar to the seed document.
In the following section, we present a ranking model using the
query term weight to calculate the probability that a document d
in a set of candidate documents C is relevant.
In SDR, we adopt the query likelihood language model with
Jelinek-Mercer (JM) smoothing, as the ranking function. The original ranking function is defined in Equation 3 (see [29] page 126 for
the derivation):


Õ
c(ti , d)
1−λ
·
score(d, q) =
c(ti , q) · log 1 +
(3)
λ
Ld · p(ti |C)

4.1

Dataset

We use the dataset provided by the CLEF 2017 eHealth Task 2.
CLEF17 dataset consists of total 50 diagnostic test accuracy (DTA)
systematic reviews. DTA SRs are known for a high level of difficulty
in finding relevant documents because of their complexities. As
shown in Table 1, in general less than one percent of candidate
documents are relevant among the 50 SRs. More detailed statistics
of the dataset are reported in [10].
In CLEF17 dataset, 50 SRs are divided into two sets: 20 SRs as
training set (for developing supervised models), and 30 SRs as test
set. We evaluate our models on both the 30 SRs so as to compare
with the best performance reported in the competition, and also
the 50 SRs as our model requires no training.
The CLEF17 dataset releases only the PubMed document identification numbers (pmids). In our experiments, we obtain titles
and abstracts of the clinical documents from PubMed8 using the
provided pmids. For relevance labels, each document has two relevance labels. One is the label from abstract screening, and the
other is from full text screening. We use the final relevance labels
in our experiments, since these labels indicate the final relevance
judgment by SR experts. In short, in our experiments, the ranking
is based on titles and abstracts of the documents, and the quality
of the ranking is evaluated by the final relevance labels. The same
setting is applied to all the models in comparison.

t i ∈d,q

where c(ti , q) and c(ti , d) denote the count of term ti in query q and
in a document d, respectively, and Ld is the length of document
d in number of terms. p(ti |C) is the probability of term ti in the
background corpus. λ is JM smoothing parameter.
Using ds as a query, we incorporate the term weighting φ(ti , ds )
into the ranking function in SDR, as follows:


Õ
1−λ
c(ti , d)
score(d, ds ) =
φ(ti , ds )·c(ti , ds )·log 1 +
·
λ
Ld · p(ti |C)
t i ∈d,d s

(4)
In Equation 4, p(ti |C) is estimated by maximum likelihood estimation (MLE) on candidate document set C. We empirically set λ = 0.2
in our experiments.

8 https://www.nlm.nih.gov/bsd/pmresources.html

459

Session 4C: Medical & Legal IR

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

There is a recently published dataset for evaluating the retrieval
process in systematic reviews [21]. However, this dataset does not
contain information (e.g., pmids) of all candidate documents of its
SRs. Hence it cannot be used in our experiments.

is the mean average precision for a given SR. The final measures
reported are the average over all SRs evaluated in a given set. We
report the measures on the 30 SRs in the test set of CLEF17 and the
50 SRs in the entire dataset of CLEF17, as two sets.

Clinical Term Extraction. Clinical terms were identified by using
NCBO BioPortal API7 and QuickUMLS [23]. Both are tools that annotate free-text with clinical terms in UMLS medical thesaurus. For
QuickUMLS, UMLS 2016AB version was used with default parameter settings. The extraction results from the two tools are mostly
similar. To avoid missing any terms, we combined the extraction
results by adding clinical terms extracted by QuickUMLS which do
not exist in the results of BioPortal.
Represented in bag-of-clinical terms, the length of a document
(in number of terms) is on average 15% of the same document
represented in bag-of-words.

LastRel% and WSS. The goal of screening in SRs is to identify all
relevant documents from candidate documents. In CLEF17 competition the submissions are evaluated by two task-specific measures:
LastRel% and W SS.
LastRel is the ranking position of the last relevant document,
i.e., the minimum number of documents need to be screened to get
all relevant documents. LastRel% is the normalized value by the
number of (unlabeled) candidate documents as SRs have different
number of candidate documents, and the smaller, the better.
Work Saved over Sampling (W SS) is originally proposed in [3].
It measures how many documents in candidate documents can
be removed from manual screening. Assuming manual screening
starts from the top of the ranking, until the last relevant document is found. Then the remaining candidate documents can be
safely ignored, hence the work saved. Specifically, W SS = (|C | −
LastRel)/|C |. Here, |C | is the number of candidate documents and
LastRel is the ranking position of the last relevant document.
The difference between LastRel% and W SS emerges during the
screening process. As the screening process proceeds and more
documents are manually labeled, LastRel changes accordingly in
the unlabeled documents. However, W SS remains unchanged since
the number of documents after the LastRel does not change. Note
that both LastRel% andW SS will change if the unlabeled documents
are re-ranked from time to time during the screening process e.g.,
based on relevance feedback or after getting more seed documents
in SDR.

4.2

Baseline Methods

We compare SDR with two classic ranking models in information
retrieval, namely BM25 and the query likelihood model (QLM).
For BM25, we used the implementation in gensim v3.2.0 toolkit9 .
For QLM, we used JM smoothing (see Equation 3). By comparing
SDR (see Equation 4) with QLM, we examine the impact of the
proposed weighting scheme φ(ti , ds ) in Equation 1. In experiments,
we also evaluate the effectiveness of ‘bag-of-clinical terms’ (BOC)
compared to ‘bag-of-words’ (BOW) representation. As a result, we
evaluate six rankings: three ranking models (BM25, QLM, and SDR)
on two representations (BOC and BOW)
In addition, we evaluate a ranking model based on word embeddings, which we call average embedding similarity (AES). Word
embedding is an unsupervised approach to learn continuous distributed vector representations of words. It is shown in many studies
that the dense vector representation is effective to encode linguistic regularities and semantic similarities among words. In AES, a
document is represented by the average embeddings of its contained words [14, 18]. A candidate document is ranked by its cosine
similarity to the seed document computed on the average word
embedding representation. In our implementation, we used the
word embeddings pre-trained on PubMed corpus and Wikipedia.10

5

5.1
4.3

RESULTS AND DISCUSSION

We report the overall ranking performances of SDR and baselines
in Section 5.1. In Section 5.2, we analyze the contribution of term
weighting scheme and the performance of SDR on individual SRs.
Then, we simulate the manual review process and evaluate the
ranking when more seed documents become available in Section 5.3.

Evaluation Metrics

Overall Performance

Table 2 reports the overall ranking performance of three ranking
models (BM25, QLM, and SDR) with two document representations
(BOW and BOC), on two SR sets (the 30 SRs in CLEF17 test set, and
the 50 SRs in CLEF17).

To evaluate ranking effectiveness, we report both standard retrieval
evaluation measures and task-specific evaluation measures.
Standard IR Measures. For overall ranking effectiveness, we report average precision (AvдPr ), precision at top k documents (Pr @k)
and recall at top k documents (Re@k).
Pr @k is the proportion of top-k ranked documents that are
relevant. Re@k is the proportion of relevant documents that ranked
in the top-k positions. AvдPr is the average of the precision values
each obtained when a relevant document is retrieved. As mentioned
earlier, for a given SR, every relevant document can be used as a seed
document to rank candidate documents. The above three measures
are computed as the average over all the values obtained after each
relevant document is used as a seed document. In this sense, AvдPr

Reference method: CLEF-Query. As a reference we include the
results of CLEF-Query, which is the best model in terms of AvдPr reported in CLEF2017 eHealth Task 2 with no relevance feedback [10].
CLEF-Query is reported on 30 SRs, the CLEF17 test set.11 CLEFQuery utilized the query keywords which retrieved documents from
PubMed to rank candidate documents [1]. Direct comparison of
CLEF-Query and other models in the table is unfair because of different problem settings. Nevertheless, the AvдPr of 0.18 demonstrates
the difficulty of the task to identify a small number of relevant
documents in systematic reviews.
11 For

CLEF-Query, individual performances of 30 SRs were not reported in [10]. We
do not have P r @K and Re@K values and we derive the values of LastRel% from
(1 − W S S ).

9 https://radimrehurek.com/gensim/index.html
10 http://bio.nlplab.org/

460

Session 4C: Medical & Legal IR

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 2: Performance of SDR and baseline ranking models on 30 SRs (CLEF17 test set) and 50 SRs (the entire CLEF17 dataset).
The best performance among all models is in boldface and italics. Among SDR and baselines (i.e., not including CLEF-Query
and SDR+AES), the best performance is in boldface and the second best is underlined. * denotes the statistically significant difference between SDR-BOC and a baseline. † and ‡ are significance test results of SDR+AES over AES and SDR-BOC, respectively.
Statistical significance is by one-tailed paired t-test with p < 0.05.
Dataset

Ranking Model

AvgPr

Pr@10

Pr@20

Pr@30

LastRel%

Re@10

Re@20

Re@30

WSS

CLEF-Query

0.18

-

-

-

46.0

-

-

-

0.54

BM25-BOW
QLM-BOW
SDR-BOW

0.161*
0.159*
0.181

0.176*
0.165*
0.201*

0.145*
0.138*
0.166*

0.126*
0.118*
0.139*

52.9*
52.0*
46.7

0.246*
0.245*
0.257

0.330*
0.324*
0.353*

0.385*
0.376*
0.401*

0.470*
0.479*
0.532*

BM25-BOC
QLM-BOC
SDR-BOC

0.213*
0.214*
0.227

0.233*
0.228*
0.238

0.180*
0.180*
0.189

0.150*
0.150*
0.157

46.5
43.3*
39.8

0.261*
0.264*
0.273

0.345*
0.361*
0.367

0.408*
0.415*
0.436

0.534*
0.566*
0.600

AES

0.211

0.224

0.175

0.149*

38.7*

0.285*

0.364

0.420*

0.612

SDR+AES
BM25-BOW
QLM-BOW
SDR-BOW

0.264†‡

0.276†‡

0.213†‡

0.177 †

32.5 †

0.315 †

0.413†

0.484†‡

0.147*
0.141*
0.170*

0.179*
0.168*
0.205*

0.146*
0.137*
0.167*

0.128*
0.119*
0.144*

57.4
55.7*
48.5

0.234
0.233*
0.247

0.305
0.297*
0.323

0.363
0.343*
0.377

0.673†‡
0.425*
0.442*
0.514

BM25-BOC
QLM-BOC
SDR-BOC

0.164*
0.167*
0.178

0.190*
0.193*
0.202

0.151*
0.156*
0.164

0.128*
0.132*
0.139

46.4*
43.3*
39.8

0.230*
0.233*
0.240

0.296*
0.307*
0.312

0.345*
0.353*
0.369

0.535*
0.567*
0.601

AES

0.147*

0.171*

0.134*

0.115*

50.5

0.238

0.294

0.333*

0.492*

SDR+AES

0.202†‡

0.226

0.179†‡

0.152†‡

37.7 †‡

0.341†‡

0.399†‡

0.622†‡

30 SRs

50 SRs

0.265

†‡

both datasets. For SDR ranking model, BOC wins in terms of AvдPr ,
LastRel%, and W SS and loses to BOW in terms of Pr @K and Re@K.
Although BOC does not beat BOW on all settings, we argue that
BOC representation has great advantages in enabling more efficient
screening process. With BOC, a candidate document is represented
by a small set of clinical terms defined in UMLS. In BOC representation, the length of a document is only about 15% of that in BOW
representation. Furthermore, SDR estimates a weight for each clinical term, making it easy for SR experts to interpret the important
clinical terms in screening.
From these results, SDR-BOC and AES are the best performers.
Interestingly, the two rankings are from two completely different
spaces. This motivates us to evaluate a combined ranking model to
take advantage of both. Next, we discuss the performance of the
combined model: SDR-BOC + AES, or simply denoted as SDR+AES.

SDR-BOC vs. Baselines. We first compare SDR-BOC with baseline models by BM25 and QLM with BOW and BOC representations,
SDR-BOW and AES. We exclude the model SDR+AES in this comparison.
On 30 SRs, for measures defined on precision, SDR-BOC achieves
the best AvдPr and all Pr @K (K ∈ {10, 20, 30}) measures. Paired
t-test results show that the improvements made by SDR-BOC on
top of other baselines are statistically significant in most cases,
except AvдPr on SDR-BOW and AES, and Pr @10 and Pr @20 on
AES. Observe that the second best values on precision measures are
achieved by BOC representations as well. On task-specific measures
LastRel% and W SS, the simple ranking model AES outperformed
SDR-BOC by a small margin. For measures defined on recall, the
performances of SDR-BOC and AES are comparable as either best
or second best performances, among all the methods in comparison.
On 50 SRs, overall SDR-BOW delivers the best performance over
Pr @K and Re@K, while SDR-BOC is the best performer in terms
of AvдPr and task-specific measures LastRel% and W SS. Word
embedding based ranking, becomes less competitive on this larger
dataset. We also note that, overall there is a performance drop on
all measures from 30 SRs to 50 SRs.
To summarize, we show the consistently better performance
of SDR ranking model over BM25 and QLM, with both BOC and
BOW representations. Word embedding based ranking, AES, shows
good potential to be a good performer, at least on the 30 SR dataset.
In terms of document representation, BOC demonstrates clear superiority over BOW for both BM25 and QLM ranking models, on

Combined Model: SDR+AES. Since SDR-BOC and AES leverage
different aspects of documents as feature for ranking candidate
documents, we combine the relevance scores computed by the two
models through linear combination. Specifically, let s(d) be the
normalized ranking score by SDR-BOC for document d, and e(d) be
the normalized ranking score by AES. The ranking score computed
for SDR+AES is: α · s(d) + (1 − α) · e(d). We set α = 0.3 for both the
30 SRs and 50 SRs and report the results.
Reported in Table 2, SDR+AES, taking advantages from both
spaces, is the clear winner on all measures, on both datasets. We also
conducted significance tests by comparing the results of SDR+AES
against each of its components, SDR-BOC and AES. The tests show

461

0.90

6000

0.85
0.80

4000

0.75
2000
0.70

[0
.9
,1
.0
]

[0
.8
,0
.9
)

[0
.7
,0
.8
)

[0
.6
,0
.7
)

[0
.5
,0
.6
)

[0
.4
,0
.5
)

[0
.3
,0
.4
)

[0
.2
,0
.3
)

[0
.1
,0
.2
)

0

Mean and variance of estimated term weights

8000

avgPr of different seed documents

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

0.95

[0
.0
,0
.1
)

Number of clinical terms (histogram)

Session 4C: Medical & Legal IR

1.0

average

0.8
0.6
0.4
0.2
0.0
Systematic reviews

Figure 5: Distribution of AvgPr of seed documents in 50 SRs.
Each boxplot is a distribution of AvgPr values in one SR,
sorted by their mean values in ascending order. Purple and
blue boxplots indicate the SRs are from the train and test set
in CLEF17, respectively. (best viewed in color)

Normalized document frequency of terms in relevant documents

Figure 4: The estimated term weight vs. document frequency
of clinical terms in 50 SRs. In line graph a filled triangle is a
mean value and an error range denotes variance.

Table 3: Simplified relevance conditions of example SRs and
their AvgPr values. The main clinical terms are manually
extracted from the detailed relevance conditions (see references for the original relevance conditions).

that on most measures the difference is statistically significant. That
is, the two types of features in the ranking model well complement
each other. We believe that although BOC representation is able
to identify the important clinical terms without ambiguity, the
reduction of terms (i.e., to the 15% of documents’ original length)
inevitably results in information loss. Dense vector representation,
in this case, captures such information loss and contributes to the
final ranking.
Performance Differences on 30 SRs and 50 SRs. Lastly, compared to 30 SRs, performance degradation is observed on 50 SRs.
To investigate the potential reason why adding the additional 20
SRs leads to poorer performance, we evaluated the nearest neighbors of relevant documents in the 30 and 20 SRs. For each relevant
document, we get the five most similar documents from candidate
documents by cosine similarities on tf-idf representations of documents. In 30 SRs, on average 3.82 documents out of the 5 most
similar document are irrelevant documents (sd = 1.19). But in the
additional 20 SRs, 4.32 documents out of 5 are irrelevant documents
(sd = 0.64). The results suggest that the 30 SRs consist of relatively easier SRs to identify the relevant documents. In other words,
relevant documents in the other 20 SRs are more similar with irrelevant documents, and thus more challenging to distinguish from
the irrelevant ones.

SRs

Index test

Reference test

[4]

Capsule endoscopy

Oesophago-gastro-duodenoscopy

AvgPr
0.975

[22]

Non-invasive biomarkers

DMSA renal scan

0.638

[7]

Ultrasound
Liver function tests

Surgical/endoscopic extraction
Surgical/endoscopic negative result

0.004

[27]

Red flags
(27 questions)

Plain radiographs
Computed tomography (CT)
Magnetic resonance imaging (MRI)
Bone scan

0.001

Term Weighting Scheme and Individual SRs

in Figure 4. The total number of terms in each bin is also presented
in histogram in the figure.
Identifying the terms having higher document frequency among
relevant documents and giving them higher weights are essential
in improving the ranking quality. As a document frequency increases, the number of terms in the corresponding range decreases
as expected. Shown in the figure, our proposed weighting scheme
is extremely effective in assigning higher weights to the terms
with higher document frequency. As the normalized document frequency increases, the average of term weights increases as well.
This results show that the proposed weighting scheme is effective
in promoting terms that appear in many relevant documents.

Term Weighting Scheme. The proposed seed-driven ranking
model incorporates domain-specific characteristics of medical documents in SRs (i.e., Observations 1 and 2). Compared with the QLM,
SDR introduces an additional term weighting component in the
ranking model. The term weighting scheme estimates the importance of a clinical term to identify relevant documents in candidate
documents. The experimental results show the significance of this
additional component as SDR significantly outperforms QLM.
In this section, we provide a close analysis on the contribution
of the term weighting scheme φ(ti , ds ) in Equation 1. We bin the
clinical terms based on their normalized document frequency in
SR’s relevant documents in 10 ranges as shown in the x-axis of
Figure 4. For the terms fall into each bin, we compute the mean and
variance of the weights estimated by Equation 1 and plot the values

Performance of Individual SRs. Each SR has its unique clinical question and accordingly, relevance conditions. Depending on
the SRs, relevance conditions vary in complexity and difficulty.
While we have discussed the overall performance on the CLEF17
dataset (30 SRs and 50 SRs), in this following section, we present
performances of individual SRs.
Figure 5 plots AvдPr of individual SRs in SDR-BOC, sorted in ascending order. Each boxplot is an individual SR and it represents its
performance distribution of using different relevant documents as
seed document. The dotted gray line is the overall averaged AvдPr
of the 50 SRs in SDR-BOC, as presented in Table 2. As shown in
Figure 5, each SR has dramatically different ranking performances.
Some SRs have exceptionally high AvдPr values and some only
manage to achieve small values.

5.2

462

Session 4C: Medical & Legal IR

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 4: Evaluation of updated ranking in simulation of manual screening process. In each iteration top 10 ranked documents
are labeled and the relevant documents are appended to the seed to update the ranking. The best performance is in boldface
and the second best is underlined. * denotes a statistically significant difference compared to the best, paired t-test with p < 0.05.
Dataset

Iteration

Model

AvgPr

Pr@10

Pr@20

Pr@30

LastRel%

Re@10

Re@20

Re@30

WSS

1

AES
SDR-BOC
SDR+AES

0.205*
0.241
0.275

0.193*
0.240*
0.276

0.156*
0.189*
0.218

0.141*
0.161*
0.190

36.0*
32.4
25.5

0.393*
0.397*
0.455

0.438*
0.470*
0.513

0.485*
0.516*
0.559

0.635*
0.670*
0.738

2

AES
SDR-BOC
SDR+AES

0.243*
0.244*
0.351

0.258*
0.196*
0.299

0.234*
0.168*
0.245

0.221
0.151*
0.220*

34.7*
28.8*
22.5

0.457*
0.483*
0.557

0.501*
0.533*
0.596

0.538*
0.562*
0.631

0.649*
0.705*
0.764

3

AES
SDR-BOC
SDR+AES

0.222*
0.242*
0.347

0.236*
0.217*
0.343

0.215*
0.201*
0.308

0.203*
0.192*
0.285

34.1*
25.6*
20.5

0.510*
0.539*
0.623

0.545*
0.573*
0.661

0.574*
0.606*
0.679

0.657*
0.733*
0.781

1

AES
SDR-BOC
SDR+AES

0.141*
0.190
0.212

0.144*
0.205*
0.227

0.118*
0.165*
0.183

0.107*
0.142*
0.162

49.0
33.4
30.0

0.314*
0.338*
0.372

0.348*
0.397*
0.425

0.382*
0.435*
0.472

0.508*
0.663*
0.696

2

AES
SDR-BOC
SDR+AES

0.167*
0.191*
0.275

0.187*
0.172*
0.265

0.165*
0.148*
0.215

0.154*
0.136*
0.191

48.4*
30.1*
26.6

0.365*
0.409*
0.464

0.396*
0.452*
0.506

0.422*
0.491*
0.547

0.516*
0.695*
0.728

3

AES
SDR-BOC
SDR+AES

0.151*
0.189*
0.282

0.170*
0.181*
0.306

0.149*
0.168*
0.259

0.140*
0.160*
0.231

48.2*
27.6*
24.5

0.405*
0.460*
0.537

0.430*
0.502*
0.582

0.451*
0.536*
0.602

0.520*
0.719*
0.748

30SRs

50SRs

In the figure, we also distinguish the 20 SRs and 30 SRs that are in
the train and test set of CLEF17 dataset by plotting them in purple
and blue boxes respectively. The plot shows that a few SRs in the
test set happen to have very high AvдPr values. This also explains
the poorer results on the 50 SRs in Table 2.
To investigate the different performances across SRs, we looked
into original relevance conditions of a few example SRs. Table 3
reports simplified relevance conditions of two SRs with high AvдPr
values and two SRs with low AvдPr values. Before discussing the
different performances of SRs, we briefly explain the meanings of
index test and reference test in SRs. Previously we introduced an
example clinical question in Section 1: ‘Can laparoscopy (diagnostic
test) assess the resectability of pancreatic cancer?’ Given the clinical
question, laparoscopy is an evaluation target and the evaluation
target is called index test in DTA SRs. Performance of index test is
compared with another test called reference test for evaluation. We
present index test and reference test as representatives of relevance
conditions in Table 3. The first two SRs with high AvдPr values, [4]
and [22], have relatively simple relevance conditions. The other
two SRs with low AvдPr values, [27] and [7], have many more
elements in their relevance conditions. More elements in relevance
conditions indicate a broader range of relevance and thus it leads to
higher complexities in relevance conditions. Hence, the relevance
conditions reflect the difficulty of SRs and also explain different
AvдPr values of individual SRs.
Additionally, seed documents show varying performances within
each SR. Even though every relevant document can be a seed document, their relation with other relevant documents may lead to
different performances. For example, two relevant documents may

cover two different reference tests listed in relevance conditions.
Regarding varying performance of different seed documents, we
acknowledge that it is an interesting direction to further explore.
For example, investigating factors of high ranking performance in
seed documents and identifying them among candidate documents
can be our future work.

5.3

Simulation of Manual Screening Process

The goal of screening process is to find all relevant documents. SR
experts will screen individual documents in practice from the top
of the ranked documents. Hence, we evaluate the ranking in the
setting of manual screening process, in an iterative manner. The
simulation process starts with a single seed document. We set a
batch size (B) and assume that SR experts each time label a batch
of documents. The obtained relevant documents from the labeled
batch can be appended to the seed document to update the ranking
of remaining unlabeled candidate documents. We run 3 iterations,
and each iteration is based on the updated ranking result of the
previous iteration.
We evaluate the three best models: SDR-BOC, AES, and SDR+AES.
We use the same value of α = 0.3 in SDR+AES and set the batch size
B to be 10. That is, in each iteration, the top 10 ranked candidate
documents in the current ranking will be labeled by SR experts.
Table 4 reports the evaluation results. Likewise, we report separated results on 30 SRs and 50 SRs. Observe from Table 4, SDR+AES
is a clear winner on both 30 SRs and 50 SRs with all measures. The
improvement over its component rankers (SDR-BOC and AES) is
statistically significant in almost all comparisons. Although the

463

Session 4C: Medical & Legal IR

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

second best performer is not always SDR-BOC, it consistently outperforms AES on task-specific measures LastRel%, W SS, and also
AvдPr and Re@K. AES wins SDR-BOC on a few Pr @K measures.
We claim that as the manual screening starts, the task-specific
measures such as LastRel% become more critical to find all relevant
documents as soon as possible. Specifically, as more documents
are labeled, SDR+AES and SDR-BOC keep improving the ranking
of last relevant documents. At the end of third iteration in 50 SRs,
both models manage to rank the last relevant document at around
top 25% among unlabeled documents. This is very promising result.
AES alone, on the other hand, achieves top 30%. Similar with the
results reported in Table 2, poorer results are observed on 50 SRs
compared to the performance obtained on 30 SRs.
In this simulation, when more relevant documents become available, we simply append them to the seed document to form a new
larger seed document in SDR. For AES, we compute the average
of representations of the relevant documents. We acknowledge
that a more advanced approach, fully utilizing multiple relevant
documents as seed documents can further improve this simulation
performance. We leave this as our future work.

6

[3] Aaron M Cohen, William R Hersh, K Peterson, and Po-Yin Yen. 2006. Reducing
workload in systematic review preparation using automated citation classification.
Journal of the American Medical Informatics Association 13, 2 (2006), 206–219.
[4] Agostino Colli, Juan Cristóbal Gana, Dan Turner, Jason Yap, Thomasin AdamsWebber, Simon C Ling, and Giovanni Casazza. 2014. Capsule endoscopy for the
diagnosis of oesophageal varices in people with chronic liver disease or portal
vein thrombosis. Cochrane Database Syst Rev 10 (2014).
[5] Gordon V. Cormack and Maura R. Grossman. 2016. Engineering Quality and
Reliability in Technology-Assisted Review. In SIGIR. 75–84.
[6] Gordon V. Cormack and Maura R. Grossman. 2017. Technology-Assisted Review
in Empirical Medicine: Waterloo Participation in CLEF eHealth 2017. In CEUR
Workshop Proceedings, Vol. 1866.
[7] Kurinchi Selvan Gurusamy, Vanja Giljaca, Yemisi Takwoingi, David Higgie, Goran
Poropat, Davor Štimac, and Brian R Davidson. 2015. Ultrasound versus liver
function tests for diagnosis of common bile duct stones. Cochrane Database Syst
Rev 2 (2015).
[8] Kazuma Hashimoto, Georgios Kontonatsios, Makoto Miwa, and Sophia Ananiadou. 2016. Topic detection using paragraph vectors to support active learning
in systematic reviews. Journal of biomedical informatics 62 (2016), 59–65.
[9] Siddhartha R Jonnalagadda, Pawan Goyal, and Mark D Huffman. 2015. Automating data extraction in systematic reviews: a systematic review. Systematic reviews
4, 1 (2015), 78.
[10] Evangelos Kanoulas, Dan Li, Leif Azzopardi, and Rene Spijker. 2017. CLEF 2017
Technologically Assisted Reviews in Empirical Medicine Overview. In CEUR
Workshop Proceedings, Vol. 1866.
[11] Youngho Kim and W. Bruce Croft. 2014. Diversifying Query Suggestions Based
on Query Documents. In SIGIR. 891–894.
[12] Youngho Kim and W. Bruce Croft. 2015. Improving Patent Search by Search
Result Diversification. In ICTIR. 201–210.
[13] Youngho Kim, Jangwon Seo, W Bruce Croft, and David A Smith. 2014. Automatic
suggestion of phrasal-concept queries for literature search. IP&M 50, 4 (2014),
568–583.
[14] Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. 2015. From word
embeddings to document distances. In ICML. 957–966.
[15] Matthew Lease, Gordon V Cormack, An T Nguyen, Thomas A Trikalinos, and
Byron C Wallace. 2016. Systematic review is e-discovery in doctor’s clothing. In
MedIR workshop, SIGIR.
[16] Yuanhua Lv, Taesup Moon, Pranam Kolari, Zhaohui Zheng, Xuanhui Wang, and
Yi Chang. 2011. Learning to Model Relatedness for News Recommendation. In
WWW. 57–66.
[17] Iain J Marshall, Joël Kuiper, and Byron C Wallace. 2015. RobotReviewer: evaluation of a system for automatically assessing bias in clinical trials. Journal of the
American Medical Informatics Association 23, 1 (2015), 193–201.
[18] Eric Nalisnick, Bhaskar Mitra, Nick Craswell, and Rich Caruana. 2016. Improving
document ranking with dual word embeddings. In WWW. 83–84.
[19] Alison ÓMara-Eves, James Thomas, John McNaught, Makoto Miwa, and Sophia
Ananiadou. 2015. Using text mining for study identification in systematic reviews:
a systematic review of current approaches. Systematic reviews 4, 1 (2015), 5.
[20] Harrisen Scells, Guido Zuccon, Bevan Koopman, Anthony Deacon, Leif Azzopardi,
and Shlomo Geva. 2017. Integrating the Framing of Clinical Questions via PICO
into the Retrieval of Medical Literature for Systematic Reviews. In CIKM. 2291–
2294.
[21] Harrisen Scells, Guido Zuccon, Bevan Koopman, Anthony Deacon, Leif Azzopardi,
and Shlomo Geva. 2017. A Test Collection for Evaluating Retrieval of Studies for
Inclusion in Systematic Reviews. In SIGIR. 1237–1240.
[22] Nader Shaikh, JL Borrell, J Evron, and MM Leeflang. 2011. Procalcitonin, Creactive protein, and erythrocyte sedimentation rate for the diagnosis of acute
pyelonephritis in children. Cochrane Database Syst Rev 1 (2011).
[23] Luca Soldaini and Nazli Goharian. 2016. Quickumls: a fast, unsupervised approach
for medical concept extraction. In MedIR workshop, SIGIR.
[24] Byron C Wallace, Joël Kuiper, Aakash Sharma, Mingxi Brian Zhu, and Iain J
Marshall. 2016. Extracting PICO sentences from clinical trial reports using
supervised distant supervision. JMLR 17, 132 (2016), 1–25.
[25] Byron C Wallace, Kevin Small, Carla E Brodley, and Thomas A Trikalinos. 2010.
Active learning for biomedical citation screening. In KDD. 173–182.
[26] Linkai Weng, Zhiwei Li, Rui Cai, Yaoxue Zhang, Yuezhi Zhou, Laurence T. Yang,
and Lei Zhang. 2011. Query by Document via a Decomposition-based Two-level
Retrieval Approach. In SIGIR. 505–514.
[27] Christopher M Williams, Nicholas Henschke, Christopher G Maher, Maurits W
van Tulder, Bart W Koes, Petra Macaskill, and Les Irwig. 2013. Red flags to
screen for vertebral fracture in patients presenting with low-back pain. Cochrane
Database Syst Rev 1 (2013).
[28] Yin Yang, Nilesh Bansal, Wisam Dakka, Panagiotis Ipeirotis, Nick Koudas, and
Dimitris Papadias. 2009. Query by Document. In WSDM. 34–43.
[29] ChengXiang Zhai and Sean Massung. 2016. Text data management and analysis:
a practical introduction to information retrieval and text mining. Morgan &
Claypool.

CONCLUSION AND FUTURE WORK

In this paper, we propose seed-driven document ranking for systematic reviews that leverages domain-specific characteristics in
document representation and ranking. The superior performances
of the proposed approach and the combined approach with word
embedding show the effectiveness of incorporating domain-specific
characteristics in screening prioritization. Furthermore, our approach has multiple advantages. Employing clinical terms reduces
the total number of terms in documents and it leads to the decreased
computational cost. A small number of clinical terms also helps
speed up manual screening process executed by SR experts. Besides,
the proposed weighting scheme can effectively promote terms that
are in common in many relevant documents.
Our work opens a new direction in the research of screening prioritization by using a seed document, which is often available after
defining the clinical question and relevance conditions. However,
as each seed document shows different ranking performance, how
to incorporate additional available resources to enhance the seed
document could be an interesting direction. For example, additional
resources can include relevance conditions and keyword queries
used for retrieving candidate documents. In our experiments, SRs
demonstrate a different level of difficulties for finding their relevant
documents. Predicting the difficulty of SRs can be another interesting problem. We discussed the potential reasons with relevance
conditions. Lastly, when multiple relevant documents are given
during the manual screening process, various approaches can be
explored to benefit from multiple labeled documents.

REFERENCES
[1] Amal Alharbi and Mark Stevenson. 2017. Ranking abstracts to identify relevant
evidence for systematic reviews: The University of Sheffield’s approach to CLEF
eHealth 2017 Task 2: Working notes for CLEF 2017. In CEUR Workshop Proceedings,
Vol. 1866.
[2] Victoria B Allen, Kurinchi Selvan Gurusamy, Yemisi Takwoingi, Amun Kalia, and
Brian R Davidson. 2013. Diagnostic accuracy of laparoscopy following computed
tomography (CT) scanning for assessing the resectability with curative intent in
pancreatic and periampullary cancer. Cochrane Database Syst Rev 11 (2013).

464

