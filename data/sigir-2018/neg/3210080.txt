Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Texygen: A Benchmarking Platform for Text Generation Models
Yaoming Zhu† , Sidi Lu† , Lei Zheng† , Jiaxian Guo† , Weinan Zhang† , Jun Wang‡ , Yong Yu†∗
† Shanghai

Jiao Tong University, ‡ University College London
{ymzhu,steve_lu,wnzhang}@apex.sjtu.edu.cn

ABSTRACT

vanishing (MaliGAN [3], RankGAN [10], BRA in LeakGAN [5]),
long-term robustness (LeakGAN).
There are three main challenges on evaluating sequential generation of textual data. First, the ultimate goal of text generation is
still unclear, though researchers have developed imperfect metrics
such as perplexity [7], oracle generated log-likelihood [15], human
scores and the BLEU metric [12]. There is no metric that is sufficiently comprehensive itself. Thus, evaluation over multiple metrics
is required to draw definitive answers. Second, there is no obligation for researchers to make their source code publicly available,
thus making reproducing the reported experimental results difficult. Third, text generation suffers from a so-called quality-diversity
tradeoff problem, i.e., to-some-extent shrink to a restricted output
pattern with mode collapse, which, on the other hand, causes researchers to release those models which only actually adjust the
tradeoff balance. To our knowledge, there is no good metric for
diversity evaluation. Thus, there is a significant need for a reliable
platform that provides a thorough evaluation of the existing text
generation models and facilitate the development of new ones in a
common framework.
In this paper, we release Texygen1 , a fully open-sourced benchmarking platform for text generation models. Texygen not only
maintains a majority of the recently published models, but also
includes a variety of metrics that evaluates the diversity, quality
and the consistency of the generated texts. With these metrics, we
can have a much more comprehensive study of different text generation models. We hope this platform could help the progress of
standardizing the research on text generation, increase the reproducibility of research work in this field, and encourage higher-level
applications.

We introduce Texygen, a benchmarking platform to support research on open-domain text generation models. Texygen has not
only implemented a majority of text generation models, but also
covered a set of metrics that evaluate the diversity, the quality
and the consistency of the generated texts. The Texygen platform
could help standardize the research on text generation and improve
the reproductivity and reliability of future research work in text
generation.

KEYWORDS
Text Generation, Benchmarking, Evaluation Metrics
ACM Reference Format:
Yaoming Zhu† , Sidi Lu† , Lei Zheng† , Jiaxian Guo† , Weinan Zhang† , Jun
Wang‡ , Yong Yu† . 2018. Texygen: A Benchmarking Platform for Text Generation Models. In SIGIR ’18: The 41st International ACM SIGIR Conference
on Research & Development in Information Retrieval, July 8–12, 2018, Ann
Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/
3209978.3210080

1

INTRODUCTION

The open-domain text generation problem aims at modeling the
sequential generation of discrete tokens. It has rich real-world applications, including, but not limited to, machine translation [2], AI
chat bots [9], question answering and information retrieval [13].
While various implementations of practical use are witnessed, the
fundamental research on text generative models has also made
significant progress. Notably, since maximum likelihood estimation (MLE) [11] is not a perfect objective function (due to exposure
bias explained in [6]), researchers have been looking for improved
alternatives.
The success of Generative Adversarial Network (GAN) [4] has
inspired people to investigate adversarial training over textual,
discrete data. SeqGAN [15], for example, is one of the very early
attempts at applying the REINFORCE algorithm [14] to solve the
optimization of GAN objective over (sequential) discrete data distribution. Since then, improved variants of SeqGAN are proposed
to further develop it in many aspects; examples include gradient
∗ W.

2

THE TEXYGEN PLATFORM

Texygen provides a standard top-to-down multi-dimensional evaluation system for text generation models. Currently, Texygen consists of two elements: well-trained baseline models and automatically computable evaluation metrics. Texygen also provides the
open source repository of the platform, in which researchers can
find the specification and manual of APIs for implementing their
models for Texygen to evaluate.

Zhang is the corresponding author.

2.1

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210080

Baseline Models

In the current version of Texygen, we implement various likelihoodbased models such as vanilla MLE language model, SeqGAN [15],
MaliGAN [3], RankGAN [10], TextGAN (feature matching) [16],
GSGAN (GAN with Gumbel Softmax trick) [8] and LeakGAN [5].
These baseline models contain supervised likelihood-based methods with and without tricks, adversarial methods and hierarchical
1 https://github.com/geek-ai/Texygen

1097

Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

LeakGAN. LeakGAN [5] is a hierarchical reinforcement learning
framework with two modules called Manager and Worker respectively. In general, the Manager learns to generate a sequence of
subgoals for the sequence and the Worker learns to fulfill it.

methods. Although more models will be added, we believe the current coverage in the collection is sufficient for good comparison of
any new model. Here we briefly introduce these models.
Vanilla MLE. Given a sequence piece st = [x 0 , x 1 , ..., x t −1 ] and
the next token to be sampled from the model x t ∼ πθ (x |st ), a
vanilla MLE language model [11] adopts an explicit likelihoodbased modeling of language with the form of
ÕÕ
max
log πθ (x t |st ),
θ

x

2.2

t

2.2.1 Document Similarity based Metrics. The most intuitive
measurement of generated documents quality is how the documents
resemble the natural language, or, the training dataset.

where x iterates over training data sentences and t iterates over
the token sequence of each sentence. By maximizing the likelihood
estimation, MLE manages to have an estimation of the generation
procedure.

BLEU. BLEU [12] is a widely used metric evaluating the word
similarity between sentences or documents.

SeqGAN. SeqGAN [15] adopts a discriminative model that is trained
to minimize the binary classification loss between real texts and
generated texts. Meanwhile, besides the pretraining procedure that
follows MLE metric, the generator uses the REINFORCE algorithm
to optimize the GAN objective




min − E
log D ϕ (Y ) − E log(1 − D ϕ (Y )) .
ϕ

Y ∼pdata

EmbSim. Inspired by BLEU, we propose EmbSim to evaluate the
similarity between two documents, whose name stands for “embedding similarity”. Instead of comparing sentences words by words,
we compare the word embeddings.
First, word embedding is evaluated on real data using a skipgram model. For each word embedding, we compute its cosine
distance with the other words, and then formulate it as a matrix W ,
where Wi, j = cos(ei , e j ) with ei , e j being the word embeddings of
the word i and j from real data. We call W the similarity matrix of
real data.
Then, calculate the similarity matrixW ′ of generated data, where
Wi,′ j = cos(ei′, e j′ ) with ei′ and e j′ being the word embedding of the
word i and j from generated data using the same skip-gram model.
The EmbSim is defined as

Y ∼G θ

For the sake of variance reduction, SeqGAN uses Monte Carlo
search to compute the Q-value for generating each token.
MaliGAN. The basic structure of MaliGAN [3] follows that of
the SeqGAN. To stablize the training and alleviate the gradient
saturating problem, MaliGAN rescales the reward in a batch with
size m by
r D (x i )
rD (x i ) ′ = Ím
− b,
r (x )
f =1 D f

EmbSim = log

where r D (·) is the reward function from the discriminator, b is the
moving average of r D (·) as the baseline.

N
Õ
i=1


cos(Wi′,Wi )/N ,

where N is the total number of words, and Wi and Wi′ denote the
i-th column of W and W ′ respectively.

RankGAN. RankGAN [10] replaces the discriminator of SeqGAN
with a ranker R ϕ , which optimizes the ranking loss




Lϕ = E
log R ϕ (s |U , C − ) − E log R ϕ (s |U , C + ) ,
s∼pdata

Metrics

Texygen implements five text generation metrics so far, covering
various aspects as categorized below. It also provides user-friendly
APIs to retrieve results of their own models and generated text.

2.2.2 Likelihood-based Metrics. We can design metrics to evaluate how good data and model is fitted by measuring the likelihood.
These metrics require details about not only data but the model as
well.

s∼G θ

where


exp(γ α(s |u))
R ϕ (s |U , C) = log Í
′
s ′ ∈C exp(γ α(s |u))

NLL-oracle. Negative log-likelihood (NLL) is originally introduced
in SeqGAN [15], which is specifically applied on synthetic data
experiment and tells how good the generated data is fitted by the
oracle language model. In NLLoracle , a randomly initialized LSTM
is regarded as a true model, i.e., the oracle. Text generation models
need to minimize average negative log-likelihood of generate data
on oracle LSTM, i.e. Ex ∼q log p(x), where x denotes the generated
data.
Since an LSTM is regarded as a true model, the metric can calculate the average loss on every sentence, word by word

α(s |u) = cos(ys , yu ).
GSGAN. Gumbel Softmax trick is a reparametrization trick used
for gradient estimation [8]. It claims that
arg max [softmax(h + д)] ∼ softmax(h),
where д is a Gumbel distribution with zero mean and unit variance.
Note that since this process is differentiable, thus backpropagation
can be directly applied to optimize the GAN objective.
TextGAN. Adversarial feature matching for text generation [16]
proposes a method that optimizes the MMD loss, which is the
reconstructed feature distance, by adding a reconstruction term in
the objective, i.e.,

NLLoracle = −EY1:T ∼G

T
hÕ
θ

t =1

i
log(G oracle (yt |Y1:t −1 )) ,

where G oracle denotes the oracle LSTM, and G θ denotes the generative model.

L recon = ∥z − ẑ ∥.

1098

Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

GAN type

gsgan

textgan

leakgan

maligan

rankgan

seqgan

ƽ

14

NLL-oracle loss

12

Figure 1: Texygen architecture.

T
hÕ
real

t =1

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

6
150

100

50

0

epochs

Figure 2: NLL-oracle loss comparison throughout training.

i
log(G θ (yt |Y1:t −1 )) ,

GAN type

ƽ

gsgan

textgan

leakgan

maligan

rankgan

seqgan

40

where G real denotes the distribution of real data.
NLLtest can only be applied to autoregressive generator like RNN
since G θ (yt |Y1:t −1 ) is involved to calculate the likelihood of certain
word based on previous ones given a generator.

NLL−test loss

30

2.2.3 Diversity based Metrics. GAN models often suffer from
mode collapse problems, which lead to generator collapsing to
produce only a single sample or a small family of very similar
samples. Thus, in open-domain text generation tasks, we include
metrics that encourage to generate more diverse patterns.

20

10
ƽ

Self-BLEU. We propose Self-BLEU, a metric to evaluate the diversity of the generated data. Since BLEU aims to assess how similar
two sentences are, it can also be used to evaluate how one sentence
resembles the rest in a generated collection. Regarding one sentence
as hypothesis and the others as reference, we can calculate BLEU
score for every generated sentence, and define the average BLEU
score to be the Self-BLEU of the document.
A higher Self-BLEU score implies less diversity of the document,
and more serious mode collapse of the GAN model.

3

ƽ

8

NLL-test. We propose NLLtest , a simple metric evaluating the
model’s capacity to fit real test data, which is dual to NLLoracle .
NLLtest = −EY1:T ∼G

ƽ

10

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

0

ƽ

ƽ

ƽ

50

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

100

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

150

epochs

Figure 3: NLL-test loss comparison throughout training.
20,000 sentence from the image COCO captions2 , with half of them
as the training set, the rest as the test set.
GAN Setting. The default initial parameter of all generator follows a Gaussian distribution N (0, 1). We use MLE training as the
pretraining process for all baseline models except GSGAN, which
requires no pretraining. In pretraining, we first train 80 epochs for
a generator, and then 80 epochs for a discriminator. The adversarial
training comes next. In each adversarial epoch, we update the generator once and then update the discriminator for 15 mini-batch
gradients. In LeakGAN, after every 10 adversarial epochs, there are
5 MLE training epochs for both the generator and the discriminator.
The total number of adversarial training epochs is 100.

PLATFORM ARCHITECTURE

Texygen is implemented over TensorFlow [1]. As shown in Fig. 1,
the system consists of two parts with three major classes, highly
decoupled with each other, and easy for customization.
In the utils part, we provide user Metrics class and Oracle class.
The former has three subclasses designed for calculating BLEU
score, NLL loss and EmbSim, while the latter one enables user to
initialize three different types of Oracle: LSTM-based, GRU-based
and SRU-based. The default oracle is LSTM.
In the model part, we enable users to begin the training process
by only interacting with the GAN class without concerning about
the classes for the generator, the discriminator and the reward
(for RL-based GANs). Texygen also provides two different types of
training processes in the GAN class: synthetic data training and
real data training. The former one uses the oracle LSTM to generate
data, while the latter one uses real-world datasets.

Evaluation Metrics. NLLoracle and NLLtest are applied to synthetic data training. Since the oracle LSTM cannot generate words
with semantic meaning, we do not calculate the BLEU score or
EmbSim on synthetic data. On the other hand, the BLEU score, the
Self-BLEU score and EmbSim are applied to real data training.

4.2

Synthetic Data Experiment

The training curves of NLLoracle and NLLtest are depicted in Figs. 2
and 3 respectively. LeakGAN converges more quickly and achieves
good performance on these two metrics. TextGAN gets best NLLoracle
results, while gets worst NLLtest performance on pretraining epochs.
Due to model similarity, SeqGAN, MaliGAN and RankGAN have
almost identical curves until adversarial epochs, after which MaliGAN becomes less competitive compared to the other two.

4 EXPERIMENT
4.1 Training Setting
Data. In our synthetic data training, the total number of words is
set to be 5,000 and the sentence length is set to be 20, and the oracle
will generate 10,000 sentences. In the real data training, we select

2 http://cocodataset.org/

1099

Short Research Papers II

GAN type

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

seqgan

ƽ

rankgan

maligan

leakgan

Table 3: Self-BLEU score
BLEU-2 BLEU-3 BLEU-4
SeqGAN
0.950
0.840
0.670
MaliGAN
0.918
0.781
0.606
RankGAN
0.959
0.882
0.762
LeakGAN
0.966
0.913
0.848
TextGAN
0.942
0.931
0.804
MLE
0.916
0.769
0.583

textgan

Embedding Similarity

-0.010
ƽ

ƽ

ƽ

ƽ

ƽ
ƽ

ƽ

ƽ

ƽ

-0.015

ƽ

ƽ

ƽ

ƽ

ƽ
ƽ

ƽ

ƽ

ƽ

ƽ
ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ
ƽ

ƽ

ƽ

ƽ

ƽ

ƽ

ƽ
ƽ

-0.020
ƽ

-0.025

5

-0.030
0

50

100

CONCLUSION AND FUTURE WORK

Texygen is a text generation benchmarking platform enabling researchers to evaluate their own models and compare them with
existing baseline models fairly and conveniently from different perspectives. Texygen has already designed and implemented various
evaluation metrics in order to provide a comprehensive benchmark.
We also discovered that some classical metrics, like context free
grammar (CFG) used in some related work [8], unable to distinguish different models and is even prone to favoring the ones with
more severe mode collapse, as these models may only learn a few
patterns. For the future work of this project, we will keep updating
new models and designing novel metrics for better benchmarking
the text generation tasks.

150

epochs

Figure 4: EmbSim comparison throughout training.
Table 1: BLEU score on training data
BLEU-2 BLEU-3 BLEU-4 BLEU-5
SeqGAN
0.917
0.747
0.530
0.348
MaliGAN
0.887
0.697
0.482
0.312
RankGAN
0.937
0.799
0.601
0.414
LeakGAN
0.926
0.816
0.660
0.470
TextGAN
0.650
0.645
0.569
0.523
MLE
0.921
0.768
0.570
0.392

Acknowledgment. The work is sponsored by National Natural
Science Foundation of China (61632017, 61702327, 61772333), Shanghai Sailing Program (17YF1428200), MSRA Collaborative Research
and DIDI Collaborative Research Funds.

Table 2: BLEU score on test data
BLEU-2 BLEU-3 BLEU-4 BLEU-5
SeqGAN
0.917
0.747
0.530
0.348
MaliGAN
0.887
0.697
0.482
0.312
RankGAN
0.937
0.799
0.601
0.414
LeakGAN
0.926
0.816
0.660
0.470
TextGAN
0.650
0.645
0.569
0.523
MLE
0.921
0.768
0.570
0.392

4.3

BLEU-5
0.489
0.437
0.618
0.780
0.746
0.408

REFERENCES
[1] Martín Abadi et al. 2016. TensorFlow: A System for Large-Scale Machine Learning.. In OSDI, Vol. 16. 265–283.
[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint
arXiv:1409.0473 (2014).
[3] Tong Che, Yanran Li, Ruixiang Zhang, R Devon Hjelm, Wenjie Li, Yangqiu Song,
and Yoshua Bengio. 2017. Maximum-Likelihood Augmented Discrete Generative
Adversarial Networks. arXiv preprint arXiv:1702.07983 (2017).
[4] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial
nets. In Advances in neural information processing systems. 2672–2680.
[5] Jiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Jun Wang, and Yong Yu. 2017.
Long Text Generation via Adversarial Training with Leaked Information. arXiv
preprint arXiv:1709.08624.
[6] Ferenc Huszár. 2015. How (not) to Train your Generative Model: Scheduled
Sampling, Likelihood, Adversary? arXiv preprint arXiv:1511.05101 (2015).
[7] Fred Jelinek, Robert L Mercer, Lalit R Bahl, and James K Baker. 1977. Perplexity a measure of the difficulty of speech recognition tasks. JASA 62, S1 (1977).
[8] Matt J Kusner and José Miguel Hernández-Lobato. 2016. Gans for sequences
of discrete elements with the gumbel-softmax distribution. arXiv preprint
arXiv:1611.04051 (2016).
[9] Jiwei Li, Will Monroe, Tianlin Shi, Alan Ritter, and Dan Jurafsky. 2017. Adversarial
learning for neural dialogue generation. arXiv preprint arXiv:1701.06547 (2017).
[10] Kevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang, and Ming-Ting Sun. 2017.
Adversarial Ranking for Language Generation. arXiv preprint arXiv:1705.11001
(2017).
[11] Tomáš Mikolov, Martin Karafiát, Lukáš Burget, Jan Černockỳ, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Eleventh
Annual Conference of the International Speech Communication Association.
[12] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a
method for automatic evaluation of machine translation. In ACL. 311–318.
[13] Jun Wang, Lantao Yu, Weinan Zhang, Yu Gong, Yinghui Xu, Benyou Wang, Peng
Zhang, and Dell Zhang. 2017. Irgan: A minimax game for unifying generative
and discriminative information retrieval models. In SIGIR. ACM, 515–524.
[14] Ronald J Williams. 1992. Simple statistical gradient-following algorithms for
connectionist reinforcement learning. Machine learning 8, 3-4 (1992), 229–256.
[15] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. 2017. SeqGAN: Sequence
Generative Adversarial Nets with Policy Gradient.. In AAAI. 2852–2858.
[16] Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, and
Lawrence Carin. 2017. Adversarial Feature Matching for Text Generation. arXiv
preprint arXiv:1706.03850 (2017).

Real Data Experiment

The training curves of EmbSim is depicted in Fig. 4. LeakGAN
achieves very high similarity at the beginning epochs while TextGAN
has rather slow improvement. All GAN models achieve the best
results on pretraining steps. Once the adversarial training starts,
only LeakGAN still maintains its EmbSim score, while other baseline models’ EmbSim scores decrease compared to the pretraining
epochs. In this part, GSGAN is excluded, since it fails to generate
any sentences with semantics in our experiment. The generated
instances can be accessed from Texygen webpage.
The BLEU score on training data, test data is shown in Tables 1
and 2 respectively. LeakGAN outperforms other baseline models
on the metric, results on test dataset shows it has rather good
generalization capacity. MaliGAN has the lowest BLEU score.
The Self-BLEU score is shown in Table 3. It is clear that all models
generate less diverse documents compared to original training
data. LeakGAN and TextGAN suffer more serious mode collapse
problem compared to the other models, while MLE and MaliGAN
can generate documents with the highest diversity.
More detailed empirical study will be made available on Texygen
project webpage. For instance, LeakGAN tends to generate longer
sentences, while TextGAN is prone to generating short sentences.

1100

