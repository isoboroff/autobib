Tutorial

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

A Tutorial on Probabilistic Topic Models for
Text Data Retrieval and Analysis
ChengXiang Zhai

Chase Geigle

University of Illinois at Urbana-Champaign
USA
czhai@illinois.edu

University of Illinois at Urbana-Champaign
USA
geigle1@illinois.edu

ABSTRACT

ACM Reference Format:
ChengXiang Zhai and Chase Geigle. 2018. A Tutorial on Probabilistic Topic
Models for Text Data Retrieval and Analysis. In SIGIR ’18: The 41st International ACM SIGIR Conference on Research & Development in Information
Retrieval, July 8–12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA,
3 pages. https://doi.org/10.1145/3209978.3210189

As text data continues to grow quickly, it is increasingly important
to develop intelligent systems to help people manage and make
use of vast amounts of text data (“big text data”). As a new family
of effective general approaches to text data retrieval and analysis,
probabilistic topic models—notably Probabilistic Latent Semantic
Analysis (PLSA), Latent Dirichlet Allocations (LDA), and their many
extensions—have been studied actively in the past decade with
widespread applications. These topic models are powerful tools for
extracting and analyzing latent topics contained in text data; they
also provide a general and robust latent semantic representation
of text data, thus improving many applications in information retrieval and text mining. Since they are general and robust, they can
be applied to text data in any natural language and about any topics.
This tutorial systematically reviews the major research progress
in probabilistic topic models and discuss their applications in text
retrieval and text mining. The tutorial provides (1) an in-depth
explanation of the basic concepts, underlying principles, and the
two basic topic models (i.e., PLSA and LDA) that have widespread
applications, (2) an introduction to EM algorithms and Bayesian inference algorithms for topic models, (3) a hands-on exercise to allow
the tutorial attendants to learn how to use the topic models implemented in the MeTA Open Source Toolkit and experiment with
provided data sets, (4) a broad overview of all the major representative topic models that extend PLSA or LDA, and (5) a discussion of
major challenges and future research directions.

1

MOTIVATION

Text data include all kinds of natural language text such as web
pages, news articles, scientific literature, emails, enterprise documents, and social media posts. In contrast to non-textual data which
are commonly generated by physical devices, text data are generated by humans and meant to be consumed by humans. Due to the
rapid growth of text data, we can no longer digest all the relevant
information in a timely manner. Thus there is a pressing need for
developing intelligent software tools to help people manage and
make use of vast amounts of text data (“big text data”) for various
tasks, especially those involving complex decision-making. Logically, to harness big text data, we would need to first identify the
relevant text data to a particular application problem (i.e., perform
text data retrieval) and then analyze the identified relevant text
data in more depth to extract any needed knowledge for a task (i.e.
text data analysis).
Due to the difficulty of natural language understanding for computers, the approaches that work well for text retrieval and text
analysis tend to be statistical approaches. In the past decade, a
class of statistical approaches called probabilistic topic models—
represented primarily by Probabilistic Latent Semantic Analysis
(PLSA) [2], Latent Dirichlet Allocation (LDA) [1], and their numerous extensions—have been studied actively with widespread
applications. These topic models provide a general and robust latent semantic representation of text data, thus improving many
applications in information retrieval and text mining. Since they are
general and robust, they can be applied to text data in any natural
language with any topic coverage.
Recent years have seen increasing uses of topic models for solving information retrieval problems and developing techniques for
new retrieval applications. Given that the PLSA paper is the top
paper in the top-10 most-cited papers listed in the SIGIR section of
ACM Digital Library1 , topic models likely will attract increasing
attention in the SIGIR community. In SIGIR 2017, the first presenter
of this tutorial (ChengXiang Zhai) gave the first tutorial on this
topic at SIGIR, which was very well attended. However, that tutorial was a half-day tutorial, which could only cover in detail the
basic topic modeling techniques (PLSA/LDA and the EM algorithm)
with the important topic of Bayesian inference algorithms of topic

CCS CONCEPTS
• Information systems → Data mining; Information retrieval;
Retrieval models and ranking; Language models; • Computing methodologies → Topic modeling; Natural language processing;

KEYWORDS
Probabilistic topic models; language models; information retrieval;
text mining; data mining

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-5657-2/18/07.
https://doi.org/10.1145/3209978.3210189

1 see

1395

http://dl.acm.org/sig.cfm?id=SP935&CFID=735079604&CFTOKEN=19334519

Tutorial

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

models left out. Based on this experience and also the suggestion
made by the SIGIR’17 tutorial reviewers of that previous tutorial
proposal, we now offer an extended full-day tutorial so as to further
cover (1) algorithms for Bayesian inference of topic models, (2)
more thorough discussion of applications of topic models, and (3) a
hands-on exercise to allow the tutorial attendants to learn how to
use the topic models implemented in the MeTA Open Source Toolkit
(https://meta-toolkit.org/) [3] and experiment with provided data
sets.

2

the boundary between text retrieval and text mining will not be
drawn rigorously, and the separation of the two is mostly to facilitate understanding of somewhat different flavors of applications of
topic models (they mostly differ in the importance of “query”). This
part will set the context for understanding applications of topic
models in text retrieval and text mining.
3.1.2 Statistical Language Models. This part will give a brief introduction to the general topic of statistical language models (topic
models are a special kind of language models). Some basic concepts
such as likelihood functions, statistical estimation, maximum likelihood estimation, Bayes’ rule and Bayesian estimation would be
introduced to facilitate understanding of probabilistic topic models.

OBJECTIVES

This tutorial will systematically review the major research progress
in probabilistic topic models and discuss their applications in text
retrieval and text mining. It will provide (1) an in-depth explanation
of the basic concepts and underlying principles of the two basic
topic models (i.e., PLSA and LDA) that have widespread applications: Expectation-Maximization (EM) algorithms and Bayesian
inference algorithms for topic models; (2) a broad overview of all
the major representative topic models (which are usually extensions of PLSA or LDA); (3) hands-on exercise to allow participants
to experiment with a topic model using an open source toolkit; and
(4) a discussion of major challenges and future research directions.
The tutorial should be appealing to anyone who would like to
learn about topic models, how and why they work, their widespread
applications, how to implement and use them, and the outstanding
challenges to be solved in terms of research, including especially
graduate students and researchers in both academia and industry
who want to do research in this area to develop new topic models.
The tutorial (especially the hands-on exercise part) should also be
appealing to industry practitioners who want to apply topic models
to solve many application problems.

3

3.2

3.2.1 Probabilistic Latent Semantic Analysis (PLSA). This part
will start from the simplest mixture model with just one topic and
a background language model to gradually introduce more general
mixture models with a detailed and thorough explanation of the
EM algorithm and why it converges. Maximum a Posteriori (MAP)
estimation would also be introduced for extending PLSA with an
informative prior.
3.2.2 Latent Dirichlet Allocation (LDA). This part will discuss
deficiencies of PLSA and how LDA can address the deficiencies.
The likelihood function and generative process of LDA will be
introduced. Notation for the plate representation of graphic models
will be introduced to facilitate understanding of more advanced
models. Inference algorithms for LDA will be explained with a focus
on variational inference and collapsed Gibbs sampling algorithms,
which are the two most popular inference algorithms for LDA and
LDA-based models.

FORMAT AND DETAILED SCHEDULE

The tutorial will be a 6-hour full-day tutorial, with breaks scheduled
by the SIGIR organizers.
The anticipated detailed outline of the tutorial is as follows with
a tentative plan to cover the Background, Basic Topic Models, and
possibly also the Hands-on Exercise in the morning and the rest
(i.e., Applications of Topics Models in Text Retrieval, Applications
of Topic Models in Text Mining, Advanced Topic Models, and Summary) in the afternoon. The Hands-on Exercise part can be flexibly
adjusted as needed to balance the materials to be covered in the
morning and afternoon.

3.1

Basic Topic Models

This part will explain the two basic topic models (i.e., PLSA and
LDA) in sufficient detail to ensure the audience to understand them
well since they are the foundation of most other topic models.

3.3

Hands-on Exercise on LDA

This part will be a short hands-on exercise that the participants
of this tutorial can work on with supervision from the tutorial
instructors. The exercise is based on the implementation of LDA
in the MeTA Toolkit [3], which is an open source toolkit for text
retrieval and text mining. Through this exercise, participants will
learn how to use a topic model to discover and analyze topics from
text data. They will also have an opportunity to experiment with
running LDA on a sample data set so that they can gain hands-on
experience with using a topic model, which should help them apply
topic models to their future applications.

Background

This part will provide any necessary background to the audience
to prepare them for understanding the main content which starts
from the next part.

3.4

3.1.1 Text Data Retrieval and Mining. This part will provide an
overview of text retrieval and text mining and position them in a
unified framework where text retrieval serves for the purpose of
converting the very large raw text collection into a much smaller
more relevant set of documents that would be actually needed for
a particular application (thus avoiding processing of a lot of nonrelevant text data), and text mining intends to further help users
digest the found relevant text data and finish their tasks. Of course,

Applications of Topic Models in Text
Retrieval

This part will review applications of basic topic models (PLSA and
LDA) in text retrieval.
3.4.1 Dimensionality Reduction and Latent Semantic Representation. This part will cover how PLSA/LDA can be used to improve
text representation. They can be used to perform dimensionality

1396

Tutorial

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

reduction so as to obtain a low-dimensional latent semantic representation of text documents, which can then be used for many
applications that rely on vector space representation of text data,
especially in combination with the keyword-based vector space
representation.

3.6.3 Supervised Topic Models. This part will cover topic models
that can model text with companion labeled data (e.g., sentiment
ratings or topic category labels). Since the companion labels or
ratings can provide additional supervision for topic modeling, these
models are often very powerful for discovering very sophisticated
latent patterns related to topics or sentiment.

Illinois at Urbana-Champaign (UIUC), where he also holds a joint
appointment at Carl R. Woese Institute for Genomic Biology, Statistics, and School of Information Sciences. His research interests
include information retrieval, text mining, natural language processing, machine learning, biomedical and health informatics, and
intelligent education systems. He has published over 300 papers in
these areas with high citations. He served as an Associate Editor
of ACM Transactions on Information Systems, and Information
Processing and Management, and Program Co-Chair of NAACL
HLT 2007, ACM SIGIR 2009, and WWW 2015. He is an ACM Fellow
and received numerous awards, including ACM SIGIR Test of Time
Award (three times), the 2004 Presidential Early Career Award for
Scientists and Engineers (PECASE), an Alfred P. Sloan Research
Fellowship, IBM Faculty Award, HP Innovation Research Award,
Microsoft Beyond Search Research Award, UIUC Rose Award for
Teaching Excellence, and UIUC Campus Award for Excellence in
Graduate Student Mentoring. He has two MOOCs on Coursera on
Text Retrieval and Text Mining, respectively. He has given many
tutorials, including a tutorial on Statistical Language Models for
Information Retrieval at HLT-NAACL 2004, SIGIR 2005, SIGIR 2006,
HLT-NAACL 2007, and 2011 CCF Advanced Disciplines Lectures, a
tutorial on Axiomatic Analysis and Optimization of Information
Retrieval Models at ICTIR 2013 and SIGIR 2014, a tutorial on Statistical Language Models for Text Data Mining at 2012 CCF Advanced
Disciplines Lectures, Statistical Methods for Mining Big Text Data
at 2014 PhD School, Queensland University, Australia, a tutorial on
Probabilistic Topic Models for Text Data Retrieval and Analysis at
SIGIR 2017, and a KDD 2017 hands-on tutorial on MeTA Toolkit for
Text Retrieval and Mining.
Chase Geigle (https://chara.cs.illinois.edu/sites/cgeigle/) is a PhD
candidate in Computer Science at the University of Illinois at UrbanaChampaign (UIUC), where he also received his B.S. degree. He is
co-owner of the MeTA toolkit organization, taught a hands-on
tutorial on MeTA at KDD 2017, and used MeTA in most of his research publications. His research interests include text mining and
information retrieval, especially approaches based on generative
probabilistic models and their applications such as education and
social computing. He served as Publication Co-Chair for CIKM
2016, and in 2015 he was named a fellow under the National Science Foundation Graduate Research Fellowship Program (GRFP).
While attending UIUC, he was the instructor for their CS 225: Data
Structures and Programming Principles for two semesters, CS 528:
Object Oriented Programming and Design for three semesters, and
CS 591txt: Text Mining Seminar for two semesters. Software he
developed to improve office hours and advising efficiency at UIUC
has handled over 100,000 questions and reached over 6,000 unique
students across 35 different course offerings.

3.7

REFERENCES

3.4.2 Topic Models for Ad Hoc Retrieval. This part will cover
how topic models have been used to improve ad hoc retrieval,
mostly involving integration of topic models with language modeling approaches to ad hoc retrieval.
3.4.3 Topic Models for Other Retrieval Tasks. This part will cover
other applications of topic models to retrieval tasks, including e.g.,
subtopic retrieval, summarization, and cross-lingual retrieval etc.

3.5

Applications of Topic Models to Text
Mining

This part will review applications of basic topic models (PLSA and
LDA) in text mining.
3.5.1 Topic Discovery and Analysis. This part will cover how
the basic output from topic models, including word distributions
characterizing topics and topic coverage distributions representing
documents , can be used for applications that require discovery of
latent topics from text data and analyze their patterns.
3.5.2 Topic Labeling and Interpretation. This part will cover
methods for automatically generating labels for interpreting topics
that are represented by word distributions.

3.6

Advanced Topic Models

This part will review a variety of more advanced topic models that
are usually extensions of the basic topic models in interesting ways.
3.6.1 Capturing Topic Structures. This part will cover topic models that introduce topic structures (e.g., hierarchical structures or
correlated topics). These models can discover not only topics but
also their latent structures.
3.6.2 Contextualized Topic Models. This part will cover topic
models extended to model both text data and the companion nontext data such as all kinds of meta-data (e.g., time, location, authors,
sources) as well as complicated context such as social networks.

Summary

[1] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet
Allocation. J. Mach. Learn. Res. 3 (March 2003), 993–1022.
[2] Thomas Hofmann. 1999. Probabilistic Latent Semantic Indexing. In Proceedings of
the 22Nd Annual International ACM SIGIR Conference on Research and Development
in Information Retrieval (SIGIR ’99). ACM, New York, NY, USA, 50–57. https:
//doi.org/10.1145/312624.312649
[3] Sean Massung, Chase Geigle, and ChengXiang Zhai. 2016. MeTA: A Unified
Toolkit for Text Retrieval and Analysis. In Proceedings of ACL-2016 System Demonstrations. Association for Computational Linguistics, Berlin, Germany, 91–96.
http://anthology.aclweb.org/P16-4016

This part will summarize the major points of the tutorial with the
main take-away messages, recommendations for applications of
topic models, and a discussion of remaining challenges and future
research directions.

4

PRESENTERS BIOGRAPHY

ChengXiang Zhai (http://czhai.cs.illinois.edu) is a Professor of
Computer Science and Willett Faculty Scholar at the University of

1397

