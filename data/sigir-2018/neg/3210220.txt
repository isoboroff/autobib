Doctoral Consortium

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Efficiency-Effectiveness Trade-Offs in Machine Learned Models
for Information Retrieval
Luke Gallagher
RMIT University
Melbourne, Australia
luke.gallagher@rmit.edu.au

CCS CONCEPTS

current state-of-the-art LTR methods such as tree boosted rankers.
Some progress toward a generalizable solution appears in Chen
et al. [1], where the authors derive a method of cost-aware feature
selection by first learning a cascade of linear models using SGD as
the optimization step with an ℓ1 regularized cumulative gradient
clipping technique from Tsuruoka et al. [3]. With this machinery
in place the selected stagewise features are used as input to retrain
a cascade of tree boosted rankers. The findings showed that a three
stage λ-Mart cascade offered a better efficiency-effectiveness tradeoff across two datasets, where the work from Wang et al. was more
efficient than effective while the full cost-insensitive tree boosted
models were more effective than efficient.
Neural methods for document ranking have recently become an
active area of research within Information Retrieval. One of the
goals of the proposed research is to compare current state-of-theart neural ranking models to ascertain for which types of queries
they fail. Following this, an extension of neural failure analysis
into associated factors of risk and how they correspond to the risk
found in current LTR models, we believe is currently an important
issue for the IR community. Another issue is the complexity of the
resulting neural models. Recent work from Mitra et al. [2] present
a neural architecture that is composed from a total of 17 neural
network layers, and while it may be effective, there is yet to exist
a body of research that begins to question what are the efficiency
problems that need to be solved, what data structures can assist the
different query-document representations required and how could
they work within a production environment. Modeling the cost
of latent features would be of interest for the cost-aware training
of deep neural networks; determining if new data structures are
required for the efficient storage and processing of the different
representations that are often used (character, word, paragraph
levels), are two issues that would direct some progress toward
better efficiency-effectiveness trade-offs within neural ranking
models. The proposed work aims to use existing neural models,
reproducing them to provide an experimental framework that can
reliably measure the efficiency aspects of these complex systems.

• Information systems → Learning to rank; Retrieval effectiveness; Retrieval efficiency; • Computing methodologies
→ Machine learning;

KEYWORDS
Learning to Rank; Cascade Ranking; Neural Information Retrieval;
Efficiency; Effectiveness
Complex machine learning models are now an integral part of
modern, large scale retrieval systems. However, collection size
growth continues to outpace advances in efficiency improvements
in the learning models which achieve the highest effectiveness. The
current literature on cascade ranking does not address the issue of
how state-of-the-art learning-to-rank (LTR) models can be adapted
to a cascaded architecture. Secondly, complex neural networks for
retrieval tasks are now in their infancy similar to LTR 15 years
earlier and are attaining effective results. New research challenges
arise with these models that require vast amounts of training data.
Most of the current research is focused on constructing new models
that improve effectiveness, leaving the questions of explainability
and efficiency as open problems. This research proposes to focus on
improving the efficiency-effectiveness trade-offs within machine
learned models for late stage re-ranking and aims to: i) devise a
cascade ranking architecture that can be used with current state-ofthe-art LTR algorithms; ii) study the robustness of current ad-hoc
neural ranking models; iii) propose solutions for current efficiency
challenges in neural ranking models and investigate how featuredriven machine learning and neural methods may be combined for
ranking tasks.
Cascade ranking has recently become an important point of
emphasis within Information Retrieval. Growing collection sizes
continue to curb the advancements of more sophisticated learning
methods where effectiveness continues to improve. Wang et al. [4]
provide one of the earliest works for cascade ranking designed
specifically for IR. The key advantage of such an architecture is
flexibility – an allowance for more fine grained control over the
trade-off between efficiency and effectiveness. However, existing
methods for cascade ranking focus on retrofitting the cascade
approach into an existing algorithm and are not generalizable across

REFERENCES
[1] R.-C. Chen, L. Gallagher, R. Blanco, and J. Culpepper. 2017. Efficient Cost-Aware
Cascade Ranking in Multi-Stage Retrieval. In Proc. SIGIR. 445–454.
[2] B. Mitra, F. Diaz, o, and N. Craswell. 2017. Learning to Match Using Local and
Distributed Representations of Text for Web Search. In Proc. WWW. 1291–1299.
[3] Y. Tsuruoka, J. Tsujii, and S. Ananiadou. 2009. Stochastic Gradient Descent
Training for L1-regularized Log-linear Models with Cumulative Penalty. In Proc.
AFNLP. 477–485.
[4] L. Wang, J. Lin, and D. Metzler. 2011. A Cascade Ranking Model for Efficient
Ranked Retrieval. In Proc. SIGIR. 105–114.

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-5657-2/18/07.
https://doi.org/10.1145/3209978.3210220

1449

