Demonstration Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

CoNEREL: Collective Information Extraction in News Articles
Minh C. Phan and Aixin Sun
School of Computer Science and Engineering, Nanyang Technological University, Singapore
phan0050@e.ntu.edu.sg;axsun@ntu.edu.sg

ABSTRACT

threads where each article comes along with multiple comments is
still lack in previous systems’ consideration.
In this demonstration, we present CoNEREL that focuses on
named entity recognition and linking for news articles including
comments posted by their readers. CoNEREL treats a news article
and its associated comments as a whole, and processes all text
in batch model to make the best use of the shared contexts. In
particular, the context inherited from an article is utilized to detect
and disambiguate mentions in user comments. Specifically, NER in
CoNEREL leverages co-reference of mentions to refine their class
labels of entity mentions in comments.
CoNEREL implements Pair-Linking [10, 11] to disambiguate
each extracted mention to a corresponding entity in Wikipedia.
In simple words, at each step, the algorithm selects a pair of entity mentions with the highest confidence for decision making, i.e.,
linking the pair of mentions to Wikipedia. It finishes linking all
mentions in a document by scanning the pairs of mentions at most
once. Pair-Linking is shown to have state-of-the-art performances
across different benchmark datasets [10, 11]. For news articles and
user comments, it also demonstrates qualitatively reasonable results.
The superior performance of Pair-Linking regardless its simplicity raises several interesting questions, e.g., how does Pair-Linking
maintain relatedness coherence among linked entities? Furthermore,
since Pair-Linking does not allow correction of mistakes (if any)
made in previous linking steps, therefore, other questions of interest are: how often does Pair-Linking make errors? and do these errors
degrade the subsequent linking assignments?
To gain more insights about the algorithm, CoNEREL provides
a visualization of the disambiguation steps made by Pair-Linking.
This visualization helps not only to better understand the linking algorithm, but also the relationships between the named entities mentioned in articles and their comments. The graph formed
through the linking process usually shows a few communities of
the named entities, providing an intuitive way to capture the semantic coherence between entity mentions in an article and in its
comments.
In summary, CoNEREL is different from existing systems in the
following aspects. First, CoNEREL performs named entity recognition and entity linking focusing on news articles and user-generated
comments in a collective manner. Specifically, article contexts are
used as a reliable resource to extract and disambiguate mentions
in user comments. Second, our system implements Pair-Linking
which is a super-fast and effective linking algorithm. The interactive
visualization of Pair-Linking process enables researchers to study
the patterns of entity connections within an article thread, and
understand how Pair-Linking utilizes it to derive accurate linking
assignments. As a result, the graph formed by Pair-Linking process
captures the semantic coherence between entities in article and
comments.

We present CoNEREL, a system for collective named entity recognition and entity linking focusing on news articles and readers’
comments. Different from other systems, CoNEREL processes articles and comments in batch mode, to make the best use of the
shared contexts of multiple news stories and their comments. Particularly, a news article provides context for all its comments. To
improve named entity recognition, CoNEREL utilizes co-reference
of mentions to refine their class labels (e.g., person, location). To
link the recognized entities to Wikipedia, our system implements
Pair-Linking, a state-of-the-art entity linking algorithm. Furthermore, CoNEREL provides an interactive visualization of the PairLinking process. From the visualization, one can understand how
Pair-Linking achieves decent linking performance through iterative evidence building, while being extremely fast and efficient.
The graph formed by the Pair-Linking process naturally becomes a
good summary of entity relations, making CoNEREL a useful tool
to study the relationships between the entities mentioned in an
article, as well as the ones that are discussed in its comments.
ACM Reference Format:
Minh C. Phan and Aixin Sun. 2018. CoNEREL: Collective Information Extraction in News Articles. In SIGIR ’18: The 41st International ACM SIGIR
Conference on Research and Development in Information Retrieval, July 8–
12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https:
//doi.org/10.1145/3209978.3210165

1

INTRODUCTION

Named entity recognition (NER) and entity linking (EL) aim at (i)
identifying entities of interest in text, such as persons, locations,
organizations and, (ii) mapping them to the corresponding entries
in a knowledge base (e.g., Wikipedia). The tasks play essential roles
in information retrieval (IR) and natural language processing (NLP).
The desirable results of NER and EL enable and benefit a myriad of
downstream applications such as relation extraction, co-reference
resolution, document retrieval, and opinion mining. As such, it has
received considerable attention across both industrial and academic
research communities.
As an activate research area for the last decade, various systems
have been developed for NER and EL including Standford NER [5],
NeuroNER [3], FOX [15], TAGME [4], Spotlight [2], Nordlys [7],
and Babelfy [9]. Although these systems are different in term of
technical methods being used, they all perform NER and EL at
either sentence or document level. Therefore, dealing with article
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-5657-2/18/07.
https://doi.org/10.1145/3209978.3210165

1273

Demonstration Papers I

2

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

CONEREL: USER INTERFACES

wider edges for more confidence. Figure 3(b) shows the graph after
all entities are linked. The visual animation illustrates that PairLinking maintains the semantic coherence assumption by growing
multiple entity relatedness trees. Furthermore, the visualization is
also a useful tool to study the semantic relatedness between article
entities and the ones discussed by users in comments.
The complete graph view, Figure 3(b), shows three groups of
entities, professional basketball players (sub-graph on the left),
professional basketball teams (sub-graph on the right), and two
cities. The three sub-graphs provide a concise summary of the
relationships between entities mentioned in the news article. The
graphs may include entities mentioned by readers in comments
which do not appear in the news article. Figure 4 gives an example,
where the entities in comments are with borders in gray color. The
title of the news article is shown above the graph.

CoNEREL provides 4 main functions: collective named entity recognition, entity linking (by Pair-Linking algorithm), Pair-Linking
visualization, and document retrieval. The main web-based GUI is
shown in Figure 1(a) and the system architecture behind the GUI is
presented in Figure 1(b).
Main Interface. The main GUI has two panels. The right panel
shows the news content and its comments if scrolling down. The
named entities recognized from the articles (and comments) are
highlighted in different background colors representing different
mention classes. CoNEREL employs 4-class scheme as in [5]. The
classes of interest are person, organization, location, and miscellany (represents other types of entity mentions such as nationality,
movie, product). Clicking a mention in document (e.g., “Bryant” in
the example article “L. A. Lakers down Pelicans for second straight
NBA win”), a small popup window shows the linked Wikipedia
entry (“Kobe Bryant” in the example) and a short description from
the Wikipedia article.
Panel on the left summarizes the number of entity mentions in
each class, with matching colors. Under the “Entities & Variants”,
the number of times a linked entity is mentioned in the article is
displayed. In this example, “Kobe Bryant” is mentioned 10 times
in different surface forms including 6 mentions of “Bryant”, and 2
mentions of “Kobe Bryant”.

3

ARCHITECTURE AND IMPLEMENTATION

Illustrated in Figure 1(b), CoNEREL is based on a three-layered
architecture which consists of Service, Logic, and Data layers.

3.1

Service Layer and Data Layer

Service Layer. The service layer provides access for end-users
to main functionalities through GUIs. The available services are
named entity recognition, entity linking, Pair-Linking visualization,
and document retrieval. The first three services are detailed through
their interfaces in Section 2. The document retrieval interface allows
users to search for specific news articles from the system.
In terms of implementation, CoNEREL Web is based on Flask
framework.1 The graphical visualization of Pair-Linking makes use
of Cytoscape.js network library [6].

Linking Process Interface. Clicking “Linking Process” from the
left panel in the main GUI will bring out the interface for linking
process. The linking process shows details of decision making on
each mention (or a pair of mentions to be more specific) to its
Wikipedia entry. To help CoNEREL users to fully understand the
challenges of entity linking, i.e., the same surface form may refer
to different named entities, CoNEREL shows candidates of each
mention. Figure 2(a) illustrates several candidate entities for the
mention ‘Pelicans’. The candidate entities are ranked based on the
local confidence scores, computed from the context (or surrounding
words) of the mention.
The linking process shows the detailed steps of executing the
Pair-Linking algorithm. Pair-Linking iteratively selects a pair of
mentions with the highest confidence at each step to disambiguate.
To be detailed shortly, the confidence of a pair linking is based
on the local confidence scores of the two candidates to the two
mentions, and the semantic coherence between the two candidates
estimated from Wikipedia. The linking of “Pelicans” in Figure 2(a)
is performed in Step 35, shown in screen capture Figure 2(b). Figure 2(b) shows that (i) the local confidence of linking mention
“Pelicans” to Wikipedia entry “New Orleans Pelicans” is 0.28, (ii)
the local confidence of linking mention “Houston Rockets” to entry
“Houston Rockets” is 0.98, and (iii) the semantic coherence between
“New Orleans Pelicans” and “Houston Rockets” is 0.87.

Data Layer. Data layer provides methods to access several resources including article documents, entity information, and pretrained models. Specifically, news articles and comments are stored
in JSON-formatted files. Indexes extracted from Wikipedia such as
entity names, entity popularity, mention-entity mapping are stored
in raw-text files. The data layer also includes entity retrieval function which is in-charge of querying Google knowledge graph API to
obtain updated entity information (e.g., entity descriptions, entity
profile images). Moreover, pre-trained models such as Standford
NER, embeddings, and local confidence model are also attached in
this data layer.

3.2

Logic Layer

The logic layer contains detailed implementations of document
pre-processing, recognition, and linking modules.
Pre-Processing module. Recall that CoNEREL processes all text
in batch model to make the best use of the shared contexts for NER
and EL. The pre-process module is in-charge of parsing article documents, separating comment texts from article texts, tokenization,
and extracting features for later usages.

Graph View Interface. While the textual visualization in “Linking
Process” represents details about each linking step, “Graph View”,
on the other hand, provides an interactive animation of the PairLinking process, from a bird’s eye view. Figure 3(a) shows the
status of the entity linking graph after the 7th step, where the left
panel lists the details of the 7 steps, and the right panel shows the
7 edges in the graph. The edges linked in the earlier steps have

Recognition module. This module extracts named entity mentions in articles and users’ comments. NER in CoNEREL is based on
a pre-trained Stanford NER model [5] with a step of label refinement.
1 http://flask.pocoo.org/

1274

Demonstration Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Preprocess

CoNEREL Web UI

Named Entity
Recognition

Named Entity
Recognition

Entity
Retrieval

Google
KG API

Label
Refinement

Indexes

Files

Documents

JSON

Pretrained
Models

Files

Entity Linking
(Pair-Linking)

Recognition

Pair-Linking
Visualization

Candidate
Generation

Document
Retrieval

Local Confidence
Estimation
Pair-Linking
Linking

Service

(a) The main Web UI of CoNEREL.

Logic

Data

(b) CoNEREL architecture.

Figure 1: The main Web UI and CoNEREL’s system architecture.
Candidate generation returns a list of candidate entities that
possibly match with a mention’s surface form. In CoNEREL, the retrieval of the candidate list is based on indexes of entities’ names and
their variants (collected from hyper-linked mentions in Wikipedia).
Furthermore, CoNEREL makes use of Google Knowledge Graph API
to obtain additional candidate entities to supplement the candidate
list.
Local confidence estimation calculates the local confidence score
for each candidate entity, given a mention and its local context as
input. We train a learning-to-rank model for this purpose. The set
of features used to train the model is similar to the one reported
in [11]. It includes surface form similarities between the mention’s
and the candidate entity’s names, entity popularity-based features,
similarities between the mention’s local context and the candidate entity’s description. The model is trained on several publicly
available datasets: N 3 dataset [13], MSNBC [1], ACE2004 [12], and
KORE50 [8]. The datasets contain both news articles and short texts
(e.g., RSS).
Regarding the linking decisions, not solely relying on the local
context of the entity mention, CoNEREL also utilizes the semantic
coherence between linked entities to improve the linking performance. This is known as collective entity linking in literature [14].
It is based on the assumption that all entities mentioned within a
document should be densely connected in the knowledge base. In
our recent work, we show that the semantic relationships between
mentioned entities in a document are in fact less dense than expected [11]. As a remedy, we propose Pair-Linking. The algorithm
works on the intuition that each linking only needs to be coherent
with another linking. Specifically, mi is linked to ei (denoted by
mi 7→ ei ), if there is another supporting assignment m j 7→ e j such
that ei and e j are strongly semantically related.
Instead of considering all the given mentions, Pair-Linking iteratively identifies and resolves a pair of mentions at each step, starting
from the most confident pair. The pairwise confidence score for a
pair of linking assignments is defined as a linear combination of local confidences and semantic relatedness of two candidate entities,

(a) List of candidate entities for mention ‘Pelicans’. The local confidence scores are
in braces.

(b) An example of a linking step in Pair-Linking. Two mentions are presented in
local contexts, with local confidences and semantic coherence of two candidate
entities.

Figure 2: The linking process interface shows candidate entities of each mention, and the steps of making decision on
each pair of mentions, based on local confidences and semantic coherence.
Compare to NER in formal text, mention recognition in comments
is more challenging because of the short and noisy nature of the
user-generated content. To improve the NER performance in user
comments, CoNEREL implements mention type refinement. The
component aims to correct misclassified mentions in comments
based on co-references of mentions (e.g., having the same surface
form, similar context, etc.). For example, the recognition of mention
‘Kobe Bryant’ with label ‘Person’ can assist the detection and typing
of another co-referential mention ‘kobe’.
Linking module. Entity linking assigns each extracted mention
(obtained from the recognition step) to its corresponding entity in
Wikipedia. It consists of three sub-modules: candidate generation,
local confidence estimation, and Pair-Linking.

1275

Demonstration Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

(a) Graph view at the 7th linking step

(b) Complete graph view, showing node details

Figure 3: Graphical visualization of Pair-Linking process, after the 7th linking step, and the complete graph view.
view the linking graph through the graph view interface (Figure 3).
Usually few sub-graphs are showed in this graph view where each
sub-graph illustrates the relationship of a subset of entities mentioned in the news article and its comments. Interesting insights
and potential improvements can be drawn by observing results
illustrated by CoNEREL.

ACKNOWLEDGEMENTS
This work was supported by Singapore Ministry of Education Research Fund MOE2014-T2-2-066.

REFERENCES

Figure 4: Entities in comments are with gray border.

[1] Silviu Cucerzan. 2007. Large-Scale Named Entity Disambiguation Based on
Wikipedia Data. In EMNLP-CoNLL. 708–716.
[2] Joachim Daiber, Max Jakob, Chris Hokamp, and Pablo N. Mendes. 2013. Improving
efficiency and accuracy in multilingual entity extraction. In SEMANTiCS. 121–
124.
[3] Franck Dernoncourt, Ji Young Lee, and Peter Szolovits. 2017. NeuroNER: an
easy-to-use program for named-entity recognition based on neural networks. In
EMNLP System Demonstrations. 97–102.
[4] Paolo Ferragina and Ugo Scaiella. 2010. TAGME: on-the-fly annotation of short
text fragments (by wikipedia entities). In CIKM. 1625–1628.
[5] Jenny Rose Finkel, Trond Grenager, and Christopher D. Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs
Sampling. In ACL. 363–370.
[6] Max Franz, Christian Tannus Lopes, Gerardo Huck, Yue Dong, Selçuk Onur Sümer,
and Gary D. Bader. 2016. Cytoscape.js: a graph theory library for visualisation
and analysis. Bioinformatics 32, 2 (2016), 309–311.
[7] Faegheh Hasibi, Krisztian Balog, Darío Garigliotti, and Shuo Zhang. 2017. Nordlys:
A Toolkit for Entity-Oriented and Semantic Search. In SIGIR. 1289–1292.
[8] Johannes Hoffart, Stephan Seufert, Dat Ba Nguyen, Martin Theobald, and Gerhard
Weikum. 2012. KORE: keyphrase overlap relatedness for entity disambiguation.
In CIKM. 545–554.
[9] Andrea Moro, Francesco Cecconi, and Roberto Navigli. 2014. Multilingual Word
Sense Disambiguation and Entity Linking for Everybody. In ISWC Posters &
Demonstrations. 25–28.
[10] Minh C. Phan, Aixin Sun, Yi Tay, Jialong Han, and Chenliang Li. 2017. NeuPL:
Attention-based Semantic Matching and Pair-Linking for Entity Disambiguation.
In CIKM. 1667–1676.
[11] Minh C. Phan, Aixin Sun, Yi Tay, Jialong Han, and Chenliang Li. 2018. PairLinking for Collective Entity Disambiguation: Two Could Be Better Than All.
CoRR abs/1802.01074 (2018).
[12] Lev-Arie Ratinov, Dan Roth, Doug Downey, and Mike Anderson. 2011. Local and
Global Algorithms for Disambiguation to Wikipedia. In ACL. 1375–1384.
[13] Michael Röder, Ricardo Usbeck, Sebastian Hellmann, Daniel Gerber, and Andreas
Both. 2014. N3 - A Collection of Datasets for Named Entity Recognition and
Disambiguation in the NLP Interchange Format. In LREC. 3529–3533.
[14] Wei Shen, Jianyong Wang, and Jiawei Han. 2015. Entity Linking with a Knowledge
Base: Issues, Techniques, and Solutions. IEEE TKDE 27, 2 (2015), 443–460.
[15] René Speck and Axel-Cyrille Ngonga Ngomo. 2014. Named Entity Recognition
using FOX. In ISWC Posters & Demonstrations. 85–88.

express as follows:


ϕ(mi 7→ ei ) + ϕ(m j 7→ e j )
con f (mi 7→ ei , m j 7→ e j ) =
+ψ (ei , e j )
2
where mi 7→ ei and m j 7→ e j denote a pair of linking assignments.
ϕ(.) is local confidence score. ψ (., .) returns semantic relatedness
score for two candidate entities, estimated through cosine similarity
of their entity embeddings as in [10, 11].
In implementation, Pair-Linking uses a priority queue to store
and query the most confident linking pairs at each step. Furthermore, early stop is implemented to skip the pairs which are less confident when initializing this priority queue. Therefore, Pair-Linking
results in very fast and efficient performance in comparison to other
collective linking methods.

4

DEMONSTRATION SCENARIO

In our demonstration, CoNEREL is preloaded with 500 news articles collected from Yahoo! news with their associated comments.
CoNEREL starts with a randomly sampled article to show in its
main interface (e.g., Figure 1(a)). To change to another article, a user
may search for an article through a keyword query or select it from
the article list. From the main interface, the user may choose to
view the detailed linking process through textual visualization. In
this interface, every specific linking decision is explained through
the list of candidate entities for a mention (Figure 2(a)), the local
contexts, as well as the local confidence scores and semantic coherence used for the linking decision (Figure 2(b)). Then, the user may

1276

