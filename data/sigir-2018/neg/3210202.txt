SIRIP: Industry Days

SIGIRâ€™18, July 8-12, 2018, Ann Arbor, MI, USA

Merchandise Recommendation for Retail Events with Word
Embedding Weighted Tf-idf and Dynamic Query Expansion
Ted Tao Yuan

Zezhong Zhang

eBay Inc.
San Jose, California, USA
teyuan@ebay.com

eBay Inc.
San Jose, California, USA
zezzhang@ebay.com
Fig.1 shows the process. Item titles (prefixed with category
name) as bag of words (BOW), are matched case-insensitively
with selected event keywords (partial or all). We rank all
retrieved items by the sum of tf-idf scores from matched words,
and keep the items with total tf-idf scores above a threshold. The
retrieval based system works well to discover relevant
merchandises for any event with given seed keywords.

ABSTRACT
To recommend relevant merchandises for seasonal retail events,
we rely on item retrieval from marketplace inventory. With
feedback to expand query scope, we discuss keyword expansion
candidate selection using word embedding similarity, and an
enhanced tf-idf formula for expanded words in search ranking.

KEYWORDS
eCommerce; recommendation; algorithm; relevance; query
expansion; word embedding
ACM Reference Format
Ted Tao Yuan and Zezhong Zhang. 2018. Merchandise Recommendation
for Retail Events with Word Embedding Weighted Tf-idf and
Dynamic Query Expansion. In SIGIR â€™18: The 41st International ACM
SIGIR Conference on Research and Development in Information Retrieval,
July 8â€“12, 2018, Ann Arbor, Michigan, USA. 2 pages.
https://doi.org/10.1145/3209978.3210202

Figure 1: An event relevant item recommendation system.
Word embedding [1], is an effective way to generate vector
representations of words given a large text corpus. It preserves
the linguistic context of words in the item title corpus. Word2vec
[2] provides likelihood probability score, or similarity, if two
words i and j tend to stay together in the item corpus. The score
is bounded and computed with cosine distance,

1 INTRODUCTION
On eBay, sellers tend to include seasonal event words in listing
titles, i.e., jewelry for Valentineâ€™s Day gift. Such items tend to sell
well during the event time period. A retail event model
comprises the eventâ€™s seasonal time window, and its relevance
context in terms of keywords. For example, a Valentineâ€™s Day
retail event is generally from mid-January to mid-February, with
curated context keywords like valentine, jewelry, etc. In this
paper we discuss a way to expand beyond the given context
keywords using word embedding, and incorporate query
relevance weight in item retrieval ranking for event based
merchandise recommendation.

ğ‘ ğ‘–ğ‘š(ğ‘–, ğ‘—) = ğ‘ğ‘œğ‘ ğ‘–ğ‘›ğ‘’ ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’(ğ‘’ğ‘šğ‘ğ‘’ğ‘‘(ğ‘–), ğ‘’ğ‘šğ‘ğ‘’ğ‘‘(ğ‘—))

(1)

In our case, it can be used to compute the probability of a word
occurring in the neighborhood of given event context keywords.

2.2 Expand Event Keyword Scope
One of the issues with the above simple item retrieval based
recommendation is that it misses items that relate to an event
but the titles do not contain the event seed keywords. For
example, some European sellers spell â€œjewelryâ€ as â€œjewelleryâ€ in
their items. More importantly, word meaning changes from
season to season that leads to different merchandises, see Table 1
for examples. Word embedding can capture the changing
seasonal synonyms of event keywords if we train embedding
models on items sold during the months of a season.

2 APPROACH
2.1 Item Retrieval And Word Embedding
Given event context keywords, i.e., jewelry for Valentineâ€™s Day,
we want to retrieve all relevant items sold in the past during the
event time period.

Table 1: Similar Words by Season

Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for third-party
components of this work must be honored. For all other uses, contact the
owner/author(s).
SIGIR '18, July 8â€“12, 2018, Ann Arbor, MI, USA
Â© 2018 Copyright is held by the owner/author(s).
ACM ISBN 978-1-4503-5657-2/18/07. https://doi.org/10.1145/3209978.3210202

Word

Month

valentine

February

cupid, hearts, conversation

August

happy, birthday, graduation

school

1347

Similar Words

April

yearbook, backpacks, college

August

backpacks, schoolbag, bookbag

SIRIP: Industry Days

SIGIRâ€™18, July 8-12, 2018, Ann Arbor, MI, USA

With embedding, we could explore related merchandise in
the embedding space using algorithms like word mover distance
[3], weighted word embedding aggregation [4], et. al. However,
we want more control of the process to separate item retrieval
from embedding quality, and combine them via query expansion.

ğ‘ƒ(ğ‘Ÿğ‘’ğ‘™ | ğ‘‘, ğ‘) âˆğ‘ âˆ‘ ğ‘¡ğ‘“(ğ‘‘, ğ‘–) Ã— ğ‘–ğ‘‘ğ‘“(ğ‘–) Ã— ğ›¿(ğ‘–, ğ‘)
ğ‘–âˆˆğ‘âˆªğ‘ â€²

where ğ›¿(ğ‘–, ğ‘) = 1 if ğ‘– âˆˆ ğ‘; and ğ›¿(ğ‘–, ğ‘) = ğ‘†(ğ‘–, ğ‘) if ğ‘– âˆˆ ğ‘â€². We call
it word embedding weighted tf-idf for query expanded words.

4 EXPERIMENTS AND RESULTS

3 DYNAMIC QUERY EXPANSION WITH
EMBEDDING AND WEIGHTED TF-IDF

eBay has vast and diverse online merchandises. It is desirable to
classify inventory in the online marketplace by year round
seasonal and social events. We built our item retrieval system on
Hadoop/Spark platform that allows us to access billions of items
during past seasons. We segment sold items by month, and train
word embedding model for each month to capture seasonal word
synonyms. For a retail event, we search relevant items by
matching expanded event keywords with item titles, and rank
the items with Formula (6). Items with scores above a threshold
are selected for further recommendation processing.
In our case of given event context seed keywords, we focus
more on expanding the inventory coverage for an event. Albeit
subjective in recall judgement, by systematically adding strongly
correlated words that are selected through seasonal word
embedding similarity, we can gradually increase relevant
inventory scope for recommendation, see Table 2 for examples.

As discussed in the Probabilistic Relevance Framework (PRF) [5],
query expansion arises from relevance feedback. In our item
recommendation case we want to include additional words with
most similar meanings during the event time window.
In PRF, the offer weight of a query expansion word candidate
is proportional to the wordâ€™s document weight and its relevance
to the query intent. The offer weight was used to rank and select
query expansion candidates. The selected words along with the
original query are included in normal retrieval and ranking.
In our case, event seed keywords define the query intent. We
want other highly related words to be included in the retrieval
and ranking, but controlled by the word correlation weight.
Using same notation as in [5], i.e., âˆğ‘ means rank equivalence,
the probability of relevance of document (item title) d and query
q is
ğ‘ƒ(ğ‘‘|ğ‘Ÿğ‘’ğ‘™, ğ‘)
ğ‘ƒ(ğ‘‡ğ¹ğ‘– = ğ‘¡ğ‘“ğ‘– | ğ‘Ÿğ‘’ğ‘™, ğ‘)
ğ‘ƒ(ğ‘Ÿğ‘’ğ‘™ | ğ‘‘, ğ‘) âˆğ‘
â‰ˆâˆ
Ì…Ì…Ì…Ì… , ğ‘)
Ì…Ì…Ì…Ì… , ğ‘)
ğ‘ƒ(ğ‘‘|ğ‘Ÿğ‘’ğ‘™
ğ‘ƒ(ğ‘‡ğ¹ğ‘– = ğ‘¡ğ‘“ğ‘– | ğ‘Ÿğ‘’ğ‘™
(2)

Table 2: Expansion Examples and Recall Impact
Seed Query

ğ‘–âˆˆğ‘‰

â‰ˆâˆ
ğ‘–âˆˆğ‘

ğ‘ƒ(ğ‘‡ğ¹ğ‘– = ğ‘¡ğ‘“ğ‘– |ğ‘Ÿğ‘’ğ‘™, ğ‘)
ğ‘ƒ(ğ‘‡ğ¹ğ‘– = ğ‘¡ğ‘“ğ‘– |ğ‘Ÿğ‘’ğ‘™, ğ‘)

âˆğ‘ âˆ‘ ğ‘™ğ‘œğ‘”
ğ‘–âˆˆğ‘

Ã—âˆ
ğ‘—âˆˆğ‘â€²

ğ‘ƒ(ğ‘‡ğ¹ğ‘— = ğ‘¡ğ‘“ğ‘— |ğ‘Ÿğ‘’ğ‘™, ğ‘)
ğ‘ƒ(ğ‘‡ğ¹ğ‘— = ğ‘¡ğ‘“ğ‘— |ğ‘Ÿğ‘’ğ‘™, ğ‘)

valentines day
jewelry
back to school

(3)

ğ‘ƒ(ğ‘‡ğ¹ğ‘– = ğ‘¡ğ‘“ğ‘– |ğ‘Ÿğ‘’ğ‘™, ğ‘)
ğ‘ƒ(ğ‘‡ğ¹ğ‘– = ğ‘¡ğ‘“ğ‘– |ğ‘Ÿğ‘’ğ‘™, ğ‘)
ğ‘ƒ(ğ‘‡ğ¹ğ‘— = ğ‘¡ğ‘“ğ‘— |ğ‘Ÿğ‘’ğ‘™, ğ‘)
+ âˆ‘ ğ‘™ğ‘œğ‘”
ğ‘ƒ(ğ‘‡ğ¹ğ‘— = ğ‘¡ğ‘“ğ‘— |ğ‘Ÿğ‘’ğ‘™, ğ‘)
ğ‘—âˆˆğ‘â€²

= âˆ‘ ğ‘ˆğ‘– (ğ‘¡ğ‘“ğ‘– ) + âˆ‘ ğ‘ˆ â€²ğ‘— (ğ‘¡ğ‘“ğ‘— , ğ‘)
ğ‘–âˆˆğ‘

(6)

Expanded Word
: Weight
jewellery : 0.93

Recall % Increase
(US site)
< 1%

bookbag : 0.8;
backpacks : 0.7

> 200%

(4)

5 CONCLUSION
For seasonal event recommendation, we create monthly word
embedding models based on item titles. We discuss a way to use
word similarity scores from the word embedding models to rank
and select query expansion candidates with given retail event
seed keywords. Further we enhance tf-idf ranking scores of the
expanded words using the embedding similarity as query
relevance weights. We believe it can be applicable to other
document weight based ranking as well, i.e., replace tf-idf with
BM25. The embedding enhanced retrieval and ranking scheme
helps us discover more relevant merchandises systematically for
event based recommendation.

(5)

ğ‘—âˆˆğ‘â€²

where V is vocabulary, qâ€™ are expanded words in V but not in q.
Given a non-stop-word i in q, candidates for qâ€™ are selected as
1.
compute similarity score (> 0.6) to i from Eq. (1),
2.
rank by the score and select the top k (<5) words.
As in PRF, in Step (2), we assume the conditional probability
of each word can be evaluated independently, and expand the
document probability as product over the words of the
vocabulary (NaÃ¯ve Bayes). Note we keep qâ€™ in Step (3) and use
monotonic transformation in Step (4).
The qâ€™ related second term in Step (5) may be estimated
heuristically through q with a parameter ğ‘†(ğ‘—, ğ‘) for ğ‘— âˆˆ ğ‘â€²,
ğ‘ˆğ‘—â€² (ğ‘¡ğ‘“ğ‘— , ğ‘) â‰ˆ ğ‘ˆğ‘— (ğ‘¡ğ‘“ğ‘— ) Ã— ğ‘†(ğ‘—, ğ‘)

REFERENCES
[1] Mikolov, Tomas; et al. "Efficient Estimation of Word Representations in Vector
Space". arXiv:1301.3781
[2] Word2vec, https://code.google.com/archive/p/word2vec/
[3] M. J. Kusner, Y. Sun, N. I. Kolkin, and K. Q. Weinberger. From word
embeddings to document distances. In Proceedings of ICML, 2015.
[4] Cedric De Boom, Steven Van Canneyt, Thomas Demeester, Bart Dhoedt. 2016.
Representation learning for very short texts using weighted word embedding
aggregation. https://arxiv.org/pdf/1607.00570.pdf
[5] Stephen Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance
Framework: BM25 and Beyond Found. Trends Inf. Retr. 3, 4 (April 2009), 333â€“
389. DOI: http://dx.doi.org/10.1561/1500000019

where ğ‘†(ğ‘—, ğ‘) measures relevance of word j to query intent q,
and ğ‘†(ğ‘—, ğ‘) â‰ˆ max(ğ‘ ğ‘–ğ‘š(ğ‘—, ğ‘š)) with m âˆˆ (non-stop-words in q).
If we use tf-idf as an approximation of U(tf), and combine all
words in q and qâ€™, we have the following formula for ranking,

1348

