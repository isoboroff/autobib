Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Ad Click Prediction in Sequence with Long Short-Term Memory
Networks: an Externality-aware Model
Weiwei Deng

Xiaoliang Ling

Yang Qi

Microsoft Bing, Beijing, China
dedeng@microsoft.com

Microsoft Bing, Beijing, China
xiaoling@microsoft.com

Microsoft Bing, Beijing, China
yanq@microsoft.com

Tunzi Tan∗

Eren Manavoglu

Qi Zhang

School of Mathematical sciences,
Univ. of Chinese Academy of Sciences
tantunzi13@mails.ucas.ac.cn

Microsoft Bing, CA, USA
ermana@microsoft.com

Microsoft Bing, Beijing, China
qizhang@microsoft.com

ABSTRACT

for overwhelming majority of income for search engines like Google
and Bing [8]. The expected revenue generated by one ad is typically calculated by the product of the bid of the ad and the number
of user clicks on this ad for a period of time, which is also called
“pay-per-click” model [2]. The basic elements of sponsored search
are advertiser-provided content, advertiser-provided bids, review
process, matching of advertiser content to user queries, display of
advertiser content and processes that gather data, meter clicks and
charge advertisers [5]. It is very important to choose the right ads
for a query and the ranking has a strong impact on the revenue
the search engine receives from ads [19]. In order to maximize the
revenue in sponsored search over the long term, it is necessary
for the search engine to estimate the click-through rate (CTR) of
ads for a given search query to determine the best allocation of
display position and appropriate payments [15]. Therefore, click
prediction is very crucial to sponsored search advertising because
it impacts user experience, profitability of advertising and search
engine revenue.
State-of-the-art sponsored search systems typically employ machine learning approaches to predict the click probability [22]. Models and features are two main factors for accurate prediction of
PClick score. Xinran et al. [10] introduced a model which combines decision trees with logistic regression to solve click prediction at Facebook. Borisov et al. [1] illustrated the distributed
representation-based approach using a set of neural click models
which outperforms traditional models on click prediction task. Graepel et al. [8] proposed that historical CTR for ad impressions1 with
respect to different elements, e.g., CTR of query, ad, user and their
combinations, are important features. Semantic relevance between
query and ad was mentioned by Hillard et al. [11] as an important
feature. Position bias, users tending to click documents in higher
position even with lower quality, was analyzed by Jochardson et
al. [13, 14] and regarded as a significant feature in click prediction. Features come in different forms: one-hot encoding ID feature
which keeps the original information, statistic feature, like counting feature and garbage feature as well as position feature [16].
Some research work (Wanhong et al. [21], Chenyan et al. [20])
proposed and examined externalities: the clicks on an ad might be
affected by the quality of the ads shown together with it and they
pointed out that they could achieve more accurate click prediction
by modeling spatial relationship. Recurrent Neural Networks (RNN)

Ad click prediction is a task to estimate the click-through rate (CTR)
in sponsored ads, the accuracy of which impacts user search experience and businesses’ revenue. State-of-the-art sponsored search
systems typically model it as a classification problem and employ
machine learning approaches to predict the CTR per ad. In this
paper, we propose a new approach to predict ad CTR in sequence
which considers user browsing behavior and the impact of top ads
quality to the current one. To the best of our knowledge, this is the
first attempt in the literature to predict ad CTR by using Recurrent
Neural Networks (RNN) with Long Short-Term Memory (LSTM)
cells. The proposed model is evaluated on a real dataset and we
show that LSTM-RNN outperforms DNN model on both AUC and
RIG. Since the RNN inference is time consuming, a simplified version is also proposed, which can achieve more than half of the gain
with the overall serving cost almost unchanged.

KEYWORDS
Click Prediction; LSTM-RNN; Externality
ACM Reference Format:
Weiwei Deng, Xiaoliang Ling, Yang Qi, Tunzi Tan, Eren Manavoglu, and Qi
Zhang. 2018. Ad Click Prediction in Sequence with Long Short-Term Memory Networks: an Externality-aware Model. In SIGIR ’18: The 41st International ACM SIGIR Conference on Research Development in Information
Retrieval, July 8–12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA,
4 pages. https://doi.org/10.1145/3209978.3210071

1

INTRODUCTION

Sponsored search is regarded as one of the most effective and profitable advertising approaches [5].Commercial web search engines
typically generate revenue by presenting sponsored results as well
as organic web results to satisfy a user query [21], which accounts
∗ This

work is done during her internship in Microsoft Bing.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210071

1 a certain ad shown to particular user in specific search result page as an ad impression

1065

Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

[22] has been leveraged to model sequential dependency. In the
training process of this model, features of each ad impression will
be feed forwarded into the hidden layer, together with previously
accumulated hidden state.
Here we define MLx as the position of an ad in mainline or north
of the search result page. For example, an ad at ML1 stands for
the first ad shown in mainline. Most of previous works predict
the PClick score by taking single ad impression as input instance.
However, we assume that user scan ads from ML1 to ML4 in one
page view, so we predict the probability of click (PClick score) of one
ad from ML1 to ML4 considering the quality of the ads in front of
the current one. To the best of our knowledge, this work is the first
attempt in the literature to use recurrent neural networks (RNN)
with Long Short-Term Memory (LSTM) cells, or the LSTM-RNN, in
sponsored search advertising to deal with externality. The paper is
organized as follows. Section 2 introduces the problem and some
data analysis of externalities. Section 3 presents the click prediction
using LSTM-RNN, while Section 4 presents experimental results.
Simplified version of the model is presented in Section 5 and a
conclusion part follows.

2

one, but when the real CTR of ML1 gets higher, the baseline model
tends to overpredict the CTR of ML2. This observation verifies that
the quality of ads will have impact on the CTR of other ads: if the
quality of the ads at ML1 is poor, users may have higher chance
to take a look at the ad bellow it hence more chance to click the
second one; but if the ad at ML1 can satisfy user, it is more likely
that the second ad will be ignored hence less chance to be clicked.

3

PREDICTION MODEL STRUCTURE

RNN is a type of deep neural networks that are extensively used in
time sequence modelling [9, 17, 18], but it is usually difficult to learn
the long term dependency within the sequence due to vanishing
gradients problem [18]. LSTM cell, proposed in [12] and completed
in [6] and [7], is an efficient way to overcome this problem.
We use the architecture of LSTM-RNN illustrated in Figure 2,
which is formed by a set of features: X i (including sub model features, semantic features, statistic features and position bias), LSTM
cells, hidden layers hi and the click prediction of each ad at ML1 to
ML4 Pi . Overall, the process of modeling PClick of ads from ML1
to ML4 can be unfolded in the following steps:

DATA ANALYSIS OF EXTERNALITIES

• Firstly, ML1 ad is fed to LSTM, without information of previous ads;
• Secondly, ML2 ad is fed with the information of the ad at
ML1;
• Thirdly, ML3 ad is fed with the aggregated information from
ads at ML1 and ML2;
• At last, ML4 ad is fed, which is aware of all the previous ads.

Externality has been analyzed by [21] and [22]. Our goal is to
analyze how the quality of ads may influence the CTR of other ads.
We focus on the first two ads in mainline. Showing the existence of
externality in two ads is a way to prove the existence of externality
in a longer sequence of ads. Firstly, sort all ML1 impressions by their
estimated CTR(from the baseline model) in ascending order and
group them into 20 bins each of which contains the same number
of ad instances. Then, assign ML2 impressions to the bin where its
related ML1 impression belongs. Then plot the real average CTR
(blue line) and predicted average PClick by baseline click prediction
model (green line) at ML2, shown in Figure 1.

Figure 1: Data analysis of externality. This reveals that when
the CTR of ML1 increases, the average CTR of ML2 will increases at first and then decreases. The predicted CTR is
lower and then higher than the real one.

Figure 2: The basic architecture of the LSTM-RNN

Details about the architecture can be found in [18] and Pt =
σ (wy ∗ ht + by ), where σ (·) stands for the sigmoid transformation.
The baseline model used in this paper is shown in Figure 3. The
details of this model can be found in [16].

We observe that when CTR of ML1 increases, the CTR of ML2
increases at first and then decreases. What is more, when the real
CTR of ML1 is low, the predicted CTR of ML2 is lower than the real

1066

Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

PClick score predicted by LSTM-RNN is more accurate than
baseline model in most bins according to Figure 4.

Figure 3: Baseline model of this paper [16]

4

ANALYSIS OF THE CLICK PREDICTION
MODEL AND PERFORMANCE EVALUATION
4.1 Datasets

Figure 4: Externality and different models

Training data used in this study consists of 15M examples which are
sampled from one week log. The model predicting accuracy is tested
against dataset with 8M samples generated from the next week
in the same way. The features used include LR features, statistic
features and position features, whose detail description can be
found in [16].

4.2

5

In a typical auction, there are hundreds or even more candidates.
RNN will bring significant computation cost increase, because
PClick score needs to be recalculated when we allocate the ad one
by one from top to bottom. This will increase the serving latency.
So we propose a simplified version which can keep the majority
gain and reduce the cost significantly.
Instead of using LSTM-RNN model, we introduce a new feature representing externality. Here, the PClick score (from a model
which doesn’t consider externality) of the ad at ML1 is regarded as
externality feature, given the ad at ML1 is the most important and
the first one in the sequence. The externality is linearly connected
to the output node of the model. For ML1 ad, it’s set to zero. For
ML2/3/4 ad, it is set to the aforementioned PClick score of the ML1
ad in same page view.

Evaluation metrics

To understand the performance of LSTM-RNN on click prediction,
we use the Area under Receiver Operating Characteristic Curve
(AUC) [3] and Relative Information Gain (RIG) [10] as our major
evaluation metrics. AUC gives the model accuracy of ranking positive case in front of negative one and RIG measures the relative
logloss reduction to an empirical model, which is defined as:
RIG =

LLpr edict
LLempir ical

− 1.

(1)

where LLpr edict is the mean logloss and LLempir ical presents the
average log-loss per impression if the CTR is predicted by a naive
model that always predicts with the empirical CTR. Higher AUC
and RIG is considered as better accuracy [16].

4.3

A SIMPLIFIED VERSION TO DEAL WITH
EXTERNALITY

Experimental results

The baseline of the experiment is the normal click prediction model
using the same features with the LSTM-RNN model, but not considering externality. The experimental results are shown in Table 1.
Delta AUC is the improvement of new model on baseline and so is
Delta RIG.
Table 1: AUC and RIG gain of new model over DNN
Figure 5: The simplified model with externality
Ad Position
ML1
ML2
ML3
ML4
ALL

Delta AUC
0.13%
1.59%
2.74%
3.31%
0.56%

Delta RIG
0.64%
11.64%
22.67%
27.63%
2.39%

By the use of this model, PClick score is calculated by the formula
(2). Once the PClick score for ML1 slot auction is calculated, the
recalculation for below slots allocation can be easily derived by
formula (3).
PClick Ml x = σ (externality + positionBias Ml x + otherSum) (2)

1067

Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

PClick Ml x = σ (externality + positionBias Ml x +
σ −1 (PClick Ml 1 ) − positionBias Ml 1 )
(3)
The benefit of modeling PClick in this way is that, as the formula
shows, the PClick is linearly depends on externality and position,
so the recalculation cost is pretty low. The performance of the
simplified version is shown in Table 2 and Figure 6.
Compared with RNN, the simplified version brings 0.13% loss on
"ALL" slice. With the method in [4] we ran the significant test and pvalue is almost zero indicating the loss is statistical significant. This
is expected due to two possible reasons. One is that the simplified
version only considers the effect of ML1 ad and ignores the other
ads. Another is that RNN has more complex model structure, there is
feature level interactions between different ads, like the features in
ML1 and ML2 ad. For simplified version, there isn’t such interaction
since only the PClick score of ML1 ad is visible to other ads.

click prediction and it can be treated as another kind of externality.
The ad semantic information can be represented as embedding in
RNN to better capture the externality from similarity to improve
the CTR prediction model.

REFERENCES
[1] Alexey Borisov, Ilya Markov, Maarten de Rijke, and Pavel Serdyukov. 2016. A
neural click model for web search. In Proceedings of the 25th International Conference on World Wide Web. International World Wide Web Conferences Steering
Committee, 531–541.
[2] Rex Briggs and Nigel Hollis. 1997. Advertising on the web: Is there response
before click-through? Journal of Advertising research 37, 2 (1997), 33–46.
[3] Corinna Cortes and Mehryar Mohri. 2004. AUC optimization vs. error rate
minimization. In Advances in neural information processing systems. 313–320.
[4] David M. DeLong Elizabeth R. DeLong and Daniel L. ClarkePearson. 1988. Learning precise timing with LSTM recurrent networks. Biometrics 44, 3 (1988), 837–
845.
[5] Daniel C. Fain and Jan O. Pedersen. 2006. Sponsored search: A brief history.
Bulletin of the Association for Information Science and Technology 32, 2 (2006),
12–13.
[6] Felix A. Gers, Jürgen Schmidhuber, and Fred Cummins. 1999. Learning to forget:
Continual prediction with LSTM. (1999), 850–855.
[7] Felix A. Gers, Nicol N. Schraudolph, and Jürgen Schmidhuber. 2002. Learning
precise timing with LSTM recurrent networks. Journal of machine learning
research 3, Aug (2002), 115–143.
[8] Thore Graepel, Joaquin Q Candela, Thomas Borchert, and Ralf Herbrich. 2010.
Web-scale bayesian click-through rate prediction for sponsored search advertising in microsoft’s bing search engine. In Proceedings of the 27th international
conference on machine learning (ICML-10). 13–20.
[9] Alex Graves. 2012. Sequence transduction with recurrent neural networks. arXiv
preprint arXiv:1211.3711 (2012).
[10] Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu, Tao Xu, Yanxin Shi, Antoine
Atallah, Ralf Herbrich, Stuart Bowers, et al. 2014. Practical lessons from predicting
clicks on ads at facebook. In Proceedings of the Eighth International Workshop on
Data Mining for Online Advertising. ACM, 1–9.
[11] Dustin Hillard, Eren Manavoglu, Hema Raghavan, Chris Leggetter, Erick CantúPaz, and Rukmini Iyer. 2011. The sum of its parts: reducing sparsity in click
estimation with query segments. Information Retrieval 14, 3 (2011), 315–336.
[12] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural
computation 9, 8 (1997), 1735–1780.
[13] Thorsten Joachims, Laura Granka, Bing Pan, Helene Hembrooke, and Geri Gay.
2005. Accurately interpreting clickthrough data as implicit feedback. In Proceedings of the 28th annual international ACM SIGIR conference on Research and
development in information retrieval. Acm, 154–161.
[14] Thorsten Joachims, Laura Granka, Bing Pan, Helene Hembrooke, Filip Radlinski,
and Geri Gay. 2007. Evaluating the accuracy of implicit feedback from clicks and
query reformulations in web search. ACM Transactions on Information Systems
(TOIS) 25, 2 (2007), 7.
[15] Mervyn King, Jill Atkins, and Michael Schwarz. 2007. Internet advertising and the
generalized second-price auction: Selling billions of dollars worth of keywords.
The American economic review 97, 1 (2007), 242–259.
[16] Xiaoliang Ling, Weiwei Deng, Chen Gu, Hucheng Zhou, Cui Li, and Feng Sun.
2017. Model Ensemble for Click Prediction in Bing Search Ads. In Proceedings of
the 26th International Conference on World Wide Web Companion. International
World Wide Web Conferences Steering Committee, 689–698.
[17] Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernockỳ, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model.. In Interspeech,
Vol. 2. 3.
[18] Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen,
Xinying Song, and Rabab Ward. 2016. Deep sentence embedding using long
short-term memory networks: Analysis and application to information retrieval.
IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP) 24, 4
(2016), 694–707.
[19] Matthew Richardson, Ewa Dominowska, and Robert Ragno. 2007. Predicting
clicks: estimating the click-through rate for new ads. In Proceedings of the 16th
international conference on World Wide Web. ACM, 521–530.
[20] Chenyan Xiong, Taifeng Wang, Wenkui Ding, Yidong Shen, and Tie-Yan Liu.
2012. Relational click prediction for sponsored search. In Proceedings of the fifth
ACM international conference on Web search and data mining. ACM, 493–502.
[21] Wanhong Xu, Eren Manavoglu, and Erick Cantu-Paz. 2010. Temporal click
model for sponsored search. In Proceedings of the 33rd international ACM SIGIR
conference on Research and development in information retrieval. ACM, 106–113.
[22] Yuyu Zhang, Hanjun Dai, Chang Xu, Jun Feng, Taifeng Wang, Jiang Bian, Bin
Wang, and Tie-Yan Liu. 2014. Sequential Click Prediction for Sponsored Search
with Recurrent Neural Networks.. In AAAI. 1369–1375.

Table 2: AUC and RIG gain of simplified version over DNN
Ad Position
ML1
ML2
ML3
ML4
ALL

Delta AUC
0.08%
1.33%
2.17%
2.43%
0.43%

Delta RIG
0.40%
9.61%
17.80%
19.48%
1.81%

Figure 6: Externality and different models

6

CONCLUSIONS AND FUTURE WORKS

We analyzed the existence of externality which impacts the accuracy of CTR prediction for ads bellow ML1 position, and we
proposed LSTM-RNN model for sequential ad click prediction instead of per single ad click prediction which significantly improves
the accuracy for ads bellow ML1. Since the computation cost of
LSTM-RNN is high, a simplified model was also proposed, where
ML1 PClick is introduced as externality and used as a new feature.
This simplified version can achieve more than half of the gain and
almostly keep the computation cost unchanged. For the future, we
will add more signals to better capture externalities. For example,
the similarity between ads are typically not used in per single ad

1068

