Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Transparent Tree Ensembles
Alexander Moore∗

Vanessa Murdock†

eBay, Inc
alexandermoore@post.harvard.edu

Amazon Research
vmurdock@amazon.com

Yaxiong Cai

Kristine Jones

Microsoft
yaca@microsoft.com

Microsoft
krjones@microsoft.com

ABSTRACT

While the technology is at times astoundingly sophisticated, the
aggregation and inference over our personal data carries with it
an inherent risk of violating our privacy and trust. As we push the
boundaries of what machine learning can be used for, we reveal
the limitations and biases inherent in the system. For this reason,
it becomes increasingly important to provide insight into how the
system made its decisions. Understanding the factors that led to
a machine-learned decision allows us to correct biases, or remove
our personal data. Revealing the reasons behind model decisions
has the potential to increase user trust and engagement with the
system. This type of transparency is healthy, and of benefit both to
the models and to their consumers.
Much previous work in model transparency centers around rulebased systems, which are used in domains where the model is an
aid to human decision-making, such as for medical diagnoses. In
such systems, the model is not the domain expert – rather, the
human is the expert – so the focus is on making the models as
understandable as possible at the expense of accuracy [2].
Many modern technologies, such as search and recommender
systems are built on highly complex, highly accurate tree ensembles.
Rather than present the exact features from the model itself, such a
system may reveal heuristics derived from a single feature, or use
the results of a second more interpretable model whose decisions
are not related to the original model.
The most obvious approach, ordering features by their weight in
the model, is not sufficient here because while it provides insight
into the model as a whole, it does not tell us what the salient features
were for a particular instance. For example, in a machine-learned
ranking, the page-rank score and the frequency of the query terms
in a document might be the most important model-level features.
A particular user may have issued the same rare query many times,
and each time clicked on the same search result. In this case, features
based on the user’s query frequency and history of clicks may be
more salient than the page-rank and term frequency of the clicked
document.
In this paper, we define model transparency as two distinct
processes. The first determines which features contributed to a
model’s decision at the instance level. The second produces a
human-readable representation of the salient features. We focus on
the first process and ask the question: What were the features that
contributed to the model’s decision for a given instance?
We start with tree ensembles, which are known to be highly
accurate for many classes of problems, and are in wide use in
industry. We use the structure of the model to derive instancelevel explanations, examining changes in expected output at each

Every day more technologies and services are backed by complex
machine-learned models, consuming large amounts of data to provide a myriad of useful services. While users are willing to provide
personal data to enable these services, their trust in and engagement
with the systems could be improved by providing insight into how
the machine learned decisions were made. Complex ML systems
are highly effective but many of them are black boxes and give no
insight into how they make the choices they make. Moreover, those
that do often do so at the model-level rather than the instance-level.
In this work we present a method for deriving explanations for
instance-level decisions in tree ensembles. As this family of models
accounts for a large portion of industrial machine learning, this
work opens up the possibility for transparent models at scale.

KEYWORDS
Model Explainability, Transparent IR, Boosted Trees
ACM Reference Format:
Alexander Moore, Vanessa Murdock, Yaxiong Cai, and Kristine Jones. 2018.
Transparent Tree Ensembles. In SIGIR ’18: The 41st International ACM SIGIR
Conference on Research & Development in Information Retrieval, July 8–
12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https:
//doi.org/10.1145/3209978.3210151

1

INTRODUCTION

Machine learned models drive more and more of our everyday
experiences, and they depend on vast amounts of data about who
we are, where we go, what we like, and how we interact with
technology. The technologies for search ranking, ad targeting, local
search, content personalization and other types of inference have
become so useful that we are increasingly comfortable providing
our data to support them.
∗ Work
† Work

done while the author was at Microsoft.
done while the author was at Microsoft.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210151

1241

Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

node along the path through the tree. Changes in expectation are
added to the influence of the splitting feature, and these changes are
aggregated across all nodes in all trees. For addditional insight, we
maintain feature ranges within which feature contributions would
remain unchanged. In this work we focus on classification with
Random Forests, but the same methodology applies to other tree
ensemble models.

2

continue through the tree until they reach a leaf node, at which
point they are assigned a prediction F (xi ) according to the leaf
node’s output.
Decision tree learning involves deciding which feature to split on
at each node n. This is done by selecting the feature j and threshold
t that minimize a function S(x, y, j, d) across all incoming instances
(x, y). The choice of split function does not affect our algorithm.
Our experiments use:
Õ
Õ
S(x, y, j, d) =
(yi − µ L )2 +
(yi − µ R )2
(1)

RELATED WORK

Ideally we would like a model that is both a highly accurate predictor, and yields reasonably accurate explanations. One approach is
to start with a model that lends itself to interpretation, and drive its
accuracy to be on par with the state-of-the art. This is the approach
in a trio of papers building off of Generalized Additive Models
(GAMs). [2, 6, 7]
GAMs are especially suited for model explanations because they
lend themselves to visualizing the distributions of the values of the
feature functions. The distributions can be plotted in 2D, making
these visualizations very easy for a person looking at them to interpret. GAMs are not as accurate as full-complexity models, such as
Random Forests and Boosted Trees, but they are far more accurate
than rule-based systems, which are currently used for systems that
must be human-interpretable, such as medical diagnoses.
Many systems have no human intervention, and the system’s
utility is first and foremost dependent on its accuracy. Further,
not all systems lend themselves to this type of visualization. If
the consumer of the model is not a domain expert, as is often
the case in search and recommender systems, the visualizations
may not be meaningful. Examining cross-sections of these plots
permits per-instance analysis, but the problem domain is limited to
those modeled well by GAMs. As the number of features increases,
the number of 2D plots produced also increases, and there is a
limit to how many can be viewed before the information becomes
overwhelming.
Ribeiro et al. [8] propose a method for instance-level explanations
of black-box classification models, and work by Hara et al. [4]
focuses on providing similar insights by building a simpler model for
interpretation. Our approach differs from the black-box approach by
taking into account the structure of tree ensemble models, allowing
for other insights besides a raw feature ranking. It differs from both
approaches by directly interpreting the original model rather than
building an additional interpretable model.

3

i ∈L

3.1

Tree Ensembles

A tree ensemble is a collection of K decision trees whose output
F (x) for an instance x takes the form:
F (x) = д({ fk (x), k = 1..K })

(2)

kth

where fk (x) is the prediction of the
tree for instance x. In
this work, we consider tree ensembles where д is a monotonically
increasing function.

3.2

Random Forests

Random forests [1] are tree ensemble models for which each tree
is trained independently. Trees are each trained on a bootstrap
sample of the data using a random subset of the features. In this
work, outputs were written as indicated above, with:
д(v) =

1

(3)

1 + e −αv+β

where v is the summed output and α, β ∈ R.

4

THE EXPLAINABILITY MODEL

The model we present aims to describe, for a given instance xi ,
the influence that each feature j’s particular value x i j had on the
final output F (xi ). We assume a tree ensemble model has already
been trained, and that its structure cannot be modified. The tree
ensemble output takes the form
F (xi ) = д

T
Õ
k =1

DECISION TREE MODELS

fk (xi )



(4)

where T is the number of trees in the ensemble, д is a monotonically
increasing function and fk is the output of the k t h tree. Our model
works as follows:
(1) Assign output O k (n) to each node n in trees k = 1..K prior
to prediction time.
(2) At prediction time, consider each tree one at a time, tracing
the path of a given instance down the tree, and monitoring
the change in output at each node. The output change for
each node is added to the influence of that feature. The sum of
all changes for a given feature is that feature’s contribution.
Just as there are multiple ways to assign outputs to leaf nodes, there
are multiple ways to assign outputs to internal nodes. In this work
we use expected value.

A predictive decision tree is a directed acyclic graph satisfying the
following conditions:
•
•
•
•

i ∈R

where µ L is the mean yi of instances with x i j ≤ d, and µ R is the
mean yi of instances with x i j > d.

There is a single node, the root, with no incoming edges.
Every other node has exactly one incoming edge.
There is only one path from the root to any given node.
The nodes at the bottom of the tree, called leaves, have no
outgoing edges. Output predictions are stored in these nodes.

Each non-leaf node splits on one or more features, but in this work
we focus on univariate splits. A split on feature x i j is described by
x i j ≤ t, where t is some threshold value. Instances satisfying the
inequality progress to the left subtree (L), while instances which
do not satisfy the inequality progress to the right (R). Instances

1242

Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

More formally, assume there is some prior distribution Pk across
all leaf nodes in the k t h tree. Let Pk (t) be the prior probability of
Í
arriving in leaf node t in the k t h tree, and t Pk (t) = 1. Similarly,
let O k (t) be the output at leaf node t. Consider a node n in tree k for
which both children are leaf nodes. The expected output at node n
is defined as the expectation across the leaves nl and nr , namely:

Categorical Features
marital status (mar)
relationship (rel)
occupation (job)
native country
work class
education
race

P (n )O (n ) + Pk (nr )O k (nr )
(5)
Ek [n] = k l k l
Pk (nl ) + Pk (nr )
More generally, the expected output at any node n can be written:
Í
t ∈T (n) Pk (t)O k (t)
Í
Ek [n] =
(6)
t ∈T (n) Pk (t)

Table 1: Features from the UCI 1995 Adult Census dataset.
“Sex" is a numeric feature, given values 0 and 1.

where T (n) is the set of all leaf nodes reachable from n. Let Pk (n)
Í
be defined as t ∈T (n) Pk (t) for every node n. If nl and nr are the
children of node n, the expectation can be rewritten as:

where dn is the split threshold for node n and Lk (n) indicates a
split where xi went left. Aggregating across all trees to find the
narrowest range, we have:

Ek [n] = Pk (nl )E[nl ] + Pk (nr )E[nr ]
(7)
This form yields the same result as Equation 6 but allows for
more efficient computation. If traversing the tree in post-order, the
expectations can be computed in time complexity O(|Nk |) where
Nk is the set of all nodes in the tree.
Given this definition of Ek [n], the influence of a node n splitting
on feature j for a given instance x is computed as the change in
expected output from that node to the next node along x’s path.
More concretely, consider a tree k and an instance x, and let nq (x)
be the q th node along the path x takes through tree k from the root
node to the leaf node. The influence Inflk (nq ) of node nq ’s split on
feature j in tree k is:
Inflk (nq ) = ∆Ek [nq → nq+1 ] = Ek [nq+1 ] − Ek [nq ]

(14)

: Inflk (n) > 0, Lk (n) = 0)
+
vk,max

(15)

= min(dn ∀n ∈ Pathk (xi )
: Inflk (n) > 0, Lk (n) = 1)

(8)

(16)

−
vk,min
= max(dn ∀n ∈ Pathk (xi )

: Inflk (n) < 0, Lk (n) = 0)

(17)

−
vk,max
= min(dn ∀n ∈ Pathk (xi )

: Inflk (n) < 0, Lk (n) = 1)

(18)

Aggregation is once again:

where Pathk (x) is the set of all nodes along instance x’s path
through tree k, and Ik (j, n) is an indicator function which is 1 if
node n splits on feature j and 0 otherwise. The feature influence
across all trees is simply:
Õ
FeatInfl(j) =
FeatInflk (j)
(10)

+
+
vmin
= max(vk,min
, k = 1..K)

(19)

= 1..K)

(20)

= 1..K)

(21)

= 1..K)

(22)

+
vmax
−
vmin
−
vmax

k =1..K

+
= min(vk,,max
,k
−
= max(vk,,min
,k
−
= min(vk,,max , k

which gives us the range values for each feature, partitioned by
contribution direction. The following section demonstrates the
above methods on a public data set.

Feature Influence Ranges

An extension of this method allows us to gain additional insight by
generating feature influence ranges. These are ranges vi, j,min ≤
x i j ≤ vi, j,max such that the path direction after each node split
on feature j for instance xi would remain unchanged. In other
words, as long as vi, j,min ≤ x i j ≤ vi, j,max , all feature influence
contributions by nodes n ∈ Path(xi ) splitting on feature j would
remain unchanged. These ranges are computed for tree k as follows:

5

CASE STUDY

To test this model, we used the publicly available UCI 1995 Adult
Census dataset [5], wherein the classification task is to determine
whether an individual is making over $50K/year or not. The training set contained 32,561 rows, and the test set contained 16,281
rows with positive class ratios of 24% and 23.6% respectively and
labels yi ∈ {−1, 1}. Categorical variables were expanded into 0/1
indicators. The set of features is shown in Table 1.
Note that for each of the categorical features, an individual will
have all values. For example, the same individual will have the

vk,i, j,min = max(dn ∀n ∈ Pathk (xi )
(11)

vk,i, j,max = min(dn ∀n ∈ Pathk (xi )
: Ik (n, j) = 1, Lk (n) = 1)

(13)

+
vk,min
= max(dn ∀n ∈ Pathk (xi )

n ∈P at h k (x)

: Ik (n, j) = 1, Lk (n) = 0)

vi, j,min = max(vk,i, j,min , k = 1..K)
vi, j,max = min(vk,i, j,max , k = 1..K)

We can gain more insight by generating two ranges for each
feature j in tree k: One for cases where Inflk (n) > 0 and one for cases
+
+
−
where Inflk (n) < 0. Let these be called vk,min
, vk,max
, vk,min
and
−
vk,max with implicit dependencies on i, j and implicit Ik (n, j) = 1.
We then have:

The overall influence of a feature j in tree k is the sum of the
influence of all nodes along x’s path that split on feature j, namely:
Õ
FeatInflk (j) =
Inflk (n)Ik (j, n)
(9)

4.1

Numerical Features
hours-per-week
education num
capital gain
capital loss
age
sex

(12)

1243

Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Person A (95.1%) (Makes >$50K)
Rank
1
2
3
1
2
3

Feature
capital gain
education num
mar (Married)
rel (Husband)
job (Prof special)
capital loss

Influence
+74.9
+22.0
+19.3
-12.27
-0.73
-0.60

Min
7073.5
12.5
0.5
−∞
−∞
−∞

as prevalent in the data as wives. The other negative influences, not
being in a specialty and having low capital losses, are also intuitive
but have relatively low influence.
Person B was more difficult for the model to discern. On the one
hand, they are a college graduate, a husband and are married, all
of which are taken as positive influencers. On the other hand, they
work 30 or fewer hours per week (as opposed to the usual 40+),
have less than 5036.5 in capital gains, and are self-employed. Once
again, the influencers make intuitive sense – someone who works
fewer hours, has fewer capital assets and is self-employed may not
make $50K/year.
Person C is another obvious case for the model. Even though
they are married (a big positive signal), are between the ages of 37
and 59 (late in career but before retirement) and are not currently
not married, there are numerous negative signals. They are not
a husband, have a high school education and work in the service
industry.
While a person thinking of the characteristics of an example
might choose a different ranking of influencers, the feature ranking
gives insight into the model’s process. This can be useful to improve
system performance or to identify bias in the data. It may also be
helpful to end-users wondering what influenced a prediction. In this
case, marriage and relationships seem to be important indicators,
along with education and capital asset activity. The data appears to
be biased toward male wage earners, which would be important to
note if the model were to be deployed in a real-world scenario.

Max
∞
∞
∞
0.5
0.5
1740.5

Person B (50.3%) (Makes ≤$50K)
Rank
1
2
3
1
2
3

Feature
education num
rel (Husband)
mar (Married)
hours-per-week
capital gain
workclass (Self-emp)

Influence
+35.19
+19.35
+19.33
-38.42
-5.39
-3.19

Min
12.5
0.5
0.5
−∞
−∞
0.5

Max
∞
∞
∞
30.5
5036.5
∞

Person C (9.15%) (Makes ≤$50K)
Rank Feature
Influence
1
mar (Married)
+22.39
2
age
+5.97
3
mar (Never married) +1.68
1
rel (Husband)
-12.97
2
education num
-10.73
3
job (Other service)
-4.32
Table 2: Three examples of instance-level
model decisions.

Min Max
0.5
∞
36.5 59.5
−∞ 0.5
−∞ 0.5
−∞ 9.5
0.5
∞
explanations for

6

features “marital status (Married) = 1” and “marital status (Never
married) = 0”.
We trained a Random Forest model of 100 trees with at most 20
leaves per tree, at least 10 instances per leaf, a bootstrap sample
size of 70% and a feature sample size of 70%, achieving 90% AUC,
and an Accuracy of 85.8%. The prior distribution Pk was computed
using the training data.

5.1

CONCLUSION

In this work, we defined the problem of explainability for tree
ensembles, and proposed a method for per-instance explainability.
We demonstrated the results in a brief case study, as a proof of
concept. We leave the full evaluation of the explanations for future
work, along with producing human-readable summaries of the
model explanations.

REFERENCES
[1] Leo Breiman. 2001. Random Forests. Machine Learning 45, 1 (2001), 5–32. DOI:
http://dx.doi.org/10.1023/A:1010933404324
[2] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie
Elhadad. 2015. Intelligible Models for HealthCare: Predicting Pneumonia Risk and
Hospital 30-Day Readmission. In Proceedings of the ACM Conference on Knowledge
Discovery and Data Mining. Sydney, Australia, 1721–1730.
[3] George Forman. 2003. An extensive empirical study of feature selection metrics
for text classification. Journal of machine learning research 3, Mar (2003), 1289–
1305.
[4] Satoshi Hara and Kohei Hayashi. 2016. Making tree ensembles interpretable.
WHI 2016. arXiv preprint arXiv:1606.05390 (2016).
[5] M. Lichman. 2013. UCI Machine Learning Repository. (2013). http://archive.ics.
uci.edu/ml
[6] Yin Lou, Rich Caruana, and Johannes Gehrke. 2012. Intelligible Models for
Classification and Regression. In Proceedings of the ACM Conference on Knowledge
Discovery and Data Mining. Beijing, China.
[7] Yin Lou, Rich Caruana, Johannes Gehrke, and Giles Hooker. 2013. Accurate Intelligible Models with Pairwise Interactions. In Proceedings of the ACM Conference
on Knowledge Discovery and Data Mining. Chicago, Illinois.
[8] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. Why Should I
Trust You?: Explaining the Predictions of Any Classifier. In Proceedings of the
22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining. ACM, 1135–1144.

Discussion

The results of our method are often intuitive. In Table 2 for Person
A, the method indicates high capital gains were a strong positive
influence. This makes sense – an individual with capital gains has
owned and sold capital assets, implying affluence. The second influencer, years of education, also makes sense. In the dataset, college
graduates had 13 years of education, so having over 12.5 years of
education implies Person A graduated from college. Additionally,
Person A is married, suggesting a higher household income or a
more stable life.
On the other hand, the strongest negative influencer was the
individual not being a husband. This illustrates an unfortunate bias
in the model. “Husband” frequently comes up as an influence, but
“Wife” does not. In the training data, 40% of individuals were husbands, while only 5% were wives. Although a similar proportion of
husbands and wives earned more than $50K, husbands were 8 times

1244

