Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

A Study of Per-Topic Variance on System Comparison
Meng Yang

Peng Zhang ∗

Dawei Song

School of Computer Science and
Technology
Tianjin University
mirandayang1213@163.com

School of Computer Science and
Technology
Tianjin University
pzhang@tju.edu.cn

School of Computer Science and
Technology
Beijing Institute of Technology
dawei.song2010@gmail.com

ABSTRACT
Under the notion that the document collection is a sample from a
population, the observed per-topic metric (e.g., AP) value varies
with different samples, leading to the per-topic variance. The results
of the system comparison, such as comparing the ranking of systems
according to the summary metric (e.g., MAP) or testing whether
there is significant difference between two systems, are affected by
the variability of per-topic metric values. In this paper, we study the
effect of per-topic variance on the system comparison. To measure
such effects, we employ two ranking-based methods, i.e., Error Rate
(ER) and Kendall Rank Correlation Coefficient (KRCC), as well as
two significance test based methods, namely Achieved Significance
Level (ASL) and Estimated Difference (ED). We conduct empirical
comparison of TREC participated systems on Robust and Adhoc
track, which shows that the effect of per-topic variance on the
ranking of systems is not obvious, while the significance test based
comparisons are susceptible to the per-topic variance.

However, Cormack and Lynam proposed [1] that the document
collection is a sample from a population of a large document collection. For different document collection samples, the per-topic
metric values can be different. On the basis of this notion, the pertopic variance is generated. The per-topic variance may have effect
on system comparison. For example, the ranking of systems or
the result of significance test between systems varies with different samples of the document collection population. We employ
two kinds of methods to measure this effect. The Error Rate (ER)
and Kendall Rank Correlation Coefficient (KRCC) are utilized to
analyze the consistency of the rankings of systems. On the other
hand, Achieved Significance Level (ASL) and Estimated Difference
(ED) are used to compute the significance test results that reflects
the statistical difference between two systems. In the premise that
document collection is a sample, this work seeks to explore the
reliability of the evaluation results on the aspects of the ranking
performance and the significance test.

KEYWORDS

2

Evaluation; Per-Topic Variance; System Comparison

Under the condition of Cranfield-Style experiment, the variance of
a system usually originates from the different topics or query variations. Zhang et al. proposed the bias-variance evaluation to study
the variance of ranking performance across topics for the query
language modeling approaches [13, 14] and the TREC participated
systems [12].
Under the notion of document collection being a sample from
a population, the variance can be generated from the different
document collection samples. Cormack and Lynam tested the testcollection variability on AP and MAP [1]. Robertson studied the reverse relationship between recall and precision [4] and re-examined
the common metrics in IR from the view of this notion [3]. In addition, Robertson et al. explored the per-topic noise caused by different document collections [6] and presented some insights to model
score distribution without real scores [5].
However, the focus of this study is the effect of per-topic variance on performance comparison among systems. The methods we
used are inspired by the prior work. Voorhees [10] defined ER to
represent the probability of one experiment that leads to the wrong
conclusion. KRCC is used to compare the rankings of systems using
different metrics [2, 8]. Sakai employed ASL to compare metrics’
sensitivity and ED to estimate the performance difference for guaranteeing ASL < α (significance level) [7]. The application of these
existing methods was on the Cranfield-Style experiment where
document collection is fixed. In this paper, however, these methods
are not applied into the Cranfield-Style setup. Instead, we modify
these methods to study the effect of per-topic variance when the
document collection is a sample.

ACM Reference Format:
Meng Yang, Peng Zhang, and Dawei Song. 2018. A Study of Per-Topic
Variance on System Comparison. In SIGIR ’18: The 41st International ACM
SIGIR Conference on Research and Development in Information Retrieval,
July 8–12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages.
https://doi.org/10.1145/3209978.3210122

1

INTRODUCTION

In traditional Cranfield-Style experiment, the evaluation is based
on the fixed document collection, a set of topics and the relevance
judgment. Given a system-topic pair, the value of a per-topic evaluation metric (e.g., Average Precision, AP) is fixed. When comparing
two systems, the values of summary metric concerning all topics
(e.g., Mean Average Precision, MAP) are used to rank systems. The
significance test is conducted to judge whether there is significant
difference between two systems based on all the per-topic metric
values.
∗ Corressonding

author.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210122

1181

RELATED WORK

Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA

3

Meng Yang, Peng Zhang, and Dawei Song

SYSTEM COMPARISON BASED ON
PER-TOPIC VARIANCE

per-topic raw metric values and simulated metric values,
respectively, deriving raw comparison result and simulated
comparison result.
• The consistency between raw comparison result and simulated comparison result can be measured by methods described below. If two results are consistent, the variance is
considered to have less effect.

The implementation of document collection being a sample is
through the method of simulation, such as bootstrap (BST), Kernel
Density Estimation (KDE) and etc. In Section 3.1, these two models
are briefly reviewed. The methods of comparing retrieval systems
are introduced in Section 3.2.

3.1

Due to the randomness of samples, simulated comparison results
may be different. Therefore, the comparison can be repeated multiple times.

Simulation Models

Given a system-topic pair, suppose that there are 1000 retrieved documents with 1000 scores correspondingly, and a number (denoted
as r ) of them are relevant. Following [6], using Poisson distribution with the mean r , we take a sample r s from it. Specifically, r s
simulated scores are taken from the distribution of relevant scores
and 1000 - r s simulated scores are from the score distribution of
non-relevant ones. These 1000 simulated scores are sorted with a
descending order. Then, a binary IR metric can be applied since
each score is labeled as relevant or non-relevant.
The BST takes samples from the relevant scores and non-relevant
scores with replacement, respectively. The KDE is a non-parametric
method to estimate the probability density function. There are different kernels for KDE and the Gaussian kernel is used in our experiment. The relevant and non-relevant probability density functions
are derived by fitting relevant scores and non-relevant scores to
KDE model, respectively. Then, simulated relevant and non-relevant
scores are sampled from the relevant and non-relevant probability
density functions, independently. Comparisons of these models are:

3.2.1 Error Rate. The assumption of computing ER is that the
simulated comparison result about deciding the ranking between
two systems should be the same as the counterpart of the raw
data, otherwise it is a wrong conclusion. Generally, the larger the
simulated summary metric values’ difference is, the smaller the
probability of deriving a wrong conclusion will be. Let M(x) and
M(y) denote the raw summary metric value for system X and Y,
respectively, and the raw performance difference is denoted as d.
M(xi ) and M(yi ) represent the simulated summary metric value
for i t h sample of system X and Y and the corresponding simulated
performance difference is denoted as di . The performance difference
can be divided into several bins, which is similar with [10]. The
algorithm for calculating the ER is shown in Algorithm 1.

ALGORITHM 1: Algorithm for computing error rate.
set Count = 0 and ErrorCount = 0 for each bin;
for each system pair (X,Y) do
d = M (x) - M (y);
for i = 1 to B do
take the i t h sample from the simulation model, respectively, xi
and yi ;
compute M (xi ) and M (yi );
d i = M (xi ) - M (yi );
Count = Count + 1 for bin corresponding d i ;
if d ∗ d i < 0 then
ErrorCount = ErrorCount + 1 for bin corresponding d i ;
end
end
end
for each bin do
ER = ErrorCount / Count;
end

Non-parametric Distribution
These two models assume no distributions for raw relevant
and non-relevant scores. Thus, these models have a wider
range of adaptability for each system-topic pair simulation.
Use of Real Data or Not
The BST employs raw scores directly while the KDE samples
scores from a fitted score distribution, which indicates the
simulated scores according to the BST model may be closer to
the raw scores compared with the simulated scores according
to the KDE model.

3.2

Methods of Comparing Systems

For two systems, X and Y, let x =(x 1 , x 2 , ..., x n ) and y =(y1 , y2 , ...,
yn ) represent the raw metric values on n topics in a topic set Q. In
previous studies, such as [7, 9, 11], the topic in Q is a sample while
the document collection is fixed, leading to the fixed metric value
for each topic. In this paper, we study the per-topic variance, so that
each topic is fixed while the document collection is a sample, where
the per-topic metric values are different for different samples. For a
simulation model, we take samples of relevant and non-relevant
scores from the corresponding score distributions and the simulated
metric values can be computed, denoted by xi =(x 1i , x 2i , .., x ni ) and
yi =(y1i , y2i , .., yni ), where xi and yi represent the i t h sample.
The methods of comparing systems under the notion of document collection being a sample are based on the following two
ideas:

3.2.2 Kendall Rank Correlation Coefficient. KRCC is used to test
the similarity of the orderings of two groups of data when the two
groups are ranked by each of the quantities. The larger coefficient
reflects the more similar rankings of two groups of data. Given a
specific metric, the KRCC between the raw and simulated system
ranking is calculated. Suppose that there are m systems, the raw and
simulated summary metric values for these systems are denoted as
S = (M(x1 ), M(x2 ), ..., M(xn )) and Si = (M(xi1 ), M(xi2 ), ..., M(xin ))
where Si represents the i t h sample. The algorithm is shown in
Algorithm 2.

• For each topic, the raw metric value and the simulated metric
value can be computed. Systems can be compared using these

1182

Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

A Study of Per-Topic Variance on System Comparison

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA

ALGORITHM 2: Algorithm for computing KRCC.

ALGORITHM 4: Algorithm for estimating the performance difference at
a given significance level α .

compute S;
set k[1...B] = 0 ;
for i = 1 to B do
take the i t h sample from the simulation model for each system;
compute Si ;
k[i] = kendall(S,Si );
end
KRCC = mean(k);

where z̄ is the mean of z and σ̄ is the standard deviation of z. Algorithm 3 shows the algorithm for computing ASL. The smaller the

1

1

0.9

0.9

0.8

0.8

0.7

0.7
Simulated AP

√
n

Simulated AP

3.2.3 Achieved Significance Level. The previous two methods
are based on the relative ranking of systems. Now, we describe a
significance test based method, namely Achieved Significance Level
(ASL). Whether there is significant difference between systems can
be reflected by significance test. Let z =(z 1 , z 2 , ..., zn ) where zi = x i
- yi . Under the the null hypothesis, the paired t test statistic is:
z̄
t(z) = σ̄
(1)

DIFF = Ø;
for each system pair (X,Y) do
sample B samples from the simulation models;
compute t(z i ) for i = 1 to B;
sort |t (z i ) | for i = 1 to B;
if |t (z i ) | is the Bα largest value then
DIFF = DIFF ∪ z i ;
end
end
ED = max(diff ∈ DIFF)

0.6
0.5
0.4

0.3
0.2
0.1
0
0

0.2

0.4

0.6
Raw AP

(a) BST

0.8

1

0

0.2

0.4

0.6

0.8

Raw AP

(b) KDE

Figure 1: The error bar of BST and KDE for a specific run of
Robust track 2004.

for a specific run of Robust track 2004 in Figure 1. The standard
deviations of per-topic metric values for two models are similar.
The per-topic variance is small because of the similarity among
samples. The simulated value is close to the raw value when the raw
value is small and vice versa. The same pattern exists for other runs
of two data sets. Thus, these two models basically simulate the raw
values well and have a wider range of adaptability. In comparison
with the KDE model, the per-topic metric value of BST simulation
is closer to the observed value, which indicates that the BST model
simulates the raw data better than KDE model.

ASL is, the stronger the evidence for that null hypothesis is false is,
which means there is significant difference between two systems
with a larger probability.
3.2.4 Estimated Difference. The summary metric values’ difference does not guarantee that there is significant difference between
two systems. In order to derive the confident difference for guaranteeing the significant difference, the ED was utilized in [7]. The
modified algorithm is shown in Algorithm 4.

4.2.2 Error Rate. We set B = 10000 in Algorithm 1 and the number of bins is 21. In the first bin, 0 < di < 0.01; In the second bin,
0.01 ≤ di < 0.02;..., and di ≥ 0.2 in the last bin. The results of
ER for each model on two data sets are shown in Figure 2. For
Robust track 2004, the ER of the BST is higher than that of KDE
in the first seven bins while the ER of these two models are close
to 0 for the last bins. The ER is decreasing when the bin is increasing, which indicates that the smaller the difference between two
sample means is, the larger the probability of deriving the wrong
conclusion is. Although the ER on Adhoc 1999 is larger than that
on Robust track 2004, the trends of these two ER lines are similar.
This further indicates that the smaller the difference between two
simulated summary metric values is, the larger the probability of
deriving the wrong conclusion will be.

4 EXPERIMENT
4.1 Experimental Set-Up
The data used for experiment are Robust track 2004 and Adhoc
track 1999. There are 249 topics and 110 runs for Robust track 2004
and 50 topics and 129 runs for Adhoc track 1999. We use Average
Precision (AP) as the per-topic metric. The summary metric is Mean
Average Precision (MAP).

4.2

0.4

0.2

0

Count = 0;
for i = 1 to B do
take the i t h sample from the simulation model;
√
t(zi ) = z¯i / (σ i / n );
i
2
if |t (z )| > |t 1−α /2 (n − 1)| then
Count = Count + 1;
end
end
ASL = Count/B;

0.5

0.3

0.1

ALGORITHM 3: Algorithm for computing achieved significance level.

0.6

Results

4.2.1 Variability of Estimated Metric Values. The simulated metric values for each system-topic pair on different samples are different. The per-topic error bar is plotted against the raw metric value

4.2.3 Kendall Rank Correlation Coefficient. We set B = 1000 in
Algorithm 2. The KRCC between the systems’ simulated ranking

1183

1

Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA

Meng Yang, Peng Zhang, and Dawei Song
overall distribution. That is the reason why that PCT(ACL < 0.05)
is not close to the raw result and changes obviously on the effect
of per-topic variance.

0.4

0.3
KDE
BST

KDE
BST

0.35

0.25
0.3
0.2
Error Rate

Error Rate

0.25
0.15

5

0.2
0.15

0.1
0.1
0.05
0.05
0

0
0

5

10

15

20

25

0

Bins

5

10

15

20

25

Bins

(a) Robust 2004

(b) Adhoc 1999

Figure 2: Results of error rate on Robust 2004 and Adhoc
1999.
Table 1: The KRCC, Per and ED for BST and KDE on Robust
2004 and Adhoc 1999.

KRCC
PCT(ACL < 0.05)
ED

Robust 2004
BST
KDE
0.9176 0.8088
0.1041 0.0896
0.3955 0.3273

6

Adhoc 1999
BST
KDE
0.8509 0.8121
0.1783 0.1756
0.4135 0.4111

CONCLUSION

Motivated by the recent development of the per-topic variance by
considering the document collection is a sample from a population, we propose to study the effect of the per-topic variance on
comparing systems. Four comparison methods are employed. Two
ranking-based methods are Error Rate (ER) and Kendall Rank Correlation Coefficient (KRCC), and two significance test based methods
are Achieved Significance Level (ASL) and Estimated Difference
(ED). We discover that the ranking of systems changes a little with
the variability of per-topic metric value and the results concerning
the significance test are obviously affected by the per-topic variance.
In the future, we will study more simulation models and comparison
methods and explore the theory behind these discoveries.

ACKNOWLEDGMENTS

This work is supported in part by the state key development program of China (grant No. 2017YFE0111900), Natural Science Foundation of China (grant No. U1636203, 61772363), and the European
Union’s Horizon 2020 research and innovation programme under
the Marie Skłodowska-Curie grant agreement No. 721321.

REFERENCES

and the raw ranking is listed in the first row of Table 1. We can
see that the coefficient value of BST is larger than that of KDE. All
the values are larger than 0.8, which represents that the simulated
ranking of systems is similar to the raw ranking.

[1] Gordon V Cormack and Thomas R Lynam. 2006. Statistical precision of information retrieval evaluation. In Proceedings of the 29th annual international ACM
SIGIR conference on Research and development in information retrieval. ACM,
533–540.
[2] Jaana KekÃďlÃďinen. 2005. Binary and graded relevance in IR evaluationsâĂŤComparison of the effects on ranking of IR systems. Information Processing &
Management 41, 5 (2005), 1019–1033.
[3] Stephen Robertson. 2007. On document populations and measures of IR effectiveness. In Proceedings of ICTIR 2007. 9–22.
[4] Stephen Robertson. 2007. On score distributions and relevance. Advances in
Information Retrieval (2007), 40–51.
[5] Stephen Robertson, Evangelos Kanoulas, and Emine Yilmaz. 2013. Modelling
score distributions without actual scores. In Proceedings of the 2013 Conference
on the Theory of Information Retrieval. ACM, 20.
[6] Stephen E Robertson and Evangelos Kanoulas. 2012. On per-topic variance in
IR evaluation. In Proceedings of the 35th international ACM SIGIR conference on
Research and development in information retrieval. ACM, 891–900.
[7] Tetsuya Sakai. 2006. Evaluating evaluation metrics based on the bootstrap. In
Proceedings of the 29th annual international ACM SIGIR conference on Research
and development in information retrieval. ACM, 525–532.
[8] Tetsuya Sakai. 2007. On the reliability of information retrieval metrics based on
graded relevance. Information Processing & Management 43, 2 (2007), 531–548.
[9] Tetsuya Sakai. 2016. Two Sample T-tests for IR Evaluation: Student or Welch?. In
International ACM SIGIR Conference on Research and Development in Information
Retrieval. 1045–1048.
[10] Ellen M Voorhees and Chris Buckley. 2002. The effect of topic set size on retrieval
experiment error. In Proceedings of the 25th annual international ACM SIGIR
conference on Research and development in information retrieval. ACM, 316–323.
[11] William Webber, Alistair Moffat, and Justin Zobel. 2008. Score standardization
for inter-collection comparison of retrieval systems. (2008), 51–58.
[12] Peng Zhang, Linxue Hao, Dawei Song, Jun Wang, Yuexian Hou, and Bin Hu.
2014. Generalized Bias-Variance Evaluation of TREC Participated Systems. In
Proceedings of the 23rd ACM International Conference on Conference on Information
and Knowledge Management (CIKM ’14). 1911–1914.
[13] Peng Zhang, Dawei Song, Jun Wang, and Yuexian Hou. 2013. Bias-variance
Decomposition of Ir Evaluation. In Proceedings of the 36th International ACM
SIGIR Conference on Research and Development in Information Retrieval (SIGIR
’13). 1021–1024.
[14] Peng Zhang, Dawei Song, Jun Wang, and Yuexian Hou. 2014. Bias-variance
Analysis in Estimating True Query Model for Information Retrieval. Inf. Process.
Manage. 50, 1 (Jan. 2014), 199–217.

4.2.4 Achieved Significance Level and Estimated Difference. Algorithm 3 reflects that whether there is significant difference between
two systems. The smaller ASL indicates that the larger probability
of null hypothesis false. We assume that if ASL < 0.05 there is
significant difference between two systems. For Robust track 2004,
there are 110*109*0.5 system comparisons and each system pair
comparison is repeated B = 10000 times. The percent of ACL < 0.05
(denoted as PCT(ACL < 0.05)) of system comparisons displays in
the second row of Table 1. The power of discriminating different
systems of two models from high to low is BST and KDE. The figures signify about 10 percent system is significantly different. The
percent value on raw systems’ comparison is 0.8148, which is much
larger than the counterpart of simulation models. The third row
shows the ED for guaranteeing the significant difference between
two systems, which is pretty large. Similar results are observed for
Adhoc 1999.
4.2.5 Summary. According to the low ER and high KRCC, the
variability of ranking of systems is small if the per-topic value
fluctuates within a certain range or the per-topic variance is small.
The reason is that the ranking is based on the summary metric
MAP. When some per-topic metric values become larger and some
become smaller, the overall change of MAP is not obvious. The
significance test makes a hypothesis about the overall distribution
in advance and then judges whether the sample is from the overall
distribution. The variability of per-topic metric values for all topics
causes that the sample distribution may not be consistent with the

1184

