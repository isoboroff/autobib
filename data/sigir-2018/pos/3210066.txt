Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Split-Lists and Initial Thresholds for WAND-based Search
Andrew Kane

Frank Wm. Tompa

University of Waterloo
Waterloo, Ontario, Canada
arkane@uwaterloo.ca

University of Waterloo
Waterloo, Ontario, Canada
fwtompa@uwaterloo.ca

ABSTRACT

and combines them to give the top-k best documents according to
a speci�ed ranking algorithm. The standard approach merges lists
ordered by docid, resulting in document-at-a-time execution that
is fast and requires little temporary memory [5].
During list merging, each iteration of the search execution loop
will SELECT the next candidate document, SCORE the candidate,
and possibly SAVE it in a top-k heap, as depicted in Figure 1 (ignoring the green italic writing for now). The exhaustive-OR method
considers all documents in all the query lists (disjunctive merging), resulting in slow queries, but rank-safe results since every
candidate is scored. The exhaustive-AND method considers only
documents contained in all the query lists (conjunctive merging), resulting in fast queries, but non-rank-safe results since high-scoring
documents missing one or more terms are not found.

We examine search engine performance for rank-safe query execution using the WAND and state-of-the-art BMW algorithms.
Supported by extensive experiments, we suggest two approaches to
improve query performance: initial list thresholds should be used
when k values are large, and our split-list WAND approach should
be used instead of the normal WAND or BMW approaches. We also
recommend that reranking-based distributed systems use smaller k
values when selecting the results to return from each partition.

CCS CONCEPTS
• Information systems → Retrieval e�ciency; Information
retrieval query processing; Search engine indexing;

KEYWORDS

next

Information retrieval, Query performance, E�ciency, Optimization

state: <query_lists, current_list_pointers, heap, max_list_scores>

ACM Reference Format:
Andrew Kane and Frank Wm. Tompa. 2018. Split-Lists and Initial Thresholds
for WAND-based Search. In SIGIR ’18: The 41st International ACM SIGIR
Conference on Research & Development in Information Retrieval, July 8–
12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https:
//doi.org/10.1145/3209978.3210066

1

start

SELECT

end

<docid, max_potential_score>

SCORE
<docid, score>

SAVE

Figure 1: Search execution loop used by WAND and BMW.

INTRODUCTION

The WAND (Weak-AND) [4] and state-of-the-art BMW (BlockMax WAND) [10] approaches start executing as an OR query and
transition towards an AND as intermediate results improve. The
details of this transition produce both fast query execution and
rank-safe results. The WAND approach uses the current top-k results stored in the heap to give a threshold allowing more e�cient
skipping of potential candidates during SELECT processing. This
is done by ordering the query lists by their current docid and summing up their maximum list scores —in order— until it exceeds the
threshold at the pivot docid; then the smallest list before the pivot
is advanced, and a new pivot is calculated, repeating until enough
lists are on the same pivot docid to score the candidate. The BMW
approach adds maximum scores for each encoded list block in the
index, and uses them to prune candidates on each pivot calculation.
Thus, both approaches change SELECT processing to reduce candidates sent to the SCORE stage. The required additions to the basic
search execution loop are marked in Figure 1 (green italic writing).
We start with a recent WAND and BMW implementation using
quantized scores and incremental scoring for fast execution [6]. We
then improve upon this original code in three ways:
First, we apply small code optimizations to both the WAND and
BMW implementations that give signi�cant runtime improvements.
Second, we improve the query runtime of the WAND and BMW
algorithms for large k values by starting at an initial threshold
generated from indexing time analysis of the query lists.

With huge amounts of data and large query volumes, search engines
consume signi�cant resources, both in terms of computer hardware
and energy usage. The end user sees fast queries because the dataset
is partitioned across many machines. However, resource costs are
still present, so any improvement in e�ciency will give signi�cant
cost savings. In this paper, we present various optimization techniques for search execution that maintain the ranking e�ectiveness
of the system, so called rank-safe execution.
The details of executing queries in a search system are complex,
but the basic idea is simple. At indexing time, the data is inverted
to form a postings list for each token containing the locations of
that token in the dataset. Each list is stored in a compressed format
and often in document identi�er (docid) order. Within-document
locations are often dropped and only <docid, frequency> pairs are
stored. At query time, the system� nds the lists for the query terms
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for pro�t or commercial advantage and that copies bear this notice and the full citation
on the� rst page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speci�c permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210066

877

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA

Andrew Kane and Frank Wm. Tompa

Third, we show that split-lists (i.e., 2-layer lists split by score) can
signi�cantly improve WAND performance, making it faster than
the state-of-the-art BMW approach for many con�gurations. While
the 2-layer approach has been used with BMW [9, 10], the 2-layer
split-list WAND approach has not been previously examined.

●
●

30
runtime (ms/query)

2

40

RELATED WORK

Compression and Reordering. There are many encoding techniques that can make postings lists both fast and compact. For this
study we use QMX [16] to compress lists, as this method is used in
the original code, but many other approaches are available, such
as OptPFD [19], PForDelta (PFD) [21], Simple-9 [1], Simple-16 [20],
and partitioned Elias-Fano codes [12]. It is common to encode in
blocks and include skips, as we do, to jump over unused portions
of the lists. Ranking information can be stored immediately inside
the encoding or in parallel on the block encoding level.
Reordering documents (i.e., renumbering internal document
identi�ers) can improve both space and runtime. One common
technique that produces superior results reorders by URL [15], but
others reorder at random or by clustering [3, 14], document size [5,
§6.3.7], or global rank [11]. We examine both URL and random
ordering and expect other approaches to lie between these results.
Query Execution. Executing a query typically involves combining postings lists containing document identi�er and occurrence
frequency pairs using a ranking function. This combining is often
done using either document-at-a-time or term-at-a-time processing [18]; the former requires little temporary space, while the latter
can give better memory access performance. When lists are ordered
by document identi�er this is a simple merge with skipping within
lists. Alternatively, lists could be stored in impact order, which
allows for score-at-a-time processing and early termination [2].
Rather than storing frequencies in postings lists, partial scores
per-term can be stored instead, often as quantized values. Using
quantized scores can give runtime performance gains when combining document ordered lists, but the indexes are larger because
quantized scores are less compressible than frequencies [6, 9].

3

●

GOV2
(URL order)

●
●
●

20

●

●
●
●

10

●
●

●
●

●
●

0
1

140

●
●

runtime (ms/query)

120

10
WAND original code
BMW original code
WAND new code
BMW new code

50

100

500

1000 2000
●

ClueWeb09b
(URL order)
●

●

100

●
●
●

80
60

●

●
●

●

●

40
20

●
●
●

0
1

10

50

100

500

1000 2000

k value

Figure 2: Query runtime performance using original and
new implementations over various top-k values.
for recent research presented in a WSDM 2017 paper [6]. This code
�rst indexes the data using ATIRE [17], then converts that index to
a document ordered encoding with QMX [16] compression on 128
element blocks and skip structures over blocks. We use the faster
variant storing precomputed partial BM25 scores in the index quantized as small integers, rather than storing document frequencies
and calculating the partial scores at runtime, even though this does
increase index size. In the query execution loop, the SCORE portion
of the query is done incrementally, which avoids full scoring for
many candidate documents and makes execution faster.
We added code to allow reordering of documents during conversion from the ATIRE index into WAND and BMW indexes. We
follow the standard approach of reordering our indexes by URL,
which produces a signi�cant performance improvement [15].
We� nd that this original implementation of the WAND and
BMW algorithms has good performance over a range of k values,
as shown for URL ordered versions of GOV2 and ClueWeb09b in
Figure 2 (orange lines). Since using smaller k values can give signi�cant performance gains, reranking systems with multiple partitions
should use smaller k values in the partitions and ensure rank-safe
results by revisiting a partition for more results as needed [13].
Thus, for example, rather than retrieving 1000 matches from each
partition, a highly distributed web search engine could use a very
small k value for each partition, and revisit very few partitions, to
produce the top-1000 matches needed for reranking.

EXPERIMENTAL SETUP

Our experiments are run on two datasets with associated workloads: the GOV2 dataset is 426GB of data in 25.2 million documents
using the TREC ’04-’06 query topic workload (701-850) and the
ClueWeb09b dataset is 1.39 TB of data in 50.2 million documents
using the TREC ’10-’12 query topic workload (51-200). Associated
user evaluations ensure no degradation of ranking e�ectiveness.
Our experiments are run on a large 4x22 core Xeon 2.2GHz Linux
machine with 1TB of memory. Postings lists are loaded into memory
and full query workloads are executed ten times single threaded
to avoid CPU caching. We explore various top-k values, namely
k 2 {1, 10, 50, 100, 500, 1000, 2000}, to give a broad understanding
of query performance. We� nd that workload runtimes are stable
with small standard deviations of less than 1.3% for k 10.

4

WAND original code
BMW original code
WAND new code
BMW new code

ORIGINAL CODE

The WAND and BMW code with which we started is a state-of-theart publicly available1 implementation developed at RMIT and used

5

NEW CODE OPTIMIZATIONS

While it is quite common to add assertion type checks in low level
methods, these can cause performance slowdowns if they are not

1 https://github.com/jmmackenzie/Quant-BM-WAND/

878

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Split-Lists and Initial Thresholds for WAND-based Search

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA

removed or turned o� in production code. As such, we removed two
assertion type checks from the underlying compressed list access
methods to give performance gains.
Standard template classes are highly useful and often well optimized, but using some generalized accessor paradigms may not
be the most e�cient choice for simple tasks. The original implementation uses std::array objects to store the sorted list pointers
and general std::iterators are used to traverse over these objects.
We changed these traversals to use operator[] accesses, which is
signi�cantly faster, especially when the arrays are small.
Returning multiple objects from a method can be accomplished
by forming std::pair or std::tuple objects to pass back to the caller,
but this can be expensive if the method is called a large number of
times. We changed such methods to pass objects by reference so
that the method can update and the caller can access the object.
The search execution loop must SAVE candidates with high
enough scores, as shown in Figure 1. This is done using a heap,
so that a candidate is added when the heap size is less than k, or
the minimum scored value in the heap is replaced by the candidate
when the candidate’s score is higher (i.e., a minimum heap). Rather
than checking if the heap size is less than k for each candidate, we
initialize the heap with k dummy values having a score of zero, and
then avoid outputting them at the end of query execution. This
improves performance and also allows for easy implementation of
the initial list thresholds described in the next section.
Combining these simple code optimizations results in runtime
performance gains, as shown in Figure 2 (blue lines). While these optimizations improve performance for both approaches, the WAND
algorithm bene�ts somewhat more than does BMW (average speedup
of 1.22x vs. 1.16x).

6

40
WAND new code
BMW new code
WAND list thresholds
BMW list thresholds

runtime (ms/query)

30

GOV2
(URL order)

20

10

0
1

140

WAND new code
BMW new code
WAND list thresholds
BMW list thresholds

120
runtime (ms/query)

10

50

100

500

1000 2000

500

1000 2000

ClueWeb09b
(URL order)

100
80
60
40
20
0
1

10

50

100

k value

Figure 3: Query runtime performance using list thresholds
over various top-k values.
performance in other situations. Using list thresholds, we� nd that
the GOV2 dataset has signi�cant gains for a range of large k values,
while ClueWeb09b has some gains for large k values, as shown in
Figure 3 (red lines). Since list thresholds can produce performance
gains without noticeably degrading any con�guration and they can
be stored in the index with little overhead, we expect that employing
list thresholds will be bene�cial in many search systems.

INITIAL LIST THRESHOLDS

During indexing, we determine the k th highest scoring value for
each list of at least size k and store the values in a separate�le
on disk. Before we start executing a query, we calculate the initial
threshold by simply taking the maximum k th score value for all the
query lists (minus one for quantized scoring or minus a small epsilon for exact ranking). Using our pre-populated heap optimization
from the last section, we set the score of the dummy values to be
this initial list threshold, thus de�ning a minimum starting threshold for query execution. Since one of the query lists can generate k
values higher than this threshold, we know that the� nal threshold
must be higher than this value, and thus, the query execution will
be rank-safe (i.e., it will produce the k highest ranked results). This
initial threshold approach was brie�y explored in recent work [7].
This approach does restrict the choice of k at query time, though
any query using a k value smaller than what was used at indexing
time to produce the stored scores can be used and still produce ranksafe results. If k is large, the number of lists of at least size k will
likely be small and require little space to store their list thresholds
(e.g., our indexes add less than 0.01% space overhead at k = 1000).
For a multi-partition index, these list thresholds could be calculated
globally, giving higher values that would improve performance;
thus the larger the system, the bigger the gain.
We� nd that using list thresholds as we have described improves
performance for large k values and does not noticeably degrade

7

SPLIT-LISTS

During indexing time, we split each list into two parts, where the
�rst part contains the highest scored documents for that term and
the second part contains the remaining documents (i.e., 2-layer
lists split by score [10]). There are many ways to decide how many
documents to place in each list, but we simply pick a percentage of
the list. Since we are using a quantized score value we split along
quantum boundaries, but these boundaries could be far apart, so
we pick the last boundary before the desired cut point. In addition,
we only split lists with more than 10,000 postings.
At query time, the split-lists are both included in the WAND
pivot processing, but the maximum list scores used depend on
whether the paired split-list has already been included in the pivot
process (i.e., both lists cannot contain the same document, so if
both are in the pivot calculation, then the maximum score of the
paired lists is used). This checking of pairs adds some overhead,
but allows for more skipping. We� nd that for our split-list WAND
implementation, checking pairs is faster than not checking.
Our split-list WAND implementation produces signi�cant gains
over traditional WAND processing, as shown in Figure 4 (green
lines). Indeed, split-list WAND is faster than the state-of-the-art

879

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA

Andrew Kane and Frank Wm. Tompa
Table 1: Runtime performance for k=10 and k=1000 (ms).

40
WAND list thresholds
BMW list thresholds
WAND split−lists

GOV2
(URL order)

runtime (ms/query)

30

WAND original code
WAND original code
WAND new code
WAND list thresholds
WAND split-lists (10%)
BMW original code
BMW original code
BMW new code
BMW list thresholds
BMW 2-layer (2%)
BMW 2-layer (10%)

20

10

0
1

140

WAND list thresholds
BMW list thresholds
WAND split−lists

120
runtime (ms/query)

10

50

100

500

1000 2000

ClueWeb09b
(URL order)

GOV2
k=10 k=1000
23.25
56.20
9.91
29.47
8.35
23.60
8.30
19.74
4.81
15.95
18.93
51.71
6.10
19.86
5.29
17.03
5.23
14.74
6.03
17.31
8.28
20.34

80
60

REFERENCES

40

[1] Vo Ngoc Anh and Alistair Mo�at. 2005. Inverted index compression using wordaligned binary codes. Information Retrieval 8, 1 (2005), 151–166.
[2] Vo Ngoc Anh and Alistair Mo�at. 2005. Simpli�ed similarity scoring using term
ranks. In SIGIR. 226–233.
[3] Dan Blandford and Guy Blelloch. 2002. Index compression through document
reordering. In DCC. 342–351.
[4] Andrei Z. Broder, David Carmel, Michael Herscovici, Aya So�er, and Jason Zien.
2003. E�cient query evaluation using a two-level retrieval process. In CIKM.
426–434.
[5] Stefan Büttcher, Charles Clarke, and Gordon V. Cormack. 2010. Information
retrieval: Implementing and evaluating search engines. The MIT Press.
[6] Matt Crane, J. Shane Culpepper, Jimmy Lin, Joel Mackenzie, and Andrew Trotman. 2017. A Comparison of Document-at-a-Time and Score-at-a-Time Query
Evaluation. In WSDM. 201–210.
[7] Caio Moura Daoud, Edleno Silva de Moura, Andre Carvalho, Altigran Soares da
Silva, David Fernandes, and Cristian Rossi. 2016. Fast top-k preserving query
processing using two-tier indexes. Information processing & management 52, 5
(2016), 855–872.
[8] Constantinos Dimopoulos, Sergey Nepomnyachiy, and Torsten Suel. 2013. A
candidate� ltering mechanism for fast top-k query processing on modern CPUs.
In SIGIR. 723–732.
[9] Constantinos Dimopoulos, Sergey Nepomnyachiy, and Torsten Suel. 2013. Optimizing top-k document retrieval strategies for block-max indexes. In WSDM.
113–122.
[10] Shuai Ding and Torsten Suel. 2011. Faster top-k document retrieval using blockmax indexes. In SIGIR. 993–1002.
[11] Xiaohui Long and Torsten Suel. 2003. Optimized query execution in large search
engines with global page ordering. In VLDB. 129–140.
[12] Giuseppe Ottaviano and Rossano Venturini. 2014. Partitioned Elias-Fano indexes.
In SIGIR. 273–282.
[13] Oscar Rojas, Veronica Gil-Costa, and Mauricio Marin. 2013. E�cient parallel
block-max WAND algorithm. In Euro-Par. 394–405.
[14] Wann-Yun Shieh, Tien-Fu Chen, Jean Jyh-Jiun Shann, and Chung-Ping Chung.
2003. Inverted� le compression through document identi�er reassignment. Information Processing & Management 39, 1 (2003), 117–131.
[15] Fabrizio Silvestri. 2007. Sorting out the document identi�er assignment problem.
Advances in Information Retrieval (2007), 101–112.
[16] Andrew Trotman. 2014. Compression, SIMD, and postings lists. In ADCS. 50–57.
[17] Andrew Trotman, Xiangfei Jia, and Matt Crane. 2012. Towards an e�cient and
e�ective search engine. In Workshop on Open Source Information Retrieval. 40–47.
[18] Howard Turtle and James Flood. 1995. Query evaluation: strategies and optimizations. Information Processing & Management 31, 6 (1995), 831–850.
[19] Hao Yan, Shuai Ding, and Torsten Suel. 2009. Inverted index compression and
query processing with optimized document ordering. In WWW. 401–410.
[20] Jiangong Zhang, Xiaohui Long, and Torsten Suel. 2008. Performance of compressed inverted list caching in search engines. In WWW. 387–396.
[21] Marcin Zukowski, Sandor Heman, Niels Nes, and Peter Boncz. 2006. Super-scalar
RAM-CPU cache compression. In ICDE. 59:1–12.

0
1

10

50

ClueWeb09b
k=10 k=1000
121.00
179.98
74.94
118.80
60.61
97.35
61.03
87.01
15.35
55.18
54.51
152.59
24.68
78.12
21.69
65.65
21.66
62.10
29.61
70.94
52.92
83.70

WAND should be considered for use in many search systems. Our
implementation of these algorithms is publicly available2 .
Acknowledgments. The computing resources used in this research were made available by the University of Waterloo.

100

20

100

500

1000 2000

k value

Figure 4: Query runtime performance using split-lists over
various top-k values.
BMW algorithm in ClueWeb09b for all k values and in GOV2 for
small k values. The improvement in the ClueWeb09b dataset is quite
signi�cant, split-list WAND at k = 10 gives a 1.41x speedup over an
already highly optimized BMW implementation for URL order (and
more for random order). Importantly, compared to using a normal
WAND implementation, the improvement from switching to splitlist WAND is large for all k values in both datasets (ranging from
1.20x to 6.39x speedup with a 10% space overhead). We expect that
split-list WAND can be combined with other existing approaches
for additional gains (e.g.,� ltering [8] and 3-phase retrieval [7]).
Previous work on 2-layer BMW suggests it is faster than BMW
when no pair checking is implemented [10], but we found 2-layer
BMW to be slower both with and without pair checking, likely
from our fast scoring limiting gains from additional BMW candidate pruning. Our split-list WAND approach can exploit much
larger high-score splits than previously recommended for the 2layer BMW approach, in particular, we use a cuto� of 10%, which
outperforms the 2% split recommended for 2-layer BMW.

8

order
rand
url
url
url
url
rand
url
url
url
url
url

CONCLUSIONS

A summary of the runtime performance of the algorithms presented
in this paper for k 2 {10, 1000} is shown in Table 1. We� nd that
using smaller k values in partitions and making some simple code
changes can greatly improve query execution times. The simple
idea of using initial thresholds based on the k th highest score in each
query list can also improve performance for both WAND and BMW
when k is large. Finally, we� nd that our split-list WAND approach
signi�cantly outperforms the basic WAND approach and can also
outperform the state-of-the-art BMW approach. As such, split-list

2 https://github.com/andrewrkane/Quant-BM-WAND/

880

