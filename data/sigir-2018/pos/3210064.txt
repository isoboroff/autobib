Session 1B: Log Analysis

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Predicting User Knowledge Gain
in Informational Search Sessions
Ran Yu

Ujwal Gadiraju

Peter Holtz

L3S Research Center
Hannover, Germany
yu@l3s.de

L3S Research Center
Hannover, Germany
gadiraju@l3s.de

Leibinz Insitut für Wissensmedien
Tübingen, Germany
holtz@iwm-tuebingen.de

Markus Rokicki

Philipp Kemkes

Stefan Dietze

L3S Research Center
Hannover, Germany
rokicki@l3s.de

L3S Research Center
Hannover, Germany
kemkes@l3s.de

L3S Research Center
Hannover, Germany
dietze@l3s.de

ABSTRACT

Search Sessions. In SIGIR ’18: The 41st International ACM SIGIR Conference
on Research and Development in Information Retrieval, July 8–12, 2018, Ann
Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/
3209978.3210064

Web search is frequently used by people to acquire new knowledge
and to satisfy learning-related objectives. In this context, informational search missions with an intention to obtain knowledge
pertaining to a topic are prominent. The importance of learning
as an outcome of web search has been recognized. Yet, there is
a lack of understanding of the impact of web search on a user’s
knowledge state. Predicting the knowledge gain of users can be an
important step forward if web search engines that are currently
optimized for relevance can be molded to serve learning outcomes.
In this paper, we introduce a supervised model to predict a user’s
knowledge state and knowledge gain from features captured during
the search sessions. To measure and predict the knowledge gain
of users in informational search sessions, we recruited 468 distinct
users using crowdsourcing and orchestrated real-world search sessions spanning 11 different topics and information needs. By using
scientifically formulated knowledge tests, we calibrated the knowledge of users before and after their search sessions, quantifying
their knowledge gain. Our supervised models utilise and derive
a comprehensive set of features from the current state of the art
and compare performance of a range of feature sets and feature
selection strategies. Through our results, we demonstrate the ability
to predict and classify the knowledge state and gain using features
obtained during search sessions, exhibiting superior performance
to an existing baseline in the knowledge state prediction task.

1

INTRODUCTION

Searching the web for information is among the most frequent
online activities. Broder categorized web search queries into having
either navigational, transactional or informational intents [4]. In
informational web search sessions, the intent of a user is to acquire
some information assumed to be present on one or more web pages.
Recent research in the search as learning (SAL) domain has recognized the importance of learning scopes and focused on observing and detecting learning needs during web search. Eickhoff et
al. investigated the correlation between several query and search
mission-related metrics and learning progress [8]. Wu et al. predicted the difficulty of search tasks from query and mission-related
features [21]. Collins-Thompson et al. investigated the effectiveness
of user interaction with respect to certain learning outcomes [7].
In addition, [22] has shown that data obtained during the search
process provides valuable indicators about the domain knowledge
of a user.
Although the importance of learning as an implicit element of
web search has been established, there is still only a limited understanding of the impact of search behavior on a user’s knowledge
state and knowledge gain. Prior work has focused on improving the
learning experience and efficiency during search sessions, but the
measurement of a user’s knowledge gain through the course of an
informational search session has not yet been addressed. This is in
part due to the difficulty in accurately quantifying knowledge gain
through the course of a search session. If web search engines that
are currently optimized for relevance can be re-molded to serve
learning outcomes, the capability to predict knowledge gain will
be a crucial step forward.
In this paper, we aim to address the aforementioned gap. We
used crowdsourcing to recruit users who participated in real-world
search sessions spanning 11 different topics and information needs.
By using scientifically formulated knowledge tests, we calibrated
the knowledge of users before and after their search sessions, quantifying their knowledge gain. We introduce a supervised model to
predict a user’s knowledge state and knowledge gain from features
captured during the search sessions.

CCS CONCEPTS
• Human-centered computing → User models; • Computing
methodologies → Supervised learning; • Applied computing →
Interactive learning environments;
ACM Reference Format:
Ran Yu, Ujwal Gadiraju, Peter Holtz, Markus Rokicki, Philipp Kemkes,
and Stefan Dietze. 2018. Predicting User Knowledge Gain in Informational
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210064

75

Session 1B: Log Analysis

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Original Contributions. Through our work in this paper, we
make the following contributions to the current body of literature:
• A model for predicting the user’s knowledge gain and state during
real-world informational search sessions.
• An analysis of the affect of user interactions (ranging from the
queries entered to their browsing behavior) on their knowledge
state and knowledge gain.
• We release a dataset capturing user behavior and interactions in
468 experimentally orchestrated informational search missions,
capturing all features across the aforementioned dimensions as
well as knowledge assessments obtained through pre- and posttests. Given the lack of comparable datasets which are both recent
and publicly available, we anticipate that this corpus can facilitate
SAL research related to different tasks.
Implications. The capability to predict a user’s knowledge state
and gain through the course of an informational search session
has the potential to reshape search engines to support learning
outcomes as an implicit part of retrieval and ranking. This is of particular importance given that Web search already augments learning processes in a variety of informal as well as formal learning
scenarios, such as classrooms, libraries and in work environments.
Our contributions advance the current understanding of learning
through web search, setting important precedents for further research.

2

our work we have also taken into consideration the set of features
have been studied in this work.
The aforementioned prior works have either studied a limited set
of features or have addressed only specific learning scenarios and
learning types. The generalizability of knowledge gain measures in
previous works has not been investigated. In this paper, we extend
the current understanding of user knowledge gain in informational
search sessions. Using real world information needs and search
sessions on the Web, we investigate the possibility of using search
activity related features to predict knowledge gain.

2.2

RELATED WORK

We discuss two main realms of closely related work – studies on
the relation between (i) a user’s search behavior and knowledge
gain, and (ii) a user’s search behavior and knowledge state.

2.1

Search Behavior and Knowledge State

By matching the learning tasks into different learning stages of
Anderson and Krathwohl’s taxonomy [1], Jansen et al. studied the
correlation between search behaviors of 72 participants and their
learning stage [15]. They showed that information searching is a
learning process with unique searching characteristics corresponding to particular learning levels. Gwizdka et al. [12] proposed to
assess learning outcomes in search environments by correlating
individual search behaviors with corresponding eye-tracking measures. Syed and Collins-Thompson [18] proposed to optimize the
learning outcome of the vocabulary learning task by selecting a
set of documents while considering keyword density and domain
knowledge of the learner.
White et al. [20] investigated the difference between the behavior
of domain experts and non-experts in seeking information on the
same topic. By analyzing the activity log of experts and non-experts
across different domains, the authors found that the distribution
of features such as number of queries and query length differed
across the levels of expertise. Zhang et al. [22, 23] explored using
search behavior as an indicator for the domain knowledge of a
user. Through a small study (n = 35), they identified features such
as the average query length or the rank of documents consumed
from the search results as being predictive. Further, Cole et al. [6],
observed that behavioral patterns provide reliable indicators about
the domain knowledge of a user, even if the actual content or topics
of queries and documents are disregarded entirely.
Other studies have focused on detecting task difficulty in search
environments based on user activity data in situations where the
subjective assessment of task difficulty is highly correlated to the
user’s domain knowledge [13, 17]. Gwizdka and Spence [13] showed
that a searcher’s perception of task difficulty is a subjective factor
that depends on the domain knowledge and some other individual
traits. Arguello [2] proposed to use logistic regression to predict
task difficulty in a search environment. Data was collected through
a crowdsourcing platform, and the author used search tasks created
by Wu et al. [21], which contain task difficulty assessments on
multiple dimensions.
The aforementioned studies focused on investigating the relation
between search behavior and a user’s knowledge state. Our work
leverages these results to derive a comprehensive feature set for our
supervised models. In contrast to prior works, we aim at predicting
the knowledge state of a user – avoiding the need for explicit postsearch knowledge assessments.

Search Behavior and Knowledge Gain

Eickhoff et al. [8] investigated the correlation between a number
of features extracted from search session as well as SERP (Search
Engine Results Page) documents with learning needs related to
either procedural or declarative knowledge. Results obtained from
an analysis of large-scale query logs showed the distinct evolution
of particular features throughout search sessions and the correlation
of document features with the actual learning intent. The influence
of distinct query types on knowledge gain was studied by CollinsThompson et al. [7], finding that intrinsically diverse queries lead
to increased knowledge gain. Gadiraju et al. [11] described the use
of knowledge tests to calibrate the knowledge of users before and
after their search sessions, quantifying their knowledge gain. They
investigated the impact of information needs on the search behavior
and knowledge gain of users.
Studies on exploratory search have also investigated a similar set
of search behaviors that influence the learning outcome. Hagen et
al. [14] investigated the relation between the writing behavior and
the exploratory search pattern of writers. The authors revealed that
query terms can be learned while searching and reading. In addition,
Vakkari [19] provided a structured survey of features indicating
learning needs as well as user knowledge and knowledge gain
throughout the search process. Zhuang et al. [24] investigated the
possibility of using 37 user search behavioral features to predict the
user engagement with supervised classifiers. As the engagement in
the search process usually is correlated with learning outcome, in
2

76

Session 1B: Log Analysis

3

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

4.1

PROBLEM DEFINITION

Definition 3.1. Intentional Learning-Related Search Session. An intentional learning-related search session comprises of the sequence
of a user’s actions, with respect to satisfying her learning intent
in a web search environment through informational queries. A
user’s sequence of actions begins with querying the web, and includes browsing through the search results, click and scroll activity,
navigation via hyperlinks, query reformulations, and so forth.
For the sake of simplicity, we henceforth refer to informational
sessions, i.e. sessions with a particular learning intent, as “sessions”.
In this paper, from the observed user interactions in informational search sessions, we aim to predict (i) the knowledge state and
(ii) knowledge gain of a user as follows.
Definition 3.2. Predicting a User’s Knowledge State and Gain During Search Sessions. Let s be a search session starting at time ti and
ending at time t j aimed at satisfying a particular information need,
that is, a learning intent ι of user u. Based on the user interactions
during session s captured in the time period [ti , t j ], we aim to:
(1) classify the knowledge state (KS) k(t j ) of u at time point t j with
respect to a particular information need. For the sake of this
work, a user’s knowledge state with respect to a particular information need is defined by the user’s capability to correctly
respond to a set of questions about the corresponding information need. We classify a user’s knowledge state into 3 classes
according to her capability: low knowledge state, moderate
knowledge state and high knowledge state (Section 4.5).
(2) classify the knowledge state change, i.e. the knowledge gain
(KG) ∆k(ti , t j ) of u during time period [ti , t j ] into different degrees. Similarly, a user’s knowledge gain with respect to a particular information need is defined as the improvement of user capability (accuracy) to correctly respond to a set of test questions
about the corresponding information need. We classify user
knowledge gain into 3 classes according to the improvement of
user capability: low knowledge gain, moderate knowledge gain
and high knowledge gain (Section 4.5).

4

Study Design and Search Environment

We recruited participants from CrowdFlower1 , a premier crowdsourcing platform. At the onset, workers were informed that the
task entailed ‘searching the Web for some information’. Workers
willing to participate were redirected to our external platform,
SearchWell 2 , a search system built on top of the Bing Web Search
API. We logged worker activity on the platform including mouse
movements, clicks, and key presses, using PHP/Javascript and the
jQuery library. Workers were first asked to respond to a few questions (called ‘items’) corresponding to a particular topic without
searching the Web for answers. The questions took the form of
statements pertaining to a topic, and workers had to select whether
the statement was ‘TRUE’, ‘FALSE’, or ‘I DON’T KNOW’ in case
they were not sure. In this way, we calibrated the knowledge of
users corresponding to a given topic. To encourage the workers to
respond without external consultation, we informed them that their
responses to these questions would not affect their pay. We also
encouraged workers to avoid guessing. The results of this pre-test
were used to calibrate the knowledge of the workers with respect to
the topic. We describe the topics and how the knowledge tests were
created in the following Section 4.2. On completing the knowledge
calibration test, workers were presented with their actual task.
Workers were presented an information need corresponding to
the topic of the calibration test they completed. They were told to
use the SearchWell platform to search the Web and satisfy their
information need. To incentivize workers towards realistic attempts
to learn about the topic, we informed them that they will have to
complete a final test on the topic to successfully finish the task.
Furthermore, workers were conveyed the message that depending
on their accuracy on the final test they could earn a bonus payment.
We subsequently logged all the activities of the workers (mouse
movements, key presses and clicks) within the SearchWell platform. Workers were allowed to begin the final test anytime after
a search session, which is when a link to the final test was made
available. Workers were encouraged to proceed to the next stage
only once they felt that their information need was satisfied and
when they were ready for the post-session test. On completing the
post-session test, workers received a unique code that they could
enter on CrowdFlower to claim their reward.
We restricted the participation to workers from English-speaking
countries to ensure that they understood the task and instructions
adequately [9, 10]. To ensure reliability of the resulting data, we
restricted the participation to Level-3 workers3 on CrowdFlower.

In the context of Web search, Broder [4] classified search queries
according to their intent into three classes: 1) navigational, 2) informational, and 3) transactional. Herein, informational queries
are defined as those queries where ‘the intent of a user is to acquire some information assumed to be present on one or more web
pages’ [4]. Thus, informational queries imply a particular learning
intent; intentional learning is generally defined as learning that
is motivated by intentions and is goal directed [3], in contrast to
latent or incidental learning.
Based on the constructs of intentional learning and informational
queries, we arrive at the following definition:

4.2

Topics – Defining Information Needs

We constructed a corpus of topics representing varying scopes of information needs (with some relatively broader than others). Topics
were selected from the TREC 2014 Web Track dataset4 , and corresponding information needs were defined accordingly. In all cases,
the knowledge of users before beginning an informational search
session was assessed using pre-tested and evaluated knowledge tests.

OBTAINING SEARCH SESSION DATA

We adopted a crowdsourcing approach and orchestrated search
sessions with varying information needs. All interactions of the
users during the search sessions were logged. We analyzed the
data to further the understanding of user knowledge evolution
in informational search sessions on the Web. In this section, we
describe the study design and experimental setup.

1 http://www.crowdflower.com/
2 http://searchwell.l3s.uni-hannover.de/?uid=12345678.
3 Level-3

contributors on CrowdFlower comprise workers who completed over 100 test
questions across hundreds of different types of tasks, and have a near perfect overall
accuracy. They are workers of the highest quality on CrowdFlower.
4 http://www.trec.nist.gov/act_part/tracks/web/web2014.topics.txt
3

77

Session 1B: Log Analysis

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: Topics and corresponding information needs presented to participants in the informational search sessions, along
with the internal reliability of the corresponding knowledge tests. ‘α1’, ‘α2’ represent Cronbach’s α for the pre-session test
and post-session test respectively. ‘N’ is the number of reliable participants after filtering.
Topic

Information Need

α1 α2 N

1. Altitude Sickness
2. American Revolutionary War
3. Carpenter Bees

In this task you are required to acquire knowledge about the symptoms, causes and prevention of altitude sickness. (20 items)
In this task, you are required to acquire knowledge about the ‘American Revolutionary War’. (10 items)
In this task, you are required to acquire knowledge about the biological species ‘carpenter bees’. How do they look? How do
they live? (10 items)
In this task, you are required to acquire knowledge about the theory of evolution. (12 items)
In this task, you are required to acquire knowledge about the past, present, and possible future of interplanetary missions
that are planned by the NASA. (20 items)
In this task you are required to acquire knowledge about the Orcas Island. (20 items)
In this task, you are required to acquire knowledge about ‘Sangre de Cristo’ mountain range. (10 items)
In this task, you are required to acquire knowledge about the Chinese author Sun Tzu - about his life, his writings, and his
influence to the present day. (15 items)
In this task, you are required to acquire knowledge about the weather phenomenon that is called ‘tornado’ (20 items)
In this task, you are required to acquire knowledge about the 2000 terrorist attack that came to be known as the ‘USS Cole
bombing’. (10 items)
In this task you are required to acquire knowledge about the transmission, prevention, and consequences of HIV infection.
(45 items)

0.59 0.79 44
0.74 0.55 39
0.79 0.58 43

4. Evolution
5. NASA Interplanetary Missions
6. Orcas Island
7. Sangre de Cristo Mountains
8. Sun Tzu
9. Tornado
10. USS Cole Bombing
11. HIV

Knowledge tests are scientifically formulated tests that measure the
knowledge of a participant on a given topic (for example, the HIV
knowledge test [5]). Topics and test items are available online5 .
Knowledge on all given topics was measured using knowledge
tests comprising of between 10 and 45 items. The answer options
were in all cases ‘TRUE’, ‘FALSE’, and ‘I DON’T KNOW’. The differences in the number of items reflects our attempt to feature varying
scopes of information needs; relatively narrow (e.g., Carpenter Bees–
10 items) as well as broad (e.g., NASA Interplanetary Missions–20
items). In the construction of all scales, an item pool comprising
of more items than finally used was constructed. After a pilot test
with 100 distinct participants recruited via CrowdFlower for each
of the 11 topics, items that proved to be either too easy (e.g., more
than 80% correct answers) or too hard/ambiguous (e.g., more false
than true answers) were discarded. Table 1 presents the topics and
corresponding information needs considered for orchestrating the
informational search sessions. It also shows the internal reliability
(using Cronbach’s α) of the pre- and post-session knowledge tests
corresponding to each topic. We observe moderate to high values
of α in the pre- and post session knowledge tests, suggesting a
desirable level of internal consistency.

4.3

0.55 0.72 42
0.80 0.75 40
0.91 0.85 38
0.70 0.52 38
0.81 0.63 35
0.82 0.62 37
0.83 0.55 38
0.87 0.84 74

alone. For the benefit of further research in this community, the
filtered data has been thoroughly anonymized and made publicly
available5 . We henceforth refer to these filtered workers as users
in our experimentally orchestrated information search sessions.

4.4

Descriptive Analysis – Important Details

We measure the knowledge gain of users in search sessions corresponding to an information need as the difference between their
knowledge calibration score and the post-session test score6 . Table
2 presents the average knowledge calibration scores, post-session
test scores, and the resulting knowledge gain of users across search
sessions with different information needs. Across all topics and
search sessions, we found that users exhibited an average knowledge gain of around 19%. Nearly 70% of all the workers exhibited a
knowledge gain, while the remaining workers did not. The standard
deviation observed in the knowledge gain of users across all topics
is notably high, due to the varying domain knowledge of users.
This is evident from the average calibration scores in Table 2.
Table 2: The average knowledge gain of users across the different topics. To enhance readability, the rows have been ordered by ascending knowledge gain.

Data Collection

Topic /
Information Need

To further ensure the reliability of responses and the behavioral
data thus produced in the search sessions, we filtered workers using
the following criteria.
• Workers who entered no queries in the SearchWell system. Since
the aim of our work is to further the understanding of how
the knowledge state of a user evolves in informational search
sessions, we discard those users who did not enter a search query.
• Workers who selected the same option; either ‘YES’ or ‘NO’, for
all items in the calibration test or the post-session test.
• Workers who did not complete the post-session test.
We filtered out 132 workers due to the aforementioned criteria, resulting in 468 workers across the 11 topics. The analysis
and results presented hereafter are based on these 468 sessions

Avg. Calibration
Score (in %)

Avg. Post
Score (in %)

Knowledge Gain
(in %)

HIV (N=74)
Evolution (N=42)
NASA Interplanetary
Missions (N=40)
Altitude Sickness (N=44)
Sangre de Cristo
Mountains (N=38)
Tornados (N=37)
Sun Tzu (N=35)
American Revolutionary
War (N=39)
Carpenter Bees (N=43)
USS Cole Bombing (N=38)
Orcas Island (N=38)

66.25 ± 14.86
34.07 ± 17.99
38.1 ± 20.53

71.68 ± 14.48
48.15 ± 22.49
52.5 ± 17.43

5.44 ± 10.02
14.07 ± 18.66
14.40 ± 22.10

55.88 ± 16.31
33.25 ± 22.40

70.66 ± 19.11
49.75 ± 18.10

14.78 ± 17.76
16.50 ± 22.31

34.44 ± 21.02
40.54 ± 23.37
34.52 ± 25.65

53.47 ± 16.28
60.18 ± 17.15
55.95 ± 20.71

19.03 ± 22.010
19.64 ± 21.59
21.43 ± 27.31

45.65 ± 27.08
30.95 ± 25.22
34.74 ± 30.08

67.17 ± 20.29
54.37 ± 16.29
65.51 ± 22.04

21.52 ± 30.50
23.41 ± 31.30
30.77 ± 30.25

Overall (N=468)

40.76 ± 22.23

59.04 ± 18.58

18.27 ± 23.07

6 We consider the ‘I DON’T KNOW’ options that were selected, as incorrect responses
while computing the knowledge calibration scores and post-session test scores.

5 https://sites.google.com/view/predicting-user-knowledge

4

78

Session 1B: Log Analysis

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

We found that on average, the highest knowledge gain was
observed through the search sessions corresponding to the topic,
‘Orcas Island’, while the least knowledge gain was observed through
those corresponding to the topic, ‘HIV ’. We found a negative linear relationship between topics that workers were familiar with
(indicated by their calibration scores) and their knowledge gain;
R= −.65, R 2 = .42, p < .001. This suggests that the more popular a
topic is, or the more familiar that users are with a topic, the lesser
they tend to learn about the topic in informational search sessions.
Thus, we found that 42% of the variance in the knowledge gain of
users can be explained by the topic familiarity.
We found that the average session length of users across the
different topics was nearly 5 mins long (M=4.82, SD=5.20). During
the search sessions, users navigated to over 5 web pages on average
(M=5.46, SD=3.41). We note that on average users entered 2 distinct
queries in a search session (M=2.20, SD=2.18), with an average
query length of just over 4 terms (M=4.56, SD=2.63). For each query
that was entered, users navigated to over 3 web pages on average
(M=3.27, SD=1.60). Users spent nearly 2 minutes actively on web
pages they navigated to (M=1.97, SD=1.52).

4.5

considered features are described in Section 5.1 and analyzed in
Section 5.2.

5.1

We extracted features according to multiple dimensions of a search
session, structured into five categories, namely features related to
the session, queries, SERP, browsing behavior and mouse movements.
The SERP category consists of features extracted from direct interactions with SERP items, while the browsing category consists of
features extracted from subsequent user navigation beyond simple
SERP clicks. The majority of features is motivated by existing literature, yet none of the features have been used on the inferential
tasks of this work.
All considered features fi are listed in Table 4 together with
the Pearson Correlation Coefficient scores Corr (fi , ∆k(ti , t j )),
Corr (fi , k(t j )) between the respective feature and the knowledge
gain (state).
Session Features. The relation between feature s_duration and
different stages of learning has been discussed by Jansen et al. [15].
It has been shown that there is a difference in the duration of
sessions among the classifications in Anderson and Krathwohl’s
taxonomy [1]. White et al. [20] also found that the sessions conducted by domain experts were generally longer than non-expert
sessions.
Query Features. Several prior works [2, 15, 20] have investigated the correlation between query activities in a search session
and learning performance. Based on the study by White et al. [20],
the number of queries (q_num) applied by experts and non-experts
show big differences across domains: non-expert users usually run
significantly more queries than experts. Jansen et al. [15] also found
that the number of queries applied on learning tasks classified as
applying stage was significantly different from other learning stages.
The length of queries (q_term_max {min, avд, total }) has been
found to have a strong correlation with learning outcome by Zhang
et al. [22]. Their study shows that the average query length and user
domain knowledge is correlated with a Pearson correlation score
of 0.344.
The complexity of queries (q_complexity_max_di f f ) has been
investigated by Eickhoff et al. [8], and has been found to evolve
during the learning process. We applied the same query complexity measure as in [8], which is computed based on the dictionary
created by Kuperman et al. [16] that contains a listing of more than
30,000 English words along with the age at which native speakers
typically learn the term. The maximum age of acquisition across
all query terms is used as query complexity.
Furthermore, the investigation from Arguello [2] shows that
beside the number of total terms, the number of unique terms
(q_uniq_term_{max, min, avд, total }, q_uniq_term_ratio) in the
session is strongly correlated with knowledge level on the task,
while the number and ratio of stop words do not have a big difference when comparing between search sessions with different levels
of domain knowledge.
As we aim at predicting knowledge state change during a session,
similarly to the features discussed above, we extract the features
q_len_{ f irst, last }, q_uniq_term_{ f irst, last }, which potentially

Knowledge State and Knowledge Gain
Classes
Table 3: User groups created based on aver aдe ± 0.5S D.
Task Mean

SD

Low

Moderate

High

KG
KS

0.231
0.191

167
145

179
171

122
152

0.193
0.618

We used a Standard Deviation Classification approach to obtain
three classes of learners with regard to their level of knowledge.
Assuming approximately normal distributions of the respective test
scores (X) for the different topics, we transformed the test scores
into Z-scores with a mean of 0 and a Standard Deviation (SD) of
1 (standardization). We then used statistically defined intervals (X
< 0.5 SD = low; -0.5 SD < X < 0.5 SD = moderate; 0.5 SD < X =
high) for the classification of the learners into roughly equal groups
with low, moderate, or high knowledge. The same procedure was
repeated for knowledge gain. Here as well, the empirical knowledge
gain for every test was transformed into corresponding Z-scores
and three roughly equal groups (low knowledge gain; moderate
knowledge gain; high knowledge gain) were defined accordingly.
In view of the substantial variety of different topics, we argue that
such a tripartite categorization of knowledge states and knowledge
gains respectively allows for the construction of robust models,
which are themselves based on a large variety of features. Thus,
insights from the learning tasks considered can be generalized to
other similar intentional learning activities. This procedure weighs
all different knowledge tests equally irrespective of the number of
items.

5

Features Considered

FEATURE EXTRACTION AND ANALYSIS

We approach the problem of predicting knowledge state (k(t j ))
and knowledge gain (∆k(ti , t j )) described in Section 3 with supervised models for classification, where details about the applied
classification models are given in Section 6.1. To this end, each session s is represented by a feature vector v® = (f 1 , f 2 , ..., fn ), where
5

79

Session 1B: Log Analysis

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 4: Features for prediction of knowledge gain and knowledge state.
Cor r (f i , ∆k(t i , t j ))

Cor r (f i , k (t j ))

Feature description

s_dur at ion
s_dur at ion_per _q

-0.020
-0.019

0.066
0.066

Duration of the search session of a worker on a given topic
Session duration per query

q_num
q_t erm_{max, min, avд, t ot al }
q_uniq_t erm_{max, min, avд, t ot al }

0.052
{0.0002,-0.094,-0.042,0.047}
{0.016,-0.087,-0.024,0.06}

0.103
{0.065,0.032,0.051,0.068}
{0.104,0.05,0.084,0.089}

Number of queries in session s
Maximum, minimum, average, total number of query terms
Maximum, minimum, average number of unique terms per query

q_uniq_t erm_r at io
q_l en_{f ir st, l ast }
q_uniq_t erm_{f ir st, l ast }
q_compl exity_{max, min, avд }
q_compl exity_max _dif f

0.083
{-0.049,0.055}
{-0.023,-0.040}
{0.097,0.086,0.093}
{0.092}

-0.002
{0.031,0.105}
{0.036,0.087}
{0.087,0.078,0.049}
{0.077}

Number of query terms / unique query terms ( q_t e r m_t ot al
First, last query length
Number of unique terms of first, last query
Maximum, minimum, average of query complexity
Difference between the maximum and minimum complexity

S ERP _click
S ERP _click _r ank _{hiдhest, lowest, avд }
S ERP _click _int erval
S ERP _click _per _query
S ERP _no_click _query_{num, pct }
S ERP _t ime_{tot al, avд, max }
S ERP _avд_t ime_t o_f ir st _cl ick

-0.009
{-0.101,-0.021,-0.017}
0.036
-0.007
{0.041,-0.051}
{0.039,0.022,0.049}
-0.002

0.063
{-0.063,0.047,0.095}
0.022
-0.012
{0.077,0.029}
{0.091,-0.008,0.043}
-0.027

Total number of click on search result
Average, highest, lowest rank of the clicks
Average interval between clicks
Average number of clicks per query
Number, percentage of SERP with no clicks
Total, average, maximum time spend on SERPs
Time till first click

-0.018
0.029
-0.017
-0.017
0.243
0.236
{0.306,0.291}
-0.058
{-0.017,0.058}
{-0.056,0.057}
-0.033
{-0.08,-0.058,-0.078,-0.109}
{0.109,-0.093,0.122,0.086}
{0.15,0.089,0.14,0.091}
{0.16,0.064,0.133,0.044}

0.075
0.109
-0.016
-0.016
0.134
0.063
{0.104,0.089}
-0.020
{0.074,0.056}
{-0.028,0.025}
0.102
{0.146,0.106,0.146,0.082}
{-0.055,-0.074,-0.057,-0.01}
{0.005,-0.028,-0.018,0.023}
{0.041,0.018,0.025,0.028}

Total number of pages browsed in session
Number of unique pages browsed in session
Average number of page browsed per query
Average number of unique page viewed per query
Total active time on the pages
Average active time on the browsed pages per query
Maximum, average active time on the browsed pages
Ratio of revisited pages
Number, percentage of pages visited through SERP
Number, percentage of pages visited through pages other than SERP
Number of distinct domains of the visited pages
Maximum, minimum, average, total page title length
Maximum, minimum, average, total page size
Maximum, minimum, average and total overlap between query and page title
Maximum, minimum, average, total term overlap between query and page URL

0.066
0.094
0.091
0.095
0.120
0.120
0.142
0.127

0.113
0.053
0.067
0.039
0.058
0.025
0.052
0.021

total number of mouseovers in the session
average number of mouseovers per query
max mouseover rank in the session
average max mouseover rank per query
total scroll distance in session
average scroll distance per query
max scroll position in session
average max scroll position per query

CategoryNotation
Session

Query

SERP

b_num
b_uniq_num
b_num_per _q
b_uniq_num_per _q
b_t ime_tot al
b_t ime_avд_per _q
Browsing b_t ime_{max, avд }_per _paдe
b_r evisit ed _r at io
b_{num, pct }_f r om_S ERP
b_{num, pct }_f r om_non_S ERP
b_dist inct _domain_num
b_t tl _l en_{max, min, avд, t ot al }
b_paдe_size_{max, min, avд, t ot al }
b_t tl _q_over l ap_{max, min, avд, t ot al }
b_ur l _q_over l ap_{max, min, avд, t ot al }

Mouse

m_num
m_num_per _q
m_r ank _max
m_r ank _max _per _q
m_scr oll _dist
m_scr oll _dist _per _q
m_scr oll _max _pos
m_scr oll _max _pos_per _q

are indicators of the knowledge level at the beginning and end of
the session.
SERP Activity Features. Some activities on SERP have
also been investigated by previous works. Specifically, CollinsThompson [7] found that the total number of clicks on SERP
(SERP_click) is strongly correlated with a user’s understanding
of the topic. The analysis shows that users tend to click more often
when having stronger interest in the topic.
The ranking position of the clicked URL on SERP has also
been shown to be a strong indicator of user domain knowledge by Zhang et al. [22]. In [2], the authors discovered that
the difficult tasks with which a user is less knowledgeable are
associated with more clicks (SERP_click), more clicks on lower
ranks (SERP_click_rank_{hiдhest, lowest, avд}), more abandoned
queries (SERP_no_click_query_{num, pct }), i.e. queries without
clicks, longer time till first click (SERP_avд_time_to_f irst_click)
and longer time till next click (SERP_click_interval).
Browsing Features. Browsing features such as number of documents viewed (b_num, b_uniq_num) and average number of documents viewed per query (b_num_per _q, b_uniq_num_per _q) were
shown by several previous works [2, 8, 13, 15] to be positively correlated with the knowledge improvement. More detailed features
corresponding to the browsing behavior have also been studied,

q_uniq_t e r m_t ot al

)

indicating that the more difficult a task is for a user, the higher the
ratio of revisited pages (b_revisited_ratio) is.
Despite the number of pages visited, the time spent (corresponds
to features b_time_total, b_time_{max, avд}_per _q etc.) on the
accessed pages are found to vary to a large extent between domain
expert and non-expert [20]. Feature SERP_time_{total, avд, max }
was shown to be effective for predicting the user’s assessment of
task difficulty [2], which is subject to the user’s knowledge state.
We further distinguish the viewed pages into two sets {pages
navigated through SERP, pages navigated through non-SERP},
by parsing its ancestor page. Hence we extract the features
b_{num, pct }_f rom_SERP and b_{num, pct }_f rom_non_SERP
that are motivated by the features introduced above.
The content of the accessed Web documents strongly influence the user’s learning outcome. White et al. [20] found
that domain experts encountered different and more diverse domains (feature b_distint_domain_num) than domain
novices. Several other document content related features:
page size (b_paдe_size_{max, min, avд, total }), title length
b_ttl_len_{max, min, avд, total } have also been found to evolve
during the learning process [8]. Based on the assumption that
domain experts and novices have different capabilities of choosing
learning resources, for instance, experts are able to recognize
6

80

Session 1B: Log Analysis

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

• τ - threshold for feature selection based on correlation between features. We also experimented with different τ in the
range of {1.0, 0.95, 0.9, 0.85, 0.8, 0.75, 0.7}. The number of features
in the feature set corresponding to given τ , β and γ is reported
in Table 5.

useful documents without query terms presented in the page title,
we computed features based on the overlap between page title
and query (b_url_q_overlap_{max, min, avд, total }). The page
URL (b_ttl_q_overlap_{max, min, avд, total }) as a complementary
source containing hints about a page’s content has also been
considered in the feature extraction process.
Mouse Features. Features in the Mouse category are indicators of quantity and quality of user interactions with a knowledge
source and were also shown to be effective for predicting the user’s
assessment of task difficulty [2].

5.2

Table 5: Number of features of different configurations.

τ
τ
τ
τ
τ
τ
τ

Feature Analysis and Selection

As a basis for feature selection, we analyze the features with respect
to their relationship to knowledge gain and knowledge state, as
well as their redundancy.
Correlation between feature and KG (KS). In order to select
the most influential features for the prediction task, we compute
Corr (fi , ∆k(ti , t j )) and Corr (fi , k(t j )), i.e. the Pearson correlation
coefficient between each feature and the knowledge gain (knowledge state). The correlation scores are shown in Table 4. Based on
the computed score, we select the features fulfilling the condition
Corr (fi , ∆k(ti , t j )) ≥ β for the knowledge gain prediction task and
Corr (fi , k(t j )) ≥ γ for the knowledge state prediction task. Performance of the prediction model using features selected based on
varied β and γ has been evaluated and corresponding results are
presented in Section 7.
Correlation between features. We compute the Pearson correlation coefficient between each pair of features Corr (fi , f j ). If
Corr (fi , f j ) ≥ τ , i.e. features appear to be not independent,
we remove the feature from the feature set, that has lower
Corr (fi , ∆k(ti , t j )) respectively lower Corr (fi , k(t j )) for knowledge
gain (state) prediction. We evaluate the performance of the prediction model for different values of τ . The feature selection results
are reported in Section 7.

= 1.0
= 0.95
= 0.9
= 0.85
= 0.8
= 0.75
= 0.7

0.0

0.05

0.1

70
66
56
43
39
37
33

43
42
37
29
26
25
24

16
16
13
10
7
7
6

β (KG)
0.15 0.2
6
6
6
6
4
4
3

4
4
4
4
2
2
1

0.25

0.3

0.0

γ (KS)
0.05 0.1

0.15

2
2
2
2
1
1
1

1
1
1
1
1
1
1

70
66
58
44
38
34
32

41
38
34
24
19
17
16

0
0
0
0
0
0
0

11
11
11
9
8
7
7

6.1.2 Baseline. As discussed in Section 2, the tasks addressed in
this paper are comparably novel. To the best of our knowledge, there
are no existing baselines for the task of knowledge gain prediction
during informational Web search missions. Therefore, we compare
our approach for a number of configurations (described above),
using multiple standard classification models. For the prediction
of knowledge state k(t j ), we compare our approach in addition to
one existing baseline [23]. KS Z hanд refers to the linear regression
model fitted by Zhang et al. [23] for domain knowledge prediction
as shown in Equation 1.
K S Z hanд = −1.466+0.039 ·Saved +0.147 ·Q l e n +0.130 · Relme an (1)

Saved represents the number of documents saved by the user,
which is an extremely sparse feature in a real search environment
and does not appear in our dataset. Ql en is the mean query length
and Relmean is the mean rank of documents opened in SERPs. As
the output of the baseline regression model is a real number, we
convert the result into 3 classes according to the definition given
in Section 4.5.

6 EVALUATION - EXPERIMENTAL SETUP
6.1 Approach Configurations & Baselines

6.2

Evaluation Metrics

For both tasks, we run repeated 10-fold cross-validation with 10
repetitions on all the approaches and configurations described in
Section 6.1 and evaluate the results according to the following
metrics:
• Accuracy (Accu) across all classes: percentage of search sessions that were classified with the correct class label.
• Precision (P), Recall (R), F1 (F 1) score of class i: we compute
the standard precision, recall and F1 score on the prediction result
of each class i.
• Macro average of precision (P), recall (R), and F1 (F 1): the
average of the corresponding score across 3 classes.
• Runtime: the time consumed for completing the 10-fold crossvalidation on experimental dataset in milliseconds.
To analyze the usefulness of individual features, we make use
of the Mean Decrease Accuracy (MDA) metric, which is based on
the Random Forest model, i.e. a very well performing model for
both tasks as shown in Section 7. MDA quantifies the importance
of a feature by measuring the change in prediction accuracy of the
Random Forest model when the values of the feature are randomly
permuted compared to the original observations.

6.1.1 Configurations and Parameters.
• Classifier. We apply a range of standard models for the classification of the knowledge gain and knowledge state, namely, Naive
Bayes (NB), Logistic Regression (LR), Support Vector Machine
(SVM), Random Forrest (RF), and Multilayer Perceptron (MP). For
our experiments, we used the Weka library for Java7 . For each
of the configurations described below, we perform grid search
to tune the hyperparameters of all of the classifiers. In Section
7, we report the result of the best performing hyperparameter
configuration for each classifier.
• β (γ )- threshold for feature selection based on correlation
between feature and KG (KS). We compare prediction performance before and after applying the selection based on featureKG (KS) correlation. We set the threshold β (γ ) for selecting the
features in the range of {0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3} ({0.0,
0.05, 0.1, 0.15}). We omit results for larger β (γ ), as the resulting
number of features is insufficient for training a classifier.
7 https://www.cs.waikato.ac.nz/ml/weka/

7

81

Session 1B: Log Analysis

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 6: Performance in knowledge gain prediction task.
Method

τ

β

#Features Runtime P

Low
R

F1

P

NB
LR
SVM
RF
MP

≥0.85
0.85
0.90
0.95
≥0.85

0.25
0.05
0.00
0.00
0.25

2
29
56
66
2

0.747
0.537
0.595
0.542
0.556

0.562
0.516
0.536
0.531
0.497

0.483
0.459
0.487
0.469
0.421

19.1
653.9
441.6
3739.3
1919.3

0.450
0.498
0.488
0.521
0.452

Moderate
R
F1
0.268
0.382
0.340
0.410
0.312

0.344
0.416
0.400
0.437
0.356

P

High
R
F1

P

0.513
0.379
0.410
0.425
0.425

0.384
0.431
0.469
0.480
0.450

0.482
0.445
0.462
0.472
0.433

0.439
0.403
0.437
0.450
0.435

Macro average
R
F1
0.467
0.450
0.468
0.477
0.439

0.448
0.445
0.458
0.473
0.429

All
Accu
0.469
0.450
0.465
0.475
0.435

Figure 1: Feature importance for knowledge gain prediction.

7

RESULTS: PREDICTION PERFORMANCE
AND FEATURE ANALYSIS

Feature Impact. The MDA results of each feature are shown
in Figure 1. Based on the result, the most important features are:
b_time_max_per _paдe, b_time_avд_per _paдe and b_time_total.
Mostly active time related features which reflect the effort
users spend on learning. Similarly, these features are immediately followed by m_scroll_dist_per _q, m_scroll_max_pos, and
SERP_click_rank_lowest, three features that are indicators for the
amount of information a user has seen during the session.
As shown in Table 6, the NB and MP classifier performs best
when using 2 features only, namely b_time_max_per _paдe and
b_time_avд_per _paдe. These two features are confirmed as the
most important features by our feature importance analysis.
Regarding feature categories, the 10 most useful features in terms
of MDA belong to the browsing, mouse and SERP categories. Surprisingly, although multiple query features had above average correlation to knowledge gain (see Table 4) – in particular, the features related to query complexity (q_complexity_{max, min, avд}
and q_complexity_max_di f f ) had correlations ranging from .086
to .097, compared to the median of .042 – q_uniq_term_total
is the only query feature among the 25 highest ranked according to MDA. Analogously, both session features s_duration and
s_duration_per _q appear among the 25 highest ranked features
despite their relatively low correlations of -.02 and -.019.

In this section, we report the evaluation results of the prediction
performance as well as an analysis of feature importance.

7.1

Knowledge Gain Prediction

Performance of different Configurations. For each of the 245
distinct configurations described in Section 6, we run repeated
cross-validation as described in the previous section.
From all the different combinations of τ and β as listed in Table 5,
we present the result of the configuration that produces the highest
accuracy for each classifier in Table 6 due to space constraints. A
complete set of the evaluation results are available online8 . We
observed that in the knowledge gain prediction task, the highest
average F1 score across classes and the highest accuracy always
appear in the same configuration for all the classifiers except for
LR, where there is a minor difference of .001 in F1 between the two.
The Random Forest classifier achieves the best performance in
terms of accuracy and average F1 score, slightly outperforming
Naive Bayes and SVM. As shown, Naive Bayes is the most efficient
classifier in terms of computation time for feature sets of comparable size. Comparing classification performance for the different
classes, the results for each of the classifiers consistently show
higher F1 scores for the ‘Low’ and ‘High’ knowledge gain classes
than for the ‘Moderate’ knowledge gain class. Overall, prediction
accuracy is above 0.435 and the F1 score is above 0.429 for all of the
classifiers, which indicates that the set of features we extracted from
search activities can provide meaningful evidence for predicting
knowledge gain.

7.2

Knowledge State Prediction

Performance of different Configurations. We have experimented with all different combinations of τ and γ as listed in Table
5 for all considered classifiers. The result of the configuration that
produces the highest accuracy for each classifier is shown in Table 7.
We observe that in the knowledge state prediction task, the highest
average F1 score across classes and the highest accuracy always

8 https://sites.google.com/view/predicting-user-knowledge

8

82

Session 1B: Log Analysis

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 7: Performance in knowledge state prediction task.
Method

τ

γ

#Features Runtime P

Low
R

F1

P

NB
LR
SVM
RF
MP

≤0.75
1.00
0.95
1.00
1.00

0.1
0.05
0.05
0.00
0.05

7
41
38
70
41

23.5
797.7
292.3
4023.4
43619

0.352
0.338
0.359
0.443
0.380

0.712
0.383
0.479
0.456
0.414

0.470
0.359
0.409
0.449
0.396

0.424
0.402
0.395
0.394
0.398

-

2

23

0.320 0.428 0.366

K S Z hanд -

Moderate
R
F1
0.218
0.368
0.303
0.358
0.298

0.287
0.384
0.342
0.374
0.341

0.328 0.240 0.277

P

High
R
F1

P

0.370
0.372
0.409
0.418
0.385

0.211
0.359
0.386
0.447
0.461

0.382
0.370
0.388
0.418
0.388

0.268
0.366
0.397
0.432
0.419

0.362 0.355 0.359

Macro average
R
F1
0.380
0.370
0.389
0.421
0.391

All
Accu

0.342
0.370
0.383
0.418
0.385

0.369
0.370
0.385
0.418
0.387

0.337 0.341 0.334

0.335

Figure 2: Feature importance for knowledge state prediction.

8

appear in the same configuration for all the classifiers except Naive
Bayes (average F1 of the highest accuracy configuration is 0.006
lower than the maximum average F1).
Among all evaluated classifiers, Random Forest reaches the highest accuracy and F1 score, outperforming the other classifiers.
Comparison to Baseline. We compare the performance of our
approach against the baseline method (KS Z hanд ), shown in the last
row in Table 7. The result suggests that, the linear regression model
fitted in previous work based on data collected through a lab study
does not perform well in the knowledge state prediction task and
is outperformed by all five classifiers following our approach.
Feature Impact. The MDA results of each feature in the
knowledge state prediction tasks are shown in Figure 2. The
most important features (q_complexity_avд, b_ttl_len_min and
b_ttl_len_avд) reflect the user’s capability of constructing a query
and choosing relevant resources. In terms of feature categories, all
of the highest ranked features for this task belong to the query and
browsing categories.
Compared to the knowledge gain task, query complexity
features (q_complexity_{min, max, avд}) are considerably more
useful, while features related to time and effort invested, like
b_time_max_per _paдe and b_time_avд_per _paдe, are among the
lowest ranked. Other query features related to the used vocabulary
(e.g. q_uniq_term_min, q_uniq_term_avд, and q_term_total) are
ranked similarly highly. Apparently, while the time taken by users
to take in the discovered documents is predictive of their knowledge gain, their capability of using complex queries and selecting
relevant resources reveals more about their knowledge state.

DISCUSSION

Based on the experimental results, we conclude that: i) knowledge
gain (state) can be predicted during informational search sessions
with a certain level of accuracy, ii) performance of the knowledge
gain prediction appears to be generally better, suggesting that the
task is easier given the nature of our data, and iii) the performance
of the prediction approach is better for more extreme classes, i.e.
for low and high knowledge gain (state) classes, whereas performance on the moderate classes is lowest in both tasks, presumably
due to the moderate classes being the most overlapping ones with
respect to their characteristics. In this section we discuss some of
the reasons behind these observations.
Most of the features we considered were found to correlate rather
weakly with knowledge gain (state). Intuitively, this could be due
to the limited duration of the search sessions (just over 5 minutes
on average). This could potentially reduce the predictive power
of certain features, such as the number of queries or the number
of accessed documents. This also rendered evolution-oriented features, which would capture the evolution of queries and behavior
throughout a session predictively poor. While these would supposedly be highly indicative of the knowledge gain, they require longer
sessions than are usually observable in real-world search sessions
as well as in our experimental data.
For the prediction of knowledge gain, our feature analysis result
shows that the most important features are the ones related to the
user’s active time. As our experimental dataset contains mostly
short sessions, it is understandable that the time spent affects the
knowledge gain strongly. However, we believe that in longer search
sessions, the learning pattern and the initial knowledge state of a
9

83

Session 1B: Log Analysis

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

REFERENCES

user might be more influential for the knowledge gain than in short
sessions. Further experiments are required to establish this.
The results suggest that with the presented approach, the knowledge gain prediction is an easier task than the knowledge state
prediction. As shown in Figure 2, the most important features for
knowledge state prediction are the features related to the content
of queries and browsed documents. Intuitively, these features are
also central to the knowledge gain prediction task. Yet, we observe
that although the topic descriptions that were given to the users
typically provided central keywords for the first query, only a very
limited set of queries (1-2) are fired by most users. Given the small
number of queries in each session, the query features are less distinguishable and hence, less indicative of the knowledge gain. Thus,
query evolution is observable only to a very limited extent.

9

[1] L. W. Anderson, D. R. Krathwohl, P. Airasian, K. Cruikshank, R. Mayer, P. Pintrich,
J. Raths, and M. Wittrock. A taxonomy for learning, teaching and assessing:
A revision of bloom’s taxonomy. New York. Longman Publishing. Artz, AF, &
Armour-Thomas, E.(1992). Development of a cognitive-metacognitive framework
for protocol analysis of mathematical problem solving in small groups. Cognition
and Instruction, 9(2):137–175, 2001.
[2] J. Arguello. Predicting search task difficulty. In ECIR, volume 14, pages 88–99,
2014.
[3] P. Blumschein. Intentional learning. In Encyclopedia of the Sciences of Learning,
pages 1600–1601. Springer, 2012.
[4] A. Broder. A taxonomy of web search. In ACM Sigir forum, volume 36, pages
3–10. ACM, 2002.
[5] M. P. Carey, D. Morrison-Beedy, and B. T. Johnson. The hiv-knowledge questionnaire: Development and evaluation of a reliable, valid, and practical selfadministered questionnaire. AIDS and Behavior, 1(1):61–74, 1997.
[6] M. J. Cole, J. Gwizdka, C. Liu, N. J. Belkin, and X. Zhang. Inferring user knowledge level from eye movement patterns. Information Processing & Management,
49(5):1075–1091, 2013.
[7] K. Collins-Thompson, S. Y. Rieh, C. C. Haynes, and R. Syed. Assessing learning
outcomes in web search: A comparison of tasks and query strategies. In Proceedings of the 2016 ACM on Conference on Human Information Interaction and
Retrieval, pages 163–172. ACM, 2016.
[8] C. Eickhoff, J. Teevan, R. White, and S. Dumais. Lessons from the journey: a
query log analysis of within-session learning. In Proceedings of the 7th ACM
international conference on Web search and data mining, pages 223–232. ACM,
2014.
[9] U. Gadiraju, R. Kawase, S. Dietze, and G. Demartini. Understanding malicious
behavior in crowdsourcing platforms: The case of online surveys. In Proceedings
of the 33rd Annual ACM Conference on Human Factors in Computing Systems,
pages 1631–1640. ACM, 2015.
[10] U. Gadiraju, J. Yang, and A. Bozzon. Clarity is a worthwhile quality–on the
role of task clarity in microtask crowdsourcing. In Proceedings of the 28th ACM
Conference on Hypertext and Social Media, pages 5–14. ACM, 2017.
[11] U. Gadiraju, R. Yu, S. Dietze, and P. Holtz. Analyzing knowledge gain of users in
informational search sessions on the web. In 2018 ACM on Conference on Human
Information Interaction and Retrieval (CHIIR). ACM, 2018.
[12] J. Gwizdka and X. Chen. Towards observable indicators of learning on search. In
SAL@ SIGIR, 2016.
[13] J. Gwizdka and I. Spence. What can searching behavior tell us about the difficulty
of information tasks? a study of web navigation. Proceedings of the Association
for Information Science and Technology, 43(1):1–22, 2006.
[14] M. Hagen, M. Potthast, M. Völske, J. Gomoll, and B. Stein. How writers search:
Analyzing the search and writing logs of non-fictional essays. In Proceedings
of the 2016 ACM on Conference on Human Information Interaction and Retrieval,
pages 193–202. ACM, 2016.
[15] B. J. Jansen, D. Booth, and B. Smith. Using the taxonomy of cognitive learning to
model online searching. Information Processing & Management, 45(6):643–663,
2009.
[16] V. Kuperman, H. Stadthagen-Gonzalez, and M. Brysbaert. Age-of-acquisition
ratings for 30,000 english words. Behavior Research Methods, 44(4):978–990, 2012.
[17] Y. Li and N. J. Belkin. A faceted approach to conceptualizing tasks in information
seeking. Information Processing & Management, 44(6):1822–1837, 2008.
[18] R. Syed and K. Collins-Thompson. Retrieval algorithms optimized for human
learning. In Proceedings of the 40th International ACM SIGIR Conference on
Research and Development in Information Retrieval, pages 555–564. ACM, 2017.
[19] P. Vakkari. Searching as learning: A systematization based on literature. Journal
of Information Science, 42(1):7–18, 2016.
[20] R. W. White, S. T. Dumais, and J. Teevan. Characterizing the influence of domain
expertise on web search behavior. In Proceedings of the second ACM international
conference on web search and data mining, pages 132–141. ACM, 2009.
[21] W.-C. Wu, D. Kelly, A. Edwards, and J. Arguello. Grannies, tanning beds, tattoos
and nascar: Evaluation of search tasks with varying levels of cognitive complexity.
In Proceedings of the 4th Information Interaction in Context Symposium, pages
254–257. ACM, 2012.
[22] X. Zhang, M. Cole, and N. Belkin. Predicting users’ domain knowledge from
search behaviors. In Proceedings of the 34th international ACM SIGIR conference
on Research and development in Information Retrieval, pages 1225–1226. ACM,
2011.
[23] X. Zhang, J. Liu, M. Cole, and N. Belkin. Predicting users’ domain knowledge
in information retrieval using multiple regression analysis of search behaviors.
Journal of the Association for Information Science and Technology, 66(5):980–1000,
2015.
[24] M. Zhuang, G. Demartini, and E. G. Toms. Understanding engagement through
search behaviour. In International Conference on Information and Knowledge Management, Proceedings, pages 1957–1966. Association for Computing Machinery,
2017.

CONCLUSIONS AND FUTURE WORK

In this paper, we propose to use classification models to predict
user knowledge state and knowledge gain from behavioral data
captured during real-world informational search sessions. Given
the lack of available datasets and ground truths, we have created an
experimental dataset using crowdsourcing, capturing 468 informational search sessions including user interactions and behavioral
traces throughout the process, together with calibration and corresponding post-session knowledge test results.
Previous work related to the aforementioned prediction tasks is
still very limited. However, existing state of the art was considered
when identifying a novel set of 70 features, partially motivated
from related work as well as from exploring our gathered data.
Classification experiments were conducted with 5 classification
models in a variety of configurations and in comparison to a baseline
in the knowledge state prediction task.
The experimental results underline that a user’s knowledge gain
and knowledge state can be modeled based on a user’s online interactions observable throughout the search process. Through feature analysis, we provide evidence for an improved understanding
between individual user behavior and the corresponding knowledge state and change. Alongside these results, we also make the
gathered dataset available. This dataset captures user interactions
throughout diverse informational search sessions and corresponding knowledge assessments, and thereby provides a resource which
can facilitate further research in this area.
As a part of future work, we aim to reproduce and refine the
findings in more varied search sessions, where durations and learning intents are more diverse; involving considerably longer search
sessions and, for instance, procedural knowledge rather than intents focused on declarative knowledge only. This would provide
the opportunity to observe evolution-oriented features, such as
considering the evolution of queries, their length and complexity.
Potential applications of this work include the consideration of
user knowledge and the expected learning progress of a user as part
of Web search engines and information retrieval approaches, or
within informal learning-oriented search settings, such as libraries
or knowledge- and resource-centric online platforms.

10

84

