Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Query Performance Prediction using Passage Information
Haggai Roitman
IBM Research AI
Haifa, Israel
haggai@il.ibm.com

ABSTRACT

2

We focus on the post-retrieval query performance prediction (QPP)
task. Specifically, we make a new use of passage information for
this task. Using such information we derive a new mean score calibration predictor that provides a more accurate prediction. Using
an empirical evaluation over several common TREC benchmarks,
we show that, QPP methods that only make use of document-level
features are mostly suited for short query prediction tasks; while
such methods perform significantly worse in verbose query prediction settings. We further demonstrate that, QPP methods that
utilize passage-information are much better suited for verbose settings. Moreover, our proposed predictor, which utilizes both documentlevel and passage-level features provides a more accurate and consistent prediction for both types of queries. Finally, we show a connection between our predictor and a recently proposed supervised
QPP method, which results in an enhanced prediction.

The query performance prediction task has been extensively studied, where two main approaches have been proposed, either preretrieval or post-retrieval prediction [4]. Yet, most of previous QPP
research has focused on ad-hoc retrieval prediction tasks that involved short (keyword-based) queries [4]. In this work, we further
study QPP in cases with verbose (long) queries [8].
Passage information has been shown to assist in ad-hoc document retrieval [2, 3] and verbose query evaluation tasks [1]. Motivated by these previous works, in this work, we rely on passage
information as a strong evidence source for query performance. A
couple of previous works [6, 9] have predicted the outcome of passage retrieval for question answering tasks. Yet, as we shall later
show, predictors that were suggested for the passage retrieval QPP
task are less suited for the document retrieval QPP task.
Our work extends Kurland et al.’s [10] probabilistic QPP framework with passage information. Finally, Roitman et al. [12] have
recently proposed an extension to [10], where the authors derived
a generic calibrated (discriminative) mean retrieval score estimator for post-retrieval QPP tasks. Using their proposed predictor
(dubbed WPM2), the authors achieved the best reported QPP accuracy [12]. In this work, we further show a connection between
our proposed passage-based QPP method and Roitman et al.’s [12]
framework, where we utilize it to provide overall better prediction.

1

INTRODUCTION

We focus on the post-retrieval query performance prediction (QPP)
task [4]. Given a query, a corpus and a retrieval method that evaluates the query, our goal is to predict the query’s performance based
on its retrieved result list [4].
Motivated by previous works on document retrieval using passage information [1–3], we propose to use such information for the
post-retrieval QPP task as well. To this end, we extend Kurland et
al.’s probabilistic QPP framework [10] and show how passage information may be utilized for an enhanced prediction.
Using an evaluation with several TREC corpora, we first demonstrate that, existing state-of-the-art document-level post retrieval
QPP methods, are mostly suited for prediction tasks that involve
short (and probably more ambiguous) queries; whereas such methods are less suited for prediction tasks that involve verbose (long
and probably more informative) queries. We next demonstrate that,
our proposed QPP method which makes use of passage information provides a more robust prediction, regardless of query type.
We further set a direct connection with Roitman et al.’s mean retrieval score estimation framework [12]. Moreover, by integrating
our proposed passage-information QPP signal as an additional calibration feature within Roitman et al.’s framework [12], we are able
to achieve the overall best QPP accuracy.

RELATED WORK

3 FRAMEWORK
3.1 Preliminaries
Let q denote a query and let C denote a corpus on which the query
is evaluated. For a given text x (e.g., a document d ∈ C or a passage
д ∈ d), let sq (x) denote the (retrieval) score assigned to x given q.
In this work, we estimate each query q’s performance using a
post-retrieval approach [4]. Accordingly, let D denote the top-k
documents in C with the highest retrieval score sq (d) as determined by the underlying retrieval method. The post-retrieval QPP
task is to estimate p(D|q, r ) – the likelihood that D contains relevant
information to query q [4].

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210070

3.2

QPP using passage information

Our goal is to predict a given query q’s performance as accurate
as possible. To this end, we propose to utilize passage information
extracted from documents in D as an additional source for QPP.
Our main hypothesis is that, relevant passage information obtained
in D may provide valuable evidence whether a given retrieval was
(overall) effective or not.

893

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Our prediction framework is built on top of Kurland et al.’s [10]
probabilistic QPP framework. According to [10], p(D|q, r ) may be
estimated1 as follows:
def

p̂doc (D|q, r ) =

Õ

Finally, we note that, similarly to many post-retrieval predictors [4], p(r |q) is a query-sensitive normalization term, estimated
def p
in this work according to q’s length: p̂(r |q) = |q|.

(1)

p(D|d, r )p(d |q, r ).

3.3

d ∈D

p(d |q, r ) denotes document d’s likelihood of being a relevant response to query q. p(D|d, r ) denotes the likelihood that such relevant response will be further included in D, which we assume in
this work to be uniformly distributed. Applying this assumption,
our QPP estimator can be defined as follows:
def

p̂psд (D|q, r ) =

Õ

(2)

p(d |q, r ).

Connection with the WPM method

We conclude this section by showing that our proposed passageenhanced QPP method shares direct connection with the recently
proposed mean retrieval score estimation framework of Roitman et
al. [12]. According to [12], many previous post-retrieval predictors
(e.g., Clarity [5], WIG [15], etc) share the following general form:
def 1 Õ
p̂(D|q, r ) =
sq (d) · ϕ r, F (d),
|D|

d ∈D

def

where ϕ r, F (d) =

We next show how p̂psд (D|q, r ) can be estimated using passageinformation. As a first step, we note that:
p(q|d, r )p(r |d)p(d)
.
p(d |q, r ) =
p(r |q)p(q)
def

The term p(r |d) denotes the likelihood that document d contains relevant information regardless of any specific query. Using
a MaxPsд estimation approach [2], we now estimate this term as
follows:
def

(4)

д ∈d

j

d ∈D

f j (d)

αj

is a Weighted Product Model

discriminative calibrator; with f j (d) is some retrieval feature and
α j ≥ 0 denotes its relative importance [12].
According to Eq. 5, our predictor is essentially a calibrated mean
retrieval estimator [12]; Our predictor utilizes two calibration features, namely f 1 (d) = p̂(r |d) (see Eq. 4) and f 2 (d) = p̂(r1|q) ; both
features are assigned with equal weights of α 1 = α 2 = 1. Using this
connection in mind, we make two important observations. First,
by calibrating the two feature weights α i ; i ∈ {1, 2}, we might improve our own predictor’s performance [12]. Second, we can utilize
f 1 (d) as a new passage-based calibration feature within Roitman
et al.’s [12] QPP framework. As we shall shortly demonstrate, such
an extension indeed significantly boosts prediction performance;
even in cases where the prediction quality was already relatively
high.

(3)

p̂(r |d) = max sq (д)p(r |д)p(д|d).

Î

sq (д) is the query score assigned to passage д(∈ d). p(r |д) represents the likelihood that passage д contains relevant information.
We estimate this term as a combination of two sub-terms as follows:
def

def

H (д) = −

Í

w ∈д

p̂(r |д) = H (д) · posBias(д).

4 EVALUATION
4.1 Datasets

p(w |д) log p(w |д) is the entropy of passage д’s

unsmoothed language model – preferring more diverse passages.
def

1
posBias(д) = 1 + log(2+д.s)
, where д.s denotes the start position
(in character offsets) of passage д within its containing document.
Hence, posBias(д) prefers passages that are located as earlier as
possible within their containing documents.
p(д|d) in Eq. 4 further captures the relationship between passage
д and its containing document d, estimated as the Bhattacharyya
similarity between their unsmoothed language models:
def Õ p
p(w |д)p(w |d).
p̂(д|d) =

Corpus
TREC5
WSJ
AP
ROBUST
WT10g
GOV2

#documents
524,929
173,252
242,918
528,155
1,692,096
25,205,179

Queries
251-300
151-200
51-150
301-450, 601-700
451-550
701-850

Disks
2&4
1-2
1-3
4&5-{CR}
WT10g
GOV2

Table 1: Summary of TREC benchmarks.

w

def

We next assume that, p(q) is uniformly distributed; p(d) = |D1 |

The details of the TREC corpora and queries that we used for
the evaluation are summarized in Table 1. These benchmarks were
used by many previous QPP works [4, 12]. We evaluated two main
types of queries, namely: short (keyword) queries and verbose queries.
To this end, for short queries evaluation, we used the titles of TREC
topics. For verbose queries evaluation, we further used the TREC
topic descriptions, following [1, 8]. We used the Apache Lucene2
open source search library for indexing and searching documents.
Documents and queries were processed using Lucene’s English
text analysis (i.e., tokenization, Porter stemming, stopwords, etc.).

def

is uniformly distributed over D; and sq (d) = p(q|d, r ).
Applying these assumptions back into Eq. 3 and using our derivation of p̂(r |d) according to Eq. 4, we obtain our new estimator according to Eq. 2 as follows:

def

p̂psд (D|q, r ) =
1 See

Eq. 3 in [10].

max s (д)p̂(r |д)p̂(д|d)
© д ∈d q
ª
1 Õ
®.
sq (d) · ­­
®
|D|
p(r |q)
d ∈D
«
¬

(5)

2 http://lucene.apache.org

894

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Clarity
WIG
NQC

TREC5
.490bp
.347p
.483bp

WSJ
.607p
.677b
.718b

AP
.596p
.526bp
.554bp

WT10g
.380bp
.434bp
.486p

Robust
.477p
.411bp
.575b

GOV2
.407p
.535b
.432bp

Clarity(psg)
WIG(psg)
NQC(psg)

.204bp
.344p
.292bp

.629p
.576bp
.497b

.622p
.397bp
.304bp

.289bp
.494bp
.488p

.477p
.435bp
.401bp

.395p
.468b
.220bp

PI
C-PI

.567c
.613c

p

.677
.718c

.684c
.694c

p

.518c
.532c

p

.577c
.585c

p

.465c
.501c

WPM2
WPM2+PI

.738w
.787w

.725w
.743w

.738w
.764w

.540w
.557w

.640w
.647w

.655w
.661w

As the underlying retrieval method we used Lucene’s Dirichletsmoothed query-likelihood implementation, with its Dirichlet parameter fixed to µ = 1000, following [10, 12].

4.2

Baselines

We compared our proposed QPP method (hereinafter referred to
as PI3 ) with several different baseline QPP methods, as follows.
As a first line of baselines, we compared with Clarity [5], WIG [15]
and NQC [13], which are commonly used document-level postretrieval QPP methods [4]. The Clarity [5] method estimates query
performance proportionally to the divergence between the relevance model [11] induced from D and the background model induced from C. The WIG method [15] estimates query performance
according to the difference between the average retrieval score in
D and that of C. The NQC [13] method estimates query performance according to standard deviation of the retrieval scores of
documents in D, further normalized by the corpus score sq (C).
As alternative QPP methods that also utilize passage information, we closely followed [6, 9] and implemented the passage-level
counterparts of the three former baselines; denoted Clarity(psg) [6,
9], WIG(psg) [9], NQC(psg) [9], respectively.
As a very strong document-level baseline, we further compared
with the WPM2 method [12]. WPM2 utilizes 10 different document retrieval score calibration features4 , whose weights need to
be learned [12].
Following the first observation we made in Section 3.3, we also
implemented a calibrated version of our PI method (denoted C-PI).
Finally, following the second observation we made in Section 3.3,
we further extended the WPM2 method with our new passagelevel calibration feature (denoted WPM2+PI).

4.3

Setup

We evaluated the various methods using two different query settings: once using the short (title) queries and once using the verbose (description) queries. On each setting, we predicted the performance of each query based on its top-1000 retrieved documents [4].
Following a common practice [4], we assessed prediction over queries
quality according to the Pearson’s-ρ correlation between the predictor’s values and the actual average precision (AP@1000) values
calculated using TREC’s relevance judgments.
In order to realize our predictor in Eq. 5, according to Eq. 4, for
each document d(∈ D), we need to obtain the passage д ∈ d with
the highest likelihood (score) p̂(r |d). To this end, given document
d(∈ D), we first extracted candidate passages from it using a fixed
L = 500 characters-windowing approach [14]; and then scored the
candidates according to Eq. 4. We used Okapi-BM25 as our choice
of sq (д), with k1 = 0.8 and b = 0.3, following [7].
Most of the methods that we evaluated (and among them the PI
and WPM2 variants) required to tune some free parameters. Comdef

To implement the three passage-level alternatives (i.e., Clarity(psg),WIG(psg)
and NQC(psg)), we first used the same window-based passage extraction approach [14] and ranked candidate passages extracted
from the various documents in D according to their Okapi-BM25
score with k1 = 0.8 and b = 0.3 [7]. We then used the top-m scored
passages over D for prediction [6, 9], with m ∈ {5, 10, 20, 50, 100, 150, 200}.
For Clarity and Clarity(psg), following [5, 6, 9], we further clipped
the induced relevance model at the top-n terms cutoff, with n ∈
{5, 10, 20, 50, 100, 150, 200}.
To learn the calibration feature weights of C-PI, WPM2 and
WPM2+PI, following [12], we used a Coordinate Ascent approach.
Similar to [12], we selected the feature weights {α j }hj=1 in the grid
[0, 5]h with a step size of 0.1 within each dimension, with h ∈
{2, 10, 11} such different features implemented within the C-PI,
WPM2 and WPM2+PI methods, respectively. Further following [12],
def

feature values were smoothed f j (d; ϵ) = max(f j (d), ϵ), where
ϵ = 10−10 is a hyperparameter.
Following [12, 13], we trained and tested all methods using a
holdout (2-fold cross validation) approach. Accordingly, on each
benchmark, we generated 30 random splits of the query set; each
split had two folds. We used the first fold as the (query) train set.
We kept the second fold untouched for testing. We recorded the
average prediction quality over the 30 splits. Finally, we measured
statistical significant differences of prediction quality using a twotailed paired t-test with (Bonferroni corrected) p < 0.05 computed
over all 30 splits.

4.4

mon to all methods is the free parameter k = |D|, which is the
number of top scored documents (out of a total of 1000 retrieved
documents) to be used for the prediction. To this end, for each
method we selected k ∈ {5, 10, 20, 50, 100, 150, 200, 500, 1000}.
3 PI

Table 2: Results of prediction over short queries. The superscript b denotes a statistically significant difference between
one of the first document-level baselines and its passagelevel counterpart. The superscript p denotes a significant difference between PI and the six first baselines. The subscript
c denotes a significant difference between PI and C-PI. The
subscript w denotes a significant difference between WPM2
and WPM2+PI.

Prediction over short queries

The results of our first evaluation setting with short queries are
summarized in Table 2. First, we observe that, the three first documentlevel QPP baselines (i.e., Clarity WIG and NQC) and their passagelevel counterparts (i.e., Clarity(psg) WIG(psg) and NQC(psg))
exhibit a mixed relative performance. This serves as a first evidence
that passage-information is an important QPP signal.

stands for “Passage Information".
full list of features is described in Section 5 of [12].

4 The

895

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Clarity
WIG
NQC

TREC5
.254p
.011bp
.271bp

WSJ
-.005bp
.368bp
.642

AP
.174bp
.510p
.528bp

WT10g
.381bp
.377p
.397bp

Robust
.288bp
.278bp
.461bp

GOV2
.281bp
.303bp
.360p

Clarity(psg)
WIG(psg)
NQC(psg)

.256p
.292bp
.338bp

.188bp
.588bp
.640

.198bp
.526p
.545bp

.267bp
.377p
.458bp

.468bp
.446bp
.499bp

.446bp
.343bp
.361p

PI
C-PI

.512p
.528

.655c
.678c

.637c
.689c

.464c
.476c

.559c
.624c

.460c
.482c

WPM2
WPM2+PI

.732
.740

.650
.660

.621w
.729w

.406w
.487w

.587w
.602w

.509w
.521w

p

p

p

p

p

Table 3: Results of prediction over verbose (long) queries.
The superscripts and subscripts notations are identical to
those defined in Table 2.

Next, in most cases, our newly proposed method, PI, had a better prediction. We note that, while the former baselines either utilize only document-level or only passage-level features, PI basically utilizes both feature types. This, therefore, supports again the
importance of passage-level QPP signals for the document-level
QPP task. Furthermore, by calibrating the two “features" of PI according to our first observation in Section 3.3, we obtained an enhanced performance of C-PI over PI.
Overall, WPM2 was the best document-level only QPP method.
Yet, further following the second observation we made in Section 3.3,
we can observe that, by solely adding the single passage-level feature of p̂(r |d) (see again Eq. 4) to WPM2, which resulted in the
WPM2+PI extension, has obtained a significant boost in prediction quality. This is yet another strong testimony on the importance of passage-level QPP signals for the document-level QPP
task.

4.5

Prediction over verbose queries

The results of our second evaluation setting with verbose (long)
queries are summarized in Table 3. First, comparing these results
with the results in Table 2, it becomes clear that those baseline
methods that only utilize document-level features, perform significantly worse in this setting compared to their performance over
short queries. Moreover, those QPP methods that utilize passagelevel information (i.e., Clarity(psg), WIG(psg), NQC(psg), PI, CPI and WPM2+PI) provide significantly better prediction. This
demonstrates that, utilizing passage-information for QPP becomes
even more eminent in verbose query settings. Verbose queries are
usually more informative than short queries [8]; yet, existing documentlevel QPP methods are not well-designed to predict their quality.
Verbose queries tend to express more focused information needs [8];
hence, passage-information may provide a better “proxy" to such
needs satisfaction within retrieved documents [1].
Further notable is that, compared to the six first baseline methods, PI provided significantly better prediction quality; and even
exceeded in some of the cases that of WPM2 – a very strong
document-level QPP baseline. This serves as another strong evidence that, a mixed document-level and passage-level prediction
strategy, such as the one employed by PI, is a better choice for

896

verbose query performance prediction settings. Moreover, such a
strategy guarantees a more robust QPP, which is less sensitive to
query type. Finally, again, we can observe that, by further calibrating PI (i.e., C-PI), even better prediction quality can be achieved.
Moreover, the contribution of PI’s passage-level calibration feature p̂(r |d) to WPM2+PI is even more notable in this setting.

5

CONCLUSIONS

The conclusions of this work are two-fold. First, this work clearly
demonstrates that existing post-retrieval QPP methods that only
focus on document-level features are not well suited to the prediction of verbose queries performance. Utilizing passage-information
for such QPP sub-task is clearly important. As we further demonstrated, a mixed strategy that considers both types of features, such
as the one employed by the PI variants, may result in an enhanced
prediction, which is less sensitive to query type.

REFERENCES
[1] Michael Bendersky and W. Bruce Croft. Modeling higher-order term dependencies in information retrieval using query hypergraphs. In Proceedings of the 35th
International ACM SIGIR Conference on Research and Development in Information
Retrieval, SIGIR ’12, pages 941–950, New York, NY, USA, 2012. ACM.
[2] Michael Bendersky and Oren Kurland. Utilizing passage-based language models
for document retrieval. In Proceedings of the IR Research, 30th European Conference on Advances in Information Retrieval, ECIR’08, pages 162–174, Berlin, Heidelberg, 2008. Springer-Verlag.
[3] James P. Callan. Passage-level evidence in document retrieval. In Proceedings
of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’94, pages 302–310, New York, NY, USA,
1994. Springer-Verlag New York, Inc.
[4] David Carmel and Oren Kurland. Query performance prediction for ir. In Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’12, pages 1196–1197, New York, NY, USA,
2012. ACM.
[5] Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft. Predicting query performance. In Proceedings of the 25th Annual International ACM SIGIR Conference
on Research and Development in Information Retrieval, SIGIR ’02, pages 299–306,
New York, NY, USA, 2002. ACM.
[6] Steve Cronen-Townsend, Yun Zhou, and W Bruce Croft. Precision prediction
based on ranked list coherence. Information Retrieval, 9(6):723–755, 2006.
[7] Mathias Géry and Christine Largeron. Bm25t: A bm25 extension for focused
information retrieval. Knowl. Inf. Syst., 32(1):217–241, July 2012.
[8] Manish Gupta and Michael Bendersky. Information retrieval with verbose
queries. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’15, pages 1121–1124,
New York, NY, USA, 2015. ACM.
[9] Eyal Krikon, David Carmel, and Oren Kurland. Predicting the performance of
passage retrieval for question answering. In Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM ’12, pages
2451–2454, New York, NY, USA, 2012. ACM.
[10] Oren Kurland, Anna Shtok, Shay Hummel, Fiana Raiber, David Carmel, and Ofri
Rom. Back to the roots: A probabilistic framework for query-performance prediction. In Proceedings of the 21st ACM International Conference on Information
and Knowledge Management, CIKM ’12, pages 823–832, New York, NY, USA, 2012.
ACM.
[11] Victor Lavrenko and W. Bruce Croft. Relevance based language models. In
Proceedings of SIGIR ’01.
[12] Haggai Roitman, Shai Erera, Oren Sar-Shalom, and Bar Weiner. Enhanced mean
retrieval score estimation for query performance prediction. In Proceedings of
the ACM SIGIR International Conference on Theory of Information Retrieval, ICTIR
’17, pages 35–42, New York, NY, USA, 2017. ACM.
[13] Anna Shtok, Oren Kurland, David Carmel, Fiana Raiber, and Gad Markovits.
Predicting query performance by query-drift estimation. ACM Trans. Inf. Syst.,
30(2):11:1–11:35, May 2012.
[14] Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron Fernandes, and Gregory Marton.
Quantitative evaluation of passage retrieval algorithms for question answering.
In Proceedings of SIGIR ’03.
[15] Yun Zhou and W. Bruce Croft. Query performance prediction in web search
environments. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’07, pages
543–550, New York, NY, USA, 2007. ACM.

