Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Modeling Multidimensional User Relevance in IR using Vector
Spaces
Sagar Uprety

Yi Su

The Open University
Milton Keynes, United Kingdom
sagar.uprety@open.ac.uk

School of Computer Science and Technology, Tianjin
University
Tianjin, China
suyi2016@tju.edu.cn

Dawei Song∗

Jingfei Li†

The Open University
Milton Keynes, United Kingdom
dawei.song@open.ac.uk

National Computer Network Emergency Response
Technical Team/Coordination Center of China
Beijing, China
jingfl@foxmail.com

ABSTRACT

1

It has been shown that relevance judgment of documents is influenced by multiple factors beyond topicality. Some multidimensional
user relevance models (MURM) proposed in literature have investigated the impact of different dimensions of relevance on user
judgment. Our hypothesis is that a user might give more importance to certain relevance dimensions in a session which might
change dynamically as the session progresses. This motivates the
need to capture the weights of different relevance dimensions using feedback and build a model to rank documents for subsequent
queries according to these weights. We propose a geometric model
inspired by the mathematical framework of Quantum theory to
capture the user’s importance given to each dimension of relevance
and test our hypothesis on data from a web search engine and TREC
Session track.

There is a growing body of work investigating different factors
which affect user’s judgment of relevance [1, 2, 6, 7, 9, 11–13]. A
multidimensional user relevance model (MURM) was proposed
[12, 13] which defined five dimensions of relevance namely "Novelty", "Reliability", "Scope", "Topicality" and "Understandability". In
a recent paper [7] an extended version of the MURM comprising
two additional dimensions "Habit" and "Interest" is proposed. The
"Interest" dimension refers to the topical preferences of users in the
past, while "Habit" refers to their behavioral preferences. For example, accessing specific websites for some particular information
or task is considered under the "Habit" dimension. Experiments on
real-world data show that certain dimensions, such as "Reliability"
and "Interest", are more important for the user than "Topicality", in
judging a document.
Our hypothesis is that in a particular search session or search
task, there is a particular relevance dimension or a combination
of relevance dimensions which the user has in mind before judging documents. For example, if the user wants to get a visa to a
country, he or she would prefer documents which are more reliable
("Reliability") for this task, but when looking to book flights to
that country, the user might go to his or her preferred websites
("Habit"). Therefore, for next few queries of the session, "Habit"
dimension becomes more important. Thus, the importance given
to relevance dimensions might change as the session progresses or
tasks switch. By capturing the importance assigned to each dimension for a query, we can model the dimensional importance and use
it to improve the ranking for the subsequent queries. The relevance
dimensions are modeled using the Hilbert space formalism of Quantum theory which unifies the logical, probabilistic and vector space
based approaches to IR [8]. We place the user’s cognitive state with
respect to a document at the center of the IR process. Such a state
is modeled as an abstract vector with multiple representations existing at the same time in different basis corresponding to different
relevance dimensions. This cognitive state comes into reality only
when it is measured in the context of user interactions.

CCS CONCEPTS
• Information systems → Personalization;

KEYWORDS
Information Retrieval, User modeling, Multidimensional Relevance
ACM Reference Format:
Sagar Uprety, Yi Su, Dawei Song, and Jingfei Li. 2018. Modeling Multidimensional User Relevance in IR using Vector Spaces. In SIGIR ’18: The 41st
International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8–12, 2018, Ann Arbor, MI, USA. ACM, New York, NY,
USA, 4 pages. https://doi.org/10.1145/3209978.3210130

∗ Also
† Also

with Beijing Institute of Technology. Correspondence Author.
with Tianjin University. Correspondence author.

ACM acknowledges that this contribution was authored or co-authored by an employee,
contractor or affiliate of a national government. As such, the Government retains a
nonexclusive, royalty-free right to publish or reproduce this article, or to allow others
to do so, for Government purposes only.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210130

993

INTRODUCTION

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA

(a) Figure 1.a

2

S. Uprety et al.

(b) Figure 1.b

GEOMETRIC REPRESENTATION OF
MULTIDIMENSIONAL RELEVANCE

It does not have a definite state, and a particular representation
comes into picture only when we talk of a particular relevance dimension. This is similar to the concept of a state vector in Quantum
theory which contains all the information about a quantum system,
yet is an abstract entity and has different representations of the
same system. We get to see a particular representation of a system
depending on how we measure it. A document may look highly relevant in one basis, if it has a high weight in that basis and the user
makes judgment from the perspective of that relevance dimension.
However, the relevance can change if the user considers a different
basis (a different perspective of looking at the document).

Consider a real-valued two dimensional Hilbert Space. Relevance
with respect to a dimension (e.g. Reliability) is a vector in this
Hilbert space. Non-relevance with respect to the same dimension is
an orthogonal vector to it. Further, we denote vectors as kets following the Dirac’s notation. For example, the vectors for relevance and
non-relevance
with respect to novelty are denoted as |novelty⟩ and
E
 . Figure 1.a shows the construction of a two-dimensional
novelty
Hilbert Space for a relevance dimension.
Next, we model the user’s perception of a document with respect to a dimension of relevance also as a vector in this Hilbert
space. This vector is a superposition of relevance and non-relevance
vectors with
E respect to a dimension, e.g., |d⟩ = α |novelty⟩ +
 . The coefficient |α | 2 is the weight (i.e., probability of
β novelty

3

CAPTURING USER’S CHANGING WEIGHTS
TO RELEVANCE DIMENSIONS

Having established the geometric representation of documents, we
can make use of it to capture the weights of relevance dimensions
for a user in response to a query (Algorithm 1).
In Algorithm 1, the input parameters docsALL and docsSAT correspond to the list of all retrieved documents and SAT-clicked [7]
documents for a query respectively. We quantitatively define each
of the seven relevance dimensions using some features. For each
query-document pair, these features are extracted and computed
(Step 3). They are integrated into the LambdaMART [4] Learning
to Rank(LTR) algorithm to generate seven relevance scores (one
for each dimension) for the query-document pair (Step 4). Please
refer to [7] for more details about the features defined for each
dimension and the hyper-parameters of the LTR algorithm. Thus,
for a query and its set of retrieved documents, we have seven different rankings, one for each relevance dimension. The seven scores
assigned to a document for a query are then normalized using the
min-max normalization technique across all the documents for the
query (Step 5). The normalized score for each dimension forms the
coefficient of superposition of the relevance vector for the respective dimension. It quantitatively represents user’s perception of
the document for that dimension. For example, for a query q, let
d 1 , d 2 , ..., dn be the ranking order corresponding to the "Interest"
dimension. Let relevance scores be λ 1 , λ 2 , ..., λn respectively. We
construct the vector for document d 1 in the ’Interest’ basis as:

relevance) the user assigns to document d in term of novelty, and
|α | 2 + |β | 2 = 1. We will talk about how to calculate these coefficients
in the next section. Figure 1.b shows the modeling of user’s cognitive state for document d with respect to the Novelty dimension.
Depending on a user’s preference of relevance dimensions for a
particular query, the user will judge the same document differently.
A document might be of interest to the user but may not be novel
when the user is looking for latest documents about the query. This
phenomena can be modeled in the same Hilbert space by having
different basis for different dimensions of relevance. The same
document d can be written in terms of another set of basis vectors
corresponding to another dimension of relevance. For example:
E

|d 1 ⟩ = α 11 |novelty⟩ + β 11 novelty
E

= α 12 |habit⟩ + β 12 habit
= α 13 |topic⟩ + β 13 t
opic

(c) Figure 1.c

(1)

and so on in all seven basis. Figure 1.c shows the construction of
such a Hilbert space showing two basis for simplicity.
We have represented user’s cognitive state with respect to a
single document in different basis corresponding to different dimensions of relevance. Similarly, we can do that for all the documents retrieved for a query. Each document will be represented in
a separate Hilbert space.
The user’s cognitive state for a document d is an abstract vector,
because the vector has different representations in different basis.


(2)
|d 1 ⟩ = α 11 |interest⟩ + β 11 interest
q
λ −min(λ)
where α 11 = max1(λ)−min(λ) , where max(λ) is the maximum
value among λ 1 , λ 2 , ..., λn . Square root is taken in accordance with

994

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Modeling Multidimensional User Relevance in IR using Vector Spaces

4

the quantum probability framework. Similarly, the second document is represented in its Hilbert space as:

|d 2 ⟩ = α 21 |interest⟩ + β 21 interest

(3)

5

3:

4:

5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

16:

RESULTS AND DISCUSSION

We summarize the evaluation results for Bing query logs in Table
1. In the paper [7], re-ranking by using the features of "Reliability"
dimension gives the best performance for Bing data. However we
show that the results obtained by using the weighted combination
gives slightly better results (Table 1). The improvement is not significant but the fact is that weighted combination is a general method
which will work for other datasets too. Similar to the results reported in [7], for TREC data, it is not "Reliability" but "Interest"
which comes out as the best dimension for re-ranking (Table 2).
Therefore one cannot use a fixed relevance dimension for ranking
the documents. Table 2 shows that our weighted combination approach also gives improved performance for the TREC data. It is
to be noted that TREC data contains information about the last
query of each session, and not all the queries. Thus our weighted
approach uses the captured weights of the last query of a session
to re-rank the documents for the last query of the next session.
Improvement over the best result (corresponding to Interest) means
that the weighted combination method for ranking works across
sessions as well. This indicates that dimensional preference is not
only dependent upon the task, but user might have an intrinsic
preference for some dimensions as well. Also note that the "Topicality" scores correspond to a traditional baseline model as we use
tf-idf and BM25 as features for the "Topicality" dimension.

Algorithm 1 Capturing weights given to relevance dimensions
1:

EXPERIMENT AND ANALYSIS

We use the same datasets as used in [7]. The first one is the query
logs of the Bing search engine and the second one is the combined
session tracks of TREC 2013 and TREC 2014. While the Bing query
logs contain information about each query in a session, the TREC
dataset only contains the data about the last query for each session.
The relevance criteria for Bing logs is SAT clicks and for TREC
data we consider relevance grades of 1 and above to correspond
to relevant documents. In Section 3, we captured the user’s dimensional preference for a query in the form of weights. We now use
these weights for the next query in the session, to take a weighted
combination of the relevance scores of all seven dimensions for
each document of the next query. Thus, for the new query, a new
relevance score for each document is created based on the weighted
dimensional preference for the previous query. We re-rank the documents according to these new scores and perform evaluation. We
use the NDCG metric for evaluation and compare the values with
those obtained in [7].
We also performed an initial analysis of the data to support
our hypothesis that some combination of relevance dimensions
are preferred by the user in a search session. For some randomly
sampled 4837 sessions of the Bing query logs, we found that in 3910
or 80.84 percent of the sessions, one of the top three dimensions
for the first query of the session remains in the top three for all
the queries of the session. Figure 2.a is the snapshot of one such
session showing that the "Reliability" remains the top dimension
throughout. Figure 2.b shows 20 consecutive sessions for TREC
data.

q
λ −min(λ)
with α 21 = max2(λ)−min(λ) . As done with the interest dimension,
we can represent each document in a Hilbert space in all the different basis corresponding to different dimensions of relevance (Steps
6 - 8).
Now in the original list of documents, suppose the user SATclicks on document d x . We have already formed the Hilbert space
for each document. The coefficients of superposition for |d x ⟩ vector
in a basis represent the importance of document d x with respect to
the relevance dimension represented by that basis. We re-capture
it by taking the projection of |d x ⟩ onto the relevance vector of
each basis by taking square of their inner products (Step 13), for example, | ⟨novelty|d x ⟩ | 2 , | ⟨reliability|d x ⟩ | 2 , etc. Here the notation
⟨A|B⟩ is the inner product of the vectors A and B. It denotes the
probability amplitude, which is in general a complex number. The
square of probability amplitude is the probability that document
B is relevant with respect to dimension A. For real valued vectors,
inner product is same as the dot product. Let α x 1 , ..., α x 7 be the
projections obtained. Note that the values of these projections are
the same normalized scores which we calculated above. If there are
more than one SAT clicked documents for a query, we average over
the projection scores for each dimension (Step 15).
Thus, for a given query in a search session, we have quantitatively captured the user’s cognitive state. This cognitive state or the
user preference for each dimension is the average relevance score
for that dimension over all SAT-clicked documents of the query.
These weights are used to re-rank documents for the next query in
the session, as explained in the next section.

2:

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA

procedure captureWeights(rel, docsALL, docsSAT )
for all r in rel do
▷ rel - list of 7 dimensions
f eatures[r ] ← дetFeatures(docsALL, r )
▷ Extract
features from all retrieved docs for a given query
scores[r ][d] ← reRank(docsALL, f eatures[r ])
▷
re-rank based on each dim and get score
normScores[r ][d] ← normalizeScores(scores[r ][d])
for all d in docsALL do
α[d][r ] ← normScores[d][r ] ▷ construct vectors
β[d][r ] ← 1 − |α[d][r ]| 2
for all r in rel do
totalW eiдht ← 0
avдW eiдht[r ] ← 0
for all d in docsSAT do
wdr ← | ⟨r |d⟩ | 2
▷ Take projections(|α[d][r ]| 2 )
totalW eiдht ← totalW eiдht + wdr
avдW eiдht[r ] ← totalW eiдht/|docsSAT | ▷ Only SAT
clicks considered
return avдW eiдht ▷ User’s importance to each dimension

995

Short Research Papers I

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA

S. Uprety et al.

(a) Figure 2.a

(b) Figure 2.b

Dimension

NDCG@1

NDCG@5

NDCG@10

NDCG@ALL

Habit
Interest
Novelty
Reliability
Scope
Topicality
Understandability
Weighted Combination

0.3772
0.4574
0.4110
0.6457
0.2501
0.2001
0.2782
0.6552

0.5958
0.6178
0.6025
0.7687
0.4692
0.4486
0.4968
0.7814

0.6533
0.6844
0.6688
0.8038
0.56156
0.5352
0.5867
0.8127

0.6645
0.6955
0.6783
0.8110
0.5726
0.5482
0.5971
0.8189

such cognitive phenomena in real world data, and the Hilbert space
representation described in this paper forms a solid basis to carry
out such experiments in the future.

ACKNOWLEDGMENTS
This work is funded by the European Union’s Horizon 2020 research
and innovation programme under the Marie Sklodowska-Curie
grant agreement No 721321.

Table 1: Bing logs evaluation

REFERENCES
Dimension

NDCG@1

NDCG@5

NDCG@10

NDCG@ALL

Habit
Interest
Novelty
Reliability
Scope
Topicality
Understandability
Weighted Combination

0.0989
0.1981
0.0966
0.1120
0.1318
0.1459
0.1653
0.2364

0.1406
0.2126
0.1180
0.1333
0.1526
0.1520
0.1913
0.2663

0.1418
0.2242
0.1316
0.1431
0.1647
0.1887
0.1878
0.2729

0.1592
0.1831
0.1557
0.1614
0.1671
0.1701
0.1764
0.1944

[1] Carol L. Barry. 1998. Document representations and clues to document relevance. Journal of the American Society for Information Science 49, 14 (1998), 1293–
1303. https://doi.org/10.1002/(SICI)1097-4571(1998)49:14<1293::AID-ASI7>3.0.
CO;2-E
[2] Ulises Cerviño Beresi, Yunhyong Kim, Dawei Song, and Ian Ruthven. 2011. Why
did you pick that? Visualising relevance criteria in exploratory search. International Journal on Digital Libraries 11, 2 (27 Sep 2011), 59. https://doi.org/10.1007/
s00799-011-0067-7
[3] Peter Bruza and Vivien Chang. 2014. Perceptions of document relevance. Frontiers
in Psychology 5 (2014), 612. https://doi.org/10.3389/fpsyg.2014.00612
[4] Christopher J. C. Burges. 2010. From RankNet to LambdaRank to LambdaMART:
An Overview.
[5] Jerome R. Busemeyer and Peter D. Bruza. 2012. Quantum Models of Cognition
and Decision (1st ed.). Cambridge University Press, New York, NY, USA.
[6] Célia da Costa Pereira, Mauro Dragoni, and Gabriella Pasi. 2009. Multidimensional
Relevance: A New Aggregation Criterion. In Advances in Information Retrieval,
Mohand Boughanem, Catherine Berrut, Josiane Mothe, and Chantal Soule-Dupuy
(Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 264–275.
[7] Jingfei Li, Peng Zhang, Dawei Song, and Yue Wu. 2017. Understanding an
enriched multidimensional user relevance model by analyzing query logs. Journal
of the Association for Information Science and Technology 68, 12 (2017), 2743–2754.
https://doi.org/10.1002/asi.23868
[8] C. J. van Rijsbergen. 2004. The Geometry of Information Retrieval. Cambridge
University Press, New York, NY, USA.
[9] Anastasios Tombros, Ian Ruthven, and Joemon M. Jose. 2005. How users assess
Web pages for information seeking. Journal of the American Society for Information
Science and Technology 56, 4 (2005), 327–344. https://doi.org/10.1002/asi.20106
[10] Benyou Wang, Peng Zhang, Jingfei Li, Dawei Song, Yuexian Hou, and Zhenguo
Shang. 2016. Exploration of Quantum Interference in Document Relevance
Judgement Discrepancy. Entropy 18, 12 (Apr 2016), 144. https://doi.org/10.3390/
e18040144
[11] Yunjie Xu and Hainan Yin. 2008. Novelty and topicality in interactive information
retrieval. Journal of the American Society for Information Science and Technology
59, 2 (2008), 201–215. https://doi.org/10.1002/asi.20709
[12] Yunjie (Calvin) Xu and Zhiwei Chen. 2006. Relevance judgment: What do
information users consider beyond topicality? Journal of the American Society for Information Science and Technology 57, 7 (2006), 961–973. https:
//doi.org/10.1002/asi.20361
[13] Yinglong Zhang, Jin Zhang, Matthew Lease, and Jacek Gwizdka. 2014. Multidimensional Relevance Modeling via Psychometrics and Crowdsourcing. In
Proceedings of the 37th International ACM SIGIR Conference on Research &#38;
Development in Information Retrieval (SIGIR ’14). ACM, New York, NY, USA,
435–444. https://doi.org/10.1145/2600428.2609577

Table 2: TREC data evaluation

6

CONCLUSION AND FUTURE WORK

We have thus shown that capturing user’s weights for relevance
dimensions and ranking based on the combination of these weights
leads to a better performance than using only one of the dimensions.
The need for a Hilbert space framework is inspired by the fact that
some relevance dimensions are incompatible for some documents. A
document may not have high relevance weights for both "Novelty"
and "Habit" dimensions at the same time. The more relevant it
is in the "Novelty" dimension, the less relevant it will be in the
"Habit" dimension. This is similar to the Uncertainty Principle in
Quantum Theory. We therefore model each relevance dimension as
a different basis. For some documents, the basis might coincide, but
in general there is incompatibility between relevance dimensions
which leads to interference and order effects [5, 10]. For example, a
user may find a document less reliable due to its source, but when
the user considers the "Topicality" dimension and reads it, it might
remove the doubts about the reliability. Thus "Topicality" interferes
with "Reliability" in relevance judgment. Such order effects were
investigated in [3] through user studies. We intend to investigate

996

