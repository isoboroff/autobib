Session 5C: New Metrics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

An Axiomatic Analysis of Diversity Evaluation Metrics:
Introducing the Rank-Biased Utility Metric
Enrique Amigó

Damiano Spina

Jorge Carrillo-de-Albornoz

NLP & IR Group at UNED
Madrid, Spain
enrique@lsi.uned.es

RMIT University
Melbourne, Australia
damiano.spina@rmit.edu.au

NLP & IR Group at UNED
Madrid, Spain
jcalbornoz@lsi.uned.es

ABSTRACT

Since then, many evaluation metrics have been proposed to measure
the effectiveness of information retrieval systems [20, 22, 27].
Selecting a suitable set of metrics for a specific task is challenging. Comparing metrics empirically against user satisfaction or
search effectiveness requires data that is often unavailable. Moreover, findings may be biased to the subjects, retrieval systems or
other experimental factors.
An alternative consists of modeling theoretically the desirable
properties of retrieval systems, as well as the abstraction of the
expected users’ behavior when performing a specific task. For instance, a metric that looks at how early the relevant document is
retrieved in the ranking – such as Reciprocal Rank [26] – would be
an appropriate metric to analyze the performance of systems on
a single-item navigational task. However, is often challenging to
come up with the proper evaluation tools for more complex search
scenarios, as is the case of search result diversification [19]. In this
context, the ranking of retrieved documents must be optimized
in such a way that diverse query aspects are captured in the first
positions. The challenge is that the evaluation of system outputs
is affected by multiple variables such as: the deepness of ranking
positions, the amount of documents in the ranking related to the
same query aspect, relevance grades, the diversity of query aspects
captured by single documents or the user’s effort when inspecting
the ranking.
Axiomatic analysis has been shown to be an effective methodology to better understand the foundamentals of evaluation metrics [3, 4, 10, 25]. In the context of evaluation, axiomatic approaches
consist of a verifiable set of formal constraints that reflect which
quality factors are captured by metrics, facilitating the metric selection in specific scenarios. To our knowledge, there is no comprehensive axiomatic analysis of the behavior of diversity metrics in
the literature. This paper provides a set of ten formal constraints
that focus on both retrieval and diversity quality dimensions.
We found that every constraint is satisfied at least by one metric. However, none of the existing diversity metrics satisfy all the
proposed constraints simultaneously. In order to solve this gap, we
define the metric Rank-Biased Utility (RBU) by integrating components from different metrics in order to capture every formal
constraints. RBU is an adaptation of the well-known Rank-Biased
Precision metric [16] that incorporates redundancy and the user’s
effort associated to the inspection of documents in the ranking. Our
experiments using standard diversity test collections validate our
axiomatic analysis. Results show that, satisfying every constraint
with a single metric leads to unanimous evaluation decisions when
compared against other existing metrics, i.e., RBU captures quality
criteria which are reflected by different metrics. Therefore, this metric offers a solution in the absence of knowledge about the specific

Many evaluation metrics have been defined to evaluate the effectiveness ad-hoc retrieval and search result diversification systems.
However, it is often unclear which evaluation metric should be used
to analyze the performance of retrieval systems given a specific task.
Axiomatic analysis is an informative mechanism to understand the
fundamentals of metrics and their suitability for particular scenarios.
In this paper, we define a constraint-based axiomatic framework to
study the suitability of existing metrics in search result diversification scenarios. The analysis informed the definition of Rank-Biased
Utility (RBU) – an adaptation of the well-known Rank-Biased Precision metric – that takes into account redundancy and the user
effort associated to the inspection of documents in the ranking. Our
experiments over standard diversity evaluation campaigns show
that the proposed metric captures quality criteria reflected by different metrics, being suitable in the absence of knowledge about
particular features of the scenario under study.

CCS CONCEPTS
• Information systems → Retrieval effectiveness;

KEYWORDS
Evaluation, Search result diversification, Axiomatic analysis
ACM Reference Format:
Enrique Amigó, Damiano Spina, and Jorge Carrillo-de-Albornoz. 2018.
An Axiomatic Analysis of Diversity Evaluation Metrics: Introducing the
Rank-Biased Utility Metric. In SIGIR ’18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July
8–12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https:
//doi.org/10.1145/3209978.3210024

1

INTRODUCTION

The development of better information retrieval systems is driven
by how improvements are measured. The design of test collections
and evaluation metrics that started with the Cranfield paradigm
in the early 1960s allowed researchers to analyze the quality of
different retrieval models in an automated and cost-effective way.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210024

625

Session 5C: New Metrics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

characteristic of a diversity-oriented retrieval scenario. Moreover,
the theoretical framework presented in this paper helps to decide
which metric should be used.
The paper is organized as follows. Section 2 describes related
work on evaluation of evaluation metrics. Section 3 introduces
the formal constraints that we propose to analyze relevance and
diversity properties of metrics. Section 4 provides a comprehensive
analysis of existing diversity metrics according to these constraints
and Section 5 defines the proposed RBU metric. Section 6 details the
results of our experiments. Finally, Section 7 concludes the work.

found only a few initial works in the context of formal constraints
for search result diversification. For instance, Leelanupab et al. [13]
reviewed the appropriateness of intent-aware, stating an extreme
particular situation in which ERR-IA does not behave as expected.
In our work, we meta-evaluate existing metrics on the basis of
ten constraints that formalize desirable properties for ranking and
diversity effectiveness.

2

We formalize the output of a document retrieval system as an ordered list of documents d⃗ = (d 1 , . . . , dn ) of length n, extracted from
a collection of documents D. In order to express formal constraints,
we use d⃗i↔j to denote the result of swapping documents between
positions i and j. Likewise, d⃗d↔d ′ denotes the result of replacing
⃗
the document d with the document d ′ in the ranking d.
For search result diversification, we consider a set of query aspects T = {t 1 , . . . , tm }. For instance, users searching for a restaurant may be interested in the menu, the offers, opening times, etc.
Each aspect has an associated weight w (t j ) and the sum of all aspect
P
weights adds up to 1: m
j=1 w (t j ) = 1.
On the other hand, r (di , t j ) ∈ [0 . . . 1] represents the graded
relevance of document di to the aspect t j . We assume the user’s behavior follows the cascade model, i.e., the user inspects the ranking
sequentially from the top to the bottom, until either (i) the user’s
information needs get satisfied or (ii) the user stops looking (i.e.,
user’s patience is exhausted). Following the same user model than
the one used by Expected Reciprocal Rank [6], we consider relevance as the suitability of the document to satisfy the user needs,
which has a negative correspondence with the probability of exploring more documents. Finally, we use Q (d⃗) to denote the ranking
quality score, i.e., the score given by applying an evaluation metric
⃗
Q to a given ranking d.
Our axiomatic approach consists of a set of ten formal constraints
that evaluation metrics may satisfy. These constraints are grouped
into two sets: relevance-oriented and diversity-oriented, that we
describe below.
In the definition of the constraints, we may refer to the following conditions: single aspect (|T | = 1); balanced aspects (∀t ∈
T (w (t ) = 1/|T |)); binary relevance (∀t, d (r (d, t ) ∈ {0, rc })); no
aspect overlap (r (d, t ) > 0 ⇒ ∀t ′ , t (r (d, t ′ ) = 0)); and relevance
contribution (r (d, t ) ≪ 1). The last condition means that finding
new relevant documents about the same topic is always effective.
In other words, there is always room for new documents to fully
satisfy the user needs.

3 AXIOMATIC CONSTRAINTS
3.1 Problem Formalization

RELATED WORK

There is no consensus of meta-evaluation criteria for search result diversification. Some works inherit meta-evaluation criteria
from ad-hoc metrics such as sensitivity to system differences [11,
14, 17, 18]. This methodology however does not give information
about to what extent metrics capture diversity properties. Smucker
and Clarke [21] studied the correspondence between metric scores
and user effort when exploring document rankings. This methodology has the advantage of being realistic – effort is calibrated from
historical log data – but only focuses on partial quality aspects.
Most of works on diversity metrics are supported by descriptive analysis. In 2008, Clarke et al. [7] meta-evaluated α-nDCG by
analyzing the effect of modifying the diversity parameter α under
different datasets. One year later, Agrawal et al. [1] checked the
intent-aware scheme for diversification by studying the evaluation
results of three search engines. Clarke et al. [8] proposed Noveltyand Rank-Biased Precision (NRBP), an extension of RBP [16] for
diversification, joining properties of the original RBP metric, αnDCG and intent-aware metrics. In 2010, Sakai et al. [17] compared
their proposed approach to α-NDCG and NRBP, in terms of metric agreement under different parameters. The authors considered
some meta-evaluation criteria such as interpretability, computability or capability to accommodate graded relevance and score ranges.
Three years later, Chandar and Carterette [5] evaluated their approach by studying correlation with previous metrics while reflecting other ranking quality issues. Luo et al. [14] proposed the Cube
Test metric. They studied the effect of the metric parameters under
synthetic system outputs, in the same manner than Clarke et al. [7].
Tangsomboon and Leelanupab [23] in 2014 and also Yu et al. [31]
in 2017, supported their proposed metrics in terms of agreement
and disagreement with previous metrics.
Not many works define a way of quantifying the suitability of
metrics to capture diversity. An exception is the work by Golbus
et al. [11] who defined Document Selection Sensitivity. This metameasure reports to what extent metrics are sensitive to document
rankings containing relevant documents but different grades of
diversity. Within this line, we define in this work Metric Unanimity
(MU), which quantifies to what extent a metric is sensitive to quality
aspects captured by other existing metrics.
On the other hand, metrics have been successfully analyzed in
terms of formal constraints in ad-hoc retrieval scenarios [3, 10,
15]. The axiomatic methodology consists of identifying theoretical
situations in which metrics should behave in a particular manner.
This methodology has several strengths: it is objective, independent
from datasets and it facilitates the interpretation of metrics. We

3.2

Relevance-Oriented Constraints

In order to isolate relevance from diversity and redundancy, for
these constraints we will assume single aspect and relevance contribution.
For the sake of legibility, we use the notation: r (d ) = r (d, t ). We
also denote d rel and d ¬rel as relevant and non-relevant documents,
respectively. That is: ∀i ∈ 1..n. r (di¬r el ) = 0 and r (dir el ) = rc .
Under these assumptions, we import the five constraints proposed

626

Session 5C: New Metrics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

3.3

by Amigó et al. [3] which capture previous axiomatic properties [10,
15].
Constraint 1 (Priority, Pri). Swapping items in concordance
with their relevance increases the ranking quality score. Being k > 0:


 
r (di+k ) > r (di ) =⇒ Q d⃗i↔i+k > Q d⃗
(1)

Constraint 6 (Query Aspect Diversity, AspDiv). Covering
more aspects in the same document (i.e., without additional effort of
inspecting more documents) increases the score. Assuming relevance
contribution (∀d, t : r (d, t ) ≪ 1):




 
∀t ∈ T . r (d ′, t ) > r (di , t ) =⇒ Q d⃗d ↔d ′ > Q d⃗
(6)

The next constraint is based on the intuition that the effect of relevance depends on the document ranking position. This constraint
is also referred as top-heaviness:

i

i

i

To calculate the gain obtained by observing a new relevant document in the ranking, most of the existing diversity metrics take
into account the number of previously observed documents that are
related with the same aspect. The more an aspect has been covered
earlier in the ranking, the less a new document relevant to this
aspect contributes to the gain. Formally:

Constraint 2 (Deepness, Deep). Correctly swapping contiguous
items has more effect in early ranking positions:




r (di ) = r (d j ) < r (di+1 ) = r (d j+1 ) =⇒ Q d⃗i↔i+1 > Q d⃗j↔j+1

Diversity-Oriented Constraints

The first diversity-oriented constraint is related to the fact that the
metric should be sensitive to the novelty of aspects covered by a
single document:

(2)

where i < j.

Constraint 7 (Redundancy, Red). Assuming binary relevance,
balanced aspects and no aspect overlap, and being d and d ′ documents
relevant to different aspects r (d, t ) = r (d ′, t ′ ) = rc , then:

The next constraint reflects that the effort spent by the user to
inspect a long (deep) list of search results is limited. In other words,
there is an area of the ranking that may never get explored by the
user:

⃗ r (di , t ) = rc }| > |{di ∈ d.
⃗ r (di , t ′ ) = rc }| =⇒
|{di ∈ d.


 
⃗ d ′ > Q d,
⃗d
Q d,

Constraint 3 (Deepness Threshold, DeepTh). Assuming binary relevance, there exists a value n large enough such that, retrieving
only one relevant document at the top of the ranking is better than
retrieving n relevant documents after n non-relevant documents:




∃n ∈ N+ . Q d 1rel , . . . > Q d 1¬rel , . . . , dn¬rel , d 1rel , . . . , dnrel (3)

(7)

The Red constraint assumes binary relevance, by counting relevant documents for each query aspect. In order to consider graded
relevance in previously observed documents, we can apply the
monotonicity principle. That is, if an aspect t is captured to a greater
extent than a second aspect t ′ in every previously observed document, then the ranking is more redundant w.r.t. t than t ′ . Formally:

On the other hand, we can assume that there exists a (short)
ranking area which is always explored by the user. In other words,
at least a few documents are inspected by the user with a minimum
effort. This means that, at the top of the ranking, the amount of
captured relevant documents is more important than their relative
rank positions.

Constraint 8 (Monotonic Redundancy, MRed). Assuming
two balanced aspects (T = {t, t ′ }), relevance contribution, and being
d and d ′ documents exclusively relevant to each aspect, 0 < r (d, t ) =
r (d ′, t ′ ) ≪ 1 and r (d, t ′ ) = r (d ′, t ) = 0:


 
⃗ r (di , t ) > r (di , t ′ )  =⇒ Q d,
⃗ d ′ > Q d,
⃗d
∀di ∈ d.
(8)

Constraint 4 (Closeness Threshold, CloseTh). Assuming
binary relevance, there exists a value m small enough such that retrieving one relevant document in the first position is worse than m
relevant documents after m non-relevant documents:




¬rel rel
rel (4)
∃m ∈ N+ . Q d 1rel , . . . < Q d 1¬rel , . . . , dm
, d 1 , . . . , dm

Intuitively, as well as the exploration capacity or patience of the
user is limited, the user’s information need is also finite. This means
that there should exists a certain point on which a new single piece
of information completely satisfies user’s information needs, in
such a way that retrieving any other documents addressing the
same query aspect is not beneficial. Formally:

In some particular scenarios, however, this may not hold. For
instance, in audio-only search scenarios, search results may be
delivered sequentially one-at-a-time.
Finally, the amount of documents returned is also an aspect of
the system quality. In the same manner that capturing diversity
in the first positions is desirable, adding non-relevant documents
to the end of the ranking should be penalized by metrics. In other
words, the cutoff used by the system to stop returning search results
has also an impact on users. Therefore, adding noise at the bottom
of the ranking should decrease its effectiveness.

Constraint 9 (Aspect Relevance Saturation, Sat). Assuming no aspect overlap, there exists a finite relevance value rmax large
enough such that:
(r (dn , t ) = rmax ) ∧ (r (dn+1 , t ) > 0) =⇒
 


(9)
⃗ dn+1
Q d⃗ ≥ Q d,
Finally, the following constraint captures the relative weight of
aspects w (t ) w.r.t. the user’s information need:
Constraint 10 (Aspect Relevance, AspRel). Aspects with higher
weights have more effect in score of the ranking quality. Formally,
assuming no aspect overlap, and being di and di′ documents that
are relevant to different aspects that have not been observed before,
∀j < i. r (d j , t ) = r (d j , t ′ ) = 0, and r (di , t ) = r (di′, t ′ ) > 0 then:


 
w (t ) > w (t ′ ) =⇒ Q d⃗di ↔di′ > Q d⃗
(10)

Constraint 5 (Confidence, Conf). Adding non-relevant documents decreases the score:


 
⃗ d ¬rel
Q d⃗ > Q d,
(5)

627

Session 5C: New Metrics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

to aspect t is considered:

In summary, we have defined a total of ten constraints: five
relevance-oriented constraints (Pri, Deep, DeepTh, CloseTh and
Conf), and five constraints for search result diversification (AspDiv,
Red, MRed, Sat, and AspRel). The next section provides an axiomatic analysis of the most popular retrieval and diversity metrics
using these constraints.

4

M-IA(d⃗) =

The central part of Table 1 includes the properties for the intentaware version of the metrics discussed before. Intent-aware metrics
converge to the corresponding standard effectiveness metric when
the query has only one aspect. Consequently, they inherit the properties of the original metric over the relevance-oriented constraints
Pri, Deep, DeepTh and CloseTh.
Let us now analyze the diversification-oriented constraints. Besides AP-IA@k, RR-IA and P-IA@k, which are undefined in the
context of graded relevance judgments, the intent-aware metrics
nDCG-IA@k, ERR-IA@k and RBP-IA satisfy the AspDiv constraint.
If a document is relevant for several aspects, then the averaged score
across query aspects increases.
Most of metrics do not satisfy Red and MRed. In the case of
P-IA@k, the precision averaged across aspects in a certain cutoff k
is independent from to which particular aspect the documents are
relevant to.3 RR-IA@k neither satisfies Red given that is sensitive
only to the first relevant document for each query aspect. In the case
of AP-IA@k, the relevance contribution of a document to the aspect
t is higher if relevant documents for t have been observed earlier in
the ranking.4 nDCG-IA@k and RBP-IA also fail to satisfy the Red
constraint. These two metrics are not sensitive to the relevance of
previously observed documents. The contribution of documents
depends on the rank position and the amount of relevant documents
in the collection.
On the other hand, the metric ERR-IA@k satisfies both Red and
Q
MRed, due to the component j <i (1 − r (d j , t )) which estimates
the probability of the user to be satisfied by previously observed
documents according to graded relevance levels.
The Sat constraint is not satisfied by P-IA@k, AP-IA@k, nDCGIA@k nor RBP-IA. The reason is that all these metrics reward new
relevant documents regardless the the gain obtained by previous
observed documents. However, the saturation relevance for RRIA@k and ERR-IA@k is 1. Finally, the AspRel constraint by all
the intent-aware metrics analyzed in this work, given that they all
consider the first relevant document for each aspect in the ranking
and all of them consider aspect weights w (t ).

METRIC ANALYSIS

Standard Metrics for Ad-hoc Retrieval

We analyze here metrics that do not consider multiple aspects of a
query or topic, including: Precision at a cutoff k (P@k), Reciprocal
Rank (RR) [26], Average Precision (AP), Rank-Biased Precision
(RBP) [16], Expected Reciprocal Rank (ERR@k) [6] and Normalized
Discounted Cumulative Gain (nDCG@k) [12].
RBP uses a parameter p that defines user’s patience, modeled
as the probability of the user to inspect the next document in the
ranking. P@k, ERR and nDCG include a cutoff k that limits the
rank positions considered in the evaluation measurement.1 The
upper part of Table 1 summarizes the properties for the retrieval
effectiveness metrics.
The constraints defined by Amigó et al. [3] assume that relevance
judgments are binary. However, our axiomatic framework defines
the constraints Pri and Deep over graded relevance (Eq. 1 and 2,
respectively). Therefore, RR, AP and P@k become undefined.2
The rest of the analysis is inline with the one presented by Amigó
et al. [3]: The other metrics (nDCG@k,ERR@k and RBP) satisfy Pri
and Deep constraints by applying a relevance discounting factor
depending on the depth of the ranking position. With regards to
DeepTh (Eq. 3) and CloseTh (Eq. 4) constraints, metrics that rewards relevance in deep ranking positions such as AP or nDCG@k
satisfy CloseTh but not DeepTh, while metrics that focus on the
top of the ranking (P@k, RR and ERR@k) satisfy DeepTh but not
CloseTh. RBP satisfies both CloseTh and DeepTh. The reason
is that RBP is supported by a probabilistic user behavior model
that takes into account the limitations of the ranking exploration
process (i.e., user’s patience). None of these metrics satisfy Conf.
This family of metrics are not applicable in the context of multiple query aspects. Therefore, they do not satisfy the diversityoriented constraints.

4.2

w (t )Mt (d⃗)

t ∈T

In this section, we firstly analyze standard metrics designed to
evaluate retrieval systems in non-diversified scenarios (i.e., singleaspect). Then we analyze the intent-aware family of metrics, as well
as a number of popular diversity metrics.

4.1

X

4.3

Other Diversity Metrics

Besides the intent-aware metrics (M-IA), other metrics have been
proposed to evaluate the effectiveness of search result diversification systems [19]. Zhai et al. [32] proposed Subtopic Recall (SRecall@k), which measures the number of aspects captured in the
first k positions. Given that the metric only measures the coverage
of aspects, does not satisfy Pri, Deep, CloseTh and Conf relevanceoriented constraints. The only diversity oriented constraint that

Intent-Aware Metrics

The intent-aware scheme [1] extends standard metrics such as AP
or ERR to make them applicable to diversification scenarios. Firstly,
each query aspect is evaluated independently and then a weighted
average considering query aspect weights is computed. Being Mt (d⃗)
the score of d⃗ according to the metric M when only the relevance

3 For instance, being n

i the amount of relevant documents for the aspect t i , the average
P
P
n
P@k across aspects is: |T1 | ti ∈T ki ∝ ti ∈T n i .
4 The contribution of a relevant document in AP is proportional to the precision
achieved at the document’s position, which is higher when relevant documents
appear in the previous positions. For instance, being N r the fixed amount of relevant documents for every aspect in the collection, and being d t , d t′ two documents related with aspect t , and d t ′ a document related with aspect t ′ then:
AP-IA@2(d t , d t′ ) = 1 N1r + 1 N2r > 1 N1r + 21 N1r = AP-IA@2(d t , d t ′ )

1

Due to lack of space, here we focus on the formal properties of the metrics and we
provide references to the definition and explanation of the metrics.
2 Amigó et al. [3]’s analysis shows that P@k does not satisfy the Pri and Deep constraints, given that it does not consider the order of documents before position k .

628

Session 5C: New Metrics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: Properties ( = constraint satisfied, #= constraint not satisfied) of existing retrieval and diversity effectiveness metrics.
Relevance-Oriented Constraints

Diversity-Oriented Constraints

Metric
Pri

Deep

P@k
RR
AP
nDCG@k
ERR@k
RBP

#
#
#

#
#
#

P-IA@k
RR-IA@k
AP-IA
nDCG-IA@k
ERR-IA@k
RBP-IA

#
#

S-Recall@k
S-RR@100%
NRBP
D#-Measure@k
α-nDCG@k
EU
CT@k

#
#

#
#

DeepTh

#
#

#
#

#
#

CloseTh
#
#

#

k P
X

t ∈T

i=1

#
#

MRed

Sat

AspRel

#
#
#
#
#
#

#
#
#
#
#
#

#
#
#
#
#
#

#
#
#
#
#
#

#
#
#
#
#
#

#
#
#

#
#

#
#
#
#
#
#
#
#

#

#
#
#
#

#

#
#

#

#

#
#
#
#
#
#
#

#
#
#
#

#
#
#
#

similarly to α-nDCG@k given that diversification is modeled in
a similar manner. Sakai and Song [18] proposed the D#-Measure
which combines a D-Measure (e.g., D-nDCG [17]) with the ratio of
aspects captured in the first k positions (modeled by S-Recall@k):
D#-Measure@k (d⃗) = λ · S-Recall@k (d⃗) + (1−λ) · D-Measure@k (d⃗)
NRBP inherits the properties from nDCG-IA@k, which already
satisfies DeepTh and AspRel. Therefore, the S-Recall@k component does not contribute with any additional constraint satisfaction.
None of previous metrics satisfy Conf. However, there exist in
the literature utility-oriented metrics that penalyze non-relevant
documents at the end of the ranking. Two examples are the Normalized Discounted Cumulated Utility (nDCU) [30], and the generalized version Expected Utility (EU) [29]. EU is very similar to
α-nDCG@k (d⃗) but includes a cost factor. Being e the estimated
effort for accessing one document, EU can be expressed as:

r (di , t )(1 − α )c (i,t )
log(i + 1)

where c (i, t ) represents the amount of documents previously observed that capture the aspect t. Similarly to the original nDCG,
it satisfies Pri, Deep and CloseTh constraints. However, unlike
nDCG, DeepTh is also satisfied due to the redundancy factor (1 −
α )c (i,t ) , which also allows to satisfy Red. AspDiv is satisfied due
to the additive relevance across aspects. In contrast, α-nDCG@k
does not satisfy the constraints MRed and Sat. The reason is that
the redundancy component (1 − α )c (i,t ) does not consider the relevance grade of previously observed documents. Finally, this metric
does not consider the weight of aspects and therefore AspRel is
not satisfied.
Clarke et al. [8] proposed Novelty- and Rank-Biased Precision
(NRBP), and adaptation of RBP for search result diversification,
defined as:
∞
X
X
NRBP(d⃗) =
p i−1
r (di , t )(1 − α )c (i,t )
i=1

Red

#
#
#
#
#
#

#
#
#
#
#

satisfies is Sat, given that S-Recall@k considers only the first relevant document for each query aspect and it does not consider
aspect weights. Likewise, the metric S-RR@100% – an extension to
RR also proposed by Zhai et al. [32], defined as the inverse of the
rank position on which a complete coverage of aspects is obtained
– satisfies the same properties as S-Recall@k.
Clarke et al. [7] proposed Novelty-Biased Discounted Cumulative
Gain (α-nDCG@k).5 This metric is defined as:
α-nDCG@k (d⃗) =

AspDiv

#
#
#
#
#
#

#

#
#

RBU@k

Conf

|d⃗ |
X

X
*.
r (t )r (di , t )(1 − α )c (i,t ) − e +/
,t ∈ T
⃗
EU inherits the α-nDCG@k (d ) properties, but capturing AspRel
and Conf. However EU does still not satisfy MRed and Sat.
The Cube Test metric (CT@k) [14] satisfies Sat by adding a
saturation factor. Assuming a linear time effort w.r.t. the amount of
inspected documents, CT@k can be expressed as:
EU(d⃗) =

1
1
+
log(i)
i=1

CT@k (d⃗) =

|d⃗ |
X
1 X
r (t )r (di , t )(1 − α )c (i,t ) f Sat
i
i=1
t ∈T

t ∈T

where f Sat is 0 or 1 depending if the sum of relevance of documents
for the aspect exceeds
 a certain saturation level. The reciprocal rank
discounting factor 1i affects the constraint CloseTh, rewarding
the positions of documents over the amount of relevant documents
in top area. In addition, Conf is neither satisfied. There is no contribution or penalty for documents with zero relevance.

Similarly to the original RBP, NRBP satisfies all relevance-oriented
constraints except Conf, given that only relevant documents affect
the score. In terms of diversity-oriented constraints, NRBP behaves
5 Note

that given that the proposed formal constraints and experiments in this work
compare metrics at topic (or query) level, the normalization factor in metrics such as
α -nDCG@k can be ignored.

629

Session 5C: New Metrics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1 also includes the proposed metric Rank-Biased Utility
(RBU), which we describe below.

5

RBU@k matches with the RBP-IA metric when assuming a zero
effort (e = 0), and a small contribution of documents in terms of
gain for query aspects,

RANK-BIASED UTILITY
r (di , t ) ≪ 1 =⇒

The quality of a diversified ranking depends (at least) on the following factors: (i) the position of relevant documents in the ranking;
(ii) the redundancy regarding each of the aspects covered by previously observed documents; (iii) the weights of the aspects seen
in the ranking and (iv) the effort – in terms of user cost or time –
derived from inspecting relevant or non-relevant documents. The
analysis described in Section 4 shows that none of the existing metrics take into account all these factors. To fill this gap, we propose
Ranking-Biased Utility (RBU), which satisfies all the retrieval and
diversity-oriented formal constraints (see proofs in the appendix).
The analysis shows that RBP [16] is the only metric that satisfies
the four first relevance constraints, while ERR-IA@k [1, 6] is the
only metric that satisfies all the five diversity-oriented constraints.
Expected Utility (EU) is the only that satisfies Conf, capturing the
suitability of the ranking cutoff.
In order to satisfy every constraint, RBU combines the user exploration deepness model from RBP with the redundancy modeled
in ERR-IA@k, and also adds the user effort component e in EU to
satisfy the Conf constraint.
The metrics RBP and ERR-IA@k can be combined together under the following user behavior assumptions: (i) The user has a
probability p to explore the next document and (ii) the user has a
probability r (d j , t ) to get gain from document d j for the topic t.
Similarly to the ERR-IA@k, the probability of being satisfied by
document di after observing the documents that occur earlier in
the ranking is:
i−1
Y
r (di , t )
(1 − r (d j , t ))

RBU@k(d⃗) =

p r (di , t )

k
X

pi

i=1

X

w (t )r (di , t )

6

t ∈T

(1 − r (d j , t ))

j=1

Similarly to EU, we define RBP in utility terms in order to capture
Conf. Being e the effort of observing a document, the rank
P biased
k pi e .
accumulated effort is weighted according to p i , that is:
i=1
Finally, combining the relevance contribution with the cumulative effort, we obtain:
RBU@k(d⃗) =

k
X
i=1

pi

EXPERIMENTS

Meta-evaluation: Metric Unanimity

We aim to quantify the ability of metrics to capture diversity in
addition to traditional ranking quality aspects. For this purpose, we
define the Metric Unanimity (MU). MU quantifies to what extent
a metric is sensitive to quality aspects captured by other existing
metrics. It follows a similar concept used by Strictness,8 proposed
by Amigó et al. [3] for the ad-hoc retrieval scenario.
Our intuition is that, if a system improves another system for
every quality criteria, this should be unanimously reflected by every
metric. A metric that captures all quality criteria should reflect these
improvements.
Considering the space of system output pair comparisons (i.e.,
Qd⃗) > Q (d⃗′ )), MU can be formalized as the Point-wise Mutual
Information (PMI) between metric decisions and improvements

(1 − r (d j , t ))

i−1
Y

j <i

We start defining our meta-evaluation metric. Then we evaluate the
metrics in different scenarios based on the TREC Web Track 2014 adhoc retrieval task [9], which includes search result diversification.
Finally, we corroborate our results under the context of the TREC
Dynamic Domain task [28].7

And the cumulative gain across rank positions until k is:
k
X

X

j <i

(1 − r (d j , t ))

i−1
Y

t ∈T

We have introduced RBU@k and shown that the proposed metric satisfies all the relevance- and diversity-oriented formal constraints. The experiments described in the following sections compare RBU@k to other metrics in the context of standard evaluation
campaigns for search result diversification.

j=1

t ∈T

1i

6.1

w (t )r (di , t )

j ≤i

i−1
Y
X
*.w (t )r (d , t )
(1 − r (d j , t )) − 0+/ =
w (t ) ERRt @k (d⃗)
i
i=1
j=1
t
∈
T
t ∈T ,
We now discuss the role of the effort component e, which represents the cost inherently associated to inspect a new document in
the ranking.6 For instance, if e = 0.1 and the inspected document
di has a relevance of 0.1 to aspect ti , then the actual gain is zero:
Y
Y

(1 − 0) − 0.1 = 0
r (di , t )
1 − r (d j , t ) − e = 0.1

In order to satisfy AspRel, the weighted sum of contributions across
aspects in T is:
X

X
 X
w (t )
p i−1r (di , t )1 − 0) =
w (t ) RBPt (d⃗)

On the other hand, RBU@k is equivalent to the metric ERR-IA@k
when the effort component is zero (e = 0), and the probability of
exploring the next document is maximal (p = 1):

j=1

pi

X
t ∈T

j=1

i−1
Y


1 − r (d j , t ) ≃ 1 =⇒

j=1

Analogously to the user model followed by RBP, the resulting contribution of a document di in the position i must be weighted
according to p i :
i

i−1 
Y

6 In

this work, the effort of inspecting or judging a relevant or non-relevant document
is the same. We leave for future work the definition of formal constraints that consider
these differences [21, 24].
7 Releasable data and scripts used in these experiments are available at https://github.
com/jCarrilloDeAlbornoz/RBU. Diversity metrics and RBU are also included in the
EvALL evaluation framework [2] http://evall.uned.es/.
8 Strictness checks to what extent a metric can outscore metrics that achieve a low
score according to other metrics.

i−1 
Y

*.w (t )r (d , t )
1 − r (d j , t ) − e +/ (11)
i
j=1
t ∈T ,
-

X

630

Session 5C: New Metrics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 2: Metric Unanimity scores (MU) for the TREC Web Track 2014 ad-hoc retrieval task: official (Section 6.2) and simulated scenarios
(Section 6.3). Given that normalization has not effect in terms of formal constraints and MU, which work at topic (query) level, normalized
version of metrics behave similarly to the metric without normalization (e.g., MU(nDCG) = MU(DCG)) and therefore are not included.
Simulated Scenarios
r ′ (d ) = rand(0, r (d ))
r ′ (t ) = rand(0, r (t ))
|d⃗| = rand(0, |d⃗|)

Official

RBUe={0.001,0.05,0.1,0.5},p=0.99
α-DCG-IA@1000α ={0.1,0.25,0.5}
DCG@1000
DCG-IA@1000
EUα ={0.1,0.25,0.5},e={0,0.05,0.1,0.5}
ERR-IA@1000
ERR@1000
NRBPp={0.8,0.9,0.99},α ={0.1,0.25,0.5}
AP
AP-IA
RBPp={0.8,0.9,0.99}
P-IA@20
P@20
RR-IA
RR
S-Recall@10
S-Recall@20
S-Recall@50
S-Recall@100

0.8024
0.7956
0.7956
0.7956
0.7956
0.7956
0.7956
0.7956
0.7926
0.7926
0.7911
0.7272
0.7192
0.6835
0.6486
0.3965
0.3538
0.3065
0.2478

RBUe={0.001,0.05,0.1,0.5},p=0.99
α-DCG-IA@1000α ={0.1,0.25,0.5,0.75}
DCG@1000
DCG-IA@1000
EUα ={0.1,0.25,0.5,0.75},e={0,0.001,0.05,0.5}
ERR-IA@1000
ERR@1000
AP
AP-IA
NRBPp={0.8,0.9,0.99},α ={0.1,0.25,0.5,0.75}
RBPp=0.99
P@{20,50}
P-IA@{20,50}
RR-IA
RR
S-Recall@10
S-Recall@20
S-Recall@50
S-Recall@100

∆mi, j ≡ m(d⃗i ) > m(d⃗j )


∆Mi, j ≡ ∀m ∈ M. m(d⃗i ) ≥ m(d⃗j )





P (∆m , ∆Mi, j )
MU M, S (m) = PMI ∆mi, j , ∆Mi, j = log P (∆m i,)j·P (∆M
)

S1
S2
S3

m3

1
0.5
0.2

0.8
0.3
0.4

1
0.2
0.5

0.9808
0.7709
0.7709
0.7709
0.7709
0.7709
0.7687
0.7679
0.7642
0.7627
0.7597
0.7077
0.6888
0.6841
0.6561
0.5137
0.4994
0.4831
0.4831

1 · P (∆M )
i, j +
MU M, S (m rand ) = log * 21
= log(1) = 0
, 2 · P (∆Mi, j ) Property 3. MU is asymmetric. A metric m can be unanimous
regarding the rest of metrics, while the rest of metrics are
not.

i, j

Let us consider the following example illustrated by the Table
below:
m2

RBUe={0.001,0.05,0.1,0.5},p={0.8,0.9,0.99}
α-DCG-IA@{50,100,1000}α ={0.1,0.25,0.5,0.75}
DCG-IA@{50,100,1000}
EUα ={0.1,0.25,0.5,0.75},e={0,0.001,0.05,0.5}
ERR-IA@{50,100,1000}
NRBPp={0.8,0.9,0.99},α ={0.1,0.25,0.5,0.75}
DCG@{50,100,1000}
ERR@{50,100,1000}
AP-IA
AP
RBPp={0.8,0.9,0.99}
P-IA@20
P-IA@10
RR-IA
RR
S-Recall@10
S-Recall@20
S-Recall@100
S-Recall@50

P (∆mi, j , ∆Mi, j )
+ ∝ P (∆mi, j , ∆Mi, j )
MU M, S (m) = log *
1 ·k
,
2
Property 2. A metric m rand which assigns random or constant
scores to every system outputs achieves a zero MU, capturing
the sensitivity of metrics:

Then MU is formalized as:

i, j

0.8568
0.7734
0.7734
0.7734
0.7734
0.7734
0.7734
0.7734
0.7734
0.7734
0.7717
0.7103
0.7103
0.6704
0.6082
0.4238
0.4084
0.3658
0.3007

∆M3,2 . m 1 agrees with M in two cases. Therefore MU M (m 1 ) =
2/6
log 3/6·3/6
= 0.415.
MU has four properties that we describe below.
Property 1. Capturing every unanimous improvement maximizes MU regardless the other decisions:

reported simultaneously by the rest of metrics. Formally, let be
m a metric, M the rest of metrics, and a set of system outputs S.
Being ∆mi, j and ∆Mi, j are statistical variables over system pairs
(d⃗i , d⃗j ) ∈ S 2 , indicating a system improvement according to the
metric and to the rest of metrics, respectively: 9

m1

r ′ (d ) = rand(0, r (d ))
r ′ (t ) = rand(0, r (t ))
|d⃗| = rand(0, 50)

MU {m2,m3 } (m 1 ) , MU {m1,m3 } (m 2 ) , MU {m1,m2 } (m 3 )
Property 4. MU is not affected by the predominance of a certain
family of metrics in the set M:

The example consists of three metrics and three system outputs.
We now compute the MU of the metric m 1 regarding the rest
of metrics M = {m 2 , m 3 }. Here, there are 6 sorted pairs of system outputs: (S 1 , S 2 ),(S 2 , S 1 ), (S 1 , S 3 ), etc. The improvements reported by m 1 are: ∆m11,2 , ∆m 11,3 , and ∆m 12,3 . The improvement reported simultaneously by the other metrics are: ∆M1,2 , ∆M1,3 , and

MU M∪{m ′ }, S (m) = MU M∪{m ′,m ′, ...,m ′ }, S (m)

6.2

Experiment 1: TREC Web Track 2014

This first experiments aims to measure MU in a standard diversification evaluation campaign: the TREC Web Track 2014 ad-hoc
retrieval task [9]. In this benchmark, systems need to perform adhoc retrieval from the ClueWeb-12 collection, for a total of 50 test
topics and return the top 10,000 documents. Some of the topics have

9 The a priori probability of a system improvement for every metric is fixed P (∆m

i, j ) =
That is, for the cases on which two system outputs obtain the same score m (d⃗i ) =
m (d⃗j ) , we add 0.5 to the statistical count.
1
2.

631

Session 5C: New Metrics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

multiple aspects –therefore, diversified rankings may be more effective. We use the 30 official runs submitted to the ad-hoc retrieval
task and available at TREC’s website.
Using our own implementation of the metrics, we execute over
the official runs the following metrics: AP, RR, AP-IA and RR-IA
which do not require any parameter; P@k, ERR@k, NDCG@k and
their corresponding intent-aware variants, using k ∈ {10, 20, 50,
100, 1000}; S-Recall@k, RBP, NRBP and α-nDCG@k; with p ∈
{0.8, 0.9, 0.99} and α ∈ {0.1, 0.25, 0.5, 0.75}; EU and our proposed
metric RBU with the effort parameter e ∈ {0.001, 0.05, 0.1, 0.5}.
For metrics that do not accept multiple query aspects, we consider the maximum relevance across aspects: r (d ) = max t ∈ T (r (d, t )).
The first column in Table 2 shows the metrics ranked by MU.
For the sake of clarity, the table includes for each metric the variant
with highest MU. Results show that metrics that satisfy only a
few constraints such as P@k or S-Recall@k are substantially less
unanimous than the rest of metrics. This means that metrics with
higher scores cover the same quality criteria captured by P@k or
S-Recall@k, but these two metrics do not capture other criteria
captured by the rest of metrics.
Our second observation is that a metric with a shallow cutoff (e.g.,
ERR@50) – i.e., it takes into account a few documents in the ranking
– has lower MU score than its deep counterpart (e.g., ERR@1000).
This behavior is consistent for every metric and variants. Likewise,
higher values for the patience parameter p in RBP obtains higher
MU scores. Intuitively, the shallower the metric is, the less probable
is to capture improvements in deep ranking positions.
RBU obtains the highest scores, when p = 0.99 (i.e., the metric
considers deep positions in the ranking) and all the tested values
for the effort component e.

6.3

single constraint, but satisfying several constraints simultaneously.
Although EU satisfies Conf and ERR-IA@k satisfies MRed and Sat,
RBU outperforms both metrics in terms of MU.
In all the previous experiments, we have seen that MU rewards
the fact of considering deeper positions in the ranking. In order
to isolate this variable, the next simulation (Table 2, third column)
reduces the length of rankings substantially, by defining a random
cutoff between 0 and 50: |d⃗| = rand(0..50). Consequently, metrics
that use a cutoff equal or greater than k = 50 will not be rewarded
by MU. Remarkably, all the RBU variants with an effort parameter
e higher than zero obtain the highest MU scores – RBU with e = 0
(omitted in the table) achieves a 0.7709 MU score.
This suggests that the effort component e plays an important
role when evaluating rankings with different lengths.

6.4

Experiment 3: Considering Metrics and
Default Parameters used in Official
Evaluation

MU scores depend on the set of metrics in consideration. Therefore,
the results could be biased by the selected metric set M and variants.
In order to avoid this bias, we consider the official metrics and
parameters used by the TREC Web Track organizers. In addition, to
avoid the effect of implementation variations or bugs, we compare
RBU (implemented by ourselves) against the official evaluation
scores released by TREC (first column in Table 3).
In this case, AP-IA gets the highest MU score. In terms of RBU,
we can see that p values and MU scores are correlated. This shows
again than MU is biased by the the amount of documents in the
ranking that are visible to the metric. Note that most of metrics
proposed by the organizers use a cutoff no greater than k = 20.
That is, most of metrics receive less information than AP-IA or
NRBP, which take into account all the documents in the ranking.
In order to avoid this effect, we focus on metrics that apply the
the cutoff k = 20, and we apply the same cutoff to RBU: RBU@2010
Maintaining the amount of documents visible to metrics constant,
RBU achieves the same MU score (0.9556) for all the tested variants,
obtaining the highest MU score among the metrics. This suggests
that the RBU performance in terms of MU is not due to differences
in the length of the observed ranking.
The high MU scores of RBU could be possibly due to the fact
of having an explicit component for the user effort (e parameter),
rather than the ability to capture other quality aspects such as diversity and redundancy. In order to isolate this variable, we consider
only three RBU variants with zero value in the effort parameter
(e = 0, p = {0.8, 0.9, 0.99}). Results at the bottom of second column
in Table 3 show that RBU also outperforms the rest of metrics when
e = 0.

Experiment 2: Simulating Alternative
Scenarios

In order to study the behavior of metrics under different situations
and to corroborate our findings, we repeat the experiment described
before after artificially modifying some parameters of the official
TREC Web Track experimental setup.
The second column in Table 2 shows the results when:
(1) Enforcing all relevance judgments to be graded: we replace
each discrete relevance value r by a random value between
zero and r : r ′ (d ) = rand(0..r (d )). This is related to the MRed
constraint.
(2) Randomly assigning a certain weight to each aspect t in such
a way that the sum of the weights for each topic (or query)
P
adds up to 1: w (t ) = rand(0..1) and t ∈ T w ′ (t ) = 1. This is
related to the AspRel constraint.
(3) The ranking of documents returned by each system is manipulated by reducing randomly its length: |d⃗| = rand(0, . . . , |d⃗|).
This variation simulates the situation in which systems should
cut their output rankings according to their confidence of
retrieving (or not) more relevant documents. This tuning is
related to the Conf constraint, which is only satisfied by EU
and the proposed metric.
As a result, the difference in terms of MU scores between RBU
and the other metrics is larger in this simulated scenario. The experiment suggests that this effect is not due to the fact of satisfying any

6.5

Experiment 4: Validation using TREC
Dynamic Domain Track

In order to check the robustness of our empirical conclusions, we
repeat the same experiment over TREC Dynamic Domain 2015 [28],
which includes 23 official runs. This track consists of an interactive
10 In this experiment we use the official evaluation scores. Therefore, we cannot adapt
AP-IA nor NRBP to this cutoff.

632

Session 5C: New Metrics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 3: MU scores over official metrics in TREC Web Track 2014 and TREC Dynamic Domain Track 2015.
TREC Web Track 2014 (Official Metrics)

TREC Dynamic Domain 2015 (Official Metrics)
k = 20

Official

AP-IA
0.9771
RBUe=∗,p=∗
RBUe={0,0.001,0.05,0.1,0.5},p=0.99
0.9770..0.9766 { α-nDCG, α-nDCG }@20
RBUe={0,0.001,0.05,0.1,0.5},p=0.9
0.9763..0.9760 { ERR-IA, nERR-IA }@20
RBUe={0,0.001,0.05,0.1,0.5},p=0.8
0.9760..0.9750 P-IA@20
{ α-DCG, α-nDCG }@20
0.9540
S-Recall@20
ERR-IA@20, nERR-IA@20
0.9539
NRBP, nNRBP
0.9509
{ ERR-IA, nERR-IA, α-DCG, α-nDCG }@10
0.9373
P-IA@20
0.9310
k = 20, e = 0
P-IA@10
{ α-DCG, α-nDCG }@5
{ ERR-IA, nERR-IA }@5
P-IA@5
S-Recall@5
S-Recall@10
S-Recall@20

0.9071
0.9001
0.8999
0.8720
0.5573
0.5001
0.4515

RBUe=0,p={0.8,0.9,0.99}
{ α-DCG, α-nDCG }@20
{ ERR-IA, nERR-IA }@20
P-IA@20
S-Recall@20

RBUe={0.001,0.05,0.1,0.5},p=0.99
RBUe=0.001,p=0.9
RBUe={0.05,0.1},p=0.9
RBUe=0.5,p=0.9
RBUe=0.001,p=0.8
RBUe={0.05,0.1,0.5},p=0.8
ACT@10
ERR (Arith. Mean)
CT@10

0.8488
0.8453
0.8441
0.8440
0.8406
0.8396
0.6276
0.5955
0.5938

0.9556
0.9428
0.9425
0.9081
0.4146

RBUe=0,p={0.8,0.9,0.99}
ERR (Harm. Mean)
P@Recall
P@Recall (modified)
RR@10

0.5937
0.5912
0.1162
0.1044
0.1031

REFERENCES

search scenario. Systems receive aspect-level feedback iteratively
and need to dynamically retrieve as many relevant documents for
aspects as possible, using as few iterations as possible. An important particularity of this task is that the system must predict the
optimal ranking cutoff which is closely related with the Conf constraint. The official metrics used in this track are Cube Test (CT@k)
and Averaged Cube Test (ACT@k) [14], which are included in our
experiments.
The rightmost column in Table 3 shows that we obtain similar
results: all the RBU variants are at the top of the metrics ranking.
In this case, the user effort parameter e is important, given that it is
necessary to outperform other metrics such as CT@k or ACT@k.
In addition, we achieved again the same result when considering
only one RBU variant, appearing at the top in terms of MU scores.

7

Official
0.9556
0.9427
0.9425
0.9080
0.4141

[1] Rakesh Agrawal, Sreenivas Gollapudi, Alan Halverson, and Samuel Ieong. 2009.
Diversifying Search Results. In Proc. WSDM. 5–14.
[2] Enrique Amigó, Jorge Carrillo-de Albornoz, Mario Almagro-Cádiz, Julio Gonzalo,
Javier Rodríguez-Vidal, and Felisa Verdejo. 2017. EvALL: Open Access Evaluation
for Information Access Systems. In Proc. SIGIR. 1301–1304.
[3] Enrique Amigó, Julio Gonzalo, and Felisa Verdejo. 2013. A General Evaluation
Measure for Document Organization Tasks. In Proc. SIGIR. 643–652.
[4] Luca Busin and Stefano Mizzaro. 2013. Axiometrics: An Axiomatic Approach to
Information Retrieval Effectiveness Metrics. In Proc. ICTIR. 8.
[5] Praveen Chandar and Ben Carterette. 2013. Preference Based Evaluation Measures
for Novelty and Diversity. In Proc. SIGIR. 413–422.
[6] Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. 2009. Expected
Reciprocal Rank for Graded Relevance. In Proc. CIKM. 621–630.
[7] Charles L.A. Clarke, Maheedhar Kolla, Gordon V. Cormack, Olga Vechtomova,
Azin Ashkan, Stefan Büttcher, and Ian MacKinnon. 2008. Novelty and Diversity
in Information Retrieval Evaluation. In Proc. SIGIR. 659–666.
[8] Charles L. Clarke, Maheedhar Kolla, and Olga Vechtomova. 2009. An Effectiveness
Measure for Ambiguous and Underspecified Queries. In Proc. ICTIR. 188–199.
[9] Kevyn Collins-Thompson, Craig Macdonald, Paul Bennett, Fernando Diaz, and
Ellen M Voorhees. 2015. TREC 2014 Web Track Overview. In Proc. TREC.
[10] Marco Ferrante, Nicola Ferro, and Maria Maistro. 2015. Towards a Formal Framework for Utility-oriented Measurements of Retrieval Effectiveness. In Proc. ICTIR.
21–30.
[11] Peter B. Golbus, Javed A. Aslam, and Charles L. A. Clarke. 2013. Increasing
Evaluation Sensitivity to Diversity. Inf. Retr. 16, 4 (2013), 530–555.
[12] Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated Gain-based Evaluation
of IR Techniques. ACM Trans. Inf. Sys. 20 (2002), 422–446.
[13] Teerapong Leelanupab, Guido Zuccon, and Joemon M. Jose. 2013. Is IntentAware Expected Reciprocal Rank Sufficient to Evaluate Diversity?. In Proc. ECIR.
738–742.
[14] Jiyun Luo, Christopher Wing, Hui Yang, and Marti Hearst. 2013. The Water
Filling Model and the Cube Test: Multi-dimensional Evaluation for Professional
Search. In Proc. CIKM. 709–714.
[15] Alistair Moffat. 2013. Seven Numeric Properties of Effectiveness Metrics. In Proc.
Asia Info. Retri. Soc. Conf. 1–12.
[16] Alistair Moffat and Justin Zobel. 2008. Rank-Biased Precision for Measurement
of Retrieval Effectiveness. ACM Trans. Inf. Sys. 27, 1 (2008), 2:1–2:27.
[17] Tetsuya Sakai, Nick Craswell, Ruihua Song, Stephen Robertson, Zhicheng Dou,
and Chin yew Lin. 2010. Simple Evaluation Metrics for Diversified Search Results.
In Proc. EVIA. 42–50.
[18] Tetsuya Sakai and Ruihua Song. 2011. Evaluating Diversified Search Results
Using Per-intent Graded Relevance. In Proc. SIGIR. 1043–1052.
[19] Rodrygo L. T. Santos, Craig Macdonald, and Iadh Ounis. 2015. Search Result
Diversification. Found. & Trends in IR 9, 1 (2015), 1–90.
[20] Falk Scholer, Diane Kelly, and Ben Carterette. 2016. Information Retrieval Evaluation Using Test Collections. Inf. Retr. 19, 3 (2016), 225–229.
[21] Mark D. Smucker and Charles L.A. Clarke. 2012. Time-based Calibration of
Effectiveness Measures. In Proc. SIGIR. 95–104.

CONCLUSIONS

We defined an axiomatic framework to analyze diversity metrics
and found that none of the existing metrics satisfy all the constraints. Inspired by this analysis, we proposed Rank-Biased Utility
(RBU, Equation 11), which satisfies all the formal constraints. Our
experiments over standard diversity evaluation campaigns show
that the proposed metric has more unanimity than the official metrics used in the campaigns, i.e., RBU captures more quality criteria
than the ones captured by other metrics. We believe our contributions would help researchers and analysts to define their evaluation
framework (e.g., which evaluation metric should be used?) in order
to analyze the effectiveness of systems in the context of scenarios
involving search result diversification. Future work includes a further parameter sensitivity analysis of metrics, as well as the study
of other meta-evaluation criteria such as sensitivity or robustness
against noise.
Acknowledgments. This research was partially supported by the
Spanish Government (project Vemodalen TIN2015-71785-R) and the
Australian Research Council (project LP150100252). The authors
wish to thank the reviewers for their valuable feedback.

633

Session 5C: New Metrics

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

[22] Karen Sparck Jones and Cornelis J. van Rijsbergen. 1976. Information Retrieval
Test Collections. J. Documentation 32, 1 (1976), 59–75.
[23] Ake Tangsomboon and Teerapong Leelanupab. 2014. Evaluating Diversity and
Redundancy-Based Search Metrics Independently. In Proc. Aust. Doc. Comp. Symp.
42–49.
[24] Andrew Turpin, Falk Scholer, Kalvero Jarvelin, Mingfang Wu, and J. Shane
Culpepper. 2009. Including Summaries in System Evaluation. In Proc. SIGIR.
508–515.
[25] Cornelis J. van Rijsbergen. 1974. Foundation of Evaluation. J. Documentation 30,
4 (1974), 365–373.
[26] Ellen M. Voorhees. 1999. The TREC-8 Question Answering Track Report. In Proc.
TREC. 77–82.
[27] Ellen M. Voorhees and Donna K. Harman. 2005. TREC: Experiment and Evaluation
in Information Retrieval. Vol. 1. MIT Press Cambridge.
[28] Hui Yang, John Frank, and Ian Soboroff. 2015. Overview of the TREC 2015
Dynamic Domain Track. In Proc. TREC.
[29] Yiming Yang and Abhimanyu Lad. 2009. Modeling Expected Utility of Multisession Information Distillation. In Proc. ICTIR. 164–175.
[30] Yiming Yang, Abhimanyu Lad, Ni Lao, Abhay Harpale, Bryan Kisiel, and Monica
Rogati. 2007. Utility-based Information Distillation over Temporally Sequenced
Documents. In Proc. SIGIR. 31–38.
[31] Haitao Yu, Adam Jatowt, Roi Blanco, Hideo Joho, and Joemon M. Jose. 2017. An
In-depth Study on Diversity Evaluation: The Importance of Intrinsic Diversity.
Inf. Proc. & Man. 53 (2017), 799–813.
[32] Cheng Xiang Zhai, William W. Cohen, and John Lafferty. 2003. Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval. In Proc.
SIGIR. 10–17.

Proof. RBU satisfies the AspDiv
constraint (Eq. 6). Under the

 
constraint conditions: RBU d⃗di ↔d ′ > RBU d⃗ is equivalent to:

APPENDIX: FORMAL PROOFS

straint conditions:

i

i −1
Y
X
*.w (t )r (d ′ , t )
*.w (t )r (d , t )
(1 − r (d j , t )) +/ > p i
(i − r (d j , t )) +/ ⇔
i
i
j=1
j=1
t ∈T ,
t ∈T ,
X 
X
X 
X


(w (t )r (d i , t )) ⇐
(r (d i , t )) ⇐
w (t )r (d i′ , t ) >
r (d i′ , t ) >

pi

i −1
Y

X

t ∈T

t ∈T

t ∈T

t ∈T



∀t ∈ T r (d i′ , t ) > r (d i , t )

Proof. RBU satisfies the Red constraint (Eq. 7). Under the constraint
conditions:
⃗ d ′ > RBU d,
⃗ d ⇔
RBU d,








w (t ′ )r (d ′, t ′ )

|d⃗ |
Y

(1 − r (d j , t ′ )) > w (t )r (d, t )

j=1
|d⃗ |
Y

|d⃗ |
Y

(1 − r (d j , t )) ⇔

j=1

(1 − r (d j , t )) >

j=1

|d⃗ |
Y

(1 − r (d j , t )) ⇔

j=1
′

(

)

(

d ∈d⃗ |r (di , t )=r c
d ∈d⃗ |r (d, t )=r c
(1 − r c ) i
> (1 − r c )
(
)
(
)
d i ∈ d⃗ |r (d i , t ) = r c > d ∈ d⃗ |r (d, t ′ ) = r c

)

⇔

Proof. RBU satisfies the MRed constraint (Eq. 8). Under the con⃗ d ′ > RBU d,
⃗ d ⇔
RBU d,


Proof. Rank-Biased Utility (RBU, Eq. 11) satisfies the constraints:
Pri (Eq. 1), Deep (Eq. 2), DeepTh (Eq. 3) and CloseTh (Eq. 4). RBU







|d⃗ |
Y

w (t ′ )r (d ′, t ′ )

is defined as:
RBU@k(d⃗) =

k
X

pi

j=1

i −1
Y

X

*.w (t )r (d )
(1 − r (d j , t )) − e +/
i
j=1
,
In the context of these constraints, it is assumed that there is only a single
aspect t for a given query or topic. Therefore, RBU can be expressed as:
i =1

|d⃗ |
Y

t ∈T

k
X

RBU@k(d⃗) =

p i *.r (d i )
(1 − r (d j , t )) − e +/
j=1
,
In addition, the condition relevance contribution is assumed, i.e., the relevance of single documents does not completely cover the user information
needs r (d ) ≪ 1. Therefore, we can assume that

j=1

i −1
Y

(1 − r (d j , t )) >





n
X

(1 − r (d j , t )) ⇐ ∀d i ∈ d⃗. (r (d i , t ) > r (d i , t ′ ))

n
X
(1 − r (d j , t ′ )) +/ − e
pi +
i =1
j=1
i =1
n−1
Y
X
*.w (t ′ )r (d
′
(1 − r (d j , t ′ )) +/ − ep n+1
n+1 )(1 − r (d n , t ))
j=1
t ′ ∈T ,
-

⃗ d n+1 =
RBU d,

X

pi

*.w (t ′ )r (d )
i

i −1
Y

t ′ ∈T ,

1=1

Given that ∀t ′ , t (r (d n+1, t ′ ) = 0), it is equivalent to:




i −1
n
Y
X
*.w (t ′ )r (d )
(1 − r (d j , t ′ )) +/ − e
pi +
i
i =1
j=1
i =1
t ′ ∈T ,
n−1
Y
*.w (t )r (d
′
(1 − r (d j , t )) +/ − ep n+1
n+1 )(1 − r (d n , t ))
j=1
,
-

⃗ d n+1 =
RBU d,

Finally, the four constraints compare rankings with the same length. This
P
means that we can eliminate the user cost component e, which is e ki=1 p i
for every ranking in comparison. Under all these assumptions, RBU is
equivalent to the traditional RBP metric [16]:
k
X

|d⃗ |
Y
j=1

j=1

RBU@k(d⃗) ∝

(1 − r (d j , t )) ⇔

Proof. RBU satisfies the Sat constraint (Eq. 9). There exists a relevance value r (d n , t ) = rmax = 1 large enough such that:

i −1
Y

(1 − r (d j , t )) ≃

|d⃗ |
Y
j=1

j=1

i =1

i −1
Y

(1 − r (d j , t ′ )) > w (t )r (d, t )

p i r (d i ) = RBP@k(d⃗)

n
X

pi

X

Given that 1 − r (d n , t ′ ) = 0, we obtain:

i =1





⃗ d n+1 =
RBU d,

According to the study by Amigó et al. [3], RBP satisfies the four constraints
enumerated above.

n
X

pi

i =1

i −1
n
Y
X
 
*.w (t ′ )r (d )
(1 − r (d j , t ′ )) +/ − e
p i + 0 = RBU d⃗
i
′
j=1
i
=1
t ∈T ,
-

X

Proof. RBU satisfies the Conf constraint (Eq. 5).
Proof. RBU satisfies the AspRel constraint (Eq. 10).
RBU can be expressed as:
Under the constraint conditions:
RBU@k (d⃗) =

k
X

pi

i =1

i −1
Y

X

k
X



RBU d⃗d ↔d ′
i
i

*.w (t )r (d )
(1 − r (d j , t )) +/ − e
pi
i
j=1
i =1
t ∈T ,
-

w (t ′ )r (d ′, t ′ )

then


 
> RBU d⃗ ⇔

i −1
Y

(1 − r (d j , t ′ )) > w (t )r (d, t )

j=1

⃗ d ¬r el ⇔
RBU@k d⃗ > RBU@k d,
 





w (t ′ )r (d ′, t ′ )

RBU@k (d⃗) > RBU@k (d⃗) − p n+1 e ⇔ 0 > −p n+1 e

i −1
Y
j=1

634

i −1
Y

(1 − r (d j , t )) ⇔

j=1

1 > w (t )r (d, t )

i −1
Y
j=1

1 ⇔ w (t ′ ) > w (t )

