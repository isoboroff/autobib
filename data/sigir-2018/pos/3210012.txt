Session 4A: Fairness & Robustness

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Ranking Robustness Under Adversarial
Document Manipulations
Gregory Goren

Oren Kurland

gregory.goren@campus.techion.ac.il
Technion — Israel Institute of Technology

kurland@ie.technion.ac.il
Technion — Israel Institute of Technology

Moshe Tennenholtz

Fiana Raiber

moshet@ie.technion.ac.il
Technion — Israel Institute of Technology

fiana@oath.com
Yahoo Research

ABSTRACT

should; i.e., black-hat search engine optimization (SEO) [11]. Furthermore, rankings can potentially (rapidly) change due to small
document perturbations that might be indiscernible.
Motivated by the ranking competitions that take place in the
adversarial Web retrieval setting, and the growing body of work
on robust classification — specifically, with respect to adversarial
manipulations [4, 6–10, 14, 20, 21, 25], we present the first (to the
best of our knowledge) theoretical and empirical study of the robustness of document relevance-ranking functions to document
manipulations. Our focus is learning-to-rank-based functions [13]
where a document-query pair is represented as a feature vector.
We start by adapting a basic classifier-robustness notion used in
recent work on robust classification [7, 20] to the case of a document
ranking function. We formally analyze implications of applying
this notion and highlight a notable drawback: the treatment of
documents independently of each other — i.e., this is a pointwise
robustness perspective. However, ranking depends on the relative
retrieval scores of documents. Hence, we formulate a definition of
pairwise robustness that addresses the effect of small document
changes on the relative ranking of pairs of documents. We formally analyze the implications of applying our pairwise robustness
definition and the connections with pointwise robustness.
The different definitions of robustness that we propose are based
on a worst-case scenario; namely, quantifying the minimal document change needed to change a ranking. Using the definitions
to compare the robustness of different ranking functions can be
quite difficult. Thus, we explore an additional aspect of robustness
which we term stability (cf. [20]): changes of retrieval scores and
relative ranking with respect to a given fixed change of a document.
We then establish formal connections for linear (in features) ranking functions between the extent of their regularization and their
stability. Motivated by these formal findings, we state a variance
conjecture: the higher the variance of a learned ranking function, the
less robust the rankings it induces. A ranking is considered robust
if it does not significantly change due to small documents’ changes.
We propose a few methods of measuring ranking robustness.
While our motivation is to address documents’ changes introduced by incentivized authors, our formal analysis makes no assumptions on the cause and nature of documents’ changes. Thus,
the analysis constitutes a general treatment of ranking robustness under document manipulations. Since, in practice, documents’
changes in competitive retrieval settings are often incentivized (adversarial), we use for evaluation a recently published dataset of
document ranking competitions held between students [17]. The

For many queries in the Web retrieval setting there is an on-going
ranking competition: authors manipulate their documents so as
to promote them in rankings. Such competitions can have unwarranted effects not only in terms of retrieval effectiveness, but
also in terms of ranking robustness. A case in point, rankings can
(rapidly) change due to small indiscernible perturbations of documents. While there has been a recent growing interest in analyzing
the robustness of classifiers to adversarial manipulations, there has
not yet been a study of the robustness of relevance-ranking functions. We address this challenge by formally analyzing different
definitions and aspects of the robustness of learning-to-rank-based
ranking functions. For example, we formally show that increased
regularization of linear ranking functions increases ranking robustness. This finding leads us to conjecture that decreased variance of
any ranking function results in increased robustness. We propose
several measures for quantifying ranking robustness and use them
to analyze ranking competitions between documents’ authors. The
empirical findings support our formal analysis and conjecture for
both RankSVM and LambdaMART.
ACM Reference Format:
Gregory Goren, Oren Kurland, Moshe Tennenholtz, and Fiana Raiber. 2018.
Ranking Robustness Under Adversarial Document Manipulations. In SIGIR
’18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8-12, 2018, Ann Arbor, MI, USA. ACM,
New York, NY, USA, 10 pages. https://doi.org/10.1145/3209978.3210012

1

INTRODUCTION

In adversarial retrieval settings (e.g., the Web) there is an on-going
ranking competition for many queries: authors of some Web documents manipulate them so as to have them ranked high. The
ranking competition can have various undesirable effects. First,
ranking effectiveness can degrade due to adversarial changes of
documents that result in having them ranked higher than they
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210012

395

Session 4A: Fairness & Robustness

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

2.1

mere motivation to change documents in the competitions was to
have them ranked high as possible.
The analysis of the ranking competitions provides support to
the formal analysis and the variance conjecture. First, increased
regularization of a linear ranking function (RankSVM [12]), which
leads to reduced variance, results in improved ranking robustness.
Second, increased regularization of a non-linear ranking function,
namely, the state-of-the-art LambdaMART method [27], also results in improved ranking robustness. Third, LambdaMART induces
rankings that are less robust than those induced by RankSVM; the
former has higher variance than the latter.
Our contributions can be summarized as follows:
• We present the first formal and empirical analysis of the robustness of learning-to-rank-based relevance ranking functions to
(adversarial) manipulations of documents.
• We formally demonstrate, and provide empirical support to, the
connection between regularization of linear learning-to-rank
functions and ranking robustness.
• Motivated by our formal findings, we post a conjecture about
the connection between the variance of a ranking function and
ranking robustness, and provide empirical support.

2

RANKING ROBUSTNESS

We assume that the following have been fixed: a query q, a document corpus D, and a document relevance ranking function f . As
is the case for many learning-to-rank-based retrieval methods [13],
the ranking function takes as input a feature vector d⃗ ∈ ℜm that
⃗ is
represents the pair of document d (∈ D) and the query1 q; d[i]
⃗ Features can model query-document relathe i’th component of d.
tions, query-independent properties of documents and documentindependent properties of the query [13]. The output of the ranking
function (wlog) is a retrieval score f (d⃗) ∈ ℜ+ used to induce a
ranking over the corpus. We assume that ties of retrieval scores
are consistently broken (e.g., using documents’ IDs). We use L 2
norms of vectors. For a linear (in features) ranking function f ,
def
⃗ d⃗ + b; w
⃗ is the weight vector and b is the intercept.
f (d⃗) = w
Our goal is to define and quantify notions of the robustness of
ranking functions f to small perturbations of documents. Rather
than directly address documents’ changes (e.g., with respect to
content, hyperlink, hypertext, etc.), we study the effects of the
resultant perturbations of the corresponding feature vectors.
We first motivate our analysis of ranking robustness in Section 2.1. Then, in Section 2.2 we adapt a recently used classifierrobustness definition [6, 20] to ranking functions. The definition
addresses documents independently. Since ranking is determined
by the relative retrieval scores of documents, we study the notion
of pairwise document robustness in Section 2.3; that is, we examine
effects of documents’ perturbations on the relative ranking of two
documents. Given our formal findings in Section 2.3, we post a
conjecture about the connection between ranking robustness and
the variance of a ranking function in Section 2.4. Finally, in Section
2.5 we present a few methods to measure ranking robustness.

1 We

Motivation

Much of the practical motivation for the recent line of work on
robustness of classifiers comes from the vision realm [7, 8, 10, 20, 25].
The assumption is that small perturbations of images not discernible
by humans should not result in changes to classification decisions.
We make a similar assumption about the ranking of documents.
That is, a ranking should not (significantly) change as a result of
small, potentially indiscernible, perturbations of documents2 . A
case in point, users might suspect the validity of rankings given
rapid changes that are hard to explain (a.k.a., “explainable IR”).
To further support our premise, we appeal to the cluster hypothesis which states that “closely associated documents tend to be
relevant to the same requests” [22]. An important operational manifestation of the hypothesis is the premise that “similar documents
should receive similar retrieval scores” [5]. That is, by the cluster
hypothesis, similar documents would be relevant to the same requests (i.e., queries). Therefore, their retrieval scores which reflect
relevance status should not be very different. In other words, small
perturbations of documents should not result in significant changes
of retrieval scores, and hence, significant ranking changes3 .
Accordingly, below we focus on the effects of documents’ perturbations on induced rankings; i.e., ranking robustness. The formal
treatment of the effects on document relevance, and consequently
ranking effectiveness, is an intriguing research venue at its own
right which we leave for future work.

2.2

Pointwise Robustness

The recently adopted notion of classifier robustness was defined
based on the minimal change of an object required to change the
class to which the object is classified [7, 20]. We conceptually adapt
this robustness notion to the case of retrieval scores. We term this
robustness “pointwise” as documents are considered independently
— i.e., the effects of the change of a document retrieval score on its
relative ranking with respect to other documents are not considered.
To simplify the following definitions and analysis, and without loss
of generality, we focus, unless otherwise stated, on the increase of
a retrieval score rather than a change which could be negative.
Definition 1 (Pointwise Robustness). The pointwise robustness of ranking function f with respect to document d and query
⃗ q) de=f min
v || s.t. f (d⃗ + v⃗ ) > f (d⃗). The
q is: ρpoint ( f ; d,
⃗ ∈ℜm ||⃗
v
robustness with respect to q is the expectation over all documents
de f

d (∈ D) sampled using some distribution P(D): ρpoint ( f ; q) =
⃗ q).
Ed∼P( D) ρpoint ( f ; d,

In other words, the pointwise robustness with respect to document d is the minimal extent of a change (i.e., norm of a vector)
needed to be applied to d⃗ so as to increase d’s retrieval score.
We now analyze the pointwise robustness of linear ranking funcde f
⃗ d⃗ + b. RankSVM [12] and coordinate ascent [15]
tions: f (d⃗) = w
are examples of commonly used, effective linear ranking functions.
2 In

contrast to spamming, white-hat SEO attempts to promote documents in rankings
often do not result in a significant drift with respect to the original document [17].
3 The cluster hypothesis was originally stated with respect to textual content of documents [22]. It could be that the document content does not change but its feature-vector
representation does — e.g., due to changes of hyperlinks and anchor text.

omit q from the feature-vector notation as it is fixed throughout the section.

396

Session 4A: Fairness & Robustness

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Proposition 1. The pointwise robustness of a linear ranking function is less or equal to ϵ for any ϵ > 0.

large as it may be, need not necessarily affect ranking. Hence, we
now turn to address the robustness of ranking functions in terms
of the relative ranking of pairs of documents.

Proof. Let d be some document. Then, f (d⃗ + v⃗ ) > f (d⃗) ⇔
⃗ v⃗ > 0. Suppose wlog that w[i]
⃗ > 0. We define a vector v⃗ such that
w
⃗ v⃗ > 0 and
v⃗ [j] = 0 for all j , i and v⃗ [i] = ϵ. Thus, we get that w
⃗ q) ≤ ||⃗
ρpoint ( f ; d,
v || = ϵ. Consequently, ρpoint ( f ; q) ≤ ϵ.
□

2.3

Pairwise Robustness

Definition 3 (Pairwise Robustness). Let d 1 and d 2 be two
documents such that f (d⃗1 ) ≥ f (d⃗2 ). The pairwise robustness of
de f
f with respect to d 1 ,d 2 and the query q is: ρpair ( f ; d⃗1 , d⃗2 , q) =
minv⃗ ∈ℜm ||⃗
v || s.t. f (d⃗2 + v⃗ ) > f (d⃗1 ). The pairwise robustness of
f with respect to the query is the expectation over document pairs:
de f
ρpair ( f ; q) = Ed ∼P( D),d ∼P( D\{d:f (d )>f (d ) }) ρpair ( f ; d⃗1 , d⃗2 , q).

There are various ranking functions whose robustness can be
substantially higher than 0. For example, in gradient boosted regression trees which are the basis of the state-of-the-art LambdaMART
ranking function [27], not every change of a document’s feature
vector necessarily results in a retrieval score change.

1

2.2.1 Pointwise stability. Definition 1 refers to the worst case
scenario per document d. That is, the minimal change to d⃗ required
so as to increase d’s retrieval score. Indeed, as we showed above,
every linear ranking function has, under this definition, robustness
that approaches 0. However, linear functions differ by the extent to
which a retrieval score changes with respect to a given magnitude
||⃗
v || of a document change v⃗ . Therefore, pointwise robustness, as
defined in Definition 1, cannot be used to quantify this specific
aspect of robustness, and we turn to the following definition:

2

1

Pairwise robustness generalizes pointwise robustness: setting
d 1 = d 2 in Definition 3 results in the pointwise robustness definition
from Definition 1. Accordingly, it directly follows that:
Proposition 2. Pairwise robustness entails pointwise robustness
but the reverse does not hold. That is, ρpair ( f ; q) ≥ ρpoint ( f ; q).
In other words, the minimal document change (on average)
needed to change the relative ranking of a document with respect
to another document is higher than the minimal change needed
to have the document’s retrieval score increase. This trivial observation touches on the fundamental difference between treating
documents independently and accounting for their relations.
Specifically, an important difference between pointwise and pairwise robustness is the potential dependence of the latter on distances between documents’ feature vectors. For example, we now
de f
⃗ d⃗ + b, the smallshow that for a linear ranking function f (d⃗) = w
⃗
est change of d 2 required to increase its retrieval score beyond that
of d⃗1 approaches its distance from d⃗1 . Furthermore, we show that
for any linear ranking function, there exists a pair of documents’
feature vectors for which the pairwise robustness of f is not smaller
than the distance between d⃗1 and d⃗2 .

Definition 2 (Pointwise Stability). A ranking function f is
pointwise stable at level K (∈ ℜ+ ) with respect to a document change
v⃗ if ∀d, | f (d⃗ + v⃗ ) − f (d⃗)| ≤ K ||⃗
v ||. The lower K, f becomes more
pointwise stable with respect to v⃗ .4
Contractive (Lipschitz) ranking functions f are pointwise stable
at level K where K is the Lipschitz coefficient. Various ranking
functions are contractive. For example, some neural-network architectures were shown to be contractive [20]5 . Linear ranking
de f
⃗ d⃗ + b we
functions are also contractive. Specifically, for f (d⃗) = w
get, using the Cauchy-Schwarz inequality:
⃗ ||||⃗
| f (d⃗ + v⃗ ) − f (d⃗)| ≤ ||w
v ||.
⃗ ||, and f is pointwise stable at
Hence, the Lipshitz bound K ≤ ||w
⃗ ||. This observation has an important implication. In
level K ≤ ||w
linear ranking functions, such as RankSVM, the loss function used
⃗ || where λ is the
for training is often regularized by adding λ||w
parameter that controls the extent of regularization. Higher value
⃗ || (i.e., stronger regularization) as the
of λ results in decreased ||w
goal is to minimize the objective function (loss+regularization). We
thus arrive to:

Proposition 3. Let f be a linear ranking function. Then, ∀d 1 , d 2
s.t. d 1 , d 2 and f (d⃗1 ) ≥ f (d⃗2 ) and ∀ϵ > 0, ρpair ( f ; d⃗1 , d⃗2 , q) ≤
||d⃗1 − d⃗2 || + ϵ. And, ∃d⃗1 , d⃗2 s.t. ρpair ( f ; d⃗1 , d⃗2 , q) > ||d⃗1 − d⃗2 ||.
⃗ > 0. Fix ϵ > 0 and some d 1 and
Proof. Suppose wlog that w[i]
d 2 (, d 1 ) s.t. f (d⃗1 ) ≥ f (d⃗2 ). Let v⃗ = d⃗1 − d⃗2 + δ⃗ where δ⃗[i] = ϵ
⃗
and δ⃗[j] = 0 for j , i. Then, f (d⃗2 + v⃗ ) − f (d⃗1 ) > 0 iff w[i]ϵ
>
0. Since ρpair ( f ; d 1 , d 2 , q) ≤ ||⃗
v || and by the triangle inequality
||⃗
v || ≤ ||d⃗1 − d⃗2 || +ϵ, we get that ρpair ( f ; d⃗1 , d⃗2 , q) ≤ ||d⃗1 − d⃗2 || +ϵ.
We now turn to prove the second part of the proposition. Let
de f
de f
d⃗1 = −⃗
w and d⃗2 = −2⃗
w; thus, f (d⃗1 ) > f (d⃗2 ). Furthermore, ∀⃗
v:
⃗ > ||w
⃗ || 2 . By the Cauchy-Schwarz
f (d⃗2 + v⃗ ) − f (d⃗1 ) > 0 iff v⃗w
⃗ ≤ ||⃗
⃗ ||. Thus, f (d⃗2 + v⃗ ) − f (d⃗1 ) > 0 ⇒ ||⃗
inequality, v⃗w
v ||||w
v || >
⃗ || = ||d⃗1 − d⃗2 ||. Thus, ρpair ( f ; d⃗1 , d⃗2 , q) > ||d⃗1 − d⃗2 ||.
||w
□

Corollary 1. Ceteris paribus, stronger regularization of linear
ranking functions increases pointwise stability.
⃗ (i.e.,
We note that when using L 2 norms, decreasing the norm of w
increasing regularization) is intended to improve generalization
and prevent overfitting; in other words, stronger regularization
increases the bias of the ranking function and decreases its variance.
We re-visit this point below.
The pointwise analysis treats documents independently of each
other. However, ranking is determined by the relative retrieval
scores of documents. Specifically, a change to a retrieval score, as

Additional question we are interested in is the connection between pointwise stability of retrieval scores and pairwise robustde f
⃗ d⃗ + b.
ness6 , specifically, for linear functions: f (d⃗) = w

a function stable at a level K is also stable at any level > K .
classification architectures were shown to be contractive [20]. It can be shown
that using these architectures for regression (ranking) also yields a contractive function.

4 Obviously,

6 Recall that we showed above that the pointwise robustness of linear ranking functions

5 Some

approaches 0; however, their pointwise stability varies across different functions.

397

Session 4A: Fairness & Robustness

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Proposition 4. Let f be a linear ranking function. There exist
d⃗1 and d⃗2 s.t. ρpair ( f ; d⃗1 , d⃗2 , q) > Kmin where Kmin is the minimal
level at which f is pointwise stable with respect to any v⃗ .

At the technical level, the pair-classification practice we have
taken resulted in backing off from addressing both documents involved to only the one which has changed. Specifically, pairwise
stability at a level K is attained with respect to a given change v⃗ for
all document pairs iff the ranking function is pointwise stable at a
level K with respect to v⃗ ; i.e., the same change introduced to any
document is not likely to result in a rank swap of this document
with another document if the change of the document’s retrieval
score is not large. It is therefore important to highlight an additional difference between the definitions of pairwise robustness
and stability. Robustness is computed as the worst case scenario
for a given pair of documents and is aggregated over all document
pairs via the expectation. Stability level is a constraint imposed on
all document pairs. Accounting for all specific pairwise relations to
derive a bound on stability level is a highly difficult task.
In Section 2.2.1 we showed that for a linear ranking function
⃗ ||||⃗
f , | f (d⃗ + v⃗ ) − f (d⃗)| ≤ ||w
v ||. Since ∆(d⃗1 , d⃗2 ; v⃗ ) = | f (d⃗2 + v⃗ ) −
⃗ ||. Recall
f (d⃗2 )| we get that f is pairwise stable at a level K ≤ ||w
that the stronger the regularization of a linear function trained by
⃗ ||, i.e., the higher the value of λ, the lower
minimizing loss + λ||w
⃗ ||. Hence, for a given small document change (i.e., small ||⃗
||w
v ||)
we get that stronger regularization results in decreased likelihood
of rank swaps. In other words:

⃗ Note that f (d⃗1 ) > f (d⃗2 ). For
Proof. Let d⃗1 = 2⃗
w and d⃗2 = w.
⃗
⃗
⃗
⃗
⃗
⃗
⃗ || 2 . Thus, by the Cauchyany v , f (d 2 + v ) > f (d 1 ) iff wv > ||w
⃗ || 2 < w
⃗ v⃗ ≤ ||w
⃗ ||||⃗
Schwarz inequality, ||w
v ||. Consequently,
⃗ ||. As shown in Section 2.2, for
v || > ||w
ρpair ( f ; d⃗1 , d⃗2 , q) ≥ ||⃗
⃗ ||||⃗
linear ranking functions | f (d⃗ + v⃗ ) − f (d⃗)| ≤ ||w
v || and therefore
⃗ || < ρpair ( f ; d 1 , d 2 , q).
Kmin ≤ ||w
□
Proposition 4 might seem counter intuitive at first glance. That
is, the higher Kmin , which means reduced stability, the higher
ρpair ( f ; d⃗1 , d⃗2 , q) can be — the pairwise robustness for some pair
of documents (or more precisely, their feature vectors). However,
recall that Kmin is determined with respect to all possible v⃗ . Thus,
it suffices that there is some v⃗ whose addition to some d⃗ increases
substantially d’s retrieval score and hence Kmin . The effect on
ρpair ( f ; d⃗1 , d⃗2 , q) can only be an increase or no change.
There are two implications of these observations. First, pointwise
stability and pairwise robustness are, in general, complementary
properties of a ranking function. Second, pointwise stability should
mainly be used to contrast different ranking functions with respect
to the same document change v⃗ .

Corollary 2. Ceteris paribus, stronger regularization of linear
ranking functions results in more robust rankings; that is, the rankings
are less likely to change as a result of changing documents.

2.3.1 Pairwise stability. Definition 3, as was the case for Definition 1, refers to the worst case scenario: the minimal extent of
a change that can be introduced to document d 2 which is ranked
lower than d 1 so as to have it ranked higher than d 1 . However,
this robustness definition does not allow to compare the effect of a
given fixed change on rankings induced by different rankers.
To address this task, we observe the following. Let d 1 and d 2 be
two documents. Their relative ranking is determined by siдn( f (d⃗2 )−
f (d⃗1 )). Thus, to estimate whether the relative ranking of the two
documents changes after d⃗2 was changed by v⃗ , we can examine
 

def 
∆(d⃗1 , d⃗2 ; v⃗ ) = | f (d⃗2 + v⃗ ) − f (d⃗1 ) − f (d⃗2 ) − f (d⃗1 ) |.

Herein we use the terms “ranking robustness” and “ranking
stability” interchangeably so as to refer to changes, or lack thereof,
of a given ranking of a document list with respect to documents’
changes; this is in contrast to the pointwise and pairwise analysis
where we used the terms “robustness” and “stability" to refer to
different notions. Measuring the robustness of a given ranking with
respect to given documents’ changes is a task we address in Section
2.5. Before delving into the details, we briefly discuss simultaneous
changes of two documents. Then, in Section 2.4, we further discuss
the connection between regularization and ranking robustness.

The lower ∆(d⃗1 , d⃗2 ; v⃗ ) the higher the likelihood that the score difference after the document change has the same sign as that before the
change; that is, the higher the likelihood that the relative ranking
would not change. Now, ∆(d⃗1 , d⃗2 ; v⃗ ) = | f (d⃗2 + v⃗ ) − f (d⃗2 )|. Thus,
we are led to the following definition and consequence:

Simultaneous change of two documents. Heretofore, we discussed
the pairwise case with respect to a change, v⃗ , of one of the two
documents involved. Now, suppose that document d 1 is changed
by v⃗1 and document d 2 is simultaneously changed by v⃗2 . Similarly
to the case above, we can examine
 

de f 
∆(d 1 , d 2 ; v⃗1 , v⃗2 ) = | f (d⃗2 + v⃗2 ) − f (d⃗1 + v⃗1 ) − f (d⃗2 ) − f (d⃗1 ) |.

Definition 4 (Pairwise Stability). A ranking function f is
pairwise stable at level K (∈ ℜ+ ) with respect to a document change
v⃗ if ∀d 1 d 2 , ∆(d⃗1 , d⃗2 ; v⃗ ) ≤ K ||⃗
v ||.

We can then adapt Definition 4 (pairwise stability) as follows:

And, we got that f is pairwise stable at level K iff it is pointwise
stable at level K; i.e., ∀d, | f (d⃗ + v⃗ ) − f (d⃗)| ≤ K ||⃗
v ||.
Note that we essentially used f to classify pairs of documents.
This practice is also the basis of RankSVM [12]: a ranking function is learned by using it as a classifier upon pairs of documents.
We argued that classification decisions are stable if the difference
between retrieval scores after a document has changed is close
to the difference before the change. A similar argument was used
in recent work on estimating classifier stability with respect to a
single object that has changed [20].

Definition 5 (Simultaneous Pairwise Stability). A ranking function f is simultaneously pairwise stable at level K (∈ ℜ+ )
with respect to pairwise simultaneous changes v⃗1 and v⃗2 if ∀d 1 d 2 ,
∆(d⃗1 , d⃗2 ; v⃗1 , v⃗2 ) ≤ K (||⃗
v 1 || + ||⃗
v 2 ||).
By the triangle inequality,
∆(d⃗1 , d⃗2 ; v⃗1 , v⃗2 ) ≤ | f (d⃗1 + v⃗1 ) − f (d⃗1 )| + | f (d⃗2 + v⃗2 ) − f (d⃗2 )|.
Thus, if f is pointwise stable with respect to each of v⃗1 and v⃗2 at a
level K, it is simultaneously pairwise stable at level K.

398

Session 4A: Fairness & Robustness

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

de f

⃗ d⃗ + b we get that:
For a linear ranking function f (d⃗) = w
∆(d⃗1 , d⃗2 ; v⃗1 , v⃗2 ) =

KT treats equally swaps between documents at high ranks and
at low ranks. However, the former have higher effect on precisionbased retrieval effectiveness measures (e.g., average precision, p@k,
NDCG) than the latter. Thus, we also consider the inter-list similarity measure RBO (rank-biased overlap) that differentiates swaps
according to their ranks [26]. (We set the free parameter of RBO, p,
to 0.7.) Additional robustness measure we are interested in is “top
change” (TC): the value is 1 if the highest ranked document in L
and L ′ is different, and 0 otherwise.
All three measures just discussed do not consider the amount of
document change. However, changes in ranking due to small (potentially indiscernible) document changes are a stronger evidence
for reduced robustness than those that result from large document
changes. Indeed, our pointwise (Definition 2), pairwise (Definition
4) and simultaneous pairwise (Definition 5) stability definitions capture this notion. Hence, we define normalized measures of ranking
robustness where normalization is with respect to the extent to
which two documents have changed. We normalize KT and TC;
normalizing RBO is more evolved and left for future work.
Let d 1 and d 2 be documents in L that might have changed to
d 1′ and d 2′ in L ′ , respectively. Inspired by Definitions 4 and 5 and
Equation 1, we use the sum, difference and relative functions to
quantify changes to d 1 and d 2 :

(1)

⃗ v⃗2 − w
⃗ v⃗1 | ≤ ||w
⃗ ||||⃗
⃗ ||(||⃗
|w
v 2 − v⃗1 || ≤ ||w
v 1 || + ||⃗
v 2 ||).
⃗ ||.
Thus, the function is simultaneously pairwise stable at level ||w
Hence, we arrive again to the conclusion that stronger regular⃗ ||,
ization of linear ranking functions, which results in reduced ||w
yields rankings of increased robustness.

2.4

The Variance Conjecture

We established above the connection between stronger regularization of linear ranking functions and increased ranking robustness.
Stronger regularization results in decreased variance and increased
bias of the ranking function.
For example, in RankSVM [12], the width of the margin of the sep2 . The lower ||w
⃗ ||, as a result of stronger
arating hyperplane is | |w
⃗ ||
regularization, the wider the margin. This results in higher bias and
lower variance of the learned ranking function. Indeed, larger margin means a more “robust/stable” decision surface, which resonates
with the fact that ranking robustness is higher. Accordingly, we
post the following variance conjecture for any ranking function:
Conjecture 1 (The variance conjecture). The lower the variance of a learned ranking function the more robust the rankings
induced by the function7 .

de f
δsum (d 1 , d 2 ) = ||d⃗2′ − d⃗2 || + ||d⃗1′ − d⃗1 ||,
de f
δdif f (d 1 , d 2 ) = ||d⃗2′ − d⃗2 || − ||d⃗1′ − d⃗1 || ,

In Section 3 we provide some empirical support to the conjecture.
Specifically, we show that reducing the variance of RankSVM, which
is a linear ranker, and that of LambdaMART which is a non-linear
ranker, results in increased ranking robustness.

2.5

de f
δr el (d 1 , d 2 ) = ||(d⃗2′ − d⃗2 ) − (d⃗1′ − d⃗1 )||.
To normalize KT, rather than count the number of discordant
pairs of documents, for each discordant pair (d 1 ,d 2 ) we use the
1
value: δ (d ,d
. We then simply sum the values for all discordant
1 2 )+1
pairs. The resulting distance measures are denoted KT-sum, KTdiff and KT-rel, respectively. Note that the smaller the changes of
two documents which are a discordant pair, the higher the effect
of the swap between them — i.e., the higher the distance and the
lower the robustness. It is also important to note that the values of
these measures are ≥ 0 but are not bound from above. If we were
to use the standard weighted Kendall-τ distance (cf., [19]), then we
1
should have normalized by the sum of δ (d ,d
over all documents
1 2 )+1
pairs. Then, the upper bound would have been 1. However, while
weighted Kendall-τ differentiates the effect of discordant pairs on
the distance between two rankings, it does not allow to effectively
compare distances between different pairs of lists. A case in point,
if a list L contains two documents, then the standard weighted
Kendall’s-τ distance between L and L ′ is equivalent to TC which
does not take into account the amount of documents’ changes that
led to ranking changes.
To normalize TC we do the following. Suppose that d 1 is the
highest ranked document in L and d 2′ (, d 1′ ) is the highest ranked
1
document in L ′ . We attribute the value δ (d ,d
to this change
1 2 )+1
of the highest ranked document. Thus, the more d 1 and d 2 were
changed to become d 1′ and d 2′ , respectively, the less weight we
attribute to this change. The resultant measures are: TC-sum, TCdiff and TC-rel.

Measuring Ranking Robustness

One of our goals is to empirically contrast the robustness of rankings induced by different ranking functions.
Suppose that L is a ranked document list retrieved using some
ranking function. Suppose that some of the authors of documents in
L changed them (e.g., to have the documents ranked higher). After
the changes, the ranking function is used to rank the documents in
L again; the result is a ranked list L ′ . The question is how to quantify
the robustness of L with respect to the changes of documents it
contains given that the resultant (re-)ranked list is L ′ .
We can use any inter-ranking similarity or distance measure
to quantify robustness. To compute these measures for L and L ′ ,
we consider a document d before its change, and after its change,
denoted d ′ , as the same item.
Kendall’s-τ distance (KT in short) is defined as the number of
discordant pairs between two paired lists normalized with respect
to the number of pairs of items in a list; its value is in [0, 1]. A
discordant pair is two items whose relative ranking in one list is
different than that in the other list. The higher the value, the higher
the “distance” between the two lists8 .
7 Low variance corresponds to improved generalization of a ranker to unseen queries.
This is a different notion of robustness than that we focus on here.
8 In contrast to Kendall’s-τ distance, Kendall’s-τ coefficient also considers concordant
pairs and its value is in [−1, 1].

399

Session 4A: Fairness & Robustness

3

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: The dataset used for experiments [17].

EMPIRICAL EXPLORATION

The goal of the exploration we present next is two-fold. First, studying the connection between the regularization of a linear ranking
function and the robustness of the rankings it induces to (adversarial) documents’ manipulations. According to the formal analysis
presented in Section 2.3, the stronger the regularization, the more
robust the rankings should be. The second goal is to empirically
study Conjecture 1. That is, we explore the connection between the
variance of the learned ranking function and ranking robustness
using two ranking functions.

3.1

Queries
Rounds
Students
Documents

31
8
52
1279

Queries per student
Students per query
Unique documents
Relevant documents

3
5−6
897
1113

those describe above (e.g., penalizing documents whose content was
duplicated), and we focus on the ranking functions of interest. Thus,
while the ranking functions we study were not exactly those used in
the competition, the manipulations of documents were adversarial
as the only incentive was to have them ranked high, and the basic
ranking function employed in the competition, LambdaMART, is
highly effective. In other words, we measure ranking robustness by
examining adversarial manipulations of documents introduced in
response to a strong ranker.
We learned two families of ranking functions: RankSVM and
LambdaMART. As was the case for the LambdaMART ranker used
in the ranking competition, the ClueWeb09 Category B collection,
henceforth ClueWeb, was used to train all ranking functions10 .
Titles of topics 1-200 from TREC 2009-2012 served for queries. We
applied Krovetz stemming and removed stopwords on the INQUERY
list from queries only. The Indri toolkit11 was used for experiments.
To learn the ranking functions, 75% randomly sampled queries
served for training and the remaining 25% for validation of hyperparameter values. Once hyper-parameter values were set, either
using the procedure just described or to some predefined values as
detailed below, all the queries were used to train the final ranking
functions. These were then applied to rank documents in each round
of the competition for each query. When we present evaluation for
ClueWeb, we report the results of using five-fold cross validation.
NDCG@20 served as the optimization criterion in all cases.
The ranking functions we use are trained with, and applied on,
document-query feature vectors composed of 26 content-based
features; 25 of these features are those used in the ranking competition [17]. We generated an additional query-independent document
quality feature as follows. Instead of directly penalizing non-valid
documents as was the case in the competition, we used these annotations to simulate Waterloo’s spam classification scores [3].
Specifically, we used 100 − 20(k + s), where k and s are the number
of keyword-stuffed and spam labels a document received, respectively; the scores for the example relevant documents were set to
100. Since k +s ≤ 5, the simulated scores are in [0, 100] as is the case
for Waterloo’s scores12 . Thus, while the learned ranking function is
based on Waterloo’s scores, for the competition dataset it is applied
with the human-annotation-based score we created which serves
as a document quality measure.

Experimental Setup

3.1.1 Dataset. To study the effect of adversarial document manipulations on ranking robustness, we used a recently published
dataset that was created as a result of an on-going ranking competition [17]. The dataset is available at https://github.com/asrcdataset/.
The competition involved 31 repeated matches that lasted for
8 rounds; each match was with respect to a different query. The
queries are a subset of TREC 2009-2012 topic titles that have a clear
commercial intent and were more likely to stir up the competition.
Students in an information retrieval course served as documents’
authors. In the first round, in addition to the query itself, students
were provided with an example relevant document, and were incentivized by bonus points to the course’s grade to modify their
documents so as to have them ranked as high as possible in the next
round. Thus, the documents’ modifications applied by the students
can be considered as adversarial. There was no incentive, however,
for producing documents relevant to the queries except for some
general encouragement. Indeed, the percentage of relevant documents produced was declining throughout the competition [17].
Starting from the second round, students were presented with the
ranking as well as the content of all documents submitted in the
previous round in the match. To alleviate the task, documents were
plain text and their length was restricted to 150 terms. Students
had no prior knowledge of the ranking function and all data was
anonymized. Statistics of the dataset is provided in Table 1.
The ranking function in all matches throughout the competition was based on LambdaMART [27] applied with a subset of the
content-based features from Microsoft’s learning-to-rank datasets9 .
In addition, several external modifications were made to ensure
high ranking quality. First, each document in the dataset was labeled as valid, keyword stuffed or spam by five Figure Eight (https:
//www.figure-eight.com/) annotators. (All annotations are available as part of the dataset.) Documents with at least four non-valid
labels were penalized in the ranking. Second, documents similar
to the example relevant document were demoted in the ranking.
Third, documents were penalized in rankings if their content was
duplicated from other documents in the previous round.

3.1.3 Outline of Experiments. The goal of the regularizationbased experiments we report is to study the connection between
the regularization of a linear RankSVM and LambdaMART and the
robustness and effectiveness of rankings they induce. An additional

3.1.2 Ranking Functions. The exploration we present uses the
documents created in the ranking competition just described. Rather
than using the original ranking induced for each query in each
round of the competition, we re-rank the documents using the
ranking functions we study. One advantage of this practice is that
we refrain from infusing noise caused by external interventions as

10 The

document collection created in the competition is too small to effectively learn
ranking functions.

11 www.lemurproject.org/indri
12 Waterloo’s

classifier scores represent the percentage of documents in a collection
that are "spammier" than the given document.

9 https://tinyurl.com/rmslr

400

Session 4A: Fairness & Robustness

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

We used RankLib’s16 implementation of LambdaMART. To study
the connection between the regularization of LambdaMART and the
effectiveness and robustness of rankings it induces, we set (#leaves,
#trees) to values in {(5, 150), (10, 160), (15, 170), . . . , (150, 440)}.
To compare the ranking robustness of LambdaMART with that
of RankSVM, we set the number of leaves and trees to values in
{250, 500} and {25, 50}, respectively, using the 75-25 split mentioned
above. These values resulted in high ranking robustness. We now
turn to describe how ranking robustness was evaluated.

experiment is intended to contrast the ranking robustness of the
two rankers. Hence, we now turn to describe the regularization
approaches we have employed for the rankers.
Regularization of RankSVM. The objective function of RankSVM
can be expressed using hinge loss:
X
def 1
q
q q
⃗ c) = ∥w
⃗ ∥2 + c
⃗ (d⃗i t − d⃗j t )l qt t qt );
min L(w;
max(0, 1 − w
d i ,d j
2
⃗
w
i, j,t
q

q

c (> 0) is a regularization parameter; di t and d j t are two documents
q
retrieved for query qt ; l qt t qt = 1 if the relevance label for qt
q
of di t

3.1.4 Evaluating Ranking Robustness. Suppose that at round i
of the competition for query q the set of documents created by the
students is S. Now, we rank S using a ranking function to produce
a ranked list Li . Then, we consider the set of documents created
by the same students in round i + 1 of the competition for query
q. Ranking this set yields the list Li+1 . We can then apply upon Li
and Li+1 each of the ranking-robustness measures we described in
Section 2.5. For each query, we average these values over pairs of
rounds i, i + 1; we then average the resultant values over queries.
Thus, for each ranking function and robustness measure we get a
single ranking robustness value.
Accordingly, we can measure the correlation between the regularization strength of RankSVM (as manifested in different values of
⃗ ||) and ranking robustness; and, between LambdaMART’s regu||w
larization strength (with (#leaves, #trees) serving as a proxy) and its
ranking robustness. As at the above, we use Spearman, Pearson and
Kendall’s-τ to measure correlations. Then, to contrast the ranking
robustness of RankSVM and LambdaMART, we compare the values
of the robustness measures. Statistically significant differences of
retrieval performance and of values assigned by robustness measures are determined using the two-tailed paired t-test at a 95%
confidence level. Recall that the robustness measure value for each
query is the average over pairs of consecutive rounds. These values
are compared for statistical significance over queries.

d i ,d j
q
that of d j t

is higher than
and −1 otherwise. The objective
function can be re-written as follows [16]:
λ
1 X
q
q q
⃗ λ) = ||w
⃗ || 2 +
⃗ (d⃗i t − d⃗j t )l qt t qt );
min L(w;
max(0, 1 − w
d i ,d j
2
n
⃗
w
i, j,t
n is the number of document pairs across queries used for training;
1 . Thus, the lower the value of c, the higher the value of λ, and
c = nλ
⃗ || — i.e., the regularization is stronger. Indeed,
hence the lower ||w
lower c corresponds to higher margin of the separating hyperplane
2 ), and therefore to reduced variance and increased bias.
( | |w
⃗ ||
⃗ || and
One of our goals is to study the correlation between ||w
ranking effectiveness and robustness. To that end, we vary the value
⃗ and study the impact on induced rankings. We
of c for learning w,
set c in these experiments to 189 different values in [0, 10000]13 . We
used SVMr ank 14 to train RankSVM; except for the value of c, all
other parameters were set to default values. We found high positive
⃗ ||. Herein,
correlation between the value of c and the resultant ||w
correlations are measured using Spearman coefficient, which is an
estimate of the monotonic relationship between variables, Pearson correlation, which is an estimate for linear relationship, and
Kendall’s-τ . The values of all three measures are in [−1, 1]: −1 and
1 indicate perfect negative and positive correlation, respectively.
The statistical significance of correlations (with respect to 0) is
determined at a 95% confidence level. The Spearman, Pearson and
⃗ || are 0.951, 0.448 and 0.867,
Kendall correlations between c and ||w
respectively; all the correlations are statistically significant15 .
As noted above, in another experiment we performed, we compared the robustness of rankings induced by RankSVM and LambdaMART. Here we set c to values in {0.001, 0.01, 0.1} using the
75-25 split for queries over ClueWeb described above. Recall that
NDCG@20 was the optimization criterion. The ranking competition did not have enough rounds so as to optimize directly for
robustness and then test it. Hence, we optimized for ranking effectiveness and evaluated the effect on robustness. We hasten to
point out, however, that these values of c resulted in high ranking
robustness. We describe below how we measured robustness.

3.1.5 Evaluating Ranking Effectiveness. All documents in the
competition were judged for relevance via Figure Eight by five
annotators [17]. The relevance judgments are available as part
of the dataset. Here, we consider a document relevant if it was
labeled as such by at least three annotators. To estimate retrieval
effectiveness we use MAP (mean average precision) and NDCG
(normalized discounted cumulative gain) at cutoff 5; this is the
number of students that participated in a competition per query17 .
We also evaluate retrieval effectiveness over ClueWeb. In this
case, MAP was computed for the top 1000 retrieved documents
and NDCG for the top 20; the latter also served as the optimization
metric for training LambdaMART.

3.2

Regularization of LambdaMART. LambdaMART uses gradient
boosted regression trees. The higher the number of trees and leaves,
the lower the regularization, and the higher the variance.

Experimental Results

3.2.1 Ranking Robustness of RankSVM. In Figure 1 we present
the values of the (unnormalized) robustness measures as a function
⃗ || in RankSVM. Recall that high values of KT and TC attest to
of ||w
decreased ranking robustness, as these are measures of inter-list

⃗ | |.
sampling of the value of c results in a very short range of values of | |w
Hence, we sampled from [10x , 9 ·10x ] with steps of size 10x for x ∈ {−4, −3, . . . , 2}.
We sampled from [1000, 10000] with steps of 1000. We further sampled 60 values uniformly from [0, 10000]; in addition, we used values in {1, 40, 80, 120, 160, . . . , 1000}
and {1, 45, 90, 135, 180, . . . , 1125}.
14 https://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html
15 We consider only values of c for which | |w
⃗ | | < 15. See Section 3.2.1 for details.
13 Random

16 https://tinyurl.com/ranklib
17 To

generate graded relevance judgments we followed Raifer at al. [17]: documents
labeled relevant by 3, 4 and 5 annotators were considered marginally relevant, fairly
relevant, and highly relevant, respectively; documents with at most 2 relevant labels
were considered non-relevant.

401

Session 4A: Fairness & Robustness

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

0.74

0.27

0.73

0.26

0.72

0.25

0.71

KT

RBO

0.28

0.24
0.23

Table 3: The correlation between #leaves and #trees in LambdaMART and the values of the robustness measures. ’S’, ’P’
and ’K’ stand for Spearman, Pearson and Kendall, respectively. ‘⋆’: the correlation is statistically significant.

0.70
0.69

0.22

0.68

KT
0.21
0

5

10

15

0.67
0

20

5

10

||w||

15

20

.725⋆

||w||

KT-sum KT-diff KT-rel
.733⋆

S
P .674⋆ .714⋆
K .549⋆ .568⋆

0.50
0.48

.754⋆

.727⋆

RBO

TC

−.600⋆

.506⋆

TC-sum TC-diff TC-rel

.521⋆
.728⋆ .732⋆ −.615⋆ .525⋆ .530⋆
.609⋆ .563⋆ −.453⋆ .366⋆ .366⋆

.507⋆ .547⋆
.568⋆ .577⋆
.379⋆ .398⋆

0.46

TC

0.44
0.42

⃗ ||)
— RankSVM in this case: stronger regularization (decreased ||w
results in increased ranking robustness. This also provides support
⃗ || means
to our variance conjecture (Conjecture 1): increased ||w
higher variance; thus, higher variance is indeed correlated with
decreased ranking robustness for RankSVM.
In comparing the normalized robustness measures in Table 2
(X-sum, X-diff and X-rel measures) with their unnormalized counterparts we see the following. For KT, the normalized versions yield
⃗ ||, although the
minor to moderate decrease of correlation with ||w
correlations remain statistically significant and high. Similarly, TCdiff yields lower correlation than TC. However, TC-sum and TC-rel
yield higher correlations than TC.
Table 2 also shows that in almost all cases, KT-rel yields higher
correlation than KT-sum and KT-diff, and TC-rel yields higher
correlation than TC-sum and TC-diff. Recall from Section 2.5 that
the X-rel measures use for normalization the norm of the difference
between the vector change of one document and that of the other.
This norm is ||⃗
v 2 − v⃗1 || used in the upper bound for simultaneous
pairwise stability of a linear ranking function in Equation 1.
Finally, Table 2 shows that KT and its normalized variants, and
RBO, yield higher correlation than TC and its variants. This is not
a surprise as KT and RBO consider changes in the entire document
list, while TC only considers changes at the highest rank; thus,
TC is a less robust measure of ranking robustness. (These findings
are aligned with the patterns observed in Figure 1.) We also see
that RBO yields lower Pearson and Kendall correlation than KT.
RBO attributes more weight to swaps of documents at high ranks
and KT does not. However, the document lists are short (composed
of 5 documents); hence, the differences in correlations potentially
should not be attributed to different weighting of different ranks.

0.40
0.38
0.36
0

5

10

15

20

||w||

⃗ || in RankSVM.
Figure 1: Robustness per ||w
⃗ || and robustness meaTable 2: The correlation between ||w
sures’ values for RankSVM. ’S’, ’P’ and ’K’ stand for Spearman, Pearson and Kendall, respectively. ‘⋆’: the correlation
is statistically significant.
KT

KT-sum KT-diff KT-rel

S .748⋆ .745⋆
P .889⋆ .872⋆
K .574⋆ .560⋆

RBO

TC

TC-sum TC-diff TC-rel

.678⋆ .737⋆ −.756⋆ .528⋆ .574⋆
.850⋆ .875⋆ −.841⋆ .567⋆ .632⋆
.509⋆ .562⋆ −.558⋆ .374⋆ .413⋆

.475⋆ .591⋆
.483⋆ .656⋆
.336⋆ .427⋆

distance, and high values of RBO attest to increased robustness as
⃗ ||,
this is an inter-list similarity measure. We see that increasing ||w
which means weaker regularization, results in decreased ranking
robustness when measured using KT and RBO. (The differences
with TC are discussed below.) This finding is aligned with our
formal analysis of linear ranking functions in Section 2.
To compute correlations between ranking robustness measures
⃗ ||, we first observe the following in Figure 1. Our sampling
and ||w
of values of c, the regularization parameter in RankSVM, resulted in
⃗ || values somewhat larger than ||w
⃗ || = 15. This
over-sampling of ||w
can substantially affect correlation values. Hence, for the correlation
analysis to follow, and for studying RankSVM’s ranking effective⃗ ||, we do not consider RankSVM functions
ness as a function of ||w
⃗ || ≥ 15. The resulting sample contains 71 RankSVM funcwith ||w
tions. We hasten to point out that this sampling did not change the
polarity of the correlations we found, nor their statistical significance, but only the actual correlation numbers which increased18 .
⃗ || and the values of
Table 2 presents the correlation between ||w
all ranking-robustness measures. As can be seen, there are high,
⃗ || and the values of
statistically significant, correlations between ||w
all robustness measures for all three correlation metrics (Spearman,
Pearson and Kendall)19 . Thus, the findings in Table 2 provide support to our formal analysis with respect to linear ranking functions

3.2.2 Ranking Robustness of LambdaMART. Table 3 shows the
correlations between the number of leaves (#leaves) and trees
(#trees) used to train LambdaMART, which were increased simultaneously, and the resulting values of ranking robustness. This increase corresponds to decreased regularization; i.e., higher variance.
Indeed, Table 3 shows that according to all three correlation metrics
and for all three ranking-robustness measures, robustness decreases
with decreased regularization. This finding provides further support to our variance conjecture (Conjecture 1): ranking functions
with higher variance yield rankings of decreased robustness. Thus,
we attained support for the conjecture for two different rankers:
the first is linear (RankSVM) and the second is not (LambdaMART).
Table 3 also shows that KT yields higher correlation than RBO
which in turn yields higher correlation than TC. These findings
are aligned with those presented above for RankSVM. We also

⃗ | | ≥ 15, the Spearman correlation
example, without removing models with | |w
for KT, RBO and TC is: .33, −.466 and .284, respectively; Pearson correlation is
.806, −.780, and .503, respectively; Kendall’s-τ numbers are: .246, −.343, and .217,
respectively. All correlations are statistically significant.
19 Recall that RBO is an inter-list similarity measure, in contrast to the other two; hence
increasing values of RBO attest to increased robustness.

18 For

402

Session 4A: Fairness & Robustness

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 4: Comparison of RankSVM and LambdaMART in
terms of ranking robustness and effectiveness. The numbers
in parentheses indicate the percentage of queries for which
the robustness number attained for one ranking function is
higher than that attained for the other. Percentages do not
necessarily sum to 100 due to ties and rounding. ‘⋆’ marks
statistically significant differences with LambdaMART. Bonferroni correction was applied for robustness comparisons.

Table 5: Correlation between regularization of RankSVM
⃗ ||) and that of LambdaMART (#leaves and #trees) and re(||w
trieval effectiveness. Competition: Raifer et al.’s dataset [17].
‘⋆’: the correlation is statistically significant.
Spearman Pearson Kendall
Competition
RankSVM
ClueWeb

0.343 (54.8)
0.599 (30.9)
0.613 (26.3)
0.361 (53.5)
0.246 (50.7)
0.213 (51.6)
2.071 (62.7)
1.399 (63.6)
1.204 (60.8)
0.825 (16.5)
0.837 (26.7)

0.264⋆ (27.2)
0.401⋆ (11.1)
0.703⋆ (60.8)
0.219⋆ (28.6)
0.153⋆ (31.3)
0.133⋆ (30.4)
1.505⋆ (31.8)
1.008⋆ (30.9)
0.874⋆ (33.6)
0.846 (22.7)
0.854 (37.5)

Competition
LambdaMART
ClueWeb

−.777⋆
−.787⋆
.220⋆
.259⋆

−.918⋆
−.897⋆
.549⋆
.636⋆

−.559⋆
−.564⋆
.079
.141

MAP
NDCG
MAP
NDCG

−.732⋆
−.726⋆
−.849⋆
−.804⋆

−.732⋆
−.710⋆
−.871⋆
−.812⋆

−.549⋆
−.545⋆
−.692⋆
−.635⋆

0.27
0.32
0.26
0.30

0.25

NDCG

RankSVM

MAP

KT
TC
RBO
TC-diff
TC-rel
TC-sum
KT-diff
KT-rel
KT-sum
MAP
NDCG

LambdaMART

MAP
NDCG
MAP
NDCG

0.24

0.23

0.28

0.26

0.22
0.24
0.21

0

5

10

15

20

||w||

see that the normalized versions of KT and TC (KT-X and TCX) yield increased correlations with respect to the unnormalized
versions (KT and TC). This finding implies here to the importance
of accounting not only for the change of ranking, but also for
the change in documents that drove the change in ranking when
measuring ranking robustness. Finally, we see that for TC, the
normalized version TC-rel yields the highest correlations which is
aligned with the findings above for both TC and KT. However, for
KT here, KT-rel does not dominate KT-sum and KT-diff.

0

5

10

15

20

||w||

⃗ || and retrieval effecFigure 2: The connection between ||w
tiveness (MAP and NDCG) on ClueWeb of RankSVM.
3.2.4 Regularization and Retrieval Effectiveness. Table 5 presents
the correlation between retrieval effectiveness (MAP and NDCG)
⃗ ||) and LambdaMART (#leaves
and regularization for RankSVM (||w
and #trees). As most documents in the ranking-competition dataset
are relevant, we also analyze retrieval performance for ClueWeb
that has a much higher percentage of non-relevant documents.
Table 5 shows that except for the case of RankSVM over ClueWeb,
there is high statistically significant positive correlation between
increased regularization and retrieval effectiveness. (Decreasing
⃗ || and #leaves, #trees amounts to increased regularization.) Fig||w
ure 2, which presents the connection between MAP/NDCG and
⃗ || of RankSVM on ClueWeb, shows a positive trend for ||w
⃗ || ≤ 5
||w
⃗ || ≥ 5. Substantially increasing (deand a negative trend for ||w
⃗ || can result in overfitting (underfitting). Thus, the
creasing) ||w
correlations for RankSVM in Table 5 seem to be quite affected by
a long range of underfitting which results in decreased retrieval
⃗ || then reduces underfitting and imperformance; increasing ||w
proves performance until the ranker overfits. In Figure 3 we see
that, in general, decreased regularization of LambdaMART (i.e.,
higher number of leaves and trees) results in decreased MAP performance over ClueWeb. This finding is aligned with those that
emerge based on Table 5.

3.2.3 Comparing RankSVM and LambdaMART. In Table 4 we
contrast the robustness and effectiveness of rankings induced by
RankSVM and LambdaMART over the ranking-competition dataset.
Recall that RBO measures the similarity between two lists, hence
higher values attest to higher robustness. In contrast, KT and its
normalized variants as well as TC and its normalized variants
measure the differences between two lists; hence, lower values
attest to higher robustness. We see that the ranked lists induced by
RankSVM are (on average) more robust than those induced using
LambdaMART according to all considered ranking-robustness measures; all the differences are statistically significant. Furthermore,
the robustness of lists induced by RankSVM surpasses that of lists
induced by LambdaMART for the majority of queries. (Refer to
the numbers in parentheses.) These findings provide additional
support to our variance conjecture. Since RankSVM is linear and
LambdaMART is not, the variance of RankSVM is in general lower,
and we saw that its ranking robustness is higher.
In the last two lines of Table 4 we compare the retrieval effectiveness of RankSVM and LambdaMART. Although the differences are
not statistically significant, we see that lists induced by RankSVM
are not only more robust but also somewhat more effective compared to LambdaMART on the competition dataset. The high MAP
and NDCG values can be attributed to the fact that most documents
generated by the students were judged to be relevant. (See Table 1.)

4

RELATED WORK

There are notions of robustness of ad hoc retrieval functions different than that we examine here: the performance variance over
queries [23] and the relative performance over queries with respect
to another ranking function [24]. We study changes to rankings
that result from documents’ (adversarial) manipulations.

403

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

0.245

0.245

0.240

0.240

0.235

0.235

0.230

0.230

0.225

0.225

MAP

MAP

Session 4A: Fairness & Robustness

0.220

0.220

0.215

0.215

0.210

0.210

0.205
0.200

0

Acknowledgments We thank the reviewers for their comments.
This work was supported by funding from the European Research
Council (ERC) under the European Union’s Horizon 2020 research
and innovation programme (grant agreement 740435).

REFERENCES

0.205

20

40

60

80

#leaves

100

120

140

160

0.200

0

100

200

300

400

500

[1] 2005–2009. AIRWeb — International Workshop on Adversarial Information Retrieval
on the Web.
[2] Carlos Castillo and Brian D. Davison. 2010. Adversarial Web Search. Foundations
and Trends in Information Retrieval 4, 5 (2010), 377–486.
[3] Gordon V. Cormack, Mark D. Smucker, and Charles L. A. Clarke. 2011. Efficient
and effective spam filtering and re-ranking for large web datasets. Informaltiom
Retrieval Journal 14, 5 (2011), 441–465.
[4] Nilesh Dalvi, Pedro Domingos, Mausam, Sumit Sanghai, and Deepak Verma. 2004.
Adversarial Classification. In Proc. of KDD. 99–108.
[5] Fernando Diaz. 2007. Regularizing query-based retrieval scores. Information
Retrieval 10, 6 (2007), 531–562.
[6] Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. 2015. Analysis of classifiers’
robustness to adversarial perturbations. CoRR abs/1502.02590 (2015).
[7] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. 2016.
Robustness of classifiers: from adversarial to random noise. In Proc. of NIPS.
1624–1632.
[8] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. 2017.
The Robustness of Deep Networks: A Geometrical Perspective. IEEE Signal
Processing Magazine 34, 6 (2017), 50–62.
[9] Amir Globerson and Sam T. Roweis. 2006. Nightmare at test time: robust learning
by feature deletion. In Proc. of ICML. 353–360.
[10] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and
Harnessing Adversarial Examples. In Proc. of ICLR.
[11] Zoltán Gyöngyi and Hector Garcia-Molina. 2005. Web Spam Taxonomy. In Proc.
of AIRWeb 2005. 39–47.
[12] Thorsten Joachims. 2006. Training linear SVMs in linear time. In Proc. of KDD.
217–226.
[13] Tie-Yan Liu. 2011. Learning to Rank for Information Retrieval. Springer. I–XVII,
1–285 pages.
[14] Daniel Lowd and Christopher Meek. 2005. Adversarial Learning. In Proc. of
SIGKDD. 641–647.
[15] Donald Metzler and W. Bruce Croft. 2007. Linear feature-based models for
information retrieval. Information Retrieval 10, 3 (2007), 257–274.
[16] Constantinos Panagiotakopoulos and Petroula Tsampouka. 2013. The Stochastic
Gradient Descent for the Primal L1-SVM Optimization Revisited. In Proc. of ECML
PKDD. 65–80.
[17] Nimrod Raifer, Fiana Raiber, Moshe Tennenholtz, and Oren Kurland. 2017. Information Retrieval Meets Game Theory: The Ranking Competition Between
Documents’ Authors. In Proc. of SIGIR. 465–474.
[18] Stephen E. Robertson. 1977. The Probability Ranking Principle in IR. Journal of
Documentation (1977), 294–304. Reprinted in K. Sparck Jones and P. Willett (eds),
Readings in Information Retrieval, pp. 281–286, 1997.
[19] Grace S.Shieh. 1998. A weighted Kendall’s tau statistic. Statistics & Probability
Letters 39, 1 (1998).
[20] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian J. Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks.
In Proc. of ICLR.
[21] Thomas Tanay and Lewis D. Griffin. 2016. A Boundary Tilting Persepective on
the Phenomenon of Adversarial Examples. CoRR abs/1608.07690 (2016).
[22] C. J. van Rijsbergen. 1979. Information Retrieval (second ed.). Butterworths.
[23] Ellen M. Voorhees. 2004. Overview of the TREC 2004 Robust Retrieval Track. In
Proc. of TREC.
[24] Lidan Wang, Paul N. Bennett, and Kevyn Collins-Thompson. 2012. Robust ranking
models via risk-sensitive optimization. In Proc. of SIGIR. 761–770.
[25] Yizhen Wang, Somesh Jha, and Kamalika Chaudhuri. 2017. Analyzing the Robustness of Nearest Neighbors to Adversarial Examples. CoRR abs/1706.03922
(2017).
[26] William Webber, Alistair Moffat, and Justin Zobel. 2010. A Similarity Measure
for Indefinite Rankings. ACM Transactions on Information Systems 28, 4 (2010),
20:1–20:38.
[27] Qiang Wu, Christopher J. C. Burges, Krysta Marie Svore, and Jianfeng Gao. 2010.
Adapting boosting for information retrieval measures. Information Retrieval 13,
3 (2010), 254–270.
[28] Yun Zhou and Bruce Croft. 2006. Ranking robustness: a novel framework to
predict query performance. In Proc. of CIKM. 567–574.

#trees

Figure 3: The connection between #leaves and #trees used
in LambdaMART and its MAP effectiveness over ClueWeb.

The changes of a ranked document list that result from synthetic
random noise introduced to documents were used to predict query
performance [28]. We address a different setting — adversarial
document changes intended to promote documents in rankings —
and present a different analysis.
Much of the work on adversarial information retrieval focuses
on identifying and addressing different types of spam (e.g., contentbased and hyperlink-based) [1, 2]. In contrast to our work, there
was no formal and empirical analysis of ranking robustness with
respect to (adversarial) documents’ manipulations.
Recent work analyzes the strategies employed by documents’
authors in ranking competitions [17]. In contrast, we analyze the
robustness of ranking functions. We use the datasets of ranking
competitions organized in this work [17] in our empirical analysis.
The probability ranking principle [18] was shown to be suboptimal in competitive retrieval settings, where authors manipulate
their documents so as to have them highly ranked. However, the
robustness of ranking functions was not studied as in our work.
There is a growing body of work on adversarial and robust classification; e.g., [4, 6–10, 14, 20, 21, 25]. The focus is on improving
classifier’s robustness to adversarial (often, minuscule) manipulations of objects and their feature values. In contrast to our work,
the robustness of document ranking functions was not studied;
specifically, the pairwise robustness notions we analyze, which are
a core aspect of ranking robustness, were not studied.
The connection between neural network regularization and
the stability of classification decisions has recently been demonstrated [20]. We demonstrate the connection between regularization of linear ranking functions and stability of retrieval scores, and
more importantly, ranking robustness.

5

CONCLUSIONS AND FUTURE WORK

We presented a formal and empirical analysis of the robustness
of rankings induced by feature-based relevance-ranking functions
to (adversarial) manipulations of documents. We formally showed
that increased regularization of linear ranking functions results in
increased ranking robustness. Accordingly, we conjectured that
increased variance of any learned ranking function results in decreased ranking robustness. We provided empirical support to our
formal findings and the conjecture by analyzing ranking competitions where authors introduced adversarial changes to documents.
We plan to further study and improve the robustness of nonlinear learning-to-rank functions. We also intend to extend the
robustness analysis to sets of queries; e.g., those representing the
same information needs.

404

