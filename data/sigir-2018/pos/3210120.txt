Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Testing the Cluster Hypothesis with Focused and Graded
Relevance Judgments
Eilon Sheetrit

Anna Shtok

Technion — Israel Institute of Technology
seilon@campus.technion.ac.il

Technion — Israel Institute of Technology
annabel@technion.ac.il

Oren Kurland

Igal Shprincis

Technion — Israel Institute of Technology
kurland@ie.technion.ac.il

Microsoft, Herzliya, Israel
igals@microsoft.com

ABSTRACT

can be retrieved (e.g., [1–3, 6, 8, 10, 13]). However, cluster hypothesis tests were not reported for passage (focused) retrieval.
We present novel cluster hypothesis tests that utilize graded
and focused relevance judgments [1, 8]; the latter are markups of
relevant text in relevant documents. We apply the tests for both
documents and passages. We set out to perform a finer analysis
than that enabled by using binary relevance judgments. Our tests
account for the extent to which a text item (passage or document)
is deemed relevant as a whole (i.e., relevance grade), or the amount
of relevant text it contains (i.e., focused relevance judgments).
The proposed tests consider the nearest neighbors of an item
(document or passage) in the similarity space. The motivation to use
a nearest-neighbor-based test, following Voorhees’ cluster hypothesis test which utilizes binary relevance judgments for documents
[23], is two-fold. First, the results of this nearest-neighbor-based test
[23] were shown to be correlated to some extent, for certain retrieval
settings, with the relative effectiveness of using inter-document similarities for retrieval [14, 17]. Second, using nearest-neighbor-based
clusters for document retrieval was shown to be highly effective
with respect to using other clustering techniques [11, 16].
Applying the proposed cluster hypothesis tests over INEX and
TREC datasets reveals the following. The cluster hypothesis holds,
as measured using the tests, not only for documents but also for passages. Furthermore, the higher the fraction of relevant text an item
(passage or document) contains, the more similar it is to other relevant items; specifically, those with a high fraction of relevant text.
Finally, documents marked as highly relevant are more similar to
other relevant documents (specifically, highly relevant documents)
than relevant documents not marked as highly relevant.
In summary, our main contributions are cluster hypothesis tests
that utilize graded and focused relevance judgments, and showing
that the cluster hypothesis holds also for passages.

The cluster hypothesis is a fundamental concept in ad hoc retrieval.
Heretofore, cluster hypothesis tests were applied to documents using binary relevance judgments. We present novel tests that utilize
graded and focused relevance judgments; the latter are markups of
relevant text in relevant documents. Empirical exploration reveals
that the cluster hypothesis holds not only for documents, but also
for passages, as measured by the proposed tests. Furthermore, the
hypothesis holds to a higher extent for highly relevant documents
and for those that contain a high fraction of relevant text.
ACM Reference Format:
Eilon Sheetrit, Anna Shtok, Oren Kurland, and Igal Shprincis. 2018. Testing
the Cluster Hypothesis with Focused and Graded Relevance Judgments.
In SIGIR ’18: The 41st International ACM SIGIR Conference on Research and
Development in Information Retrieval, July 8–12, 2018, Ann Arbor, MI, USA.
ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210120

1

INTRODUCTION

The cluster hypothesis is: “closely associated documents tend to be
relevant to the same requests” [9, 22]. Several cluster hypothesis
tests were proposed [5, 9, 20, 23]. Most of these measure similarities
between relevant documents with respect to their similarities with
non-relevant documents using various similarity estimates [18]1 .
Cluster hypothesis tests operate on documents and utilize binary
relevance judgments2 . A relevant document can contain (much)
non-relevant information which could affect inter-document similarity estimates, and hence, cluster hypothesis tests3 . Furthermore,
to address the drawbacks of retrieving relevant documents with
much non-relevant information, passages (instead of documents)
1 One

of these measures [18] is based on a “reversed” cluster hypothesis: documents
should be deemed similar if they are co-relevant to the same queries [7].
2 An exception is a cluster hypothesis test applied to entities [19].
3 Some work uses passage-based information to induce inter-document similarity
measures so as to address this issue [12, 18].

2

TESTING THE CLUSTER HYPOTHESIS

Our goal is to test the cluster hypothesis for documents and passages, henceforth referred to as items, using, when available, graded
or focused relevance judgments. As in past work [15, 17, 21], we
perform the test on a list of items retrieved for a query. We use
language-model-based retrieval and inter-item similarity estimates.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGIR ’18, July 8–12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00
https://doi.org/10.1145/3209978.3210120

2.1

Document and Passage Retrieval

Let q, д, d, and D denote a query, a passage, a document and a corpus
of documents, respectively. We measure textual similarities using

1173

Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

de f

cross entropy (CE) [24]: Sim(x, y) = exp(−CE(pxM LE (·) || pyDir (·));
pxM LE (·) is the maximum likelihood estimate induced from x; pyDir (·)
is the Dirichlet smoothed language model induced from y.
We use L to denote a list of n items (documents or passages)
most highly ranked with respect to q. The document list, Ldoc , is
retrieved using a standard language-model-based approach [24]:
document d in the corpus is scored by Sim(q, d). The passage list,
Lpsд , is created by ranking all passages of documents in Ldoc .
Specifically, passage д in document dд (∈ Ldoc ) is scored by [2, 3]:
(1−λ) Í

For the TREC datasets (GOV2 and ClueWeb) there are graded relevance judgments at the document level, but no passage-level nor
focused relevance judgments. The 0/1/2 graded relevance degrees
we use (see Section 2.2) were set as follows. For GOV2 queries, and
for ClueWeb queries from TREC 2009, we used the given relevance
grades 0/1/2 as relevance degrees. For ClueWeb queries from TREC
2010, 2011 and 2012, additional relevance grades are available: "key"
and "nav". We assign a relevance degree 1 to documents marked
as “nav” (navigation) as the relevance description of “nav” does not
directly translate to "highly relevant". Documents marked as “key”
are assigned a relevance degree 2 following TREC’s description
(https://plg.uwaterloo.ca/~trecweb/2011.html). The motivation for
“folding” the relevance grades of the years 2010–2012 to 0/1/2 relevance degrees is two-fold. There are relatively very few documents
whose relevance labels are “nav” and “key”; hence, using them as is
will bias the cluster hypothesis tests. Second, given the differences
of labeling schemes for ClueWeb across the years, using the original
relevance grades as relevance degrees would have required addressing the query set of each year separately. This would have resulted
in sparsifying the data used to perform the cluster hypothesis tests.
Titles of topics were used as (short) queries. (Queries with no
relevant documents in the qrels files were removed.) Krovetz stemming was applied to queries and documents. Stopwords on the
INQUERY list were removed only from queries. The two-tailed
paired permutation test with p ≤ 0.05 was used to determine statistically significant differences of cluster hypothesis test results.
We apply Bonferroni correction for multiple comparisons where
needed. We used non-overlapping fixed-length windows of 50, 150
and 300 terms for passages; using non-overlapping passages was
a requirement in INEX’s focused retrieval tracks [1, 8]. The Indri
toolkit was used for experiments (www.lemurproject.org).
Document and passage retrieval were performed as described
in Section 2.1. For passage retrieval, the passages in the 1000 most
highly ranked documents were ranked. For ClueWeb we removed
from the document ranking documents with a Waterloo’s spam classifier score below 50 [4]. The Dirichlet smoothing parameter, µ, was
set to 1000 [24] for document retrieval and for measuring inter-item
similarities (see Section 2.1); µ was set to values in {500, 1500, 2500}
for passage retrieval. The value of λ for passage retrieval was in
{0.1, 0.2, . . . , 0.9}. To set the values of µ and λ which affect passage
retrieval, we used leave-one-out cross validation; MAiP@1500, the
mean (over 101 standard recall points) interpolated average precision computed for the top-1500 passages, was used to optimize
passage retrieval performance over the train folds following the
suggested practice in INEX’s focused retrieval tracks [1, 8].
We applied the cluster hypothesis tests to the document (Ldoc )
and passage (Lpsд ) lists with the number of nearest neighbors, k,
in {4, 9}, and the number of items in a list, n, in {50, 100, 150, 200}.

Sim(q,dд )
Sim(q,д)
Í
Sim(q,д ′ ) +λ d ′ ∈Ld oc Sim(q,d ′ ) ; λ is a free parameter.

д ′ ∈Lpsд

We apply cluster hypothesis tests either to Ldoc or to Lpsд . The
tests rely on analyzing the nearest-neighbors of an item x in the
list L it is part of: N N (x, L; k) is the set of k items y in L (y , x)
that yield the highest Sim(x, y).

2.2

Nearest-Neighbor-Based Tests

Inspired by Voorhees’ nearest-neighbor cluster hypothesis test [23],
we present a suite of novel tests. The tests account for the relevance
degree of items: their relevance grade (binary or graded relevance
judgments), or the fraction of query-pertaining text they contain
(focused relevance judgments [1, 8]). Thus, the tests help to explore
how items of low and high relevance degrees are situated in the
similarity space with respect to each other.
A test is performed for query q with respect to a retrieved item
list L. As a first step, we define the seed set S (⊂ L); seed items are
those whose relevance degree is at least β which is a free parameter.
For each seed x in S we compute the average relevance degree of
its nearest neighbors in N N (x, L; k). The values attained for the
seeds in S are averaged to yield a test value for query q. We report
the average over queries of a per-query test value.
We use A-B to name a cluster hypothesis test where A is the type
of relevance degree used for selecting seeds and B is the type used to
quantify relevance degrees of the seeds’ neighbors. A and B, which
can differ, can be: (i) Binary. The relevance degree of an item is 0
(not relevant) or 1 (relevant) based on binary relevance judgments;
for seed selection: β = 1. (ii) Focused. The relevance degree of an
item is the fraction of relevant text it contains as measured using
focused relevance judgments; for seed selection: β = 0+ (i.e., a seed
is an item with relevance degree > 0). (iii) Graded. The relevance
degree of an item is its graded relevance judgment: 0 (not relevant),
1 (relevant), 2 (highly relevant); for seed selection: β = 1 or β = 2.
The Binary-Binary cluster hypothesis test, where the test value is
the average number of relevant items among the nearest neighbors
of a relevant item, is Voorhees’ original test which was applied to
documents [23]. All other tests are novel to this study.

3

EXPERIMENTAL SETUP

The datasets used for experiments are specified in Table 1. The INEX
dataset contains English Wikipedia articles which were flattened
by removing all XML markups. It was used for the focused retrieval
tracks in 2009 and 2010 [1, 8] and includes binary document-level,
and focused, relevance judgments. That is, annotators marked every
piece of relevant text in a relevant document. The fraction of text,
measured in characters, in an item (document or passage) that was
marked relevant is the item’s focused relevance degree.

4

EXPERIMENTAL RESULTS

Binary-Binary tests. We first study the cluster hypothesis for
passages. For reference, we report the test results for documents.
We use the INEX dataset, which in contrast to the TREC datasets
includes focused relevance judgments, and therefore allows to assign relevance degrees to passages. For both seed selection and
quantification of the relevance-degree of nearest neighbors, we use

1174

Short Research Papers II

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

Table 3: The effect of β for seed selection on the FocusedBinary and Focused-Focused tests for INEX (n = 50). ’1’, ’2’,
’3’ and ’4’: statistically significant differences with “β = 0+ ”,
“β = .10”, “β = .25” and “β = .50”, respectively.

Table 1: Datasets used for experiments.
Corpus

# of docs

Data

Avg doc. length

Queries

552
930
807

2009001-2009115, 2010001-2010107
701-850
1-200

INEX
2,666,190
2009&2010
GOV2
25,205,179
GOV2
ClueWeb 50,220,423 ClueWeb09 (Category B)

Focused-Binary

Focused-Focused

k β = 0+ β = .10 β = .25 β = .50 β = .75 β = 0+ β = .10 β = .25 β = .50 β = .75
4 .714 .7191 .72812 .73512 .74012 .652 .6561 .66212 .66712 .67112
Psg50
9 .678 .6821 .68712 .69112 .69712
.621 .6241 .62712 .62912 .63412
3
12
12
4 .733 .740 .75512 .76712
.774
.624 .6331 .65012 .66512
3
3
3 .67634
Psg150
12
12
9 .670 .6761 .68812 .69512
.569 .5761 .58712 .59512
3 .7003
3 .5993
12
12
4 .706 .7241 .73612 .75512
.549 .5681 .58712 .60512
3 .77434
3 .62534
Psg300
12
12
9 .653 .6671 .6741 .68612
.502 .5151 .52412 .53612
3 .6963
3 .54434
4 .662 .6951 .7061 .7101 .7171
.369 .3941 .3971 .40713 .4131
Doc
9 .595 .6161 .62612 .6311 .6321
.328 .3411 .3441 .3481 .3501
item

Table 2: Binary-Binary test (“bb”) over INEX for passages of
different lengths and documents. “rand”: the expected test
value if neighbors are randomly selected rather than via
inter-item similarities as in “bb”. “ratio”: the (average over
queries) ratio between “bb” and “rand”. Underline: the best
result in a row. Boldface: the highest ratio per n (list size).
k
4
Psg50
9
4
Psg150
9
4
Psg300
9
4
Doc
9
item

n = 50
bb

.680
.646
.698
.642
.699
.652
.662
.591

rand

.520
.520
.433
.433
.408
.408
.387
.387

n = 100
ratio

1.95
1.67
2.26
1.85
2.10
1.92
2.28
1.90

bb

.690
.656
.710
.664
.680
.637
.625
.563

rand

.457
.457
.377
.377
.340
.340
.306
.306

n = 150
ratio

2.61
2.14
2.57
2.31
2.65
2.43
3.27
2.63

bb

.689
.648
.705
.660
.672
.629
.604
.539

rand

.402
.402
.339
.339
.301
.301
.260
.260

n = 200
ratio

2.74
2.39
2.97
2.67
3.19
2.93
4.20
3.28

bb

.699
.659
.696
.651
.659
.612
.584
.520

rand

.377
.377
.310
.310
.270
.270
.227
.227

ratio

passages. Accordingly, the highest ratio values per list size (n) are
attained for documents. (Refer to the boldfaced numbers.)

2.78
2.49
3.27
2.95
3.69
3.34
5.07
3.93

Focused-Binary and Focused-Focused tests. Table 3 presents
the results of the Focused-Binary and Focused-Focused tests over
INEX. In both tests, an item is selected as a seed if its fraction of
relevant text ≥ β. In the Focused-Binary test, we report the average
number of relevant neighbors of the seed, while in the FocusedFocused test we report the average fraction of relevant text that
the neighbors contain. We set n = 50, and below we study the
effect of varying n. Note that posting the constraint β = 0+ for the
Focused-Binary test results in the Binary-Binary test reported in
Table 2. However, the numbers in Table 3 differ from those in Table
2 as we perform the test only for queries that have in their retrieved
item lists at least one seed with respect to β = .75 and an additional
relevant item. We do not use the ratio values as in Table 2 since
we compare test values only for the same item list; i.e., the type of
items and n are fixed. Hence, the test results are comparable.
Table 3 shows that increasing the seed’s relevance degree (i.e.,
increasing β) increases both the average number of its neighbors
which are relevant (Focused-Binary) and the average fraction of
relevant text they contain (Focused-Focused). Many of the improvements in tests’ results when increasing β are statistically significant.
(Bonferroni correction was applied for multiple comparisons.) Thus,
we see that relevant items with a high fraction of relevant text are
more likely to be similar to other relevant items, specifically those
which also contain a high fraction, than to non-relevant items.
In Figure 1 we present the effect of varying the list size, n, on the
results of the Focused-Focused test for k = 4. (The results for k = 9
are not presented as they do not convey additional insight.) As can
be seen, for all item types, and all values of n, the higher β, the
higher the test result. This further supports the conclusion based
on Table 3 about the connection between the fraction of relevant
text items contain and inter-item similarities.
For each graph in Figure 1, curves that correspond to a lower β
depict milder increase or steeper decline in test values as a function
of n, compared to curves that correspond to a higher β. Indeed,
larger lists (higher n) and/or those containing longer items have
lower precision (see the "rand" column in Table 2). For these lists,
the higher the fraction of non-relevant text in an item, the higher
the likelihood it will be more similar to non-relevant items.

the Binary regime so as to follow previous work on testing the
cluster hypothesis for documents [15, 17, 23]. That is, we use the
Binary-Binary test. A passage is considered relevant if it contains at
least one character marked as relevant (cf., [1, 8]). For consistency
across experimental settings, only queries (95 out of 120) for which
all item lists contain at least two relevant items were used.
As a reference comparison to the test result we present the mean,
over queries and seeds per query, precision of the n − 1 items in a
list excluding the seed at hand. This baseline, denoted rand, is an
estimate of the probability of randomly drawing a relevant item
as a neighbor of a seed; hence, this is the expected test result if all
neighbors of a seed were randomly selected. (Thus, the result is the
same for k = 4 and k = 9 neighbors.) Naturally, the higher n, the
lower the rand value. We also report the average ratio over queries
between the Binary-Binary test value and the random baseline: the
higher the ratio, the cluster hypothesis holds to a larger extent.
Using the ratio serves to normalize the test result with respect to
the number of relevant items in the list being analyzed which can
considerably affect test results.
We see in Table 2 that the ratio, for all passage lengths (50, 150,
300), number of nearest-neighbors considered (k) and sizes of the
passage list (n), is substantially larger than 1. The ratio is also
substantially larger than 1 for documents which is aligned with
previous findings [15, 17, 21]. Hence, we conclude that the cluster
hypothesis, as measured using a nearest-neighbor test, holds not
only for documents, but also for passages.
Table 2 also shows that the ratio for passages and documents is
higher for k = 4 neighbors than for k = 9 neighbors. This finding,
which is aligned with those in reports for documents [15, 17], can
be attributed to the fact that the more distant the neighbors are
from a relevant seed, the less likely they are to be relevant.
We also see in Table 2 that for a given number of neighbors, k,
it is often the case that the longer the passage, the higher the ratio
value. Indeed, the likelihood of vocabulary mismatch between two
long relevant passages is lower than that for two short relevant

Graded-Binary and Graded-Graded tests. Table 4 presents the
results of Graded-Binary and Graded-Graded tests performed over

1175

Short Research Papers II

β = 0+

β = .10

SIGIR’18, July 8-12, 2018, Ann Arbor, MI, USA

β = .25

β = .50

that contain a high fraction of relevant text are more similar to
other relevant items (specifically, those with a high fraction of
relevant text) than relevant items with low fraction; and, (iii) documents marked as highly relevant are more similar to other relevant
documents (specifically, highly relevant) than relevant documents
not marked as such. These findings motivate the development of
passage retrieval methods that utilize inter-passage similarities.
Acknowledgments. We thank the reviewers for their comments.
This paper is based upon work supported in part by the German
Research Foundation (DFG) via the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1).

β = .75

Psg50

Psg150

0.71

0.69
0.68
0.67
0.66
0.65
0.64
0.63
0.62
0.61
0.60

0.70
0.69
0.68
0.67
0.66
0.65
50

100

150

200

50

100

n

150

200

n

Psg300

Doc

0.64

0.42
0.41
0.40
0.39
0.38
0.37
0.36
0.35
0.34
0.33
0.32

0.62
0.60
0.58
0.56
0.54
0.52
0.50
50

100

150

200

REFERENCES

50

100

n

150

[1] Paavo Arvola, Shlomo Geva, Jaap Kamps, Ralf Schenkel, Andrew Trotman, and
Johanna Vainio. 2011. Overview of the INEX 2010 ad hoc track. In Comparative
Evaluation of Focused Retrieval. Springer, 1–32.
[2] James P. Callan. 1994. Passage-Level Evidence in Document Retrieval. In Proc. of
SIGIR. 302–301.
[3] David Carmel, Anna Shtok, and Oren Kurland. 2013. Position-based contextualization for passage retrieval. In Proc. of CIKM. 1241–1244.
[4] Gordon V Cormack, Mark D Smucker, and Charles LA Clarke. 2011. Efficient
and effective spam filtering and re-ranking for large web datasets. Information
retrieval 14, 5 (2011), 441–465.
[5] Abdelmoula El-Hamdouchi and Peter Willett. 1987. Techniques for the measurement of clustering tendency in document retrieval systems. Journal of Information
Science 13, 6 (1987), 361–365.
[6] Ronald T. Fernández, David E. Losada, and Leif Azzopardi. 2011. Extending the
language modeling framework for sentence retrieval to include local context.
Information Retrieval 14, 4 (2011), 355–389.
[7] Norbert Fuhr, Marc Lechtenfeld, Benno Stein, and Tim Gollub. 2012. The optimum
clustering framework: implementing the cluster hypothesis. Information Retrieval
15, 2 (2012), 93–115.
[8] Shlomo Geva, Jaap Kamps, Miro Lethonen, Ralf Schenkel, James A Thom, and
Andrew Trotman. 2010. Overview of the INEX 2009 ad hoc track. In Focused
retrieval and evaluation. Springer, 4–25.
[9] Nick Jardine and C. J. van Rijsbergen. 1971. The use of hierarchic clustering in
information retrieval. Information storage and retrieval 7, 5 (1971), 217–240.
[10] Mostafa Keikha, Jae Hyun Park, W Bruce Croft, and Mark Sanderson. 2014.
Retrieving passages and finding answers. In Proc. of ADCS. 81.
[11] Oren Kurland. 2009. Re-ranking search results using language models of queryspecific clusters. Information Retrieval 12, 4 (2009), 437–460.
[12] Sylvain Lamprier, Tassadit Amghar, Bernard Levrat, and Frédéric Saubion. 2008.
Using text segmentation to enhance the cluster hypothesis. In Proc. of AIMSA.
69–82.
[13] Vanessa Murdock and W Bruce Croft. 2005. A translation model for sentence
retrieval. In Proc. of HLT-EMNLP. 684–691.
[14] S.-H. Na, I.-S. Kang, and J.-H. Lee. 2008. Revisit of nearest neighbor test for direct
evaluation of inter-document similarities. In Proc. of ECIR. 674–678.
[15] Fiana Raiber and Oren Kurland. 2012. Exploring the cluster hypothesis, and
cluster-based retrieval, over the Web. In Proc. of CIKM. 2507–2510.
[16] Fiana Raiber and Oren Kurland. 2013. Ranking document clusters using markov
random fields. In Proc. of SIGIR. 333–342.
[17] Fiana Raiber and Oren Kurland. 2014. The correlation between cluster hypothesis
tests and the effectiveness of cluster-based retrieval. In Proc. of SIGIR. 1155–1158.
[18] Fiana Raiber, Oren Kurland, Filip Radlinski, and Milad Shokouhi. 2015. Learning
asymmetric co-relevance. In Proc. of ICTIR. 281–290.
[19] Hadas Raviv, Oren Kurland, and David Carmel. 2013. The cluster hypothesis for
entity oriented search. In Proc. of SIGIR. 841–844.
[20] Mark D Smucker and James Allan. 2009. A new measure of the cluster hypothesis.
In Proc. of ICTIR. 281–288.
[21] Anastasios Tombros, Robert Villa, and C. J. Van Rijsbergen. 2002. The effectiveness of query-specific hierarchic clustering in information retrieval. Information
processing & management 38, 4 (2002), 559–582.
[22] C. J. van Rijsbergen. 1979. Information Retrieval (second ed.). Butterworths.
[23] Ellen M. Voorhees. 1985. The cluster hypothesis revisited. In Proc. of SIGIR.
188–196.
[24] Chengxiang Zhai and John Lafferty. 2001. A study of smoothing methods for
language models applied to ad hoc information retrieval. In Proc. of SIGIR. 334–
342.

200

n

Figure 1: The effect of n on the Focused-Focused test for
INEX with k = 4. Note: graphs are not to the same scale.
Table 4: Graded-Binary and Graded-Graded tests performed
for documents. ’1’ marks statistically significant difference
with respect to “β = 1”.

n
50
100
150
200

k
4
9
4
9
4
9
4
9

Graded-Binary
GOV2
ClueWeb

β =1
.737
.673
.710
.657
.709
.650
.699
.644

β =2
.7711
.7031
.7621
.6911
.7551
.6871
.7521
.6831

β =1
.641
.581
.606
.540
.580
.506
.558
.487

β =2
.656
.598
.631
.561
.597
.525
.585
.512

Graded-Graded
GOV2
ClueWeb

β =1
1.001
.908
.950
.875
.934
.854
.911
.837

β =2
1.1231
.9751
1.1351
.9871
1.1301
.9861
1.1211
.9781

β =1
.952
.859
.881
.784
.829
.723
.798
.697

β =2
1.0171
.8901
.9971
.8581
.9381
.8001
.9301
.7891

document lists for the TREC datasets. These datasets have no focused relevance judgments; therefore, the tests are performed only
for documents. Furthermore, the INEX dataset is not used as it has
no graded relevance judgments. The queries used are those whose
retrieved list contains at least one relevant seed with β = 2 and an
additional relevant document. Since the comparisons we discuss are
for the same list (determined by n), the test results are comparable
and there is no need to use the ratio values as in Table 2.
Table 4 shows that in all cases, increasing the relevance degree
(grade) of the seed document results in its neighborhood containing
more relevant documents (Graded-Binary) with higher relevance
grades (Graded-Graded). These findings are conceptually reminiscent of those presented above for the Focused-Binary and FocusedFocused tests. We also see that the increase in the tests’ values
when moving from β = 1 to β = 2 is always statistically significant
for GOV2; for ClueWeb, statistically significant increase is observed
for the Graded-Graded test.

5

CONCLUSIONS AND FUTURE WORK

We presented novel cluster hypothesis tests that utilize graded and
focused relevance judgments. We found that (i) the cluster hypothesis holds for passages; (ii) relevant items (documents or passages)

1176

