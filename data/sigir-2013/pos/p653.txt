Modeling Term Dependencies with
Quantum Language Models for IR
Alessandro Sordoni
Jian-Yun Nie
sordonia@iro.umontreal.ca nie@iro.umontreal.ca

Yoshua Bengio
bengioy@iro.umontreal.ca

DIRO, Université de Montréal
Montréal, H3C 3J7, Québec

ABSTRACT

problems is to find an effective way of representing and scoring documents based on such dependencies. As pointed out
by Gao et al. [9], dependencies can be handled in two ways.
The first approach is to extend the dimensionality of the
representation space. In early geometrical retrieval models
such as the Vector Space Model (VSM), dependencies arising
from phrases (compound terms) are represented by defining
additional dimensions in the space, i.e. both the phrase and
its component single terms are regarded as representation
features [8, 21, 28]. For example, computer architecture is
considered as disjoint from computer and architecture, which
is a strong modeling assumption, and does not take advantage of the semantic relation that generally exists between
a compound phrase and its component terms.
The second approach is more principled in such that simple terms are kept as representational units and term dependencies are modeled statistically as joint probabilities,
i.e. p(computer, architecture). Proposed dependence models such as n-gram Language Model (LM) for IR [30], biterm
LM [31] or the dependence LM [9] adopt such a representation. However, the gain from integrating dependencies
was smaller than hoped [35] and it came with higher computational costs due to dependency parsing or n-gram models [13, 30], or unsupervised iterative methods for estimating
the joint probability [9].
Recently, non bag-of-words models such Markov random
field (MRF) [19], quasi-synchronous dependence model [24]
and the query hypergraph model [2] have been proposed.
Most of these retrieval models take a log-linear form, which
offers a very flexible way of taking into account term dependencies by integrating different sources of evidence, such as
proximity heuristics and exact matching. However, the LM
is used as a black box to estimate single-term and compoundterm influences separately and then the model combines
them to compute the final score. We believe that, from a
representational point of view, these models have implicitly
made a turn back to the first VSM approach in the sense
that the dependencies are assumed to represent additional
concepts, i.e. atomic units for the purpose of document
and query representation, thus disjoint from the component
terms [2, 3]. This choice indeed allows for flexible scoring
functions. However, the retrieval model boils down to a combination of scores obtained separately from matching single
terms and from matching compound dependencies. This
is the main cause of the weight-normalization problem [9,
11] which is that a dependency may be counted twice, as
a compound and as component terms. In the context of
phrases, Sparck Jones et al. note that “the weight of the

Traditional information retrieval (IR) models use bag-ofwords as the basic representation and assume that some form
of independence holds between terms. Representing term
dependencies and defining a scoring function capable of integrating such additional evidence is theoretically and practically challenging. Recently, Quantum Theory (QT) has
been proposed as a possible, more general framework for IR.
However, only a limited number of investigations have been
made and the potential of QT has not been fully explored
and tested. We develop a new, generalized Language Modeling approach for IR by adopting the probabilistic framework
of QT. In particular, quantum probability could account for
both single and compound terms at once without having to
extend the term space artificially as in previous studies. This
naturally allows us to avoid the weight-normalization problem, which arises in the current practice by mixing scores
from matching compound terms and from matching single
terms. Our model is the first practical application of quantum probability to show significant improvements over a robust bag-of-words baseline and achieves better performance
on a stronger non bag-of-words baseline.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval

Keywords
Density Matrices; Language Modeling; Retrieval Models

1.

INTRODUCTION

The quest for the effective modeling of term dependencies has been of central interest in the information retrieval
(IR) community since the inception of first retrieval models.
However, the gradual shift towards non bag-of-words models is strewn with modeling difficulties. One of the central

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

653

phrase should reflect not the increased odds of relevance implied by its presence as compared to its absence, as a whole
unit, but the increased odds compared to the presence of its
components words” [11]. When integrating the evidence, the
weights for the combination are usually estimated by optimizing a retrieval measure such as mean average precision
(MAP). In this sense, a principled probabilistic interpretation of these models is difficult.
The pioneering work by Van Rijsbergen [33] officially formalized the idea that Quantum Theory (QT) could be seen
as a “formal language that can be used to describe the objects and processes in information retrieval”. The idea of QT
as a framework for manipulating vector spaces and probability is appealing. However, the methods that stem from this
initial intuition provided only limited evidence about the
usefulness and effectiveness of the framework for IR tasks.
For example, Piwowarski et al. [25] test if acceptable performance for ad-hoc tasks could be achieved with a quantum
approach to IR. The authors represent documents as subspaces and queries as density operators. However, both documents and queries representations are estimated through
passage-retrieval like heuristics, i.e. a document is divided
into passages and is associated to a subspace spanned by the
vectors corresponding to document passages [25]. Different
representations for the query density matrix are tested but
none of them led to good retrieval performance. Successively, a number of works took inspiration from quantum
phenomena in order to relax some common assumption in
IR [37, 38]. Zuccon and Azzopardi [38] introduce interference effects into the Probability Ranking Principle (PRP)
in order to rank interdependent documents. Although this
method achieves good results, it does not make principled
use of the quantum probability space and cannot be considered as evidence towards the usefulness of the enlarged
probabilistic space. In general, these methods made heuristic use of the concepts of the theory and no clear probabilistic interpretation can be given.
The intrinsic heuristic flavor in preceding approaches motivated some authors to provide evidence to the hypothesis
that there exists an IR situation in which classical probabilistic IR fails, or it is severely limited, and it is thus necessary to switch to a more general probabilistic theory [16, 17,
34]. Although these works are theoretically grounded and
heavily influenced our general vision of the theory, no clue
is given on how to operationalize such results in real-world
applications.
In this paper, we propose a novel retrieval framework for
modeling term dependencies based on the probabilistic calculus offered by QT. In our model, both single terms and
compound dependencies are mathematically modeled as projectors in a vector space, i.e. elementary events in an enlarged probabilistic space. In particular, a compound dependency is represented as a superposition event which is
a special kind of projector that is neither disjoint from its
component terms, nor a joint event. Documents and queries
are represented as a sequence of projectors associated to a
quantum language model (QLM), encapsulated in a particular matrix. The scoring function is a divergence between
query and document QLMs. We will show that our model
is a generalization of classical unigram LMs. To our knowledge, this work can be seen as the first work to use the quantum probabilistic calculus in order to achieve improvements
over state-of-the-art models.

Our contributions are as follows:
1. We propose a novel application of quantum probability
to IR.
2. Using this approach, we show significant improvements
over a strong baseline bag-of-words model and a strong
non bag-of-words model.
3. We propose a new way of representing dependencies
without artificially extending the term space and without estimating expensive n-gram probabilities.
4. We show how the new representation of the dependency permits to specify how the dependency behaves
with respect to its component terms.
5. In our model, the dependency information is not integrated in the scoring phase, but in the estimation
phase. Hence, our model does not suffer the weightnormalization problem.

2.
2.1

A BROADER VIEW ON PROBABILITY
The Quantum Sample Space

In quantum probability, the probabilistic space is naturally encapsulated in a vector space, specifically a Hilbert
space, noted Hn , but for the sake of simplicity, in this paper we limit ourselves to finite real spaces, noted Rn . We
will be using Dirac’s notation restricted to the real field, for
which a unit vector ~
u ∈ Rn , k~
uk2 = 1 and its transpose ~
u⊤
are respectively written as a ket |ui and a bra hu|. Using
this notation, the projector onto the direction u writes as
|uihu|. The inner product between two vectors writes as
hu|vi. Moreover, we note by |ei i the elements of the standard basis in Rn , i.e. |ei i = (δ1i , . . . , δni )⊤ , where δij = 1 iff
i = j.
Events are no more defined as subsets but as subspaces,
more specifically as projectors onto subspaces [23, 34]. Given
a 1-dimensional subspace spanned by a ket |ui, the projector
onto the unit norm vector |ui, |uihu|, is an elementary event
of the quantum probability space, also called a dyad. A dyad
is always a projector onto a 1-dimensional space. Given the
bijection between subspaces and projectors, it is correct to
state that |ui is itself an elementary event. For example,
if n = 2, the quantum elementary events |e1 i = (1, 0)⊤ ,
|f1 i = ( √12 , √12 )⊤ , can be represented by the following dyads:




1 0
0.5 0.5
|e1 ihe1 | =
, |f1 ihf1 | =
.
(1)
0 0
0.5 0.5
P
Generally, any ket |vi = i υi |ui i is called a superposition
of the {|ui i} where {|u1 i, . . . , |un i} form an orthonormal
basis. In order to see the generalization that is taking place,
one has to consider that in Rn there is an infinite number of
vectors even if the dimension n is finite. Hence, contrary to
the classical case, an infinite number of elementary events
can be defined.

2.2

Density Matrices

A quantum probability measure µ is the generalization of
a classical probability measure such that (i) for every dyad
|uihu|, µ(|uihu|) ∈ [0, 1] and (ii) it reduces to a classical probability
P measure for any orthonormal basis {|u1 i, . . . , |un i},
i.e.
i µ(|ui ihui |) = 1. Gleason’s Theorem [10] states that,

654

0
(a) ρ = ( 0.75
0 0.25 )

0.5
(b) ρ = ( 0.5
0.5 0.5 )

0.5 0.25
(c) ρ = ( 0.25
0.5 )

Figure 1: The ellipses depict the set of points {ρ|ui : |ui ∈ R2 }. The eigenvalues of ρ define how much each ellipse is
stretched along the corresponding eigenvectors. To the left, ρ corresponds to a classical probability distribution. To
the center, ρ is a pure state, thus the ellipse degenerates along the eigenvector corresponding to its unit eigenvalue.
To the right, a general density matrix for which we vary both the eigenvalues and the eigensystem.

Consider a vocabulary of two terms V = {a, b}. A unigram
language model θ~ = (0.75, 0.25) defined on V is represented
by:


1
3
0.75
0
.
ρθ = |ea ihea | + |eb iheb | =
0
0.25
4
4

for any real vector space with dimension greater than 2,
there is a one-to-one correspondence between quantum probability measures µ and density matrices ρ. The form of this
correspondence is given by:
µρ (|vihv|) = tr(ρ|vihv|).

(2)

⊤

A real density matrix is symmetric, ρ = ρ , positive semidefinite, ρ ≥ 0, and of trace 1, tr ρ = 11 . From now on, the set
of n × n real density matrices would be noted S n .
By Gleason’s theorem, a density matrix can be seen as
the proper quantum generalization of a classical probability
distribution. It assigns a quantum probability to each one
of the infinite dyads. For example, the density matrix:


0.5 0.5
ρ=
,
(3)
0.5 0.5

Hence, term projectors are orthogonal, i.e. terms correspond
to disjoint events. For example, the probability of the term
a is computed by tr(ρθ |ea ihea |) = 0.75. As conventional
probability distributions are restricted to the identity eigensystem, they differ in their eigenvalues, which correspond to
diagonal entries. On the contrary, general density matrices
can differ also in the eigensystem. For example, the density
matrix ρ of Eq. 3 has eigenvector |f1 i = ( √12 , √12 )⊤ with
eigenvalue 1 and the eigenvector |f2 i = ( √12 , − √12 )⊤ with
eigenvalue 0. Hence, it can be represented as a one-element
mixture containing the projector ρ = |f1 ihf1 |. When the
mixture weights are concentrated into a single projector, the
corresponding density matrix is called pure state. Otherwise,
it is called mixed state.
When defined over Rn , density matrices can be seen as ellipsoids, i.e. deformations of the unit sphere (Figure 1) [34].
Classical probability distributions, i.e. diagonal density matrices, are ellipsoids stretched along the identity eigensystem. As quantum probability has access to an infinite number of eigensystems, the ellipsoid can be “rotated”, i.e. defined on a different eigensystem. In this work, we will use
this additional feature in order to build a more reliable representation of documents and queries taking into account
more complex information than single terms.

assigns probabilities tr(ρ|e1 ihe1 |) = 0.5 and tr(ρ|f1 ihf1 |) =
1. Hence, the event |f1 ihf1 | is certain and still there is
non-classical uncertainty on |e1 ihe1 |. Only if {|u1 i, . . . , |un i}
form an orthonormal system of Rn can the dyads |ui ihui | be
understood as disjoints events of a classical sample space, i.e.
their probabilities sum to one. The relation that ties |e1 ihe1 |
and |f1 ihf1 | is purely geometrical and cannot be expressed
using set theoretic operations.
Any classical discrete probability distribution can be seen
as a mixture over n elementary P
points, i.e. a parameter θ~ =
(θ1 , . . . , θn ), where θi ≥ 0 and i θi = 1. The density matrix is the straightforward generalization of thisP
idea by considering a mixture over
orthogonal
dyads
ρ
=
i υi |ui ihui |
P
where υi ≥ 0 and
i υi = 1. Given a density matrix ρ,
one can find the components dyads by taking its eigendecomposition and building a dyad for each eigenvector.
We
P
note such decomposition by ρ = RΛR⊤ = n
i=1 λi |ri ihri |,
where |ri i are the eigenvectors and λi their corresponding
eigenvalues. This decomposition always exists for density
matrices [23].
Conventional probability distributions can be represented
by diagonal density matrices. The sample space corresponds
to the standard basis E = {|ei ihei |}n
i=1 . Hence, the density
matrix corresponding to the parameter θ~ above can be repre~ = P θi |ei ihei |.
sented as a mixture over E, i.e. ρθ = diag(θ)
i
1

3.

QUANTUM LANGUAGE MODELS

The approach Quantum Language Modeling (QLM) retains the classical Language Modeling for IR as a special
case. Hereafter, we will present in details the quantum
counterpart of unigram language models. Although it is not
explicitly developed in this paper, we argue that arbitrary
n-gram models could be modeled as well.

3.1

Representation

In classical bag-of-words language models, a document
d is represented by a sequence of i.i.d. term events, i.e.

The trace is equal to the sum of the diagonal terms in a matrix.

655

Wd = {wi : i = 1, . . . , N }, where N is the document length.
Each wi belongs to a sample space V, corresponding to the
vocabulary, of size n. It is assumed that such sequences correspond to a sample from an unknown distribution θ~ over
the vocabulary V, for which we want to gain insight.
A quantum language model assigns quantum probabilities
to arbitrary subsets of the vocabulary. It is parametrized by
an n × n density matrix ρ, ρ ∈ S n , where n is the size of
the vocabulary V. In QLM, a document d is considered as
a sequence of M quantum events associated with a density
matrix ρ:
Pd = {Πi : i = 1, . . . , M },

(4)

where each Πi is a general dyad |uihu| and represents a subset of the vocabulary. Note that the number of dyads M can
be different from N , the total number of terms in the document. The sequence Pd is constructed from the observed
terms Wd : we have to define how to map subsets of terms to
projectors. Separating the observed text from the observed
projectors constitutes the main flexibility of our model. In
what follows, we define a way of mapping single terms and
arbitrary dependencies to quantum elementary events. Formally, we seek to define a mapping m : P(V) → L(Rn ),
where P(V) is the powerset of the vocabulary and L(Rn ) is
the set of dyads on Rn . As an initial assumption, we set
m(∅) = O, where O is the projector onto the zero vector.

3.1.1

Figure 2: The dependency κca is modeled as a projector onto |κca i, i.e. as a superposition event.
|κi. The well-defined dyad |κihκ| is a superposition event.
As we showed in Section 2.2, superposition events are justifiable only in the quantum probabilistic space. They are neither disjoint from their constituents |ewi ihewi | nor do they
solely constitute joint events in the sense of n-grams: here,
the compound dependency is not considered as an additional
entity, as done in previous models [2, 3, 19, 21]. The proposed mapping allows for the representation of relationships
within a group of terms by creating a new quantum event
in the same n-dimensional space.
In addition, superposition events come with a flexible way
in quantifying how much evidence the observation of dependency κ brings to its component terms. This is achieved by
changing the distribution of the σi : if one wants to attempt
a classical interpretation, the σi can be viewed as relative
pseudo-counts, i.e. observing |κihκ| adds fractional occurrence to the events of its component terms |ewi ihewi |. To
our knowledge, until now this feature has been only modeled heuristically, or not modeled at all. In our framework,
it fits nicely in the quantum probabilistic space by specifying how a compound dependency event and its constituent
single terms events are related.
As an example, one could model the compound dependency between computer and architecture, κca = {computer,
architecture},
p
p by the dyad Kca = |κca ihκca |, where |κca i =
2/3|ec i + 1/3|ea i (Figure 2). With respect to the example taken above, the event is represented by the matrix:
√
 2

2
0
3
3
√
1
Kca =  2
(8)
0 .
3
3
0
0 0

Representing Single Terms

In Section 2.2, we showed that unigram sample spaces
can be represented as the set of projectors on the standard
basis E = {|ei ihei |}n
i=1 and unigram language models can
be represented as mixtures over E, i.e. diagonal matrices.
Therefore, a straightforward mapping from single terms to
quantum events is:
m({w}) = |ew ihew |,

(5)

where w ∈ V. This choice associates the occurrence of
each term to a dyad |ew ihew |, and these dyads form an orthonormal basis. Hence, occurrences of single terms are still
represented as disjoint events. Consider n = 3 and V =
{computer, architecture, games}. If Wd = {computer, architecture} and one applies m to each of the terms, the sequence
of corresponding projectors is Pd = {Ecomputer , Earchitecture }
where Ew = |ew ihew |:




1 0 0
0 0 0
Ecomputer = 0 0 0 , Earchitecture = 0 1 0 . (6)
0 0 0
0 0 0
Note that if we decide to observe only single terms, Pd turns
out to be the quantum counterpart of classical observed
terms Wd , i.e. M = N .

3.1.2

The superposition coefficients entail that observing Kca adds
more evidence to |ec ihec | than to |ea ihea |.

3.1.3

Representing Dependencies

In this paper, by dependency, we mean a relationship
linking two or more terms and we represent such an entity abstractly by a subset of the vocabulary, i.e. κ =
{w1 , . . . , wK }. We define the following mapping for an arbitrary dependency κ:
m(κ) = m({w1 , . . . , wK }) = |κihκ|, |κi =

K
X

Choosing When and What to Observe

Once we have defined the mapping m, one must ask three
questions:
1. Which compound dependencies to consider?
2. When does such a compound dependency hold in a
document?
3. When the compound dependency is detected, should
we also consider the projectors for its subsets as observed events?

σi |ewi i, (7)

i=1

where
P 2 the coefficients σi ∈ R must be chosen such that
i σi = 1, in order to ensure the proper normalization of

Regarding the first question, one may (a) use a dictionary of phrases or frequent n-grams, or (b) assume that any

656

w1

w2

w3

w4

w5

w6

w7

w8

Sakamura says he created Tron a computer architecture

Π1

Π2

Π3

Π4

Π5

Π6

Π7

Π8

ESakamura Esays Ehe Ecreated ETron Ea Ecomputer Earchitecture

Π1

Π9

Π2

Π3

Π4

Π5

Π6

ESakamura Esays Ehe Ecreated ETron Ea

Kca

Π7

Kca

Figure 3: Two possible quantum sequences Pdi of an excerpt Wd from a TREC collection. The observation of
computer architecture is associated to a superposition projector Kca = |κca ihκca | while Ew = |ew ihew | are classical
projectors. For Pd2 we observed only the compound while in Pd1 we also added its subsets.
subset of terms that appear in short queries are candidate
compound dependencies to capture. In this paper, we want
to make the approach as independent as possible of any linguistic resource. So the second approach (b) is used. This
will also allow us to make a fair comparison with the previous approaches using the same strategy (such as the MRF
model [19]).
The second question regards whether such selected compound dependencies hold in a given document. In other
words, one has to decide when to add the selected dependency projector into a document sequence Pd . This can be
done for example by assuming that the components terms
in the dependency appear as a bigram in a document, as
biterm or in a unordered window of L terms. Convergent
evidence from different works [1, 12, 14, 18, 31, 36] confirms
that proximity is a strong indicator of dependence. Therefore, in this work we choose to detect a dependency if its
component terms appear in a fixed-window of length L.
The third question regards how to apply the mapping m
and can be more easily understood by a practical example.
Consider a document Wd = {computer, architecture} and
a query Wq = {computer, architecture}. Once the dependency κca = {computer, architecture} has been detected
in the document, i.e. the component terms appear next to
each other, one can further decide:

that work: (1) we give a clear probabilistic status to such
concepts and (2) we do not assume that concepts are atomic
units of information, completely unrelated from each other.
In classical dependence models, single terms and compound
dependencies are scored separately and then the scores are
combined together [2, 19, 35]. A critical aspect of such models is that the occurrence of the phrase computer architecture will be counted twice - as single terms and as a compound. That is why the score on compound dependencies
must be reweighed before integrating it with the independence score [9, 11, 19]. Contrary to classical models, our
model does not suffer from such a problem because the evidences brought by the compound dependency as a whole
and by its component terms are integrated in the estimation phase. Even if not reported explicitly in the experiments section, conducted experiments show that including
projectors for both the dependency and its subsets is much
more effective for the ad-hoc task evaluated here and thus
this strategy will be preferred throughout this paper. In
addition, an algorithm building the sequence of projectors
from the document sequence will be presented in Section
4.3.1.

3.2
3.2.1

1. to map only the dependency, i.e. Pd = {Kca },

Estimation
Maximum Likelihood Estimation

Given that a document is represented by a set of observed
projectors, one has to find ways to learn a quantum language model ρ to associate with a document. In QT, a
number of objective functions have been proposed to estimate an unknown density matrix from a set of projectors:
Linear Inversion [23] and Hedged ML [4] are notorious examples. In this work, we use the Maximum Likelihood (ML)
formulation proposed in [15], because (1) it can easily be
seen as a quantum generalization of a classical likelihood
function (2) contrary to linear inversion, ML generates a
well-defined density matrix, i.e. ρ ∈ S n , and (3) proposed
estimation methods remain computationally affordable in
high-dimensional spaces.
Given the observed projectors Pd = {Π1 , . . . , ΠM } for
document d, we define as training criterion for the quantum language model ρ the maximization of the following
product proposed in [15] and corresponding in the unigram
case to a proper likelihood:

2. to map both the dependency and the component terms,
i.e. Pd = {Ecomputer , Earchitecture , Kca }.
These two choices are illustrated in Figure 3. The first choice
is a highly non-classical one because it completely steals the
occurrence of its component terms. Nevertheless, it becomes
a valid choice in our framework. Differently from classical
approaches, the fact that we only consider a count for the
compound computer architecture does not mean that we assume that the terms computer and architecture do not occur.
The dependency event is not disjoint from the single term
events, and its occurrence partially entails the occurrence
of its component terms. However, this choice is more dangerous because it over-penalizes the component terms: we
should know very precisely when such a strong dependency
is observed and which coefficients to assign to it.
The second choice is implicitly done in current dependency
models and is at the basis of the weight-normalization problem. From this point of view, the sequence Pd could be seen
as composed by concepts as recently formalized by Bendersky et al. [2, 3]. However, there are crucial differences from

LPd (ρ) =

657

M
Y

i=1

tr(ρΠi ).

(9)

The estimate ρb can be obtained by approximately solving
the following maximization problem:
maximize

log LPd (ρ)

subject to

ρ ∈ S n.

ρ

in the quantum setting has just started to be the subject
of intensive research [5, 34]. In this work, we propose to
smooth density matrices by linear interpolation [35]. If ρbd is
a document quantum language model obtained by ML, its
smoothed version is obtained by interpolation with the ML
collection quantum language model ρbc :

(10)

This maximization is difficult and must be approximated
by using iterative methods. In [15], the following iterative
scheme is proposed, also called the “RρR algorithm”. One
introduces the operator:
R(ρ) =

M
X
i=1

1
Πi ,
tr(ρΠi )

ρd = (1 − αd ) ρbd + αd ρbc ,

where αd ∈ [0, 1] controls the amount of smoothing. As
the set of density matrices S n is convex, the resulting ρd
is a proper density matrix. In this work, we assume that
µ
, which is the well-known form of the parameter
αd = (µ+M
)
for Dirichlet smoothing [35].

(11)

and updates an initial density matrix ρb(0) by applying repetitive iterations:
1
ρb(k+1) = R(b
ρ(k) )b
ρ(k) R(b
ρ(k) ),
(12)
Z

3.3

−∆V N (ρq kρd )

− tr(ρq (log ρq − log ρd ))
tr(ρq log ρd ),

=
=

rank

(15)

where log applied to a matrix denotes the matrix logarithm,
i.e. the classical logarithm applied to the matrix eigenvalues.
Rank equivalence is obtained by noting that tr(ρq log ρq )
does P
not depend on the particular
document. Denote by
P
ρq = i λqi |qi ihqi |, ρd = i λdi |di ihdi | the eigendecompositions of the density matrices ρq and ρd respectively. By
substituting into the above equation, the scoring function
rewrites as:
X
X
rank
λqi
log λdj hqi |dj i2 .
(16)
−∆V N (ρq ||ρd ) =

(13)

where γ ∈ [0, 1) controls the amount of damping and is optimized by linear search in order to ensure the maximum increase of the training objective2 . As S n is convex [23], ρe(k+1)
is a proper candidate density matrix. The process stops if
the change in the likelihood is below a certain threshold or
if a maximum number of iterations is attained.
From an IR point of view, the metric divergence problem [22] tells us that the maximization of the likelihood does
not mean that the evaluation metric under consideration,
such as mean average precision, is also maximized. In the
experiments section, we address the two following questions
from a perspective closer to IR concerns:

i

j

Compared to a classical KL divergence, the additional
term hqi |dj i2 quantifies the difference in the eigenvectors
between the two models. Following the representation introduced in Section 2.2, the VN divergence compares two
ellipsoids not only by differences in the “shape” but also by
differences in the “rotation”.
If a VSM-like interpretation is attempted, one can think
about {|qi i}, {|dj i} as semantic concepts for the query and
the document respectively, whereas the vectors of eigenvalues ~λq , ~λd denote the importance of the corresponding semantic concepts in the two models. The VN divergence
offers a way of matching query concepts by analyzing how
much such concepts are related
P to documents concepts, i.e.
∀i, j, hqi |dj i2 . Particularly, j hqi |dj i2 = 1. Thus, hqi |dj i2
can be interpreted as the quantum probability associated
with the pure state |qi ihqi | for the elementary event |dj ihdj |,
i.e. µqi (|dj ihdj |) = tr(|qi ihqi |dj ihdj |) = hqi |dj i2 . Hence, one
could rewrite Eq. 16 as:
h
i
X
rank
λqi Eµqi log ~λd .
(17)
−∆V N (ρq ||ρd ) =

1. Which initial matrix ρb(0) to choose?
2. When to stop the update process?

As the estimation of a quantum document model requires an
iterative process, one may believe that the complexity will
make the process intractable. In Section 4.5, we provide
an analysis of the complexity of the proposed computation,
which will show that the process is quite tractable.

3.2.2

Scoring

The flexibility of the Kullback Liebler (KL) divergence
approach in keeping distinct query and document representations makes it attractive for a candidate scoring function
in our new framework. The direct generalization of classical KL divergence was introduced by Umegaki in [32] and is
called quantum relative entropy or Von-Neumann (VN) divergence. Given two quantum language models ρq and ρd for
the query and a document respectively, our scoring function
is the negative query-to-document VN divergence:

where, Z = tr(R(b
ρ(k) )b
ρ(k) R(b
ρ(k) )) is a normalization factor
in order to ensure that ρb(k+1) respects the constraint of unitary trace [15]. Despite the RρR algorithm being a quantum
generalization of the well-behaving Expectation Maximization (EM) algorithm, the likelihood is not guaranteed to
increase at each step because the nonlinear iteration may
overshoot, similarly to a gradient descent algorithm with a
too big step size. Characterizing such situations still remains
an open problem [27]. In this work, in order to ensure convergence, if the likelihood is decreased at k + 1, we use the
following damped update:
ρe(k+1) = (1 − γ)b
ρ(k) + γ ρb(k+1) ,

(14)

Smoothing Density Matrices

The ML estimation presented above suffers from a generalization of the usual zero-probability problem of classical ML, i.e. the estimator assigns zero probability to unseen data [35]. This is also called the zero eigenvalue problem [4]. Bayesian smoothing for density matrices has not
yet been proposed. This may be because Bayesian inference

i

Therefore, the VN divergence scores a document based on
the expectation of how important concept |qi i is in document
d even if it does not appear in it explicitly.

2

Similar damped updates were successfully used in [26] to improve convergence and stability of the loopy belief propagation
algorithm.

658

collections in order to vary (1) the collection size and (2)
collection type. This will produce a comprehensive test set
in order to verify the properties of our approach. All the
Name
SJMN
TREC7-8
WT10g
ClueWeb-B

Content
Newswire
Newswire
Web
Web

# Docs
90,257
528,155
1,692,096
50,220,423

Topic Numbers
51-150
351-450
451-550
51-200

Table 1: Summary of the TREC collections used to
support the experimental evaluation.
(a)

−∆V N (ρq kρd1 ) ∝ −.76

(b)

−∆V N (ρq kρd2 ) ∝ −1.06

o

Wo

Po

q
d1
d2

{computer, architecture}
{computer, architecture, and, games}
{computer, games, and, architecture}

{Ec , Ea , Kca }
{Ec , Ea , Kca , Eg }
{Ec , Eg , Ea }

collections have been stemmed with the Krovetz stemmer.
Both documents and queries have been stopped using the
standard INQUERY stopword list. For all the methods, the
Dirichlet smoothing parameter µ is set to the default Indri
value (µ = 2500). The optimization of all the other free parameters for the proposed model and the baselines is done
using five-fold cross validation using coordinate ascent [18]
with mean average precision (MAP) as the target metric.
The performance is measured on the top 1000 ranked documents. In addition to MAP, for newswire collections we
report the early precision metric @10 (precision at 10) and
for web collections with graded relevance judgements we report the recent ERR@10, which correlates better with click
metrics than other editorial metrics [6]. The statistical significance of differences in the performance of tested methods is determined using a two-sided Fisher’s randomization
test [29] with 25,000 permutations evaluated at α < 0.05.

Figure 4: A synthetic example of QLM with a vocabulary of n = 3 terms. The orthogonal rays are the
eigenvectors of the ellipsoids. ρq is not smoothed
thus degenerates onto a ray. ρd1 rotates towards the
direction of observed query dependencies and is thus
ranked higher.

3.4

Final Considerations

The estimation and scoring process of quantum language
models retains classical unigram LMs and KL divergence
as special cases. The classical unigram LM is recovered by
restricting the maximization in Eq. 10 to diagonal density
matrices and including into the sequence of projectors Pd
only an orthonormal basis, such as the elements of E. Classical KL divergence is recovered by noting that if ρq and ρd
are diagonal density matrices, they share the same eigensystem. Hence, |qi i = |di i and λqi = θqi , λdi = θdi , where θ~q ,
θ~d are the parameters of classical unigram LMs for the query
and the document respectively. In this setting, hqi |dj i2 = 0
for i 6= j and the VN divergence reduces to classical KL, i.e.
rank P
−∆V N (ρq kρd ) = −∆KL (θ~q kθ~d ) =
i θqi log θdi .
In Figure 4, we report a synthetic example of the application of the model. We plot the density matrices obtained
by the MLE (Section 3.2.1) on the sequence of projectors
reported in the table. As usual in ad-hoc tasks, we smooth
only the QLMs of the documents. The model corresponding
to the query is a projector, i.e. it has two zero eigenvalues,
because we did not apply smoothing. If the dependencies
are included in the sequence Po , the MLE rotates the corresponding QLM towards the direction spanned by the observed projector (i.e. Kca ). This entails that the model ρd1
is considered more similar to the query than the model ρd2
which corresponds to a classical language model.

4.
4.1

4.2

EVALUATION
Experimental Setup

All the experiments reported in this work were conducted
using the open source Indri search engine (version 5.3)3 . The
test collections used are reported in Table 1. We choose the
3

Methodology

Our experimental methodology goes as follows. In a first
step, we compare our QLM approach to a unigram Language
Modeling baseline (denoted LM) based on Dirichlet smoothing [35], which is a strong bag-of-words baseline. This comparison is done by assigning uniformp
superposition weights
to each dependency κ, i.e. σi = 1/ |κ|, where |κ| is the
cardinality of κ (denoted QLM-UNI). This step has two main
objectives: (1) to test if quantum probability can bring better performance than a standard bag-of-words model and
(2) to test if uniform superposition weights are a reasonable
baseline setting.
As a second step, we test the proposed model against the
strong non bag-of-words MRF model, which has shown to be
highly effective especially for large scale web collections [19,
20]. We test the full dependence version of the model (denoted MRF-FD) which captures dependencies between all the
query terms and thus is the most natural choice for a comparison with our model. However, MRF-FD exploits both
proximity (#uw) and exact matching (#1). As our model
only exploits proximity as an indicator of dependence, we
also propose to test the variant MRF-FD-U, which is a MRF
using only the proximity feature. This could provide interesting insights on how the models score based upon the same
evidence.
Finally, we propose a slightly more elaborate version of
our model (denoted QLM-IDF) in which the superposition
weights are no more assumed to be uniform. Instead, we
assign to each σi the normalized idf weight of the corresponding term wi . The objective is to test if a more reasonable parametrization of superposition weights can improve
the retrieval effectiveness.

http://www.lemurproject.org

659

Figure 5: Plots of MAP (QLM-UNI and LM) and MLE objective against the number of updates of the density
matrix for SJMN, TREC7-8 and WT10g (left, center and right).

4.3.2

All the results exposed in this paper have been obtained by
reranking. We rerank a pool of 20000 documents retrieved
using LM in order to make a fair comparison between our
method and the baselines.

4.3
4.3.1

Setting up QLM
Building the Sequence of Projectors

Very similarly to MRF-FD, given a query Q = {q1 , . . . , qn },
we assume that the interesting dependencies to consider correspond to the power set P(Q)4 . In order to build the set of
projectors for the given document we apply Algorithm 1.
Algorithm 1 Builds the sequence Pd given Wd , Q
Require: Wd , Q
1: Pd ← ∅
2: for κ ∈ P(Q) do
3:
for #(κ, Wd ) do
4:
Pd ← Pd ⊕ m(κ) %Adds the projector to the
5:
end for
6: end for
7: return Pd

MLE Convergence Analysis

Before doing any comparisons, we answer the questions
related to the construction of a quantum language model,
i.e. (1) how to initialize ρb(0) ? (2) when to stop the update
process? In order to help the maximum likelihood process to
converge faster, we initialize the matrix ρb(0) to the density
matrix corresponding to the classical maximum likelihood
language model θ~M L of the document or query under consideration. This is a diagonal matrix ρb(0) = diag(θ~M L ). We
also tested with the uniform density matrix, as suggested
in [15], but we found that the MAP was severely harmed.
In order to address the second question, we analyze the
variation of MAP with respect to the maximum number of
iterations nit ∈ [1, 50]. The damping factor γ is optimized
over the set of values Γ = {0, 0.1, ..., 0.9}. The iterative
process stops before nit if the change in the likelihood is
below 10−4 . In order to check for possible variations due
to the collection type, we plot the iteration-MAP curve for
two similar collections, i.e. SJMN and TREC7-8, and a web
collection, WT10g. We also plot P
the training objective in
1
ρd ), where
Eq. 10 over the set of topics: |R|
d∈R log LPd (b
R is the multiset of retrieved documents. The trend is shown
in Figure 5. Generally, at any number of iterations, the
MAP stays significantly above the baseline. It seems that
there is a good correlation between likelihood maximization
and MAP, although one can note some overfitting at high
number of iterations. Capping by 10 ≤ nit ≤ 20 seems a
good trade-off between likelihood maximization and MAP.
However, to provide a fair comparison with the baselines,
we choose to include nit as a free parameter to train by
coordinate ascent.

sequence

For each dependency κ in P(Q), the algorithm scans the
document sequence Wd . For each occurrence of κ, it adds a
projector m(κ) to the sequence Pd . The function #(κ, Wd )
returns how many times the dependency κ is observed in Wd .
Therefore, the algorithm adds as many projectors as the
number of detected compound dependencies. Note that by
looping on P(Q), we are actually implementing the strategy
exposed in Section 3.1.3, i.e. adding both the dependence
and all of its subsets. Following Section 3.1.3, we choose to
parametrize # as the unordered window operator in Indri
(#uwL). Therefore, a given dependency κ will be detected if
the component terms appear in any order in a fixed-window
of length L = l|κ|. This kind of adaptive parametrization of
the window length is state-of-the-art for dependence models
such as MRF-FD [2, 19]. For all the dependence models, the
coordinate ascent for l spans {1, 2, 4, 8, 16, 32}, which is a
robust pool covering different window lengths, including the
standard value (l = 4) for MRF-FD.

4.4

Results

The results discussed in this section are compactly reported in Table 2.

4.4.1

Language Modeling Baseline

From the comparisons with the LM baseline, one can see
that QLM-UNI outperforms LM significantly, with relative improvements in MAP going up to 12.1% in the case of WT10g
collection and 19.2% for the ClueWeb-B collection. This
seems to be in line with the hypothesis formulated in [19],
for which dependence models may yield larger improvements
for large collections.
The weight-normalization problem seems to be addressed
automatically: our model does not need for any combina-

4

In order to keep the retrieval complexity reasonable both for
MRF and QLM, we limit ourselves to query term subsets with at
most three terms.

660

SJMN
LM
MRF-FD-U
MRF-FD
QLM-UNI
QLM-IDF

P@10
.3064
.3138
.3074

TREC7-8
P@10
MAP
.4230
.2120
.4350
.2228
.4460
.2243

MAP
.1995
.2071
.2061

WT10g
ERR@10
MAP
.1068
.1975
.1136
.2097
.1147
.2146

ClueWeb-B
ERR@10
MAP
.0718
.1003
.0828
.1103
.0881
.1137

.3181

.2077

.4480

.2240

.1162

.2215αβ

.1015αβ

.1196αβ

(+1.4/+3.5)

(+0.3/+0.8)

(+3.0/+0.4)

(+0.5/-0.1)

(+2.2/+1.3)

(+5.6/+3.2)

(+22.6/+15.2)

(+8.4/+5.2)

.3170

.2093

.4450

.2254

.1176

.2264αβ

.0997αβ

.1189αβ

(+1.0/+3.1)

(+1.1/+1.6)

(+2.3/-0.2)

(+1.2/+0.5)

(+3.5/+2.6)

(+7.9/+5.5)

(+20.4/+13.1)

(+7.8/+4.5)

Table 2: Evaluation of the performance for the five methods tested. Best results are highlighted in boldface.
Numbers in parentheses indicate relative improvement (%) in MAP over MRF-FD-U/MRF-FD. All the results
for dependence models are significant with respect to the baseline LM. The symbols α ,β means statistical
significance over MRF-FD-U, MRF-FD respectively.
ing a different value of l ∈ {1, 2, 4}. The best performing
model obtained a MAP of 10.91. It seems that our model
can exploit this short range information in a better way than
MRF models.

tion weights. Moreover, it is robust across the folds. From
an analysis of the optimal values of the parameters obtained
across the different folds, we found that optimal window
sizes were l ∈ {1, 2}. This can be explained by considering
that in the current version of QLM, it is possible to decide
if the dependency is detected or not, but the model cannot
discriminate its “importance”. If one decides to increase l,
more inaccurate dependencies will be detected and the performance will be deteriorated. However, even with a larger
window size, statistical significance over LM is maintained.
From these considerations, we suggest l = 2 as a default
setting for our model. Finally, the results endorse that our
QLM does not need an engineered estimation of superposition weights to perform well.

4.4.2

4.4.3

Setting Superposition Weights

Our last test aimed at verifying if a more reasonable setting of the superposition weights could further improve retrieval performance.
For a dependency {w1 , . . . , wK }, we
q
P
set σi = idfwi / i idfwi . This has the effect of attributing
a larger count to the more “important” term in the dependency. QLM-IDF generally increases MAP. However, this is
not the case for ClueWeb-B. From a query-by-query analysis,
we noticed that QLM-IDF increases the performance for noisy
queries by promoting the most “important” terms in unnecessary subsets. For multiword expressions such as ClueWebB topics continental plates and rock art, weighting by idf
may be misleading by assigning more weight to one of the
terms. In this cases, a uniform parametrization is far more
effective. This demonstrates that there is still room for improvement by a clever tuning of superposition parameters,
for example by leveraging feature functions [2, 3].

Markov Random Fields Baseline

As a second test, we report the results obtained for the
MRF-FD and MRF-FD-U baselines. These have proved to be
very robust non bag-of-words baselines [2, 19, 20]. Contrary
to our model, MRF does not handle dependency information in the estimation phase. One has to specify the coefficients (λT , λO , λU ) for the combination of dependence and
independence scores. To limit per-fold overfitting, for the
dependence models, we first train combination parameters
(λf ∈ {0, 0.01, ..., 1}) then l for each fold. For MRF-FD-U, we
set λO = 0.
Results show that for SJMN and TREC7-8, QLM-UNI, MRFFD and MRF-FD-U are essentially equivalent. However, for
the two Web collections, our model significantly outperforms
both MRF variants. On ClueWeb-B, statistical significance
is attained for the two reported measures. As conjectured
in [19], noisy web collections could be a more discriminative
testbed for dependence models. Optimal l values for MRFFD were very small for SJMN (l ∈ {1, 2}) in contrast to the
optimal setting for ClueWeb-B (l ∈ {16, 32}). In [19], the
authors suggest that for homogenous newswire collections
a small window is enough to capture useful dependencies,
while for large, noisy web collections, a larger span must be
set. However, the performances obtained by our model seem
to suggest that it can greatly benefit from term dependencies, on a variety of collections, even when a small window
size is used. This elucidates the fact that even short range
information can be extremely useful if integrated in the estimation phase. In order to get a more comprehensive view
on such issues, we trained on the entire set of ClueWeb-B
topics three versions of MRF-FD-U, each obtained by clamp-

4.5

Complexity Analysis

Complexity issues can be tackled by noting that it is not
necessary to manipulate n × n matrices. We associate a
dimension for each query term and an additional dimension for a “don’t care” term that will store the probability
mass for the other terms in the vocabulary. Therefore, a
multinomial over n points is reduced to a multinomial over
|Q| + 1 points, where |Q| is the number of unique terms
in the query and the additional dimension is simply a relabeling of the other term events. In this way, the QLM
to manipulate is k × k, where k = |Q| + 1. The eigendecomposition generally requires O(k3 ). The iterative process
requires at most |P(Q)| = 2|Q| matrix multiplications for
the expectation step, where 2|Q| is the maximum number of
unique projectors in Pd and 2 matrix multiplications for the
maximization step. In the case the likelihood is decreased,
|Γ| more iterations are done giving a worst-case complexity
of O(nit |Γ|2k + k3 ), i.e. if each iteration needs damping.
We showed that 10 ≤ nit ≤ 20 is enough; we use |Γ| = 10
and k is very small for title queries, which make the process
computationally tractable. In practice, we observed that the
damping process is very effective and dramatically improves
convergence speed. As an example, the mean number of iter-

661

ations for ClueWeb-B when nit = 15 is 7.02 which is orders
of magnitude less than nit |Γ| = 150. Finally, we conjecture
that such process could be executed at indexing time, thus
eliminating any additional on-line costs.

5.

[8] J. L. Fagan. Automatic phrase indexing for document retrieval.
In Proc. of SIGIR, pages 91–101, 1987.
[9] J. Gao, J. Y. Nie, G. Wu, and G. Cao. Dependence language
model for information retrieval. In Proc. of SIGIR, pages
170–177, 2004.
[10] A. Gleason. Measures on the closed subspaces of a hilbert
space. Journ. Math. Mech., 6:885–893, 1957.
[11] K. S. Jones, S. Walker, and S. E. Robertson. A probabilistic
model of information retrieval: development and comparative
experiments. Inf. Proc. Manag., pages 779–840, 2000.
[12] M. Lease. An improved markov random field model for
supporting verbose queries. In Proc. of SIGIR, pages 476–483,
2009.
[13] C. Lee, G. G. Lee, and M. G. Jang. Dependency structure
applied to language modeling for information retrieval. ETRI,
28(3):337–346, 2006.
[14] Y. Lv and C. Zhai. Positional language models for information
retrieval. In Proc. of SIGIR, pages 299–306, 2009.
[15] A. I. Lvovsky. Iterative maximum-likelihood reconstruction in
quantum homodyne tomography. Journ. Opt. B6, pages
S556–S559, 2004.
[16] M. Melucci. Deriving a quantum information retrieval basis.
The Computer Journal, 2012.
[17] M. Melucci and K. Rijsbergen. Quantum mechanics and
information retrieval. Advanced Topics in Information
Retrieval, 33:125–155, 2011.
[18] D. Metzler and W. Bruce Croft. Linear feature-based models
for information retrieval. Inf. Retr., 10(3):257–274, 2007.
[19] D. Metzler and W. B. Croft. A markov random field model for
term dependencies. In Proc. of SIGIR, pages 472–479, 2005.
[20] D. Metzler, T. Strohman, Y. Zhou, and W. B. Croft. Indri at
TREC 2005: Terabyte Track. In Proc. of TREC, 2005.
[21] M. Mitra, C. Buckley, A. Singhal, and C. Cardie. An analysis
of statistical and syntactic phrases. In Proc of RIAO, pages
200–217, 1997.
[22] W. Morgan, W. Greiff, and J. Henderson. Direct maximization
of average precision by hill-climbing, with a comparison to a
maximum entropy approach. In Proc. of HLT-NAACL, pages
93–96, 2004.
[23] M. A. Nielsen and I. L. Chuang. Quantum Computation and
Quantum Information. Cambridge University Press, 2004.
[24] J. H. Park, W. B. Croft, and D. A. Smith. A
quasi-synchronous dependence model for information retrieval.
In Proc. of CIKM, pages 17–26, 2011.
[25] B. Piwowarski, I. Frommholz, M. Lalmas, and K. van
Rijsbergen. What can quantum theory bring to information
retrieval. In Proc. of CIKM, pages 59–68, 2010.
[26] M. Pretti. A message-passing algorithm with damping. J. Stat.
Mech., page P11008, 2005.
[27] J. R̆ehác̆ek, Z. Hradil, E. Knill, A. I. Lvovsky. Diluted
maximum-likelihood algorithm for quantum tomography. Phys.
Rev. A, 75:042108, 2007.
[28] G. Salton, C. S. Yang, and C. T. Yu. A Theory of Term
Importance in Automatic Text Analysis. JASIST, 26(1):33–44,
1975.
[29] M. D. Smucker, J. Allan, and B. Carterette. A comparison of
statistical significance tests for information retrieval
evaluation. In Proc. of CIKM, pages 623–632, 2007.
[30] F. Song and W. B. Croft. A general language model for
information retrieval. In Proc. of SIGIR, pages 279–280, 1999.
[31] M. Srikanth and R. Srihari. Biterm language models for
document retrieval. In Proc. of SIGIR, pages 425–426, 2002.
[32] H. Umegaki. Conditional expectation in an operator algebra.
Kodai Mathematical Seminar Reports, 14(2):59–85, 1962.
[33] K. van Rijsbergen. The Geometry of Information Retrieval.
Cambridge University Press, 2004.
[34] M. K. Warmuth and D. Kuzmin. Bayesian generalized
probability calculus for density matrices. Machine Learning,
78(1-2):63–101, 2009.
[35] C. Zhai. Statistical language models for information retrieval a
critical review. Found. Trends Inf. Retr., 2(3):137–213, 2008.
[36] J. Zhao and Y. Yun. A proximity language model for
information retrieval. In Proc. of SIGIR, pages 291–298, 2009.
[37] X. Zhao, P. Zhang, D. Song, and Y. Hou. A novel re-ranking
approach inspired by quantum measurement. In Proc. of
ECIR, pages 721–724, 2011.
[38] G. Zuccon and L. Azzopardi. Using the quantum probability
ranking principle to rank interdependent documents. In Proc.
of ECIR, page 357–369, 2010.

CONCLUSION

In this paper, we presented a principled application of
quantum probability for IR. We showed how the flexibility
of vector spaces joined with the powerful tools of probabilistic calculus can be mixed together for a flexible, yet principled account of term dependencies for IR. In our model,
dependencies are neither represented as additional dimensions, nor stochastically as joint probabilities. They assume
a new status as superposition events. The relationship of
such an event to the traditional term events are encoded by
the off-diagonal values in the corresponding projection matrix. Both documents and queries are associated to density
matrices estimated through the maximization of a product,
which in the classical case reduces to a likelihood. As our
model integrates the dependencies in the estimation phase,
it has no need for combination parameters. Experiments
showed that it performs equivalently to the existing dependence models on newswire test collections and outperforms
the latter on web data.
To our knowledge, this work provides the first experimental result showing the usefulness of this kind of probabilistic
calculus for IR. The marriage between vector spaces and
probability can be endlessly improved in the future. One
straightforward direction is to relax the assumption that single terms represent orthogonal projectors. This could lead
to a new way of integrating latent directions as estimated by
purely geometric methods such as Latent Semantic Indexing
(LSI) [7] into a probabilistic model. In this work, we did not
exploit the full machinery of complex vector spaces. We do
not have a practical justification for the use of the complex
field for IR tasks. However, we speculate that this could
bring improved representational power and thus remains an
interesting direction to explore. At last, we believe that our
model could be potentially applied to other fields of natural
language processing only by means of a principled Bayesian
calculus capable of manipulating density matrices. We hope
that this work will foster future research in this direction.

6.

ACKNOWLEDGMENTS

We would like to thank the anonymous reviewers for their
valuable comments and suggestions.

7.

REFERENCES

[1] J. Bai, Y. Chang, H. Cui, Z. Zheng, G. Sun, and X. Li.
Investigation of partial query proximity in web search. In Proc.
of WWW, pages 1183–1184, 2008.
[2] M. Bendersky and W. B. Croft. Modeling higher-order term
dependencies in information retrieval using query hypergraphs.
In Proc. of SIGIR, pages 941–950, 2012.
[3] M. Bendersky, D. Metzler, and W. B. Croft. Parametrized
concept weighting in verbose queries. In Proc. of SIGIR, pages
605–614, 2011.
[4] R. Blume-Kohout. Hedged maximum likelihood estimation.
Phys. Rev. Lett., 105:200504, 2010.
[5] R. Blume-Kohout. Optimal, reliable estimation of quantum
states. New J. Phys., 12:043034, 2010.
[6] O. Chapelle, D. Metzler, Y. Zhang, P. Grinspan. Expected
reciprocal rank for graded relevance In Proc. of CIKM, 2009.
[7] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer,
and R. Harshman. Indexing by latent semantic analysis.
JASIST, 41:391–407, 1990.

662

