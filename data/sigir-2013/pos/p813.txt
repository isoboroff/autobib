Flat vs. Hierarchical Phrase-Based Translation Models for
Cross-Language Information Retrieval
Ferhan Ture1,2 , Jimmy Lin3,2,1
1

Dept. of Computer Science, 2 Institute for Advanced Computer Studies, 3 The iSchool
University of Maryland, College Park

fture@cs.umd.edu, jimmylin@umd.edu
ABSTRACT

signiﬁcant gains in eﬀectiveness—however, it is unclear if
such gains could have been achieved from “ﬂat” representations. This question is interesting because it opens up a
diﬀerent region in the design space: ﬂat representations are
faster, more scalable, and exhibit less complexity—encoding
a diﬀerent tradeoﬀ between eﬃciency and eﬀectiveness.
There are two main contributions to this work: First, we
test the robustness of query translation techniques introduced in earlier work [24] by comparing ﬂat and hierarchical
phrase-based translation models. In addition, we examine
the eﬀects of three diﬀerent heuristics for handling one-tomany word alignments. We show that a combination-ofevidence approach consistently outperforms a strong tokenbased baseline as well as a one-best translation baseline for
three diﬀerent languages, Arabic (Ar), Chinese (Zh) and
French (Fr), using either ﬂat or hierarchical translation grammars. Second, we discuss diﬀerences between the two MT
models and provide insights on the tradeoﬀs each represent.
Experiments show that a hierarchical translation model yields
higher eﬀectiveness, which suggests that there is value in
more sophisticated modeling of linguistic phenomena.

Although context-independent word-based approaches remain popular for cross-language information retrieval, many
recent studies have shown that integrating insights from
modern statistical machine translation systems can lead to
substantial improvements in eﬀectiveness. In this paper, we
compare flat and hierarchical phrase-based translation models for query translation. Both approaches yield signiﬁcantly
better results than either a token-based or a one-best translation baseline on standard test collections. The choice of
model manifests interesting tradeoﬀs in terms of eﬀectiveness, eﬃciency, and model compactness.
Categories and Subject Descriptors: H.3.3 [Information
Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Experimentation
Keywords: SCFG, query translation

1.

INTRODUCTION

Despite the prevalence of context-independent word-based
approaches for cross-language information retrieval (CLIR)
derived from the IBM translation models [4], recent studies have shown that exploiting ideas from machine translation (MT) for context-sensitive query translation produces
higher-quality results [17, 19, 24]. State-of-the-art MT systems take advantage of sophisticated models with “deeper”
representations of translation units, e.g., phrase-based [13],
syntax-based [25, 27], and even semantics-based [11] models.
In particular, hierarchical phrase-based machine translation
(PBMT) systems [5] provide a middle ground between efﬁcient “ﬂat” phrase-based models and expressive but slow
syntax-based models. In terms of translation quality, efﬁciency, and practicality, ﬂat and hierarchical PBMT systems have become very popular, partly due to successful
open-source implementations.
This paper explores ﬂat and hierarchical PBMT systems
for query translation in CLIR. Previously, we have shown
that integrating techniques from hierarchical models lead to

2. BACKGROUND AND RELATED WORK
Although word-by-word translation provides the starting
point for query translation approaches to CLIR, there has
been much work on using term co-occurrence statistics to
select the most appropriate translations [10, 15, 1, 21]. Explicitly expressing term dependency relations has produced
good results in monolingual retrieval [9, 18], but extending that idea to CLIR has not proven to be straightforward.
Another thread of research has focused on translating multiword expressions in order to deal with ambiguity [2, 28].
Borrowing ideas from MT for IR dates back to at least
Ponte and Croft’s work on retrieval using language modeling [20]. That work was later extended to translation models
for retrieval [3], followed by a series of successful adaptations
to the cross-language case [26, 14, 8].
As MT systems have evolved away from the token-based
translation approach, researchers have started exploring ways
to integrate various components of modern MT systems for
better CLIR eﬀectiveness. Magdy et al. [17] showed that
preprocessing text consistently for MT and IR systems is
beneﬁcial. Nikoulina et al. [19] built MT models tailored
to query translation by tuning model weights with queries
and reranking the top n translations to maximize eﬀectiveness on a held-out query set. While improvements were
more substantial using the latter method, another interesting ﬁnding was the low correlation between translation and

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM or the author must be honored. To
copy otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

813

retrieval quality. This indicates that better translation may
not necessarily help retrieval.

In R1 , the non-terminal variable [X] allows an arbitrarily
long part of the sentence to be moved from the left of the
sentence in English to the middle of the sentence in French,
even though it generates a single token (i.e., maternal) using
R2 in this particular example. As a result, an SCFG can
capture distant dependencies in language that may not be
realized in ﬂat models.
Each sequence of rules that covers the entire input is called
a derivation, D, and produces a translation candidate, t,
which is scored by a linear combination of features. One
can use many features to score a candidate, but two features are the most important: the product of rule likelihood
values indicates how well the candidate preserves the original meaning, TM(t, D|s), whereas the language model score,
LM(t), indicates how well-formed the translation is. Combining the two, the decoder searches for the best translation:

2.1 Context-Independent Baseline
As a baseline, we consider the technique presented by
Darwish and Oard [6]. Given a source-language query s,
we represent each token sj by its translations in the target language, weighted by the bilingual translation probability. These token-to-token translation probabilities, called
P rtoken , are learned independently from a parallel bilingual corpus using automatic word alignment techniques [4].
In this approach, the score of document d, given sourcelanguage query s, is computed by the following equations:

Score(d|s) =
Weight(tf(sj , d), df(sj ))
(1)
j

tf(sj , d) =



tf(ti , d)P rtoken (ti |sj )

(2)

df(ti )P rtoken (ti |sj )

(3)

t(1) = arg max[ max TM(t, D|s)LM(t)]
t

ti

df(sj ) =



D∈D(s,t)

(4)

There is a tradeoﬀ between using either ﬂat or hierarchical
grammars. The latter provides more expressivity in representing linguistic phenomena, but at the cost of slower
decoding [16]. On the other hand, ﬂat models are faster
but less expressive. Also, due to the lack of variables, ﬂat
grammars contain more rules, resulting in a more verbose
translation grammar.

ti

In order to reduce noise from incorrect alignments, we impose a lower bound on the token translation probability, and
also a cumulative probability threshold, so that translation
alternatives of sj are added (in decreasing order of probability) until the cumulative probability has reached the threshold. Any weighting function can be used in conjunction with
the tf and df values, and we chose the Okapi BM25 term
weighting function (with parameters k1 = 1.2, b = 0.75).

3. QUERY TRANSLATION WITH MT
In our previous work [24], we described two ways to construct a context-sensitive term translation probability distribution using internal representations from an MT system.
These distributions can then be used to retrieve ranked documents using equations (1)–(3).

2.2 Flat vs. Hierarchical Phrase-based MT
Machine translation can be divided into three steps: training the translation model, tuning parameters, and decoding.
We will mostly focus on the ﬁrst step, since that is where
ﬂat and hierarchical MT approaches diﬀer the most.
The output of the ﬁrst step is the translation model (called
TM hereafter). For both ﬂat and hierarchical variants, the
TM consists of a set of rules (i.e., the translation grammar)
in the following format:

3.1 Using the Translation Model
With appropriate data structures, it is possible to eﬃciently extract all rules in a TM (either ﬂat or hierarchical)
that apply to a given source query, s, called TMs . For each
such applicable rule r, we identify each source token sj in r,
ignoring any non-terminal symbols. From the token alignment information included in the rule structure, we can ﬁnd
all target tokens aligned to sj . For each such target token ti ,
the likelihood value of sj being translated as ti is increased
by the likelihood score of r. At the end of the process, we
have a list of possible translations and associated likelihood
values for each source token that has appeared in any of the
rules. We can then convert each list into a probability distribution, called P rPBMT for ﬂat and P rSCFG for hierarchical
grammars by normalizing the sum of likelihood scores:

1
(r)
(5)
P rSCFG/PBMT (ti |sj ) =
ψ r∈TM

α = α0 α1 . . . || β = β0 β1 . . . || A || (α → β)
We call the sequence of αi ’s the source side of the rule,
and sequence of βj ’s the target side of the rule. The above
indicates that the source side translates into the target side
with a likelihood of (α → β).1 A contains token alignments
in the format i-j, indicating that source token αi is aligned
to target token βj .
A hierarchical model [5] diﬀers from a flat model [13] in
terms of rule expressivity: rules are allowed to contain one
or more nonterminals, each acting as a variable that can
be expanded into other expressions using the grammar, carried out in a recursive fashion. These grammars are called
synchronous context-free grammars (SCFG), as each rule
describes a context-free expansion on both sides.
Consider the following two rules from an SCFG:

s

sj ↔ti in r

where sj ↔ ti represents an alignment between tokens sj
and ti and ψ is the normalization factor.
When a source token sj is aligned to multiple target tokens
in a rule, it is not obvious how to distribute the probability
mass. In our previous implementation [24], each alignment
was treated as an independent event with the same probability. We call this the one-to-one heuristic, and introduce
two alternatives due to the following drawback: the target
tokens aligned to sj are usually not independent. For example, the token brand is aligned to three tokens marque, de,

R1 . [X] leave in europe || congé de [X] en europe
|| 1-0 2-3 3-4 || 1
R2 . maternal || maternité || 0-0 || 0.69
1
The likelihood function  is not a probability density function
because it is not normalized.

814

fabrique (En. brand, of, factory), which is an appropriate
translation when put together. Even if de is discarded as a
stopword, the one-to-one heuristic will learn the token pair
(brand, fabrique) incorrectly. An alternative heuristic is to
ignore these rules altogether, assuming that good translation pairs will appear in other rules, thus discarding these
cases would not cause any harm (we call this the one-tonone technique). A third approach is to combine the target
tokens into a multi-token expression. Thus, in the above
example, we would learn the translation of brand as marque
de fabrique, which is a useful mapping that we might not
learn otherwise. We call the third technique one-to-many,
and compare these three heuristics in our evaluation.

respectively. We used the title text of the 50 topics for the
Arabic and French collections, and we treated the 73 wellformed questions in NTCIR-8 as queries.
For the ﬂat and hierarchical translation models, we used
Moses [12] and cdec [7], respectively. The training data
consisted of Ar-En GALE 2010 evaluation (3.4m sentence
pairs), Zh-En FBIS corpus (0.3m pairs), and Fr-En Europarl
corpus v7 (2.2m pairs). A 3-gram language model was built
for Arabic and Chinese using the target side of the parallel
corpora. For French, we trained a 5-gram LM from the
monolingual dataset provided for WMT-12. More details of
the experimental setup can be found in [23].
Source code for replicating all the results presented in this
paper is available in the open-source Ivory toolkit.2

3.2 Using N-best Translations

4.1 Effectiveness

Given t(1) , the most probable translation of query s computed by equation (4), we can score a document d as follows:

(1)
(1)
Score(d|s) =
Weight(tf(ti , d), df(ti ))
(6)

The baseline token-based model yields a Mean Average
Precision (MAP) of 0.271 for Arabic, 0.150 for Chinese,
and 0.262 for French. These numbers are competitive when
compared to similar techniques applied to these collections.
For each collection, we evaluated the three CLIR techniques
(P rtoken , P rSCFG/PBMT , and P rnbest , with n ∈ {1, 10}), exploring the eﬀect of the diﬀerent alignment heuristics as well
as ﬂat vs. hierarchical phrase-based translation models. Parameters of the interpolated model were learned by a grid
search. Experimental results are summarized in Table 1.3
Based on a randomized signiﬁcance test [22], the interpolated model outperforms (with 95% conﬁdence, marked *)
the token-based model for all runs except for Arabic with
Moses, consistently with the one-to-many heuristic and in
some cases with the two other heuristics. Furthermore, in
ﬁve out of the six conditions, the interpolated model with
the one-to-many heuristic is signiﬁcantly better than the
one-best MT approach (marked †). This conﬁrms that combining diﬀerent query translation approaches is beneﬁcial,
and is also robust with respect to the test collection, language, and underlying MT model. The one-to-many term
mapping heuristic seems to be the most eﬀective overall.
However, the two MT models display signiﬁcant diﬀerences in the “grammar” column, as the hierarchical model
signiﬁcantly outperforms the ﬂat model. This supports the
argument that the former is better at representing translation alternatives since it is more expressive. Also as a
result of this diﬀerence, the ﬂat grammar is much larger
than the hierarchical one, which leads to an order of magnitude increase in processing time for P rPBMT .4 These diﬀerences become especially important for the Arabic collection,
where P rSCFG/PBMT performs much better than P r10-best ,
using either MT system. An additional beneﬁt of using
P rSCFG/PBMT is that we do not need to tune model parameters for translation, which is computationally intensive.
It is also interesting that the diﬀerences between the two
MT models are insigniﬁcant for the 10-best approach, where
the decoder ﬁnds similar translations in both cases. Therefore, it might be better to use ﬂat representations for the
10-best approach for eﬃciency, since the end-to-end translation process is faster than hierarchical models.

i

Since MT systems generate a set of candidate translations
in the process of computing equation (4), we can consider
the n most likely candidates. For each candidate translation
t(k) , and for each source token sj , we use token alignments
to determine which tokens in t(k) are associated with sj . If
there are multiple target tokens, we apply one of the three
methods introduced previously: one-to-none, one-to-one, or
one-to-many. By the end of the process, we obtain a probability distribution of translations for each sj based on the
n best query translations. If source token sj is aligned to
(i.e., translated as) ti in the kth best translation, the value
(t(k) |s) is added to its probability mass, producing the following for P rnbest (where ϕ is the normalization factor):
P rnbest (ti |sj ) =

1
ϕ

n


(t(k) |s)

(7)

k=1
sj ↔ti in t(k)

3.3 Evidence Combination
For P rtoken , translation probabilities are learned from all
sentence pairs in a parallel corpus, whereas P rSCFG/PBMT
only uses portions that apply to the source query, which
reduces ambiguity in the probability distribution based on
this context. P rnbest uses the same set of rules in addition
to a language model to search for most probable translations. This process ﬁlters out some irrelevant translations
at the cost of less diversity, even among the top 10 or 100
translations. Since the three approaches have complementary strengths, we can perform a linear interpolation of the
three probability distributions:
P rc (ti |sj ; λ1 , λ2 ) =λ1 P rnbest (ti |sj ) + λ2 P rSCFG/PBMT (ti |sj )
+ (1 − λ1 − λ2 )P rtoken (ti |sj )

(8)

Replacing any of these probability distributions introduced
above for P rtoken in equations (1)–(3) yields the respective
scoring formula.

4.

2

EVALUATION

3

http://ivory.cc/

For the 1-best model, one-to-one and one-to-many perform very
similarly, so we present only the former for space considerations.
4
On the other hand, decoding with a ﬂat grammar is substantially
faster than decoding with hierarchical MT due to constraints imposed by language modeling.

We performed experiments on three CLIR test collections:
TREC 2002 En-Ar CLIR, NTCIR-8 En-Zh Advanced CrossLingual Information Access (ACLIA), and CLEF 2006 En-Fr
CLIR, with sizes 383,872, 388,589 and 177,452 documents,

815

Language

MT

token

Ar

cdec
Moses
cdec
Moses
cdec
Moses

0.271

Zh
Fr

0.150
0.262

many
0.293
0.274
0.182
0.156
0.297
0.264

grammar
one
none
0.282 0.302
0.266 0.273
0.188 0.170
0.167 0.151
0.288 0.292
0.257 0.262

1-best
one
none
0.249 0.249
0.249 0.232
0.155 0.155
0.155 0.146
0.276 0.235
0.297 0.242

many
0.255
0.264
0.159
0.169
0.307
0.289

10-best
one
0.249
0.254
0.159
0.163
0.304
0.300

none
0.248
0.249
0.159
0.163
0.295
0.282

interpolated
many
one
none
0.293∗† 0.282
0.302∗
0.280†
0.274
0.276
0.192∗† 0.193∗ 0.182∗
0.183∗† 0.177∗ 0.188∗
0.318∗† 0.314∗ 0.315∗
0.307∗
0.301
0.300

Table 1: A summary of experimental results under diﬀerent conditions, for all three CLIR tasks. Superscripts
* and † indicate the result is signiﬁcantly better than the token-based and one-best approaches, respectively.

5.

CONCLUSIONS AND FUTURE WORK

In this paper, we extended an MT-based context-sensitive
CLIR approach [24], comparing ﬂat and hierarchical phrasebased translation models on three collections in three diﬀerent languages. We make a number of interesting observations about the tradeoﬀs in incorporating machine translation techniques for query translation.
A combination-of-evidence approach was found to be robust and eﬀective, but we have not examined how the interpolation model parameters can be learned using held-out
data—this is the subject of ongoing work. Also, we are
exploring ways of leveraging the translation of multi-token
source-side expressions. Although we demonstrated the beneﬁts of hierarchical grammars, we still do not explicitly take
advantage of non-terminal information in the rules. It might
be beneﬁcial to perform a detailed error analysis to see what
types of topics are improved with the use of SCFGs over ﬂat
grammars. Finally, we brieﬂy discussed interesting tradeoﬀs between eﬃciency and eﬀectiveness, but more detailed
experiments are required to better understand diﬀerent operating points and the overall design space.

6.

[9]
[10]

[11]

[12]

[13]
[14]

[15]

[16]
[17]

ACKNOWLEDGMENTS

This research was supported in part by the BOLT program of the Defense Advanced Research Projects Agency,
Contract No. HR0011-12-C-0015; NSF under awards IIS0916043 and IIS-1144034. Any opinions, ﬁndings, conclusions, or recommendations expressed are those of the authors and do not necessarily reﬂect views of the sponsors.
The second author is grateful to Esther and Kiri for their
loving support and dedicates this work to Joshua and Jacob.

7.

[18]
[19]

[20]
[21]

REFERENCES

[1] M. Adriani and C. Van Rijsbergen. Phrase identiﬁcation in
cross-language information retrieval. RIAO, 2000.
[2] L. Ballesteros and W. Croft. Phrasal translation and query
expansion techniques for cross-language information
retrieval. SIGIR Forum, 31:84–91, 1997.
[3] A. Berger and J. Laﬀerty. Information retrieval as
statistical translation. SIGIR, 1999.
[4] P. Brown, V. Pietra, S. Pietra, and R. Mercer. The
mathematics of statistical machine translation: parameter
estimation. CL, 19(2):263–311, 1993.
[5] D. Chiang. Hierarchical phrase-based translation. CL,
33(2):201–228, 2007.
[6] K. Darwish and D. Oard. Probabilistic structured query
methods. SIGIR, 2003.
[7] C. Dyer, J. Weese, H. Setiawan, A. Lopez, F. Ture,
V. Eidelman, J. Ganitkevitch, P. Blunsom, and P. Resnik.
cdec: a decoder, alignment, and learning framework for
ﬁnite-state and context-free translation models. ACL
Demos, 2010.
[8] M. Federico and N. Bertoldi. Statistical cross-language

[22]

[23]

[24]

[25]
[26]

[27]
[28]

816

information retrieval using n-best query translations.
SIGIR, 2002.
J. Gao, J.-Y. Nie, G. Wu, and G. Cao. Dependence
language model for information retrieval. SIGIR, 2004.
J. Gao, J.-Y. Nie, and M. Zhou. Statistical query
translation models for cross-language information retrieval.
TALIP, 5(4):323–359, 2006.
B. Jones, J. Andreas, D. Bauer, K. Hermann, and
K. Knight. Semantics-based machine translation with
hyperedge replacement grammars. COLING, 2012.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst.
Moses: open source toolkit for statistical machine
translation. ACL Demos, 2007.
P. Koehn, F. Och, and D. Marcu. Statistical phrase-based
translation. NAACL-HLT, 2003.
W. Kraaij, J. Nie, and M. Simard. Embedding web-based
statistical translation models in cross-language information
retrieval. CL, 29(3):381–419, 2003.
Y. Liu, R. Jin, and J. Chai. A maximum coherence model
for dictionary-based cross-language information retrieval.
SIGIR, 2005.
A. Lopez. Statistical machine translation. ACM Computing
Surveys, 40(3):8:1–8:49, 2008.
W. Magdy and G. Jones. Should MT systems be used as
black boxes in CLIR? ECIR, 2011.
D. Metzler and W. Croft. A Markov random ﬁeld model for
term dependencies. SIGIR, 2005.
V. Nikoulina, B. Kovachev, N. Lagos, and C. Monz.
Adaptation of statistical machine translation model for
cross-language information retrieval in a service context.
EACL, 2012.
J. Ponte and W. Croft. A language modeling approach to
information retrieval. SIGIR, 1998.
H.-C. Seo, S.-B. Kim, H.-C. Rim, and S.-H. Myaeng.
Improving query translation in English-Korean
cross-language information retrieval. IP&M, 41(3):507–522,
2005.
M. Smucker, J. Allan, and B. Carterette. A comparison of
statistical signiﬁcance tests for information retrieval
evaluation. CIKM, 2007.
F. Ture. Searching to Translate and Translating to Search:
When Information Retrieval Meets Machine Translation.
PhD thesis, University of Maryland, College Park, 2013.
F. Ture, J. Lin, and D. Oard. Looking inside the box:
context-sensitive translation for cross-language information
retrieval. SIGIR, 2012.
D. Wu. A polynomial-time algorithm for statistical machine
translation. ACL, 1996.
J. Xu and R. Weischedel. Empirical studies on the impact
of lexical resources on CLIR performance. IP&M,
41(3):475–487, 2005.
K. Yamada and K. Knight. A syntax-based statistical
translation model. ACL, 2001.
W. Zhang, S. Liu, C. Yu, C. Sun, F. Liu, and W. Meng.
Recognition and classiﬁcation of noun phrases in queries for
eﬀective retrieval. CIKM, 2007.

