A General Evaluation Measure for Document Organization
Tasks
Enrique Amigó

Julio Gonzalo

Felisa Verdejo

E.T.S.I. Informática UNED
Juan del Rosal, 16
Madrid, Spain

E.T.S.I. Informática UNED
Juan del Rosal, 16
Madrid, Spain

E.T.S.I. Informática UNED
Juan del Rosal, 16
Madrid, Spain

enrique@lsi.uned.es

julio@lsi.uned.es

felisa@lsi.uned.es

ABSTRACT

tablishes priority (ranking) and relatedness (clustering) relationships between documents. Let us think of a generic
document organization system as a function from document
pairs d, d0 into one of these possible relationships: , which
means that d has more priority than d0 , and ∼, which means
that d and d0 have some kind of topical equivalence. We will
use the notation k for the cases in which the other two relations do not hold, and the notation {d1 , d2 . . . dn } to indicate
that d1 . . . dn are all related via the topical equivalence relation ∼.
This general problems subsumes:
Document Ranking. This case is illustrated in Table 1.
The gold standard establishes (at least) two priority levels
(relevant versus non-relevant), and the system output returns an ordered list with one priority level per document.
For clarity, in the table we use a bold font for relevant documents. Note that, in this problem, it is assumed that there
is an unlimited amount of irrelevant documents, while the
set of relevant documents is limited. Both the gold standard
and the system output contain, implicitly, an unlimited set
of documents in the last level. The gold standard contains
two priority levels (with the documents manually judged)
while the system output contains as many levels as returned
documents.
Document Filtering. Table 2 illustrates this case; now,
both the gold standard and the system output consist of two
priority levels (relevant and irrelevant).
Document Clustering is exemplified in Table 3. Now
there is only one priority level, and a set of clusters which
contain related documents. Both the gold standard and the
system output have the same form. Table 4 illustrates the
variant of overlapping clustering, where a document may
simultaneously appear in more than one cluster.
Evaluation metrics have been extensively discussed for
each of these tasks. There are, however, many practical
problems where the system must be able both to detect topical relationships (clustering documents) and relative priorities (some clusters are more relevant than others). Let us
give a couple of examples:
Alert detection. A number of practical information
access problems involve detecting, in an incoming stream
of documents, new information that is both novel and of
high priority. Online Reputation Management, for instance,
involves monitoring online information about an entity (a
company, brand, product, person, etc.), clustering texts into
the main topics, and establishing which of them have higher
priority (for instance, those that may potentially damage
the reputation of the entity).

A number of key Information Access tasks – Document Retrieval, Clustering, Filtering, and their combinations – can
be seen as instances of a generic document organization problem that establishes priority and relatedness relationships
between documents (in other words, a problem of forming
and ranking clusters). As far as we know, no analysis has
been made yet on the evaluation of these tasks from a global
perspective. In this paper we propose two complementary
evaluation measures – Reliability and Sensitivity – for the
generic Document Organization task which are derived from
a proposed set of formal constraints (properties that any
suitable measure must satisfy).
In addition to be the first measures that can be applied to
any mixture of ranking, clustering and filtering tasks, Reliability and Sensitivity satisfy more formal constraints than
previously existing evaluation metrics for each of the subsumed tasks. Besides their formal properties, its most salient
feature from an empirical point of view is their strictness:
a high score according to the harmonic mean of Reliability
and Sensitivity ensures a high score with any of the most
popular evaluation metrics in all the Document Retrieval,
Clustering and Filtering datasets used in our experiments.

Categories and Subject Descriptors
B.8 [Performance and Reliability]: General

General Terms
Measurement, Performance

Keywords
IR effectiveness measures

1.

INTRODUCTION

Some key Information Access tasks can be seen as instances of a generic document organization problem that esPermission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

643

Gold Standard
d1 , d2 , d3
d4 , d5 , d6 , d7 ..dn

System Output
d1
d2
d6
d3
d14
d4 , d5 , d7 , ..dn

Gold standard
{d1 , d2 , d3 }, {d4 ,d5 ,d6 }, d7
System output
{d1 ,d2 }, d3 , {d4 , d5 , d6 , d7 }
Table 3: Example of Clustering task.
Gold standard
{d1 , d2 , d3 }, {d4 , d5 , d6 }, {d4 , d7 }
System output
{d1 , d2 }, d3 , {d4 , d5 , d6 , d7 }, {d6 , d7 }

Table 1: Example of Document Ranking task. Vertical ordering indicates relative priorities
Goldstandard
d1 , d2 , d3
d4 , d5 , d6 , d7 , d8
System output
d1 , d2 , d4
d3 , d5 , d6 , d7 , d8

Table 4: Example of overlapping clustering

for the general document organization task that (i) satisfies
formal constraints for all the tasks; (ii) turns into suitable
existing measures when mapped into each of the subsumed
tasks.
Our research leads to propose Reliability and Sensitivity
which are, in short, precision and recall over document pair
relationships established in the gold standard, with a suitable weighting scheme. Comparing them with state of the
art measures for each particular scenario, we find that Reliability and Sensitivity satisfy more formal constraints than
previously existing measures in most cases. In addition, its
harmonic mean is stricter than previously existing measures
– a high score implies high scores with respect to all other
standard measures –, which is an interesting property when
the application scenario does not clearly prescribe a more
specific evaluation measure.
We will start by establishing the set of formal constraints
that any suitable metric should satisfy (Section 2); then we
present our proposed measures (Section 3) and discuss its
application to the different tasks (Section 4). We end by
showing how the measures apply to the more complex document organization task, and discussing the main implications of our results.

Table 2: Example of filtering task

Search Results Organization. Given the set of top
ranked documents retrieved for a query, a mechanism to
group related documents and assign priorities between clusters can be used to improve search results in many ways: (i)
to enhance search results diversity by maximizing the number of highly relevant topics represented in the top results;
(ii) to provide keyword suggestions to refine the query, sampled from the most relevant clusters; (iii) to directly display
search results as a ranked list of relevant topics corresponding to alternative query interpretations, subtopics or facets
in the retrieved documents.
This type of tasks (and others such as composite retrieval)
– which are more complex than the standard ranking and
clustering problems – still match the generic Document Organization task as we have defined it. An example is shown
in Table 5. In the gold standard there is, for instance, one
topic with two documents (d1 ∼ d2) at the top priority level,
and two topics at the second level (d3 ∼ d4 and d5 ∼ d6).
In the next three priority levels there are two single documents and another topic. The example includes overlapping
clusters: for instance, d5 is relevant for two topics, one in
the second priority level and another one in the fourth.
Finally, the (potentially long) list of documents at the bottom of the gold standard ranking (d11 , d12 , d13 , ... dn ) represents irrelevant documents which are not judged in terms
of topical similarity, and are therefore represented via the
empty relationship k.
Evaluation measures have been proposed to evaluate clustering outputs in the context of document retrieval [14, 8,
18] and to include the notion of diversity in search results[15,
24, 9]. But, to the best of our knowledge, no measure has
been previously designed to evaluate the general document
organization problem and all the tasks subsumed by this
one.
In order to find appropriate evaluation measures for the
generic Document Organization problem, we will focus on
the specification of the formal constraints that they should
satisfy in each of the subsumed tasks (filtering, clustering
and ranking). Our goal is finding an evaluation measure

2.

INFORMATION ACCESS MEASURES AND
FORMAL CONSTRAINTS

Our approach to measure design is to start defining a set of
formal constraints, i.e. a set of formal, verifiable properties
that any suitable measure should satisfy. In addition, they
explain the nature of different evaluation measure families.
We will start by reviewing (or proposing) formal constraints
for each of the subsumed tasks, and then merge all collected
constraints into a single list of properties for the generalized
measures. Table 6 summarize the results of this analysis.
Gold standard
{d1 , d2 }
{d3 , d4 }, {d5 , d6 }
d7
{d5 , d8 , d9 }
d10
d11 , d12 , d13 , ... , dn

System output
d1
d2
{d3 , d4 }, {d5 , d6 }
{d5 , d7 , d9 }
d8
d12
d10 , d11 , d13 , ... dn

Table 5: An example of gold standard and system
output for the generic information retrieval task.

644

Measure

Homo.

Comp.

Set matching
Entropy based
Edit distance
Counting pairs
Bcubed
MAP, DCG, Q measure
P10
MRR
RBP
UTILITY, F, LAM%
R*S

4
4
7
4
4

7
4
4
4
4

4

4

Rag
Bag
7
7
7
7
4

Size vs
Quantity
4
4
4
7
7(overlap)

4

4

Priority

Deepness

Deepness
Threshold

Closeness
Threshold

Confidence

4
7
4
4
4
4

4
7
4
4

7
4
4
4

4
4
7
4

7
7
7
7

4

4

4

4

Table 6: Formal constraints satisfied by standard measures and R*S

2.1

Document Clustering

measure, Purity and Inverse Purity), entropy-based measures (Entropy, class Entropy, Mutual Information), measures based on counting pairs and edit distance measures.
Interestingly, there is only one pair of measures, BCubed
Precision and Recall, that satisfies all constraints simultaneously, and forms an independent family. However, we will
show in this paper that the extended version of Bcubed for
overlapping clustering does not satisfy the last constraint.

[2] provides a detailed analysis of clustering evaluation
measures (grouped by families) and the constraints they
should satisfy. We briefly describe here these constraints
and refer to [2] for its justification and formal description.
We will say that the system output produce clusters while
the gold standard is composed by classes. Q represents the
quality of a clustering distribution and a, b, c, d.. are documents from different classes:
Cluster Homogeneity: This restriction was firstly proposed in [22]. Given a certain system output document
distribution, splitting documents that do not belong to the
same class, must increase the output quality:

2.2

Document Filtering

Filtering is a binary classification problem where there is
a relative priority between the two classes: the system must
classify each document as positive or negative, being the
positive class the one that stores the relevant information.
In the filtering scenario all existing measures satisfy a basic
constraint:
Priority Constraint Moving a positive (relevant) document from the predicted negative set to the predicted positive set, or moving a negative (irrelevant) document from
the predicted positive to the predicted negative set must increase the system output quality. Being dr and d¬r a judged
relevant/irrelevant document respectively:

Q({a, a, a}, {b, b}...) > Q({a, a, a, b, b}...)
Although it seems a very basic constraint, in [2] it is shown
that measures based in editing distance do not satisfy it.
Cluster Completeness: The counterpart to the first
constraint is that documents belonging to the same assessed
class should be grouped in the same cluster [2, 22]:
Q({a, a, a, a, a, }...) > Q({a, a, a}, {a, a}...)
Measures based on set matching, such as Purity and Inverse
Purity do not satisfy this contraint.
Rag Bag: This constraint states that introducing disorder into a disordered cluster (rag bag) is less harmful than
introducing disorder into a clean cluster. That is:

Q({dr ..}, {..}) > Q({..}, {dr ..})
Q({..}, {d¬r ..}) > Q({d¬r ..}, {..})
This constraint is satisfied by every standard measure.
Filtering is a basic binary classification task that can be
evaluated in multiple ways, but it is difficult to define objectively desirable boundary constraint that discriminate measures. However, there are descriptive properties which are
mutually exclusive and explain the nature of different measure families [3]. These properties focus on how the metrics
score non-informative outputs D¬inf depending on the size
of the positive set S(D¬inf ). A non-informative output is a
random distribution of the documents that is independent
on the content of the input documents. An ”all positive”
baseline, for instance, is a non-informative output where the
size of the positive set is equivalent to the size of the input
set.
For instance, there is a large set of filtering measures
that assign a fixed score to every non-informative output:
Lam [11], the Macro Average Accuracy [20] and, in general,
measures based on correlation such as the Kappa statistic
[10]. (Q(D¬inf ) = constant). Other family of measures
assumes that, if the filtering system does not know any-

Q({a, a, a, a}, {b, c, d, e}..) > Q({a, a, a, a, b}, {c, d, e}..)
In general, all traditional measures fail to comply with this
constraint.
Cluster size vs. quantity: A small error in a big cluster is preferable to a large number of small errors in small
clusters [2, 19, 22]. This constraint prevents the problem
that measures based on counting pairs [19, 13], overweight
big clusters. That is:
Q({a, a, a, a}, {a}, {b, b}{c, c}{d, d}{e, e}...) >
Q({a, a, a, a, a}, {b}, {b}, {c}, {c}, {d}, {d}, {e}, {e}..)
Measures based on counting pairs are sensitive to the combinatory explosion of pairs in big clusters, failing on this
constraint.
In [2] it is shown how the above contraints discriminate
between four families of evaluation measures for the clustering problem: measures based on set matching (e.g. F

645

thing, returning all is better than removing documents randomly. In this case, the score for a non-informative system is correlated with the size of the positive output set:
(Q(D¬inf ) ∝ S(D¬inf )) This is the case of the harmonic
mean of Precision and Recall over the positive class. Finally, measures such as Accuracy or Utility assign relative
weights to documents in each of the cases in the confusion
matrix; Depending on how these parameters are set and on
the distribution of classes in the input, the optimal positive
class size for a non-informative output varies.

2.3

using it is that in some cases it is advisable to assume a practical deepness threshold. In other words, there is a ranking
area which will never be explored by the user. We can express this as a constraint by saying that there exists a value
n large enough such that retrieving one relevant document
at the top of the rank is better than retrieving n relevant
documents after n irrelevant documents:
Q({r1 , ¬r2 , ¬r3 ..¬r2n }) > Q({¬r1 , ¬r2 ..¬rn−1 , rn ..r2n })
We will not include all the formal proofs on how measures
satisfy this constraint, due to space availability; we will only
discuss the proof that MAP does not satisfy it.
The score for the leftmost distribution in the constraint
is N1r , assuming that there are Nr relevant documents in
the collectionWe can prove that the score for the rightmost
distribution is always higher:

Document Ranking

In order to define a set of constraints for the document
ranking (or document retrieval) task, we have to take into
account some aspects. First, documents at the top of the
ranking must have more weight in the evaluation process:
even if the system is able to sort all documents, the user will
not be able to explore all of them. The sizes of the relevant
and the irrelevant set are expected to be heavily unbalanced:
potentially, the amount of irrelevant documents for a given
query is (in practice) unlimited. Second, traditional document retrieval is a mixture of filtering and ranking tasks:
an optimal system should not only rank the documents, but
also decide on the size of the output set, depending on the
amount of relevant documents in the collection and the selfassessed quality of the system output. These two features
– which are related the fact that the gold standard and the
system output take different forms, unlike the filtering and
clustering problems – make document retrieval evaluation
harder than it seems a priori.
There is a large number of proposed measures in the state
of the art. Some of the most popular are: precision at certain recall levels or ranking positions, AUC [12], MAP (Mean
Average Precision), Discounted Cumulative Gain [16], Expected Reciprocal Rank (ERR)[25], Q-measure, Binary Preference [4], or Rank Biased Precision [21], among many others. Let us analyze them in terms of formal constraints.
First, the Priority Constraint from the filtering problem also applies here (see previous subsection) and it is satisfied by most measures. A notable exception is P@10 (precision at the top ten documents retrieved), given that it is not
sensitive to relationships after the tenth position in the system output ranking, and to internal reorderings in the top
ten setWe can express this constraint in the context of Document Retrieval evaluation as, being r and ¬r relevant and
irrelevant documents respectively, and being {d1 , d2 , d3 ..dn }
an output ranking:

M AP =

i=n
i=n
1 X i
1 X i
>
=
Nr i=1 n + i
Nr i=1 2n

i=n
(n − 1)
1 (n − 1)n
1 X
i=
=
2nNr i=1
2nNr
2
4Nr

which is bigger than N1r for any n > 4. DCG does not comply with this constraint either: DCG for the first distribution
is 1, before normalization. And for the second distribution
we have:
DCG =

i=n
X
i=1

i=n

X
1
n
1
>
=
>1
log2 (i + n)
log
log
2 (2n)
2 (2n)
i=1

For instance, according to MAP or DCG, finding 1,000 relevant documents after 1,000 irrelevant documents is better
than having only one relevant document, but at the top of
the rank. This is counterintuitive in many practical settings.
The Q-measure is an extension of MAP for multigraded
relevance, having a similar behavior. However, the measure ERR does satisfy this constraint (as well as P@10), due
to the strong relevance discount for positions deeper in the
rank. Measures with similar weighting schemes also satisfy
this constraint, but at the cost of failing to satisfy the next
one.
Closeness Threshold Constraint: There exists a (short)
ranking area which is always explored by the user. For instance, we can assume that the top three documents returned by a search engine for informational queries are always inspected. We formalize this constraint as the counter
part of the previous constraint: There exists a value n small
enough such that retrieving one relevant document in the
first position is worse than n relevant documents after n
irrelevant documents:

Q({..ri+1 , ¬ri+2 ..}) > Q({..¬ri+1 , ri+2 ..})
Deepness Constraint: The more we go to a deeper point
in the output ranking, the less the probability of documents
being explored by the user. Therefore, the effect of a document priority relationship in the system quality depends on
the depth of the documents in the ranking. Being i<j:

Q({r1 , ¬r2 , ¬r3 ..¬r2n }) < Q({¬r1 , ¬r2 ..¬rn−1 , rn ..r2n })

Q({..ri , ¬ri+1 ..}) − Q({..¬ri , ri+1 ..}) <

In the case of P@10, n is 9 (i.e. for any n lower than 9, the
constraint is satisfied). ERR, on the other hand, does not
satisfy the constraint: given its strong discount with ranking
depth, one relevant document at the first position has always
more weight than n relevant documents after the position n.
The measures RBP and the discounting function proposed
by Smucker and Clarke [23] satisfy all the previous constraints. The common characteristic of both measures is
that they are based on a probabilistic user behavior model.

Q({..rj , ¬rj+1 ..}) − Q({..¬rj , rj+1 ..})
Measures based on traditional correlation, such as AUC or
Kendall, do not satisfy this contraint, because they give the
same weight to all elements in the ranking. Also, P@10
obviously does not satisfy this contraint.
Deepness Threshold Constraint: Although P@10 does
not satisfy the previous two constraints, one motivation for

646

However, all proposed measures fail on the following constraint.
Confidence Constraint. The output ranking does not
necessarily include all documents in the collection and, therefore, the amount of documents returned is also an aspect of
the system quality. The classical TREC ad-hoc evaluation
does not consider this aspect, given that the length of the
rank is fixed; nevertheless, there is research focused on the
prediction of ranking quality, in order to determine when an
output rank must be shown to the user. We include this
aspect in our constraints by stating that extending the rank
with irrelevant documents should decrease the output score:

will be related to its probability of being inspected by the
user, which is
Prelated at least to its position in the ranking;
(ii) w(d) = d0 w(r(d, d0 )), i.e., the weight of all relations
starting from a document d determines the weight of document d; this restriction prevents the quadratic effect of
counting binary relationships and is related to the ”size vs
quantity” restriction for the clustering problem that we want
to satisfy; and finally (iii) the contribution w(d, d0 ) of each
d0 related to d should be proportional to the weight of d0 .
Then, we can express R and S in terms of weights of single
documents. Being wX (d) the weight of d in X and being
Wx,d the sum weight of documents related with d:
X
WX ,d =
wx (d0 )

Q({d1 , d2 ..dn }) > Q({d1 , d2 ..dn , ¬rn+1 })

d0 /r(d,d0 )∈X

Given that most measures are based on accumulative relevance weighted by the location in the ranking [5], we can
conclude that irrelevant documents at the bottom of the
ranking do not affect the score and the constraint is not satisfied. As far as we know, current evaluation measures do
not consider this aspect.
In summary, for the tasks subsumed under our document
organization problem, the result of our analysis is that (i) in
clustering, only the Bcubed measure satisfies all constraints,
with the exception of one of them in the case of overlapping
clustering; (ii) in the filtering scenario, all measures satisfy
the priority constraint, but behave in very different manners
with respect to how they score non-informative outputs; and
(iii) Finally, in the case of document retrieval, we have detected a few measures that satisfy all constraints except the
last one, but none that satisfies all constraints.
Our goal is finding a measure that can be applied to all
of the subsumed tasks and to any combination of them (i.e.
to the general document organization problem), and that
satisfies all constraints coming from each of the subsumed
tasks. In the next section we introduce our proposal.

3.

we can compute R and S as:
X
wx (d0 )
wx (d)
R(X ) =
P (r(d, d0 ) ∈ G)
Wx,d
0
r(d,d )∈X

S(v) =

X
r(d,d0 )∈G

P (r(d, d0 ) ∈ X )

wg (d0 )
wg (d)
Wg,d

If all documents have the same weight in the distributions, R
and S simply turn into the average R(d) and S(d) associated
to each document:
R(X ) = Avgd P (r(d, d0 ) ∈ G|r(d, d0 ) ∈ X )
S(X ) = Avgd P (r(d, d0 ) ∈ X |r(d, d0 ) ∈ G)

3.1

PROPOSAL: RELIABILITY AND SENSITIVITY

Our proposal consists of two complementary measures,
Reliability and Sensitivity, with a straightforward initial definition. Let us consider a system output X and a gold
standard G, which are both a set of document relationships
r(d, d0 ) ∈ {, ≺, ∼}. The Reliability (R) of relationships in
the system output is the probability of finding them in the
gold standard. Reversely, the Sensitivity (S) of predicted
relationships is the probability of finding them in the system output when they appear in the gold standard. In other
words, R and S are precision and recall of the predicted set
of relationships with respect to the true set of relationships:
R(X ) ≡ P (r(d, d0 ) ∈ G|r(d, d0 ) ∈ X )
S(X ) ≡ P (r(d, d0 ) ∈ X |r(d, d0 ) ∈ G)
We can express Reliability as a sum of probabilities pondered
by the weight of each relationship in X :
X
R(X ) ≡
P (r(d, d0 ) ∈ G)wX (r(d, d0 ))
r(d,d0 )∈X

We want to observe three restrictions on relationship weights:
(i) wX (r(d, d0 )) = f (wX (d), wX (d0 )), i.e., the weight of a relationship is a function of the weights of the documents involved. In Document Retrieval, the weight of a document

647

Estimating Document Weight Discounting

In the generic document organization task we must assume that there exists a virtually unlimited amount of documents in the collection. Therefore, we must weight documents according to their priority in the system output or in
the gold standard. There exist several studies on predicting the weight of a document in the ranking in terms of the
probability to be explored by the user. Most of them are
based on assumptions over user behavior [6, 7, 23]. However, there is no clear consensus yet on how to model the
empirical user behavior. Rather than this, we focus on basic constraints and interpretability as the main criteria to
choose a weighting scheme.
We model the weight of the document in the i position
as the weight integration between i − 1 and i. The first
constraint is that the sum weight for all documents must be
finite. Therefore, we must employ a function with a converging integral. We select i12 because it is a simple, soft decay,
integrable and convergent function. We leave the refinement
of the discounting curve for a latter parameterization step;
ideally, our evaluation measure should be as general as possible, and therefore it must have parameters to establish how
much of the ranking (v.g. the top 10 vs the top 100) carries
on how much of the weight (v.g. 50% or 99% of the overall
score) in the evaluation.
According to i12 , the weight of the document in position i
in the priority ordering is:


Z c+i
1
1
1
wX (d) = c1
dx
=
c
−
(1)
1
2
c+i−1
c+i
c+i−1 x
where c1 is a normalization factor to ensure that the sum is
1. c is another parameter that moves the function in order

amount of irrelevant/discarded documents X¬r (see Table
5). The weight of a single document in the set of prioritized
documents Xr is computed as in Equation 2. The weight of
prioritized documents Xr in the system output is:
Z c+|Xr |
1
c
W (Xr ) = c
=1−
x2
c + |Xr |
c

to give more or less weight to the high priority documents.
Stating that the total sum weight is one:
Z ∞
1
1
dx = c1 = 1
c1
x2
c
c
Therefore, c = c1 . The next constraint is that we should be
able to parameterize the weighting curve in an interpretable
way. Ideally, we want to be able to set two parameters n
and Wn which mean that the first n documents must cover
a Wn weight ratio of the overall evaluation score. For instance, we may want to state that the first 30 positions in
the ranking (n = 30) will have an 80% of the total weight
in the evaluation measure (Wn = 0.8). Therefore:
Z c+n
(1 − Wn )n
1
c
dx = Wn =⇒ c =
2
x
Wn
c

The weight of the long tail of non prioritized documents X¬r
in the system output is:
Z ∞
1
c
WX (X¬r ) = c
=
2
x
c
+
|Xr |
c+|Xr |
The sum weight of documents priority related with d is 1
minus the sum weight of documents in the same priority
level:
X
WX ,d,≺ = 1 −
wx (d0 )

Now, we can estimate the weight of each document in the
system output or in the gold-standard according to Formula
1. Documents at the same priority level share the interval
weight. Being n and n= the amount of documents with
more and equal relevance than d respectively we can estimate the weight of each document as:
Z c+n +n=
c1
1
wX (d) =
dx
(2)
n= c+n
x2

d0 /¬(d0 ≺X d)

The probability P (r(dij , dkl ) ∈ G) for a document relationship between two document occurrences in Xr is computed
as in Equation 3 for both the clustering and priority relationships.
As for the relationships between the unlimited tail X¬r
and documents in Xr , we must consider that all documents
in the infinite set X¬r have the same weight. Therefore,
the finite amount of relevant documents in the long tail has
no effect. Then, any relevant document in Xr is correctly
related with all documents in X¬r if it appears between the
prioritized documents in Gr . According to Equation 3, being
dij the j occurrence of document di in the system output
and being Di its set of occurrences1 :

Smucker and Clarke proposed a discounting model based
on exploration time calibration[23] which considers additional aspects such as the relevance and length of documents.
Actually, this model is compatible with our proposal: we
can incorporate this by replacing the i position of documents
with a time function. We leave this analysis for future work.

3.2

Overlapping Clusters

P (dij G X¬r ) =

As we mentioned earlier, a document may appear in multiple clusters (corresponding, for instance, to different information nuggets in the document), and therefore it may
appear at multiple priority levels. If overlapping between
clusters is allowed, a document has potentially a different
number of occurrences in the gold and system output distribution. If there exists only one instance of d and d0 in both
the gold standard and the system output, the probability of
coocurrence is 1 when the relationships match. Otherwise,
following the extended Bcubed measure proposed in [2], we
assume the best possible correspondence between relationships in X and G. For instance, if two documents are related
in the system output less times than in the gold standard,
then all the predicted relationships are assumed to be correct. Otherwise, the probability is the ratio of gold relationships per system relationships. Formally, being |rG (d, d0 )|
and |rX (d, d0 )| the number of occurrences of r(d, d0 ) in G
and X respectively:
P (r(d, d0 ) ∈ G) =

min(|rG (d, d0 )|, |rX (d, d0 )|)
|rX (d, d0 )|

min(|rG (d, d0 )|, |rX (d, d0 )|)
P (r(d, d ) ∈ X ) =
|rG (d, d0 )|
0

3.3

min(|Di ∩ Gr |, |Di ∩ Xr |)
|Di ∩ Xr |

According to all of this, Reliability over priority relationships can be computed as follows2

R≺ =

X
dij dkl ∈Xr
r(dij ,dkl )∈X

X

P (r(dij , dkl ) ∈ G)wx (di,j )wx (dk,l )
+
Wx,dij ,≺


P (dij g X¬r )wx (dij )W (X¬r )

dij ∈Xr

1
Wx,dij ,≺

1
+
Wx (Xr )

Assuming that the documents in the long tail do not have
clustering relationships with each other, Reliability over clustering relationships can be computed as follows:
X P (dij ∼g dkl )wx (dij )wx (dkl )
R∼ =
+ Wx (X¬r )
Wx,dij ,∼
dij ∈Xr
dkl ∈Xr
dij ∼x dkl

(3)

Sensitivity is computed in the same way, but exchanging
X by G and x by g. The complexity of this computation
is O(n2 ), being n the amount of ranked documents Xr or
prioritized documents Gr .

(4)

Measure Computation

1
For the sake of simplicity, we notate P (dij  X¬r ∈ G) as
P (dij G X¬r )
2
The first component covers the relationships within documents in Xr . The second and third components cover the
relationship Xr → X¬r and X¬r → Xr respectively.

Here we state the method to compute R and S over the
general document organization task. We assume that there
is a set of prioritized documents Xr organized by levels and
clusters and a special, bottom level containing an unlimited

648



Figure 1: Comparing Evaluation Measures over Document Filtering Task.

4.

RELIABILITY AND SENSITIVITY: METAEVALUATION

As far as we know, Reliability and Sensitivity are the first
measures which are applicable to the general document organization task. For this reason, our comparison will focus
on how R and S behave in each of the subsumed tasks (filtering, clustering, ranking) with respect to previously existing
measures.

4.1

Meta-evaluation Criteria

There are many ways of meta-evaluating a new measure.
The most direct one consists of comparing measure scores
vs. human assessments of quality, or system results in some
extrinsic task. Other meta-evaluation criteria focus on the
hability to capture information from limited data sets. Some
examples are discriminativeness [23], statistical significant
differences between systems[21], stability method, swap method,
robustness against noisy data, correlation between rankings
over different data sets [5], etc. The main drawback of these
methods is that a measure can be, for instance, perfectly
discriminative under limited data sets without giving information about quality. As an extreme example, the ranking length can be perfectly discriminative but not useful for
evaluation purposes.
in this study we want to investigate how useful is to evaluate measures in terms of a basic, intuitive set of formal
constraints. According to this, our first meta-evaluation criterion is the ability to satisfy the stated formal constraints.
After that formal analysis, and in order to compare measures empirically over data sets, we will assume that current
standard measures used by the community are, to a certain
extent, reliable: we assume that all of them give some useful information about system quality in certain scenarios.
The problem is that, in most cases, we do not know exactly
the real scenarios in which the system will be employed. In
previous experiments, particularly in the case of clustering
and filtering, it has been shown that there can be a very
low correlation between measure results [3]. Therefore, we
cannot expect to find a measure which is correlated with all
of them. However, we can at least ensure that a high score
according to the measure implies a high score according to
all measures. This is strictness. In other words, a good score
in a reliable measure should ensure a good system regardless of the environmental conditions. Note that strictness
itself is not enough as a meta-evaluation criterion (a measure that always scores zero is the strictest of all). Note
also that strictness could easily be achieved by computing a
harmonic mean of the most popular measures, but we would
have to solve scale-normalization issues and we would end

649

up with a measure that would be hard to interpret, and
would not cover all quality aspects in an homogeneous manner. Therefore, and ad-hoc combination of metrics is not
the best solution to have a strict measure.
Let us quantify Strictness in the following way: we compute, for each measure, all the rank positions obtained by
each system output o ∈ O for each topic and measure. Then
we define strictness as the largest difference between a high
rank assigned by our measure and a low rank assigned by a
traditional measure:
Strictness(m) = −

M axo,m0 (Rankm (o) − Rankm0 (o))
|O|

In order to maintain a correspondence with standard metaevaluation criteria, we will also compute the robustness of
measures in terms of the average Spearman correlation of
system scores across topics. Being Rnk(m, ti ) the ranking
of systems produced by metric m for topic ti :
Robustness(m) = Avgi,j (Spearman(Rnk(m, ti ), Rnk(m, tj )))
We now discuss each of the subsumed tasks.

4.2

Clustering Scenario

In the non overlapped clustering scenario where all documents has the same weight, R and S turn into Bcubed
Precision and Bcubed Recall:
RX ,G ≡ Avgd P (r(d, d0 ) ∈ G|r(d, d0 ) ∈ X ) = BR(X)
SX ,G ≡ Avgd P (r(d, d0 ) ∈ X |r(d, d0 ) ∈ G) = BR(X)
Bcubed measures are the only ones that satisfy all the clustering constraints [2]. However, in the extended version for
overlapping clustering, the Cluster Size vs Quantity restriction is no longer satisfied. This is due to the fact that, in the
extended Bcubed version, all the relationships from one document are computed as a single unit, even when it belongs
to several clusters at the same time. The result is that if two
documents d and d0 are related to each other several times
(e.g. they share more than one information nugget), breaking all these relationships is penalized only once. Therefore,
splitting n clusters can have less effect than splitting one
document from a cluster with size n. The solution provided
by R and S consists of considering the document in each
different cluster as a separate document. BCubed measures
are well-known and have already been studied formally and
empirically and compared with other measures in previous
work [2]. Therefore, we will focus on the other subsumed
tasks for our meta evaluation.

Rob.
Strict.

Accuracy
0.3
-0.91

Utility
0.39
-0.91

Lam%
0.38
-0.96

F Measure
0.49
-0.92

R*S
0.70
-0.78

The measures F, Utility and Accuracy achieve -0.92 of Strictness. There exists a clear difference in robustness between
R*S and other measures. In this scenario, robustness seems
to be correlated with strictness.

Table 7: Robustness and Strictness achieved by Filtering Evaluation Measures, WEPS2 test set

4.3
4.3.1

4.4
4.4.1

Document Filtering Task
Formal Constraints

The Filtering scenario consists of distributing a finite set
of documents into two priority levels (binary classification).
Being Xr and Gr the sets of prioritized documents in the
system output and gold standard respectively, the correct
relationships in the system output are priority relationships
between relevant documents in Xr and irrelevant documents
in ¬Xr . Therefore, R corresponds with:
R(X) = Avgd P (rg (d, d0 )|rx (d, d0 )) =
P (Gr |Xr )P (Xr )P (¬Gr |¬Xr ) + P (¬Gr |¬Xr )P (¬Xr )P (Gr |Xr ) =
P (¬Gr |¬Xr )P (Gr |Xr )(P (Xr ) + P (¬Xr )) = P (¬Gr |¬Xr )P (Gr |Xr )

This corresponds with the product of precisions over positive and negative sets in the output. Analogously, Sensitivity corresponds with the product of Recalls over the positive
and negative sets.
S(X) = P (¬Xr |¬Gr )P (Xr |Gr )
Just like any other filtering measure, the combination of R
and S (using the F measure) satisfies the priority constraint.
Let us denotate the harmonic mean of R and S as R*S. With
respect to how R*S scores non-informative outputs, its behavior is a mixture of the other measure families: it assigns
a zero-score to the all-relevant and all-irrelevant baselines,
because they are not able to distinguish any useful priority
relationship between documents. Other arbitrary partitions
receive scores that depend on how the ground truth partitions the test set.

4.3.2

Formal Constraints

Just like MAP and DCG, Reliability and Sensitivity satisfy the first two constraints, Priority and Deepness. Adding
an incorrect relationship produces a score decrease, and the
effect of an incorrect relationship depends on the deepness of
the related documents in the system output ranking. However, unlike MAP or DCG, Reliability and Sensitivity also
satisfy the third constraint, Deepness Threshold. And, unlike MRR, they also satisfy the fourth constraint, Closeness
Threshold. Due to space constraints, we do not include here
the formal proofs.
Note that the parameters n and Wn offer great flexibility, and permit to accomodate scenarios where only the top
documents in the rank matter (as in Web search) as well
as recall-oriented scenarios, simply adjusting the parameters accordingly. The formal constraints are satisfied for any
paratemer setting. For example, with the setting W30 = 0.8,
ranking 30 relevant documents after 30 irrelevant documents
is worse than retrieving one document in the first position,
but retrieving five relevant documents in the first 10 positions is better than retrieving only one document at the top
1. The cut point is n=20.
As for the confidence constraint, note that the more we
include irrelevant documents in the ranking, the more we include incorrect priority relationships between the priorized
documents and the long tail. We could also satisfy this property with measures like RBP by using a discounting score
for each irrelevant document, but then the nature of RBP
would change and its formal properties would not hold any
longer. Table 6 summarizes the results of the formal constraint analysis.

4.4.2

Empirical meta-evaluation

Document Retrieval

Empirical Meta-evaluation

We have used queries 701 to 750 in the GOV-2 collection,
which were employed in the TREC 2004 Terabyte Track.
The GOV-2 corpus consists of approximately 25 million documents. Therefore, we can assume that the amount of documents in the collection is unlimited for practical purposes.
Relevant documents were manually annotated for each query.
We consider the results of 60 retrieval systems developed by
the participants in the track. Two relevance levels (high and
medium) were considered in the human annotation.
Table 8 shows the strictness and robustness3 of measures.
We consider the strictness of all measures against the standard measures MAP, DCP, P@10, MRR, RBPp=0.8 and
RBPp=0.95 . The strictest measure is R*S with W80 = 30
(-0.58) followed by MRR (-0.63), MAP (-0.74) and P@10 (0.75). However, R*S with W80 = 30 achieves low robustness,
while R*S with n80 = 800 and DCG have higher robustness
at the cost of strictness. It seems that there is a trade-off
between strictness and robustness. A possible explanation
is that the robustness of measures across test cases depends
on the amount of data that is considered for the evalua-

For the filtering scenario we employ the evaluation corpus and system results from the second task in the WePS3
competition [1]. The task consisted of classifying Twitter
entries [17] that contain a company name as relevant when
they do refer to the company and irrelevant otherwise. The
test set includes tweets for 47 companies and the training
set includes 52 company names. For each company, around
400 tweets were retrieved using the company name as query.
The ratio of related tweets per company name is variable,
covering both extremely low and high ratios. We will refer
to a company tweet set retrieved by a query in a time slot
as an input stream or topic. Five research teams participated in the competition, and sixteen runs were evaluated.
The organizers included two baseline systems: the placebo
system (all true) and its opposite (all false).
Figure 1 shows the correspondence between R*S and standard measures employed in different evaluation campaigns;
F represents the harmonic mean of precision and recall over
the positive class. The most relevant fact is that a high
score in all standard measures is necessary to achieve a high
score according to R*S. Table 7 shows the Robustness and
strictness of measures (see Section 4.1) in this test set. The
strictest measure is R*S (-0.78), followed by Lam% (-0.89).

3

Discriminating systems that return short rankings is easy.
In order to avoid this noise, we only consider in this test
those systems that return at least 1000 documents

650

Measure

MAP

DCG

P@10

MRR

Robustness
Strictness

0.55
-0.89

0.60
-0.68

0.3
-0.76

0.4
-0.63

RBP
p=0.8
0.28
-0.92

RBP
p=0.95
0.39
-0.65

R*S
W30
0.35
-0.58

R*S
W800
0.57
-0.73

Table 8: Robustness and strictness of Document Retrieval Evaluation Measures, GOV2 test set.
Gold Standard
d1 {d2 d3 d4 }
{d4 d5 } {d6 d7 }
{d6 d7 }
R≺ =1 S≺ =1
R∼ =1 S∼ =1
System Output 2
d1 {d2 d3 }{ d4 }
{d4 d5 } {d6 d7 }
{d6 d7 }
R≺ =1 S≺ =1
R∼ =1 S∼ =0.8
System Output 4
d1 {d3 d4 }
{d4 d5 } {d6 d7 }
{d6 d7 d8 }
R≺ =0.95 S≺ =0.85
R∼ =0.96 S∼ =0.74

System Output 1
d1 {d2 d3 d4 }
{d4 d5 }{d6 d7 }
d6 d7
R≺ =1 S≺ =1
R∼ =1 S∼ =0.97
System Output 3
d1 {d3 d4 }
{d4 d5 } {d6 d7 }
{d6 d7 }
R≺ =1 S≺ =0.86
R∼ =1 S∼ =0.74
System Output 5
{d6 d7 }
{d4 d5 } {d6 d7 }
d1 {d2 d3 d4 }
R≺ =0.64 S≺ =0.59
R∼ =1 S∼ =1

6.

Table 9: Sensitivity and Reliability examples for the
generic Document Organization scenario.

tion. Therefore, measures focused on the top of the ranking
are less robust. Given that the max function in the strictness definition is very sensitive to outlier systems, we have
also computed strictness considering the 10 maximum differences; results were the same.

5.

GENERIC DOCUMENT ORGANIZATION
SCENARIO

As far as we know, there is no measure that can be directly compared with R*S in the generic document organization scenario as we have defined it. In this section, we
illustrate the behavior of R*S across several instances of
system outputs. See Table 9. The gold standard consists
of seven relevant documents distributed along three priority
levels. Each priority level contains two clusters (or information nuggets) and documents 4,6 and 7 appear in two clusters
and priority levels simultaneously. The Reliability and Sensitivity over priority and clustering relationships have been
computed with W1 0 = 0.8, i.e., we require that the first 10
occurrences represent 80% of the score.
Of course, Reliability and Sensitivity are maximal for the
gold standard. Starting from this, we can identify in the table the following behaviors: (System 1) breaking clusters at
low priority levels decreases slightly S∼ , (System 2) breaking clusters at high priority levels decreases S∼ to a greater
extent; (System 3) Removing one document (d2 ) decreases
priority and clustering sensitivity; (System 4) removing and
adding noisy documents (d2 and d8 ) decreases both sensitivity and reliability scores and in (System 5) we swap all
priority levels. Then, clustering is perfect and the priority R and S decrease, but not to zero. Notice that the 10
first documents have only a 80% of weight in the evaluation.
There exists a long tail of documents from which this set is
priorized.

CONCLUSIONS

In this paper we have discussed how some prominent Information Access tasks – Document Retrieval, Clustering and
Filtering – can be subsumed under a generic Document Organization task that establishes two kinds of binary relationships between documents: relatedness (which forms clusters)
and priority (ranking). We have then introduced two evaluation measures for the document organization problem: Reliability and Sensitivity, which are precision and recall of the
predicted set of relationships with respect to the true relationships, with a specific relative weighting scheme between
relations. The main contribution of this paper is that R and
S can be applied to complex tasks which involve ranking,
clustering and filtering at the same time. An example task
is online reputation monitoring, where systems have to (i)
filter out irrelevant information, (ii) organize relevant information in topics, and (iii) decide which topics have more
priority from the point of view of reputation management.
R and S are able to provide a unique evaluation measure for
this combined problem.
In addition, R and S satisfy all formal constraints in each
of the subsumed tasks; in particular, they satisfy more formal constraints than any previous measure in the Document
Retrieval task, and they are able to accomodate several retrieval scenarios (from precision-oriented to recall-oriented)
via two parameters that establish that the first n levels in
the rank carry on a fraction Wn of the overall quality score.
Our empirical study indicates that R and S are stricter than
standard measures, i.e., a high result with R and S ensures
a high result with any other standard measure in all the
subsumed tasks. That makes R and S a preferable choice in
cases where the application scenario does not clearly point
towards any of the previously existing measures, because it
guarantees that a good result will still hold according to any
other standard measure.
For its application to combined tasks, future work involves
extending the set of binary relations – the general principle
of R and S can be applied to any kind of relations – together
with a a careful analysis on how to assign relative weights to
different types of relations; for instance, in certain application scenarios one type of relation (priority or relatedness)
may dominate and obscure what is going on with the other,
making R and S less transparent and/or useful.
Code to use R and S is available at http://nlp.uned.es.

Acknowledgments
This work has been partially funded by EU FP7 project
Limosine (grant number 288024), a Google Faculty Research
Award (Axiometrics, project Holopedia (grant from the Spanish goverment) and project MA2VICMR (grant from the
government of Comunidad de Madrid).

651

7.

REFERENCES

[1] E. Amigó, J. Artiles, J. Gonzalo, D. Spina, B. Liu, and
A. Corujo. WePS3 Evaluation Campaign: Overview of
the On-line Reputation Management Task. In 2nd
Web People Search Evaluation Workshop (WePS
2010), CLEF 2010 Conference, Padova Italy, 2010.
[2] E. Amigó, J. Gonzalo, J. Artiles, and F. Verdejo. A
comparison of extrinsic clustering evaluation metrics
based on formal constraints. Inf. Retr., 12:461–486,
August 2009.
[3] E. Amigó, J. Gonzalo, and F. Verdejo. A comparison
of evaluation metrics for document filtering. In
Proceedings of CLEF’11, CLEF’11, pages 38–49,
Berlin, Heidelberg, 2011. Springer-Verlag.
[4] C. Buckley and E. M. Voorhees. Retrieval evaluation
with incomplete information. In Proceedings of the
27th annual international ACM SIGIR conference on
Research and development in information retrieval,
SIGIR ’04, pages 25–32, New York, NY, USA, 2004.
ACM.
[5] B. Carterette. System effectiveness, user models, and
user utility: a conceptual framework for investigation.
In Proceedings of the 34th international ACM SIGIR
conference on Research and development in
Information Retrieval, SIGIR ’11, pages 903–912, New
York, NY, USA, 2011. ACM.
[6] B. Carterette, E. Kanoulas, and E. Yilmaz. Simulating
simple user behavior for system effectiveness
evaluation. In CIKM, pages 611–620, 2011.
[7] O. Chapelle and Y. Zhang. A dynamic bayesian
network click model for web search ranking. In
WWW, pages 1–10, 2009.
[8] J. M. Cigarrán, A. Peñas, J. Gonzalo, and F. Verdejo.
Automatic selection of noun phrases as document
descriptors in an fca-based information retrieval
system. In ICFCA, pages 49–63, 2005.
[9] C. L. A. Clarke, M. Kolla, G. V. Cormack,
O. Vechtomova, A. Ashkan, S. Büttcher, and
I. MacKinnon. Novelty and diversity in information
retrieval evaluation. In SIGIR, pages 659–666, 2008.
[10] J. Cohen. A Coefficient of Agreement for Nominal
Scales. Educational and Psychological Measurement,
20(1):37, 1960.
[11] G. Cormack and T. Lynam. Trec 2005 spam track
overview. In Proceedings of the fourteenth Text
Retrieval Conference 8TREC 2005), 2005.
[12] G. V. Cormack and T. R. Lynam. TREC 2005 Spam
Track Overview. In Proceedings of the fourteenth Text
REtrieval Conference (TREC-2005), 2005.
[13] M. Halkidi, Y. Batistakis, and M. Vazirgiannis. On
Clustering Validation Techniques. Journal of
Intelligent Information Systems, 17(2-3):107–145,
2001.
[14] M. A. Hearst and J. O. Pedersen. Reexamining the
cluster hypothesis: Scatter/gather on retrieval results.
pages 76–84, 1996.
[15] B. Hu, Y. Zhang, W. Chen, G. Wang, and Q. Yang.
Characterizing search intent diversity into click
models. In Proceedings of the 20th international
conference on World wide web, WWW ’11, pages
17–26, New York, NY, USA, 2011. ACM.
[16] K. Järvelin and J. Kekäläinen. Cumulated gain-based

[17]

[18]

[19]
[20]
[21]

[22]

[23]

[24]

[25]

652

evaluation of ir techniques. ACM Trans. Inf. Syst.,
20:422–446, October 2002.
B. Krishnamurthy, P. Gill, and M. Arlitt. A few chirps
about twitter. In WOSP ’08: Proceedings of the first
workshop on Online social networks, pages 19–24, New
York, NY, USA, 2008. ACM.
A. Leuski. Evaluating document clustering for
interactive information retrieval. In CIKM, pages
33–40, 2001.
M. Meila. Comparing clusterings. In Proceedings of
COLT 03, 2003.
T. M. Mitchell. Machine learning. McGraw Hill, New
York, 1997.
A. Moffat and J. Zobel. Rank-biased precision for
measurement of retrieval effectiveness. ACM Trans.
Inf. Syst., 27(1):2:1–2:27, Dec. 2008.
A. Rosenberg and J. Hirschberg. V-measure: A
conditional entropy-based external cluster evaluation
measure. In Proceedings of EMNLP-CoNLL 2007,
pages 410–420, 2007.
M. D. Smucker and C. L. Clarke. Time-based
calibration of effectiveness measures. In Proceedings of
the 35th international ACM SIGIR conference on
Research and development in information retrieval,
SIGIR ’12, pages 95–104, New York, NY, USA, 2012.
ACM.
S. Vargas and P. Castells. Rank and relevance in
novelty and diversity metrics for recommender
systems. In 5th ACM Conference on Recommender
Systems (RecSys 2011), pages 109–116, Chicago,
Illinois, October 2011.
E. M. Voorhees. The trec-8 question answering track
report. In In Proceedings of TREC-8, pages 77–82,
1999.

