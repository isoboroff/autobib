Bias-Variance Decomposition of IR Evaluation
1

Peng Zhang1 , Dawei Song1,2 , Jun Wang3 , Yuexian Hou1
Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin University, China
2
The Computing Department, The Open University, United Kingdom
3
Department of Computer Science, University College London, United Kingdom

{darcyzzj, dawei.song2010}@gmail.com, jun_wang@acm.org, yxhou@tju.edu.cn

ABSTRACT

age of queries for which the retrieval performance of a query
model is worse than that of the original query model. The
robustness index is deﬁned as RI(Q) = (n+ − n− )/|Q| [2],
where n+ is the number of queries for which the performance is improved, n− is the number of queries hurt, over
the original model, and |Q| is the number of all queries.
These existing measures have some limitations, as shown
by the example in Table 1. Let us consider model A as
the original query model, as well as regard B and C as two
query expansion models. The example shows that A is less
eﬀective (with a lower MAP), but more stable (with a lower
< Init) than B. In this case, we can not judge which model
(A or B) has the better overall performance (considering
both eﬀectiveness and stability). For comparison of B and
C, both <Init and robustness index can not distinguish the
stability between them. The MAPs of B and C are also
the same. Intuitively, B would seem more stable due to the
less variance of its AP values for diﬀerent queries. However,
the above stability metrics do not take into account such
variance (denoted as V ar in Table 1). For example, one
way of computing the variance of AP for model A can be
[(0.3 − 0.2)2 + (0.1 − 0.2)2 ]/2 = 0.01, which calculates the
derivation of AP from its MAP (i.e., 0.2). In Table 1, V ar
distinguishes the models A, B and C: the smaller V ar can
indicate the better retrieval stability.
Now let us change the AP values of C to 0.32 and 0.11 (for
q1 and q2 respectively), then its MAP, < Init and Robustness Index become 0.215, 0 and 1 respectively, suggesting
C is more stable but less eﬀective than B. We are not sure
which one (B or C) is overall better when considering both
eﬀectiveness and stability. This is mainly because the existing stability metrics are often deﬁned separately from the
eﬀectiveness metric (MAP). There is a lack of a uniﬁed formulation of the eﬀectiveness and stability to allow them to
be looked at in an integrated manner.
In this paper, we present a formulation based on a fundamental concept in Statistics, namely the bias-variance decomposition, to tackle the problem. In a nutshell, we view
the unsatisfactory overall performance as one total error,
which can be decomposed into bias and variance of the AP
values of diﬀerent queries. The bias captures the expected
diﬀerence of the APs from their upper bounds (the best AP
obtained by model T in Table 1). The variance has been
illustrated earlier. The detailed formulation will be given
in the next section. Brieﬂy speaking, the smaller bias or
variance reﬂects the better retrieval eﬀectiveness or stability, respectively. The total error (denoted as Bias2 +V ar)
can reﬂect the overall quality of a model. As an illustration,

It has been recognized that, when an information retrieval
(IR) system achieves improvement in mean retrieval eﬀectiveness (e.g. mean average precision (MAP)) over all the
queries, the performance (e.g., average precision (AP)) of
some individual queries could be hurt, resulting in retrieval
instability. Some stability/robustness metrics have been
proposed. However, they are often deﬁned separately from
the mean eﬀectiveness metric. Consequently, there is a lack
of a uniﬁed formulation of eﬀectiveness, stability and overall retrieval quality (considering both). In this paper, we
present a uniﬁed formulation based on the bias-variance decomposition. Correspondingly, a novel evaluation methodology is developed to evaluate the eﬀectiveness and stability in
an integrated manner. A case study applying the proposed
methodology to evaluation of query language modeling illustrates the usefulness and analytical power of our approach.
Category and Subject Descriptors: H.3.3 [Information
Search and Retrieval]
General Terms: Theory, Measurement, Performance
Keywords: Bias-Variance, Decomposition, Eﬀectiveness,
Stability, Robustness, Evaluation

1.

INTRODUCTION

Recently, it has been noticed that when we are trying
to improve the mean retrieval eﬀectiveness (e.g., measured
by MAP [3]) over all queries, the stability of performance
across diﬀerent individual queries could be hurt. For example, compared with a baseline (using the original query in
the ﬁrst round retrieval), query expansion based on pseudo
relevance feedback can generally achieve better MAP, but it
can hurt the performance for some individual queries [2].
In the literature, various stability (or robustness) measures, e.g., R−Loss [2], Robustness Index [2], and < Init [9]
have been proposed. R − Loss computes the averaged net
loss of relevant documents (due to query expansion failure)
in the retrieved documents. <Init calculates the percentPermission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

1021

Table 2: Examples of Additional Bias-Variance (
ρ)

Table 1: Examples of Bias-Variance (AP)
Model
AP
MAP
Robust Index
< Init
Bias
V ar
Bias2 + V ar

A

B

C

T

q1
q2
0.3 0.1
0.2
0
0
0.25
0.01
0.0725

q1
q2
0.6 0.08
0.34
0
0.5
0.11
0.0646
0.0797

q1
q2
0.65 0.03
0.34
0
0.5
0.11
0.0961
0.182

q1
q2
0.7 0.2
0.45
1
0
0
0.0625
0.0625

Model
ρ
Bias
V ar
Bias2 + V ar

A
q1
q2
0.4 0.1
0.25
0.0225
0.0850

B
q1
q2
0.1 0.12
0.11
0.0001
0.0122

C
q1
0.05

q2
0.17 0
0.11
0.0036
0.0157

T
q1
0

q2
0
0
0
0

We now add up the bias and variance, yielding:
Bias2 (AP) + V ar(AP)
= [E(APT ) − E(AP)]2 + E(AP − E(AP))2

in Table 1, Bias2 +V ar indicates that (1) the target model
T has the best overall retrieval quality; (2) the model B is
more desirable than C; (3) in comparison with the baseline
A, both query expansion models B and C reduce the bias
but enlarge the variance as well as the total error.
Recently, Wang and Zhu [7] studied the mean-variance
analysis regarding the document relevance scores. The pertopic variance of AP (of each query) was investigated in [6].
In this paper, we explore the variance of AP (across queries)
and the bias-variance decomposition.

= E(AP − MAPT )

(3)

2

It turns out E(AP − MAPT )2 is not exactly the expected
squared error E(AP − APT )2 . However, we will show later
that E(AP − MAPT )2 is a simpliﬁed version of an expected
squared error E(AP−1)2 in Section 2.3. This error formulation will help us understand the diﬀerence between diﬀerent
bias-variance decompositions (see Section 2.3).

2.2 Additional Bias-Variance
2.

To clarify the above observation, let us ﬁrst look at the
decomposition of the expected squared error E(AP−APT )2 .
We need to deﬁne another random variable

BIAS-VARIANCE DECOMPOSITION OF
RETRIEVAL PERFORMANCE

In Statistics [1], bias and variance, which are measurements of estimation quality in diﬀerent aspects, are decomposed from the expected squared error of the estimated values with respect to the target value. In this paper, we formulate the bias-variance decomposition in the IR evaluation
scenario. Let us ﬁrst assume there exists a target model T
that can have an upper-bound performance for each query 1 .

ρ = APT − AP

As an illustration, for the model A in Table 2, ρ is 0.4 (0.70.3) and 0.1 (0.2-0.1), for q1 and q2 respectively.
Let ρT be the target value of ρ, which is indeed 0, since for
model T, ρ = APT − APT =0. We need ρT in the following
analysis, since the target value is an important component
in the standard deﬁnition of bias. We deﬁne

2.1 Bias-Variance of AP
We can let AP be a random variable over queries. Bias(AP)
essentially calculates the average diﬀerence between the target AP (i.e. the AP of the target model T, denoted APT )
and the actual AP of a retrieval model over all diﬀerent
queries. Speciﬁcally,
Bias(AP) = E(APT − AP) = E(APT ) − E(AP)

(1)

(5)

where E(
ρ) is the averaged ρ over all queries. Since ρT is 0,
Bias(
ρ) equals to E(
ρ) = E(APT −AP), which is Bias(AP).
The variance based on ρ, denoted V ar(
ρ), computes the
variance of the diﬀerence between the AP (of the test model)
and the AP target across all queries. Formally,
(6)

As shown in Table 2, V ar(
ρ) of B and C are smaller than
V ar(
ρ) of A, indicating that B and C are more stable than
A. This observation is diﬀerent from < Init and V ar(AP)
which shows that B and C are less stable than A in Table 1.
Now, we derive the decomposition of E(AP − APT )2 :
E(AP − APT )2 = E(
ρ − ρ T )2
= E(
ρ − E(
ρ))2 + (E(
ρ) − ρT )2

(7)

2

= V ar(
ρ) + Bias (
ρ)
The above equations show the expected squared error
E(AP − APT )2 can be exactly decomposed into bias and
variance on ρ. The expected squared error E(AP − APT )2
always computes the error of the target model as zero, no
matter whether or not the target model still has room for
improvement. It is likely that the current best performance
for some queries can be further advanced in the near future.

(2)

The smaller V ar(AP) indicates the better retrieval stability
of AP across queries.
1

Bias(
ρ) = E(
ρ) − ρ T

V ar(
ρ) = E(
ρ − E(
ρ))2

where the expectation E(·) is over a set of queries that are
assumed to be uniformly distributed. E(APT ) corresponds
to the target MAP (i.e., the MAP of the target model T,
denoted MAPT ), and E(AP) corresponds to the MAP of
the retrieval model under evaluation.
It turns out that the smaller bias indicates the better mean
retrieval eﬀectiveness over queries. In Table 1, the bias for
model A is 12 [(0.7 − 0.3) + (0.2 − 0.1)] = 0.25. According to
Eq. 1, it can be equivalently calculated as 0.45-0.2 = 0.25,
where 0.45 is the target MAP and 0.2 is the MAP of A.
Similarly, the variance of the APs for diﬀerent queries is
deﬁned as:
V ar(AP) = E(AP − E(AP))2

(4)

We will relax this constraint in Sections 2.4.

1022

2.3 Further Investigation on Decomposition of
Expected Squared Error

models. The third setting can correspond to a real target
model (see the model settings in the experiments.) With
the ﬁrst and second upper-bound settings, either of biasIn order to further investigate two aforementioned biasvariance metrics (based on AP or ρ) can be adopted. The
variance decompositions, we set 1 (the maximum AP) as the
third setting is suitable for bias-variance of AP (see Secupper-bound AP for each query. We can have an expected
tion 2.4). Once an upper-bound setting and a variable (AP
squared error E(AP − 1)2 and its decomposition as:
or ρ) are chosen, we can calculate bias-variance ﬁgures for
E(AP − 1)2
a series of models and then see which model can have the
minimum bias, or minimum variance, or the sum of them
= E(AP − APT + APT − 1)2
(i.e., the total error). We can also get an overview on the
= E(AP − APT )2 + E(APT − 1)2 + 2E(AP − APT )(APT − 1) trend of bias and variance over parameters.
(8)

3. EMPIRICAL EVALUATION

which shows that E(AP−APT )2 is only one part of E(AP−
1)2 , and the target model T still has an error E(APT − 1)2 ,
suggesting there is still a room for improvement.
We can also decompose E(AP − 1)2 as:

We use the query modeling as an example of bias-variance
evaluation. Due to the page limit, we only report the evaluation results on two standard TREC collections, i.e., WSJ
(87-92) over queries 151-200 and ROBUST 2004 over queries
601-700. The title ﬁeld of the queries is used. Lemur 4.7 [5]
is used for indexing and retrieval. The top n = 30 ranked
documents in the initial ranking by the original query model
are selected as the pseudo-relevance feedback (PRF) documents. The number of expanded terms is ﬁxed as 100. 1000
documents are retrieved by the KL-divergence model [5].
Model Settings We customize a number of query models according to three factors (i.e., model complexity, model
combination, ground-truth data) that can generally inﬂuence the bias-variance tradeoﬀ [1]. The ﬁrst model is the
original query model which is a maximum likelihood estimation of original query. We also evaluate an expanded
query model RM (i.e., RM1 in [4]) which is generated from
feedback documents D. Compared with the original query
model, the expanded query model is more complex due to
the fact that it has more terms and additional assumptions (e.g., assuming that all feedback documents are relevant). The above two models can be combined, leading to a
combined query model (also called as RM3). Let λ be the
combination coeﬃcient which is in the range (0,1). When
λ gets close to 0, the combined model is towards the RM, and otherwise towards the original model. In RM, we
can gradually remove a percentage (denoted by rn ) of nonrelevant documents DN along the initial ranking of feedback
documents, based on the available relevance judgements (as
ground-truth) [8]. We ﬁnally derive a target model which
are generated by keeping only relevant documents DR in D:

p(w|θd )
(10)
p(w|θq(t) ) ∝

E(AP − 1)2
= E[AP − E(AP) + E(AP) − 1]2
= E(AP − E(AP))2 + [E(AP) − 1]2
= V ar(AP) + (MAP − 1)

(9)

2

= (1 − MAP)2 + V ar(AP)
It turns out the variance parts in Eq. 3 and Eq. 9 are the
same (i.e., V ar(AP)). The term (1 − MAP)2 in Eq. 9 has
the same trend as Bias2 (AP) (i.e., (MAPT −MAP)2 ), where
MAPT is the upper bound of the MAP. The above observations show that the decomposition in Eq. 3 can be considered
as a simpliﬁed version of the decomposition in Eq. 9.

2.4 Comparison between Two Bias-Variance
First, the bias-variance of ρ requires a target AP for every query. On the other hand, the bias-variance of AP (in
Eq. 3) can be used when only a target MAP (i.e., MAPT ) is
given. Speciﬁcally, two biases (i.e., Bias(AP) and Bias(
ρ))
are equivalent and both can be computed by MAPT −MAP.
Regarding variance, the variance of ρ requires APT for each
query (see Eq. 6 and Eq. 4), while the variance of AP can
be calculated without knowing APT (see Eq. 2). Thus, the
bias-variance based on AP is more ﬂexible for practical use.
Second, the bias-variance of ρ is an exact decomposition of
E(AP − APT )2 and it always regards the target model as a
zero-error model. However, the decomposition of E(AP−1)2
in Eq. 8 shows that the target model can still has error. On
the other hand, under the bias-variance of AP, the variance
for the target model still exists, indicating that although we
can assume a target model as an upper-bound at a certain
stage, it can be further improved.

d∈DR

which gives the best MAP over the aforementioned query
models. In Eq. 10, the document weights are set as uniform (diﬀerent from the weights in RM), since the relevant
judgements of relevant documents are the same (i.e., 1).
Bias-Variance Metrics Setting We report the biasvariance on AP in our evaluation, since the target query
model used in our evaluation only guarantees the upperbound MAP. According to the discussion in Sections 2.4
and 2.5, the bias-variance of AP is more suitable.
Evaluation Results We ﬁrst look at the bias-variance
results in Figure 1. Recall that the smaller bias or variance
(based on AP) corresponds to the better retrieval eﬀectiveness or stability, respectively. Figure 1(a) shows that on
WSJ8792, the original query model has the smallest variance (the highest stability), but the largest bias (the lowest
MAP). This is a tradeoﬀ between bias and variance. One

2.5 Bias-Variance Evaluation
To carry out IR evaluation based on the proposed the
bias-variance decomposition methods, one needs to choose
the upper-bound settings (i.e., the target model T). There
can be a number of ways. First, the upper-bound AP for
each query can be simply set as 1, which corresponds to a
perfect target model. Second, the upper-bound AP for each
query can be obtained based on the best AP among evaluated retrieval models in the historic TREC results. Third,
one can simply set an upper-bound MAP (i.e., MAPT ).
The ﬁrst and second upper-bound settings correspond to
a virtual target model. For example, in the second setting,
the target model collects the best AP among many retrieval

1023

Bias−Variance (AP)

Bias−Variance (AP)

0.066

0.06

Original Model

0.062

Combinaed Model

0.056

Removing DN

0.054

Target Model
Var(AP)

Var(AP)

0.058

Expanded Model RM

0.064

0.06

0.058

0.052
0.05
0.048

Original Model

0.046

Expanded Model RM
Combinaed Model

0.044

0.056

Removing DN
0.042

0.054

0

0.005

0.01

0.015

0.02

0.025

0.03

0.04

0.035

Target Model
0

0.005

0.01

0.015

0.02

2

0.025

0.03

0.035

0.04

0.045

2

Bias (AP)

Bias (AP)

(a) WSJ8792

(b) ROBUST2004

Figure 1: Results of Bias and Variance of AP of all the concerned query models. The x-axis shows the squared
bias and the y-axis shows the variance.

4.

Bias−Variance (AP)

Bias−Variance (AP)
0.085

0.09

0.08

Bias2(AP)+Var(AP)

0.085

Bias2(AP)+Var(AP)

reason in Statistics language is that the expanded models are
all complex than the original one. The bias of the expanded
model RM is much smaller, while its variance is much larger, than the original model. The diﬀerent variants of the
combined model (with λ in [0.1, 0.9] with increment 0.1) are
lying between the expanded and original models. We can
also see that 2 (out of 9) combined models (when λ=0.1 and
0.2) has smaller bias and smaller variance than the expanded query model RM. This suggests that the combined query
model with good parameters can achieve better eﬀectiveness
and stability over RM. Among the combined query models, bias and variance are negatively correlated, indicating a
bias-variance tradeoﬀ occurs. On the other hand, if we remove the non-relevant documents DN (with rn in [0.1, 1]
with increment 0.1) in RM, the bias and variance are positively correlated, and 9 (out of 10) models (when rn = 0.2
to 1) can reduce both the bias and variance over RM. The
target model has zero bias, although there is a variance of
its AP across diﬀerent queries. On ROBUST2004, the results are similar. The diﬀerences are that: 1) by removing
non-relevant documents, only 6 (out of 10) models (when
rn = 0.5 to 1) reduce both bias and variance over RM; 2)
there are 3 (out of 9) (when λ = 0.1 to 0.3) variants of the
combined model for which both bias and variance can be
smaller than those of RM.
Figure 2 shows Bias2 + V ar results for all the concerned
models. On both collections, the target method achieves the
smallest Bias2 + V ar and the original query model achieves
the largest Bias2 +V ar. Recall that Bias2 +V ar represents
the total error and the smaller error can reﬂect the better
overall quality (or performance), by considering both the
eﬀectiveness and stability in one single criterion (Bias2 +
V ar). The results also suggest that the model combination
and the relevance judgements are helpful to reduce Bias2 +
V ar over the original model and the expanded model RM.

0.08

0.075

Original Model
Expanded Model RM

0.07

0.075

0.07

Original Model
0.065

Expanded Model RM

Combinaed Model

Combinaed Model

Removing DN

0.065

0.06

Removing DN

Target Model
0.06

0

0.005

0.01

0.015

0.02

0.025

0.03

Target Model
0.035

0.055

0

0.005

0.01

2
Bias (AP)

0.015

0.02

0.025

0.03

0.035

0.04

2
Bias (AP)

(a) WSJ8792

(b) ROBUST2004
2

Figure 2: Results of Bias and Bias2 +V ar of AP of all
the concerned query models. The x-axis shows the
squared bias and the y-axis shows the Bias2 + V ar.
test with more retrieval models and explore the deep insights
behind the bias-variance trends.

5. ACKNOWLEDGMENTS
This work is supported in part by the Chinese National Program on Key Basic Research Project (973 Program,
grant No. 2013CB329304), the Natural Science Foundation
of China (grant No. 61272265, 61070044), and EU’s FP7
QONTEXT project (grant No. 247590).

6. REFERENCES
[1] C. M. Bishop. Pattern Recognition and Machine Learning.
Springer-Verlag New York, Inc., 2006.
[2] K. Collins-Thompson. Reducing the risk of query expansion
via robust constrained optimization. In CIKM ’09, pages
837–846, 2009. ACM.
[3] G. V. Cormack and T. R. Lynam. Statistical Precision of
Information Retrieval Evaluation. In SIGIR ’06, pages
533–540, 2006. ACM.
[4] V. Lavrenko and W. B. Croft. Relevance-based language
models. In SIGIR ’01, pages 120–127, 2001.
[5] P. Ogilvie and J. Callan. Experiments using the lemur
toolkit. In TREC ’02, pages 103–108, 2002.
[6] S. E. Robertson and E. Kanoulas. On per-topic variance in
ir evaluation. In SIGIR ’12, pages 891–900, 2012.
[7] J. Wang and J. Zhu. Portfolio theory of information
retrieval. In SIGIR ’09, pages 115–122, 2009.
[8] P. Zhang, Y. Hou and D. Song. Approximating true
relevance distribution from a mixture model based on
irrelevance data. In SIGIR ’09, pages 107–114, 2009.
[9] L. Zighelnic and O. Kurland. Query-drift prevention for
robust query expansion. In SIGIR ’08, pages 825–826, 2008.

CONCLUSIONS AND FUTURE WORK

In this paper, a novel evaluation strategy based on the
bias-variance decomposition has been presented. We formulate two forms of the bias-variance decomposition and
theoretically compare them. We also use query expansion
as an example to demonstrate the use of the bias-variance
for IR evaluation and analysis, in terms of bias, variance or
sum of them. In the future, we will further explore the other
upper-bound settings and variables discussed in Section 2.5,

1024

0.045

