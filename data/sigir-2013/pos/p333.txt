Ranking Document Clusters Using Markov Random Fields
Fiana Raiber
fiana@tx.technion.ac.il

Oren Kurland
kurland@ie.technion.ac.il

Faculty of Industrial Engineering and Management, Technion
Haifa 32000, Israel

ABSTRACT
An important challenge in cluster-based document retrieval
is ranking document clusters by their relevance to the query.
We present a novel cluster ranking approach that utilizes
Markov Random Fields (MRFs). MRFs enable the integration of various types of cluster-relevance evidence; e.g., the
query-similarity values of the cluster’s documents and queryindependent measures of the cluster. We use our method to
re-rank an initially retrieved document list by ranking clusters that are created from the documents most highly ranked
in the list. The resultant retrieval effectiveness is substantially better than that of the initial list for several lists that
are produced by effective retrieval methods. Furthermore,
our cluster ranking approach significantly outperforms stateof-the-art cluster ranking methods. We also show that our
method can be used to improve the performance of (stateof-the-art) results-diversification methods.
Categories and Subject Descriptors: H.3.3 [Information Search
and Retrieval]: Retrieval models
General Terms: Algorithms, Experimentation
Keywords: ad hoc retrieval, cluster ranking, query-specific clusters, markov random fields

1.

INTRODUCTION

The cluster hypothesis [33] gave rise to a large body of
work on using query-specific document clusters [35] for improving retrieval effectiveness. These clusters are created
from documents that are the most highly ranked by an initial search performed in response to the query.
For many queries there are query-specific clusters that
contain a very high percentage of relevant documents [8,
32, 25, 14]. Furthermore, positioning the constituent documents of these clusters at the top of the result list yields
highly effective retrieval performance; specifically, much better than that of state-of-the art retrieval methods that rank
documents directly [8, 32, 25, 14, 10].
As a result of these findings, there has been much work on
ranking query-specific clusters by their presumed relevance

to the query (e.g., [35, 22, 24, 25, 26, 14, 15]). Most previous
approaches to cluster ranking compare a representation of
the cluster with that of the query. A few methods integrate
additional types of information such as inter-cluster and
cluster-document similarities [18, 14, 15]. However, there
are no reports of fundamental cluster ranking frameworks
that enable to effectively integrate various information types
that might attest to the relevance of a cluster to a query.
We present a novel cluster ranking approach that uses
Markov Random Fields. The approach is based on integrating various types of cluster-relevance evidence in a principled manner. These include the query-similarity values of
the cluster’s documents, inter-document similarities within
the cluster, and measures of query-independent properties
of the cluster, or more precisely, of its documents.
A large array of experiments conducted with a variety of
TREC datasets demonstrates the high effectiveness of using
our cluster ranking method to re-rank an initially retrieved
document list. The resultant retrieval performance is substantially better than that of the initial ranking for several
effective rankings. Furthermore, our method significantly
outperforms state-of-the-art cluster ranking methods. Although the method ranks clusters of similar documents, we
show that using it to induce document ranking can help to
substantially improve the effectiveness of (state-of-the-art)
retrieval methods that diversify search results.

2. RETRIEVAL FRAMEWORK
Suppose that some search algorithm was employed over
a corpus of documents in response to a query. Let Dinit be
the list of the initially highest ranked documents. Our goal
is to re-rank Dinit so as to improve retrieval effectiveness.
To that end, we employ a standard cluster-based retrieval
paradigm [34, 24, 18, 26, 15]. We first apply some clustering method upon the documents in Dinit ; C l(Dinit ) is the
set of resultant clusters. Then, the clusters in C l(Dinit ) are
ranked by their presumed relevance to the query. Finally,
the clusters’ ranking is transformed to a ranking of the documents in Dinit by replacing each cluster with its constituent
documents and omitting repeats in case the clusters overlap.
Documents in a cluster are ordered by their query similarity.
The motivation for employing the cluster-based approach
just described follows the cluster hypothesis [33]. That is,
letting similar documents provide relevance status support
to each other by the virtue of being members of the same
clusters. The challenge that we address here is devising a
(novel) cluster ranking method — i.e., we tackle the second
step of the cluster-based retrieval paradigm.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

333

Figure 1: The three types of cliques considered for graph G. G is composed of a query node (Q) and three
(for the sake of the example) nodes (d1 , d2 , and d3 ) that correspond to the documents in cluster C. (i) lQD
contains the query and a single document from C; (ii) lQC contains all nodes in G; and, (iii) lC contains only
the documents in C.
Formally, let C and Q denote random variables that take
as values document clusters and queries respectively. The
cluster ranking task amounts to estimating the probability
that a cluster is relevant to a query, p(C|Q):
p(C|Q) =

p(C, Q) rank
= p(C, Q).
p(Q)

specifically, its clique set L(G); and, (ii) associate feature
functions with the cliques. We next address these two tasks.

2.1.1 Cliques and feature functions

(1)

The rank equivalence holds as clusters are ranked with respect to a fixed query.
To estimate p(C, Q), we use Markov Random Fields (MRFs).
As we discuss below, MRFs are a convenient framework for
integrating various types of cluster-relevance evidence.

2.1 Using MRFs to rank document clusters
An MRF is defined over a graph G. Nodes represent
random variables and edges represent dependencies between
these variables. Two nodes that are not connected with an
edge correspond to random variables that are independent of
each other given all other random variables. The set of nodes
in the graph we construct is composed of a node representing
the query and nodes representing the cluster’s constituent
documents. The joint probability over G’s nodes, p(C, Q),
can be expressed as follows:
Q
l∈L(G) ψl (l)
;
(2)
p(C, Q) =
Z
L(G) is the set of cliques in G and l is a clique; ψl (l)
is
(i.e., positive function) defined over l; Z =
P a potential
Q
C,Q
l∈L(G) ψl (l) is the normalization factor that serves
to ensure that p(C, Q) is a probability distribution. The
normalizer need not be computed here as we rank clusters
with respect to a fixed query.
A common instantiation of potential functions is [28]:

We consider three types of cliques in the graph G. These
are depicted in Figure 1. In what follows we write d ∈ C to
indicate that document d is a member of cluster C.
The first clique (type), lQD , contains the query and a single document in the cluster. This clique serves for making
inferences based on the query similarities of the cluster’s
constituent documents when considered independently. The
second clique, lQC , contains all nodes of the graph; that is,
the query Q and all C’s constituent documents. This clique
is used for inducing information from the relations between
the query-similarity values of the cluster’s constituent documents. The third clique, lC , contains only the cluster’s constituent documents. It is used to induce information based
on query-independent properties of the cluster’s documents.
In what follows we describe the feature functions defined
over the cliques. In some cases a few feature functions are
defined for the same clique, and these are used in the summation in Equation 3. Note that the sum of feature functions
is also a feature function. The weights associated with the
feature functions are set using a train set of queries. (Details
are provided in Section 4.1.)

The lQD clique. High query similarity exhibited by C’s
constituent documents can potentially imply to C’s relevance [26]. Accordingly, let d (∈ C) be the document in
def

1

lQD . We define fgeo−qsim;lQD (lQD ) = log sim(Q, d) |C| ,
where |C| is the number of documents in C, and sim(·, ·) is
some inter-text similarity measure, details of which are provided in Section 4.1. Using this feature function in Equation
3 for all the lQD cliques of G amounts to using the geometric
mean of the query-similarity values of C’s constituent documents. All feature functions that we consider use logs so as
to have a conjunction semantics for the integration of their
assigned values when using Equation 3.1

def

ψl (l) = exp(λl fl (l)),
where fl (l) is a feature function defined over the clique l
and λl is the weight associated with this function. Accordingly, omitting the normalizer from Equation 2, applying the
rank-preserving log transformation, and substituting the potentials with the corresponding feature functions results in
our ClustMRF cluster ranking method:
X
rank
p(C|Q) =
λl fl (l).
(3)

The lQC clique. Using the lQD clique from above results
in considering the query-similarity values of the cluster’s
documents independently of each other. In contrast, the
lQC clique provides grounds for utilizing the relations between these similarity values. Specifically, we use the log

l∈L(G)

This is a generic linear (in feature functions) cluster ranking
function that depends on the graph G. To instantiate a specific ranking method, we need to (i) determine G’s structure,

1

Before applying the log function we employ add-ǫ (=
10−10 ) smoothing.

334

of the minimal, maximal, and standard deviation2 of the
{sim(Q, d)}d∈C values as feature functions for lQC , denoted
min-qsim, max-qsim, and stdv-qsim, respectively.

dsim, could have been described in an alternative way. That
1
is, using log P(d) |C| as a feature function over a clique containing a single document. Then, using these feature functions in Equation 3 amounts to using the geometric mean.3

The lC clique. Heretofore, the lQD and lQC cliques served
for inducing information from the query similarity values of
C’s documents. We now consider query-independent properties of C that can potentially attest to its relevance. Doing so
amounts to defining feature functions over the lC clique that
contains C’s documents but not the query. All the feature
functions that we define for lC are constructed as follows.
We first define a query-independent document measure, P,
and apply it to document d (∈ C) yielding the value P(d).
Then, we use log A({P(d)}d∈C ) where A is an aggregator
function: minimum, maximum, and geometric mean. The
resultant feature functions are referred to as min-P, maxP, and geo-P, respectively. We next describe the document
measures that serve as the basis for the feature functions.
The cluster hypothesis [33] implies that relevant documents should be similar to each other. Accordingly, we measure for document d in C its similarity with all documents
def 1 P
in C: Pdsim (d) = |C|
di ∈C sim(d, di ).
The next few query-independent document measures are
based on the following premise. The higher the breadth of
content in a document, the higher the probability it is relevant to some query. Thus, a cluster containing documents
with broad content should be assigned with relatively high
probability of being relevant to some query.
High entropy of the term distribution in a document is a
potential indicator for content breadth [17, 3]. This is because the distribution is “spread” over many terms rather
than focused over a few ones. Accordingly, we define
P
def
Pentropy (d) = − w∈d p(w|d) log p(w|d), where w is a term
and p(w|d) is the probability assigned to w by an unsmoothed
unigram language model (i.e., maximum likelihood estimate)
induced from d.
Inspired by work on Web spam classification [9], we use
the inverse compression ratio of document d, Picompress (d),
as an additional measure. (Gzip is used for compression.)
High compression ratio presumably attests to reduced content breadth [9].
Two additional content-breadth measures that were proposed in work on Web retrieval [3] are the ratio between the
number of stopwords and non-stopwords in the document,
Psw1 (d); and, the fraction of stopwords in a stopword list
that appear in the document, Psw2 (d). We use INQUERY’s
stopword list [2]. A document containing many stopwords
is presumably of richer language (and hence content) than
a document that does not contain many of these; e.g., a
document containing a table composed only of keywords [3].
For some of the Web collections used for evaluation in
Section 4, we also use the PageRank score [4] of the document, Ppr (d), and the confidence level that the document is
not spam, Pspam (d). The details of the spam classifier are
provided in Section 4.1.
We note that using the feature functions that result from
applying the geometric mean aggregator upon the queryindependent document measures just described, except for

3. RELATED WORK
The work most related to ours is that on devising cluster
ranking methods. The standard approach is based on measuring the similarity between a cluster representation and
that of the query [7, 34, 35, 16, 24, 25, 26]. Specifically, a
geometric-mean-based cluster representation was shown to
be highly effective [26, 30, 15]. Indeed, ranking clusters by
the geometric mean of the query-similarity values of their
constituent documents is a state-of-the-art cluster ranking
approach [15]. This approach rose as an integration of feature functions used in ClustMRF, and is shown in Section 4
to substantially underperform ClustMRF.
Clusters were also ranked by the highest query similarity exhibited by their constituent documents [22, 31] and by
the variance of these similarities [25, 19]. ClustMRF incorporates these methods as feature functions and is shown to
outperform each.
Some cluster ranking methods use inter-cluster and clusterdocument similarities [14, 15]. While ClustMRF does not
utilize such similarities, it is shown to substantially outperform one such state-of-the-art method [15].
A different use of clusters in past work on cluster-based
retrieval is for “smoothing” (enriching) the representation of
documents [20, 16, 24, 13]. ClustMRF is shown to substantially outperform one such state-of-the-art method [13].
To the best of our knowledge, our work is first to use
MRFs for cluster ranking. In the context of retrieval tasks,
MRFs were first introduced for ranking documents directly
[28]. We show that using ClustMRF to produce document
ranking substantially outperforms this retrieval approach;
and, that which augments the standard MRF retrieval model
with query-independent document measures [3]. MRFs were
also used, for example, for query expansion, passage-based
document retrieval, and weighted concept expansion [27].

4. EVALUATION
4.1 Experimental setup
corpus
AP

# of docs
242,918

data
Disks 1-3

ROBUST

528,155

Disks 4-5 (-CR)

WT10G
GOV2
ClueA
ClueAF
ClueB
ClueBF

1,692,096
25,205,179

WT10g
GOV2

queries
51-150
301-450,
600-700
451-550
701-850

503,903,810

ClueWeb09 (Category A)

1-150

50,220,423

ClueWeb09 (Category B)

1-150

Table 1: Datasets used for experiments.
The TREC datasets specified in Table 1 were used for
experiments. AP and ROBUST are small collections, composed mostly of news articles. WT10G and GOV2 are Web

2
It was recently argued that high variance of the querysimilarity values of the cluster’s documents might be an indicator for the cluster’s relevance, as it presumably attests
to a low level of “query drift” [19].

3
Similarly, we could have used the geometric mean of the
query-similarity values of the cluster constituent documents
as a feature function defined over the lQC clique rather than
constructing it using the lQD cliques as we did above.

335

collections; the latter is a crawl of the .gov domain. For
the ClueWeb Web collection both the English part of Category A (ClueA) and the Category B subset (ClueB) were
used. ClueAF and ClueBF are two additional experimental
settings created from ClueWeb following previous work [6].
Specifically, documents assigned by Waterloo’s spam classifier [6] with a score below 70 and 50 for ClueA and ClueB,
respectively, were filtered out from the initial corpus ranking described below. The score indicates the percentage of
all documents in ClueWeb Category A that are presumably
“spammier” than the document at hand. The ranking of the
residual corpus was used to create the document list upon
which the various methods operate. Waterloo’s spam score
is also used for the Pspam (·) measure that was described in
Section 2.1. The Pspam (·) and Ppr (·) (PageRank score) measures are used only for the ClueWeb-based settings as these
information types are not available for the other settings.
The titles of TREC topics served for queries. All data
was stemmed using the Krovetz stemmer. Stopwords on
the INQUERY list were removed from queries but not from
documents. The Indri toolkit (www.lemurproject.org/indri)
was used for experiments.

used in all methods that are based on cluster ranking (including ClustMRF) to order documents within the clusters.
The second initial list used for re-ranking, DocMRF (discussed in Section 4.2.4), is created by enriching MRF’s SDM
with query-independent document measures [3].
The third initial list, LM, is addressed in Section 4.2.5.
The list is created using unigram language models. In contrast, the MRF and DocMRF lists were created using retrieval methods that use term proximity information. Let
Dir[µ]
pz
(·) be the Dirichlet-smoothed unigram language model
induced from text z; µ is the smoothing parameter. The LM
def
similarity
between
texts x and y 
is simLM (x, y) =


Dir[0]

exp −CE px

(·)

Dir[µ]

py

(·)

[37, 17], where CE is

the cross entropy measure; µ is set to 1000.4 Accordingly,
the LM initial list is created by using simLM (Q, d) to rank
the entire corpus.5 This measure serves as the documentquery similarity measure for all methods operating over the
LM list, and for the inter-document similarity measure used
by the dsim feature function.
Unless otherwise stated, to cluster any of the three initial lists Dinit , we use a simple nearest-neighbor clustering
approach [18, 25, 14, 26, 13, 15]. For each document d
(∈ Dinit ), a cluster is created from d and the k − 1 documents di in Dinit (di 6= d) with the highest simLM (d, di ); k
is set to a value in {5, 10, 20} using cross validation as described below. Using such small overlapping clusters (all of
which contain k documents) was shown to be highly effective
for cluster-based document retrieval [18, 25, 14, 26, 13, 15].
In Section 4.2.6 we also study the performance of ClustMRF
when using hierarchical agglomerative clustering.

Initial retrieval and clustering. As described in Section
2, we use the ClustMRF cluster ranking method to re-rank
an initially retrieved document list Dinit . Recall that after ClustMRF ranks the clusters created from Dinit , these
are “replaced” by their constituent documents while omitting repeats. Documents within a cluster are ranked by
their query similarity, the measure of which is detailed below. This cluster-based re-ranking approach is employed
by all the reference comparison methods that we use and
that rely on cluster ranking. Furthermore, ClustMRF and
all reference comparison approaches re-rank a list Dinit that
is composed of the 50 documents that are the most highly
ranked by some retrieval method specified below. Dinit is relatively short following recommendations in previous work on
cluster-based re-ranking [18, 25, 26, 13]. In Section 4.2.7 we
study the effect of varying the list size on the performance
of ClustMRF and the reference comparisons.
We let all methods re-rank three different initial lists Dinit .
The first, denoted MRF, is used unless otherwise specified.
This list contains the documents in the corpus that are the
most highly ranked in response to the query when using the
state-of-the-art Markov Random Field approach with the
sequential dependence model (SDM) [28]. The free parameters that control the use of term proximity information in
SDM, λT , λO , and λU , are set to 0.85, 0.1, and 0.05, respectively, following previous recommendations [28]. We also use
MRF’s SDM with its free parameters set using cross validation as one of the re-ranking reference comparisons. (Details provided below.) All methods operating on the MRF
initial list use the exponent of the document score assigned
by SDM — which is a rank-equivalent estimate to that of
log p(Q, d) — as simM RF (Q, d), the document-query similarity measure. This measure was used to induce the initial
ranking using which Dinit was created. More generally, for a
fair performance comparison we maintain in all the experiments the invariant that the scoring function used to create
an initially retrieved list is rank equivalent to the documentquery similarity measure used in methods operating on the
list. Furthermore, the document-query similarity measure is

Evaluation metrics and free parameters. We use MAP
(computed at cutoff 50, the size of the list Dinit that is reranked) and the precision of the top 5 documents (p@5) and
their NDCG (NDCG@5) for evaluation measures.6 The free
parameters of our ClustMRF method, as well as those of all
reference comparison methods, are set using 10-fold cross
validation performed over the queries in an experimental
setting. Query IDs are the basis for creating the folds. The
two-tailed paired t-test with p ≤ 0.05 was used for testing
statistical significance of performance differences.
For our ClustMRF method, the free-parameter values are
set in two steps. First, SVMrank [12] is used to learn the values of the λl weights associated with the feature functions.
The NDCG@k of the k constituent documents of a cluster
serves as the cluster score used for ranking clusters in the
learning phase7 . (Recall from above that documents in a
4
The MRF SDM used above also uses Dirichlet-smoothed
unigram language models with µ = 1000.
5
Queries for which there was not a single relevant document
in the MRF or LM initial lists were removed from the evaluation. For the ClueWeb settings, the same query set was
used for ClueX and ClueXF.
6
We note that statAP, rather than AP, was the official
TREC evaluation metric in 2009 for ClueWeb with queries
1–50. For consistency with the other queries for ClueWeb,
and following previous work [3], we use AP for all ClueWeb
queries by treating prel files as qrel files. We hasten to point
out that evaluation using statAP for the ClueWeb collections
with queries 1–50 yielded relative performance patterns that
are highly similar to those attained when using AP.
7
Using MAP@k as the cluster score resulted in a slightly
less effective performance. We also note that learning-to-

336

AP

ROBUST

WT10G

GOV2

ClueA

ClueAF

ClueB

ClueBF

MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5

Init
10.1
50.7
50.6
19.9
51.0
52.5
15.8
37.5
37.2
12.7
59.3
48.6
4.5
19.1
12.6
8.6
46.3
32.4
12.5
33.1
24.4
15.8
44.8
33.2

TunedMRF
9.9
48.7
49.4
20.0
51.0
52.7
15.4
36.9
35.3i
12.7
60.8
49.5
4.9i
21.1
15.6i
8.7
47.8
33.1
13.5i
35.5
27.0
16.3i
46.8
34.3

ClustMRF
10.8
53.0
54.4t
21.0it
52.4
54.7
18.0it
44.9it
42.8it
14.2it
70.1it
56.2it
6.3it
44.6it
29.4it
8.9
50.2
33.9
16.1it
48.7it
37.4it
17.0
48.5
36.9

ClustMRF
AP

ROBUST
WT10G

GOV2

MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5

10.8
53.0
54.4
21.0
52.4
54.7
18.0
44.9
42.8
14.2
70.1
56.2
ClustMRF

ClueA

ClueAF
ClueB

ClueBF

Table 2: The performance of ClustMRF and a tuned
MRF (TunedMRF) when re-ranking the MRF initial list (Init). Boldface: the best result in a row. ’i’
and ’t’ mark statistically significant differences with
Init and TunedMRF, respectively.

MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5

6.3
44.6
29.4
8.9
50.2
33.9
16.1
48.7
37.4
17.0
48.5
36.9

stdvqsim
9.4
43.7c
45.0c
19.0c
50.7
52.4
15.4c
38.4c
37.8c
12.7c
59.3c
48.2c
maxsw2
5.4c
28.7c
20.3c
8.6
47.2
32.5
14.2c
41.9c
30.1c
16.3
45.0
35.5

maxsw2
9.7
44.6c
45.8c
17.7c
46.9c
49.1c
12.2c
31.7c
28.6c
12.9c
62.3c
48.8c
maxsw1
5.3c
29.3c
20.5c
7.8c
40.4c
28.9c
15.4
42.9c
32.5c
15.7c
42.3c
32.8

geoqsim
10.6
50.9
52.0
20.6
50.4
52.4
16.3c
39.3c
39.0c
13.2c
58.0c
46.6c
maxqsim
4.5c
18.7c
12.4c
8.3
49.3
34.3
12.8c
33.9c
25.5c
14.8c
42.9c
32.8

minsw2
9.6
49.1
50.4
16.8c
44.7c
45.9c
14.2c
33.9c
32.4c
14.2
66.3
52.3
geoqsim
4.8c
20.9c
14.0c
8.6
48.7
33.9
12.9c
34.2c
25.6c
15.9
43.2
33.6

Table 3: Using each of ClustMRF’s top-4 feature
functions by itself for ranking the clusters so as to
re-rank the MRF initial list. Boldface: the best performance per row. ’c’ marks a statistically significant difference with ClustMRF.

cluster are ordered based on their query similarity.) A ranking of documents in Dinit is created from the cluster ranking,
which is performed for each cluster size k (∈ {5, 10, 20}), using the approach described above; k is then also set using
cross validation by optimizing the MAP performance of the
resulting document ranking. The train/test split for the
first and second steps are the same — i.e., the same train
set used for learning the λl ’s is the one used for setting the
cluster size. As is the case for ClustMRF, the final document ranking induced by any reference comparison method
is based on using cross validation to set free-parameter values; and, MAP serves as the optimization criterion in the
training (learning) phase.
Finally, we note that the main computational overhead,
on top of the initial ranking, incurred by using ClustMRF is
the clustering. That is, the feature functions used are either
query-independent, and therefore can be computed offline;
or, use mainly document-query similarity values that have
already been computed to create the initial ranking. Clustering of a few dozen documents can be computed efficiently;
e.g., based on document snippets.

the free parameters of ClustMRF; TunedMRF denotes this
method. We found that using exhaustive search for finding
SDM’s optimal parameter values in the training phase yields
better performance (on the test set) than using SVMrank
[12] and SVMmap [36]. Specifically, λT , λO , and λU were
set to values in {0, 0.05, . . . , 1} with λT + λO + λU = 1.
We first see in Table 2 that while TunedMRF outperforms
the initial MRF ranking in most relevant comparisons (experimental setting × evaluation measure), there are cases
(e.g., for AP and WT10G) for which the reverse holds. The
latter finding implies that optimal free-parameter values of
MRF’s SDM do not necessarily generalize across queries.
More importantly, we see in Table 2 that ClustMRF outperforms both the initial ranking and TunedMRF in all relevant comparisons. Many of the improvements are substantial and statistically significant. These findings attest to the
high effectiveness of using ClustMRF for re-ranking.

4.2.2 Analysis of feature functions

4.2 Experimental results

We now turn to analyze the relative importance attributed
to the different feature functions used in ClustMRF; i.e., the
λl weights assigned to these functions in the training phase
by SVMrank . We first average, per experimental setting and
cluster size, the weights assigned to a feature function using
the different training folds. Then, the feature function is
assigned with a score that is the reciprocal rank of its corresponding (average) weight. Finally, the feature functions
are ordered by averaging their scores across experimental
settings and cluster sizes. Two feature functions, pr and
spam, are only used for the ClueWeb-based settings. Hence,
we perform the analysis separately for the ClueWeb and nonClueWeb (AP, ROBUST, WT10G, and GOV2) settings.

4.2.1 Main result
Table 2 presents our main result. Namely, the performance of ClustMRF when used to re-rank the MRF initial
list. Recall that the initial ranking was induced using MRF’s
SDM with free-parameter values set following previous recommendations [28]. Thus, we also present for reference the
re-ranking performance of using MRF’s SDM with its three
free parameters set using cross validation as is the case for
rank methods [23] other than SVMrank , which proved to
result in highly effective performance as shown below, can
also be used for setting the values of the λl weights.

337

MAP
AP
p@5
NDCG@5
MAP
ROBUST p@5
NDCG@5
MAP
WT10G
p@5
NDCG@5
MAP
GOV2
p@5
NDCG@5
MAP
ClueA
p@5
NDCG@5
MAP
ClueAF
p@5
NDCG@5
MAP
ClueB
p@5
NDCG@5
MAP
ClueBF
p@5
NDCG@5

Init
10.1
50.7
50.6
19.9c
51.0
52.5
15.8c
37.5c
37.2c
12.7c
59.3c
48.6c
4.5c
19.1c
12.6c
8.6
46.3
32.4
12.5c
33.1c
24.4c
15.8
44.8
33.2

Inter
10.4
55.9i
56.0i
20.8i
52.2
53.9
15.1c
38.0c
36.8c
12.9c
62.9c
50.2c
5.3c
24.3c
17.8c
8.9
44.8
32.6
14.9i
44.5i
34.3i
16.7
48.2
36.4

AMean
10.6
51.1
52.2
20.3c
49.1c
51.2c
16.6ic
39.6ic
38.5c
13.1ic
58.8c
47.8c
4.6c
19.3c
13.2c
8.8
49.8i
35.0i
13.0ic
34.7c
26.1ic
15.9
45.6
34.4

GMean
10.6
50.9
52.0
20.6i
50.4
52.4
16.3c
39.3c
39.0c
13.2ic
58.0c
46.6c
4.8c
20.9c
14.0c
8.6
48.7
33.9
12.9c
34.2c
25.6c
15.9
43.2
33.6

CRank
10.0
50.0
50.5
19.7c
46.6ic
49.1ic
14.5c
34.2c
32.7ic
12.7c
62.3c
48.4c
5.2c
24.3c
18.5ic
8.3
41.5c
30.0
16.0i
46.6i
35.3i
17.7i
50.3
38.0i

CMRF
10.8
53.0
54.4
21.0i
52.4
54.7
18.0i
44.9i
42.8i
14.2i
70.1i
56.2i
6.3i
44.6i
29.4i
8.9
50.2
33.9
16.1i
48.7i
37.4i
17.0
48.5
36.9

Table 4: Comparison with cluster-based retrieval
methods used for re-ranking the MRF initial list.
(CMRF is a shorthand for ClustMRF.) Boldface
marks the best result in a row. ’i’ and ’c’ mark statistically significant differences with the initial ranking and ClustMRF, respectively.

For the non-ClueWeb settings, the feature functions, in
descending order of attributed importance, are: stdv-qsim,
max-sw2, geo-qsim, min-sw2, max-sw1, max-qsim, min-dsim,
geo-sw2, min-icompress, min-qsim, min-sw1, geo-icompress,
max-dsim, geo-dsim, max-icompress, geo-entropy, min-entropy,
geo-sw1, max-entropy. For the ClueWeb settings the feature
functions are ordered as follows: max-sw2, max-sw1, maxqsim, geo-qsim, max-spam, geo-sw2, min-icompress, minsw2, geo-sw1, min-sw1, min-qsim, stdv-qsim, max-pr, mindsim, min-entropy, max-entropy, min-spam, geo-icompress,
geo-entropy, max-icompress, geo-spam, geo-pr, geo-dsim, minpr, max-dsim.
Two main observations rise. First, each of the three types
of cliques used in Section 2.1 for defining the MRF has at
least one associated feature function that is assigned with
a relatively high weight. For example, the geo-qsim function defined over lQD , the max-qsim function defined over
lQC , and the max-sw2 function defined over lC , are among
the 4, 6 and 2 most important functions in both cases (nonClueWeb and ClueWeb settings). Second, for the ClueWeb
settings, the feature functions defined over the lC clique and
which are based on query-independent document measures
(e.g., max-sw1, max-sw2, max-spam) are attributed with
high importance. In fact, among the top-10 feature functions for the ClueWeb settings only two (max-qsim and geoqsim) are not based on a query-independent measure. This
is not the case for the non-ClueWeb settings where different
statistics of the query-similarity values are among the top10 feature functions. We note that using some of the queryindependent document measures utilized here was shown in
work on Web retrieval to be effective for ranking documents
directly [3]. We demonstrated the merits of using such measures for ranking document clusters.

338

In Table 3 we present the performance of using each of the
top-4 feature functions (for the non-ClueWeb and ClueWeb
settings) by itself as a cluster ranking method. As in Section 4.2.1, we use the cluster ranking to re-rank the MRF
initial list. We see in Table 3 that in almost all relevant
comparisons ClustMRF is more effective — often to a substantial and statistically significant degree — than using one
of its top-4 feature functions alone. Thus, we conclude that
ClustMRF’s effective performance cannot be attributed to
a single feature function that it utilizes.
We also performed ablation tests as follows. ClustMRF
was trained each time without one of its top-10 feature functions. This resulted in a statistically significant performance
decrease with respect to at least one of the three evaluation
metrics of concern (MAP, p@5 and NDCG@5) for all top-10
feature functions for the ClueWeb settings. (Actual numbers are omitted as they convey no additional insight.) Yet,
there was no statistically significant performance decrease
for any of the top-10 feature functions for the non-ClueWeb
settings. These findings attest to the redundancy of feature
functions when employing ClustMRF for the non-ClueWeb
settings and to the lack thereof in the ClueWeb settings.
Finally, we computed the Pearson correlation of the learned
λl ’s values (averaged over the train folds and cluster sizes)
between experimental settings. We found that for pairs of
non-ClueWeb settings, excluding AP, the correlation was
at least 0.5; however, the correlation with AP was much
smaller. For the ClueWeb settings, the correlation between
ClueB and ClueBF was high (0.83) while that for other pairs
of settings was lower than 0.5. Thus, we conclude that the
learned λl values can be collection, and setting, dependent.

4.2.3 Comparison with cluster-based methods
We next compare the performance of ClustMRF with that
of highly effective cluster-based retrieval methods. All methods re-rank the MRF initial list.
The InterpolationF method (Inter in short) [13] ranks
documents directly using the score function:
def
Score(d; Q) = (1 − λ) P ′ sim(Q,d)
+
sim(Q,d′ )
λP

P

d ∈Dinit

C∈C l(Dinit )

d′ ∈Dinit

P

sim(Q,C)sim(C,d)

C∈C l(Dinit )

sim(Q,C)sim(C,d′ )

. This state-of-the-

art re-ranking method represents the class of approaches
that use clusters to “smooth” document representations [13].
In contrast to Inter, ClustMRF belongs to a class of methods that rely on cluster ranking. Accordingly, the next reference comparison methods represent this class. Section 4.1
provided a description of how the cluster ranking is transformed to a ranking of the documents in Dinit . The AMean
method [26, 15], for example, scores cluster C by the arithmetic mean of the query similarity values of its constituent
def 1 P
documents. Formally, Score(C; Q) = |C|
d∈C sim(Q, d).
Scoring C by the geometric mean of the query-similarity
def
values
qQ of its constituent documents, Score(C; Q) =
|C|
d∈C sim(Q, d), was shown to yield state-of-the-art cluster ranking performance [15]. This approach, henceforth
referred to as GMean, results from aggregating several feature functions (geo-qsim) that are used in our ClustMRF
method. (See Section 2.1 for details.)
An additional state-of-the-art cluster ranking method is
ClustRanker (CRank in short) [15]. Cluster C is scored by
def

Score(C; Q) = (1 − λ) P

sim(Q,C)p(C)
+
sim(Q,C ′ )p(C ′ )

C ′ ∈C l(Dinit )

AP

ROBUST
WT10G
GOV2

ClueA

ClueAF

ClueB

ClueBF

MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5
MAP
p@5
NDCG@5

DocMRF
9.9
50.7
51.0
20.3
52.1
54.0
17.1
42.0
40.4
15.0
66.3
54.0
9.8
42.4
28.4
9.5
52.6
35.7
16.6
45.6
33.6
17.6
50.3
37.5

ClustMRF
11.0
53.5
53.5
21.2d
53.2
55.3
17.7
42.5
40.3
15.3
68.7
55.8
10.0
49.3d
33.4d
9.5
49.6
35.7
18.9d
52.9d
39.9d
19.4d
55.3d
41.9d

To that end, we follow some recent work [3]. We re-rank
the 1000 documents that are the most highly ranked by
MRF’s SDM that was used above to create the MRF initial list. Re-ranking is performed using an MRF model that
is enriched with query-independent document measures [3].
We use the same document measures utilized by ClustMRF,
except for dsim which is based on inter-document similarities and which was not considered in this past work that
ranked documents independently of each other [3]. The resultant ranking, induced using SVMrank for learning parameter values, is denoted DocMRF. (SVMrank yielded better
performance than SVMmap .) We then let ClustMRF rerank the top-50 documents. In doing so, we use the exponent of the score assigned by DocMRF to document d,
which is a rank equivalent estimate to that of log p(Q, d),
as the sim(Q, d) value used by ClustMRF. Thus, we maintain the invariant mentioned above that the scoring function
used to induce the ranking upon which ClustMRF operates
is rank equivalent to the document-query similarity measure
used in ClustMRF. We note that ClustMRF is different from
DocMRF in two important respects. First, by the virtue of
ranking clusters first and transforming the ranking to that
of documents rather than ranking documents directly as is
the case in DocMRF. Second, by the completely different
ways that document-query similarities are used.
Comparing the performance of DocMRF in Table 5 with
that of the MRF initial ranking in Table 2 attests to the
merits of using DocMRF for re-ranking. We can also see
in Table 5 that applying ClustMRF over the DocMRF list
results in performance improvements in almost all relevant
comparisons. Many of the improvements for the ClueWeb
settings are substantial and statistically significant.

Table 5: Using ClustMRF to re-rank the DocMRF
[3] list. Boldface: best result in a row. ’d’ marks a
statistically significant difference with DocMRF.
λP

P
C ′ ∈C l(D

d∈C

init )

sim(Q,d)sim(C,d)p(d)
P
;
′
d∈C ′ sim(Q,d)sim(C ,d)p(d)

p(C) and p(d) are

estimated based on inter-cluster and inter-document (across
clusters) similarities, respectively. These similarities, computed using the language-model-based measure simLM (·, ·),
are not utilized by ClustMRF that uses inter-document similarities only within a cluster.
Following the original reports of Inter [13] and CRank [15],
we estimate sim(Q, C) and sim(C, d) in these methods using
simLM (·, ·); C is represented by the concatenation of its constituent documents. For a fair comparison with ClustMRF,
sim(Q, d) is set in all reference comparisons considered here
to simM RF (·, ·), which was used to create the initial MRF
list that is re-ranked.
All free parameters of the methods are set using cross validation. Specifically, λ which is used by Inter and CRank
is set to values in {0, 0.1, . . . , 1}. The graph out degree
and the dumping factor used by CRank are set to values
in {4, 9, 19, 29, 39, 49} and {0.05, 0.1, . . . , 0.9, 0.95}, respectively. The cluster size used by each method is selected from
{5, 10, 20} as is the case for ClustMRF. Table 4 presents the
performance numbers.
We can see in Table 4 that in a vast majority of the relevant comparisons ClustMRF outperforms the reference comparison methods. Many of the improvements are substantial
and statistically significant. In the few cases that ClustMRF
is outperformed by one of the other methods, the performance differences are not statistically significant.

4.2.5 Using ClustMRF to re-rank the LM list
The third list we re-rank using ClustMRF is LM, which
was created using unigram language models. For reference
comparison we use the cluster-based Inter method which was
used in Section 4.2.3. Experiments show — actual numbers are omitted due to space considerations — that for reranking the LM list, the GMean cluster ranking method is
more effective in most relevant comparisons than the other
two cluster ranking methods used in Section 4.2.3 for reference comparison (AMean and CRank). Hence, GMean is
used here as an additional reference comparison.
ClustMRF, Inter and GMean use the simLM (·, ·) similarity measure, which was used for inducing the initial ranking,
for sim(Q, d). All other implementation details are the same
as those described above. As a result, ClustMRF, as well
as Inter and GMean, use only unigram language models in
the LM setting considered here. This is in contrast to the
MRF-list setting considered above where term-proximities
information was used.
An additional reference comparison that uses unigram language models is relevance model number 3 [1], RM3, which
is a state-of-the-art query expansion approach. RM3 is also
used to re-rank the LM list. All (50) documents in the list
are used for constructing RM3. Its free-parameter values
are set using cross validation. Specifically, the number of
expansion terms and the interpolation parameter that controls the reliance on the original query are set to values in
{5, 10, 25, 50} and {0.1, 0.3, . . . , 0.9}, respectively. Dirichletsmoothed language models are used with µ = 1000.

4.2.4 Using ClustMRF to re-rank the DocMRF list
Heretofore, we studied the performance of ClustMRF when
used to re-rank the MRF initial list. The analysis presented
in Section 4.2.2 demonstrated the effectiveness — especially
for the ClueWeb settings — of using feature functions that
utilize query-independent document measures. Thus, we
now turn to explore ClustMRF’s performance when employed over a document ranking that is already based on
using query-independent document measures.

339

MAP
AP
p@5
NDCG@5
MAP
ROBUST p@5
NDCG@5
MAP
WT10G
p@5
NDCG@5
MAP
GOV2
p@5
NDCG@5
MAP
ClueA
p@5
NDCG@5
MAP
ClueAF
p@5
NDCG@5
MAP
ClueB
p@5
NDCG@5
MAP
ClueBF
p@5
NDCG@5

Init

Inter

GMean RM3

ClustMRF

9.9
49.6
49.9
19.3c
49.5c
51.6c
15.0
36.4c
35.8
11.8c
56.6c
46.5c
3.3c
16.1c
10.7c
8.0c
47.4
32.3
11.4c
29.0c
21.2c
14.7c
42.9c
32.1c

10.6i
56.1ic
55.6i
20.1i
50.9
53.1
14.9
37.5
37.1
12.6ic
62.4ic
50.4i
5.0i
24.6ic
17.9ic
8.5i
46.7
32.6
13.8ic
40.5i
29.6i
15.6
46.3
34.6

10.8i
50.7
51.8
20.6i
52.1
53.8
14.9
37.5
35.5
12.4ic
60.8ic
48.8c
3.7ic
17.2c
11.5c
8.2
45.7
32.3
12.0ic
31.6ic
23.4ic
15.5
43.4
33.4c

10.5
51.3
51.7
20.5i
52.9i
55.6i
14.6
42.2i
39.3
13.5i
68.4i
54.3i
5.5i
43.3i
27.7i
8.7i
51.5
35.6
16.0i
46.0i
34.8i
16.8i
49.2i
38.7i

9.9
49.1
49.3
19.7ic
49.7c
52.1c
14.5
36.6c
35.9
12.7ic
60.4ic
49.1c
3.8ic
17.4c
11.0c
8.7i
47.6
34.3
13.9ic
40.2i
30.0i
16.4i
48.9i
36.6i

MAP
p@5
NDCG@5
MAP
ROBUST p@5
NDCG@5
MAP
WT10G
p@5
NDCG@5
MAP
GOV2
p@5
NDCG@5
MAP
ClueA
p@5
NDCG@5
MAP
ClueAF
p@5
NDCG@5
MAP
ClueB
p@5
NDCG@5
MAP
ClueBF
p@5
NDCG@5
AP

Table 6: Re-ranking the LM initial list. Boldface:
the best result in a row. ’i’ and ’c’ mark statistically
significant differences with the initial ranking and
ClustMRF, respectively.

Init

HAC
NN
CRank ClustMRF CRank ClustMRF

10.1
50.7
50.6
19.9
51.0
52.5
15.8
37.5
37.2
12.7
59.3
48.6
4.5
19.1
12.6
8.6
46.3
32.4
12.5
33.1
24.4
15.8
44.8
33.2

9.9
49.8
50.5
19.1
50.1
51.7
14.8
36.6
34.4
13.2i
61.5
49.7
5.6i
23.7
16.9i
8.4
43.9
32.0
14.4i
39.5i
30.6i
15.3
43.9
32.7

9.6i
46.5i
46.8i
19.6
50.4
51.9
15.8
38.2
37.0
13.6i
63.9
51.5
5.8i
31.7ic
21.0i
9.2
48.9
33.4
14.5i
39.7i
30.3i
15.2
43.1
32.5

10.0
50.0
50.5
19.7
46.6i
49.1i
14.5
34.2
32.7i
12.7
62.3
48.4
5.2
24.3
18.5i
8.3
41.5
30.0
16.0i
46.6i
35.3i
17.7i
50.3
38.0i

10.8
53.0
54.4
21.0ic
52.4c
54.7c
18.0ic
44.9ic
42.8ic
14.2ic
70.1ic
56.2ic
6.3ic
44.6ic
29.4ic
8.9
50.2c
33.9
16.1i
48.7i
37.4i
17.0
48.5
36.9

Table 7: Using nearest-neighbor clustering (NN) vs.
(complete link) hierarchical agglomerative clustering (HAC). The MRF initial list is used. Boldface:
the best result in a row per clustering algorithm; underline: the best result in a row. ’i’ and ’c’: statistically significant differences with the initial ranking
and CRank, respectively.

We see in Table 6 that ClustMRF outperforms the reference comparisons in a vast majority of the relevant comparisons. Many of the improvements are substantial and
statistically significant. These results, along with those presented in Sections 4.2.1 and 4.2.4, attest to the effectiveness
of using ClustMRF to re-rank different initial lists.

clusters created by HAC. The improved effectiveness of using NN in comparison to HAC echoes findings in previous
work on cluster-based re-ranking [13]. For CRank, the performance of using neither NN nor HAC dominates that of
using the other.

4.2.6 Varying the clustering algorithm
Thus far, we used ClustMRF and the reference comparisons with nearest-neighbor (NN) clustering. In Table 7 we
present the retrieval performance of using hierarchical agglomerative clustering (HAC) with the complete link measure. This clustering was shown to be among the most effective hard clustering methods for cluster-based retrieval
[24, 13]. We use simLM1(d1 ,d2 ) + simLM1(d2 ,d1 ) for an interdocument dissimilarity measure; and, cut the clustering dendrogram so that the resultant average cluster size is the closest to a value k (∈ {5, 10, 20}). Doing so somewhat equates
the comparison terms with using the NN clusters whose size
is in {5, 10, 20}. Cross validation is used in all cases for
setting the value of k.
The MRF initial list is clustered and serves as the basis for re-ranking. Experiments show (actual numbers are
omitted due to space considerations) that among the three
cluster ranking methods which were used above for reference comparison (AMean, GMean, and CRank) CRank is
the most effective when using HAC. Hence, CRank serves
as a reference comparison here.
We see in Table 7 that in the majority of relevant comparisons, ClustMRF improves over the initial ranking when
using HAC. In contrast, CRank is outperformed by the initial ranking in most relevant comparisons for HAC. Indeed,
ClustMRF outperforms CRank in most cases for both NN
and HAC. We also see that ClustMRF is (much) more effective when using the overlapping NN clusters than the hard

4.2.7 The effect of the size of the initial list
Until now, ClustMRF and all reference comparison methods were used to re-rank an initial list of 50 documents. Using a short list follows common practice in work on clusterbased re-ranking [18, 25, 26, 13] as was mentioned in Section
4.1. We now turn to study ClustMRF’s performance when
re-ranking longer lists. To that end, we use for the initial list
the n (∈ {50, 100, 250, 500}) documents that are the most
highly ranked by MRF’s SDM [28] which was used above
for creating the MRF initial list. For reference comparisons
we use TunedMRF (see Section 4.2.1); and, the AMean and
GMean cluster ranking methods described in Section 4.2.3.
Nearest-neighbor clustering is used.
We see in Figure 2 that in almost all cases — i.e., experimental settings and values of n — ClustMRF outperforms both the initial ranking and TunedMRF; often, the
performance differences are quite substantial. Furthermore,
in most cases (with the notable exception of AP) ClustMRF
outperforms AMean and GMean.

4.2.8 Diversifying search results
We next explore how ClustMRF can be used to improve
the performance of search-results diversification approaches.
Specifically, we use the MMR [5] and the state-of-the-art
xQuAD [29] diversification methods.

340

AP
13.5

ROBUST
23.0

Init
TunedMRF
AMean
GMean
ClustMRF

13.0
12.5

WT10G
19.0

Init
TunedMRF
AMean
GMean
ClustMRF

22.5
22.0

18.5
18.0

21.0

11.0

17.0
16.5

20.5

10.5

20.0

15.5

9.5

19.5

15.0

100

250

500

50

100

250

n

9.0

500

13.0
12.0
50

100

250

50

100

250

ClueBF
19.0

Init
TunedMRF
AMean
GMean
ClustMRF

19.0
18.0

500
n

ClueB
20.0

Init
TunedMRF
AMean
GMean
ClustMRF

9.5

500
n

ClueAF
10.0

Init
TunedMRF
AMean
GMean
ClustMRF

10.0

14.0

n

ClueA
11.0

15.0

16.0

10.0
50

Init
TunedMRF
AMean
GMean
ClustMRF

18.0

17.0

8.5

17.0

16.0

MAP

7.0

MAP

9.0

8.0

MAP

MAP

16.0
MAP

MAP

11.5

21.5

Init
TunedMRF
AMean
GMean
ClustMRF

17.0

17.5
MAP

MAP

12.0

GOV2
18.0

Init
TunedMRF
AMean
GMean
ClustMRF

15.0

16.0

14.0

6.0

13.0

8.0
5.0

15.0

12.0

4.0

7.5
50

100

250

500

11.0
50

100

250

n

500

14.0
50

100

n

250

500

50

100

n

250

500
n

Figure 2: The effect on MAP(@50) performance of the size n of the MRF initial list that is re-ranked.
Init
ClueA

ClueAF

ClueB

ClueBF

α-NDCG
ERR-IA
P-IA
α-NDCG
ERR-IA
P-IA
α-NDCG
ERR-IA
P-IA
α-NDCG
ERR-IA
P-IA

24.5
16.0
11.8
42.6
32.0
21.0
33.2
21.1
15.4
41.6
29.7
18.9

MRF
26.2c
17.3c
10.3c
42.9
32.3
20.2c
33.6c
21.3c
14.4ic
42.6ic
30.2ic
18.4

MMR
QClust
ClustMRF
25.4c
17.5c
9.6ic
39.0ic
29.8c
14.9ic
33.9c
21.5c
12.8ic
38.7ic
27.0ic
14.5ic

38.7i
30.5i
16.7i
43.8
34.2
17.6i
43.7i
32.0i
17.4i
45.4i
33.3i
17.8

MRF
27.4ic
17.9ic
13.3c
44.3i
33.4i
21.0
39.7ic
25.9ic
19.4ic
46.1ic
33.2i
21.4ic

xQuAD
QClust
ClustMRF
28.9ic
19.6ic
13.6ic
43.7
33.1
20.0
39.3ic
25.3ic
19.2ic
44.2ic
31.2c
20.9ic

38.8i
30.6i
17.2i
45.5i
34.9i
20.6
45.5i
32.9i
21.0i
48.1i
34.8i
22.0i

Table 8: Diversifying search results. Underline and boldface mark the best result in a row, and per diversification method in a row, respectively. ’i’ and ’c’ mark statistically significant differences with the initial
ranking (Init) and ClustMRF, respectively. The MRF initial list is used.
MMR and xQuAD iteratively re-rank an initial list Dinit .
In each iteration the document in Dinit \ S assigned with
the highest score is added to the set S; S is empty at the
beginning. The final ranking is determined by the order of
insertion to S.
The score MMR assigns to document d (∈ Dinit \ S) is
βsim1 (Q, d)−(1−β) maxdi ∈S sim2 (d, di ); β is a free parameter; sim1 (·, ·) and sim2 (·, ·) are discussed below. In contrast
to MMR, xQuAD uses information about Q’s subtopics,
T (Q), and assigns
score βp(d|Q)+ i
h d with the Q
P
(1 − β) t∈T (Q) p(t|Q)p(d|t) di ∈S (1 − p(di |t)) ; p(t|Q) is
the relative importance of subtopic t with respect to Q;
p(d|Q) and p(d|t) are the estimates of d’s relevance to Q
and t, respectively.
The parameter β controls in both methods the tradeoff
between using relevance estimation and applying diversification. Our focus is on improving the former and evaluating the resulting (diversification based) performance. This
was also the case in previous work that used cluster ranking

for results diversification [11]. Hence, this work serves for
reference comparison below.8
We study three different estimates for sim1 (Q, d) (used
in MMR) which we also use for p(d|Q) (used in xQuAD).9
The first, simM RF (Q, d), is that employed in the evaluation above to create the MRF initial list that is also used
here for re-ranking. (Further details are provided below.)
The next two estimates are based on applying cluster ranking and transforming it to document ranking using the ap1
proach described in Section 4.1. In these cases, r(d)
serves
for sim1 (Q, d), where r(d) is the rank of d in the document
result list produced by using the cluster ranking method.
The first cluster ranking method is ClustMRF. The second,
QClust, was used in the work mentioned above on utilizing
cluster ranking for results diversification [11]. Specifically,
cluster C is scored by simLM (Q, C) (see Section 4.1 for de8
There is work on using information induced from clusters
for the diverisification itself (e.g., [21]). Using ClustMRF for
cluster ranking in these approaches is future work.
9
For scale compatibility, the two resultant quantities that
are interpolated (using β) in MMR and xQuAD are sum
normalized with respect to all documents in Dinit before the
interpolation is performed.

341

tails of simLM (·, ·)); C is represented by the concatenation
of its documents.
We use MMR and xQuAD to re-rank the MRF initial
list that contains 50 documents. simLM (·, ·) serves for the
sim2 (·, ·) measure used in MMR and for p(d|t) that is used in
xQuAD. The official TREC subtopics, which are available
for the ClueWeb settings that we use here, were used for
def
experiments. Following the findings in [29], we set p(t|Q) =
1
. The value of β is selected from {0.1, 0.2, . . . , 0.9}
|T (Q)|
using cross validation; α-NDCG (@20) is the optimization
metric. In addition to α-NDCG (@20), ERR-IA (@20) and
P-IA (@20) are used for evaluation.
Table 8 presents the results. We see that using the MRF
similarity measure in MMR and xQuAD outperforms the initial ranking, which was created using this measure, in most
relevant comparisons. This attests to the diversification effectiveness of MMR and xQuAD. Using QClust outperforms
the initial ranking in most cases, but is consistently outperformed by using the MRF measure and our ClustMRF
method. More generally, the best performance for each diversification method (MMR and xQuAD) is almost always
attained by ClustMRF, which often outperforms the other
methods in a substantial and statistically significant manner. Thus, although ClustMRF ranks clusters of similar
documents, using the resultant document ranking can help
to much improve results-diversification performance.

5.

[9] D. Fetterly, M. Manasse, and M. Najork. Spam, damn spam,
and statistics: Using statistical analysis to locate spam web
pages. In Proc. of WebDB, pages 1–6, 2004.
[10] N. Fuhr, M. Lechtenfeld, B. Stein, and T. Gollub. The
optimum clustering framework: implementing the cluster
hypothesis. Information Retrieval Journal, 15(2):93–115, 2012.
[11] J. He, E. Meij, and M. de Rijke. Result diversification based on
query-specific cluster ranking. JASIST, 62(3):550–571, 2011.
[12] T. Joachims. Training linear svms in linear time. In Proc. of
KDD, pages 217–226, 2006.
[13] O. Kurland. Re-ranking search results using language models of
query-specific clusters. Journal of Information Retrieval,
12(4):437–460, August 2009.
[14] O. Kurland and C. Domshlak. A rank-aggregation approach to
searching for optimal query-specific clusters. In Proc. of
SIGIR, pages 547–554, 2008.
[15] O. Kurland and E. Krikon. The opposite of smoothing: A
language model approach to ranking query-specific document
clusters. Journal of Artificial Intelligence Research (JAIR),
41:367–395, 2011.
[16] O. Kurland and L. Lee. Corpus structure, language models, and
ad hoc information retrieval. In Proc. of SIGIR, pages
194–201, 2004.
[17] O. Kurland and L. Lee. PageRank without hyperlinks:
Structural re-ranking using links induced by language models.
In Proc. of SIGIR, pages 306–313, 2005.
[18] O. Kurland and L. Lee. Respect my authority! HITS without
hyperlinks utilizing cluster-based language models. In Proc. of
SIGIR, pages 83–90, 2006.
[19] O. Kurland, F. Raiber, and A. Shtok. Query-performance
prediction and cluster ranking: Two sides of the same coin. In
Proc. of CIKM, pages 2459–2462, 2012.
[20] K.-S. Lee, Y.-C. Park, and K.-S. Choi. Re-ranking model based
on document clusters. Inf. Process. Manage., 37(1):1–14, 2001.
[21] T. Leelanupab, G. Zuccon, and J. M. Jose. When two is better
than one: A study of ranking paradigms and their integrations
for subtopic retrieval. In Proc. of AIRS, pages 162–172, 2010.
[22] A. Leuski. Evaluating document clustering for interactive
information retrieval. In Proc. of CIKM, pages 33–40, 2001.
[23] T.-Y. Liu. Learning to Rank for Information Retrieval.
Springer, 2011.
[24] X. Liu and W. B. Croft. Cluster-based retrieval using language
models. In Proc. of SIGIR, pages 186–193, 2004.
[25] X. Liu and W. B. Croft. Experiments on retrieval of optimal
clusters. Technical Report IR-478, Center for Intelligent
Information Retrieval (CIIR), University of Massachusetts,
2006.
[26] X. Liu and W. B. Croft. Evaluating text representations for
retrieval of the best group of documents. In Proc. of ECIR,
pages 454–462, 2008.
[27] D. Metzler. A feature-centric view of information retrieval.
Springer, 2011.
[28] D. Metzler and W. B. Croft. A Markov random field model for
term dependencies. In Proc. of SIGIR, pages 472–479, 2005.
[29] R. L. T. Santos, C. Macdonald, and I. Ounis. Exploiting query
reformulations for web search result diversification. In Proc. of
WWW, pages 881–890, 2010.
[30] J. Seo and W. B. Croft. Geometric representations for multiple
documents. In Proc. of SIGIR, pages 251–258, 2010.
[31] J. G. Shanahan, J. Bennett, D. A. Evans, D. A. Hull, and
J. Montgomery. Clairvoyance Corporation experiments in the
TREC 2003. High accuracy retrieval from documents (HARD)
track. In Proc. of TREC-12, pages 152–160, 2003.
[32] A. Tombros, R. Villa, and C. van Rijsbergen. The effectiveness
of query-specific hierarchic clustering in information retrieval.
Inf. Process. Manage., 38(4):559–582, 2002.
[33] C. J. van Rijsbergen. Information Retrieval. Butterworths,
second edition, 1979.
[34] E. M. Voorhees. The cluster hypothesis revisited. In Proc. of
SIGIR, pages 188–196, 1985.
[35] P. Willett. Query specific automatic document classification.
International Forum on Information and Documentation,
10(2):28–32, 1985.
[36] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A support
vector method for optimizing average precision. In Proc. of
SIGIR, pages 271–278, 2007.
[37] C. Zhai and J. D. Lafferty. A study of smoothing methods for
language models applied to ad hoc information retrieval. In
Proc. of SIGIR, pages 334–342, 2001.

CONCLUSIONS

We presented a novel approach to ranking (query specific)
document clusters by their presumed relevance to the query.
Our approach uses Markov Random Fields that enable the
integration of various types of cluster-relevance evidence.
Empirical evaluation demonstrated the effectiveness of using our approach to re-rank different initially retrieved lists.
The approach also substantially outperforms state-of-the-art
cluster ranking methods and can be used to substantially
improve the performance of results diversification methods.

6.

ACKNOWLEDGMENTS

We thank the reviewers for their comments. This work has
been supported by and carried out at the Technion-Microsoft
Electronic Commerce Research Center.

7.

REFERENCES

[1] N. Abdul-Jaleel, J. Allan, W. B. Croft, F. Diaz, L. Larkey,
X. Li, M. D. Smucker, and C. Wade. UMASS at TREC 2004 —
novelty and hard. In Proc. of TREC-13, 2004.
[2] J. Allan, M. E. Connell, W. B. Croft, F.-F. Feng, D. Fisher,
and X. Li. INQUERY and TREC-9. In Proc. of TREC-9, 2000.
[3] M. Bendersky, W. B. Croft, and Y. Diao. Quality-biased
ranking of web documents. In Proc. of WSDM, pages 95–104,
2011.
[4] S. Brin and L. Page. The anatomy of a large-scale hypertextual
web search engine. In Proc. of WWW, pages 107–117, 1998.
[5] J. G. Carbonell and J. Goldstein. The use of MMR,
diversity-based reranking for reordering documents and
producing summaries. In Proc. of SIGIR, pages 335–336, 1998.
[6] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient
and effective spam filtering and re-ranking for large web
datasets. Informaltiom Retrieval Journal, 14(5):441–465, 2011.
[7] W. B. Croft. A model of cluster searching based on
classification. Information Systems, 5:189–195, 1980.
[8] D. R. Cutting, D. R. Karger, J. O. Pedersen, and J. W. Tukey.
Scatter/Gather: A cluster-based approach to browsing large
document collections. In Proc. of SIGIR, pages 318–329, 1992.

342

