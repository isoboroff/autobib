Utilizing Query Change for Session Search
Dongyi Guan, Sicong Zhang, Hui Yang
Department of Computer Science
Georgetown University
37th and O Street, NW, Washington, DC, 20057

{dg372, sz303}@georgetown.edu, huiyang@cs.georgetown.edu
Table 1: Examples of TREC 2012 Session queries.

ABSTRACT

session 6
1.pocono mountains pennsylvania
2.pocono mountains pennsylvania hotels
3.pocono mountains pennsylvania things to do
4.pocono mountains pennsylvania hotels
5.pocono mountains camelbeach
6.pocono mountains camelbeach hotel
7.pocono mountains chateau resort
8.pocono mountains chateau resort attractions
9.pocono mountains chateau resort getting to
10.chateau resort getting to
11.pocono mountains chateau resort directions
session 85
1.glass blowing
2.glass blowing science
3.scientific glass blowing

Session search is the Information Retrieval (IR) task that
performs document retrieval for a search session. During a
session, a user constantly modifies queries in order to find
relevant documents that fulfill the information need. This
paper proposes a novel query change retrieval model (QCM),
which utilizes syntactic editing changes between adjacent
queries as well as the relationship between query change and
previously retrieved documents to enhance session search.
We propose to model session search as a Markov Decision
Process (MDP). We consider two agents in this MDP: the
user agent and the search engine agent. The user agentâ€™s
actions are query changes that we observe and the search
agentâ€™s actions are proposed in this paper. Experiments
show that our approach is highly effective and outperforms
top session search systems in TREC 2011 and 2012.

From Table 1, we notice that queries change constantly
in a session. The patterns of query changes include general
to specific (pocono mountains â†’ pocono mountains park),
specific to general (france world cup 98 reaction â†’ france
world cup 98), drifting from one to another (pocono mountains park â†’ pocono mountains shopping), or slightly different expressions for the same information need (glass blowing
science â†’ scientific glass blowing). These changes vary and
sometimes even look random (gun homicides australia â†’
martin bryant port arthur massacre), which increases the
difficulty of understanding user intention. However, since
query changes are made after the user examines search results, we believe that query change is an important form
of feedback. We hence propose to study and utilize query
changes to facilitate better session search.
One approach to handle query change is to classify them
based on various types of explorations [20], such as specification, generalization, drifting, or slight change, then perform
retrieval. Another approach is mapping queries into semantic graphical representations, such as ontologies [7] or query
flow graphs developed from query logs [2], then studying
how queries move in the graphs. However, ontology mapping is challenging [17], which may introduce inaccurate intermediate results and hurt the search accuracy. Moreover,
relying on large scale query logs may not be applicable due
to lack of such data. Therefore, although these approaches
have been applied to IR tasks such as query reformulation
[3] and query suggestion [2, 30], they have yet to be directly
applied to session search. It is therefore necessary to explore
new solutions to utilize query change for session search.
We propose to model session search as a Markov Decision
Process (MDP) [16, 28], which is applicable to many human
decision processes. MDP models a state space and an action
space for all agents participating in the process. Actions
from the agents influence the environment/states and the
environment/states influence the agentsâ€™ subsequent actions

Categories and Subject Descriptors
H.3.3 [Information Systems ]: Information Storage and
Retrievalâ€”Information Search and Retrieval

Keywords
Session search; query change model; retrieval model

1.

session 28
1.france world
cup 98 reaction
stock market
2.france world
cup 98 reaction
3.france world
cup 98
session 32
1.bollywood
legislation
2.bollywood law
session 37
1.Merck lobbists
2.Merck lobbying
US policy

INTRODUCTION

Session search is the Information Retrieval (IR) task that
retrieves documents for a search session [4, 8, 13, 14, 15, 25,
32]. During a search session, a user keeps modifying queries
in order to find relevant documents that fulfill his/her information needs. In session search, many factors, such as relevance feedback, clicked data, changes in queries, and user
intentions, are intertwined together and make it a quite challenging IR task. TREC (Text REtrieval Conference) 20102012 Session tracks [18, 19, 20] studied session search with
a focus on the â€œcurrent queryâ€ task, which retrieves relevant
documents for the current/last query in a session based on
previous queries and interactions. Table 1 shows examples
from the TREC 2012 Session track.1
1

All examples mentioned in this paper are from TREC 2012.
For simplicity, we use â€˜sxâ€™ to refer to a TREC 2012 session
where x is the session identification number.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM or the author must be honored. To
copy otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
SIGIRâ€™13, July 28â€“August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

453

Table 2: Evidence that query change âˆ†q appears in previous
search results Diâˆ’1 .

qtheme
+âˆ†q
âˆ’âˆ†q
Total

2.
Figure 1: Session search MDP. (example is from s32)

# in TRECâ€™11
âˆˆ Diâˆ’1
âˆˆ
/ Diâˆ’1
184
20
80
124
141
63
204 adjacent query
pairs, 76 sessions

# in TRECâ€™12
âˆˆ Diâˆ’1
âˆˆ
/ Diâˆ’1
178
21
97
102
112
87
199 adjacent query
pairs, 98 sessions

USER AGENT: QUERY CHANGE AS A
FORM OF FEEDBACK

We define a search session S = {Q, D, C} as a combination
of a series of queries Q = {q1 , ..., qi , ..., qn }, retrieved document sets D = {D1 , ..., Di , ..., Dn }, and clicked information
C = {C1 , .., Ci , ..., Cn }, where n is the number of queries in
the session (i.e., the session length) and i indexes the queries.
In TREC 2010-2012 Session tracks, each retrieved document
set Di contains the top 10 retrieval results di1 , ..., di10 ranked
in decreasing relevance for qi . Each clicked data Ci contains
the user-clicked documents, clicking order, and dwell time.
For instance, for s6 q6 , pocono mountains camelbeach hotel
(Table 1), C6 tells us that the user clicked the 4th ranked
search result, followed by the 2nd , with dwell time 15 seconds
and 17 seconds, respectively.
TREC 2010-2012 Session Tracks aim to retrieve a list of
documents for the current query, i.e. the last query qn in
a session, ordered in decreasing relevance. Without loss of
specificity, we assume that any query between q1 to qn could
be the last query. We therefore study the problem of retrieving relevant documents for qi , given all previous queries q1
to qiâˆ’1 , previous retrieval results D1 to Diâˆ’1 , and previous
clicked data C1 to Ciâˆ’1 .
We define query change âˆ†qi as the syntactic editing changes
between two adjacent queries qiâˆ’1 and qi :

based on certain policies. A transition model between states
indicates the dynamics of the entire system. In our MDP,
queries are modeled as states. Previous queries that the
user wrote influence the search results; the search results
again influence the userâ€™s decision of the next query. This
interaction continues until the search stops.
As illustrated in Figure 1, we consider two agents in this
entire process: the user agent and the search engine agent.
The user agentâ€™s actions are mainly human actions that are
able to change search results, such as adding and deleting
query terms, i.e. query change. Clicking is a human action;
however, it does not explicitly impact the retrieval. Therefore, it is not considered as a user action here. Query change
is the only form of user action in this paper. Based on the
user actions, we design corresponding policies for the search
engine agent; which is the main focus of this paper.
It is difficult to interpret the user intent [5, 31] behind
query change. For instance, for a query change from Kurosawa to Kurosawa wife (s38), there is no indication about
â€˜wifeâ€™ in the search results returned for the first query. However, Kurosawaâ€™s wife is actually among the information
needs provided to the user by NISTâ€™s topic descriptions. Our
experience with TREC Session tracks suggests that information needs and previous search results are two main factors
that influence query change. However, knowing information
needs before search could not easily be achieved. This paper focuses on utilizing evidence found in previous search
results and the relationship between previous search results
and query change to improve session search.
In this paper, we summarize various types of query changes
based on potential user intents into user agent policies. We
further propose corresponding policies for the search engine
agent and model them in the query change retrieval model
(QCM), a novel reinforcement learning [16] inspired framework. The relevance of a document to the current query
is recursively calculated as the reward beginning from the
starting query and continuing until the current query. This
research is perhaps the first to employ reinforcement learning to tackle session search. Our experiments demonstrate
that the proposed approach is highly effective and outperforms the best performing TREC 2011 and 2012 session
search systems.
The remainder of this paper is organized as follows. Section 2 analyzes query change and summarizes policies for
the user agent. Section 3 proposes policies for the search engine agent. Section 4 elaborates the query change retrieval
model. Section 5 discusses how to handle duplicated queries.
Section 6 evaluates our approach, followed by a discussion in
Section 7. Section 8 presents the related work and Section
9 concludes the paper.

âˆ†qi = qi âˆ’ qiâˆ’1
qi can be written as a combination of the shared portion
between qi and qiâˆ’1 and query change: qi = (qi âˆ©qiâˆ’1 )+âˆ†qi .
The query change âˆ†qi comes from two sources. First, the
added terms, which we call positive âˆ†q, are new terms that
the user adds to the previous query. Second, the removed
terms, which we call negative âˆ†q, are terms that the user
deletes from the previous query. For example, in Table 1 s37,
â€˜USâ€™ and â€˜policyâ€™ are the added terms; while in s28, â€˜stockâ€™
and â€˜marketâ€™ are the removed terms.
We call the common terms shared by two adjacent queries
theme terms since they often represent the main topic of a
session. For example, in Table 1 s37 the theme terms are
â€œMerck lobbyâ€.2
We thus decompose a query into three parts as theme
terms, added terms, and removed terms and write it as:
qi = (qi âˆ©qiâˆ’1 )+(+âˆ†qi )âˆ’(âˆ’âˆ†qi ) = qtheme +(+âˆ†qi )âˆ’(âˆ’âˆ†qi )
where qtheme are the theme terms, +âˆ†qi and âˆ’âˆ†qi represent
added terms and removed terms, respectively.
Our observations suggest that documents that have been
examined by the user factor in deciding the next query
change. We therefore propose the following important assumption between âˆ†qi , the query change between adjacent
2
We perform K-stemming to all query terms. For instance,
â€˜lobbistsâ€™ and â€˜lobbyingâ€™ are both stemmed to â€˜lobbyâ€™.

454

Table 3: User agentâ€™s policies and actions about a query term t âˆˆ qiâˆ’1 . (Refer to sessions shown in Table 1)

t âˆˆ Diâˆ’1

tâˆˆ
/ Diâˆ’1

user intention
1. find more information about
t
2.
satisfied &
move to the next
information need
3. satisfied

4.
inspired by
terms t0 in Diâˆ’1

user likes Diâˆ’1
user action
example
add new terms s85
t0 about t
q1 â†’ q2

user dislikes Diâˆ’1
user action
example
remove t
s28
q1 â†’ q2

type
specification

user intention
5. remove the wrong
terms
6.
not satisfied &
move to the next information need
7. try different expression for t

remove
t
& add new
terms t0
slight
change of t
to t0

s6
q6 â†’ q7

drift

s85
q2 â†’ q3

slight change

8. try different expression for t to get
more documents for t

slight
change of t
to t0

s32
q1 â†’ q2

slight change

remove t &
add new terms
t0 as new focus
keep t

s6
q8 â†’ q9

drift

theme
term

no change

add terms t00
about t0

s37
q1 â†’ q2

specification

queries qi and qiâˆ’1 , and Diâˆ’1 , the search results for qiâˆ’1 :

type
generalization

results Diâˆ’1 (the left most column) and whether the user
likes Diâˆ’1 and the occurrence of t in Diâˆ’1 (the top most
row). Combinations of the two dimensions yield 4 main
cases (as in a contingency table) and 8 sub-cases. For each
case, we identify four items: a rough guess of user intention,
the userâ€™s actual action, an example, and the semantic exploration type for this action. For example, query change
in s6 q8 â†’ q9 , pocono mountains chateau resort attractions
â†’ pocono mountains chateau resort getting to can be interpreted as the following. Previous query term â€˜attractionsâ€™
appears in Diâˆ’1 and the user likes the returned documents
Diâˆ’1 . One possibility is that he is satisfied with what he
reads and moves to the next information need. Therefore,
the user removes â€˜attractionâ€™ and adds new terms â€œgetting
toâ€ as the new query focus. This is a drift in search focus.
(case 2 in Table 3)
We further group the cases in Table 3 by types of user
actions, i.e., query change, and summarize them into:

âˆ†qi â† Diâˆ’1 .
The assumption basically says that previous search results
decide query change. In fact, previous search results Diâˆ’1
could influence query change âˆ†qi in quite complex ways. For
instance, the added terms in s37 (Table 1) q1 to q2 , are â€˜USâ€™
and â€˜policyâ€™. D1 contains several mentions of â€˜policyâ€™, such
as â€œA lobbyist who until 2004 worked as senior policy advisor
to Canadian Prime Minister Stephen Harper was hired last
month by Merckâ€. However, these â€˜policyâ€™-related mentions
are about â€œCanada policyâ€ whereas the user adds â€œUS policyâ€
in q2 . This suggests that the user might have been inspired
by â€˜policyâ€™ in D1 , however he preferred the policy in US, not
in Canada. Therefore, instead of simply cutting and pasting
identical terms from Diâˆ’1 , the user creates related terms to
add for the next search.
In another example, s28 (Table 1) q1 , â€˜stockâ€™ and â€˜marketâ€™
are frequent terms that are similar to stopwords. Documents in D1 are hence all about them and totally ignore the
theme terms â€œfrance world cup 98.â€ In q2 , the user removes
â€œstock marketâ€ to boost rankings for documents about the
theme terms. In this case, removing terms is not only about
generalization, but also about document re-ranking.
To provide a convincing foundation for our approach, we
look for evidence to support our assumption. We investigate
whether âˆ†qi (at least) appears in Diâˆ’1 . Table 2 shows how
often theme terms, added terms, and removed terms are
present in Diâˆ’1 for both TREC 2011 and 2012 datasets.
Around 90% of the time theme terms occur in Diâˆ’1 and
most removed terms (>60%) appear in Diâˆ’1 .3 Added terms
are new terms for the previous query qiâˆ’1 ; we thus expect
to see few occurrences of added terms in Diâˆ’1 . Surprisingly,
however, more than a third of them appear in Diâˆ’1 . It
suggests that it is quite probable that previous search results
motivate the subsequent query change.
Table 3 summarizes various types of query changes into
possible policies for the user agent. This table mainly serves
as a guide for us to design the policies for the search engine
agent. We do not perform a thorough user study to validate
this table. However, we believe that it is a good representative of various search scenarios and can help design a good
session search agent.
Along two dimensions, Table 3 summarizes the user agentâ€™s
actions and possible policies. The dimensions are whether
a previous query term t âˆˆ qiâˆ’1 appears in previous search

â€¢ Theme terms (qtheme ), terms that appear in both qiâˆ’1
and qi . In fact, they often appear in many queries in
a session. It implies a strong preference for those terms
from the user. If they appear in Diâˆ’1 , it shows that the
user favors them since the user issues them again in qi . If
they do not appear in Diâˆ’1 , the user still favors towards
them and insists to include them in the new query. This
corresponds to t in cases 1 and 3 in Table 3.
â€¢ Added terms (+âˆ†q), terms that appear only in qi , not in
qiâˆ’1 . They indicate specification, destination of drifting,
or destination of slight change. If they appear in Diâˆ’1 ,
for the sake of novelty [14], they will not be favored in Di .
If they do not appear in Diâˆ’1 , which means that they are
novel and the user favors them now. This corresponds to
t0 in cases 1, 2, 6, 7, and 8, and t00 in case 4 in Table 3.
â€¢ Removed terms (âˆ’âˆ†q), terms that appear only in qiâˆ’1 ,
not in qi . They indicate generalization, source of drifting, and source of slight change. If they appear in Diâˆ’1 ,
removing them means that the user observes them and
dislikes them. If they do not appear in Diâˆ’1 , the user still
dislikes the terms since they are not in qi anyway. This
corresponds to t in cases 2, 5, 6, 7, and 8 in Table 3.

3.

SEARCH ENGINE AGENT: STRATEGIES
TO IMPROVE SEARCH

The search engine agent observes query change from the
user agent and takes corresponding actions. For each type of
query change, theme terms, added terms, and removed terms,
we propose to adjust the term weights accordingly for better
retrieval accuracy. The search engine agentâ€™s action include

3
A third of query terms that do not appear in Diâˆ’1 are
removed by the user.

455

Table 4: Search engine agentâ€™s policy. Actions are adjustments on the term weights. â†‘ means increasing, â†“ means
decreasing, and â†’ means keeping the original term weight.
âˆˆ Diâˆ’1 action Example
Y
â†‘
â€œpocono mountainâ€ in s6
qtheme
â€œfrance world cup 98 reactionâ€ in
N
â†‘
s28, q1 â†’ q2
Y
â†“
â€˜policyâ€™ in s37, q1 â†’ q2
+âˆ†q
N
â†‘
â€˜USâ€™ in s37, q1 â†’ q2
Y
â†“
â€˜reactionâ€™ in s28, q2 â†’ q3
âˆ’âˆ†q
N
â†’
â€˜legislationâ€™ in s32, q2 â†’ q3

system. A policy Ï€(s) = a indicates that at a state s, what
are the actions a can be taken by the agent. In session
search, we employ queries as states. Particularly, we denote
q as state, T as the transition model P (qi |qiâˆ’1 , aiâˆ’1 ), D
as documents, and A as actions. Actions include keeping,
adding, and removing query terms for the user agent and
increasing, decreasing, and maintaining the term weights for
the search engine agent.
In a MDP, each state is associated with a reward function
R that indicates possible positive reward or negative loss
that a state and an action may result. In session search, we
consider the reward function to be the relevance function.
Reinforcement learning [16] offers general solutions to MDP
and seeks for the best policy for an agent. Each policy has
a value associated with the policy and denoted as VÏ€ (s),
which is the expected long-term reward starting from state
s and continuing with policy Ï€ from then on. In a MDP, it
is believed that a future reward is not worth quite as much
as a current reward and thus a discount factor Î³ âˆˆ (0, 1) is
applied to future rewards. By considering the discount factor, the value function starting from s0 for a policy Ï€ can be
2
written
Pâˆ as:t VÏ€ (s0 ) = EÏ€ [R(s0 ) + Î³R(s1 ) + Î³ R(s2 ) + ...] =
EÏ€ [ t=0 Î³ R(si )]. The Bellman equation [16] describes the
optimal value V âˆ— for a state s in the long run and is often
used to obtain the best value for a MDP:
X
V âˆ— (s) = max R(s, a) + Î³
P (s0 |s, a)V âˆ— (s0 )

increasing, decreasing, and maintaining the term weights.
Based on the observed query change as well as whether the
query terms appeared in the previous search results Diâˆ’1 ,
we can sense whether the user will favor the query terms
in the current run of search. Table 4 illustrates the policies
that we propose for the search engine agent.
As shown in Section 2, theme terms qtheme often appear
in many queries in a session and there is a strong preference
for them. Thus, we propose to increase the weights of theme
terms no matter whether they appeared in Diâˆ’1 or not (rows
1 and 2 in Table 4). In the latter case, if a theme term was
not found in Diâˆ’1 (top retrieval results), it is likely that the
documents containing them were ranked low. Therefore, the
weights of theme terms need to be raised to boost the rankings of those documents (row 2 in Table 4). However, since
theme terms are topic words in a session, they could appear
like stopwords within the session. To avoid biasing too much
towards them, we lower their term weights proportionally to
their numbers of occurrences in Diâˆ’1 .
For added terms +âˆ†q, if they occurred in previous search
results Diâˆ’1 , we propose to decrease their term weights
for the sake of novelty [14]. For example, in s5 q1 â†’ q2 ,
â€œpocono mountainsâ€â†’â€œpocono mountains parkâ€, the added
term â€˜parkâ€™ appeared in a document in D5 . If we use the
original weight of â€˜parkâ€™, this document might still be ranked
high in D2 and the user may dislike it since he read it before.
We hence decrease added termsâ€™ weights if they are in Diâˆ’1
(row 3 in Table 4). On the other hand, if the added terms
did not occur in Diâˆ’1 , they are the new search focus and
we increase their term weights (row 4 in Table 4). In an interesting case (s37 q1 â†’ q2 ), part of +âˆ†q, â€˜policyâ€™, occurred
in D1 whereas the other part, â€˜USâ€™, did not. To respect the
userâ€™s preference, we increase the weight of â€˜USâ€™ while decreasing that of â€˜policyâ€™ to penalize documents about other
â€˜policesâ€™ including â€œCanada policyâ€.
For removed terms âˆ’âˆ†q, if they appeared in Diâˆ’1 , their
term weights are decreased since the user dislikes them by
deleting them (row 5 in Table 4). For example, in s28 q2 â†’
q3 , â€˜reactionâ€™ existed in D2 and is removed in q3 . However, if
the removed terms are not in Diâˆ’1 , we do not change their
weights since they are already removed from qi by the user
(row 6 in Table 4).
In the sections below, we follow policies proposed for the
search engine agent as shown in Table 4 and incorporate
them into a novel query change retrieval model (QCM).

4.

a

s0

where s0 is the next state after s, V âˆ— (s) and V âˆ— (s0 ) are the
optimal values for s and s0 .
For session search, we observe that the influence of previous queries and previous search results to the current queries,
becomes weaker and weaker. The userâ€™s desire for novel documents also supports this argument. We hence propose to
employ the reinforcement learning model backwards. That
is, instead of discounting the future rewards, we discount
the past rewards, i.e. the relevant documents that appeared
in the previous search results.
We propose the query change retrieval model (QCM) as
the following. We consider the task of retrieving relevant
documents for qi as ranking documents based on the reward, i.e., how relevant it is to qi . Inspired by the Bellman
equation, we model the relevance of a document d to the
current query qi as:
Score(qi , d) = P (qi |d)+Î³

X
a

P (qi |qiâˆ’1 , Diâˆ’1 , a) max P (qiâˆ’1 |Diâˆ’1 )
Diâˆ’1

(1)
which recursively calculates the reward starting from q1 and
continues with the search engine agentâ€™s policy until qi . Î³ âˆˆ
(0, 1) is the discount factor, maxDiâˆ’1 P (qiâˆ’1 |Diâˆ’1 ) is the
maximum of the past rewards, P (qi |d) is the current reward,
and P (qi |qiâˆ’1 , Diâˆ’1 , a) is the query transition model.
The first component in Eq.1, P (qi |d), measures the relevance between qi and a document d that is under evaluation. This component can be estimated
Q by the Bayesian
belief network model [27]: P (qi |d) = 1 âˆ’ tâˆˆqi (1 âˆ’ P (t|d)),
where P (t|d) is calculated by the multinomial query generation language model with Dirichlet smoothing [33]: P (t|d) =
#(t,d)+ÂµP (t|C)
, where #(t, d) denotes the number of occur|d|+Âµ
rences of term t in document d, P (t|C) calculates the probability that t appears in corpus C based on Maximum Like-

MODELING SESSION SEARCH

Markov Decision Process (MDP) [16, 28] models a state
space S and an action space A. Its states S = {s1 , s2 , ...}
change from one to another according to a transition model
T = P (si+1 |si , ai ), which models the dynamics of the entire

456

lihood Estimation (MLE), |d| is the document length, and
Âµ is the Dirichlet smoothing parameter (set to 5000).
The remaining challenges of calculating Eq.1 include maximizing the reward function maxDiâˆ’1 P (qiâˆ’1 |Diâˆ’1 ) and estimating the transition model P (qi |qiâˆ’1 , Diâˆ’1 , a). They are
described in Section 4.1 and Section 4.2, respectively.

4.1

ognize added terms +âˆ†q and removed terms âˆ’âˆ†q. Generally, the terms that occur in the current query but not in
the previous query constitute +âˆ†q; while the terms occur in
the previous query but not in the current query constitute
âˆ’âˆ†q. In the above example, âˆ’âˆ†q7 = â€œcamelbeach hotelâ€,
and +âˆ†q7 = â€œchateau resortâ€.
The search engine actions are decreasing, increasing, and
maintaining term weights. According to Table 4 rows 3 and
5, we decrease a termâ€™s weight if the query change, either
+âˆ†q or âˆ’âˆ†q, occurred in the effective previous search results Diâˆ’1 . We propose to deduct term tâ€™s weight by P (t|d),
i.e. tâ€™s default contribution to the relevance score between
qi and the document under evaluation (denoted as d). Furthermore, since t already occurred in Diâˆ’1 , for the sake of
novelty, we deduct more weight that is proportional to tâ€™s
frequency in Diâˆ’1 such that the more frequently t occurred
in Diâˆ’1 , the more heavily tâ€™s weight is deducted from the
current query qi and d. We formulate this weight deduction
for a term t âˆˆ +âˆ†q or t âˆˆ âˆ’âˆ†q as:

Maximizing the Reward Function

When considering the past/future rewards, MDP uses only
the optimal (the maximum possible) values from those past
/future rewards. This is reflected in maxDiâˆ’1 P (qiâˆ’1 |Diâˆ’1 )
as part of Eq. 1.
Prior research [10, 22] suggests that Satisfying (SAT) clicks,
i.e., clicked documents with dwell time longer than 30 seconds [10, 22], are probably the only ones that are effective
at predicting user behaviors and relevance judgments. Since
the user also skims snippets in search interactions, in this
work, we consider both the top 10 returned snippets and
SAT clicks as effective previous search results and denote
e
them as Diâˆ’1
.
To obtain an maximum reward from all possible reward
functions P (qiâˆ’1 |diâˆ’1 ), i.e., the text relevance of previous
query qiâˆ’1 and all previous search results diâˆ’1 âˆˆ Diâˆ’1 , we
propose to generate a maximum rewarding document, denoted as dâˆ—iâˆ’1 . We further propose that the candidates for
the dâˆ—iâˆ’1 should only be selected from the effective previe
ous search results Diâˆ’1
. We define dâˆ—iâˆ’1 as the document(s)
that is the most relevant to qiâˆ’1 . To discover dâˆ—iâˆ’1 , we first
rank all the documents (either a snippet or a document)
e
diâˆ’1 âˆˆ Diâˆ’1
by measuring the
Q relevance between qiâˆ’1 and
diâˆ’1 as: P (qiâˆ’1 |diâˆ’1 ) = 1 âˆ’ tâˆˆqiâˆ’1 {1 âˆ’ P (t|diâˆ’1 )}, where

log Pnew (t|d) = (1 âˆ’ P (t|dâˆ—iâˆ’1 )) log P (t|d)

(2)

dâˆ—iâˆ’1

where
denotes the maximum rewarded document, d is
the document under evaluation, and P (t|d) is calculated by
MLE. We apply the log function to avoid numeric underflow.
We notice that Eq. 2 has an interesting connection with
the Kullback-Leibler divergence (KL divergence) [33]:
âˆ’ P (t|dâˆ—iâˆ’1 ) log P (t|d) = P (t|dâˆ—iâˆ’1 ) log

1
P (t|d)

P (t|dâˆ—iâˆ’1 )
(3)
= P (t|dâˆ—iâˆ’1 ) log
P (t|d)


#(t,diâˆ’1 )
rank
, #(t, diâˆ’1 )
P (t|diâˆ’1 ) is calculated by MLE: P (t|diâˆ’1 ) = |diâˆ’1
= KLDt Î¸dâˆ—iâˆ’1 ||Î¸d
|
is the number of occurrences of term t in document diâˆ’1 , and


|diâˆ’1 | is the document length. We do not apply smoothing
where KLDt Î¸dâˆ—iâˆ’1 ||Î¸d denotes the contribution of term
e
here since P (t|diâˆ’1 ) can be zero, i.e., t âˆˆ
/ Diâˆ’1 . In fact, we
t to the KL divergence between two documentsâ€™ language
rely on this property in later calculation.
models Î¸dâˆ—iâˆ’1 and Î¸d . In Eq. 3, the larger the divergence bee
âˆ—
After ranking documents diâˆ’1 in Diâˆ’1 , we generate diâˆ’1
tween Î¸dâˆ—iâˆ’1 and Î¸d , the more novel document d is compared
by the following options: (1) using the document with the
to Diâˆ’1 , and the less deduction to the relevance score. In
largest P (qiâˆ’1 |diâˆ’1 ), (2) concatenating the top k documents
e
this sense, Eq. 2 models novelty for the added terms and
in Diâˆ’1
with the largest P (qiâˆ’1 |diâˆ’1 ), or (3) concatenating
e
the removed terms during a query transition.
all documents in Diâˆ’1 . Experiments show that option (1)
According to Table 4 row 4, we increase a termâ€™s weight if
works the best and we use this setting throughout the paper.
it is an added term and did not occur in Diâˆ’1 . We propose
For notation simplicity, we use Diâˆ’1 from now on to denote
to raise the term weight proportional to its inverse document
effective previous search results.
frequency (idf). This is to make sure that while increasing
4.2 Estimating the Transition Model
a preferred termâ€™s weight, we avoid increasing its weight
P
too much if it is a common term in many documents. We
The transition model indicated in Eq. 1 is a P (qi |qiâˆ’1 ,
formulate this weight increase for a novel added term t (t âˆˆ
Diâˆ’1 , a). It includes the probabilities of query transitions
+âˆ†q and t âˆˆ
/ Diâˆ’1 ) as:
under various actions. We incorporate polices designed in
Table 4 to calculate it.
log Pnew (t|d) = (1 + idf (t)) log P (t|d)
(4)
Search engine agent performs actions based on user agentâ€™s
actions. We need to identify userâ€™s actions, i.e. query change
where idf (t) is the inverse document frequency of t in Corpus
C and P (t|d) is calculated by MLE. Note that this term
âˆ†q before search engine takes actions. Particularly, we recognize âˆ†q by the following procedure. First, we generate
weight adjustment is in a form of tf-idf.
qtheme based on the Longest Common Subsequence (LCS)
The increasing in term weights also applies to theme terms,
[11] in both qiâˆ’1 and qi . A subsequence is a sequence that
which corresponds to rows 1 and 2 in Table 4. Theme terms
appears in two strings in the same relative order but is not
repeatedly appear in a session, which implies the impornecessarily continuous. The LCS can be the common prefix
tance of them. Similar to the novel added terms, we should
or the common suffix of the two queries; it can also consist
avoid increasing their weights too much. We could discount
of several discontinuous common parts from the two queries.
the increment proportional to idf. However, theme terms
Take s6 q6 â†’ q7 as an example: q6 =â€œpocono mountains
are topical/common terms within a session, not necessarily
common terms in the entire corpus. Therefore, idf may not
camelbeach hotelâ€, q7 =â€œpocono mountains chateau resortâ€,
qtheme = LCS(q6 , q7 ) = â€œpocono mountainsâ€. Next, we recbe applicable here. We hence employ the negation of the
rank

457

Table 5: Dataset statistics for TREC 2011 and 2012 Session.

number of occurrences of t in previous maximum rewarding
document, 1 âˆ’ P (t|dâˆ—iâˆ’1 ), to substitute idf. We formulate
this weight increase for a theme term t âˆˆ qtheme as:
log Pnew (t|d) = (1 + (1 âˆ’ P (t|dâˆ—iâˆ’1 ))) log P (t|d)

#topics
#sessions
#queries
#dups

(5)

dâˆ—iâˆ’1

where
denotes the maximum rewarded document and
P (t|d) is calculated by MLE.
For removed terms that did not appear in Diâˆ’1 (Table 4
row 6), the search agent does not change their term weights.
By considering all possible cases for the transition model
as defined in Eq. 1, the relevance score between the current
query qi and a document d is represented as below:
X
Score(qi , d) = log P (qi |d) + Î±
[1 âˆ’ P (t|dâˆ—iâˆ’1 )] log P (t|d)
tâˆˆqtheme

X

âˆ’Î²

P (t|dâˆ—iâˆ’1 ) log P (t|d) + 

X

idf (t) log P (t|d)

tâˆˆ+âˆ†q
tâˆˆd
/ âˆ—
iâˆ’1

tâˆˆ+âˆ†q
tâˆˆdâˆ—
iâˆ’1

âˆ’Î´

X

P (t|dâˆ—iâˆ’1 ) log P (t|d)

(6)
where Î±, Î², , and Î´ are parameters for each types of actions.
Note that we apply different parameters Î² and Î´ on +âˆ†q and
âˆ’âˆ†q, since added terms and removed terms may affect the
retrieval differently. We report the parameter selection in
Section 6.

2011
3.68
1.23
19,413
2

EVALUATION

The evaluation datasets are from TREC 2011 and 2012
Session tracks [18, 19]. Table 5 lists the statistics about
these two datasets. Each search session includes several
queries and the corresponding search results. The users
(NIST assessors) were given a topic description about information needs before they searched. For example, s85 (Table
1) are related to topic 43 â€œWhen is scientific glass blowing
used? What are the purposes? What organizations do scientific glass blowing?â€ Multiple sessions can relate to the
same topic. The search engine used to create the sessions
was Yahoo! BOSS. The top 10 returned documents were
shown to the users and they clicked documents that were
interesting to them and interacted with the system. We use
TRECâ€™s official ground truth and official evaluation metrics
nDCG@10 and MAP.
The corpus used in this evaluation is ClueWeb09 Category B collection (CatB).4 CatB contains the first 50 million English pages crawled from the Web during January
to February 2009. We filter out the spam documents by
removing documents whose WateQCMooâ€™s â€œGroupXâ€ spam
ranking scores [6] are less than 70.
We compare the following systems in this evaluation:

(7)

as a base case. P (q1 |d) is calculated by Eq. 4.
Using Eq. 7 as the base case for the recursive function
described in Eq. 1, we obtain the overall document relevance
score Scoresession (qn , d) for a session that starts at q1 and
ends at qn by considering all queries in the session:
Scoresession (qn , d) = Score(qn , d) + Î³Scoresession (qnâˆ’1 , d)
= Score(qn , d) + Î³ [Score(qnâˆ’1 , d) + Î³Scoresession (qnâˆ’2 , d)]
n
X
Î³ nâˆ’i Score(qi , d)
=
i=1

(8)
where q1 , q2 , Â· Â· Â· , qn are in the same session, and Î³ âˆˆ (0, 1)
is the discount factor. Eq. 8 provides a form of aggregation
over the relevance functions of all the queries in a session.

5.

#queries/session
#sessions/topic
#pages judged
#sessions w/o rel. docs

Next we determine exact string matches between every query
pair. The exactly matched query pairs are identified as duplicated queries.
Since the user may dislike the queries and their corresponding search results between two duplicated queries, we
propose to eliminate from the MDP the undesired queries
and their interactions. We achieve this by setting the discount factor to zero for any interaction between two duplicated queries as well as that for the earlier query in the two.
The new discount factor Î³ 0 can be calculated as:
(
0 {i|i âˆˆ [j, k), âˆƒqj = qk , j < k)}
0
(9)
Î³i =
Î³i otherwise

6.

Scoring the Entire Session

It is worth noting that Eq. 6 is valid only when i > 1.
When i = 1, there is no previous result for q1 . We thus use
Score(q1 , d) = log P (q1 |d)

2012
48
98
297
5

where Î³i is the original discount factor for the ith query, Î³i0
is the updated discount factor for the ith query after deduplication.
For the above example s6, the effects from q2 and q3 on the
session are eliminated. The entire session is now equivalent
to q1 , q4 , q5 , ..., q11 .

tâˆˆâˆ’âˆ†q

4.3

2011
62
76
280
16

DUPLICATED QUERIES

Duplicated queries sometimes occur in a search session.
Prior work shows that removing duplicated queries could
effectively boost the search accuracy [8, 19]. Duplicated
queries often occur when a user is frustrated by irrelevant
documents in search results and comes back to one of the
previous queries for a fresh start. For example, in s6 (Table 1), q2 and q4 are duplicates and both search for pocono
mountains pennsylvania hotels. The query between them is
q3 : pocono mountains pennsylvania things to do. It suggests
that the user might dislike the search results for q3 and he
returns to q2 to search again (q2 = q4 ).
To detect query duplicates, we first remove punctuations
and white spaces in queries, then apply stemming on them.

â€¢ Lemur : Directly submitting the current query qn (with
punctuations removed) to the Lemur search engine [21]
(language modeling + Dirichlet smoothing) and obtain
the returned documents.
â€¢ TREC best : The top TREC system as reported by NIST
[13, 14]. It adopts a query generation model with relevance feedback and handles document novelty. CatB was
used in their TREC submissions. This system is used as
the baseline system in this evaluation.
â€¢ Nugget: Another top TREC 2012 session search system
groups semantically coherent query terms as nuggets and
4

458

http://lemurproject.org/clueweb09/.

2012
3.03
2.04
17,861
4

Figure 2: nDCG@10 for TREC 2012 against the parameters. (a), (b), (c), and (d) are about Î±, Î², , and Î´ respectively.
Table 6: nDCG@10, MAP, and their improvements over
the baseline (%chg) for TREC 2012 sessions. The runs are
sorted by nDCG@10. A statistical significant improvement
over the baseline is indicated with a â€  at p < 0.05 level.
Approach
Lemur
TREC median
Nugget
TREC best
QCM
QCM+Dup

nDCG@10
0.2474
0.2608
0.3021
0.3221
0.3353
0.3368

%chg
-21.54%
-17.29%
âˆ’4.19%
0.00%
4.10%â€ 
4.56%â€ 

MAP
0.1274
0.1440
0.1490
0.1559
0.1529
0.1537

5). However, the best approach for TREC 2011 is the nugget
approach, which is slightly better than QCM+Dup.
Table 5 illustrates the dataset differences between TREC
2011 and 2012. These differences may affect search accuracy.
The average number of sessions per topic is 2.04 in 2012,
that is more than that in 2011 (1.23). Moreover, on average,
TREC 2012 sessions contain less queries per session (3.03)
than 2011 (3.68). As a result, the shorter sessions in 2012
may make the search task more difficult than 2011 since less
information are provided by previous interactions. Another
difference is that 2012 sessions have fewer (sometimes even
none) relevant documents than 2011 sessions in CatB ground
truth. It unavoidably hurts the performance for any retrieval
system. Generally, we observe lower search accuracy in 2012
(Table 6) than in 2011 (Table 7).

%chg
-18.28%
-7.63%
-4.43%
0.00%
-1.92%
-1.41%

Table 7: nDCG@10, MAP, and their improvements over
the baseline (%chg) for TREC 2011 sessions. The runs are
sorted by nDCG@10. A statistical significant improvement
over the baseline is indicated with a â€  at p < 0.05 level.
Approach
Lemur
TREC median
TREC best
QCM
QCM+Dup
Nugget

nDCG@10
0.3378
0.3544
0.4409
0.4728
0.4821
0.4836

%chg
-23.38%
-19.62%
0.00%
7.24%â€ 
9.34%â€ 
9.68%â€ 

MAP
0.1118
0.1143
0.1508
0.1713
0.1714
0.1724

%chg
-25.86%
-24.20%
0.00%
13.59%â€ 
13.66%â€ 
14.32%â€ 

6.2

We investigate good values for parameters in Eq. 6. A
supervised learning-to-rank method should be able to find
the optimal values for those parameters. However, in this
paper, we take a step-by-step parameter tuning procedure
and leave the supervised learning method as future work.
We add each component, i.e., theme terms, added terms,
and removed terms, one by one into Eq. 6. The tuning
is performed for QCM only and the parameters are shared
between QCM and QCM+Dup.
First, we plot nDCG@10 against Î± while setting other
parameters to 0 (Figure 2(a)). Î± represents the parameter
for theme terms. Î± ranges over [1.1, 2.5] by an interval of 0.1.
We notice that nDCG@10 reaches its maximum at Î± = 2.2.
We find 2 other local maximums at 1.6 and 1.2 for Î±.
Next, we fix Î± to the above values and plot nDCG@10
against Î² (Figure 2(b)). Î² is the parameter for added terms
that appeared in effective previous search results; we call
them old added terms. Î² ranges over [1.0, 2.4] by an interval
of 0.2. We choose the top 2 local values from each curve and
pick 6 combinations for (Î±, Î²) as indicated in Figure 2(c).
Then, we fix (Î±, Î²) and plot nDCG@10 against  (Figure
2(c)).  is the parameter for added terms that did not appear
in effective previous search results; we call them novel added
terms.  ranges over [0.05, 0.1] by an interval of 0.01. All the
curves show similar trends and reach the highest nDCG@10
at around 0.07. We hence fix  to 0.07.
Finally, we plot nDCG@10 against Î´ (Figure 2(d)) with
the parameter combinations that we discover eerlier. Eventually, nDCG@10 reaches its peak 0.3353 at Î± = 2.2, Î² =
1.8,  = 0.07, and Î´ = 0.4. We apply this set of parameters
to both QCM and QCM+Dup.
As we can see, Î±, Î², and Î´ are much larger than . This
is because that in Eq. 4, idf (t) = log(N/nd ) falls in the
range of [1, 10], while in Eq. 2 and Eq. 5, P (t|dâˆ—iâˆ’1 ) falls

creates structured Lemur queries [8]. We re-implement
and apply it on both TREC 2011 and 2012.
â€¢ TREC median: The median TREC system as reported by
NIST [18, 19].
â€¢ QCM : The proposed query change retrieval model.
â€¢ QCM + De-Duplicate (Dup): The proposed query change
retrieval model with duplicated queries removed.

6.1

Parameter Tuning

Search Accuracy

Table 6 and Table 7 demonstrate search accuracy for all
systems under comparison for TREC 2012 and TREC 2011,
respectively. The evaluation metrics are nDCG@10 and
MAP, the same as in the official TREC evaluations. TREC
best serves as the baseline.
Table 6 shows that the proposed QCM approach outperforms the best TREC 2012 system on nDCG@10 by 4.1%,
which is statistically significant (one sided t-test, p = 0.05).
The search accuracy is further improved by 0.46% through
removing the duplicated queries. The experimental results
strongly suggest that our approach is highly effective.
Table 7 shows that for TREC 2011, our approach again
outperforms the baseline by a statistically significant 7.24%
(one sided t-test, p = 0.05) and achieves a further improvement of 9.34% by the QCM+Dup approach. For TREC
2011, the performance gain by performing de-dup is 2.1%,
which is bigger than that for TREC 2012 (0.46%). The
reason is probably because that TREC 2012 only has 5 duplicated queries while TREC 2011 has 16 (shown in Table

459

by an interval of 0.02. Figure 3 illustrates the relationship
between nDCG@10 and Î³. nDCG@10 climbs to its peak
0.3368 when Î³ = 0.92. The result suggests that a good
discount factor Î³ is very close to 1, implying that previous
queries contribute to the overall search accuracy nearly the
same as the last query. It suggests that in QCM, a discount
between two adjacent queries should be mild.

7.
Figure 3: Discount factor Î³.

7.1

Figure 4: Error types.

DISCUSSION
Advantages of Our Approach

A main contribution of our approach is that we treat a
Table 8: Aggregation schemes.
search session as a continuous process by studying changes
Approach
qn
qi (i âˆˆ [1, n âˆ’ 1])
among query transitions and modeling the dynamics in the
Query change model
1
Î³ nâˆ’i
entire session. Through the reinforcement learning style
Uniform
Î»n = 1
Î»i = 1
framework, our system provides the best aggregation scheme
Aggregation
PvC
Î»n = 1 âˆ’ Î»p
Î»i = Î»p
Scheme
for all queries in a session (Table 9). This allows us to better
Î»p
Distance-based Î»n = 1 âˆ’ Î»p
Î»i = nâˆ’i
handle sessions that demonstrate evolution and exploration
in nature than most existing systems do. On the contrary,
Table 9: nDCG@10 for various aggregation schemes. Î»p is
for sessions that are clear in search goals and lack of a ex0.4 in PvC. Î³ is 0.92 in QCM and QCM+Dup. TREC 2012
ploratory nature, the advantage of our system over other
best serves as the baseline. A significant improvement over
systems looks less significant.
the baseline is indicated with a â€  at p < 0.05 level.
This can be seen in Table 10, which illustrates the search
Aggregation
TREC 2011
TREC 2012
accuracy
for the TREC best, Nugget, and our system for
Scheme
nDCG@10
%chg
nDCG@10
%chg
various classes of sessions. The TREC best is used as the
Distance-based
0.4431
-2.40%
0.3111
âˆ’3.42%
baseline and we also show the percentile improvement over it
TREC best
0.4540
0.00%
0.3221
0.00%
in Table 10. TREC 2012 sessions were created by considerUniform
0.4626
1.89%â€ 
0.3316
2.95%â€ 
ing and hence can be classified into two facets: search target
PvC
0.4713
3.81%â€ 
0.3351
4.04%â€ 
(factual or intellectual) and goal quality (specific/good or
QCM
0.4728
4.14%â€ 
0.3353
4.10%â€ 
amorphous/ill) [19]. Table 10 shows that QCM works very
QCM+Dup
0.4821
6.19%â€ 
0.3368
4.56%â€ 
well for all classes of sessions. Specifically, QCM works even
in the range of [0,0.1]. Therefore, the values of  are two
better, i.e. outperforms the TREC best even more signifimagnitudes less than that for the other parameters. Among
cantly, for sessions that search for intellectual targets as well
Î±, Î², and Î´, we find that Î± and Î² are larger than Î´, which
as sessions that search with amorphous goals. In our opinimplies that theme terms and added terms may play more
ion, this is due to that intellectual tasks produce new ideas
important roles in session search than removed terms.
or new findings (e.g. learn about a topic or make decision
based on the information collected so far) while searching.
6.3 Aggregation for the Entire Session
Both intellectual and amorphous sessions rely more on preQCM proposes an effective way to aggregate all queries in
vious search results. Thus, users reformulate queries based
a session as in Eq.8. We compare how effective it is to prior
more on what they have retrieved, not the vague informaquery aggregation methods. A query aggregation
scheme
tion need. This is a scenario where our approach is good at
Pn
can be represented as: Score(session, d) = i=1 Î»i Â· Score(qi , d), since we employ previous search results to guide the search
where Score(qi , d) is the relevance scoring function of d and
engineâ€™s action. For specific and factual sessions, users are
qi and Î»i is the query weight for qi .
clearer in search goals, query changes may come less from the
[8] proposed several aggregation schemes for TREC 2012
previous search results. In summary, our good performance
Session track. The schemes are: uniform (all queries are
on both intellectual task and amorphous task is consistent
equally weighted), previous vs. current (known as PvC;
with our efforts of modeling query changes.
all previous queries are discounted by Î»p , while the current
Moreover, we benefit from term-level manipulation in varquery uses a complementary and higher coefficient (1 âˆ’ Î»p ),
ious aspects in our system. The first aspect is novelty. Both
and distance-based (previous queries are discounted based
the TREC best system and our system handle novelty in
on a reciprocal function of queriesâ€™ positions in the session).
a session. The TREC best system only deals with novelty
We express various query aggregation schemes in terms
at the document level. They consider documents that have
of the discount factor Î³ in order to compare them with our
been examined by the user in a previous interaction not
approach. From Table 8, we find that QCM degenerates to
novel and the rest are novel [14]. That is, they determine
uniform when Î³ = 1. Previous queries in PvC and Distancenovelty purely based on document identification number, not
based schemes are also discounted as they are in QCM, but
the actual content. Through studying whether query terms
with different decay functions.
appeared in previous search results, our approach evaluates
The search accuracy for different aggregation schemes are
and models novelty at the term level (or concept level),
compared in Table 9. QCM performs the best for both
which we believe better represents the evolving informaTREC 2011 and 2012. The PvC scheme is the second best
tion needs in a session. The second aspect is query hanscheme, which confirms what is reported in [8]. The Distancedling. The Nugget approach [8] treats queries at the phrase
based scheme gives the worst performance.
level and formulates structured queries based on phrase-like
We explore the best discount factor Î³ for QCM over (0, 1)
nuggets. The approach achieves good performance, espe-

460

Table 10: nDCG@10 for different classes of sessions in TREC 2012.
TREC best
Nugget
QCM
QCM+DUP

Intellectual
0.3369
0.3305
0.3870
0.3900

%chg
0.00%
-1.90%
14.87%
15.76%

Amorphous
0.3495
0.3397
0.3689
0.3692

cially for TREC 2011. However, due to complexity in natural language, nugget detection is sensitive to dataset and
the approachâ€™s performance is not quite as stable as ours on
different datasets.
Lastly, our system benefits from trusting the user. Our approach does not use too much materials from other resources
such as anchor texts, meta data, or click orders, as many
other approaches do [8, 26]. We believe that the most direct
and valuable feedback is the next query that the user enters.
In this work, we manage to capture the query change and
investigate the reasons behind it. We use ourselves as users
to summarize possible human usersâ€™ reasoning and actions.
More detailed analysis about user intent might be useful for
researchers to understand web users, however, it might be
overwhelming (too fine-grained or too much semantics) for
a search engine that essentially only counts words.

7.2

%chg
0.00%
-2.80%
5.55%
5.64%

Specific
0.3007
0.2736
0.3091
0.3114

%chg
0.00%
-9.01%
2.79%
3.56%

Factual
0.3138
0.2871
0.3066
0.3072

%chg
0.00%
-8.51%
-2.29%
-2.10%

future work, we will explore effective query suggestion by
studying sessions that share the same topic.
The third type of errors is â€œtoo few relevant documentsâ€.
For sessions with too few relevant documents in the ground
truth, our system do not perform well. In total 2,573 relevant documents exist in CatB for all 48 TREC 2012 topics;
on average 53.6 relevant documents per topic. However,
topics 10, 45, 47 and 48, each has no more than 2 relevant
documents and topic 47 (s92 to s95) has no relevant document in CatB (Table 5). This problem could be reduced if
we index the entire ClubWeb09 CatA collection.
Figure 4 also indicates in which classes of sessions these
errors lie. We find that all â€œtwo theme conceptâ€ errors belong to sessions created with amorphous goals while all â€œtoo
few relevant documentsâ€ errors belong to those with specific
goals. Moreover, â€œill queriesâ€ tend to occur more in sessions
with amorphous goals. Note that â€œill queryâ€ and â€œfew relevant documentsâ€ are errors due to either the user or the
data. There might not be much room for our system to improve over them. However, â€œtwo theme conceptsâ€ is where
our system can certainly make further improvements.

Error Analysis & Future Work

Our system retrieves nothing for 22 out of 98 sessions in
TREC 2012. To analyze the reason for the poor performance
for those sessions, we study their topic descriptions, queries,
and ground truth documents. We summarize the types of
errors as â€œtwo theme conceptsâ€, â€œill queryâ€, â€œfew relevant
documentsâ€, and others. Figure 4 shows how many sessions
that we fail to retrieve under each error type.
We call the first type of errors â€œtwo theme conceptsâ€. It
comes from a type of session where the information need
cover more than one concepts. For instance, s17 and s18
share the the same topic â€œ... To what extent can decisions
and policies of the Indian government be credited with these
wins?â€. Queries in s17 and s18 ask about both concepts â€œindian politicsâ€™ and â€œmiss universeâ€. Unfortunately, very few
relevant documents about both theme concepts exist in the
corpus. The retrieved documents are about either concept,
but none is about both. Eight sessions belong to this type.
As future work, we can improve our system by incorporating
structures in queries, and enable more sophisticated operators such as Boolean and proximity search.
The second type of errors is â€œill queryâ€, where in such
sessions, queries themselves are ill formulated and do not
well-represent the information needs indicated in the given
topic. A common mistake is that the user misses some subinformation need. For example, the topic for s16 is: â€œ... you
want to reduce the use of air conditioning in your house ...
you could protect the roof being overly hot due to sun exposure... Find information of ... how it could be done.â€ A
good query for this topic should include roof and air conditioning. However, the queries that the user issued for s60,
â€œreduce airconditioningâ€ and â€œattic insulation air conditioning costsâ€, do not mention roof at all. Because of this ill
query formulation, our system yields no relevant documents
for s60. On the other hand, for s59, which shares the same
information need with s60, our system achieves a nDCG@10
of 0.48 simply because s59 queries â€œcool roofâ€. It suggests
that ill queries mislead the search engine and yield poor retrieval performance. Four sessions belong to this type. As

8.

RELATED WORK

Session search is a challenging IR task [4, 8, 13, 14, 25, 32].
Existing approaches investigate session search from various
aspects such as semantic meanings of search tasks [23], document novelty [14], and phrase structure in queries [8]. The
best TREC system [13, 14] employs an adaptive browsing
model by considering both relevance and novelty; however it
does not demonstrate improvement by handling novelty. In
this paper, we successfully model query and document novelty by investigating the relationship between query change
and previous search results. Moreover, our analysis on query
change does not require knowledge of semantic types for the
sessions as [23] proposed.
Our proposed work is perhaps the most similar to the
problem of query formulation [1, 9, 12, 24] and query suggestion [29]. [12] showed that certain query changes such as
adding/removing words, word substitution, acronym expansion, and spelling correction are more likely to cause clicks,
especially on higher ranked results. The finding is generally
consistent with our view of query change. However, their
work only emphasized on understanding of query changes,
without showing how to apply it to help session search. [24]
examined the relationship between task types and how users
change queries. They classified query changes by semantic
types: Generalization, Specialization, Word Substitution,
Repeat, and New. Similar to [12], however, [24] stopped at
understanding query changes and didnâ€™t apply their findings
to help session search. This probably makes us the first to
utilize query changes in actual retrieval. [1] derived queryflow graph, a graph representation of user query behavior,
from user query logs. The approach detected query chains
in the graph and recommended queries based on maximum
weights, random walk, or just the previous query. Other
mining approaches [1, 29] identify the importance of query

461

change in sessions; however, they require the luxury of large
user query logs.
This research is perhaps the first to employ reinforcement
learning to solve the Markov Decision Process demonstrated
in session search. Reinforcement learning is complex and difficult to solve. Its solutions include model-based approaches
and model-free approaches [16]. The former learn the transition model and the reward function for every possible states
and actions and mainly employ MLE to estimate the model
parameters. Others also use matrix inversion or linear programming to solve the Bellman equation. It works well when
state spaces are small. However, in our case, the state space
is large since we use natural language queries as the states;
hence we could not easily apply model-based approaches in
practice. In this work, we effectively reduce the search space
by summarizing usersâ€™ and search engineâ€™s actions into a few
types and employ a model-free approach to learn value functions directly.

9.

[7] D. Guan and H. Yang. Increasing stability of result
organization for session search. In ECIR â€™13.
[8] D. Guan, H. Yang, and N. Goharian. Effective structured
query formulation for session search. In TREC â€™12.
[9] J. Guo, G. Xu, H. Li, and X. Cheng. A unified and
discriminative model for query refinement. In SIGIR â€™08.
[10] Q. Guo and E. Agichtein. Ready to buy or just browsing?:
detecting web searcher goals from interaction data. In
SIGIR â€™10.
[11] D. S. Hirschberg. Algorithms for the longest common
subsequence problem. J. ACM, 24(4), Oct. 1977.
[12] J. Huang and E. N. Efthimiadis. Analyzing and evaluating
query reformulation strategies in web search logs. In CIKM
â€™09.
[13] J. Jiang, S. Han, J. Wu, and D. He. Pitt at trec 2011
session track. In TREC â€™11.
[14] J. Jiang, D. He, and S. Han. Pitt at trec 2012 session track.
In TREC â€™12.
[15] R. Jones and K. L. Klinkner. Beyond the session timeout:
automatic hierarchical segmentation of search topics in
query logs. In CIKM â€™08.
[16] L. P. Kaelbling, M. L. Littman, and A. W. Moore.
Reinforcement learning: a survey. J. Artif. Int. Res., 4(1),
May 1996.
[17] Y. Kalfoglou and M. Schorlemmer. Ontology mapping: the
state of the art. Knowl. Eng. Rev., 18(1), Jan. 2003.
[18] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and
M. Sanderson. Overview of the trec 2011 session track. In
TRECâ€™11.
[19] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and
M. Sanderson. Overview of the trec 2012 session track. In
TRECâ€™12.
[20] E. Kanoulas, P. D. Clough, B. Carterette, and
M. Sanderson. Session track at trec 2010. In TRECâ€™10.
[21] Lemur Search Engine. http://www.lemurproject.org/.
[22] C. Liu, N. J. Belkin, and M. J. Cole. Personalization of
search results using interaction behaviors in search sessions.
In SIGIR â€™12.
[23] C. Liu, M. Cole, E. Baik, and J. N. Belkin. Rutgers at the
trec 2012 session track. In TRECâ€™12.
[24] C. Liu, J. Gwizdka, J. Liu, T. Xu, and N. J. Belkin.
Analysis and evaluation of query reformulations in different
task types. In ASIST â€™10.
[25] J. Liu and N. J. Belkin. Personalizing information retrieval
for multi-session tasks: the roles of task stage and task
type. In SIGIR â€™10.
[26] A. M-Dyaa, K. Udo, N. Nikolaos, N. Brendan, L. Deirdre,
and F. Maria. University of essex at the trec 2011 session
track. In TREC â€™11.
[27] D. Metzler and W. B. Croft. Combining the language
model and inference network approaches to retrieval. Inf.
Process. Manage., 40(5), Sept. 2004.
[28] S. P. Singh. Learning to solve markovian decision processes.
Technical report, Amherst, MA, USA, 1993.
[29] Y. Song and L.-w. He. Optimal rare query suggestion with
implicit user feedback. In WWW â€™10.
[30] Y. Song, D. Zhou, and L.-w. He. Query suggestion by
constructing term-transition graphs. In WSDM â€™12.
[31] J. Teevan, S. T. Dumais, and D. J. Liebling. To personalize
or not to personalize: modeling queries with variation in
user intent. In SIGIR â€™08.
[32] R. W. White, I. Ruthven, J. M. Jose, and C. J. V.
Rijsbergen. Evaluating implicit feedback models using
searcher simulations. ACM Trans. Inf. Syst., 23(3), July
2005.
[33] C. Zhai and J. Lafferty. A study of smoothing methods for
language models applied to information retrieval. ACM
Trans. Inf. Syst., 22(2):179â€“214, Apr. 2004.

CONCLUSION

This paper presents a novel session search approach (QCM)
by utilizing query change and modeling the dynamic of the
entire session as a Markov Decision Process. We assume
that query change is an important form of feedback. Based
on this assumption, through studying editing changes between adjacent queries, and their relationship with previous
retrieved documents, we propose corresponding search engine actions to handle individual term weights for both the
query and the document. In a reinforcement learning inspired framework, we incorporate various ingredients present
in session search, such as query changes, satisfactory clicks,
desire for document novelty, and duplicated queries. The
proposed framework provides a theoretically sound and general foundation that allows more novel features to be incorporated. Experiments on both TREC 2011 and 2012 Session tracks show that our approach is highly effective and
outperforms the best session search systems in TREC. This
research is perhaps the first to employ reinforcement learning in session search. Our MDP view of modeling session
search can potentially benefit a wide range of IR tasks.

10.

ACKNOWLEDGMENT

This research was supported by NSF grant CNS-1223825.
Any opinions, findings, conclusions, or recommendations expressed in this paper are of the authors, and do not necessarily reflect those of the sponsor.

11.

REFERENCES

[1] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gionis, and
S. Vigna. The query-flow graph: model and applications. In
CIKM â€™08.
[2] I. Bordino, C. Castillo, D. Donato, and A. Gionis. Query
similarity by projecting the query-flow graph. In SIGIR â€™10.
[3] P. Bruza, R. McArthur, and S. Dennis. Interactive internet
search: keyword, directory and query reformulation
mechanisms compared. In SIGIR â€™00.
[4] B. Carterette, E. Kanoulas, and E. Yilmaz. Simulating
simple user behavior for system effectiveness evaluation. In
CIKM â€™11.
[5] M.-A. Cartright, R. W. White, and E. Horvitz. Intentions
and attention in exploratory health search. In SIGIRâ€™11.
[6] G. V. Cormack, M. D. Smucker, and C. L. Clarke. Efficient
and effective spam filtering and re-ranking for large web
datasets. Inf. Retr., 14(5), Oct. 2011.

462

