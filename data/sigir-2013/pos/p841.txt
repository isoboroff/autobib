The Cluster Hypothesis for Entity Oriented Search
Hadas Raviv, Oren Kurland

David Carmel

Faculty of Industrial Engineering and
Management, Technion, Haifa 32000, Israel
hadasrv@tx.technion.ac.il,kurland@ie.technion.ac.il

ABSTRACT
In this work we study the cluster hypothesis for entity oriented search (EOS). Specifically, we show that the hypothesis can hold to a substantial extent for several entity similarity measures. We also demonstrate the retrieval effectiveness
merits of using clusters of similar entities for EOS.
Categories and Subject Descriptors: H.3.3 [Information Search
and Retrieval]: Retrieval models
Keywords: cluster hypothesis, entity oriented search

1.

INTRODUCTION

The entity oriented search (EOS) task has attracted much
research attention lately. The main goal is to rank entities
in response to a query by their presumed relevance to the
information need that the query expresses. For example, the
goal in the TREC’s expert search task was to rank employees
in the enterprise by their expertise in a topic [6]. The goal
in INEX entity ranking track was to retrieve entities that
pertain to a topic in the English Wikipedia [7, 8, 9]. The
goal in TREC’s entity track was to rank Web entities with
respect to a given entity by their relationships [2, 3, 4].
The EOS task is different than the standard ad-hoc document retrieval task as entities are somewhat more complex
than (flat) documents. That is, entities are characterized
by different properties such as name, type (e.g., place or
person), and potentially, an associated document (e.g., a
homepage or a Wikipedia page). Despite the fundamental
difference between the two tasks, we set as a goal to study
whether an important principle in ad-hoc document retrieval
also holds for the EOS task; namely, the cluster hypothesis
[22]. We present the first study of the cluster hypothesis
for EOS, where the hypothesis is that “closely associated
entities tend to be relevant to the same requests”.
We use several inter-entity similarity measures to quantify the association between entities, which is a key point
in the hypothesis. These measures are based on the entity
type which is a highly important source of information [18,
14]. We then show that the cluster hypothesis, tested using

Yahoo! Research, Haifa 31905, Israel
david.carmel@ymail.com

Voorhees’ nearest neighbor test [23], can hold to a substantial extent for EOS for several of the similarity measures.
Motivated by the findings about the cluster hypothesis,
we explore the merits of using clusters of similar entities
for entity ranking. We show that ranking entity clusters
by the percentage of relevant entities that they contain can
be used to produce extremely effective entity ranking. We
also demonstrate the effectiveness of using cluster ranking
techniques that are based on estimating the percentage of
relevant entities in the clusters for entity ranking.
Our main contributions are three fold: (i) showing that for
several inter-entity similarity measures the cluster hypothesis holds for EOS to a substantial extent as determined by
the nearest neighbor test; (ii) demonstrating the considerable potential of using clusters of similar entities for EOS;
and, (iii) showing that using simple cluster ranking methods can help to improve retrieval performance with respect
to that of an effective initial search.

2. RELATED WORK
Any entity in the INEX entity ranking track, which we
use for our experiments, has a Wikipedia page. Hence, some
previously proposed estimates for the entity-query similarity are based on Wikipedia’s category information [20, 1,
18, 14]. Similarly, we measure the similarity between two
entities based on several measures of the similarity between
the sets of categories of the two entity pages.
In Cao et al.’s work on expert search [5] the retrieval score
assigned to an entity was smoothed with the retrieval score
assigned to a cluster constructed from the entity. In contrast, we explore a retrieval paradigm that ranks entity clusters and transforms the ranking to entity ranking. Work on
document retrieval showed that cluster-based (score) smoothing and cluster ranking are complementary [16]. We leave
the exploration of this finding for the EOS task for future
work.
There are several tests of the cluster hypothesis for document retrieval [13, 10, 23, 19]. We use Voorhees’ nearestneighbor test for the EOS task as it is directly connected
with the nearest neighbor clusters we use for ranking.
The findings we present for the EOS task echo those reported for document retrieval. Namely, the extent to which
the cluster hypothesis holds [23], and the (potential) merits
of using cluster ranking [12, 21, 15, 17].

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

3. THE CLUSTER HYPOTHESIS
Our first goal is to explore the extent to which the cluster
hypothesis holds for EOS. To this end, we use the nearest

841

Data set
2007
2008
2009

[n]

neighbor test [23]. Let Lq be the list of n entities that
are the highest ranked by an initial search performed in
[n]
response to query q. For each relevant entity in Lq , we
record the percentage of relevant entities among its K near[n]
est neighbors in Lq . The nearest neighbors are determined
using one of the inter-entity similarity measures specified in
Section 5.1. The test result is the average of the recorded
[n]
percentages over all relevant entities in Lq , averaged over
all test queries.
Some of the inter-entity similarity measures assign discrete values including 0. Hence, for some relevant entities
there could be less than K neighbors as we do not consider
neighbors with a 0 similarity value. In addition, a relevant
entity might be assigned with more than K nearest neighbors due to ties in the similarity measure. That is, we keep
collecting all entities having the same similarity value as that
of the last one in the K neighbors list.

4.

[n]
e∈Lq

Inter-entity similarity measures. The inter-entity simi-

Sinit (e;q)

. The cluster score is the
|c|+1
mean retrieval score of a cluster composed of c’s entities
and an additional “pseudo” entity whose score is the mean
score in the initial list. This method helps to address, among
others, cluster-size bias issues.

5.

# of test topics
46
35
55

were derived from the ad hoc 2007 assessments, and additional 25 topics that were created by the participants specifically for the track. In 2008, 35 topics were created and
used for testing. The topics used for testing in 2009 were 55
topics out of the 60 test topics used in 2007 and 2008.
We used Lucene (http://lucene.apache.org/core/) for experiments. The data was pre-processed using Lucene, including tokenization, stopword removal, and Porter stemming.

CLUSTER-BASED ENTITY RANKING

1
Sinit (e;q)+ n

# of documents
659, 388
659, 388
2, 666, 190

Table 1: INEX entity ranking datasets.

Our second goal is studying the potential merits of using entity clusters to induce entity ranking. We re-rank the
[n]
initial entity list Lq using a cluster-based paradigm which
is very common in work on document retrieval [17]. Let
[n]
[n]
Cl(Lq ) be the set of clusters created from Lq using some
clustering method. The inter-entity similarity measures used
for creating clusters are those used for testing the cluster hypothesis. (See Section 5.1 for further technical details.) The
[n]
clusters in Cl(Lq ) are ranked by the presumed percentage
of relevant entities that they contain. Below we describe
two cluster ranking methods. Then, each cluster is replaced
with its constituent entities while omitting repeats. Within
cluster entity ranking is based on the initial entity retrieval
[n]
scores which were used to create the list Lq .
The MeanScore cluster ranking method scores cluster
c by
P the mean retrieval score of its constituent entities:
1
e∈c Sinit (e; q); Sinit (e; q) is the initial retrieval score
|c|
of entity e; |c| is the number of entities in c.
When Sinit (e; q) is a rank equivalent estimate to that of
log(P r(q, e)) [18], the cluster score assigned by MeanScore
is rank equivalent to the geometric mean of the joint queryentity probabilities’ estimates in the cluster. Using a geometricmean-based representation for document clusters was shown
to be highly effective for ranking document clusters [17].
The regularized mean score method, RegMeanScore in
short,
which is novel
to this study, smoothes c’s score:
P
P
e∈c

Collection size
4.4 GB
4.4 GB
50.7 GB

EVALUATION

5.1 Experimental setup
We conducted experiments with the datasets of the INEX
entity ranking track of 2007 [7], 2008 [8], and 2009 [9]. Table
1 provides a summary of the datasets. The tracks for 2007
and 2008 used the English Wikipedia dataset from 2006,
while the 2009 track used the English Wikipedia from 2008.
The set of test topics for 2007 is composed of 21 topics that

842

larity measures that we use utilize Wikipedia categories.
Specifically, the categories associated with the Wikipedia
page of the entity, henceforth referred to as its category set,
serve as the entity type.
The Tree similarity between two entities e1 and e2 is
exp(−αd(e1 , e2 )) where d(e1 , e2 ) is the minimum distance
over Wikipedia’s categories graph between a category in e1 ’s
category set and a category in e2 ’s category set; α is a decay
constant determined as in [18]. The SharedCat measure is
the cosine similarity between the binary vectors representing
two entities. An entity vector is defined over the categories
space. An entry in the vector is 1 if the corresponding category is associated with the entity and 0 otherwise. Thus,
SharedCat measures the (normalized) number of categories
shared by the two entities [20]. The CE measure is based
on measuring the language-model-based similarity between
the documents associated with the category sets of two entities [14]. More specifically, each category is represented
in this case by the text that results from concatenating all
Wikipedia pages associated with the category. The similarity between the texts x and y that represent two categories is
[0]
[µ]
exp(−CE(px (·)||py (·))); CE is the cross entropy measure;
[µ]
pz (·) is the Dirichlet-smoothed unigram language model induced from z with the smoothing parameter µ (=1000). The
CE similarity between two entities is defined as the maximal
similarity, over all pairs of categories, one in the first entity’s
category set and the other in the second entity’s category set,
of the texts representing the categories. Finally, the ESA
(Explicit Semantic Analysis) [11] similarity measure is the
cosine between two vectors, each represents the category set
of an entity. The vectors representing the category sets are
defined over the entities space. The value of an entry in the
vector is the number of the categories in the given category
set that are associated with the corresponding entity. Using
ESA to measure inter-entity similarity is novel to this study.
[n]
Three different initially retrieved entity lists, Lq , are used
for both the cluster hypothesis test and cluster-based ranking. The lists are created in response to the query using
highly effective entity retrieval methods [18]. The first list,
LDoc , is created by representing an entity with its Wikipedia
document (page). The documents are ranked in response
to the query using the standard language-model-based approach with Dirichlet-smoothed unigram language models
and the cross entropy similarity measure. The second list,

Similarity measure
Tree

SharedCat

CE

ESA

Initial list
LDoc
LDoc;T ype
LDoc;T ype;N ame
LDoc
LDoc;T ype
LDoc;T ype;N ame
LDoc
LDoc;T ype
LDoc;T ype;N ame
LDoc
LDoc;T ype
LDoc;T ype;N ame

2007
30.0
29.8
32.7
35.7
33.5
37.9
33.4
34.5
37.5
34.3
33.7
37.3

2008
32.0
35.5
37.7
41.0
45.5
44.3
36.2
38.6
41.7
36.2
41.0
39.1

2009
42.7
44.9
44.9
45.4
52.2
52.7
46.0
50.3
49.7
46.2
49.5
49.0

ties among the nearest neighbors of relevant entities, we can
conclude that the cluster hypothesis holds to a substantial
extent, according to the nearest neighbor test, with various
inter-entity similarity measures.
Table 2 also shows that for most of the data sets and
similarity measures the test results for the LDoc;T ype and
LDoc;T ype;Name lists are higher than for LDoc . This finding
is not surprising as LDoc;T ype and LDoc;T ype;Name were created using entity-query similarity measures that account for
category information, while the similarity measure used to
create LDoc does not use this information. The highest test
results are obtained for the SharedCat similarity measure
which, as noted above, measures the (normalized) number
of shared categories between two entities.

Table 2: The cluster hypothesis test: the average
percentage of relevant entities among the 5 nearest
neighbors of a relevant entity.

5.2.2 Cluster-based entity ranking
Table 3 presents the results of employing cluster-based
entity re-ranking, as described in Section 4, upon the three
initial entity lists. The various inter-entity similarity measures are used for creating the clusters. ’Initial’ refers to
the initial ranking of a list. ’Oracle’ is the ranking of entities that results from employing the cluster-based re-ranking
paradigm described in Section 4; the clusters are ranked by
the true percentage of relevant entities that they contain.
The high performance numbers for Oracle, which are substantially and statistically significantly better than those for
Initial, attest to the existence of clusters that contain a very
high percentage of relevant entities. More generally, these
numbers attest to the incredible potential of employing effective cluster ranking methods to rank entities.
RegMeanScore, which outperforms MeanScore due to the
regularization discussed in Section 4, is in quite a few cases
more effective than Initial; specifically, using the Tree and
SharedCat inter-entity similarity measures. While the improvements for LDoc are often statistically significant, this
is not the case for LDoc;T ype and LDoc;T ype;Name . Naturally, the more effective the initial ranking (Initial), the more
challenging the re-ranking task. Yet, the very high Oracle
numbers for LDoc;T ype and LDoc;T ype;Name imply that effective cluster ranking methods can yield performance that is
much better than that of the initial ranking. Finally, for
both LDoc;T ype and LDoc;T ype;Name the best performance is
in most cases attained by using RegMeanScore.

LDoc;T ype , is created by scoring entities with an interpolation of two scores. The first is that used to create the list
LDoc . The second is the similarity between the category
set of the entity and the query target type (the set of categories that are relevant to the query, as defined by INEX
topics). The Tree estimate described above is used for measuring similarity between the two category sets. The third
list, LDoc;T ype;Name , is created by scoring an entity with an
interpolation of the score used to create LDoc;T ype , and an
estimate for the proximity-based association [18] between
the query terms and the entity name (i.e., the title of its
Wikipedia page) in the corpus. We employ the same traintest approach as in [18] to set the free-parameter values of
the ranking methods used to create the initial lists. The
[n]
number of entities in each initial list Lq is n = 50.
We use a simple nearest neighbor clustering method to
[n]
cluster entities in the initial list Lq . Specifically, each en[n]
[n]
tity in Lq and the K (= 5) entities in Lq that are the
most similar to it, according to the inter-entity similarity
measures described above, form a cluster. Using such small
overlapping clusters was shown to be highly effective for
cluster-based document retrieval [16, 15, 17]. We note that
not all clusters necessarily contain K + 1 documents due to
the reasons specified in Section 3. For consistency, we also
use K = 5 in the cluster hypothesis test.
Following the INEX guidelines, the evaluation metric for
INEX 2007 is mean average precision (MAP) while that for
INEX 2008 and 2009 is infAP. We also report the precision of
the top 5 entities (p@5). Statistically significant differences
of retrieval performance are determined using the two tailed
paired t-test with a 95% confidence level.

6. CONCLUSIONS AND FUTURE WORK
We showed that the cluster hypothesis can hold to a substantial extent for the entity oriented search (EOS) task with
several inter-entity similarity measures. We also demonstrated the potential merits of using a cluster-based retrieval
paradigm for EOS that relies on ranking entity clusters. Devising improved cluster ranking techniques is a future venue
we intend to explore.

5.2 Experimental results
5.2.1 The cluster hypothesis
Table 2 presents the results of the nearest neighbor cluster
hypothesis test that was described in Section 3. The test is
performed on the different initially retrieved entity lists using the various inter-entity similarity measures. We see that
the average percentage of relevant entities among the nearest neighbors of a relevant entity ranges between 30% and
53% across the various experimental settings. We also found
out that, on average, the percentage of relevant entities in
a list is often lower than 25% and can be as low as 10%.
Thus, due to the relatively high percentage of relevant enti-

7. ACKNOWLEDGMENTS
We thank the reviewers for their comments. Part of the
work reported here was done while David Carmel was at
IBM. This paper is based on work that has been supported
in part by the Israel Science Foundation under grant no.
433/12 and by Google’s faculty research award. Any opinions, findings and conclusions or recommendations expressed
in this material are the authors’ and do not necessarily reflect those of the sponsors.

843

8.

REFERENCES

[1] K. Balog, M. Bron, and M. De Rijke. Query modeling for entity
search based on terms, categories, and examples. ACM Trans.
Inf. Syst., 29(4), 2011.
[2] K. Balog, A. P. de Vries, P. Serdyukov, P. Thomas, and
T. Westerveld. Overview of the trec 2009 entity track. In
Proceedings of TREC, 2009.
[3] K. Balog, P. Serdyukov, and A. P. de Vries. Overview of the
trec 2010 entity track. In Proceedings of TREC, 2010.
[4] K. Balog, P. Serdyukov, and A. P. de Vries. Overview of the
trec 2011 entity track. In Proceedings of TREC, 2011.
[5] Y. Cao, J. Liu, S. Bao, and H. Li. Research on expert search at
enterprise track of TREC 2005. In Proceedings of TREC,
volume 14, 2005.
[6] N. Craswell, A. P. de Vries, and I. Soboroff. Overview of the
trec 2005 enterprise track. In Proceedings of TREC, 2005.
[7] A. P. de Vries, A.-M. Vercoustre, J. A. Thom, N. Craswell, and
M. Lalmas. Overview of the INEX 2007 entity ranking track. In
Proceedings of INEX, pages 245–251, 2007.
[8] G. Demartini, A. P. de Vries, T. Iofciu, and J. Zhu. Overview of
the INEX 2008 entity ranking track. In Proceedings of INEX,
pages 243–252, 2008.
[9] G. Demartini, T. Iofciu, and A. P. de Vries. Overview of the
INEX 2009 entity ranking track. In Proceedings of INEX,
pages 254–264, 2009.
[10] A. El-Hamdouchi and P. Willett. Techniques for the
measurement of clustering tendency in document retrieval
systems. Journal of Information Science, 13:361–365, 1987.
[11] E. Gabrilovich and S. Markovitch. Computing semantic
relatedness using wikipedia-based explicit semantic analysis. In
Proceedings of IJCAI, pages 1606–1611, 2007.
[12] M. A. Hearst and J. O. Pedersen. Reexamining the cluster
hypothesis: Scatter/Gather on retrieval results. In Proceedings
of SIGIR, pages 76–84, 1996.
[13] N. Jardine and C. J. van Rijsbergen. The use of hierarchic
clustering in information retrieval. Information Storage and
Retrieval, 7(5):217–240, 1971.
[14] R. Kaptein and J. Kamps. Exploiting the category structure of
wikipedia for entity ranking. Artificial Intelligence, 0:111 –
129, 2013.
[15] O. Kurland and C. Domshlak. A rank-aggregation approach to
searching for optimal query-specific clusters. In Proceedings of
SIGIR, pages 547–554, 2008.
[16] O. Kurland and L. Lee. Corpus structure, language models, and
ad hoc information retrieval. In Proceedings of SIGIR, pages
194–201, 2004.
[17] X. Liu and W. B. Croft. Evaluating text representations for
retrieval of the best group of documents. In Proceedings of
ECIR, pages 454–462, 2008.
[18] H. Raviv, D. Carmel, and O. Kurland. A ranking framework for
entity oriented search using markov random fields. In
Proceedings of the 1st Joint International Workshop on
Entity-Oriented and Semantic Search, 2012.
[19] M. D. Smucker and J. Allan. A new measure of the cluster
hypothesis. In Proceedings of ICTIR, pages 281–288, 2009.
[20] J. A. Thom, J. Pehcevski, and A.-M. Vercoustre. Use of
wikipedia categories in entity ranking. CoRR, abs/0711.2917,
2007.
[21] A. Tombros, R. Villa, and C. Van Rijsbergen. The effectiveness
of query-specific hierarchic clustering in information retrieval.
Information Processing & management, 38(4):559–582, 2002.
[22] C. J. van Rijsbergen. Information Retrieval. Butterworths,
second edition, 1979.
[23] E. M. Voorhees. The cluster hypothesis revisited. In
Proceedings of SIGIR, pages 188–196, 1985.

Initial
Oracle
MeanScore
RegMeanScore
Oracle
MeanScore
RegMeanScore
Oracle
MeanScore
RegMeanScore
Oracle
MeanScore
RegMeanScore

Initial
Oracle
MeanScore
RegMeanScore
Oracle
MeanScore
RegMeanScore
Oracle
MeanScore
RegMeanScore
Oracle
MeanScore
RegMeanScore

Initial
Oracle
MeanScore
RegMeanScore
Oracle
MeanScore
RegMeanScore
Oracle
MeanScore
RegMeanScore
Oracle
MeanScore
RegMeanScore

LDoc
2007
2008
MAP
p@5
infAP
p@5
20.2
26.1
12.6
19.4
Tree
31.8i
51.3i
22.3i
49.7i
21.6
26.1
13.3
17.1
21.6
26.0
13.4
18.9
SharedCat
36.2i
60.0i
26.8i
66.3i
22.5
23.9
12.6
18.9
23.1i
27.8i
13.4i
20.6i
CE
32.3i
53.5i
23.3i
53.7i
22.6
27.4
13.5
20.6
22.0
26.5
13.5
20.6
ESA
i
i
i
33.7
57.0
26.0
60.6i
20.9
22.6
12.8
14.9
21.9
23.9
12.9
14.9
LDoc;Type
2007
2008
MAP
p@5
infAP
p@5
30.8
37.4
28.2
44.0
Tree
37.7i
58.7i
32.5i
50.9i
31.6
40.0
27.7
37.7
31.6
40.0
28.1
40.6
SharedCat
i
i
i
43.8
65.7
38.3
65.1i
30.8
36.1
28.7
42.3
31.1
37.0
28.9
42.3
CE
39.0i
60.0i
34.3i
58.3i
31.3
38.3
28.5
40.6
31.0
37.4
28.7
40.0
ESA
42.1i
64.3i
37.7i
66.9i
28.6
34.3
28.4
42.3
29.1
34.8
28.9
44.0
LDoc;Type;Name
2007
2008
MAP
p@5
infAP
p@5
33.3
40.4
35.4
46.9
Tree
39.6i
57.4i
42.3i
62.3i
34.1
43.5
35.7
42.3
34.0
42.6
35.7
43.4
SharedCat
47.4i
70.4i
47.8i
74.9i
32.9
38.7
33.5
42.9
33.1
39.1
34.8
44.0
CE
40.7i
59.1i
43.1i
63.4i
33.6
38.7
34.5
45.1
33.6
38.7
34.5
45.1
ESA
i
i
i
44.7
66.5
45.7
70.3i
33.9
41.3
33.7
43.4
34.0
42.2
34.2
44.6

2009
infAP
p@5
19.1
35.3
25.5i
20.2i
20.2i

68.0i
36.7
36.7

30.5i
19.7
19.9

83.3i
37.1
38.5

28.0i
19.1
19.1

74.2i
36.7
36.7

28.8i
19.2
19.2

80.0i
35.6
35.3

2009
infAP
p@5
23.8
43.6
29.5i
23.6
23.4

70.2i
39.6
39.3

34.1i
23.2
23.6

87.6i
44.0
45.1

31.3i
23.7
23.7

75.6i
42.2
42.2

32.9i
22.8
22.7

83.6i
42.5
41.5

2009
infAP
p@5
24.4
44.0
30.3i
24.7
24.6

72.0i
42.9
42.2

34.3i
24.6
24.9

87.i 3
41.5
42.9

31.6i
24.6
24.8

76.4i
43.3
43.6

32.9i
23.0
23.3

81.5i
35.3
35.6

Table 3: Retrieval performance. The best result in
a column (excluding that of Oracle) per an initial
list is boldfaced. ’i’ marks statistically significant
differences with Initial.

844

