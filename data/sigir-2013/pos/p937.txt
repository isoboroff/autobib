Relating Retrievability, Performance and Length
Colin Wilkie and Leif Azzopardi
School of Computing Science,
University of Glasgow
Glasgow, United Kingdom

{Colin.Wilkie,Leif.Azzopardi}@glasgow.ac.uk

ABSTRACT

fore discussing how it relates to performance. Next, we perform an empirical analysis where we hypothesize that lower
retrievability bias will lead to better retrieval performance.

Retrievability provides a different way to evaluate an Information Retrieval (IR) system as it focuses on how easily documents can be found. It is intrinsically related to
retrieval performance because a document needs to be retrieved before it can be judged relevant. In this paper,
we undertake an empirical investigation into the relationship between the retrievability of documents, the retrieval
bias imposed by a retrieval system, and the retrieval performance, across different amounts of document length normalization. To this end, two standard IR models are used on
three TREC test collections to show that there is a useful
and practical link between retrievability and performance.
Our findings show that minimizing the bias across the document collection leads to good performance (though not the
best performance possible). We also show that past a certain amount of document length normalization the retrieval
bias increases, and the retrieval performance significantly
and rapidly decreases. These findings suggest that the relationship between retrievability and effectiveness may offer a
way to automatically tune systems.
Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Performance Evaluation
Terms Theory, Experimentation Keywords Retrievability,
Simulation

1.

2.

BACKGROUND

In [3], retrievability was defined as a measure that provides
an indication of how easily a document could be retrieved
under a particular configuration of an IR system. Put formally the retrievability r(d) of a document d with respect
to an IR system is:
X
f (kdq , {c, g})
(1)
r(d) ∝
q∈Q

where q is a query in a very large query set Q, kdq is the rank
at which d is retrieved given q, and f (kdq , c) is an access
function which denotes how retrievable the document d is
for the query q given the rank cutoff c. This implies that
retrievability of a document is obtained by summing over
all possible queries Q. The more queries that retrieve the
document, the more retrievable the document is.
A simple measure of retrievability is a cumulative-based
approach, which employs an access function f (kdq , c), such
that if d is retrieved in the top c documents given q, then
f (kdq , c) = 1 else f (kdq , c) = 0. Essentially, this measure
provides an intuitive value for each document as it is simply the number of times that the document is retrieved in
the top c documents. A gravity based approach instead
weights the rank at which the document is retrieved, as follows: f (kdq , g) = 1/kdq g . This introduces a discount g, such
that documents lower down the list are assumed to be less
retrievable.
Retrievability Bias: To quantify the retrievability bias
across the collection, the Gini coefficient [8] is often used [3,
5, 6] as it measures the inequality within a population. For
example, if all documents were equally retrievable according
to r(d), then the Gini coefficient would be zero (denoting
equality), but if all but one document had r(d) = 0 then the
Gini coefficient would be one (denoting total inequality).
Usually, most documents have some level of retrievability
and the Gini coefficient is somewhere between zero and one.
Essentially, the Gini coefficient provides an indication of the
level of inequality between documents given how easily they
can be retrieved using a particular retrieval system and configuration.
Uses: Retrievability - and the theory of - has been used
in numerous contexts, for example, in the creation of reverted indexes that improve the efficiency and performance
of retrieval systems by capitalizing on knowing what terms

INTRODUCTION

Traditionally IR systems are evaluated in terms of efficiency and performance [13]. However IR systems can also
be evaluated in terms of retrievability [3], which assesses how
often documents are retrieved (as opposed to the speed of retrieval and the quality of retrieval respectively). Retrievability is fundamental to IR because it precedes relevancy [3]. In
this paper, we investigate the relationship between retrievability and retrieval performance in the context of ad-hoc
topic retrieval for two standard best match retrieval models.
To this end, we shall first formally define retrievability bePermission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

937

within a document makes that document more retrievable [10].
Retrievability has also been used to study search engine and
retrieval system bias on the web [2] and within patent collections [4] to improve the efficiency of systems when pruning [14]. It has also been related to navigability when tagging
information to improve how easily users browsing through
the collection could find documents [12]. While these show
that retrievability is useful in a number of respects, here, we
are interested in how retrievability relates to performance.
There have been a few works exploring this research direction in different ways [3, 1, 5, 6].
Relating Retrievability and Performance: In [1], Azzopardi discusses the relationship with respect to the definition of retrievability and claim that a purely random IR
system would ensure equal retrievability (resulting in a Gini
= 0). However, the author argues that this would also result in very poor retrieval performance. Conversely, if an
oracle IR system retrieved only the set of known relevant
documents, and only these documents, then there would be
a very high inequality in terms of retrievability across the
collection. They suggest that neither extreme is desirable
and instead suggest that there is likely to be a trade-off between retrievability and retrieval performance. In [3], it is
acknowledged that some level of bias is necessary because
a retrieval system must try and discriminate relevant from
non-relevant. The preliminary study conducted in [1] investigating this relationship on two small TREC test collections
showed that as retrieval performance increased, the retrievability bias tended to decrease: suggesting a positive and
useful relationship.
In [5], Bashir and Rauber studied the effect of Pseudo
Relevance Feedback (PRF) on performance and retrievability. They found that standard Query Expansion methods,
while increasing performance, also increased the retrievability bias. To combat the increase in bias, they devised a
method of PRF that used clustering; this resulted in a reduction in bias, as well as an increase in performance over
other QE techniques. When employing their PRF technique
to patent retrieval, they showed that the decrease in bias led
to improved recall for prior art search [6]. This later work
suggested that there may be a positive correlation between
recall and retrievability. In [7], they compared a number of
retrieval models in terms of retrievability bias and performance as a way to rank different retrieval systems. They
found that there was a positive correlation between the bias
and performance for standard best match retrieval models.
It is important to note that since retrievability can be estimated without recourse to relevance judgements it provides
an attractive alternative for evaluation.

3.

EXPERIMENTAL METHOD

Previous work suggests that the relationship between retrievability bias and retrieval performance is much more
complicated than previously thought and the relationship
is somewhat unclear. In this paper, we shall perform a more
detailed analysis across larger TREC collections and explicitly plot the relationship between the retrievability bias and
retrieval performance in order to form a deeper understanding of how they relate to each other.
To this end, three TREC test collections were used AP,
Aquaint (AQ) and DotGov (DG) along with their corresponding ad-hoc retrieval topics (see Table 1 for details).
These collections had stop words removed and were Porter

stemmed. For the purposes of this study, we will focus on
the effect of document length normalization within two standard retrieval models (Okapi BM25 and DFRs PL2). This
is due to the fact that retrieval models will often favour
longer documents which introduces an undesirable retrieval
bias that if accounted for, has been shown to improve performance [11]. For each of these retrieval models we shall
investigate how retrievability bias and performance changes
as we manipulate the normalization parameter b and c respectively. In our experiments for BM25’s b parameter we
used values between 0 and 1 with steps of 0.11 , and for PL2’s
c parameter we used values 0.1, 1, . . . 10, 100 with steps of 1
between 1 and 10.
For each collection, model and parameter value, we recorded
the retrieval performance (Mean Average Precision (MAP)
and Precision @ 10 (P@10)) and calculated the retrievability
bias using the approach employed in prior work [3, 1, 5, 7].
This was performed as follows; we extracted all terms that
co-occurred more than once with each other from the collection and used these as bigram queries (Table 1 shows the
number of queries per collection issued). These queries were
issued to the retrieval system given its parametrization and
the results were used to compute retrievability measures. We
computed both cumulative scores with cut-offs of 5, 10, 20,
50, 70, 100, and gravity based scores with cut-off of 100 and
β (where β is the discount factor) values of 0.5, 1, 1.5 and 2.
For brevity, we only report on a subset of these. These were
used to compute the corresponding Gini coefficient for each
measure, model, normalization parameter and collection.
Collections
Docs
Collection Type
Trec Topics
# Bigrams/Queries

AP
164,597
News
1-200
81964

AQ
1,033,461
News
303-369
273245

DG
1,247,753
Web
551 - 600
337275

Table 1: Collection Statistics

4.

RESULTS

During the course of our analysis we examined the following research questions:
How does the retrievability bias change across length
normalization parameters? In Figure 1, the left hand
plots show the relationship between the retrievability bias
(denoted by the Gini coefficient for different retrievability
measures) and the parameter settings for BM25 and PL2.
Immediately we can see that as the parameter setting is
manipulated, the retrievability bias changes. For BM25, the
minimum bias was when b=0.7 on AP and AQ, and b=0.9
on DG, whereas on PL2 the minimum bias was was between
c=1 and c=3 (see Table 2). This was regardless of the Gini
measure used i.e. the cumulative and gravity based measures resulted in the same minimum. Subsequently, for the
remainder of this paper, we will use the gravity based retrievability measure when g = 1. What was different between Gini measures was the magnitude of the bias. For
example, when the cut-off is low then the bias observed was
higher than when the cutoff was increased. This finding is
consistent with prior work [3]. When the parameter value of
the retrieval was increased or decreased, then we observed
that retrieval biased increased, sometimes quite dramatically. This suggests that either longer or shorter documents
1

When b = 0, this is equivalent to BM15 and when b = 1 this is
equivalent to BM11

938

Aquaint BM25 gini vs b
0.95

0.75
0.7

0.55
0.5
0

0.85
0.8
0.75

5
4
3

7
6
5
4

0.2

0.4

0.6

0.8

1

0

2

4

6

8

.Gov PL2 gini vs. c

16

Gini Coefficient

0.8

C10
C100
G0.5
G1.0
G1.5

0.92
0.9
0.88

C10
C100
G0.5
G1.0
G1.5

0.86
0.84
0.82

0.8

BM25 b Parameter Values

1

Average R(d)

0.94

0.9
0.85

0.8
0

2

4

6

1000

0
0

1500

500

8

.Gov PL2 (G1.0) R(d) vs. Length
18
16

14
12
10
8

1
3
10

14
12
10
8

6

6

4

4
2

0
0

PL2 c Parameter Values

1500

20

0.1
0.7
0.9

2

10

1000

Average Length in Bucket

.Gov BM25 (G1.0) R(d) vs. Length
18

0.6

500

Average Length in Bucket
20

0.4

1

0
0

10

PL2 c Parameter Values

0.96

0.2

2

1

0.98

0

1
3
10

3

2
0.65

.Gov BM25 gini vs b

Gini Coefficient

6

1

0.65

8

7

0.95

0.7

9

0.7

BM25 b Parameter Values

0.75

10

0.1
0.7
0.9

Average R(d)

0.6

C10
C100
G0.5
G1.0
G1.5

8

Average R(d)

Gini Coefficient

Gini Coefficient

0.9

9

Average R(d)

C10
C100
G0.5
G1.0
G1.5

0.9

0.8

Aquaint PL2 (G1.0) R(d) vs. Length

10

1

0.85

0.65

Aquaint BM25 (G1.0) R(d) vs. Length

Aquaint PL2 gini vs. c

1
0.95

5000

10000

Average Length in Bucket

15000

0
0

5000

10000

15000

Average Length in Bucket

Figure 1: Left: Retrieval Bias versus b/c. Right: Retrievability versus Document Length

Collections
AP
Aquaint
.Gov

Model
BM25
PL2
BM25
PL2
BM25
PL2

Minimum Gini (G1.0)
b/c Gini MAP
P10
0.7
0.643
0.232
0.354
3
0.701
0.235
0.374
0.7
0.701
0.162
0.316
2
0.746
0.169
0.331
0.9
0.749
0.167
0.222
1
0.872
0.182
0.220

Maximum
b/c Gini
0.3
0.708
3
0.701
0.3
0.785
6
0.774
0.6
0.707
2
0.883

Map (G1.0)
MAP
P10
0.239* 0.378*
0.235
0.374
0.185* 0.390*
0.178
0.374*
0.179
0.216
0.184
0.222

Min. Gini (G1.0) Corr.
Gini-MAP
Gini-P10
-0.075
0.184
-0.978*
-0.925
0.186
0.451
-0.844*
-0.523
-0.646
-0.775
-0.887
-0.916*

Table 2: Minimum Gini/Maximum MAP and corresponding parameter settings/values. ∗ represent statistical
significance. Far right columns show the Pearson’s Correlation between Retrievability Bias and performance.
were being favoured depending on the setting of b and c.
In BM25, as b tends to 1, longer documents are penalized,
while when b tends to 0, longer documents are favoured. For
PL2, as c tends to 0 then longer documents are penalized,
and as c tends to infinity, longer documents are favoured.
How does the retrievability of documents change
across length normalization parameters? To examine
this intuition, we sorted the documents in each collection by
length and placed them into buckets. We then computed the
mean length within each bucket, and the mean retrievability
r(d) given the documents in the bucket. The plots on the
right in Figure 1 show how the retrievability changes when
the length normalization parameter changes on BM25/PL2
and AQ/DG for a subset of parameters with a retrievability measure of g = 1. The plots clearly indicate that when
the retrieval bias is minimized (for example, when b = 0.7 on
AQ) the retrievability across length is about as equal/fair as
it gets across the collection. This shows that the measures
of retrievability bias (i.e. the Gini coefficients) capture the
bias towards longer and shorter documents as the document
length parameter is manipulated.
What is the relationship between Performance and
Retrievability? In Figure 2, we plotted the Gini coefficient against MAP for each collection using the results from
BM25. Similar plots can be found in Figure 3 for other retrieval models and other performance measures. On these
plots, a large diamond has been added to the lines to indicate the length normalization parameter that favours longer
documents (ie. b = 0 and c = 100). As the parameter value
is increased/decreased for BM25/PL2 the retrieval model

setting favours shorter documents (as shown in the plots in
Figure 1).
The plots provide a number of key insights into the relationship between retrievability bias and performance. Firstly,
the relationship is non-linear with a trend that suggests that
minimizing bias leads to better performance. If we consider
the relationship in Figure 2 for DotGov, we can see that a
reduction in bias leads to successive improvements in terms
of performance for MAP (and P@10). With this collection,
the effect is the most pronounced and minimizing retrieval
bias does tend to the best retrieval performance. Once too
much length normalization has occurred, such that shorter
documents are overly favoured, then the retrievability bias
increases and performance begins to decrease.
When we examine the relationship for AP and AQ, we see
that as shorter documents become more favoured, a trade-off
between bias and performance quickly develops, but again
once the minimum bias point has been reached, the kick
back (i.e. loss in performance and increase in bias) is much
more pronounced. These findings suggest that as document
length normalization is applied in order to penalize longer
documents, going past the point of minimum bias will degrade performance and retrievability.
Table 2 shows the performance of the retrieval models
when bias is minimized and when performance is maximized.
Included in the table are the results of t-tests conducted to
determine whether there was a statistical difference in performance between the two settings (assuming p < 0.05, ∗
indicates a significant difference). For AP and AQ, we found
there was a significant difference for BM25 (both MAP and

939

AP, Aquaint & .Gov BM25 (G1.0) MAP vs. gini

P@10), however the differences were less pronounced for PL2
and on DotGov. These findings suggest that minimizing bias
may not lead to the best performance. It does however, tend
to give reasonably good retrieval performance that on many
occasions, is not significantly different from the best possible retrieval performance. We speculate that the mis-match
between may be due to the length bias with the TREC relevance pools as discovered in [9]. Thus, in the absence of
relevance judgements tuning a retrieval system such that it
minimizes retrieval bias is likely to be a good starting point.

1

AP
Aquaint
.Gov

0.95
0.9

0.9

Gini

Gini

0.8

0.7

0.7

0.05

0.1

0.15

0.2

0.65
0.05

0.25

0.1

0.15

MAP

0.2

0.25

MAP

AP, Aquaint & .Gov BM25 (G1.0) P@10 vs. gini

AP, Aquaint & .Gov PL2 (G1.0) P@10 vs. gini

1

1

AP
Aquaint
.Gov

0.9

AP
Aquaint
.Gov

0.95
0.9

0.9

Gini

AP
Aquaint
.Gov

Gini

0.85

0.95

0.8

0.85
0.8

0.75
0.75

0.7

0.7

0.65
0.1

0.85

Gini

0.8
0.75

0.65

1

0.15

0.2

0.25

0.3

P@10

0.8

0.35

0.4

0.45

0.5

0.65
0.1

0.15

0.2

0.25

0.3

0.35

0.4

0.45

0.5

P@10

Figure 3: MAP (Top plots) and P@10 (Bottom
plots) vs. Gini for all collections on BM25 (right
plots) and PL2 (left plots).

0.75
0.7

[3] L. Azzopardi and V. Vinay. Retrievability: An
evaluation measure for higher order information access
tasks. In Proc. of the 17th ACM CIKM, pages
561–570, 2008.
[4] R. Bache. Measuring and improving access to the
corpus. In Current Challenges in Patent Information
Retrieval, volume 29 of The Information Retrieval
Series, pages 147–165. 2011.
[5] S. Bashir and A. Rauber. Improving retrievability of
patents with cluster-based pseudo-relevance feedback
documents selection. In Proc. of the 18th ACM CIKM,
pages 1863–1866, 2009.
[6] S. Bashir and A. Rauber. Improving retrievability of
patents in prior-art search. In Proc. of the 32nd ECIR,
pages 457–470, 2010.
[7] S. Bashir and A. Rauber. Improving retrievability and
recall by automatic corpus partitioning Transactions
on large-scale data- and knowledge-centered systems ii.
chapter I, pages 122–140. 2010.
[8] J. Gastwirth. The estimation of the lorenz curve and
gini index. The Review of Economics and Statistics,
54:306–316, 1972.
[9] D. E. Losada, L. Azzopardi, and M. Baillie. Revisiting
the relationship between document length and
relevance. In Proc. of the 17th ACM CIKM’08, pages
419–428, 2008.
[10] J. Pickens, M. Cooper, and G. Golovchinsky. Reverted
indexing for feedback and expansion. In Proc. of the
19th ACM CIKM, pages 1049–1058, 2010.
[11] A. Singhal, C. Buckley, and M. Mitra. Pivoted
document length normalization. In Proc. of the 19th
ACM SIGIR, pages 21–29, 1996.
[12] A. Singla and I. Weber. Tagging and navigability. In
Proc. of the 19th WWW, pages 1185–1186, 2010.
[13] C. J. van Rijsbergen. Information Retrieval. 1979.
[14] L. Zheng and I. J. Cox. Document-oriented pruning of
the inverted index in information retrieval systems. In
Proc. of the 2009 WIANA, pages 697–702, 2009.

0.65
0.1

0.15

0.2

0.25

MAP
Figure 2: MAP vs. Gini for all collections on BM25.

SUMMARY

In this paper we have analysed the relationship between
retrievability bias and performance in the context of adhoc retrieval across length normalization parameters for two
standard IR models. Empirically, we have shown that the
relationship is much more complex that previously thought
and in fact non-linear. Nonetheless, we have shown that
reducing the bias within the collection leads to reasonably
good performance, and crucially, if too much document length
normalization is performed then this will invariably result
in a degradation of performance and an increase in bias.
This is a useful finding suggesting that retrievability could
be used to tune retrieval systems without recourse to relevance judgements. Future work will be directed to studying
the relationship between retrievability and performance (in
terms of MAP and P@10) in more detail across other collections and across different parameter settings (i.e. query
parameters, smoothing parameters) and with different retrieval models (language models, query expansion, link/click
evidence, etc).
Acknowledgments This work is supported by the
EPSRC Project, Models and Measures of Findability
(EP/K000330/1).

6.

0.85

0.75

AP, Aquaint & .Gov BM25 (G1.0) MAP vs. gini

5.

AP
Aquaint
.Gov

0.95

0.85

0.95

0.05

AP, Aquaint & .Gov PL2 (G1.0) MAP vs. gini

1

REFERENCES

[1] L. Azzopardi and R. Bache. On the relationship
between effectiveness and accessibility. In Proc. of the
33rd international ACM SIGIR, pages 889–890, 2010.
[2] L. Azzopardi and C. Owens. Search engine
predilection towards news media providers. In Proc. of
the 32nd ACM SIGIR, pages 774–775, 2009.

940

