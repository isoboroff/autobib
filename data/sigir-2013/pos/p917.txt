Composition of TF Normalizations: New Insights on
Scoring Functions for Ad Hoc IR
François Rousseau* , Michalis Vazirgiannis*†‡
*

LIX, École Polytechnique, France
Department of Informatics, AUEB, Greece
‡
Institut Mines-Télécom, Télécom ParisTech, France
†

rousseau@lix.polytechnique.fr, mvazirg@aueb.gr
ABSTRACT

H.3.3 [Information Search and Retrieval]: Retrieval
models

mation retrieval (IR) should satisfy. In particular, these constraints involve term frequency, term discrimination, document length and the interactions between them. For instance, they stated that a scoring function should favor
document matching more distinct query terms. It is one
of the earliest works that formally defined the properties
that both the TF and the IDF components of any weighting
model should possess. It is a unifying theory in IR that applies to the vector space model (TF-IDF [13]), probabilistic
(BM25 [10]), language modeling (Dirichlet prior [15]) and
information-based (SPL [2]) approaches and the divergence
from randomness framework (PL2 [1]).
The definition of these constraints contributed to the improvement of the overall eﬀectiveness of most modern scoring functions. Constraints on the term frequency result in
successive normalizations on the raw TF, each one satisfying one or more properties. In our work, we intended to go
one step further and we propose the use of composition to
explain how the normalizations are applied successively in
the general TF×IDF weighting scheme. In section 2, we describe in details the mathematical framework we designed.
In section 3, we present the experiments we conducted over
standard datasets and the results obtained that indicate
how important the order of composition is, along with a
novel and eﬀective weighting model, namely TFl◦δ◦p ×IDF.
Finally, in section 4, we conclude and mention future work.

General Terms

2.

Theory, Algorithms, Experimentation

In ad hoc IR, a scoring function associates a score to a
term appearing both in a query and a document. This function consists of three components supposedly independent of
one another: one at the query level (QF), one at the document level (TF) and one at the collection level (IDF). These
components are aggregated through multiplication to obtain
a final score for the term denoted hereinafter by TF×IDF.
We omit voluntarily to mention QF in the name since it is
a function of the term frequency in the query and it has
usually a smaller impact on the score, in particular for Web
queries that tend to be short. We make here a diﬀerence between the TF×IDF general weighting scheme and TF-IDF,
the pivoted normalization weighting defined in [13]. Note
that because we rank documents, these term scores will be
aggregated through sum to obtain a document score but this
is beyond the scope of the current paper.

Previous papers in ad hoc IR reported that scoring functions
should satisfy a set of heuristic retrieval constraints, providing a mathematical justification for the normalizations historically applied to the term frequency (TF). In this paper,
we propose a further level of abstraction, claiming that the
successive normalizations are carried out through composition. Thus we introduce a principled framework that fully
explains BM25 as a variant of TF-IDF with an inverse order of function composition. Our experiments over standard
datasets indicate that the respective orders of composition
chosen in the original papers for both TF-IDF and BM25
are the most eﬀective ones. Moreover, since the order is different between the two models, they also demonstrated that
the order is instrumental in the design of weighting models.
In fact, while considering more complex scoring functions
such as BM25+, we discovered a novel weighting model in
terms of order of composition that consistently outperforms
all the rest. Our contribution here is twofold: we provide a
unifying mathematical framework for IR and a novel scoring
function discovered using this framework.

Categories and Subject Descriptors

Keywords
IR theory; scoring functions; TF normalizations; heuristic
retrieval constraints; function composition

1. MOTIVATION
Fang et al. introduced in [3] a set of heuristic retrieval
constraints that any scoring function used for ad hoc inforPermission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

2.1

MATHEMATICAL FRAMEWORK

A set of TF normalizations

Since the early work of Luhn [4], term frequency (TF)
has been claimed to play an important role in information

917

retrieval and is at the center of all the weighting models.
Intuitively, the more times a document contains a term of
the query, the more relevant this document is for the query.
Hence, it is commonly accepted that the scoring function
must be an increasing function of the term frequency and
the simplest TF component can be defined as follows:
T F (t, d) = tf (t, d)

length and avdl the average document length across the collection of documents as defined in [12].

2.2

Composition of TF normalizations

Based on this set of properties and their associated functions, it seems natural to apply them to the raw TF successively by composing them. In the literature, the document
length normalization has usually been applied to either the
overall term score or the document score like one would normally normalize a vector. However, it was then hard to fully
fit BM25 in the TF×IDF weighting scheme. With composition, it is just a matter of ordering the functions. We present
here the two compositions behind TF-IDF and BM25, respectively TFp◦l (= TFp ◦ TFl ) and TFk◦p (= TFk ◦ TFp ):

(1)

where tf (t, d) is the term frequency of the term t in the
document d. However, as the use of the raw term frequency
proved to be non-optimal in ad hoc IR, the research community started normalizing it considering multiple criteria,
mainly concavity and document length normalization. Later,
these normalizations were explained as functions satisfying
some heuristic retrieval constraints as aforementioned [3].

T Fp◦l (t, d) =

Concave normalization.

(2)

(k1 + 1) · tf (t, d)
T Fk (t, d) =
k1 + tf (t, d)

(3)

T Fk◦p (t, d) =

=
=

2.3

When collections consist of documents of varying lengths
(like web pages for the Web), longer documents will – as
a result of containing more terms – have higher TF values without necessary containing more information. For instance, a document twice as long and containing twice as
more times a term should not get a score twice as large but
rather a very similar score. As a consequence, it is commonly accepted that the scoring function should be an inverse function of the document length to compensate that
eﬀect. Early works in vector space model suggested to normalize the score by the norm of the vector, be it the L1
norm (document length), the L2 norm (Euclidian length) or
the L∞ norm (maximum TF value in the document) [11].
These norms still mask some subtleties about longer documents – since they contain more terms, they tend to score
higher anyway. Instead, the research community has been
using a more complex normalization function known as pivoted document length normalization and defined as follows:
1−b+b·

|d|
avdl

|d|

1−b+b· avdl

(k1 + 1) · tf (t, d)
k1 · [1 − b + b ·

|d|
]
avdl

+ tf (t, d)

(k1 + 1) · tf (t, d)
K + tf (t, d)

(6)

Inverse Document Frequency

While the higher the frequency of a term in a document is,
the more salient this term is supposed to be, this is no longer
true at the collection level. This is actually quite the inverse
since these terms have a presumably lower discrimination
power. Hence, the use of a function of the term specificity,
namely the Inverse Document Frequency (IDF) as defined
in [14] and expressed as follows:

Document length normalization.

tf (t, d)

k1 +

|d|

1−b+b· avdl
tf (t,d)

|d|
where K = k1 · (1 − b + b · avdl
) as defined in [10].
Note that under this form, it is not obvious that TFk◦p is
really a composition of two functions with the same properties as the one in TFp◦l . We think this is the main reason
why composition has never been considered before. By doing so, we provide not only a way to fully explain BM25 as a
TF×IDF weighting scheme but also a way to easily consider
variants of a weighting model by simply changing the order
of composition. As we will see in section 3, this led us to a
new TF×IDF weighting scheme that outperforms BM25.

where k1 is a constant set by default to 1.2 corresponding
to the asymptotical maximal gain achievable by multiple
occurrences compared to a single occurrence.

T Fp (t, d) =

(5)

|d|
avdl
tf (t,d)

1−b+b·
(k1 + 1) ·

The marginal gain of seeing an additional occurrence of a
term inside a document is not constant but rather decreasing. Indeed, the change in the score caused by increasing
TF from 1 to 2 should be much larger than the one caused
by increasing TF from 100 to 101. Mathematically, this corresponds to applying a concave function on the raw TF. We
prefer the term concave like in [2] to sublinear like in [5]
since the positive homogeneity property is rarely respected
(and actually not welcomed) and the subadditivity one, even
though desirable, not suﬃcient enough to ensure a decreasing marginal gain.
There are mainly two concave functions used in practice:
the one in TF-IDF [13] and the one in BM25 [10] that we respectively called log-concavity (TFl ) and k-concavity (TFk ):
T Fl (t, d) = 1 + ln[1 + ln[tf (t, d)]]

1 + ln[1 + ln[tf (t, d)]]

IDF (t) = log

N +1
df (t)

(7)

where N is the number of documents in the collection and
df (t) the document frequency of the term t.

2.4

TF-IDF versus BM25

By TF-IDF, we refer to the TF×IDF weighting model
defined in [13], often called pivoted normalization weighting.
The weighting model corresponds to TFp◦l ×IDF:
T F -IDF (t, d) =

1 + ln[1 + ln[tf (t, d)]]
1−b+b·

|d|
avdl

× log

N +1
df (t)

(8)

By BM25, we refer to the scoring function defined in [10],
often called Okapi weighting. It corresponds to TFk◦p ×IDF
when we omit QF (k-concavity of parameter k3 for tf (t, q)).
Thus, it has an inverse order of composition between the

(4)

where b ∈ [0, 1] is the slope parameter, |d| the document

918

concavity and the document length normalization compared
to TF-IDF. The within-document scoring function of BM25
is written as follows when using the IDF formula defined in
subsection 2.3 to avoid negative values, following [3]:
BM 25(t, d) =

(k1 + 1) · tf (t, d)
N +1
× log
K + tf (t, d)
df (t)

the extensions considered by the research community [6, 7]
in terms of composition. This led us to a novel weighting
model that outperforms them (see subsection 3.4).

3.1

(9)

2.5 Lower-bounding TF normalization
Through composition, we also allow additional constraints
for the TF component to be satisfied easily. Subadditivity is for instance a desirable property – if two documents
have the same total occurrences of all query terms, a higher
score should be given to the document covering more distinct query terms. Here, it just happens that TFl and TFk
already satisfy it as noted in [3].
Recently, Lv and Zhai introduced in [6] two new constraints to the work of Fang et al. to lower-bound the TF
component. In particular, there should be a suﬃciently large
gap in the score between the presence and absence of a query
term even for very long documents where TFp tends to 0 and
a fortiori the overall score too. Mathematically, this corresponds to be composing with a third function TFδ that is
always composed after TFp since it compensates the potential null limit introduced by TFp and it is defined as follows:

{

T Fδ (t, d) =

tf (t, d) + δ
0

if tf (t, d) > 0
otherwise

Datasets and evaluation

We used two TREC collections to carry out our experiments: Disks 4&5 (minus the Congressional Record) and
WT10G. Disks 4&5 contains 528,155 news releases while
WT10G consists of 1,692,096 crawled pages from a snapshot of the Web in 1997. For each collection, we used a
set of TREC topics (title only to mimic Web queries) and
their associated relevance judgments: 301-450 and 601-700
for Disks 4&5 (TREC 2004 Robust Track) and 451-550 for
WT10G (TREC9-10 Web Tracks).
We evaluated the scoring functions in terms of Mean Average Precision (MAP) and Precision at 10 (P@10) considering only the top-ranked 1000 documents for each run.
Our goal is to compare weighting models that use the same
functions but with a diﬀerent order of composition and select the best ones on both metrics. For example, in Table 1,
TFp◦l ×IDF is compared with TFl◦p ×IDF and TFk◦p ×IDF
with TFp◦k ×IDF. The statistical significance of improvement was assessed using the Student’s paired t-test considering p-values less than 0.01 to reject the null hypothesis.

3.2
(10)

Platform and models

We have been using Terrier version 3.5 [8] to index, retrieve and evaluate over the TREC collections. For both
datasets, the preprocessing steps involved Terrier’s built-in
stopword removal and Porter’s stemming. We did not tune
the slope parameter b of the pivoted document length normalization on each dataset. We set it to the default value
suggested in the original papers: 0.20 when used with logconcavity [13] and 0.75 when used with k-concavity [10].

where δ is the gap, set to 0.5 if TFδ is composed immediately
after TFp and 1 if concavity is applied in-between. These
are the two values defined in the original papers [6, 7] and
we just interpreted their context of use in terms of order of
composition. We did not change nor tune the values.
The weighting models Piv+ and BM25+ defined in [6]
correspond respectively to TFδ◦p◦l ×IDF and TFδ◦k◦p ×IDF
while BM25L defined in [7] to TFk◦δ◦p ×IDF. We clearly see
that the only diﬀerence between BM25+ and BM25L is the
order of composition: this is one of the advantages of our
framework – easily represent and compute multiple variants
of a same general weighting model. In the experiments, we
considered all the possible orders of composition between
TFk or TFl , TFp and TFδ with the condition that TFp
always precedes TFδ as explained before.
For instance, we will consider a novel model TFl◦δ◦p ×IDF
defined as follows:
tf (t, d)
T Fl◦δ◦p ×IDF (t, d) = 1+ln[1+ln[
+δ]] (11)
|d|
1 − b + b · avdl

3.3

Results for TF-IDF versus BM25

We report in Table 1 the results we obtained on the aforementioned datasets when considering concavity and pivoted
document length normalization. To the best of our knowledge, experiments regarding the same functions (TFk , TFl
and TFp ) with a diﬀerent order of composition have never
been reported before. They indeed show that the original order chosen for both TF-IDF and BM25 is the most eﬀective
one: TFp◦l ×IDF outperforms TFl◦p ×IDF and TFk◦p ×IDF
outperforms TFp◦k ×IDF. But since the order is diﬀerent between the two, this also indicates that the order does matter
depending on which function is chosen for each property.
For these two models (TF-IDF and BM25), the use of
a diﬀerent concave function to meet the exact same constraints requires the pivoted document length normalization
to be applied before or after the function. The impact is even
more significant on the Web dataset (WT10G) that corresponds the most to contemporary collections of documents.

where b is set to 0.20 and δ to 0.5.

3. EXPERIMENTS
Following our mathematical framework that relies on composition, we wondered why the order of composition was different between two widely used scoring functions – TF-IDF
and BM25. In the original papers [11, 9], there was no mention of the diﬀerence in the order and this motivated us to
investigate the matter. Our initial thought was that using
an inverse order of composition in BM25 could improve it
or vice-versa for TF-IDF. As a consequence, we tried exhaustively the combinations among TFk , TFl and TFp and
report the results. Thereafter, as mentioned in subsection
2.5, we followed the same procedure considering a third function to compose with: TFδ . Indeed, we wanted to explore

3.4

Results for lower-bounding normalization

In Table 2, we considered in addition the lower-bounding
normalization function TFδ defined in subsection 2.5. The
best-performing weighting model on both datasets is a novel
one – TFl◦δ◦p ×IDF – and it even outperforms BM25+ and
BM25L (significantly using the t-test and p < 0.01). This
model has never been considered before in the literature to
the best of our knowledge. In fact, the results from Table 1
establish that TFl should apparently be applied before TFp

919

are confident that refining the mathematical properties behind scoring functions will continue to improve the eﬀectiveness of these models in ad hoc IR.

Table 1: TF-IDF vs. BM25: an inverse order of
composition; bold indicates significant performances
Weighting model
IDF
TF
TFp [b=0.20]
TFp [b=0.75]
TFl
TFk
TFp◦k
TFl◦p
TFp◦l
TFk◦p
TFp◦k ×IDF
TFl◦p ×IDF
TFp◦l ×IDF [TF-IDF]
TFk◦p ×IDF [BM25]

TREC 2004 Robust
MAP
P@10
0.1396
0.2040
0.0480
0.0867
0.0596
0.1193
0.0640
0.1289
0.1591
0.3141
0.1768
0.3269
0.0767
0.1932
0.1645
0.3651
0.1797
0.3647
0.2045
0.3863
0.1034
0.2293
0.1939
0.3964
0.2132
0.4064
0.2368
0.4161

TREC9-10 Web
MAP
P@10
0.0539
0.0729
0.0376
0.0833
0.0531
0.1021
0.0473
0.1000
0.1329
0.2063
0.1522
0.2104
0.0465
0.0604
0.0622
0.1854
0.1260
0.1875
0.1702
0.2208
0.0507
0.0833
0.0750
0.2125
0.1430 0.2271
0.1870 0.2479

5.

6.

Table 2: TFl◦δ◦p ×IDF vs. BM25+ and BM25L; bold
indicates significant performances.

TFδ◦p◦k
TFδ◦l◦p
TFl◦δ◦p
TFδ◦p◦l
TFk◦δ◦p
TFδ◦k◦p
TFδ◦p◦k ×IDF
TFδ◦l◦p ×IDF
TFl◦δ◦p ×IDF
TFδ◦p◦l ×IDF [Piv+]
TFk◦δ◦p ×IDF [BM25L]
TFδ◦k◦p ×IDF [BM25+]

TREC 2004 Robust
MAP
P@10
0.1056
0.2349
0.1807
0.3751
0.2130
0.4064
0.2002
0.3876
0.2155
0.3936
0.2165
0.3956
0.1466
0.2723
0.2096
0.4048
0.2495
0.4305
0.2368
0.4157
0.2472
0.4217
0.2466
0.4145

REFERENCES

[1] G. Amati and C. J. Van Rijsbergen. Probabilistic
models of information retrieval based on measuring
the divergence from randomness. ACM Transactions
on Information Systems, 20(4):357–389, Oct. 2002.
[2] S. Clinchant and E. Gaussier. Information-based
models for ad hoc IR. In Proceedings of SIGIR’10,
pages 234–241, 2010.
[3] H. Fang, T. Tao, and C. Zhai. A formal study of
information retrieval heuristics. In Proceedings of
SIGIR’04, pages 49–56, 2004.
[4] H. P. Luhn. A statistical approach to mechanized
encoding and searching of literary information. IBM
Journal of Research and Development, 1(4):309–317,
Oct. 1957.
[5] Y. Lv and C. Zhai. Adaptive term frequency
normalization for BM25. In Proceedings of CIKM’11,
pages 1985–1988, 2011.
[6] Y. Lv and C. Zhai. Lower-bounding term frequency
normalization. In Proceedings of CIKM’11, pages
7–16, 2011.
[7] Y. Lv and C. Zhai. When documents are very long,
BM25 fails! In Proceedings of SIGIR’11, pages
1103–1104, 2011.
[8] I. Ounis, G. Amati, V. Plachouras, B. He,
C. Macdonald, and C. Lioma. Terrier: A high
performance and scalable information retrieval
platform. In Proceedings of SIGIR’06, 2006.
[9] S. E. Robertson and S. Walker. Some simple eﬀective
approximations to the 2-poisson model for
probabilistic weighted retrieval. In Proceedings of
SIGIR’94, pages 232–241, 1994.
[10] S. E. Robertson, S. Walker, K. Spärck Jones,
M. Hancock-Beaulieu, and M. Gatford. Okapi at
TREC-3. In Proceedings of the TREC-3, pages
109–126, 1994.
[11] G. Salton and C. Buckley. Term-weighting approaches
in automatic text retrieval. Information Processing
and Management, 24(5):513–523, 1988.
[12] A. Singhal, C. Buckley, and M. Mitra. Pivoted
document length normalization. In Proceedings of
SIGIR’96, pages 21–29, 1996.
[13] A. Singhal, J. Choi, D. Hindle, D. Lewis, and
F. Pereira. AT&T at TREC-7. In Proceedings of
TREC-7, pages 239–252, 1999.
[14] K. Spärck Jones. A statistical interpretation of term
specificity and its application in retrieval. Journal of
Documentation, 28(1):11–20, 1972.
[15] C. Zhai and J. Laﬀerty. A study of smoothing
methods for language models applied to ad hoc
information retrieval. In Proceedings of SIGIR’01,
pages 334–342, 2001.

like in TF-IDF. With lower-bounding normalization, it no
longer holds. The formula for TFl◦δ◦p ×IDF was given in
equation 11. Without the use of our formal framework and
composition, it would have been harder to detect and test
these variants that can outperform state-of-the-art scoring
functions when the order of composition is chosen carefully.

Weighting model

ACKNOWLEDGMENTS

We thank the anonymous reviewers for their useful feedbacks. This material is based upon work supported by the
French DIGITEO Chair grant LEVETONE.

TREC9-10 Web
MAP
P@10
0.0556
0.0771
0.0668
0.2021
0.1907
0.2625
0.1436
0.2021
0.1806
0.2292
0.1835
0.2354
0.0715
0.1000
0.0806
0.2292
0.2084 0.2771
0.1643
0.2438
0.2000 0.2563
0.2026 0.2521

4. CONCLUSIONS AND FUTURE WORK
Scoring function design is a cornerstone issue in information retrieval. In this short paper, we intended to provide
new insights on scoring functions for ad hoc IR. In particular, we proposed a unifying mathematical framework that
explains how weighting models articulate around a set of
heuristic retrieval constraints introduced in related work.
Using composition to combine the successive normalizations historically applied to the term frequency, we were
able to fully explain BM25 as a TF×IDF weighting scheme
with just an inverse order of composition between the concavity and the document length normalization compared to
TF-IDF. Besides, the framework also allowed us to discover
and report a novel weighting model – TFl◦δ◦p ×IDF – that
consistently and significantly outperformed BM25 and its
extensions on two standard datasets in MAP and P@10.
Future work might involve the design of novel retrieval
constraints and their compositions with existing ones. We

920

