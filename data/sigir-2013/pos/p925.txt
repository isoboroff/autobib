A Comparison of the Optimality of Statistical
Significance Tests for Information Retrieval Evaluation
Julián Urbano
jurbano@inf.uc3m.es
∗

Mónica Marrero
mmarrero@inf.uc3m.es

∗

University Carlos III of Madrid
Department of Computer Science
Leganés, Spain

∗

Diego Martín
dmartin@dit.upm.es
†

†

Technical University of Madrid
Department of Telematics Engineering
Madrid, Spain

ABSTRACT

will be (at least) as large as currently observed. If p > α
the diﬀerence is not signiﬁcant (A  B), and she can not be
conﬁdent that the observed diﬀerence is indeed real.
Unfortunately, there has been a debate regarding statistical signiﬁcance testing in IR evaluation. Classical tests such
as the paired t-test, the Wilcoxon test and the sign test
make diﬀerent assumptions about the distributions, and effectiveness scores from IR evaluations are known to violate
these assumptions. The bootstrap test is an alternative that
makes fewer assumptions and has other advantages over classical tests, and the permutation or randomization test is an
even less stringent test in terms of assumptions that theoretically provides exact p-values. Because IR evaluations violate
most of the assumptions, it is very important to know how
robust these tests are in practice and which one is optimal.
Previous work [4, 5] compared these ﬁve tests with TREC
Ad Hoc data, reaching the following conclusions: a) the
bootstrap, t-test and permutation test largely agree with
each other, so there is hardly any practical diﬀerence in using one or another; b) the permutation test should be the
test of choice, though the t-test seems suitable as well; the
bootstrap test shows a bias towards small p-values; c) the
Wilcoxon and sign tests are unreliable and should be discontinued for IR evaluation. However, all these conclusions
were based on the assumption that the permutation test is
optimal. For example, authors showed that the Wilcoxon
and sign tests fail to detect signiﬁcance when the permutation test does and vice versa. That is, they are unreliable
according to the permutation test.
But we may follow diﬀerent criteria to chose an optimal
test. We may want the test to be powerful, that is, to produce signiﬁcant results as often as possible. Additionally, we
may want it to be safe and yield low error rates so that it
is unlikely that we draw wrong conclusions. But power and
safety are inversely related; diﬀerent tests show diﬀerent relations depending on the signiﬁcance level. The lower α the
lower the power, because we need p ≤ α for the result to be
signiﬁcant. Error rates are expected to be at the nominal
α level, so the higher the signiﬁcance level the higher the
expected error rate. The test is exact if we can trust that
the actual error rate is as dictated by the signiﬁcance level.
If it is below it means we are being too conservative and we
are missing signiﬁcant results; if it is above it means we are
deeming as signiﬁcant results that probably are not.
This paper presents a large-scale empirical study that
compares all ﬁve tests according to these optimality criteria,
providing signiﬁcance and error rates at various signiﬁcance
levels for 50-topic sets. Our main ﬁndings are:

Previous research has suggested the permutation test as
the theoretically optimal statistical signiﬁcance test for IR
evaluation, and advocated for the discontinuation of the
Wilcoxon and sign tests. We present a large-scale study
comprising nearly 60 million system comparisons showing
that in practice the bootstrap, t-test and Wilcoxon test outperform the permutation test under diﬀerent optimality criteria. We also show that actual error rates seem to be lower
than the theoretically expected 5%, further conﬁrming that
we may actually be underestimating signiﬁcance.

Categories and Subject Descriptors
H.3.4 [Information Storage and Retrieval]: Systems
and Software—Performance evaluation.

Keywords
Evaluation, Statistical signiﬁcance, Randomization, Permutation, Bootstrap, Wilcoxon test, Student’s t-test, Sign test.

1. INTRODUCTION
An Information Retrieval (IR) researcher is often faced
with the question of which of two IR systems, A and B, performs better. She conducts an experiment with a test collection, and chooses an eﬀectiveness measure such as Average Precision or nDCG. Based on the eﬀectiveness diﬀerence
she concludes that, for instance, system A is better. But we
know there is inherent noise in the evaluation for a wealth
of reasons concerning document collections, topic sets, relevance assessors, etc. Therefore the researcher needs the
conclusion to be reliable, that is, the observed diﬀerence unlikely to have happened just by random chance. She employs
a statistical signiﬁcance test to compute this probability (the
p-value). If p ≤ α (the signiﬁcance level, usually α = 0.05 or
α = 0.01) the diﬀerence is considered statistically signiﬁcant
(A  B). In practice this means that she can be conﬁdent
that the diﬀerence measured with a similar test collection
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

925

• In practice the bootstrap test is optimal in terms of
power, the t-test is optimal in terms of safety, and the
Wilcoxon test is optimal in terms of exactness.
• For all tests the actual error rate seems to be lower
than the nominal 0.05 level, meaning that we are actually being too conservative.
• In practice the permutation test is not found to be
optimal under any criterion.

A powerful test minimizes the non-signiﬁcance rate, a safe
test minimizes the minor and major error rates, and an exact
test maintains the global error rate at the nominal α level.

3. RESULTS
For every statistical signiﬁcance test we computed the
non-signiﬁcance, success, lack of power and error rates at
32 signiﬁcance levels α ∈ {0.0001, ..., 0.0009, 0.001, ...,
0.009, ..., 0.1, ..., 0.5}. Tables 1 and 2 report the results for
a selection of signiﬁcance levels, and Figures 1 and 2 plot
detailed views in the arguably most interesting [0.001, 0.1]
range. Please note that all plots are log-scaled.
Non-significance rate. The bootstrap test consistently
produces smaller p-values, and it is therefore the most powerful of all tests across signiﬁcance levels. Next are the permutation test for α < 0.01 and the Wilcoxon test for the
usual α ≥ 0.01. The t-test is consistently less powerful,
though the diﬀerence is as small as roughly 1% fewer signiﬁcant results at the usual α = 0.05. The sign test is by far the
least powerful of all ﬁve. Its stair-like behavior is explained
by its resolution: p-values depend only on the sign of the
score diﬀerences, not on the magnitude (see Figure 5 in [4]).
Success rate. The bootstrap and Wilcoxon tests are
the most successful overall. For small signiﬁcance levels
α ≤ 0.001 the bootstrap test shows the highest success rate,
but for the more usual levels 0.001 < α ≤ 0.05 the Wilcoxon
test performs better. Next are again the permutation test
and the t-test, with very similar success rates about 0.3%
lower than the Wilcoxon and bootstrap tests at the usual α
levels. The sign test is clearly the worst of all.
Lack of power rate. Most of the unsuccessful comparisons are due to a lack of power with the second topic subset
T  . Relative results are comparable to results above: the
bootstrap test dominates at small signiﬁcance levels and the
Wilcoxon tests dominates at the usual levels, again followed
by the permutation test and the t-test.
Minor error rate. Except for rare occasions where the
sign test’s step-like behavior results in the smallest minor
error rate, the t-test is generally the safest of all ﬁve across
signiﬁcance levels. The permutation test follows next with
rates about 0.03% higher. The bootstrap test is consistently outperformed by the t-test and the permutation test;
it yields 0.13% more minor errors at α = 0.05. The Wilcoxon
test performs even better than the permutation test for low
signiﬁcance levels, but it performs worse at the usual levels.
As mentioned, the sign test wiggles between the other tests.
Major error rate. Similarly the t-test consistently performs best in terms of major errors, followed by the permutation and bootstrap tests. It is noticeable that for small
signiﬁcance levels neither of these three tests show any major error at all. For instance, at α = 0.005 the t-test provides
as many as 3,006,441 (50.2%) signiﬁcant comparisons, and
yet none of them results in a major error with the second
topic subset. The Wilcoxon test outperforms the permutation test sporadically, but it performs worse overall. In
general though, it is important to the bear in mind the magnitudes of the major error rates. For instance, at α = 0.05
the t-test produced 1,082 major errors and the bootstrap
test produced 1,523. While the diﬀerence may seem small
compared to the total of signiﬁcants (0.0277% vs 0.0383%),
this is actually a large +41% relative increase. The sign test
is clearly the worst of all, having an extremely large major
error rate at small signiﬁcance levels.

2. DATA AND METHODS
To compare the ﬁve statistical signiﬁcance tests at hand,
we employed data from the TREC 2004 Robust Track. A
total of 249 topics were used, 100 of which were originally developed in the TREC 7 and 8 Ad Hoc tracks (50 and 50). A
total of 110 runs were submitted by 14 diﬀerent groups. This
dataset is unusually large both in terms of topics and runs,
given that TREC tracks usually employ 50 topics. The subset with the 100 Ad Hoc topics is especially interesting: all
100 topics were developed and judged by the same assessors
for the most part, and they were developed using the same
methodology and pooling protocol with roughly the same
number of runs contributing to the pools [6]. Additionally,
all three tracks used disks 4 and 5 as document collection.
Therefore, we can consider these two sets of 50 topics as two
diﬀerent samples drawn from the same universe of topics.
We randomly split these 100 topics into two disjoint subsets of 50 topics each: T and T  . For each of these two
subsets we evaluated all 110 runs as per Average Precision.
This provides us with 5,995 system pairwise comparisons
with T and another 5,995 with T  . We ran all ﬁve statistical signiﬁcance tests between each of these system pairs1 .
This gives us a total of 5,995 pairs of p-values per test, which
can be regarded as the two p-values observed with two different test collections for any two systems. We performed
1,000 random trials of this experiment, so we have a total of
5,995,000 system pairwise comparisons and the corresponding 5,995,000 with another topic subset. Thus, this paper
reports results on nearly 12 million p-values for each of the
ﬁve tests, for a grand total of nearly 60 million p-values. To
our knowledge, this is to date the largest study of this type.
Given an arbitrary topic set split, the 5,995 pairs of pvalues provided by a test can be used to study its optimality.
Consider a researcher that used topic subset T and ran a
test to compute a p-value; under the signiﬁcance level α he
draws a conclusion. What can he expect with a diﬀerent
topic subset T  ?. One of these situations can occur:
• Non-significance. The result with T is A  B. We
can really expect any result with T  ; there is a lack of
statistical power in the experiment.
• Success. The result with both T and T  is A  B.
Both experiments show evidence of one system outperforming the other.
• Lack of power. The diﬀerence is A  B with T but
it is A  B with T  . There is evidence of a lack of power
in the second experiment.
• Minor error. The result with T is A  B, but with
T  it is A ≺ B. The second experiment shows some
evidence of a wrong conclusion in the ﬁrst one.
• Major error. The result with T is A  B, but with
T  it is A ≺≺ B. The two experiments conﬂict.
1
As in [4, 5], we calculated 100,000 samples in the permutation and bootstrap tests.

926

α
.0001
.0005
.001
.005
.01
.05
.1
.5

t-test
.67698
.61184
.5807
.49842
.45752
.34779
.29264
.12398

Non-significance rate
perm. boot.
Wilcox.
.65006 .6402 .67189
.59202 .58186 .60471
.56367 .5532 .5722
.48755 .47647 .48911
.44937 .43847 .4485
.34539 .33613 .34215
.29235 .28412 .28725
.12581 .12153 .11957

sign
.72367
.6782
.63438
.58347
.53308
.42762
.37308
.14934

t-test
.78749
.80788
.81328
.82051
.82777
.85579
.86905
.8836

perm.
.79451
.8107
.81491
.82145
.82893
.85565
.86899
.88369

Success rate
boot.
Wilcox.
.79757 .78859
.8123 .80765
.81547 .8147
.82365 .82598
.83233 .83338
.85856 .85935
.87086 .86941
.8836
.88429

sign
.73691
.75479
.76847
.78018
.79225
.81999
.83013
.85641

t-test
.21222
.19138
.18556
.1764
.16753
.13157
.1107
.05175

Lack of power rate
perm. boot.
Wilcox.
.20503 .20191 .21107
.18827 .18653 .19146
.18359 .18285 .18392
.17503 .17243 .17039
.16595 .16205 .16115
.1314 .12743 .12624
.11072 .10736 .10805
.05232 .05088 .04985

sign
.26264
.24431
.22998
.2169
.20276
.16709
.15031
.07511

Table 1: Non-significance rates over total of pairs (lower is better), success rates over total of significants
(higher is better), and lack of power rates over total of significants (lower is better). Best per α in bold face.

.005
.01
Significance level α

.05

.1

0.22
0.20
0.18
0.16
0.12

0.76

.001

0.14

Lacks of power / Total significants

0.86
0.84
0.82
0.80

Successes / Total significants

0.35

0.4

0.45

0.5

0.55

t−test
permutation
bootstrap
Wilcoxon
sign

Lack of power rate

0.3

Non−significants / Total

Success rate

0.78

0.65

Non−significance rate

.001

.005
.01
Significance level α

.05

.1

.001

.005
.01
Significance level α

.05

.1

Figure 1: Non-significance rates over total of pairs (lower is better), success rates over total of significants
(higher is better), and lack of power rates over total of significants (lower is better).
α
.0001
.0005
.001
.005
.01
.05
.1
.5

t-test
.00029
.00074
.00116
.00309
.00469
.01236
.01903
.03403

Minor error
perm. boot.
.00046 .00051
.00104 .00117
.00149 .00168
.00352 .00392
.00511 .0056
.01264 .01363
.01906 .02027
.03409 .03389

rate
Wilcox.
.00034
.00089
.00138
.00362
.00546
.014
.02123
.03645

sign
.00045
.00089
.00155
.00282
.00484
.01251
.01862
.03518

t-test
0
0
0
0
.00001
.00028
.00122
.03062

Major error rate
perm. boot. Wilcox.
0
0
5.08e-7
0
0
4.22e-7
0
0
3.9e-7
0
6.37e-7 1.96e-6
.00001 .00002 .00001
.0003 .00038 .0004
.00123 .00152 .00131
.0299 .03163 .02941

sign
6.04e-7
5.18e-7
4.56e-7
.0001
.00016
.00041
.00095
.0333

t-test
.00029
.00074
.00116
.00309
.0047
.01264
.02025
.06465

Global error
perm. boot.
.00046 .00051
.00104 .00117
.00149 .00168
.00352 .00392
.00512 .00562
.01294 .01402
.02029 .02178
.06399 .06552

rate
Wilcox.
.00034
.00089
.00138
.00362
.00547
.01441
.02254
.06586

sign
.00045
.00089
.00155
.00292
.00499
.01292
.01956
.06849

.005
.01
Significance level α

.05

.1

.001

.05

.1

2e−02
5e−03
2e−03

Minor and Major errors / Total significants
.005
.01
Significance level α

5e−04

5e−04
5e−05
5e−07

5e−06

0.005
0.002
.001

Global error rate

5e−02

Major error rate

Major errors / Total significants

0.010

Minor error rate
t−test
permutation
bootstrap
Wilcoxon
sign
y=x

0.001

Minor errors / Total significants

0.020

Table 2: Minor error rates over total of significants (lower is better), major error rates over total of significants
(lower is better), and global error rates over total of significants (errors = α is better). Best per α in bold face.

.0001

.0005.001

.005 .01
.05 .1
Significance level α

.5

Figure 2: Minor error rates over total of significants (lower is better), major error rates over total of significants (lower is better), and global error rates over total of significants (errors = α is better).

927

Global error rate. Aggregating minor and major errors
we have a global error rate that can be used as an overall
indicator of test safety and exactness. Given the relative size
of minor and major error rates, the trends are here nearly
the same as fwith minor errors, but for the sake of completeness we plot the full range of signiﬁcance levels. The t-test
approximates best the nominal error rate for low signiﬁcance
levels, but the Wilcoxon test does better for the usual levels
and best overall. Surprisingly the permutation test does not
seem to be the most exact at any signiﬁcance level.

p ≤.0001
.0001< p ≤.0005
.0005< p ≤.001
.001< p ≤.005
.005< p ≤.01
.01< p ≤.05
.05< p ≤.1
.1< p ≤.5

t-test
.03603
.10124
.13059
.16716
.20724
.25275
.29734
.31624

perm.
.04348
.11635
.12999
.17044
.21624
.26685
.31101
.31855

boot.
.04475
.11923
.1623
.20032
.2387
.29801
.33996
.33816

Wilcox.
.03514
.09976
.14516
.17841
.21454
.25779
.30015
.31804

sign
.0556
.13014
.14619
.18024
.21737
.26114
.30344
.31802

Table 3: RMS Error of all five tests with themselves
(lower is better). Best per bin in bold face.

4. DISCUSSION

subset T  . The Wilcoxon test turns out to be the most stable of all for very small p-values, and generally more so than
the permutation test. The t-test is the most stable overall.
Indeed, if we compute the diﬀerence between the actual and
nominal error rates we ﬁnd that the Wilcoxon test is the one
that best tracks the signiﬁcance level and therefore seems to
be the most exact (RMSE 0.1146), followed by the bootstrap, t-test, sign and permutation tests (RMSEs 0.1148,
0.1153, 0.1153 and 0.1155). This is particularly interesting
for the bootstrap test: it provides the most signiﬁcant results and the actual error rate is still lower than expected.
In summary, a researcher that wants to maximize the
number of signiﬁcant results may use the more powerful
bootstrap test and still be safe in the usual scenario. Researchers that want to maximize safety may use the t-test,
and researchers that want to be able to trust the signiﬁcance level may proceed with the Wilcoxon test. For large
meta-analysis studies we encourage the use of the t-test and
Wilcoxon test because they are far less computationally expensive and show near-optimal behavior. Unlike previous
work concluded, our results suggest that in practice the permutation test is not optimal under any criterion. Further
analysis with varied test collections and eﬀectiveness measures should be conducted to clarify this matter, besides
devising methods to better approximate what actual Type I
error rates we have in IR evaluation. We further support
the argument of discontinuing the sign test.

Zobel [7] compared the t-test, Wilcoxon test and ANOVA
at α = 0.05, though with only one random split in 25-25
topics. He found lower error rates with the t-test than with
the Wilcoxon test, and generally lower than the nominal
0.05 level. Given that the latter showed higher power and
has more relaxed assumptions, he recommended it over the
t-test. Sanderson and Zobel [3] ran a larger study also with
splits of up to 25-25 topics. They found that the sign test has
higher error rates than the Wilcoxon test, which has itself
higher error rates than the t-test. They also suggested that
the actual error rate is below the nominal 0.05 level when
using 50 topic sets. Voorhees [6] also observed error rates
below the nominal 0.05 level for the t-test, but more unstable
eﬀectiveness measures resulted in higher rates. Cormack and
Lynam [1] used 124-124 topic splits and various signiﬁcance
levels. They found the Wilcoxon test more powerful than the
t-test and sign test; and the t-test safer than the Wilcoxon
and sign test. Sakai [2] proposed the bootstrap method for
IR evaluation, but did not compare it with other tests.
Smucker et al. [4] compared the same ﬁve tests we study
in this paper, arguing that the t-test, permutation and bootstrap tests largely agree with each other. Nonetheless, they
report RMS Errors among their p-values of roughly 0.01,
which is a large 20% for p-values of 0.05. Based on the argument that the permutation test is theoretically exact, they
concluded that the Wilcoxon and sign tests are unreliable,
suggesting that they should be discontinued for IR evaluation. They ﬁnd the bootstrap test to be overly powerful,
and given the appealing theoretical optimality of the permutation test they propose its use over the others, though
the t-test admittedly performed very similarly. In a later
paper [5] they found that the tests tended to disagree with
smaller topic sets, though the t-test still showed acceptable
agreement with the permutation test, again assumed to be
optimal. The bootstrap test tended again to produce smaller
p-values, so authors recommend caution if using it.
In this paper we ran a large-scale study to revisit these
issues under diﬀerent optimality criteria. In terms of safety,
the t-test produced the smallest error rates across signiﬁcance levels, followed by the Wilcoxon test for low levels
and the permutation test for usual levels. In general, all
tests yielded error rates higher than expected for low signiﬁcance levels, but much lower for the usual levels. This
suggests that we are being too conservative when assessing
statistical signiﬁcance at α = 0.05; we expect 5% of our
signiﬁcant results to be wrong, but in practice only about
1.3% do indeed seem wrong. We must note though that this
global error rate, as the sum of minor and major errors, is
just an approximation of the true Type I error rates [1].
Table 3 shows the agreement of the ﬁve tests with themselves: p-values with topic subset T compared to those with

5. REFERENCES
[1] G. V. Cormack and T. R. Lynam. Validity and Power
of t-test for Comparing MAP and GMAP. In ACM
SIGIR, pages 753–754, 2007.
[2] T. Sakai. Evaluating Evaluation Metrics Based on the
Bootstrap. In ACM SIGIR, pages 525–532, 2006.
[3] M. Sanderson and J. Zobel. Information Retrieval
System Evaluation: Eﬀort, Sensitivity, and Reliability.
In ACM SIGIR, pages 162–169, 2005.
[4] M. D. Smucker, J. Allan, and B. Carterette. A
Comparison of Statistical Signiﬁcance Tests for
Information Retrieval Evaluation. In ACM CIKM,
pages 623–632, 2007.
[5] M. D. Smucker, J. Allan, and B. Carterette. Agreement
Among Statistical Signiﬁcance Tests for Information
Retrieval Evaluation at Varying Sample Sizes. In ACM
SIGIR, pages 630–631, 2009.
[6] E. M. Voorhees. Topic Set Size Redux. In ACM SIGIR,
pages 806–807, 2009.
[7] J. Zobel. How Reliable are the Results of Large-Scale
Information Retrieval Experiments? In ACM SIGIR,
pages 307–314, 1998.

928

