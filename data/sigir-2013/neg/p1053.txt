The Bag-of-Repeats
Representation of Documents
Matthias Gallé

Xerox Research Centre Europe

matthias.galle@xrce.xerox.com

ABSTRACT
n-gram representations of documents may improve over a
simple bag-of-word representation by relaxing the independence assumption of word and introducing context. However, this comes at a cost of adding features which are nondescriptive, and increasing the dimension of the vector space
model exponentially.
We present new representations that avoid both pitfalls.
They are based on sound theoretical notions of stringology,
and can be computed in optimal asymptotic time with algorithms using data structures from the suﬃx family. While
maximal repeats have been used in the past for similar tasks,
we show how another equivalence class of repeats – largestmaximal repeats – obtain similar or better results, with only
a fraction of the features. This class acts as a minimal generative basis of all repeated substrings. We also report their
use for topic modeling, showing easier to interpret models.

Categories and Subject Descriptors
H.3.1 [Information Storage and Retrieval]: Content
Analysis and Indexing

Keywords
document representation; stringology; maximal repeats

1.

INTRODUCTION

Documents are not per se mathematical objects. In order
to treat them like that, the ﬁrst step is deﬁne a mapping that
represents them as a data structure. For most type of documents, the most natural such data structure seems to be a
sequence. While this is standard in ﬁelds like bioinformatics, the very high dimensionality and lack of ﬂexibility makes
such an approach impopular for natural language applications. In these applications the most popular representation
is the vector space model, where a document d is mapped
into a vector v(d) ∈ RD . Normally, such a mapping proceeds
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

in two steps: at ﬁrst a set of deﬁned set of features are extracted from d, which are in second place weighted through
a scoring scheme. For text documents the standard is to use
the bag-of-words, where each dimension in the vector space
model represents one word. This standard approach of just
counting words (or unigrams) has some well-known shortcomings. It is a lossy representation and as such can map
diﬀerent documents into the same representation (as in: the
department chair couches oﬀers and the chair department
oﬀers couches [14] ).
While less problematic in query systems (queries are short
and modelling them as simple set of words is a good approximation of the real use), this may become an issue when
comparing full-length documents between them. Another
drawback of counting single words is that multi-words expressions (collocations) are missed. A document where “New
Haven” occurs is probably diﬀerent from one which contains
separate occurrences of the words “new” and “haven”.
Both examples are consequences of the independence assumption of the unigram model. A common way of bypassing these issue is by using a higher level language model
(n−grams), which however introduces other errors, such as
counting diﬀerently Sally has and Sally is. Any choice of
n will cut constituents that are of size n + 1, while adding
noise to constituents of size smaller than n. At the same
time the dimension of the vocabularies increases exponentially with n (which is bad for eﬃciency reasons) and the
vector becomes much sparser (which is bad for computing
similarities). In general, for a n−gram models with n > 3
this becomes so problematic that performance decreases considerably (although this is of course application dependent,
for string kernels for instance [8] this threshold is reached at
n = 5). The general solution to this is to combine several ngram models: this requires extensive cross-checking to ﬁnd
the right subset of n’s and increases even more the feature
space.
In this paper we propose to use repeats (exact substrings
that appear more than once in the documents) as basic features and to represent documents by a bag-of-repeats. They
carry the same advantage as n-grams of providing an higher
context for each term, while avoiding having to specify the
value of n. Moreover, the length of a repeat is unbounded
(except by the length of the document) and this can therefore be considered as an inﬁnite-gram model. The length
of each repeat is simply determined by how long the substring is repeated. The use of maximal repeats has already
been proposed in the past for the task of classiﬁcation [11]
and clustering for languages without explicit word bound-

1053

aries [9]. They permit to control the quadratic explosion of
repeats with an equivalence class of repeats that contains
all of them. We improve upon this by using largest-maximal
repeats, another class of repeats that acts as a basis (in the
algebraic sense of generator) of all substrings. We report
experiments classifying and clustering text documents from
various domains (20 newsgroups, TDT and HR forms) which
show improved performance. Moreover, largest-maximal repeats permit easier to interpret topics, addressing one of the
major disadvantages of probabilistic topic models.

2.

Dear
Dear
Dear
Dear
Dear
Dear

2.1 Maximal Classes of Repeats
Maximal repeats appear in the literature [7] as a compact
representation of all repeats. Diﬀerently from normal repeats, the number of maximal repeats inside a sequence is
linear in n and it is trivial to recover all repeats from the set
of maximal repeats.
A maximal repeat is a repeat such that if it would be
extended to its left or right it would lose some of its occurrences. Formally:
Definition 1 (Maximal Repeats). The set of maximal repeats (MR) is the set of repeats that are left- and
right-context diverse:
MR(s) = {r ∈ R(s) : lcds (r, poss (r)) ∧ rcds (r, poss (r))}
The property of maximality is strongly related to the context
of a repeat. If the symbol to the left (right) of any occurrence
of r is always the same, then r is not a maximal repeat
because it could be extended to its left (right) without losing
any occurrence.
As running example we will suppose the set of documents
as shown in Table 1. For simplicity, we assume that the
text represented by the dots there is unique. As in the rest
of our experiments, we take the words to be symbols (not
the characters). There are 32 repeats, but only 4 maximal
ones (Dear valued customer, Dear valued customer X, Dear valued customer Y, Dear valued customer Y with respect to ). The

customer
customer
customer
customer
customer
customer

X
X
Y
Y
Y
Y

...
...
...
...
with respect to . . .
with respect to . . .

Table 1: Running example. Each line is a diﬀerent
document, and the dots represent unique text.
class
R
MR
LMR

DEFINITIONS AND METHOD

A sequence s is a concatenation of symbols s[1] . . . s[n],
which s[i] ∈ Σ, the alphabet. The length of s, |s| is the numbers of symbols, which we will generally denote by n. Without loss of generality, we will assume that s starts and ends
with a special symbol, not appearing elsewhere. Another
sequence r is said to occur in s at position k if r[i] = s[k + i]
for i = 1 . . . |r|. The set of occurrences of r in s is denoted by
poss (r). If poss (r) ≥ 2, r is called a repeat of s. The set of
all repeats of s is denoted by R(s). While in general we will
work with a set of sequences, this can always be represented
as one big sequence concatenating all individual sequences
intercalating unique separators.
The left (right) context of a repeat r in s for a subset
of its occurrences p is deﬁned as lcs (r, p) = {s[i − 1] : i ∈
p} (rcs (r, p) = {s[i + |r|] : i ∈ p}). For one occurrence o
of r, we deﬁne it to be left-context unique if it is the only
occurrence with this left-context: lcus (r, o) = (lcs (r, {o}) ⊆
lcs (r, poss (r) \ {o})). For a given r, a subset p ⊆ poss (r)
is said to be left-context diverse (lcds (r, p)) if |lcs (r, p)| ≥ 2.
Note that a subset of occurrences can be left-context diverse
and not having any occurrence that is left-context unique.
We will also use the respective deﬁnitions for right-context
diverse (rcds (r, p)) and right-context unique (rcus (r, p)).

valued
valued
valued
valued
valued
valued

ηX (n)
Θ(n2 )
Θ(n)
Θ(n)

ωX (n)
Θ(n2 )
Θ(n2 )
3
Ω(n 2 )

Table 2: Upper and lower bounds for the number of
normal, maximal, largest-maximal; and for the total
number of occurrences of these classes.
repeat with respect for instance is not maximal, as it can be
extended to with respect to without reducing its number of
occurrences.
However, suppose that all occurrences of a repeat r are
covered not by one, but by two or more other repeats r1 . . . r .
In this case r is still a maximal repeat, even if can be considered as redundant by using repeats r1 . . . r . In our example,
the repeat Dear valued customer can be covered by combining the occurrences of Dear valued customer X and Dear
valued customer Y. Such is the intuition captured by the
following deﬁnition:
Definition 2 (Largest-maximal Repeats). The set
of largest-maximal repeats (LMR) is the set of repeats which
have at least one occurrence right- and left-context unique :
LMR(s) = {r ∈ R(s) : ∃o ∈ poss (r) : lcus (r, o) ∧ rcus (r, o)}
Any such occurrence is called a largest-maximal one.
In our example, Dear valued customer is not a LMR although the remaining three maximal repeats do belong to
this set. Largest-maximal repeats cover the whole sequence
(except for unique symbols), which is not necessarily true
for more restrictive classes of repeats (like super-maximal
repeats [7]). But they do it in a less redundant way than
maximal repeats. Even more, as in the case of maximal repeats, all occurrences of all repeats can be recovered from the
set of largest-maximal occurrences. This condition does not
hold any more for any strict subset. In this sense, largestmaximal repeat form a minimal basis in the algebraic sense,
as they can been seen as a smallest generator of all repeats.
Table 2 gives an overview on the known bounds for these
classes of repeats. ηX (n) is the upper bound on the number
of members of class X: maxs:|s|=n {|X(s)|} where X stands
for one of R, MR, LMR. ωX (n) is the
P total number of occurrences of repeats in X: maxs:|s|=n{ r∈X(s) |poss (r)|} [5].
Of course ωLMR (n) is upper-bounded by O(n2 ). It is however an open problem if this bound is tight.
There are well-known algorithms for computing maximal
repeats in linear time, using a data structure from the suﬃx
family (like suﬃx tree or suﬃx array) [12] and Gusﬁeld [7]
outlines an algorithm to compute largest-maximal repeat.
For our experiments we used in all cases a linear implementation using the suﬃx array which processes roughly 500K-

1054

MR
LMR

P
0.4513
0.4551

20NG
R
0.5169
0.5296

F1
0.4813
0.4891

P
0.2632
0.2721

HR forms
R
F1
0.3199
0.2879
0.3312 0.2985

Table 3: Clustering results on 20NG and HR forms

Figure 1: Regularization parameter versus mean accuracy for 5-fold cross validation on HR Forms
Figure 2: Clustering Performance (F1 ) on TDT5

1.5M words per second on a commodity machine, depending
on the type of sequence.

3.

EXPERIMENTS

We ﬁrst evaluated if using repeats as document representation instead of unigram does impact information retrieval tasks. For this, we used three types of datasets.
The ﬁrst two are well-understood classical text collections,
whose classiﬁcation is rather easy (this is, where the standard performance is close to 90%). In this group we used the
20 newsgroup dataset1 (18 774 posts from 20 newsgroups)
and the TDT5 news articles collection2 (6 496 news articles labeled with 126 stories). The other group consists
of a non-public collection of scanned and OCR’ed Human
Resources (HR) forms of a multinational conglomerate, of
4 615 forms belonging to 34 categories. In all three cases,
we learned a multi-label classiﬁer through logistic regression (2 -normalized, using the liblinear software3 ). We report the results in Fig. 1 for the HR forms. The results on
the other two datasets are similar, although the high performance achieved with the baselines makes the diﬀerences
less substantial. As evaluation measure, we report the mean
accuracy on 5-fold cross-validation (y-axis) for diﬀerent values of c, the regularization parameter in logistic regression
(x). Note that – except for low values of c – the representation of maximal and largest-maximal repeats perform better
than any of the n-grams approaches. Statistical signiﬁcance
tests (Wilcoxon signed-rank) on the performances between
any of the baselines (n-grams) and the repeat-based methods gave a p-value less than 5% for all values of c ≥ 0.1. Also
note that while there where 527K features in the combined
uni-, bi- and trigram model; there were only 112K maximal repeats and 80K largest-maximal ones (6.5 times less).
Adding n-grams for higher values of n slowly approaches the
performance of the maximal repeats, which is expected as
the learning process of the classiﬁers starts ﬁltering those
n-grams which are meaningful.
The better performance of maximal repeats conﬁrms existing results [11], but the comparable results of largestmaximal repeats with respect to maximal ones are new.
1

http://qwone.com/~jason/20Newsgroups/
www.ldc.upenn.edu/Projects/TDT5
3
http://www.csie.ntu.edu.tw/~cjlin/liblinear/
2

Remember that the latter is a strict subset of the former
and if provided enough test data the supervised algorithm
could learn to dismiss redundant features. We therefore performed an unsupervised experiment and clustered all three
datasets. For the HR forms and the 20NG, we used k-means,
reporting the average over 50 runs. We sampled a subset
of 2 000 documents in the case of 20NG (diﬀerent for each
run). For TDT5 it is known that cluster methods where the
number of clusters is ﬁxed perform poorly [6] and we used
the simple incremental clustering algorithm [13], that takes
as parameter a threshold on the similarity and which is a
standard method for TDT tasks. Instead of the number of
clusters, this algorithms takes as input a threshold on the
cosine similarity for a document to belong to a cluster. The
order of the documents is important here, so we report the
average on 100 runs of random permutations. As measure
we use precision and recall of the best mapping between proposed clusters and ground-truth clusters. In Table 3 are the
results for 20NG and the HR Forms (using k-means) and
Fig. 2 shows the evolution of the F1 measure with changing
threshold parameter for TDT5. Again, all the results are
statistically signiﬁcant (Wilcoxon signed-rank). Note that
in all cases the results using LMR are better, while at the
same time using less features (77% for 20NG and 73% for
HR, compared to MR)
Because they tend to be larger than simple words, we tried
how the expressiveness of repeats could be used directly,
instead of indirectly as in the classiﬁcation and clustering
task we studied until now. Probabilistic Topic Models are
one of the most popular unsupervised methods to explore a
document collection. One of the main drawbacks of these
methods is their interpretability [2]. The standard way is to
show the k most probable words for each topic, but this unigram representation may be redundant or not informative
enough. Instead of using word counts as input for them, we
provided repeat counts and used LDA [3], the most popular of the existing probabilistic topic models. The vanillaLDA model tends to favor very frequent features over less
frequent ones, a known phenomenon that makes stop-word
removal crucial, for example. Because shorter repeats tend
to appear more frequent than longer, this therefore favors

1055

west germany
east germany
west german
east german
german
britain
mrs. thatcher
berlin
east germans
kohl
sweden
west berlin
germany
united germany
kephart
unification
british
norway
ec
allies

market
dollar
points
yen
stock market
stocks
investors
million shares
trading
traders
analysts
fell
rose
french francs
stock prices
italian lire
news
dealers
today
wall street

soviet
soviets
soviet union
united states
reagan
moscow
summit
talks
baker
treaty
president reagan
countries
negotiations
trip
nations
president bush
state department
military
missiles
leaders

bush
congress
budget
president
president bush
administration
deficit
bush administration
nation
america
spending
package
cuts
billion
proposal
programs
tax
social security
cut
defense

providing a potential inﬁnite context. Moreover, by using
a less well known class of repeats that acts as minimal basis of all repeated substrings we show how to obtain better
performance for clustering and comparable one for classiﬁcation tasks than using maximal repeats and using less
features. Finally, their use for topic modeling permits in
a straightforward way to obtain topics that are easier to
interpret. Right now, for clustering we used a standard TFIDF weighting schema, which does not take into account the
length of the repeats: we plan to address this in future work.

Table 4: 20 most probable words for a selection of
topics, learned on a vanilla-LDA inferred with 50
topics, using Gibbs sampling (1000 iterations).

[1] A. Apostolico, O. Denas, and A. Dress. Eﬃcient tools
for comparative substring analysis. Journal of
biotechnology, 149(3):120–6, Sept. 2010.
[2] D. Blei. Probabilistic Topic Models. CACM,
55(4):77–84, Nov. 2012.
[3] D. M. Blei and M. I. Jordan. Modeling annotated
data. In SIGIR, pages 127–134, New York, NY, USA,
2003. ACM.
[4] D. M. Blei and J. D. Laﬀerty. Visualizing Topics with
Multi-Word Expressions. arXiv, 2009.
[5] M. Gallé. Searching for Compact Hierarchical
Structures in DNA by means of the Smallest Grammar
Problem. Université de Rennes 1, February 2011.
[6] M. Gallé and J.-M. Renders. Full and semi-batch
clustering of news articles with Star-EM. In ECIR.
Springer, 2012.
[7] D. Gusﬁeld. Algorithms on Strings, Trees, and
Sequences: Computer Science and Computational
Biology. Cambridge University Press, January 1997.
[8] H. Lodhi, C. Saunders, J. Shawe-Taylor,
N. Cristianini, and C. Watkins. Text Classiﬁcation
using String Kernels. Journal of Machine Learning
Research, 2:419–444, 2002.
[9] T. Masada, A. Takasu, Y. Shibata, and K. Oguri.
Clustering documents with maximal substrings. In
Enterprise Information Systems, volume 102, pages
19–34. Springer Berlin Heidelberg, 2012.
[10] J. Nicolas, C. Rousseau, A. Siegel, P. Siegel, F. Coste,
P. Durand, S. Tempel, A.-S. Valin, and F. Mahé.
Modeling local repeats on genomic sequences.
Technical report, INRIA, 2008.
[11] D. Okanohara and J.-I. Tsujii. Text Categorization
with All Substring Features. In SDM, pages 838–846,
2009.
[12] S. J. Puglisi, W. F. Smyth, and M. Yusufu. Fast
optimal algorithms for computing all the repeats in a
string. In PSC, pages 161–169, 2008.
[13] C. van Rijsbergen. Information Retrieval.
Butterworths, 1979.
[14] H. M. Wallach. Topic Modeling : Beyond
Bag-of-Words. In ICML, pages 977 – 984. ACM, 2006.
[15] X. Wang, A. McCallum, and X. Wei. Topical
N-Grams: Phrase and Topic Discovery, with an
Application to Information Retrieval. In ICDM, pages
697–702. IEEE, Oct. 2007.
[16] J. G. Wolﬀ. Learning syntax and meanings through
optimization and distributional analysis. Categories
and processes in language acquisition, Jan 1988.

repeated single words. To balance this phenomena, and also
to reduce any bias due to over-counting the same words, we
counted only largest-maximal occurrences (see Def. 2). As
we said there, these occurrences are the minimal necessary
information from which the frequency information for all repeats can be deduced. In Fig. 4 we report the most probable
repeats for some topics, inferred on the AP dataset [3].

4.

RELATED WORK

Repetitions have a long history in linguistics [16] and it is
common to eliminate unique words in practical applications
The analysis of genetic sequences lacks of a precise deﬁnition of words, and it is there where notions of repeats to
represent documents have been used the most. The growth
of available sequences has popularized alignment-free sequence comparison methods: Apostolico [1] in particular
maps the maximal repeats inside one sequence to a vectorspace model, and compares those vectorial representations,
although the repeats there are calculated intra-document/
sequence, while we propose to calculate them inter-document,
taking into consideration the whole collection.
The use of MR for natural language documents has been
particular useful in languages lacking a clear word segmentation [9], although the improvement they oﬀer for classiﬁcation has already been reported previously [11]. LMR
are called near-supermaximal repeats by Gusﬁeld [7] but
there they are used only to facilitate the deﬁnition of supermaximal repeats. Recently, this class of repeats were
re-discovered [10] and successfully applied to the automatic
detection of certain genomic structures.
With respect to improving the interpretability of probabilistic topic models with phrases, good results have been
obtained with the Turbotopics [4] where the representative
multi-words expressions are computed a-posterior. Two variants of the vanilla-LDA have been proposed that integrate
an n-gram modeling: [14] combines an hierarchical bigram
model with LDA, while [15] extends LDA by modeling the
generation of documents as a process of either generating a
unigram or an n−gram. Of course, this approaches heritates
all the drawback of using n-grams for document modeling.

5.

CONCLUSIONS

We presented a way of modeling documents ﬁtting the
vector space model that generalizes the n-gram approach by

6. REFERENCES

1056

