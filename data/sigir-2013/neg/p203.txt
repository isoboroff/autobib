An Unsupervised Topic Segmentation Model Incorporating
Word Order∗
Shoaib Jameel and Wai Lam
Department of Systems Engineering and Engineering Management,
The Chinese University of Hong Kong,
Hong Kong.

{msjameel, wlam}@se.cuhk.edu.hk
ABSTRACT

widely used to ﬁnd topics in a document collection. Typically, a topic is a probability distribution over words. But
the LDA model has been criticized for its bag-of-words assumption [32] as the model does not consider the structural
information inherent in the text which could help tap extra
knowledge from the text. It is well known that the bagof-words assumption is mainly a simplifying assumption to
reduce the complexity of the model [24].
Some recent topic models have demonstrated better qualitative and quantitative performance when the bag-of-words
assumption is relaxed [18], [20], [1], and [23]. Maintaining the word order during the processing of documents introduces some computational overhead, but it allows us to
achieve what the bag-of-words models cannot do in general
[15]. In order to address the shortcoming inherent in the LDA
model, the authors in [39] introduced the Topical N-gram
model (TNG) to ﬁnd n-gram words in topics. By n-gram we
mean a word can be a unigram, a bigram, a trigram word,
etc. The TNG model has the ability to decide whether to
form a unigram or a bigram during the topic discovery process. The TNG model mainly extends the LDA Collocation
model [14] (LDACOL) and the Bigram Topic Model [35] (BTM).
All these models advocate that the word order in a document is essential. But one shortcoming of these models is
that they lack the ability to consider the document’s structure such as paragraphs and sentences. Thus they cannot
segment a document into coherent topics. This sometimes
becomes essential in tasks such as tackling the word sense
disambiguation problem as shown in [15], segmenting news
articles and ﬁnding topics in each segment [30], topic detection and tracking [40], and a plethora of other tasks which
motivate us to explore deeper into the topic segmentation
model with n-gram topic word discovery.
We propose a new unsupervised topic discovery model,
called NTSeg, for a collection of text documents. NTSeg maintains the segment structure of the document such as paragraphs and sentences. In addition, it preserves the word order in the document. NTSeg can help capture topical changes
in the document from one segment to another. As a result,
it can generate two levels of topics of diﬀerent granularity,
namely, segment-topics and word-topics. In addition, it can
generate n-gram words in each word-topic. We also conduct extensive experiments on publicly available datasets to
demonstrate the superiority of NTSeg in comparison to the
state-of-the-art models in solving several text mining tasks
such as the ability to support ﬁne grained topics with ngram words in the correlation graph, the ability to segment

We present a new unsupervised topic discovery model for
a collection of text documents. In contrast to the majority of the state-of-the-art topic models, our model does not
break the document’s structure such as paragraphs and sentences. In addition, it preserves word order in the document.
As a result, it can generate two levels of topics of diﬀerent granularity, namely, segment-topics and word-topics. In
addition, it can generate n-gram words in each topic. We
also develop an approximate inference scheme using Gibbs
sampling method. We conduct extensive experiments using
publicly available data from diﬀerent collections and show
that our model improves the quality of several text mining
tasks such as the ability to support ﬁne grained topics with
n-gram words in the correlation graph, the ability to segment a document into topically coherent sections, document
classiﬁcation, and document likelihood estimation.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval - Clustering

Keywords
Topic Modeling, Topic Segmentation, N-gram words, Gibbs
Sampling, Document Classiﬁcation

1. INTRODUCTION
Simplicity may not always lead to greatness! Topic models such as Latent Dirichlet Allocation (LDA) [5] have been
∗The work described in this paper is substantially supported by
grants from the Research Grant Council of the Hong Kong Special
Administrative Region, China (Project Code: CUHK413510) and
the Direct Grant of the Faculty of Engineering, CUHK (Project
Codes: 2050476 and 2050522). This work is also aﬃliated with
the CUHK MoE-Microsoft Key Laboratory of Human-Centric
Computing and Interface Technologies. The authors would like to
thank Xiaojun Qian for his help with the experiments and some
discussions related to the technical content in the paper.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2034-4/13/07 ...$15.00.

203

tures correlation between word-topics and document-topics
(or super-topics). The LDCC model is a hierarchical topic
model where unigram words are assigned to the word-topics
and paragraphs are assigned to the document-topics. The
model can also ﬁnd correlations between the word-topics and
the document-topics. In [3], the authors proposed correlated
topic model (CTM) that can capture the evolution of topics
over time without considering the word order. In [22], the
authors proposed Pachinko Allocation Model (PAM) where
the concept of topic is extended to not only including distributions over words, but also distributions over topics. This
model assumes the structure of an arbitrary DAG in which
each leaf is associated with a word and each non-leaf node
is a distribution over its children. The interior nodes are
distributions over topics called super-topics. Recently, in
[6], the authors presented a new model to ﬁnd correlation
among topics in a corpus using the Generalized Dirichlet
distribution model instead of the Dirichlet distribution.
One similarity between our work and the topic correlation models is that our work also introduces two levels of
topic assignments. For example, word-topics and documenttopics as described in Shaﬁei et al. [31] share the same notion as the word-topics and segment-topics described in our
proposed approach. We adopt the name segment-topics because known text segments such as paragraphs or sentences
are assigned to the segment-topics. However, all the correlation topic models mentioned above assume exchangeability
among the words in a document. The importance of capturing n-gram words is that it reduces the ambiguity in the
mind of the reader as to what the word is referring to in the
correlation graph. For example, presenting the word “networks” in a topic is ambiguous especially for a person who
is not a domain expert. In contrast, showing the word “neural networks” in a topic signiﬁcantly reduces ambiguities. In
addition, all the topic correlation models mentioned above
cannot conduct topic segmentation.
Topic segmentation: In [16], the author presented the
TextTiling algorithm for segmenting a textual discourse
into coherent segments. TextTiling is a domain-independent
text segmentation technique that assigns a score to each
segment boundary candidate based on a cosine similarity
measure between chunks of words appearing to the left and
right of the candidate. Then the segment boundaries are
placed at the locations of valleys under this measure, and
are then adjusted to coincide with known paragraph boundaries. TextTiling algorithm does not ﬁnd topics. In [12],
the authors presented a topic segmentation based approach
which takes into account the lexical cohesion. The authors
modeled lexical cohesion in a Bayesian context and obtained
signiﬁcant improvement against the existing models. They
assumed that words are drawn from a multinomial language
model. Our work is signiﬁcantly diﬀerent from the above two
models in that ours is a domain-independent topic segmentation method based on a topic model. Apart from ﬁnding a
low-dimensional representation of the original vector space,
our model also segments a document and ﬁnds n-gram words
in each topic.
In [25], the authors presented a method for topic segmentation based on topic modeling where the authors used the
LDA model to segment texts into coherent topics that assume
exchangeability among the words in a document. In [4], the
authors described another topic segmentation method by
unifying the segmenting hidden Markov model in [27] and

a document into topically coherent sections, document classiﬁcation, and document modeling.

2.

RELATED WORK

Topic models: Some previous works have considered the
order of the words in a document during the topic discovery process. For example, in [35], the author described the
Bigram Topic Model (BTM) which incorporates the hierarchical Dirichlet language model into the unigram based topic
model in order to capture the dependencies between the
words in sequence. One drawback of this model is that it
only generates bigram words in a topic. This limitation was
addressed in the LDA Collocation model (LDACOL) [14] which
introduces a new set of status variables in the model called
the bigram status variable. This variable indicates whether
two consecutive words form a bigram or not. One limitation of the LDACOL model is that it cannot decide whether
to form a unigram or a bigram for the same two consecutive words depending on their nearby context. Another
issue with the model is that only the ﬁrst term in a bigram
has a topic assignment. One needs to make some assumptions in order to give the topic assignment to every term
in a bigram [38], and [37]. The limitations inherent in the
LDACOL model have been addressed in the Topical N-gram
model, TNG [39]. The TNG model allows for consecutive words
in a topic model to depend on each other in that they can
be selected either to come from a unigram term distribution or from a bigram distribution. However, one limitation
of the Topical N-gram model is that it cannot segment a
document into topically coherent units, known as topic segmentation task. There are also other limitations and they
have been addressed recently in [23] where the authors gave
the same topic assignment to every term in a phrase and the
words share the same probability mass in the phrase by introducing the hierarchical Pitman-Yor processes (HPYP) [34]
in their model named PDLDA. The PDLDA model also cannot
perform topic segmentation. Incorporating the HPYP model
in our NTSeg model to capture phrasal terms would make our
model NTSeg overly complex leading to ineﬃcient processing
of large text corpora.
Recently in [26] the authors proposed a topic model where
n-gram words are viewed as random variables. The authors
manually generated n-gram words from the dataset and considered those n-grams as part of the vocabulary. This model
focuses on handling discussions or debates. In [19], the author proposed another topic n-gram model which mines sentiments from text corpora. Wang et al. [36] proposed an
n-gram topic model for academic retrieval. They apply an
online inference algorithm and ﬁnd unigrams and bigrams
in a topic. In [42], the authors presented an n-gram based
news thread extraction model that uses the TNG model with
a background distribution. A method which has adopted a
diﬀerent approach to n-gram topic modeling is [20]. It combines the paradigms of frequent pattern mining and topic
modeling. All the topic models proposed above do not employ topic segmentation. In [41], the authors proposed an
Auto Topic Number LDA (ATNLDA) model for topic segmentation and apply the model on stem cell research literature. This model can automatically calculate the optimal
topic number. A diﬀerence between ATNLDA and ours is that
ATNLDA does not consider the word order.
Topic Correlations: Shaﬁei et al. in [31] described a latent
Dirichlet co-clustering method, known as LDCC, which cap-

204

Ω

ρ

α
K

π (d)

τ (d)

(d)

(d)

ys

ys−1
(d)

cs−1
(d)

cs

(d)

(d)

θs

θs−1

(d)

(d)

zs−1,i−1

(d)

(d)

(d)

(d)

(d)

ws,i−1

zs,i+1
(d)

xs,i
ws−1,i+1

(d)

zs,i

zs,i−1

xs−1,i+1

ws−1,i

(d)

(d)

zs−1,i+1
(d)

(d)

xs−1,i

ws−1,i−1

(d)

zs−1,i

xs,i+1
(d)

(d)

ws,i

ws,i+1

M

φ

β

ψ

Z

ZV

γ

σ

δ
ZV

Figure 1: Our proposed model NTSeg in plate notation.

for a faster posterior inference. They employ a hierarchical Pitman-Yor processes to handle hierarchical modeling,
which our model does not incorporate. In [33], the authors
presented a topic segmentation model which does not ﬁnd
topics in a segment. In contrast to the above models, our
model does not assume exchangeability among the words
in a document. In [8], the authors proposed a subsequence
based topic segmentation approach which uses a suﬃx tree
model for representing text and measures coherence between
sentences based on subsequence. Their model maintains the
order of the words in each segment, but the model does not
ﬁnd collocations in that text segment.

the aspect model in [17]. Recently, in [30], the authors presented TopicTiling based on LDA. Their algorithm is very
similar to the TextTiling algorithm, and segments documents using the LDA topic model. Also, in [29], the authors
presented methods in which topic models can help segmentation based methods by extending their own TopicTiling
model. Similarly, in [32] the authors proposed a topic segmentation based topic model, known as LDSEG, where they
assumed that the word order is not important. The authors introduced the notion of topic hierarchy where sentences are assigned to the document-topics and unigrams
are assigned to the word-topics. The LDSEG model represents documents as a distribution over document-topics or
super-topics in such a way that each segment is assigned a
super-topic or document-topic, which is then used to choose
the parameters of a document independent Dirichlet distribution from which the word-topics for the segment is
drawn. In order for consecutive segments to have similar
word-topic distributions, an additional binary variable per
segment encodes whether the document-topic is forced to
be the same as that of the previous segment. In [9], the
authors proposed a topic model based hierarchical segmentation approach where they assumed that the word order
within the segment is not important and apply variational
Bayesian Expectation-Maximization procedure for computing the posterior inference. This model has been designed for
segmenting the speech data. In [11], the authors proposed a
collapsed Gibbs sampler for the topic segmentation problem

3. OUR PROPOSED MODEL (NTSeg)
We depict our proposed NTSeg model in Figure 1 using a
graphical model in plate notation where shaded circles represent observed variables and unshaded ones are the latent
variables. Each document, in general, is organized as atomic
segments such as paragraphs or sentences. Our model preserves this structure. One characteristic of our model is
that a document comprises of several topically coherent segments. Another characteristic of our model is due to the
preservation of the ordering of the words it is able to capture
word-collocations. Thus our proposed model is no longer invariant to the reshuﬄing of the words in each segment.
For a properly written discourse comprehension, documents are generally composed of coherent segments which

205

removing the segmentation switch variable of NTSeg reduces
to the LDCC model. NTSeg has the ability to decide whether
to form a unigram or bigram based on context which the
LDSEG model cannot achieve.
The following generative process of our model, NTSeg,
helps better understand the graphical model shown in Figure 1:

are semantically linked to one another so that a reader could
relate the storyline as one moves forward in the discourse
[21]. Our model comprises of two levels of topic of diﬀerent granularity. One is the segment-topic to which atomic
segments in a document are assigned and the ordering of
segments as they appear in the document deﬁnes the topic
change-points in the document. The other is the wordtopic to which n-gram words in the segment are assigned.
Segment-topics come from a predeﬁned number of segmenttopics K. Each segment-topic comprises of a mixture of
several word-topics where the mixture coeﬃcients uniquely
specify the segment-topic. Word-topics come from a predeﬁned number of word-topics Z. In general, the number of
segment-topics will be less than the number of word-topics.
The reason is that the number of segments in a document is
less than compared to the number of words [31].
In the graphical model shown in Figure 1, M denotes the
number of documents in the collection and V denotes the
number of words in the vocabulary. In each atomic segment s, NTSeg ﬁnds n-gram words in a word-topic z. It
can also ﬁnd correlations between both kinds of topics i.e.
word-topics z and segment-topics y. The segments of each
document are assumed to follow a Markov structure on the
topic distributions of each segment. We assume that there
will be a high probability that the topic for the segment s in
the document will be the same as that of the segment s − 1.
(d)
A segment binary switching variable cs for the segmenttopic in the document d indicates whether there is a change
of topic between the segments. The states of the switching
variable correspond to the segmentation of the document
into coherent topical units. Apart from the segment switching binary variable, NTSeg also incorporates another random
variable known as the bigram status variable x which indicates the bigram status i.e. whether a word w at position i
(d)
in the segment s in the document d, denoted as wsi , forms
(d)
a bigram with the previous word ws,i−1 . The mechanism is
(d)

(d)

1. Draw φz from Dirichlet(β) for each word-topic z,
where φz is the word (unigrams only) distribution for
the word-topic z; β is the parameter of the Dirichlet
prior on the per-word-topic word (unigrams only) distribution
2. Draw ψzw from Beta(γ) for each word-topic z and
each word w, where ψzw is the Bernoulli distribution
for the bigram status variables for the word-topic z
and the word w; γ is the parameter of the Beta prior
3. Draw σzw from Dirichlet(δ) for each word-topic z,
and each word w, where σzw is the bigram word distribution for bigrams; δ is the parameter of the Dirichlet
prior on word-topic bigram word distribution
4. For each document d in the collection
(a) Draw τ (d) from Dirichlet(ρ), where τ (d) is the
mixing proportion of the segment-topics in the
document d; ρ is the parameter of the Dirichlet
prior on the segment-topics
(b) Draw π (d) from Beta(Ω), where π (d) deﬁnes the
parameter of the Bernoulli distribution for the
segment switch variable in the document d; Ω is
the parameter of the Beta prior
(c) For each segment s in the document d
(d)

i. Draw cs

from Bernoulli(π (d) )
(d)

ii. Draw the segment-topic ys for s from Multi(d)
(d)
(d)
nomial(τ (d) ) if cs = 0 else ys = ys−1 ,
(d)
where ys is the segment-topic that is assigned to the segment s in the document d
(d)
iii. Draw θs for s in the document d from Dirichlet(αy(d) z ); α is a K × Z matrix where each
s
row represents the mixing proportion of the
(d)
word-topics in a segment-topic; θs is the
mixing proportion of the word-topics in the
segment s in the document d
(d)
iv. For each of Ns words in the segment s in the
(d)
document d, where Ns denotes the number
of words in the segment s in the document d
(d)
A. Draw xsi from Bernoulli(ψz(d) w(d) ),

(d)

that if xsi = 1, then ws,i−1 and wsi form a bigram else
they do not.
It can be observed that the existing topical n-gram model
(TNG) [39], LDSEG, [32], and LDCC, [31] are special cases derived from our model. For example, consider only a segment, for instance segment s, in Figure 1. Removing the
segmentation scheme along with a set of arrows pointing
(d)
(d)
(d)
(d)
from zs,i−1 → zsi and xsi → zsi reduces to the TNG model.
One can observe that our model, NTSeg, has the capability
of deciding whether to generate a unigram or a bigram in
a topic and the topic assignment for the words in a bigram
are the same. This aspect diﬀerentiates NTSeg from TNG.
Similar to TNG, NTSeg assumes a ﬁrst order Markov assumption i.e. it is mainly a bigram model, but the basic generation process produces unigram or bigram words. However,
NTSeg has the ability to produce higher order n-grams (i.e.
n > 2) by concatenating consecutive n-grams (unigram or
bigram words) having the same topic and the bigram status
variable between them is 1. In this way, the words in the
n-gram share the same topic. This again contrasts NTSeg
from TNG where TNG analyzes each n-gram post hoc as if the
topic of the ﬁnal word in the n-gram was the topic assignment of the entire n-gram. But it violates the principle of
non-compositionality [23]. Removing the bag-of-words assumption in each segment of our proposed NTSeg model reduces to the LDSEG model. Relaxing both bag-of-words and

s,i−1

(d)

s,i−1

where xsi is the bigram status variable
(d)
(d)
between words ws,i−1 and wsi in the segment s of the document d
(d)
(d)
(d)
B. Draw zsi from Multinomial(θs ) if xsi =
(d)
(d)
(d)
0 else zsi = zs,i−1 , where zsi is the
(d)

word-topic assignment for the word wsi
in the segment s in the document d.
(d)
C. Draw wsi from Multinomial(σz(d) w(d) )
(d)

(d)

si

s,i−1

if xsi = 1 else draw wsi from Multinomial(φz(d) )
si

206

(d)

cs indicates whether there is a change in the segment-topic
between the segments s − 1 and s in the document d. If
(d)
(d)
(d)
cs = 1 then it means that ys = ys−1 i.e. segment-topic
does not change between the segments in the document d.
(d)
(d)
However, when cs = 0, then ys is drawn from a Multinomial distribution parameterized by τ (d) . The computation
(d) (d)
(d)
of the probability P (ys |cs , τ (d) , ys−1 ) is done based on
(d)
(d)
(d)
two conditions i.e. ρ(ys , ys−1 ) when cs = 1 or sampling
(d)
from Multinomial (τ (d) ) when cs = 0.
(d) (d)
(d)
The segment distribution P (ys |cs , τ (d) , ys−1 ) is not properly deﬁned for the ﬁrst segment of every document. There(d)
fore, cs = 1 is deﬁned for the ﬁrst segment which is drawn
(d)
from Multinomial(τ (d) ). Similarly we assume that xs1 is
observed and only unigram is allowed at the beginning of
every segment.

4.

(d)

×

(d)

(d)

(d)

(d)

(αy(d) z(d) + h (d) − 1) × (γx(d) + pz(d) w(d) x(d) − 1)
s
szsi
si
si
s,i−1 s,i−1 si
8 β (d) +n (d) (d) −1
w
z
w
>
(d)
si `
si
si ´
>
if xsi = 0
>
P
>
< Vv=1 βv +n (d) −1
δ

z

si

+m

v

−1

(d)
(d) (d)
(d)
w
w
w
z
>
si `
si
s,i−1 si ´
>
>
>
: PVv=1 δv +m (d)
(d) −1

(d)

(d) (d)
P (ys(d) , c(d)
s |z, y¬s , c¬s , w, x, α, β, γ, δ, ρ, Ω) ∝
8
(d)
(d)
(ρ (d) + b (d) − 1) × (αy(d) z(d) + h (d) − 1)×
>
>
s
ys
szsi
> ys
si
>
>
”
“
(d)
>
+Ω0
>
(d)
s ,0
< P κc(d)
if cs = 0
1
x=0 κcs ,x +Ω0 +Ω1
”
“
(d)
>
κc ,1 +Ω1
(d)
>
s
>
(α
(d) (d) + h (d) − 1) ×
P1
>
(d)
> ys zsi
szsi
>
x=0 κcs ,x +Ω0 +Ω1
>
:
(d)
(d)
(d)
if cs = 1 & s > 1 & ys = y(s−1)
(4)

Note that in our model the hyperparameter α captures the
relationships between the segment-topics and word-topics.
This hyperparameter must be estimated from the data. Although there are many ways to estimate this hyperparameter [31], we adopt the moment matching method which is
computationally less expensive [31], and [22]. Therefore at
each iteration of the Gibbs sampling (Line 37 of Algorithm
1), we update:
!2
(d)
X
(d)
h
1
sz
X
hsz
1
−λkz
(d)
λkz =
(5)ν kz = qk
Ns
qk s∈S Ns(d)
s∈Sk
k
(6)

(1)

(2)

Z
X

(7) λkz =
PZ

λkz (1 − λkz )
− 1 (8)
ν kz
!

log(λkz )
(9)
Z
−1
z=1
where Sk is the set of segments assigned to the segment-topic
k. qk is the number of segments assigned to the segmenttopic k. λkz and ν kz are the sample mean and sample variance, respectively, of the number of times the word-topic z
is assigned to the segment-topic k.
The posterior estimates for θ, φ, ψ, π, τ , σ are:

(d)
w¬si

deﬁnes all the words in the segment except
Note that
(d)
(d)
the current word wsi in segment s in the document d. z¬si
is the word-topic assignment for all other words except the
(d)
current word wsi . In Equations 3 and 4, nzw is the number
of times the word w is assigned to the word-topic z as a
unigram. mwvz is the number of times the word w appears
as a second word of a bigram with a previous word v and
both words in the bigram are assigned to the same wordtopic z. pzwt denotes the number of times the status variable
x = t (0 or 1) given the previous word w and the previous
(d)
word’s word-topic z. hsz is the number of times a word in
(d)
segment s of document d is assigned to word-topic z. κcs ,0
(d)

(d)

(3)

αkz ∝ λkz
(d) (d)
P (ys(d) , c(d)
s |z, y¬s , c¬s , w, x, α, β, γ, δ, ρ, Ω)

(d)

if xsi = 1 & zsi = zs,i−1

w
vz
s,i−1
si

POSTERIOR INFERENCE

(d)

(d)

(d)

The inference problem is related to computing the posterior probability of the hidden variables when the input parameters β, γ, δ, ρ, Ω and the observed variable w are given.
Also, an estimate of the α hyperparameter has to be made.
It can be shown that computing the exact inference in our
model is intractable. Hence, we need to resort to approximation techniques such as Gibbs sampling [7]. Adoption of
Bayesian methods results in some hidden parameters being
integrated out instead of being explicitly estimated. Assuming conjugate priors on the model parameters also eases the
inference algorithm signiﬁcantly. Algorithm 1 depicts the
Gibbs sampling used in our approximate inference for NTSeg
We need to compute the two conditional distributions:
P (zsi , xsi |z¬si , x¬si , w, c, y, x, α, β, γ, δ, ρ, Ω)

(d)

P (zsi , xsi |w, z¬si , x¬si , y, c, α, β, γ, δ, ρ, Ω) ∝

z=1

αkz = exp

(d)
θ̂s,yz
= PZ

(d)

αyz + hsz

z=1 (αyz

+

(d)
hsz )

(10)

βw + nzw
φ̂z,w = PV
v=1 (βv + nzv )
(11)

γx + pzwx
δw + mwvz
σ̂zw,v = PV
(γ
+
p
)
t
zwt
t=0
v=1 (δv + mwvz )
(12)
(13)

ψ̂zw,x = P1

(d)

and κcs ,1 is the number of times the switching variable cs
is set to 0 and 1 in the document d, respectively. ρy(d)
s
is the corresponding Dirichlet parameter for the segment(d)
(d)
topic ys . bk is the number of times a segment in the
(d)
document d has been assigned to the segment-topic k. y¬s
is the segment-topic assignments for all the segments except
the current segment s in the document d.
Beginning with the joint probability of a dataset, and using the chain rule, we obtain the conditional probabilities
conveniently. We obtain the following equations:

(d)
Ωr + κcs ,r
ρy + by
τ̂y(d) = PK
(d)
r=0 (Ωr + κcs ,r )
k=1 (ρk + bk )
(14)
(15)
The target distribution is the posterior distribution of the
word-topics, the segment-topics, the topic switching variables of the segments, and the bigram status variables. When
we use the Gibbs sampling technique, at each iteration, we

π̂r(d) = P1

207

sample from the conditional distribution of the word-topics
in a document conditioned on the word-topic assignments
for all other words except the current word (Line 23 in Algorithm 1). In addition, we also sample the bigram status
variable (Line 24). We sample from the conditional distribution of a segment-topic for a segment (Line 14) and also the
corresponding switching variable given the segment-topic assignments (Line 14).
At each iteration of the Gibbs sampling procedure, we
only sample a subset of the variables which are directly related to the conditional probability. We perform this step
repeatedly until we arrive at some approximation. A variable is sampled from the conditional distribution given that
the assignments for all other variables are known which is a
standard procedure in a Gibbs sampler. As the list of words
is being scanned along with the bigram status variables, the
sampler keeps track of any new segment being encountered.
For each new segment, the sampler decides about the topic
assignment of the segment i.e., whether it should assign the
current segment to the same topic as the previous segment
or a new segment-topic. If the segment has to be assigned to
a new segment-topic, the sampler estimates the probability
of assigning the segment to the segment-topic. These probabilities are computed from the conditional distribution for a
segment given all other topic assignments to every other segment and all words in the segment as depicted in Algorithm
1.

1
2

3

4

5
6

7
8
9
10
11
12
13

14

15
16

17

5.

EXPERIMENTS AND RESULTS

Evaluation of topic models is a challenging task. Simply
showing the highly probable n-gram words obtained from
each topic may not be able to portray the underlying strengths
or weaknesses of a topic model. Therefore, we evaluate our
model on several text mining tasks including the ability to
support ﬁne grained topics with n-gram words in the correlation graph, the ability to segment a document into topically coherent sections, document classiﬁcation, and document likelihood estimation.
In each experiment, we chose several existing closely related comparative methods for comparison purpose. We will
describe those comparative methods in the subsections that
follow. For our proposed framework, NTSeg, the segment
granularity is basically a paragraph because topical changes
typically occur at paragraph boundary and this strategy is
also used in [31]. Note that NTSeg can also work at the
granularity of a sentence which has also been used in one of
our experiments (refer Section 5.2). In our experiments, the
number of iterations for the Gibbs sampler is 1000 which
is the value of the M axIteration used in Algorithm 1. We
have chosen the following hyperparameter values β = 0.01,
γ = 0.1, δ = 0.1, Ω = 0.1, and ρ = 0.1. Other topic models
such as TNG, LDSEG etc, also assume ﬁxed hyperparameter
values. We did not perform any stemming, but removed
stopwords1 from the collection.

18

5.1 Correlation Graph

38

19
20
21
22
23

24

25
26
27
28
29
30
31
32
33
34
35
36
37
39

NTSeg produces two levels of topics, namely, segmenttopics and word-topics. A word-topic is comprised of ngrams. We show the correlation graph for the purpose of
depicting how our model ﬁnds correlations among various

40
41

1
http://jmlr.csail.mit.edu/papers/volume5/lewis04a/a11smart-stop-list/english.stop

208

Input : γ, δ, Z, K, ρ, Ω, β, Corpus, M axIteration
Output: N-gram words derived from the word-topic
assignments; assignments of segment-topics to
segments in documents; an estimate of α
Initialize count variables in Equations 3 and 4 to 0;
(d)
(d)
(d)
Initialize bk , κcs ,0 and κcs ,1 for all values of
k ∈ {1, · · · , K} in all documents;
(d)
Initialize hsz for all values of z ∈ {1, · · · , Z} in all
documents and their segments;
Initialize nzw and pzwt for all values of z ∈ {1, · · · , Z}
and for all words in the collection;
Initialize mwvz for all values of z ∈ {1, · · · , Z} and for
all bigrams in the collection;
Randomly initialize word-topic assignments,
segment-topic assignments, segment-topic switch
variables, and bigram status variables;
if performing parameter value estimation then
Initialize α using Equations 5, 6, 7, 8, 9;
end
for iter ← 1 to M axIteration do
foreach document d ∈ [1, M ] in the collection do
foreach segment s in the document d do
Exclude segment s and its assigned topic k
from the count variables;
(newk , newc ) ← sample new segment-topic
and segment switching variable for segment s
using Equation 4;
if (newc == 0) then
Assign newk as the new segment-topic
for segment s;
(d)
(d)
Update variables bk , and κcs ,0 using the
new segment-topic newk for segment s;
end
if (newc = 1) then
(d)
Update variable κcs ,1 ;
end
foreach word i in segment s in the
document d, according to order do
Exclude word i and its assigned
word-topic z from the count variables;
(newz , newx )← sample new word-topic
for word i and bigram status variable
using Equation 3;
if (newx ==0) then
Assign newz as the new word-topic;
(d)
Update nzw , hsz , pzw0 using
word-topic newz for word i;
end
if (newx ==1) then
(d)
Update hsz , pzw1 , mwvz for word i;
end
end
(d)
Update the posterior estimate for θs for
each segment using Equation 10;
end
Update the posterior estimates for π (d) and τ (d)
for each document using Equations 14, and 15;
end
if performing parameter value estimation then
Update α using Equations 5, 6, 7, 8, and 9;
end
end
Update the posterior estimates for φzw , ψzwt and σzw
using Equations 11,12, and 13,;
Algorithm 1: Inference algorithm for NTSeg.

Books Dataset

hiv
human immunodeficiency virus
aids
infected
infection

10 Segment-Topics (K)

Pk

0.325

0.320

0.310

0.310

women
confidence interval
risk
men
risk factors

0.320
0.315

0.315
20 40 60 80 100
Word-Topics (Z)
50 Segment-Topics (K)

Figure 2: Correlation identiﬁed by NTSeg between the wordtopics and the segment-topics on OHSUMED collection considering Z = 200 and K = 100. Each circle shows a segment-topic
and each box corresponds to a word-topic. We can notice that
a segment-topic can capture correlations between several wordtopics.

20 40 60 80 100
Word-Topics (Z)
10 Segment-Topics (K)

0.330
WinDiﬀ

0.350

Pk

0.325
0.320

0.340
0.330

0.315

segment-topics and word-topics. Details regarding constructing and interpreting such correlation graphs can be found in
[22], and [31].
We have used the OHSUMED2 collection to show the correlation graph. The collection is composed of 348,566 documents with 154,711 words in the vocabulary without stopwords.
We have experimented by varying both number of the
word-topics Z and the number of the segment-topics K. Z
was varied from 50 to 200 in steps of 50 whereas K was
varied from 50 to 150 in steps of 50. However, we did not
observe signiﬁcant diﬀerence in the quality of the results.
The resulting correlation graph is shown in Figure 2 which
is obtained by setting Z = 200 and K = 100. Due to space
constraint, we only show the graph obtained from our NTSeg
model. Note that other models such as PAM, LDCC, LDSEG, GDLDA [6] and CTM, only form unigrams in a topic leading to
ambiguous interpretation. For example, presenting the unigram “conﬁdence” will not be that insightful in a correlation
graph. In contrast, presenting the term “conﬁdence interval”
is more meaningful as shown in Figure 2.

WinDiﬀ

20 40 60 80 100
Word-Topics (Z)
30 Segment-Topics (K)

20 40 60 80 100
Word-Topics (Z)
50 Segment-Topics (K)

0.350

0.350

0.345

0.345

0.340
0.335

0.340
0.335
0.330

0.330

20 40 60 80 100
Word-Topics (Z)

20 40 60 80 100
Word-Topics (Z)

Lectures Dataset

Pk

10 Segment-Topics (K)

5.2 Topic Segmentation Experiment

30 Segment-Topics (K)
0.380

0.380
0.375
0.370
0.365
0.360
0.355

0.370
0.360
0.350
20 40 60 80 100
Word-Topics (Z)
10 Segment-Topics (K)

20 40 60 80 100
Word-Topics (Z)
50 Segment-Topics (K)

The purpose of this experiment is to show how well NTSeg
generates segmentation of documents corresponding to coherent topical units. The segmentation information is ob(d)
tained via the segmentation switch variable cs which gives
the segment topic change-points in the document. In our
problem setting we know the segment boundaries in advance such as paragraphs or sentences, but we do not know
the word and segment topics. Our purpose is thus to learn
the segment and word topics from the document collection.
The prediction output of the segment status variable will
deﬁne the segmentation of a document. To evaluate the
performance, we make use of the annotated segmentation
information. We use two standard metrics, namely, Pk and
WinDiﬀ which are widely used in the topic segmentation
literature [32]. As described in [32], Pk is deﬁned as the
probability that two segments drawn randomly from a document are incorrectly identiﬁed as belonging to the same
topic [2]. WinDiﬀ [28] moves a sliding window across the
text and counts the number of times the hypothesized and
reference segment boundaries are diﬀerent from within the
2

0.325

WinDiﬀ

infants
gestational age
minor
pregnant woman
preterm infants

medical
health
family physicians
primary care
family practice

0.330

Pk

method
results obtained
system
clinical laboratory
methods

0.330

0.460

0.380
Pk

0.375

WinDiﬀ

children
child
sexual abuse
adults
young children

Pk

vaccine
protection
antibody response
immunization
protective

30 Segment-Topics (K)

0.370
0.365

0.455
0.450
0.445

0.360

0.440

WinDiﬀ

20 40 60 80 100
Word-Topics (Z)
30 Segment-Topics (K)
0.460
0.455
0.450
0.445
0.440
0.435
0.430
20 40 60 80 100
Word-Topics (Z)

http://ir.ohsu.edu/ohsumed/ohsumed.html

209

20 40 60 80 100
Word-Topics (Z)
50 Segment-Topics (K)

WinDiﬀ

weeks
fetal
umbilical artery
fetal heart
adult

0.461
0.458
0.455
0.452
0.449
0.446
0.443
20 40 60 80 100
Word-Topics (Z)

Figure 3: Comparison of NTSeg (depicted by
marker)
marker) on topic segmenagainst TopicTiling (depicted by
tation task.

LDSEG
PAM
LDACOL
TNG
PDLDA
NTSeg

Precision
0.580
0.550
0.400
0.490
0.580
0.640

Recall
0.420
0.450
0.300
0.420
0.500
0.520

F-Measure
0.487
0.495
0.343
0.452
0.537
0.574

LDSEG
PAM
LDACOL
TNG
PDLDA
NTSeg

Table 1: Document classiﬁcation results for the Computer
Dataset of the 20 Newsgroups corpus.
LDSEG
PAM
LDACOL
TNG
PDLDA
NTSeg

Precision
0.440
0.500
0.420
0.560
0.580
0.620

Recall
0.400
0.330
0.370
0.470
0.510
0.560

Precision
0.390
0.540
0.550
0.550
0.590
0.620

Recall
0.320
0.490
0.410
0.450
0.410
0.570

F-Measure
0.352
0.514
0.470
0.495
0.484
0.594

Table 3: Document classiﬁcation results for the Politics Dataset
of the 20 Newsgroups corpus.

F-Measure
0.419
0.398
0.393
0.511
0.543
0.588

LDSEG
PAM
LDACOL
TNG
PDLDA
NTSeg

Precision
0.330
0.368
0.200
0.340
0.380
0.420

Recall
0.320
0.360
0.180
0.290
0.210
0.380

F-Measure
0.325
0.363
0.189
0.313
0.271
0.399

Table 2: Document classiﬁcation results for the Science Dataset
of the 20 Newsgroups corpus.

Table 4: Document classiﬁcation results for the Sports Dataset
of the 20 Newsgroups corpus.

window. The lower the values obtained for these two metrics, the better is the segmentation result.
We use two publicly available datasets that contain segment boundaries corresponding to the topic changes. The
ﬁrst dataset, called Lectures in our experiment, consists of
spoken lecture transcripts from an undergraduate physics
class and a graduate artiﬁcial intelligence class. The transcripts consist of a 90 minute lecture recording and have
500 to 700 sentences with about 9000 words. Note that here
the segment granularity is a sentence. More details about
this dataset can be obtained from [32]. Our second dataset,
called Books in our experiment, is the books3 dataset in
which each document is a chapter extracted from a medical
textbook.
We chose a recently proposed topic segmentation method
TopicTiling [30] which has outperformed many state-ofthe-art text segmentation models proposed in the literature
and chose the best performing variant of TopicTiling from
[30]. Note that TopicTiling only has the notion of wordtopics. For each of the segment and word-topics, we run the
Gibbs sampler ﬁve times and take the average of the Pk and
WinDiﬀ values at the end of the ﬁfth run.
We illustrate the segmentation results in Figure 3. From
the results, we note that our model performs extremely well
in both datasets compared to the state-of-the-art topic segmentation model. Using a two-tailed signiﬁcance test, our
results are statistically signiﬁcant with p < 0.05 against TopicTiling. In the Books dataset, NTSeg performs reasonably
better, but the improvement obtained is not very high considering both Pk and WinDiﬀ metrics. However, good improvement is obtained in the Lectures dataset using both
metrics.

the highest likelihood. Note that this procedure is also used
in [22].
We measure the classiﬁcation performance using precision,
recall and F-measure. The meaning of precision for a class
is the number of true positives divided by the total number
of documents predicted to that class. Recall is deﬁned as
the number of true positives divided by the total number
of elements that actually belong to that class in the gold
standard. F-measure is the harmonic mean of precision and
recall.
We use the 20 Newsgroups corpus4 and generated four
datasets. The ﬁrst dataset comprises of documents related
to computer technology (the “comp”directory in the dataset).
It is composed of several classes such as “graphics”, “windows”, “hardware”, etc. Each of these classes consists of
1000 documents. We split the documents in each of these
classes into 75% training and 25% test documents. For each
class, we trained and tested the model by varying the number of word-topics from 10 to 100 in steps of 10 and the
number of segment-topics from 10 to 50 in steps 20. We compute precision and recall using the test set for each class for
each word-topic and segment-topic values and then we compute the average result for one class across all word-topics
and segment-topics. Similarly, we follow the same precision
and recall computation for all classes. Finally we compute
the average over all precision and recall values for all the
classes. We then compute F-measure from the obtained precision and recall values. The experimental setup is similar
for the other three datasets, namely, “sci” (called Science
Dataset), “politics” (called Politics Dataset), and “sports”
(called Sports Dataset).
The comparative methods include LDSEG, PAM, LDACOL,
TNG, and PDLDA. All these models are described in Section 2.
Note that some of the comparative methods such as TNG,
PDLDA, and LDACOL have no notion of segment-topics.
The classiﬁcation performance results are presented in Tables 1, 2, 3 and 4. We can observe that in all the datasets
our model, NTSeg, has outperformed all the comparative
methods. Compared to all the comparative methods, our
results are also statistically signiﬁcant using the sign test
with p < 0.05. Gain obtained in the Computer and Science
datasets is more when compared to the gain in Sports and

5.3 Document Classification Experiment
We conduct document classiﬁcation experiment using topic
models. In the training phase, a topic model is learned for
each class using the set of training documents in that class.
In testing, to conduct document classiﬁcation for a testing
document, we compute the likelihood of the testing document against each trained topic model for each class. The
testing document is classiﬁed to the model that produces
3

4

http://groups.csail.mit.edu/rbg/code/bayesseg/

210

http://qwone.com/ejason/20Newsgroups/

NIPS Dataset

−8.700
−8.800

Log-Likelihood

Log-Likelihood

Log-Likelihood

−8.600

·106 50 Segment-Topics (K)

·106 30 Segment-Topics (K)

·106 10 Segment-Topics (K)
−8.600
−8.700
−8.800

50 100 150 200
Word-Topics (Z)

−8.600
−8.700
−8.800

50 100 150 200
Word-Topics (Z)

50 100 150 200
Word-Topics (Z)

OHSUMED Dataset

−3.250

·107 100 Segment-Topics (K)

−3.200
−3.250

200 300 400 500
Word-Topics (Z)

−3.150
Log-Likelihood

−3.200

−3.300

−3.150
Log-Likelihood

Log-Likelihood

−3.150

·10 50 Segment-Topics (K)
7

200 300 400 500
Word-Topics (Z)

·107 150 Segment-Topics (K)

−3.200
−3.250
−3.300

200 300 400 500
Word-Topics (Z)

Figure 4: Performance of NTSeg (depicted by
) in terms of generalizing on the new data. We can see that our model NTSeg
), PAM (depicted by
), LDACOL (depicted by
generalizes better than other comparative methods which are: LDSEG (depicted by
), TNG (depicted by
), and PDLDA (depicted by
).

Politics datasets. PDLDA also proved to be a better model in
comparison to the other comparative methods.

In order to calculate the likelihood of held-out data, we
must integrate out the sampled multinomials and sum over
all possible topic assignments which has no closed-form solution. Griﬃths et al. [13] have used Gibbs sampling for
computing such approximations. First, we randomly split
each of the datasets into 80% training and 20% testing. We
trained each of the topic models on the training set. We then
tested the models on the testing set by running the inference
algorithms ﬁve times for each word-topic and segment-topic
pair. We then took average value for all ﬁve runs. We varied
the number of segment-topics from 10 to 50 in steps of 20
and the number of word-topics from 20 to 200 in steps of
20 in the NIPS collection. As the OHSUMED collection is
larger compared with the NIPS collection, so we varied the
number of segment-topics from 50 to 150 in steps of 50 and
word-topics from 150 to 490 in steps of 20.
From the results in Figure 4 we can see that in the NIPS
collection, NTSeg performs better than the comparative methods especially when the number of segment-topics is 10.
However, its performance deteriorates a bit when the number of segment-topics is increased, but still remains competitive with the comparative methods. Moreover, we notice that as the number of word-topics increases, the performance of NTSeg deteriorates to some extent in the NIPS collection. However, in the OHSUMED collection, NTSeg again
performs better against the comparative methods when the
number of word-topics is increased. We can observe that
NTSeg outperforms the comparative methods considerably
when the number of segment-topics is 100. The results suggest that NTSeg can perform very well on large document collections as large collections provide richer information about
word co-occurrences.

5.4 Document Likelihood Experiment
Another evaluation scheme to compare the relative performance of topic models is to study how the models generalize on an unseen data. The entire corpus in this method
is ﬁrst split into training and testing set. The training set
generally contains more number of documents as compared
to the testing set. A model is ﬁrst learned on the training
data, and the testing set is used to measure the generalization performance of the topic models. In the topic modeling literature, metrics such as perplexity computation or
log-likelihood have often been used. For example, PAM uses
empirical log-likelihood [10] as an evaluation metric and so
does a recently proposed method GD-LDA [6]. Log-likelihood
has also been widely used as one of the evaluation metrics,
for example in [3]. We chose log-likelihood metric for comparing the topic models. The comparative methods here are
LDSEG, PAM, LDACOL, TNG, and PDLDA.
We use the NIPS dataset5 . The NIPS collection is widely
used in the topic modeling literature. Note that the original
raw NIPS dataset consists of 17 years of conference papers.
But we supplemented this dataset by including some new
raw NIPS documents6 and it has 19 years of papers in total.
Our NIPS collection consists of 2741 documents comprising
of 453,606,9 non-unique words and 94961 words in the vocabulary. In addition to the NIPS collection we also use the
OHSUMED collection.
5
6

http://www.cs.nyu.edu/eroweis/data.html
http://ai.stanford.edu/egal/Data/NIPS/

211

6. CONCLUSIONS AND FUTURE WORK

[20] H. D. Kim, D. H. Park, Y. Lu, and C. Zhai. Enriching text
representation with frequent pattern mining for
probabilistic topic modeling. Journal of American Society
for Information Science and Technology, 49(1):1–10, 2012.
[21] W. Kintsch. The role of knowledge in discourse
comprehension: A construction-integration model.
Psychological Review, 95(2):163, 1988.
[22] W. Li and A. McCallum. Pachinko allocation:
DAG-structured mixture models of topic correlations. In
Proc. of ICML, pages 577–584, 2006.
[23] R. V. Lindsey, W. P. Headden, and M. J. Stipicevic. A
phrase-discovering topic model using hierarchical
Pitman-Yor processes. In Proc. of EMNLP, pages 214–222,
2012.
[24] D. Metzler and W. B. Croft. A Markov Random Field
model for term dependencies. In Proc. of SIGIR, pages
472–479, 2005.
[25] H. Misra, F. Yvon, J. M. Jose, and O. Cappe. Text
segmentation via topic modeling: An analytical study. In
Proc. of CIKM, pages 1553–1556, 2009.
[26] A. Mukherjee and B. Liu. Mining contentions from
discussions and debates. In Proc. of KDD, pages 841–849,
2012.
[27] P. V. Mulbregt, I. Carp, L. Gillick, S. Lowe, and
J. Yamron. Text segmentation and topic tracking on
broadcast news via a hidden markov model approach. In
Proc. of ICSLP, pages 2519–2522, 1998.
[28] L. Pevzner and M. A. Hearst. A critique and improvement
of an evaluation metric for text segmentation. Comput.
Linguist., 28(1):19–36, 2002.
[29] M. Riedl and C. Biemann. How text segmentation
algorithms gain from topic models. In Proc. of NAACL
HLT, pages 553–557, 2012.
[30] M. Riedl and C. Biemann. TopicTiling: A text
segmentation algorithm based on LDA. In Proc. of ACL
2012 Student Research Workshop, pages 37–42, 2012.
[31] M. Shaﬁei and E. Milios. Latent Dirichlet Co-Clustering. In
Proc. of ICDM, pages 542 –551, 2006.
[32] M. Shaﬁei and E. Milios. A statistical model for topic
segmentation and clustering. In Proc. of Advances in
Artificial Intelligence, pages 283–295, 2008.
[33] A. Tagarelli and G. Karypis. A segment-based approach to
clustering multi-topic documents. In Proc. of SDM, pages
1–33, 2008.
[34] Y. Teh, M. Jordan, M. Beal, and D. Blei. Hierarchical
Dirichlet processes. JASA, 101(476):1566–1581, 2006.
[35] H. M. Wallach. Topic modeling: Beyond bag-of-words. In
Proc. of ICML, pages 977–984, 2006.
[36] H. Wang and B. Lang. Online Ngram-enhanced topic model
for academic retrieval. In Proc. of ICDIM, pages 137–142,
2011.
[37] L. Wang, B. Wei, and J. Yuan. Topic discovery based on
LDACOL model and topic signiﬁcance re-ranking. Journal
of Computers, 6(8):1639–1647, 2011.
[38] X. Wang and A. McCallum. A note on Topical N-grams.
Technical report, DTIC Document, 2005.
[39] X. Wang, A. McCallum, and X. Wei. Topical N-Grams:
Phrase and topic discovery, with an application to
Information Retrieval. In Proc. of ICDM, pages 697 –702,
2007.
[40] Y. Wang, E. Agichtein, and M. Benzi. TM-LDA: Eﬃcient
online modeling of latent topic transitions in social media.
In Proc. of KDD, pages 123–131, 2012.
[41] Q. Wu, C. Zhang, and X. An. Topic segmentation model
based on ATNLDA and co-occurrence theory and its
application in stem cell ﬁeld. Journal of Information
Science, pages 1–14, 2012.
[42] Z. Yan and F. Li. News thread extraction based on Topical
N-gram model with a background distribution. In Proc. of
ICONIP, pages 416–424, 2011.

We have developed a generative topic discovery model,
known as NTSeg, which maintains the document’s structure
such as paragraphs and sentences and also keeps the order
of the words in the document intact. NTSeg incorporates
the notion of word-topics and segment-topics. We have
conducted extensive experiments and shown results using
both qualitative analysis where we show the n-gram words
in the correlation graph and quantitative performance. Experimental results demonstrate that by relaxing the bagof-words assumption in each segment improves the performance of the model.
Giving an arbitrary number of word-topics and segmenttopics to the model is one issue that we would look into for
future work. We would attempt to work towards a model
which could automatically ﬁnd out the desirable number of
word-topics and segment-topics in the collection.

7.

REFERENCES

[1] B. Adams, D. Phung, and S. Venkatesh. Discovery of latent
subcommunities in a blog’s readership. ACM Trans. on the
Web, 4(3):12:1–12:30, 2010.
[2] D. Beeferman, A. Berger, and J. Laﬀerty. Statistical models
for text segmentation. Machine Learning, 34:177–210, 1999.
[3] D. Blei and J. Laﬀerty. Correlated topic models. Proc. of
NIPS, pages 147–155, 2006.
[4] D. M. Blei and P. J. Moreno. Topic segmentation with an
aspect Hidden Markov model. In Proc. of SIGIR, pages
343–348, 2001.
[5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet
Allocation. JMLR, 3:993–1022, 2003.
[6] K. L. Caballero, J. Barajas, and R. Akella. The Generalized
Dirichlet Distribution in enhanced topic detection. In Proc.
of CIKM, pages 773–782, 2012.
[7] G. Casella and E. I. George. Explaining the Gibbs sampler.
The American Statistician, 46(3):pp. 167–174, 1992.
[8] X. Chen and S. Chen. Subsequence-based text segmentation
and labeling. In Proc. of ECTS, pages 582–587, 2009.
[9] J. Chien and C. Chueh. Topic-based hierarchical
segmentation. IEEE Transactions on Audio, Speech, and
Language Processing, 20(1):55–66, 2012.
[10] P. Diggle and R. Gratton. Monte Carlo methods of
inference for implicit statistical models. Journal of the
Royal Statistical Society, pages 193–227, 1984.
[11] L. Du, W. Buntine, and H. Jin. A segmented topic model
based on the two-parameter Poisson-Dirichlet process.
Machine Learning, 81(1):5–19, 2010.
[12] J. Eisenstein and R. Barzilay. Bayesian unsupervised topic
segmentation. In Proc. of EMNLP, pages 334–343, 2008.
[13] T. L. Griﬃths and M. Steyvers. Finding scientiﬁc topics.
National Academy of Sciences of the United States of
America, 101(Suppl 1):5228–5235, 2004.
[14] T. L. Griﬃths, M. Steyvers, and J. B. Tenenbaum. Topics
in semantic representation. Psychological Review,
114(2):211, 2007.
[15] A. Gruber, Y. Weiss, and M. Rosen-Zvi. Hidden topic
Markov models. In Proc. of AI and Statistics, pages
163–170, 2007.
[16] M. A. Hearst. TextTiling: Segmenting text into
multi-paragraph subtopic passages. Comput. Linguist.,
23(1):33–64, 1997.
[17] T. Hofmann. Unsupervised learning by Probabilistic Latent
Semantic Analysis. Machine Learning, 42:177–196, 2001.
[18] S. Jameel and W. Lam. An N-gram topic model for
time-stamped documents. In Proc. of ECIR, pages
292–304. 2013.
[19] N. Kawamae. Identifying sentiments over n-gram. In Proc.
of WWW, pages 541–542, 2012.

212

