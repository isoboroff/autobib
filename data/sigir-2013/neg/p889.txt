Improving LDA Topic Models for Microblogs
via Tweet Pooling and Automatic Labeling
Rishabh Mehrotra

Scott Sanner

BITS Pilani
Pilani, India

NICTA & ANU
Canberra, Australia

erishabh@gmail.com
Wray Buntine

Scott.Sanner@nicta.com.au
Lexing Xie

NICTA & ANU
Canberra, Australia

ANU & NICTA
Canberra, Australia

Wray.Buntine@nicta.com.au lexing.xie@anu.edu.au
ABSTRACT

Table 1: Example Topic Words
Unpooled LDA (poor)
Proposed approach (coherent)

Twitter, or the world of 140 characters poses serious challenges to
the efficacy of topic models on short, messy text. While topic models such as Latent Dirichlet Allocation (LDA) have a long history
of successful application to news articles and academic abstracts,
they are often less coherent when applied to microblog content like
Twitter. In this paper, we investigate methods to improve topics
learned from Twitter content without modifying the basic machinery of LDA; we achieve this through various pooling schemes that
aggregate tweets in a data preprocessing step for LDA. We empirically establish that a novel method of tweet pooling by hashtags
leads to a vast improvement in a variety of measures for topic coherence across three diverse Twitter datasets in comparison to an
unmodified LDA baseline and a variety of pooling schemes. An
additional contribution of automatic hashtag labeling further improves on the hashtag pooling results for a subset of metrics. Overall, these two novel schemes lead to significantly improved LDA
topic models on Twitter content.

barack cool apple health iphone
los barackobama video uma gop

text, these patterns are represented as distributions over words, called
topics. Probabilistic topic models such as Latent Dirichlet Allocation (LDA) [1] are a class of Bayesian latent variable models that
have been adapted to model a diverse range of document genres.
We consider the application of LDA to Twitter content, which
poses unique challenges different to much of standard NLP content. Posts are (1) short – 140 characters or less, (2) mixed with
contextual clues such as URLs, tags, and Twitter names, and (3)
use informal language with misspelling, acronyms and nonstandard
abbreviations (e.g. O o haha wow). Hence, effectively modeling
content on Twitter requires techniques that can readily adapt to this
unwieldy data while requiring little supervision.
Unfortunately, it has been found that topic modeling techniques
like LDA do not work well with the messy form of Twitter content [15]. Topics learned from LDA are formally a multinomial distribution over words, and by convention the top-10 words are used
to identify the subject area or give an interpretation of a topic. The
naïve application of LDA to Twitter content produces mostly incoherent topics. Table 1 shows examples of poor topic words by
directly applying LDA and topic words which are much more coherent and interpretable.
How can we extract better topics in microblogging environments
with standard LDA? While linguistic “cleaning” of text could help
somewhat, for instance [3], a complementary approach using LDA
is also needed because there are so few words in a tweet. An intuitive solution to this problem is tweet pooling [13, 5]: merging related tweets together and presenting them as a single document to
the LDA model. In this paper we examine existing tweet-pooling
schemes to improve LDA topic quality and propose a few novel
schemes. We compare the performance of these methods across
three datasets, constructed to be representative of the diverse content collections in the microblog environment. We examine a variety of topic coherence evaluation metrics, including the ability of
the learned LDA topics to reconstruct known clusters and the interpretability of these topics via statistical information measures.
Overall, we find that the novel method of pooling tweets by hashtags yields superior performance for all metrics on all datasets,
and an automatic hashtag assignment scheme further improves the
hashtag pooling results on a subset of metrics. Hence this work

Categories and Subject Descriptors
I.2.7 [Artificial Intelligence]: Natural Language Processing—Text
analysis; H.3.3 [Information Storage And Retrieval]: Information Search and Retrieval—Clustering

Keywords
Topic modeling, LDA, Microblogs

1.

flu swine news pandemic health
death flight h1n1 vaccine confirmed

INTRODUCTION

The “undirected informational” search task, where people seek
to better understand the information available in document corpora,
often uses techniques such as multidocument summarisation and
topic modeling. Topic models uncover the salient patterns of a collection under the mixed-membership assumption: each document
can exhibit multiple patterns to different extents. When analysing
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

889

provides two novel methods for significantly improving LDA topic
modeling on microblog content.

2.

Table 2: Datasets
Dataset
Generic

TWEET POOLING FOR TOPIC MODELS

The goal of this paper is to obtain better LDA topics from Twitter
content without modifying the basic machinery of standard LDA.
As noted in Section 1, microblog messages differ from conventional text: message quality varies greatly, from newswire-like utterances to babble. To address the challenges, we present various
pooling schemes to aggregate tweets into “macro-documents” for
use as training data to build better LDA models. The motivation behind tweet pooling is that individual tweets are very short (≤ 140
characters) and hence treating each tweet as an individual document does not present adequate term co-occurence data within documents. Aggregating tweets which are similar in some sense (semantically, temporally, etc.) enriches the content present in a single
document from which the LDA can learn a better topic model. We
next describe various tweet pooling schemes.
Basic scheme – Unpooled: The default treats each tweet as a single
document and trains LDA on all tweets. This serves as our baseline
for comparison to pooled schemes.
Author-wise Pooling: Pooling tweets according to author is a standard away of aggregating Twitter data [13, 5] and shown to be superior to unpooled Tweets. To use this method, we build a document
for each author which combines all tweets they have posted.
Burst-score wise Pooling: A trend on Twitter [7] (sometimes referred to as a trending topic) consists of one or more terms and a
time period, such that the volume of messages posted for the terms
in the time period exceeds some expected level of activity. In order
to identify trends in Twitter posts, unusual “bursts" of term frequency can be detected in the data. We run a simple burst detection
algorithm to detect such trending terms and aggregate tweets containing those terms having high burst scores. To identify terms that
appear more frequently than expected, we will assign a score to
terms according to their deviation from an expected frequency. Assume that M is the set of all messages in our tweets dataset, R is
a set of one or more terms (a potential trending topic) to which we
wish to assign a score, and d ∈ D represents one day in a set D
of days. We then define M (R, d) as the subset of Twitter messages
in M such that (1) the message contains all the terms in R and (2)
the message was posted during day d. With this information, we
can compare the volume in a specific day to the other days. Let
1
Σd∈D M (R, d). Correspondingly, SD(R) is the
Mean(R) = |D|
standard deviation of M (R, d) over the days d ∈ D. The burstscore is then defined as:
|M (R, d) − Mean(R)|
burst-score(R, d) =
SD(R)

Specific
Events

Term/%
music/17.9 business/15.8 movie/14.5 design/10.8 food/9.6 fun/9.1 health/6.9 family/6.4
sport/4.9 space/3.2
Obama/23.2 Sarkozy/0.4 baseball/3.5 cricket/1.8 Mcdonalds/1.5 Burgerking/0.5 Apple/16.3 Microsoft/6.8 United-states/40.7 France/4.9
Flight-447/0.9 Jackson/13.9 Lakers/13.8 attack/13.8 scandal/4.1 swine-flu/13.8 recession/12.3 conference/14.1 T20/4.4 Iran-election/8.6

viewed as topical markers, an indication to the context of the tweet
or as the core idea expressed in the tweet, therefore hashtags are
adopted by other users that contribute similar content or express
a related idea. One example of the use of hashtags is "ask GAGA
anything using the tag #GoogleGoesGaga for her interview! RT so
every monster learns about it!! " referring to an exclusive interview
for Google by Lady Gaga (singer). For the hashtag-based pooling
scheme, we create pooled documents for each hashtag. If any tweet
has more than one hashtag, this tweet gets added to the tweet-pool
of each of those hashtags.
Other Pooling: While a few other combinations of pooling schemes
(eg.author-time, hashtag-time, etc) are possible, the initial results
obtained were not as good as those presented for the currently outlined pooling schemes.

3.

TWITTER DATASET CONSTRUCTION

We construct three diverse datasets from a 20-30% sample of
public Twitter posts in 2009.1 We chose one or two term queries
(often with similar pairs of queries to encourage a non-strongly
diagonal confusion matrix) to search a tweet collection and each
resulting set of tweets was labeled by the query that retrieved it.
Since the number of queries (equivalently the number of clusters)
is known beforehand, we could use this knowledge to evaluate how
well the topics output by LDA match with known clusters. A brief
description of the three datasets is as follows:
Generic Dataset: 359,478 tweets from 11 Jan’09 to 30 Jan’09. A
general dataset with tweets containing generic terms.
Specific Dataset: 214,580 tweets from 11 Jan’09 to 30 Jan’09. A
dataset composed of tweets that refer to named entities.
Event Dataset: 207,128 tweets from 1 Jun’09 to 30 Jun’09. A
dataset composed of tweets pertaining to specific events. The
query terms represent these events and the time period was
chosen specifically due to the high number of co-occurring
events being discussed in this month relative to other months.
Table 2 provides the exact query terms and the percentage of
tweets in the datasets retrieved by each query. Typically, less than
one percent of tweets were retrieved by more than one query with
the highest case of 4.6% overlap occurring in the generic dataset
for the two queries “family” and “fun”. We have removed tweets
retrieved by more than one query in order to preserve uniqueness
of tweet labels for later analysis with clustering metrics.

Let us denote an individual term having burst-score greater than
some threshold τ on some day d ∈ D as a burst-term. Then our first
novel aggregation method of Burst Score-wise Pooling aggregates
tweets for each burst-term into a single document for training LDA,
where we found τ = 5 to provide best results. If any tweet has more
than one burst-term, this tweet gets added to the tweet-pool of each
of those burst-terms.
Temporal Pooling: When a major event occurs, a large number
of users often start tweeting about the event within a short period
of time. To capture such temporal coherence of tweets, the fourth
scheme and our second novel pooling proposal is known as Temporal Pooling, where we pool all tweets posted within the same hour.
Hashtag-based Pooling: A Twitter hashtag is a string of characters
preceded by the hash (#) character. In many cases hashtags can be

4.

EVALUATION METRICS

Because there is no single method for evaluating topic models,
we evaluate a range of metrics including those used in clustering
(purity and NMI) and semantic topic coherence and interpretability
(PMI) as discussed below.
In order to cluster with LDA, we let a topic represent each cluster
and assign each tweet to its corresponding mixture topic of highest probability (an inferred quantity via LDA). Then by analysing
clustering-based metrics, we wish to understand how well the different tweet pooling schemes are able to reproduce clusters representing the original queries used to produce the datasets.
1

890

http://snap.stanford.edu/data/twitter7.html

Formally, let Ti be the set of tweets in LDA topic cluster i and Qj
be the set of tweets with query label j. Then let T = {T1 , . . . , T|T | }
be the set of all |T | clusters and Q = {Q1 , . . . , Q|Q| } be the set of
all |Q| query labels. Now we define our clustering-based metrics as
follows.
Purity: To compute purity [6], each LDA topic cluster is assigned
the query label most frequent in the cluster. Purity then simply measures the average “purity” of each cluster, i.e., the fraction of tweets
in a cluster having the assigned cluster query label. Formally:
Purity(T, Q) =

seven in Hashtag-based pooling when compared against Unpooled
or Author-wise pooling schemes. On the other extreme lies the
Hourly pooling scheme with many fewer documents of a much
larger average size.

5.2

1
Σi∈{1...|T |} maxj∈{1...|Q|} |Ti ∩ Qj |
|T |

Obviously, high purity scores reflect better original cluster reconstruction.
Normalized Mutual Information (NMI): As a more informationtheoretic measure of cluster quality, we also evaluate normalized
mutual information (NMI) defined as follows
NMI (T, Q) =

6.

2I(T ; Q)
,
H(T ) + H(Q)

1 10 10
Σi=1 Σj=1 P M I(wi , wj ).
100
The average of the PMI score over all the topics is used as the final
measure of the PMI score.
PMI -Score(Tk ) =

7.

RESULTS FOR POOLING SCHEMES

RELATED WORK

Our work is different from many pioneering studies on topic
modeling of Tweets, as we focus on how to improve clustering
metrics and topic coherence with existing algorithms. Prior work
on topic modeling for tweets includes the following: [11] provide
a scalable implementation of a partially supervised learning model
(Labeled LDA). [5] provide a topical classification of Twitter users
and messages. [4] propose a novel method for normalising ill-formed
out-of-vocabulary words in short microblog messages. The TwitterRank system [13] and [5] used Author-wise pooling to apply LDA
to tweets. [15] compared topic characteristics between Twitter and
a traditional news medium – the New York Times; they propose to
use one topic per tweet (similar to PLSA), and argue that this is better than the Unpooled LDA scheme and the author-topic model. [9]
proposed two methods to regularize the resulting topics towards
better coherence. [8] used retweet as an indicator of “interesting-

In this section we discuss the results of the experimental evaluation of the tweet pooling schemes introduced in Section 2. The
datasets used were described in Section 3 while the evaluation metrics used were described in Section 4.

5.1

AUTOMATIC HASHTAG LABELING

Hashtag-based pooling is clearly the best pooling scheme based
on the results of Table 4, yet if we examine how many tweets have
hashtags, we find the following distribution: generic – 22.3%, specific, specific – 9.4%, event – 19.5%. In short, the majority of our
data is not being used effectively by Hashtag-based pooling. Consequently, we conjecture that automatically assigning hashtags to
some tweets should improve the overall evaluation metrics. Next
we present an algorithm for performing this automated hashtag assignment with a tunable confidence threshold.
Hashtag Labeling Algorithm: First we pool all tweets by existing
hashtags. Now, if the similarity score between an unlabeled and
labeled tweet exceeds a certain confidence threshold C, we assign
the hashtag of the labeled tweet to the unlabeled tweet (and hence
it joins the pool for this hashtag). For our similarity metric between
tweets, two obvious candidates are cosine similarity using TF and
TF-IDF vector space representations [12].
On an initial exploratory analysis, we found that a medium-range
confidence threshold of C = 0.5 to give best results for both Purity
and NMI. This is because tweets with the same class label have a
higher average TF-IDF similarity than tweets with a different class
label, so pooling these tweets together makes more topic-aligned
hashtag pools that aid cluster reconstruction. On the other hand,
PMI improved most for a rather high confidence threshold of C =
0.9 since otherwise, tweets that were only marginally relevant to
a hashtag reduced the overall topical coherence of hashtag-pooled
documents and led to a noisier LDA model.
The overall results obtained via hashtag assignment are shown
in Table 5 and demonstrate that Hashtag-based pooling with TFsimilarity tag assignment results in the best cluster reproductions
with the highest Purity and NMI scores of all methods examined,
while simple Hashtag-based pooling without tag assignment generally provides best results for topic coherence measured via PMI.

where I(·, ·) is mutual information, H(·) is entropy as defined
in [6], T and Q are as defined previously. NMI is always a number
between 0 and 1 and will be 1 if the clustering results exactly match
the category labels while 0 if the two sets are independent.
As an alternative to clustering quality, we can instead measure
topic interpretability or coherence, which is a human-judged quality that depends on the semantics of the words, and cannot be measured by model-based statistical measures that treat the words as
exchangeable tokens. It is possible to automatically measure topic
coherence with near-human accuracy [10] using a score based on
pointwise mutual information (PMI). We use this to measure coherence of the topics from different tweet-pooling schemes.
Pointwise Mutual Information (PMI): PMI is a measure of the
statistical independence of observing two words in close proximity
where the PMI of a given pair of words (u, v) is P M I(u, v) =
p(u,v)
log p(u)p(v)
. Both probabilities are determined from the empirical statistics of the full collection, and we treat two words as cooccurring if both words occur in the same tweet.
For a topic Tk (k ∈ {1 . . . |Tk |}), we measure topic coherence as
the average of PMI for the pairs of its ten highest probability words
{w1 , ..., w10 }:

5.

Comparison of Pooling Schemes

For the three datasets (viz. Generic, Specific and Events) and
four pooling schemes (plus Unpooled), we evaluate Purity scores,
NMI scores and PMI scores obtained by training LDA for 10 topics
using each setup. As seen in Table 4, the Hashtag-based pooling
scheme clearly performs better than the Unpooled scheme as well
as other pooling schemes.

Document Characteristics

We first look at characteristics of the documents in the different
pooling schemes for the three datasets as they may affect LDA.
Table 3 presents these statistics.
The statistics presented above highlight the differences in the
characteristics of the documents on which LDA models have been
trained. The number of documents is the largest for the Unpooled
and Author-wise schemes and smallest for the Hourly scheme, while
the corresponding size of the documents displays an inverse relationship. On average the document size increases by a factor of

891

Table 3: Document Characteristics for different pooling schemes.
Pooling Scheme
Generic
208300
359478
7658
465
8535

Authorwise
Unpooled
Burst Score
Hourly
Hashtag

#of docs
Specific
118133
214580
7436
464
7029

Events
67387
207128
5434
463
4099

Avg # of words/doc
Generic Specific Events
17.6
20.4
15.4
10.2
10.9
9.7
76.5
154.2
71.6
8493.4
5387.5
2422
70.4
187.2
78.4

Max # of words/doc
Generic Specific Events
4893
3586
2775
35
49
32
61918
420249 57794
20144
18869
38893
61918
420249 57794

Table 4: Results of different pooling schemes.
Scheme
Unpooled
Author
Hourly
Burstwise
Hashtag

Generic
0.49 ± 0.08
0.54 ± 0.04
0.45 ± 0.05
0.42 ± 0.07
0.54 ± 0.04

Purity
Specific
0.64 ± 0.07
0.62 ± 0.05
0.61 ± 0.06
0.60 ± 0.04
0.68 ± 0.03

Events
0.69 ± 0.09
0.60 ± 0.06
0.61 ± 0.07
0.64 ± 0.06
0.71 ± 0.04

Generic
0.28 ± 0.04
0.24 ± 0.04
0.07 ± 0.04
0.18 ± 0.05
0.28 ± 0.04

NMI Score
Specific
0.22 ± 0.05
0.17 ± 0.04
0.09 ± 0.04
0.16 ± 0.04
0.23 ± 0.03

Events
0.39 ± 0.07
0.41 ± 0.06
0.32 ± 0.05
0.33 ± 0.04
0.42 ± 0.05

Generic
−1.27 ± 0.11
0.21 ± 0.09
−1.31 ± 0.12
0.48 ± 0.16
0.78 ± 0.15

PMI score
Specific
0.47 ± 0.12
0.79 ± 0.15
0.87 ± 0.16
0.74 ± 0.14
1.43 ± 0.14

Events
0.47 ± 0.13
0.51 ± 0.13
0.22 ± 0.14
0.58 ± 0.16
1.07 ± 0.17

Table 5: Overall percentage improvement of Hashtag-based pooling variants over Unpooled scheme.
Pooling Scheme
Purity
NMI Scores
PMI Scores
Generic Specific
Generic Specific
Generic Specific
Events
Events
Hashtag
+8.16% +5.88% +2.89%
-3.44% +4.54% +7.69%
+161% +204%
Hashtag + Tag-Assignment (TF)
+21% +12.5% +8.69% +20.6% +9.1%
+12.8% +164% +155%
Hashtag + Tag-Assignment (TF-IDF) +12.2%
+9.4%
+4.34% +10.3%
+4.5% +10.25% +155% +159%

ness” to improve retrieval quality. This related research suggests a
number of orthogonal methods that could be used to complement
our Hashtag-based tweet pooling scheme.
For automatic hashtag labeling that proved crucial to improving
topics in our Hashtag-based pooling model, additional features for
hashtag assignment can be found in the comprehensive study [14],
which can be leveraged in future extensions along with novel socialmedia based similarity metrics like those incorporating inverse author frequency [2] and other social network properties.

8.

[3] B. Han, P. Cook, and T. Baldwin. Automatically constructing
a normalisation dictionary for microblogs. In Proc. of
EMNLP-CoNLL 2012, pages 421–432, Korea, 2012.
[4] B. Han, P. Cook, and T. Baldwin. Lexical normalisation of
social media text. ACM Transactions on Intelligent Systems
and Technology, 4(1), Feb. 2013.
[5] L. Hong and B. Davison. Empirical study of topic modeling
in Twitter. 1st ACM Workshop on Social Media Analytics,
2010.
[6] C. Manning, P. Raghavan, and H. Schütze. Introduction to
Information Retrieval. Cambridge University Press, 2008.
[7] M. Naaman, H. Becker, and L. Gravano. Hip and trendy:
Characterizing emerging trends on Twitter. J. Am. Soc. Inf.
Sci. Technol., 62(5):902–918, May 2011.
[8] N. Naveed, T. Gottron, J. Kunegis, and A. C. Alhadi.
Searching microblogs: coping with sparsity and document
quality. In CIKM ’11, pages 183–188, 2011.
[9] D. Newman, E. Bonilla, and W. Buntine. Improving topic
coherence with regularized topic models. NIPS, 2011.
[10] D. Newman, J. Lau, K. Grieser, and T. Baldwin. Automatic
evaluation of topic coherence. NAACL, 2010.
[11] D. Ramage, S. Dumais, and D. Liebling. Characterizing
microblogs with topic models. AAAI Conference on
Weblogs and Social Media, 2010.
[12] G. Salton and M. McGill. Introduction to modern
information retrieval. McGraw-Hill, 1983.
[13] J. Weng, E.-P. Lim, J. Jiang, and Q. He. Twitterrank: finding
topic-sensitive influential twitterers. In WSDM’10, pages
261–270, 2010.
[14] L. Yang, T. Sun, M. Zhang, and Q. Mei. We know what
@you #tag: does the dual role affect hashtag adoption?
WWW ’12, pages 261–270, 2012.
[15] W. X. Zhao, J. Jiang, J. Weng, J. He, E.-P. Lim, H. Yan, and
X. Li. Comparing twitter and traditional media using topic
models. In ECIR’11, pages 338–349, 2011.

CONCLUSION

This paper presented a way of aggregating tweets in order to
improve the quality of LDA-based topic modeling in microblogs
as measured by the ability of topics to reconstruct clusters and
topic coherence. The results presented in Table 4 on three diverse
selections of Twitter data suggest the novel scheme of Hashtagbased pooling leads to drastically improved topic modeling over
Unpooled and other schemes. The further addition of automatic TF
similarity-based hashtag assignment to Hashtag-based pooling outperforms all other pooling strategies and the Unpooled scheme on
cluster reconstruction metrics as shown in Table 5. In conclusion,
these two novel schemes present LDA users with novel methods for
significantly improving LDA topic modeling on Twitter without requiring any modification of the underlying LDA machinery.

Acknowledgments
NICTA is funded by the Australian Government as represented
by the Department of Broadband, Communications and the Digital Economy and the Australian Research Council through the ICT
Centre of Excellence program.

9.

Events
+127%
+124%
+100%

REFERENCES

[1] D. Blei, A. Ng, and M. Jordon. Latent Dirichlet allocation.
volume 3, pages 993–1022, 2003.
[2] S. E. Chan, R. K. Pon, and A. F. Cárdenas. Visualization and
clustering of author social networks. pages 30–31, Arizona,
USA, 2006.

892

