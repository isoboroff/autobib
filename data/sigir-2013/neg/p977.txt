Mapping Queries to Questions: Towards Understanding
Users’ Information Needs
Yunjun Gao†,‡
‡

Lu Chen†

Rui Li§

Gang Chen†

†
College of Computer Science, Zhejiang University, Hangzhou, China
State Key Lab of CAD&CG, College of Computer Science, Zhejiang University, Hangzhou, China
§
Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL, USA
†,‡

{gaoyj, chenl, cg}@zju.edu.cn, § ruili1@uiuc.edu

ABSTRACT
In this paper, for the ﬁrst time, we study the problem of
mapping keyword queries to questions on community-based
question answering (CQA) sites. Mapping general web queries
to questions enables search engines not only to discover
explicit and speciﬁc information needs (questions) behind
keywords queries, but also to ﬁnd high quality information
(answers) for answering keyword queries. In order to map
queries to questions, we propose a ranking algorithm containing three steps: Candidate Question Selection, Candidate Question Ranking, and Candidate Question Grouping. Preliminary experimental results using 60 queries from
search logs of a commercial engine show that the presented
approach can eﬃciently ﬁnd the questions which capture
user’s information needs explicitly.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval

General Terms
Algorithms, Experimentation, Performance

Keywords
Question Search, Community-based QA, Data Integration

1. INTRODUCTION
With the development of Web 2.0, users can share their
knowledge by asking and answering questions in the community based question answering (CQA) sites, such as Yahoo! Answers and Live QnA. As more and more persons
seek information on them, CQA services have became another popular information seeking mechanism except search

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

977

engines, e.g., according to Yahoo!’s statement, Yahoo! Answers has a million users worldwide, and has compiled 400
million questions. Compared with keywords queries used in
search engines, questions in CQA sites express users’ information needs explicitly. Moreover, each question has several
answers contributed by other users, and one of them is labeled as the best answer. Thus, CQA data is a type of
high quality data, which contains users’ information needs
expressed in questions as well as answers for these information needs. An interesting problem will be “Can general web
queries be mapped to questions? ”. If yes, not only can we
serve question answers to users but also, by understanding
user information needs as explicit questions, we can better
serve Web search results.
Motivated by this, we study the problem of mapping web
queries to questions (MQ2Q) on CQA sites, which can signiﬁcantly improve the performance of search engines in the
following two aspects:
The ﬁrst aspect is integrating CQA data, a type of high
quality information source, into general search engines. CQA
data, contributed by millions of users, contain much human
knowledge and experience. This kind of knowledge is rare in
general web pages. Through MQ2Q, users can directly get
related questions as well as the answers. For example, given
a query “clean PSP UMD”, the related question “What’s the
best way to clean a PSP UMD? ”, and its answers can satisfy
the information needs directly. Hence, MQ2Q can integrate
high quality CQA data into search engines.
The second aspect is deepening interpretations of queries.
Using MQ2Q, queries can be interpreted into popular and
relevant questions, which express users’ information needs
explicitly. For instance, given a query q1 : “iPod nano vs
iPod shuﬄe”, related questions like Q1 : “Diﬀerence between
the iPod nano and iPod shuﬄe? ” or Q2 : “How can I transfer
songs from an iPod nano to an iPod shuﬄe, while they are
both plugged into my Laptop? ”, and Q3 : “Which is better
size and cost wise, an iPod nano or an iPod shuﬄe? ” show
users’ intentions behind the query clearly. With deepened
interpretations through query mapping, MQ2Q can boost
search engines’ performance signiﬁcantly.
As our target, we deﬁne MQ2Q as the problem of mapping
a query to a set of questions that represent various popular information needs behind the query. Therefore, MQ2Q
contains the following challenges:
Challenge 1: How to ﬁnd related questions, which may
not contain all the keywords for a given query? A related

2.

question is a question that captures a clear information need
behind the given query. As an example, in the above examples, Q1 , Q2 , and Q3 are relevant questions for the query
q1 , although they do not match the keyword “vs”. However,
“iPod nano vs iPod shuﬄe? ” is not a relevant question, because it does not express user’s information needs explicitly.
Challenge 2: How to ﬁnd popular questions among a
set of related questions? A popular question is a question
that denotes an information need which may be asked by
most users. Continuing the aforementioned example, given a
query q1 , Q1 , Q2 , and Q3 are all relevant questions; whereas
obviously, Q1 and Q3 , which are about comparing two items,
are more likely to meet the user’s information needs than Q2 .
Consequently, Q1 and Q3 should be ranked higher than Q2 .
Challenge 3: How to ﬁnd various kinds of information
needs and group popular questions accordingly? Given a set
of ranked questions Q1 , Q3 and Q2 , some of them like Q1
and Q3 , representing the same kind of information needs for
the query, should be grouped together. The Q2 , which is
diﬀerent from Q1 and Q3 should be in another group.
To the best of our knowledge, the most related work is
question retrieval —ﬁnding similar questions for a speciﬁed
quesiton [1, 2, 5], but they do not study the MQ2Q problem speciﬁcally dealing with all the above challenges. First,
most question retrieval requires either large amount of training data [2, 5] or linguistic strcutrues [1] that are diﬃcult
or impossible to obtain in keyword queries. Second, all the
above methods do not take the popularity of questions into
consideration when ranking is performed. Intuitively, among
a set of relevant questions, the question that has been asked
frequently should be the most likely to capture the information needs behind the query.
Another possible solution is the traditional information
retrieval method. However, the ranking function based on
the similarity between bags of words are biased to short
questions. For instance, given a query “iPod ”, questions returned by either the language model or the vector space
model are “iPod..? ” or “iPod? ”. Nevertheless, these questions are meaningless for answering the query. Yahoo! Answers also provides a search function for searching questions.
We do not know the details of its algorithm, but in some
cases, it suﬀers from the same problem with the traditional
IR model. Furthermore, the ranking functions of both traditional IR method and Yahoo! Answers do not consider the
question popularity neither.
To tackle the MQ2Q problem, we propose a three stage
algorithm. First, it extracts “topics” from questions and
queries, and relevant questions are selected based on matching “topics” of queries. Second, for each relevant question,
a score is calculated by exploring a relevance feature and a
popularity feature. The popularity feature is based on the
assumption that if a question is asked frequently in CQA
sites, it is much more likely to be queried again. Third, the
popular questions are grouped into various kinds of information needs, using a clustering algorithm. For evaluation,
we compare the algorithm with the traditional language
model. Experimental results demonstrate that our proposed
approach signiﬁcantly outperformed the traditional language
model, and 47% improvement is obtained in terms of top 5
precision.
In what follows: Section 2 presents our solution to handle
MQ2Q; Section 3 reports experimental results; and Section
4 concludes the paper.

MAPPING QUERIES TO QUESTIONS

In this section, we describe our three stage algorithm, including Candidate Selection, Candidate Ranking, and Candidate Grouping. Before the discussion, we formulate a
question representation as a data record containing a T itle,
a Description, Answers, a Category, and other meta data.

2.1

Candidate Question Selection

Given a query, the algorithm ﬁrst selects a set of relevant
questions. The selection method is based on the following
assumption: Given an information need, a user can directly
post a question according to the need, or select some words
from the question in mind as parts of a query. These words
shared by the question, and the query are often noun phrases
conﬁning the query into certain topics. We call these words
topic terms. Thus, if a question represents an information
need behind the query, the question must have topic words
of the query.
We deﬁne topic words of a question as noun phrases appearing in question T itle and terms appeared in question
Category. Noun phrases in Question Title can be extracted
by a standard POS tagger. Hence, a question Qi can be represented as a topic set, denoted by QT Si = (Ti1 , Ti2 , ..., Tin ).
For example, given a question “How do you transfer iPod
songs to your library that thinks it already exists on iTunes? ”,
which belongs to the category “Music\Music Players”, the
topic set are “iPod, Songs, Library, iTunes, Music, Players”.
We also deﬁne topic words of a query as noun phrases in
the query. However, the POS tagger can not be directly
applied to simple keywords without a sentence structure.
In order to extract topics from a query, we use topic terms
appeared in questions to estimate whether a term in a query
is a topic, which can be calculated as:
T opic(ti )ti ∈q =

T opicF req(ti )ti ∈QS
F req(ti )ti ∈QS

(1)

where T opicF req(ti )ti ∈QS is the frequency of ti in questions as a topic term, F req(ti )ti ∈QS is the times of term
ti appeared in all questions. Then, whether a term ti is a
topic term can be determined by whether T opic(ti ) exceeds
a threshold θ. All topic terms in the query form a query
topic set, denoted as qT S.
After extracting topics from queries and questions, a candidate question Qi is obtained by determining whether topics of the question cover topics of the query q. The candidate
question set Canq can be constructed by following equation.
Canq = {Qi ∈ CQA|QT Si ⊇ qT S}

2.2

(2)

Candidate Question Ranking

In order to determine which question is most likely to
represent the user’s information needs for a given query, our
algorithm ranks candidate questions based on the relevance
feature and the popularity feature.
The relevance feature aims to measure the similarity
between the queries and questions. Evidently, the more similar to the query, the more possible the question represents
the information needs behind the query. There are many
similarity measurements. We utilize the score P (Q|q) estimated by the language model as the similarity metric for a
question Q and a query q.
Rel(Q, q) =

n
∏
i=1

978

p(wi |Q)

(3)

Algorithm 1 Candidate Question Grouping Algorithm
Input: a ranked candidate question list RQL,
the number of questions N
Output: selected question list SQL
Procedure:
1: add the ﬁrst question Q of RQL to SQL
2: remove the question Q from RQL
3: for each question Qr in RQL do
4:
for each question Qs in SQL do
5:
sim = CosineSimilarity(Qr , Qs )
6:
record the maximum of sim as maxsim
7:
end for
8:
if maxsim < SimilarityT hreshold then
9:
insert Qr into SQL
10:
end if
11:
add Score(Qr ) to Score(Qs )
12:
if size(SQL)>N then
13:
return
14:
end if
15: end for
16: score(Qs )/ = N umberof SimilarQuesitons
17: rank SQL
18: return SQL

where wi is a term in query q, n is the length of q, and the
p(wi |D) is calculated as:
p(wi |q) = λ ×

f req(wi )
Count(wi )
+ (1 − λ) ×
length(Q)
|N |

(4)

where f req(wi ) is the frequency of wi appeared in Q, N is
the frequency of total terms in the data set, and Count(wi )
is the frequency of wi appeared in the data set.
The popularity feature aims to measure the popularity of questions. Obviously, the more frequently be asked,
the more possible the question represents a popular information need behind the current query. In order to compute
the popularity of a question, we follow the idea of Question
Utility [4], which uses a centrality score C(Q) to estimate
P (Q) based on the lexRank [3] over a graph of connected
questions. However, this algorithm can not get good results
for questions distributed in diﬀerent categories, and it is also
biased to short questions containing general words such as
a question “iPod help? ”. The reason may be that original
LexRank algorithm is developed for ranking a small number
of sentences appeared in one document. To overcome deﬁciencies of the previous approach, we propose a topic-based
LexRank. First, the algorithm builds a directed graph with
the consideration the topics and category information. In
the graph, each node represents a question. If a asymmetric
similarity called topic coverage between Qi and Qj exceeds
a threshold value, there will be an edge from a node Qi to
a node Qj . Then, for a question Q, the centrality C(Q) is
calculated based on the Random Walk Algorithm with the
following weighting scheme:
∑
C(v)
1
+ (1 − d)
(5)
C(Qi ) =
N
deg(v)

3. EXPERIMENTAL RESULTS
This section experimentally evaluates the eﬃciency of our
proposed approach.
Experimental Setup: We collected question data from
Yahoo! Answers. To reduce the overhead of crawling, while
maintaining a meaningful scope of experiments, we crawled
questions related to the “iPod ” topic. To crawl questions
in the “iPod” topic, we extracted 2,366 queries containing
“iPod” from search logs of a commercial engine, and submitted them to Yahoo! Answers’ Search API, resulting in
155,003 questions. We randomly sampled 60 queries from
“iPod” queries for evaluation. We only use words in question
titles, and all words are stemmed with the Porter stemmer.
To verify our model, we use the Query Likelihood Language Model (LM) as the baseline method, since it is a
state-of-the-art retrieval model. We compare it with our
MQ2Q without question grouping model (ranking according
to Score(Qi )) and MQ2QC with question grouping model.
For each model, we got 10 results for each query. Given
results for each query, an assessor is asked to label it with
“relevant” or “irrelevant”. As mentioned in Section 1, a relevant question must contain a clear information need. Hence,
a question “iPod battery help! ” is not relevant for the query
“iPod batteries”. Besides, the accessor is asked to select popular questions from results for each question. If the accessor
does not ﬁnd any popular/interesting question, there will be
no popular question for these results.
Based on the labeled data, we make use of two metrics for
evaluating each model. They are TOP N precision (P @N )
and Mean Reciprocal Rank (M RR). The P @N is used to
measure the percentage of relevant
questions among top N
∑

v∈adj(Q)

where N is the total number of nodes in the graph, d is a
dampening factor, adj(Q) is the set of nodes adjacent to Q,
and deg(v) denotes the degree of node v. Finally, we use
C(Qi ) as our popularity score P op(Qi ).
The topic coverage is deﬁned as follows: if Qi and Qj
belong to the same category, the T CS(Qi , Qj ) is deﬁned to
be 0; otherwise it is deﬁned as:
∥QT Si ∩ QT Sj ∥
+(1−λ)×cosine(Qi , Qj )
∥QT Si ∥
(6)
where QT Si is the topic set of question Qi .
Given the relevance feature score P (Qi |q) and the popularity feature P (Qi ), we employ a single formula to combine
them as a ﬁnal score, denoted by Score(Qi ), for each question. Then, questions are ranked by Score(Qi ). One simple
way is taking the popularity score P op(Qi ) as the probability of Q in the language model. Therefore, the ﬁnal score
can be calculated as:
T CS(Qi , Qj ) = λ×

Score(Qi ) = log(rel(Qi |q)) + log(P op(Qi ))

(7)

K

2.3 Candidate Question Grouping

P (N )

returned results, deﬁned as i=1K
. The M RR is a standard measurement used by TREC QA. For a query, the Reciprocal Rank (RR) of a model is deﬁned as 1r , where r is
the highest rank of a document retrieved by the model that
is judged relevant. In contrast, we evaluate r as the highest
rank of a question retrieved by the model judged popular.
Quantitative Results: Table 1 lists results of the Top
5 precision (P @5), Top 10 precision (P @10), and M RR of
three models. We can observe that either MQ2Q model or

In CQA sites, many questions are very similar, which
makes the top 10 returned questions for a query nearly the
same. Thus, in order to get popular questions focusing on
various topics of the speciﬁed query, the third step of the algorithm use a simple grouping algorithm to re-organize the
ranked questions. The pseudo-code of candidate question
grouping algorithm is depicted in Algorithm 1. The selected
question of each cluster is outputted as results.

979

Table 1: Experimental results
Method
LM
MQ2Q
MQ2QC

P @5
0.41
0.883
0.886

P @10
0.423
0.749
0.789

Table 3: More Results of MQ2QC Model

MMR
0.246
0.439
0.514

Query: iPod batteries
(Q1:)Why does my ipod touch battery go down when its
oﬀ? (Q2:) iPod Nano...my battery never fully charges?
Query: videos don’t play on iPod
(Q1:) What video format do I need so I can play a video
on my iPod? (Q2:) My iPod is not playing my music
videos and my movies what do I do?
Query: free iPod games
(Q1:) Where can i get free iPod game’s movie’s and music
video’s? (Q2:) Free iPod games for the ipod mini?
Query: CD player iPod
(Q1:) What devices do I need in order to connect iPod
with car CD player? (Q2:) Anyone know what you connect to a CD player to use it with your iPod?

Table 2: Results for “iPod video download ”
LM Results
iPod downloaded video...?
iPod video downloads?
Downloading video on video-iPod?
MQ2Q Results
where can I download free music video’s for my iPod?
How do you download music videos on an iPod?
Where can I download music videos for my iPod nano?
MQ2QC Results
How do you download music videos on an iPod?
How do I get a YouTube video onto my iPod?
How can I download dvds on my video iPod?

lated software pages, apple’ home pages, and a how-to page.
However, these pages do not answer user information needs
directly, which require users to make eﬀorts in exploring
these sites. Questions returned by the MQ2Q and MQ2QC
models do capture the popular information needs behind the
query. Moreover, results in MQ2QC cover 3 diﬀerent popular information needs, while questions returned by MQ2Q
are essentially the same. We also show some query and question pairs returned by our algorithm in Table 3. Note that,
popular information needs behind each query can be clearly
captured by mapping the query to questions.

MQ2QC model outperform the baseline method signiﬁcantly
in all three measurements. Furthermore, the MQ2QC model
performs nearly the same as the MQ2Q model in terms of
Top N precision, since they are based on the same ranking method. The MQ2QC model performs better than the
MQ2Q model in terms of M RR, since top 10 questions returned by MQ2QC are more diverse than that returned by
MQ2Q, which are more likely to contain a popular information need. For the LM result, we can ﬁnd that P @5 is lower
than P @10. This is because, its ranking method can not
rank relevant questions appropriately. On the other hand,
P @5 of both the MQ2Q and MQ2QC models are higher than
P @10. It shows that relevant questions are ranked in the
top places. Consequently, based on the aforementioned results and analysis, we can conclude that our ranking method
can ﬁnd popular and relevant questions eﬃciently.
Case Study: Table 2 presents top 3 results of three models for the query “iPod downloaded videos”, and Figure 1
illustrates the top 3 results returned by Google. The information needs behind the query may be where to download videos for iPod. For the LM results, questions seems
to be matched with the query, but they do not contain a
clear information need. Users will not be interested in these
questions. Google also returns some relevant pages like a re-

4.

CONCLUSION

The key contributions of this paper can be summarized
as follows: (1) We ﬁrst investigate the problem of mapping
web queries to questions, and present some potential applications of this problem. (2) We propose a MQ2QC model
for solving the challenges in the problem. (3) Experimental
results demonstrate that it can eﬃciently ﬁnd popular questions that capture various users’ information needs behind
a general keyword query.

5.

ACKNOWLEDGEMENTS

This work was supported in part by NSFC 61003049, the
Fundamental Research Funds for the Central Universities
under Grants 2012QNA5018 and 2013QNA5020, and the
Key Project of ZJU Excellent Young Teacher Fund.

6.

REFERENCES

[1] Y. Cao, H. Duan, C.-Y. Lin, Y. Yu, and H.-W. Hon.
Recommending questions using the mdl-based tree cut
model. In WWW, pages 81–90, 2008.
[2] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar
questions in large question and answer archives. In
CIKM, pages 84–90, 2005.
[3] D. R. Radev. Lexrank: graph-based centrality as
salience in text summarization. Journal of Artiﬁcial
Intelligence Research, 22:457–479, 2004.
[4] Y.-I. Song, C.-Y. Lin, Y. Cao, and H.-C. Rim. Question
utility: A novel static ranking of question search. In
AAAI, pages 1231–1236, 2008.
[5] X. Xue, J. Jeon, and W. B. Croft. Retrieval models for
question and answer archives. In SIGIR, pages 475–482,
2008.

Figure 1: Top 3 results returned by Google

980

