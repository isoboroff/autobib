Building Test Collections
An Interactive Tutorial for Students and Others Without Their Own Evaluation Conference Series

Ian Soboroff
National Institute of Standards and Technology

While existing test collections and evaluation conference
efforts may sufficiently support one’s research, one can easily
find oneself wanting to solve problems no one else is solving
yet. But how can research in IR be done (or be published!)
without solid data and experiments? Not everyone can talk
TREC, CLEF, INEX, or NTCIR into running a track to
build a collection.

1. Introduction to test collections: basic concepts:
task, documents, topics, relevance judgments, and measures; history of Cranfield paradigm.
2. Task: a task-centered approach to conceiving test collections; metrics as an operationalization of task success; understanding the user task and the role of the
system.

This tutorial aims to teach how to build a test collection
using resources at hand, how to measure the quality of that
collection, how to understand its limitations, and how to
communicate them. The intended audience is advanced students who find themselves in need of a test collection, or
actually in the process of building a test collection, to support their own research. The goal of this tutorial is to lay
out issues, procedures, pitfalls, and practical advice.

3. Documents: the relationship between documents and
task; naturalism vs constructivism; opportunity sampling and bias; distribution and sharing.
4. Topics: designing topics from a task perspective. sources
for topics. exploration or topic development. extracting topics from logs. topics and queries. topic set size.

Attendees should come with a specific current need for
data, and/or details on their in-progress collection building
effort. The first half of the course will cover history, techniques, and research questions in a lecture format. The second half will be devoted to collaboratively working through
attendee scenarios.

5. Relevance: defining relevance and utility starting from
the task; obtaining labels, explicit and implicit elicitation (highlighting); Interface considerations; interannotator agreement; errors; crowdsourcing for relevance judgments; validation, process control, quality
assurance; annotator skill set.

Upon completion of this tutorial, attendees will be familiar with the history of the test collection evaluation paradigm;
understand the process of beginning from a concrete user
task and abstracting that to a test collection design; understand different ways of establishing a document collection;
understand the process of topic development; understand
how to operationalize the notion of relevance, and be familiar
with issues surrounding elicitation of relevance judgments;
understand the pooling methodologies for sampling documents for labeling, and be familiar with sampling strategies
for reducing effort; be familiar with procedures for measuring and validating a test collection; and be familiar with
current research issues in this area.

6. Pooling: problem of scale and bias; breadth of pools,
multiple systems; completeness vs. samples; methods;
estimating pool depths; leave-one-out test, reusability;
double pooling.
7. Analysis: bounding the score resolution; contrasting many different systems or variations of a system;
PCA of topic:run scores, bounding of topic breadth;
ANOVA, factor analysis, regression; statistical significance and meaningful differences; calibration by user
pilot study; per-topic analysis and failure analysis; bias.
8. Test collection diagnosis: LOO test revisited; unjudged == irrelevant; differentiating poor systems from
collection bias.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Misc.

9. Validation: user study; side-by-side comparison; a/b
testing; interleaving.

Keywords: test collections; experimental methods

10. Pooling and sampling: pooling as a sampling method;
pooling as optimization; move-to-front pooling; uniform sampling, stratified sampling, measure sampling;
minimal test collections.

Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact
the owner/author(s).
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
ACM 978-1-4503-2034-4/13/07.

11. Advanced task concepts: filtering, supporting system adaptation; sessions, time, user adaptation; context, feedback; exploration and fuzzy tasks; novelty,
differential relevance; fundamental limits of Cranfield.

1132

