Sentiment Analysis of User Comments for
One-Class Collaborative Filtering over TED Talks
Nikolaos Pappas

Andrei Popescu-Belis

Idiap Research Institute
Rue Marconi 19
CH-1920 Martigny, Switzerland

Idiap Research Institute
Rue Marconi 19
CH-1920 Martigny, Switzerland

nikolaos.pappas@idiap.ch

andrei.popescu-belis@idiap.ch

ABSTRACT

that unlabeled comments can also be leveraged to improve
recommendations, in a challenging one-class setting. Other
studies have used comments to enrich user profiles for news
recommendation, but without attempting sentiment analysis and the inference of item ratings [9, 5].

User-generated texts such as reviews, comments or discussions are valuable indicators of users’ preferences. Unlike
previous works which focus on labeled data from user-contributed reviews, we focus here on user comments which are
not accompanied by explicit rating labels. We investigate
their utility for a one-class collaborative filtering task such
as bookmarking, where only the user actions are given as
ground truth. We propose a sentiment-aware nearest neighbor model (SANN) for multimedia recommendations over
TED talks, which makes use of user comments. The model
outperforms significantly, by more than 25% on unseen data,
several competitive baselines.

2.

SENTIMENT ANALYSIS

The first stage of our proposal is the sentiment classification of user comments, with two possible labels: positive
(pos) and negative (neg). Given the lack of ground-truth
labels, we focused on dictionary-based methods and specifically on an extension of the rule-based sentiment classifier
presented in [10] and implemented in [7]1 . The classifier
uses the MPQA polarity lexicon and can deal with negation,
intensifiers, and polarity shifters. The rule-based classifier
estimates the polarity of a sentence as a positive or negative
numerical value. This value determines the sentiment label,
pos or neg of the sentence; for zero, the neutral label (neu)
is applied. The polarity of a comment is the sum over the
polarities of the sentences that compose it, and its label (pos
or neg) is given by the sign of the total.
To test the sentiment analysis component, we performed
human labeling of a subset of the TED comments, with
pos, neg or neu polarity labels (the latter included also
undecided cases)1 . Six human judges annotated 160 comments with 320 sentences, randomly selected from the TED
data, with some overlap to assess agreement, using Fleiss’
kappa (κ) metric. We obtained 260 labels for sentences and
135 for comments (excluding neu). Among these, 61 sentences and 29 comments were common across annotators,
and agreement was found to be, respectively, κ = 0.834 and
κ = 0.650. As agreement was substantial, we used the entire
set as ground truth to evaluate automatic sentiment analysis
(cases of disagreements were reconciled by a majority vote).
The results of our rule-based classifier (RB) and of a random baseline (Rand) are shown in Table 1. Our system
reaches F-score of 74.90% and 72.60% on sentences and respectively comments for the classification task (plus moderate agreement with the ground truth of κ = 0.53 and
κ = 0.43), a level that is suﬃcient to improve the one-class
recommendation task, as we will show.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval—Information filtering

Keywords
Sentiment Analysis; One-Class Collaborative Filtering

1. INTRODUCTION
Many problems in collaborative filtering (CF) such as social bookmarking, news and video recommendations make
use of binary user ratings in terms of ‘action’ or lack thereof,
i.e. ‘inaction’. The diﬃculty of such one-class CF problems [6] comes from the lack of a negative class: it is inherently unsure whether user inaction means that an item was
not seen or was not liked (hence not bookmarked). In this
paper, we study the one-class CF problem of lecture recommendation over TED talks. We show how to infer additional
user ratings by performing sentiment analysis (SA) of user
comments and integrating its output in a nearest neighbor
(NN) model. The resulting SANN model outperforms several competitive baselines, is robust to noisy SA results and
improves its performance with the number of comments.
Work on inferring user ratings has mostly focused on reviews with explicit ratings [3, 11, 4]. Here, however, we show
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

3.

ONE-CLASS CF MODELS

The one-class collaborative filtering problem can be formalized as follows. Let U be the set of users of size |U | = M
and I the set of items of size |I| = N . The matrix of useritem ratings is R (of size M × N ), with rui = 1 indicating an
1

773

https://github.com/nik0spapp/unsupervised sentiment

Methods
Rule Based (RB)
Baseline (Rand)
Annotators

Sentences (pos=123, neg =137)
Precision Recall
F
Kappa
73.43
76.42
74.90
0.53
47.36
48.64
47.90
-0.01
0.83

Comments (pos=76, neg =59)
Precision Recall
F
Kappa
75.71
69.73
72.60
0.43
56.24
53.42
54.63
-0.02
0.65

Table 1: Performance of annotators, ruled-based and random classifier measured with Precision, Recall, F1
and Fleiss’ kappa (κ). Inter-annotator agreement is substantial (κ ≥ 0.65).
In this equation, Dck (u; i) is the neighborhood of the k
most similar items that the user has already rated or com′
mented and ruj
is the result of the mapping function that
accounts for both explicit ratings and those inferred from
comments, defined as follows:
{
1,
if ruj = 1
′
ruj =
(5)
cuj , if ruj ̸= 1

‘action’ rating (favorite item) and rui = 0 an ‘inaction’ one
(not seen or not liked). Our goal is to predict the preference
of the users in the future, therefore (as in previous studies)
we hide for evaluation a certain proportion of ‘1’ values per
user and measure how well we predict them.

3.1 Neighborhood Models
Nearest neighbor (NN) models are often used in collaborative filtering and have been proven quite eﬀective despite
their simplicity [2]. Here, we use item-based neighborhood
models which are described by Eq. 1. The prediction function r̂ui estimates the rating of a user u for an unseen item
i. It relies on the bias estimate bui of the user u for various items j (given in Eq. 2) and on a score computed using
the k most similar items to i that the user u has already
rated, i.e. the neighborhood Dk (u; i). The denominator is a
normalization factor.
∑
j∈D k (u;i) dij (ruj − buj )
∑
(1)
r̂ui = bui +
j∈D k (u;i) dij

cuj is a mapping function which maps the polarity level of
a comment Cj of user u to a rating understandable by the
SANN model. We will compare three such mapping functions, formally defined in Table 2: two of them, noted ‘randSANN’ and ‘SANN’, generate 3-way ratings (1, 0, -1) from
the random (Rand) and respectively the rule-based (RB)
sentiment classifiers, while the third one, noted ‘polSANN’,
generates a polarity value between -1 and 1 by combining the
polarity scores of the sentences that constitute a comment.
For all the three functions, the pos class has a positive eﬀect
on r̂ui , while neu and neg classes have a negative eﬀect. An
optimal function (per user or global) could also be learned
from the data, in future work.
Moreover, the additional training data from commented
items are considered for the creation of the co-rating matrix
(N × N ) used for the similarity sij in Eq. 2.

The bias estimate bui is computed as the sum of the average ratings µ of items in a given dataset, the average rating
bu of a user u and the average rating bi for a given item i.
The coeﬃcient dij is the similarity between item i and item
j (Eq. 2) and is computed using the similarity sij between
items i and j, weighted by the normalized importance of the
number of common raters nij (close to 1 if nij ≫ λ). We
determine the optimal values of λ and of the neighborhood
size k using cross-validation.
nij
dij = sij
; bui = µ + bu + bi
(2)
nij + λ

Mapping function
cuj = signrand (Cj )
cuj = signRB (Cj ) (
)
∑
cuj = 1 + zj · s∈Cj polRB (s)/|s|

Notation
randSANN
SANN
polSANN

Table 2: Three mapping functions for the SANN
model. Notations: sign(Cj ) is the sentiment of comment Cj (1 for pos, 0 for neu and -1 for neg ); pol (s)
returns the polarity of a sentence s; and zj is a normalization factor over all comments C u of a user u,
defined as zj = 1/(1 + |C u | · |{C s.t. C ∈ C u ∧ sign(C) =
sign(Cj )}|).

The similarity sij can be computed using a function such
as cosine similarity or Pearson’s correlation between vectors
representing i and j in the N × N co-rating matrix derived
from R. However, given that ratings are binary and not
real-valued, the biases should not be computed linearly, but
using the formulas proposed below, which take into account
the number of the items and the maximally rated items.
1 ∑ ri
ru
ri
µ=
; bu =
; bi =
(3)
|I| i∈I rmax
|I|
rmax

4.
4.1

EXPERIMENTAL SETUP
The TED Dataset

To evaluate our models on the one-class problem, we focus
on multimedia recommendations on the TED dataset [8]2 .
TED (www.ted.com) is a popular online repository of public
talks and user-contributed material (favorites, comments)
under a Creative Commons license. We crawled the TED
dataset in September 2012 and gathered data from 74,760
users and 1,203 talks, with 134,533 favorites and 209,566
comments. According to our RB classifier, 63% are positive,
27% are negative and 10% are neutral comments.
For this study, we selected users with at least 4 favorites
(5,657 users) and included only comments that appear in
the first level of the commenting area, i.e. excluding all the
replies to other comments, because the target of their polarity judgment is uncertain (comments on comments rather

3.2 Sentiment-Aware Neighborhood Model
We propose a sentiment-aware nearest neighbor model
(SANN) which integrates into a NN model the preferences
of the users that are extracted from user-generated text by
sentiment analysis. In order to achieve that, the model in
Eq. 1 must be modified as follows: firstly, the neighborhood
Dk (u; i) must account for the additional training data, and
secondly, the rating function ruj must map the output of
sentiment analysis over comments to rating values that are
understandable by the model. Thus, we modify the Eq. 1 of
the traditional neighborhood models as follows:
∑
′
r̂ui = bui +
dij (ruj
− buj )
(4)

2

k (u;i)
j∈Dc

774

https://www.idiap.ch/dataset/ted

Methods
TopPopular
normNN(PC)
NN(COS)
NN(PC)
SANN(COS)
SANN(PC)
Improvement

k parameter (1 to 50)
MAP@50 MAR@50 MAF@50
3.46
12.69
5.44
4.06
13.70
6.27
4.70
16.30
7.29
4.75
16.47
7.37
5.07
17.90
7.91
5.27
18.68
8.22
+10.9%
+13.4%
+11.5%

λ parameter (1 to 100)
MAP@50 MAR@50 MAF@50
3.46
12.69
5.44
4.13
14.00
6.37
4.67
16.31
7.27
4.76
16.58
7.40
6.12
20.65
9.45
6.27
20.99
9.66
+31.8%
+26.5%
+30.6%

Table 3: Performance of various methods using 5-fold cross validation. The last row displays the improvement
of the SANN model over the best NN model, which is always significant at the p < 0.01 level.
than on the talk). The resulting data set has 129,633 ratings
(favorites) and 20,792 comments (see Table 4).

4.2 Evaluation Protocol
For evaluation, we split the dataset into a training set
(80%) and a testing set (20%). More precisely, for each
user we keep 80% of their positive ratings (‘1’ values) for
training and hold out 20% for testing. Furthermore, we
divide the test set in two subsets based on comment sparsity:
a sparse and a dense one (see Table 4). For the sparse set,
we keep all users with at least 12 ratings and for the dense
one, all users with at least 12 ratings and one comment.
We optimize the parameters on the training set using 5-fold
cross-validation. When training, we use all users to obtain
good approximations of item similarities (sij in Eq. 2).
Set
Favorites Comments Users
Training
108,256
20,792
5,657
Sparse held-out
17,227
8,728
2,409
Dense held-out
9,303
8,728
1,181

Figure 1: The eﬀect of the neighborhood size k on
MAP values at 50, using 5-fold cross-validation.
models. The best performing model over all k values is the
SANN(PC) model (with significance at the p < 0.01 level).
Both NN and SANN models stabilize their performance as
k increases, which conforms to the theory of stability for
k-nearest neighbor algorithms [1]. The decrease in performance of SANN as k increases is due to the inclusion of noisy
ratings in the training data (along with ground-truth ones),
while the performance of NN increases with k as no noise
is included in this case. For the evaluation on the held-out
sets (next section), the following values were selected based
on cross-validation: k = 1, λ = 7 for the SANN(PC) model
and k = 28, λ = 19 for the NN(PC) model.

Table 4: Statistics of the TED dataset [8].

5. EXPERIMENTAL RESULTS
We evaluate the SANN model, comparing it with several
baselines, and show the following: (i) the use of sentiment
analysis of comments as additional training data improves
performance over competitive baselines; (ii) the RB sentiment classifier is the reason for the improvement as compared to a random classifier; and (iii) performance increases
with the number of comments.We present first the parameter selection using cross-validation (5.1), and then we evaluate the best configurations on the two held-out sets (5.2).

5.2

5.1 Parameter Selection
Both NN models and SANN models rely on two parameters: the first one is the k parameter (the neighborhood size)
and the second one is the λ parameter (the shrinking factor).
For the selection of k, we fixed λ = 60 for all models. For
the selection of λ, we then fixed k to the optimal values that
were obtained for each model. Moreover, two other options
can be selected, namely the similarity function (Pearson’s
correlation (PC) or cosine similarity (COS)) and the use of
normalization in Eq. 1 (noted as ‘norm’).
In Table 3, we present the results of 5-fold cross-validation
on the training set for six diﬀerent models, namely: TopPopular baseline which provides fixed recommendations based
on the popularity of items, NN(COS), normNN(PC), NN(PC),
SANN(COS) and SANN(PC). We also experimented with
other normalized NN models, but as their results were inferior, we do not discuss them here.
Figure 1 displays the eﬀect of the neighborhood size k on
the performance of the models. All models perform considerably better than the TopPopular, as expected from CF

775

Evaluation of the SANN Model

We compare the performance of the SANN model with
the best performing baseline NN model (NN(PC)) and with
the TopPopular baseline on the two held-out sets. We consider the two variants of the SANN model, namely randSANN(PC) and polSANN(PC) (also with k = 1 and λ = 7)
to demonstrate the value of aggregated polarities.
Figure 2 displays performance on the dense held-out test
set. The SANN model performs significantly better than
the baseline models on unseen data. The ordering of the
methods based on performance is the same as in the crossvalidation experiments. The randSANN(PC) model performs slightly better than the NN(PC) model (the diﬀerence
is not significant) but considerably worse than SANN(PC)
and polSANN(PC), demonstrating the utility of sentiment
analysis over comments.
Table 5 shows the results on both held-out test sets with
MAP, MAR and MAF scores. The polSANN(PC) model
outperforms significantly the other models, showing a relative improvement of 26.8% on mean average f-metric (MAF)
on the dense held-out set and 11.4% on the sparse held-out
set. When averaged over MAP values from 1 to 50, the
improvement is significant at the p < 0.01 level (t-test).

Methods
TopPopular
NN(PC)
randSANN(PC)
SANN(PC)
polSANN(PC)
Improvement

Dense held-out test set
MAP@50 MAR@50 MAF@50
3.85
13.52
5.99
5.67
18.07
8.63
5.88
17.79
8.84
6.90
20.72
10.35
7.29
22.01
10.95
+28.5%
+21.8%
+26.8%

Sparse held-out test set
MAP@50 MAR@50 MAF@50
3.61
13.48
5.70
5.23
18.06
8.11
5.22
17.56
8.05
5.69
18.85
8.75
5.89
19.48
9.04
+12.6%
+7.8%
11.4%

Table 5: Performance of various methods on the two held-out test sets. The last row displays the improvement
of the polSANN model over the best NN model.

Figure 2: Method comparison in terms of average
precision (AP) and recall (AR) (1 to 50).

Figure 3: SANN models’ performance (MAP at 50)
when varying the number of comments for training.

The final experiment concerns the learning ability of the
algorithm in relation to the proportion of comments used for
training. Figure 3 displays the performance of two SANN
models on the dense held-out set, when varying the proportion of training comments for each user. The increase in the
proportion of comments leads to a clear increase in performance. When fewer than 20% of the comments are taken
into account, the models perform similarly to the NN(PC)
model for neighborhood size k = 1 (see also Fig. 1).

content. In 12th Int. Workshop on the Web and
Databases, Rhode Island, USA, 2009.
C. Leung, S. Chan, F.-L. Chung, and G. Ngai. A
probabilistic rating inference framework for mining
user preferences from reviews. World Wide Web, 2011.
A. Messenger and J. Whittle. Recommendations based
on user-generated comments in social media. In 3rd
Int. Conf. on Social Computing, Boston, USA, 2011.
R. Pan, Y. Zhou, B. Cao, N. Liu, R. Lukose,
M. Scholz, and Q. Yang. One-class collaborative
filtering. In 8th Int. Conf. on Data Mining, Pisa, Italy,
2008.
N. Pappas, G. Katsimpras, and E. Stamatatos.
Distinguishing the popularity between topics: A
system for up-to-date opinion retrieval and mining in
the Web. In 14th Int. Conf. on Intelligent Text Proc.
and Computational Linguistics, Samos, Greece, 2013.
N. Pappas and A. Popescu-Belis. Combining content
with user preferences for TED lecture recommendation. In 11th Int. Workshop on Content Based
Multimedia Indexing, Veszprém, Hungary, 2013.
J. Wang, Q. Li, Y. P. Chen, and Z. Lin.
Recommendation in Internet forums and blogs. In
48th Annual Meeting of the Association for
Computational Linguistics, Uppsala, Sweden, 2010.
T. Wilson, J. Wiebe, and P. Hoﬀmann. Recognizing
contextual polarity in phrase-level sentiment analysis.
In Conf. on Human Language Technology and
Empirical Methods in Natural Language Processing,
Vancouver, Canada, 2005.
W. Zhang, G. Ding, L. Chen, and C. Li. Augmenting
Chinese online video recommendations by using
virtual ratings predicted by review sentiment
classification. In Int. Conf. on Data Mining
Workshops, Washington, USA, 2010.

[4]

[5]

[6]

6. CONCLUSION AND FUTURE WORK
The proposed SANN models were shown experimentally
to be able to overcome some diﬃculties of the one-class collaborative filtering task and to outperform significantly several competitive baselines. Moreover, they demonstrated
robustness to noise and exhibited appropriate learning abilities with respect to the amount of available user-generated
text. The results of this study suggest several directions
for future work: (i) extending the results to other datasets;
(ii) learning the optimal mapping function from sentiment
analysis scores to ratings; and (iii) generalizing the proposed
approach to more advanced recommendation models.

[7]

[8]

[9]

7. ACKNOWLEDGMENTS
The work described in this article was supported by the
European Union through the inEvent project FP7-ICT n.
287872 (see http://www.inevent-project.eu).

[10]

8. REFERENCES
[1] O. Bousquet and A. Elisseeﬀ. Stability and
generalization. J. of Machine Learning Research, 2002.
[2] P. Cremonesi, Y. Koren, and R. Turrin. Performance
of recommender algorithms on top-N recommendation
tasks. In 4th Int. Conf. on Recommender Systems,
Barcelona, Spain, 2010.
[3] G. Ganu, N. Elhadad, and A. Marian. Beyond the
stars: Improving rating predictions using review text

[11]

776

