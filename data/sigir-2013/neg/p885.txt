A Novel Topic Model for Automatic Term Extraction
Sujian Li1
1

Jiwei Li1

Tao Song1

Wenjie Li2

Baobao Chang1

Key Laboratory of Computational Linguistics (Peking University), Ministry of Education,
School of Electronics Engineering and Computer Science, CHINA
2
The Innovative Intelligent Computing Center,
The Hong Kong Polytechnic University Shenzhen Research Institute, Shenzhen, CHINA
{lisujian, bdlijiwei, stao, chbb} @pku.edu.cn; cswjli@comp.polyu.edu.hk
obvious that frequency-based measures will keep many real terms
out of the door. In fact, a domain is described semantically from
various aspects. Again, let’s take the electric engineering domain
for example. The words like “input” emphasize some specific
topic in a domain while the words like “electrical” provide the
background of that domain. There also exist a cluster of words
like “NRZ” which occur in the corpus infrequently, but tend to
occur in a few documents frequently. Such words can reflect
some special characteristics of the domain. Based on these
observations, we argue that three semantic aspects can be used in
the representation of words: Domain background words (e.g.
electrical) describe the domain in general. Domain topic words
(e.g. input) represent a certain topic in a given domain. Domain
documents-specific words (e.g. NRZ) are specific to a small
number of documents and exhibit the characteristics of the
domain. We assume that a term can be recognized by identifying
whether its constituent words belong to some of the three
semantic aspects.

ABSTRACT
Automatic term extraction (ATE) aims at extracting domainspecific terms from a corpus of a certain domain. Termhood is one
essential measure for judging whether a phrase is a term. Previous
researches on termhood mainly depend on the word frequency
information. In this paper, we propose to compute termhood
based on semantic representation of words. A novel topic model,
namely i-SWB, is developed to map the domain corpus into a
latent semantic space, which is composed of some general topics,
a background topic and a documents-specific topic. Experiments
on four domains demonstrate that our approach outperforms the
state-of-the-art ATE approaches.

Categories and Subject Descriptors
H.3.1 [Information Storage and Retrieval]: Content Analysis
and Indexing – linguistic processing, Thesauruses.

General Terms
Algorithms, Experimentation.

As for semantic representation of words, unsupervised topic
models have shown their advantages [1] [3]. Latent Dirichlet
Allocation (LDA) is a well-known example of such models. It
posits that each document can be seen as a mixture of latent topics
and each topic as the distribution over a given vocabulary. To
trade-off generality and specificity of words, Chemudugunta et al.
[3] further defined the special words with background (SWB)
model that allowed words to be modeled as originating from
general topics, or document-specific topics, or a corpus-wide
background topic. The existing work proves that topic models are
competent for the semantic representation of words. However, to
our knowledge, no prior work has introduced such kind of
semantic representation to term extraction.

Keywords
Term Extraction, Topic Model, Termhood.

1. INTRODUCTION
So far, most researches on automatic term extraction have been
guided by two essential measures defined by [6], namely unithood
and termhood. Unithood examines syntactic formation of terms or
the degree (or significance) of the association among the term
constituents. Termhood, on the other hand, aims to capture the
semantic relatedness of a term to a domain concept. However,
there is no uniform definition of what is semantic relatedness, and
how to compute termhood is still an open problem.

Inspired by Chemudugunta’s idea of generality and specificity [3],
in this paper we propose a novel topic model, namely i-SWB to
model the three suggested semantic aspects. In i-SWB, three
kinds of topics, namely background topic, general topics, and
documents-specific topic are correspondingly constructed to
generate the words in a domain corpus. Compared with
Chemudugunta’s SWB model, there are two main improvements
in i-SWB to tailor to term extraction. First, specificity in i-SWB is
modeled at the corpus level and one documents-specific topic is
set to identify a cluster of idiosyncratic words from the whole
corpus. Thus, i-SWB avoids the computationally intensive
problem in SWB where the number of document-specific topics
grows linearly with the number of documents. Second, i-SWB
makes use of both document frequency (DF) and topic
information to control the generation of words, while SWB only
uses a simple multinomial variable to control which topic a word
is generated from. This improvement comes from the following
findings that have been verified in the experiments: the words
occurring in many documents and distributing over many general

Previous researches have attempted to measure termhood by
applying several statistical measures within a domain or across
domains, such as TF-IDF, C-value/NC-value [5], co-occurrence
[4] and inter-domain entropy [2]. These statistical measures often
ignore the informative words with very high frequency or very
low frequency and do not take into account the semantics carried
by terms. Taking the term “NRZ electrical input” in the electric
engineering domain for example, “NRZ” only occurs in a few
documents while “electrical” occurs in many documents
frequently. Using TF-IDF to measure termhood, low scores will
be assigned to both “NRZ” and “electrical”, which in turn causes
the term “NRZ electrical input” to have a low termhood. It is
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for
components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to
post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright © 2013 ACM 978-1-4503-2034-4/13/07…$15.00.

885

to set a variable (e.g. d in SWB) which is drawn from a
symmetric Dirichlet prior. As a rule of thumb, document
frequency (DF) information can be used as a determining factor to
examine the specificity or generality of words related to a domain.
In addition, we use the word distribution in general topics as
another factor to determine the specificity and generality of words.
Fig. 2 and 3 show the graphic model and the generation process of
i-SWB. Instead of d, a three-dimensional vector d,n is used to
control topic generation and its value is determined by an
experience function p ( DFwd ,n ,  wd ,n ) where DFw denotes the

topics with higher probability in LDA are likely to present
background information, while the words occurring in a few
documents only and distributing over a certain topic in LDA with
medium or low probabilities are usually idiosyncratic and provide
some special information of the domain. Next, with the semantic
representation of words in i-SWB, we implement an ATE system
which outperforms the existing ATE approaches.

2. MODEL DESCRIPTION
2.1 LDA and SWB Fundamentals

d ,n

document frequency of the word wd,n and  wd ,n denotes the

The hierarchical Bayesian LDA models the probability of a
corpus on hidden topics as in Fig. 1(a). The topic distribution of
each document θd is drawn from a prior Dirichlet distribution
Dir(α), and each document word wd,n is sampled from a topicword distribution  z specified by a drawn from the topicdocument distribution θd. The topic assignments z for each word
in the corpus can be efficiently sampled via Gibbs sampling, and
the predictive distributions for θ and  can be computed by
averaging over multiple samples.
u d ,n
D

d



z d ,n

wd ,n



t

wd ,n

Nd

zd ,n

d



d





T


(a)

t

B

0

1

probability vector that the word wd,n distributes over all the
general topics.
DF

d 0, ( d ,n )

 ( d ,n )

 ( d ,n )

0

d , ( d ,n )

p (  d ,n  1| w, x ( d ,n ) , z ( d ,n ) , 1 ,  ) 

p (  d ,n  2 | w , x ( d ,n ) , z  ( d ,n ) ,  2 ,  ) 

N d , ( d ,n )  3

N d 2, ( d ,n )  
N d , ( d ,n )  3






t

B

D

0

1

2

d



1. Draw  B ~ Dir ( 1 ) ,  D ~ Dir (  2 )
2. For each topic t  [1, T ] :

2

WT
wt , ( d ,n )
WT
w ' w ' t , ( d ,n )

draw  t ~ Dir (  0 )
3. For each document d  [1, D] :
draw  d ~ Dir ( )
(a)
(b)
For each word wd,n :
i. draw  d ,n ~ p( DFwd ,n ,  wd ,n )

w'

C

WB
w ' b , ( d ,n )

 V 1

 2

WD
wd , ( d ,n )
WD
w ' w ' d , ( d ,n )

C

C

ii. draw  d ,n ~ multi ( d ,n )
iii. draw :
if  d ,n  2 , draw wd ,n ~ multi ( )
D

if  d ,n  1 , draw wd ,n ~ multi ( B )
if  d ,n  0 , draw zd ,n ~ multi ( d )
and wd ,n ~ multi (

0

WB
Cwb
, ( d ,n )  1



zd ,n

Figure 2．Graphical model of i-SWB.

TD
td , ( d ,n )
TD
t ' t ' d , ( d ,n )

N d 1, ( d ,n )  

u d ,n

D

To formulate background and special information, Chemudugunta
et al. (2006) proposed the SWB model as illustrated in Fig. 1(b).
In SWB, the variable ud,n acts as a switch: if ud,n = 0, the current
word is generated by the general topics as in LDA, whereas if
ud,n=1 or ud,n=2, words are sampled from a corpus specific
multinomial or a document-specific multinomial (with symmetric
Dirichlet priors β1 and β2) respectively. ud,n is sampled from a
document-specific multinomial d , and it has a symmetric
Dirichlet prior γ. Applying a Gibbs sampler on SWB, we can get
the sampling equations for each word wd,n in document d:
N

C

C

(1)
p(   0, z  t | w,x


,z
, ,  ,  ) 
N
 3  C
 T  C
V 
d ,n

d ,n

wd ,n

Figure 1. Graphical models for (a) LDA and (b) SWB.

d ,n



 V 2

zd , n

)

0

Figure 3．Generation process of i-SWB.
To formally present the i-SWB model, let V be the vocabulary
size and D be the number of documents. There are T general
t
B
topics  (1  t  T ) , one background topic  and one

(2)
(3)

documents-specific topic  which have symmetric Dirichlet
priors of β0, β1 and β2 respectively. Each topic is characterized by
a distribution over the V words. α is the fixed parameter of
symmetric Dirichlet prior for the D document-topic multinomials
D

where –(d,n) indicates that the current word wd,n is excluded for
the count, V denotes the vocabulary size, Nd is the number of the
words in document d and Nd0, Nd1, and Nd2 are the number of the
words in document d assigned to the latent topics, background
topic and document topics, respectively. CtdTD is the number of the
WB
WD
and Cwd
words that are assigned topic t in document d. CwtWT , Cwb
are the number of the words that wd,n is assigned to topic t, to the
background topic and to the documents-specific topic respectively.

represented by a D×T matrix θ. Let wd,n be the observed variable
representing the nth word in document d, ud,n the hidden variable
denoting which kind of topic wd,n is assigned to, and zd,n the
hidden variable indicating that the general topic wd,n may be
assigned to.

2.2 i-SWB Model

2.3 Model Inference

In i-SWB, the documents-specific topic is defined at the corpus
level and the corpus is composed of three kinds of topics
including background topic, documents-specific topic and general
topics. To control the generation of these topics, a simple way is

We use a Gibbs sampler to perform model inference. Due to space
limitation, we give the result of Gibbs Sampling directly.

886

P(ud ,n  0, zd ,n  t | w, z(d ,n) ,d ,n , u(d ,n) ,)
 d(0),n 



TD
td ,( d , n)
TD
t ' t ' d ,(d ,n)

C

C

 T



 0

WT
wt ,(d ,n)
WT
w' w' t ,(d , n)

C



P (ud , n  1| w , z,  d , n , u ( d , n ) ,)   d(1), n 

C



P (ud , n  2 | w , z,  d , n , u ( d , n ) )   d(2), n 

(4)

V 0

WB
Cwb
,  ( d , n )  1
w'



CwWB' b ,  ( d , n )  V 1

WD
Cwd
,(d ,n)  2
w'

Step 2: Linguistic filtering: Since most terms are noun phrases
which consist of nouns, adjectives and some variants of verbs and
end with a noun word, here we present our method with the filter:
(FW | Verb | Adj | Noun)* Noun, where FW usually denotes the
unknown words.
Step 3: Frequency recording: for each term candidate, we record
its frequency occurring in the whole corpus and exclude those
occurring only once from the candidate list.

CwWD' d ,  ( d , n )  V  2

(5)
(6)

Now we get a list of term candidates with their frequency {ci, tfi}.
Suppose each candidate ci consists of Li words wi1 wi 2 ...wiLi . To

Eqs. (4), (5) and (6) are similar to Eqs. (1), (2) and (3), except the
first terms in each of them. Then, we determine  d , n , whose
value depends on DFw

d ,n

compute termhood, each candidate is scored according to tfi and
the results of i-SWB model. According to the values of  t ,  B

and  wd ,n as follows:

 d(0),n  (1  E ( w )) 

DFwd ,n

d ,n

D

and  D in Eqs. (9), we extract the top H (e.g. 200) highest
distributed words for each topic, namely Vt, VB and VD, which can
be seen as the typical words of the corresponding topic. An
intuitive idea is that, a good candidate should be composed of
typical words which are representative of a certain topic. Thus,
the termhood of ci is computed as:

 d(1),n  E ( w ) ;

;

d ,n

DFwd ,n

 d(2),n  (1  E ( w ))  (1 

)
(7)
D
) is used to measure the evenness of distribution
d ,n

where E ( w

d ,n

that wd,n is assigned to each general topic.
C wWdT, n t

log
WT
t T  C w d ,n t '
t'
E (  wd ,n ) 
lo g T

Termhood ( ci )  log(tf i ) 

C wWdT, n t

C
t'



1 j  Li , w j {Vt }tT { B , D }

mt w j  arg max( wt j )

WT
wd ,n t '

mt w j

w

j

(10)

t T  { B , D }

(8)

where mtw j denotes the topic which wj is most likely to be

where CwWTd ,n t denotes the number of times that wd,n is assigned to

assigned. To compute ci, we only consider those constituent
words that are typical of a certain topic. Then the candidates with
the highest termhood values are taken as terms.

the general topic t. In Eq. (8), the numerator computes the entropy
that the word wd,n is assigned to the general topics, and the
denominator denotes the maximum entropy value that wd,n is
evenly assigned to each general topic. The range of E ( wd ,n ) is

4. EXPERIMENTS
4.1 Experiment Setup

(0,1]. The experience Equations (7) and (8) reflect that a
background word is more likely to uniformly distribute on general
topics and a documents-specific word is more likely to have a
larger DF value. With one Gibbs sampling, we can also make the
following estimation:
WT
CtdTD  
Cwt
 0
, wt 
,
 dt 
TD
 t ' Ct ' d  T
 w ' CwWT' t  V 0
WB
Cwb
 1
, wD 
 
WB


C
V
 w' w 'b
1
B
w



WD
Cwd
 2
WD

w'

C w ' d  V 2

We evaluate the i-SWB based term extraction method on four
domain specific patent corpora, including molecular biology
(C12N), metallurgy(C22C), electric engineering(G01C) and
mechanical engineering(H03M). Statistics of the documents in
each domain are summarized in Table 1.
Table 1. Description of corpora
No. of
documents
CorpusC12N molecular biology
1,880,739
11,496
CorpusC22C
metallurgy
1,915,066
12,226
1,741,820
9,462
CorpusG01C mechanical engine.
935,059
5,492
CorpusH03M electric engine.
It is difficult to collect a complete list of domain terms which is
necessary for evaluation. Inspired by the pooling technique used
in Information Retrieval (IR), we semi-automatically construct a
lexicon for each domain. For each domain, every baseline system
and our system submits one term list. The five baseline systems
used will be introduced in the next subsections. We take the top
500 terms from every system to form the pool for that domain.
Then from the pool, four graduate students majoring in the
corresponding domain manually pick out the correct ones to form
a pseudo-lexicon for each domain. The sizes of the pseudolexicons are 1473, 1380, 1509 and 1370 for molecular biology,
metallurgy, mechanical engineering and electric engineering.
Then, we compare system performances with the popular IR
evaluation measures - precision at 6 different cut-off values P@n,
where n=50, 100, 200, 300, 400, 500.

(9)

Corpus

In our experiments, we set T=20,  =50/T, 0=0.1, 1=0.01 and
2=0.01. We initialize the corpus by sampling each word from the
general topics and run 1000 iterations to stabilize the distribution
of z and u.

3. TERM EXTRACTION
This section will introduce the proposed i-SWB based term
extraction. As stated in Section 1, the ATE process is usually
guided by two measures: unithood for acquiring term candidates
and termhood for further identifying terms from the candidates.
Following the work of Frantzi [5], the unithood technique adopted
in our work is mainly based on a linguistic filter as the following
three steps:
Step 1: Part-of-speech (POS) tagging: We use a Maximumentropy POS tagger 1 implemented by Standford NLP group to
assign a grammatical tag (e.g. noun, verb, adjective etc.) to each
word in the corpus.
1

http://www-nlp.stanford.edu/software/tagger.shtml

887

Domain

No. of words

technique performs unstably: better than TerMine in the
metallurgy domain, worse than TerMine and TermoStat in the
domain of mechanical engineering. This may suggest that the
performance of TF-IDF heavily depends on the corpus quality.

4.2 Comparison with Other Topic Models
The i-SWB model is the key of our ATE system and we compare
it with two commonly used topic models, i.e. SWB (Baseline 1)
and LDA (Baseline 2). Except the construction of topic model,
the whole process of term extraction is the same as introduced in
Section 3. Fig. 4 below illustrates the P@n values of 3 different
topic models on four domains. The blue bars represent the P@n
values of our system, the red bars represent the SWB model, and
the green bars represent the LDA model. From the figure, we can
see that our model significantly outperforms the other two models.
Taking the P@50 measure for example, the relative improvement
of i-SWB over SWB in the domains of molecular biology,
metallurgy, mechanical engineering and electric engineering
reaches respectively 12.2%, 11.9%, 9.8% and 11.4% respectively.

Domain of molecular biology

Domain of mechanical engineering

Domain of metallurgy

Domain of electric engineering

Figure 5. Comparison with other ATE approaches.
Domain of molecular biology

Domain of mechanical engineering

5. CONCLUSION

Domain of metallurgy

In this paper, we argue that the termhood measure should take
consideration of semantic information. To cater to this idea, we
design a novel topic model i-SWB to map the domain corpus into
the latent semantic space, which includes general topics,
background topic and documents-specific topic. Based on i-SWB,
we implement our ATE system and evaluate it on four domains
(i.e. molecular biology, metallurgy, mechanical engineering, and
electric engineering). The experimental results show that our
approach outperforms the state-of-the-art ATE approaches.

Domain of electric engineering

Figure 4. Comparison of different topic models.

When further analyzing the terms that are wrongly identified by
each system, we find that if SWB is applied, the word “signal”
reflecting some general topic is assigned to some documentspecific topics due to its high frequency in some documents and
then the phrases such as “original signal” and “resulting signal”
are wrongly identified as terms. On the other hand, if LDA is used,
some general words such as “purpose” and “problem” which
occur frequently in the corpus are assigned to specific topics and
the phrases of “signal purpose” and “problem solving” are
wrongly identified as terms. These kinds of problems can be
overcome in i-SWB. However, the terms identified by i-SWB that
are formed by sequences of several typical words are not always
the real terms. For example, “signal” and “component” are both
correctly assigned to general topics with higher probability but
“signal component” is not a real term. To solve this problem,
semantic representation of terms is worth exploring.

6. ACKNOWLEDGMENTS
This research has been supported by NSFC grants (No. 61273278
and 61272291), National Key Technology R&D Program
(No:2011BAH1B0403),
National
863
Program
(No.
2012AA011101) and National Social Science Foundation (No:
12&ZD227),. We also thank the anonymous reviewers for their
helpful comments. Corresponding authors: Sujian Li and Baobao
Chang.

7. REFERENCES
[1] Blei, D. M., Ng, A. Y., Jordan, M.I. 2003. Latent Dirichlet
allocation, The Journal of Machine Learning Research, pp.
993-1022.
[2] Chang J.-S. 2005. Domain specific word extraction from
hierarchical web documents: a first step toward building
lexicon trees from web corpora. In Proceedings of the 4th
SIGHAN Workshop on Chinese Language Learning: 64-71.

4.3 Comparison with Online ATE Service
We also compare our system with three state-of-the-art ATE
systems. We use two online free systems - TerMine 2 and
TermoStat3, as Baseline 3 and Baseline 4. TerMine uses the Cvalue/ NC-value to compute termhood, while TermoStat is based
on a statistical test and the target items are highly specific to the
domain corpus being analyzed. In addition, we implement a
baseline system (Baseline 5) which adopts TF-IDF in termhood
computation. Fig. 5 compares our system with the three baseline
systems over the P@n values and shows that our system
obviously performs better than the other three. This verifies that
the semantic analysis of words are helpful to the ATE task.
TerMine performs slightly better than TermoStat. But the TF-IDF
2
3

[3] Chemudugunta, C., Smyth, P. and Steyvers, M. 2006.
Modeling general and specific aspects of documents with a
probabilistic topic model. In NIPS 19, pp. 241–248.
[4] Hisamitsu, T., Niwa, Y., and Tsujii, J. 2000. A method of
measuring term representativeness - baseline method using
co-occurrence distribution, COLING 2000, pp.320-326.
[5] Frantzi, K., Ananiadou, S. and Mima, H. 2000. Automatic
recognition of multi-word terms: the C-value/NC-value
method. International Journal of Digital Libraries, 3(2): 117132
[6] Kageura, K. and Umino, B. 1996. Methods of automatic term
recognition. Terminology, 3(2).

http://www.nactem.ac.uk/software/termine/
http://idefix.ling.umontreal.ca/~drouinp/termostat_web/

888

