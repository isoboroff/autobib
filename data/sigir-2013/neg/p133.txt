An Effective Implicit Relevance Feedback Technique Using
Affective, Physiological and Behavioural Features
Yashar Moshfeghi

Joemon M. Jose

School of Computing Science
University of Glasgow
Glasgow, UK

School of Computing Science
University of Glasgow
Glasgow, UK

Yashar.Moshfeghi@glasgow.ac.uk

Joemon.Jose@glasgow.ac.uk
General Terms: Performance, Experimentation
Keywords: Implicit Relevance Feedback, Affective, Physiological, Behavioural, Dwell Time, Search Intentions

ABSTRACT
The effectiveness of various behavioural signals for implicit
relevance feedback models has been exhaustively studied.
Despite the advantages of such techniques for a real time information retrieval system, most of the behavioural signals
are noisy and therefore not reliable enough to be employed.
Among many, a combination of dwell time and task information has been shown to be effective for relevance judgement prediction. However, the task information might not
be available to the system at all times. Thus, there is a need
for other sources of information which can be used as a substitute for task information. Recently, affective and physiological signals have shown promise as a potential source
of information for relevance judgement prediction. However, their accuracy is not high enough to be applicable on
their own. This paper investigates whether affective and
physiological signals can be used as a complementary source
of information for behavioural signals (i.e. dwell time) to
create a reliable signal for relevance judgement prediction.
Using a video retrieval system as a use case, we study and
compare the effectiveness of the affective and physiological
signals on their own, as well as in combination with behavioural signals for the relevance judgment prediction task
across four different search intentions: seeking information,
re-finding a particular information object, and two different
entertainment intentions (i.e. entertainment by adjusting
arousal level, and entertainment by adjusting mood). Our
experimental results show that the effectiveness of studied
signals varies across different search intentions, and when
affective and physiological signals are combined with dwell
time, a significant improvement can be achieved. Overall,
these findings will help to implement better search engines
in the future.

1.

INTRODUCTION

It has been understood for some time that the formulated
queries in an information seeking process do not always provide an adequate description necessary to retrieve relevant
documents [39], since it is only an approximation of the actual information need [33]. Likewise, queries formulated may
not adequately define the characteristics of relevant documents, or indeed any relevant information because of an
ill-defined information need situation. Since more complete
representations generally lead to more precise search results,
supplementing textual queries with additional sources of information, such as documents users have found relevant [26],
can be advantageous [37]. Research has shown that searchers
can specify the relevance of a document to their needs more
easily than explicitly specifying their information needs [31].
Therefore, in order to help the searcher, IR systems have previously employed a range of relevance feedback techniques,
where feedback is gathered through explicit [20], implicit
[37], and/or affective feedback [1].
Despite the robustness of explicit feedback in improving
retrieval effectiveness [20], it is not always applicable or reliable due to the cognitive burden that it places on users
[38]. To overcome this cognitive burden, implicit relevance
feedback (IRF) is proposed; with relevance inferred instead
from behaviour signals (i.e. interactional data) in an indirect and unobtrusive manner [18]. For example, researchers
try to understand how dwell time (the time spent by a user
on a retrieved document) [19] and task [37] relate to relevance. Dwell time on a document has got a particular interest since it can be detected in real-time [22]. Recent studies
have shown that dwell time combined with task information
can improve IRF algorithm performance [37]. However, in a
real-time situation, task information might not be apparent
or easy to obtain in many cases. Hence, there is a need for
other sources of information which can be used as a substitute for the task information.
Due to recent advances in technology, new sensory devices
have been developed. It is now possible to leverage affective
and physiological signals from user while they are interacting
with a system. For instance, facial expression recognition,
galvanic skin response, heart rate monitoring and electroencephalography provide more information about interaction
than ever before. Currently, there are numerous efforts in

Categories and Subject Descriptors: H.3.3 Information
Storage and Retrieval - Information Search and Retrieval Search Process
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

133

disciplines that extend beyond IR to understand how such
information can be used to improve human-machine interactions, e.g., [29]. The output of these sensory devices has
gained notable attention from the IR community, in particular for their effectiveness in predicting relevance. As a
result of such studies, affective feedback has been proposed
[1] where the intention is to capture facial expression [4],
and physiological signals [3] (such as skin temperature) and
use them as implicit relevance judgement. Despite the great
potential of this information for IRF techniques, the accuracy of such information in the past was not high enough to
be used on its own in a real time system.
Our hypothesis is that signals obtained from sensory channels can be considered as a suitable replacement of task information for IRF techniques based on dwell time information. This hypothesis is motivated by the findings of recent
studies from both Poddar et al. [30] where they show that
tasks with different characteristics stimuli different emotions
in the searcher, and Arapakis et al. [1] where they show that
relevance information that derives from the selected sensory channels is correlated to user affective behaviour. They
investigated whether they can deduce topical relevance by
measuring physiological signals taken from the participants,
in the context of online information seeking. However, the
effect of combining signals obtained from sensory devices
with behavioural data has yet to be studied. Successful results steer the situation for using IRF in a real-time system.
This paper is an attempt in this vein of research.
To further investigate our hypothesis, we study the effectiveness of such IRF techniques for search tasks with different intentions. Recent advances in query log analysis for
search engines such as Google, Yahoo! and Bing has shown
that Web searchers’ intentions do not always fit into the typical taxonomy of informational, navigational or transactional
intentions [34]. Although informational (i.e. to acquire information present on one or more sites), navigational (i.e.
to reach a particular site) and transactional (i.e. to perform
some Web-mediated activity) intentions are all commonly
found in query logs [7], there is an increasing body of evidence showing that re-finding and re-retrieving the visited
information is a regular activity by searchers [34, 12]. In
addition, there is also recent interest in the notion of entertainment, where the searchers do not have any particular
information need [13] and the prime motivation behind their
search is to satisfy an emotion need [27]. Thus the effectiveness of these sources of information on their own and in
combination with behavioural data (i.e. dwell time) has yet
to be studied for search processes with different intentions,
i.e. information seeking, re-finding information and/or entertainment.
Therefore, two important questions emerge. First, whether
combining sensory channels and behavioural signals provide
a reliable set of features for IRF techniques. Second, what
would be the effect of different search intentions (e.g. seeking
information, re-finding or entertainment) on the relevance
judgement predictability of this set of features. In order
to investigate our research questions, we use an interactive
information retrieval evaluation framework. In particular,
we devised four search tasks each of which cover a search
intention. We then capture affective and physiological signals of the participants while they were performing the tasks
along with their relevance assessment for the visited videos.
Finally, we investigate the discriminative power of the cap-

tured signals with and without dwell time for predicting the
relevance of the assessed videos across different search intentions.
This paper has three novel contributions. First, we studied and compared the discriminative power of multitude sensory channels (i.e. EEG, heart rate, facial expression, skin
temperature, etc.) for relevance judgement prediction task.
Second, we investigated the possibility of substituting task
information with these sensory signals for dwell time based
IRF techniques. Finally, we investigated the prediction accuracy of these signals for search processes with different
intentions. The rest of the paper is organised as follows: related work is presented in Section 2, the experiment methodology is described in Section 3. Results and discussion are
presented in Section 4, and finally, the paper is concluded
in Section 5.

2.

RELATED WORK

Dwell Time and IRF: Much past research has attempted finding a reliable set of features for IRF techniques
[18]. One of the main features investigated researchers is
dwell time, due to its applicability for real time systems
[22]. Despite early studies by Morita et al. [25] and Claypool et al. [8] where they proposed dwell time as a reliable
behavioural signal for IRF, later studies conducted by Kelly
and Belkin [19] show that dwell time by its own is not reliable and that it can differ significantly according to specific
task, and according to specific user. More recent studies in
this path by White and Kelly [37] show that a combination
of dwell time and task information is a reliable source of information for IRF techniques. However, information about
the search task is not available all the time. Thus many
approaches try to predict this information. One of the common way to do so is to use the behavioural signals such as
dwell time [21]. However, this approach cannot be useful if
the predicted task information is required to be combined
with the dwell time itself for IRF. Therefore, clearly there
is a need for other sources of information that can be substituted with the task information for dwell time based IRF
techniques. This paper is an attempt to investigate this research question.
Task and Emotion: In recent years the relation between the search tasks within an information seeking process
(ISP) and emotion has gained much attention. For example, several works studied the emotional impact of search
tasks within the ISP [30, 2, 23]. Poddar and Ruthven [30]
examined how participants emotion responses are influenced
by tasks of different natures. In particular, their results indicate that (i) artificial tasks have higher uncertainty and
less sense of ownership than genuine search tasks, and (ii)
more complex search tasks have lower positive emotions and
more uncertainty before and after searching. Similar findings were earlier reported also by [2] in an information seeking activity. They concluded that users’ emotions progressively transit from positive to negative valence as the degree of task difficulty increases. They also found that emotions both interweave with different physiological, psychological and cognitive processes during an information seeking process, and form distinctive patterns according to specific tasks. However, [23] reported no significant relationship
between searchers’ mood and search tasks due to the complexity involved in such studies. In another study, [17] investigated the relationship between the subjective (e.g. hap-

134

piness levels, feeling lost during search, etc.) and objective
(e.g. search outcomes and search task characteristics, etc.)
factors in the ISP. Their results show that “higher happiness
levels before the search and during the search correlate with
better feelings after the search, but also correlate with worse
search outcomes and lower satisfaction, suggesting that, perhaps, it pays off to feel some ‘pain’ during the search in order to ‘gain’ quality outcomes” [17]. These valuable insights
into the relation between emotion and search task motivated
our research. This paper investigates the possibility of substituting task information with experienced emotion to be
combined with dwell time in order to have a reliable real
time features for IRF techniques. The question that might
be raised is how one can capture human emotion.
Affective Relevance Feedback: Recent advances in
information technology have facilitated the development of
sensory channels to capture human emotion [28, 29, 11]. The
argument is that such changes are often expressed through
a psycho-physiological mobilisation that is reflected by a series of more or less observable cues, such as facial expressions, body movements, localised changes in the electrodermal activity, variations in the skin temperature, and many
more [3]. The affective and physiological (A&P) signals
obtained from such devices have provided researchers with
additional sources of information not previously available.
Thus these A&P signals have gained significant attention in
the IR community, and their effectiveness has been studied
for numerous tasks and purposes, e.g [1, 3, 4].
The most related work to this paper are the studies conducted by Arapakis and his colleagues on the effectiveness of
A&P information for relevance judgment prediction (known
as affective feedback) [2]. For example, Arapakis et al. [4]
show that affective features captured from facial expression
during an information seeking process can be used to develop
a multimodal recommendation system. In another study,
Arapakis et al. [3] explored the role of affective (captured
via facial expression) and physiological signals (captured via
skin temperature and body movement) in designing multimedia search systems. They investigated whether topical
relevance could be deduced by measuring key physiological
signals taken from the user. Finally, In a more recent study,
the role of affective signals was studied in a personalised
system [1] for topical relevance.
However, two main limitations can be associated with
these studies. First, the accuracy of A&P information in
the past was not high enough to be used by its own in a real
time system. For example, Feild [16] found little benefit
from the physiological signals when they tried to used them
for searcher frustration prediction where three physical sensors were used: a mental state camera, a pressure sensitive
mouse, and a pressure sensitive chair. Given the relation
between task characteristics and stimulated emotion in the
searcher (explained above) and the possibility of extracting
emotion using A&P sensory channels, we hypothesis that
a combination of A&P signals with behavioural signals (in
particular dwell time) can lead to a reliable set of features
for IRF. Second, the effectiveness of A&P information was
only investigated for information seeking scenarios where a
clear information need is defined (i.e. topical relevance).
Thus the effectiveness of such sensory channels for a more
realistic representation of real life search process has yet to
be studied, where variety of tasks with different intentions

are proposed. This paper is an attempt to investigate this
research problem.
Seeking Information on the Web: Traditionally, the
general belief of the Information Retrieval (IR) community
was that the dominant intention of people engaging in search
processes was informational [7]. However, Broder [7] shows
the existence of search intents other than the assumed informational intent, i.e. navigational and transactional. In more
recent studies, researchers have identified a wider range of
search intents that go beyond Broader’s taxonomy of intentions, i.e. re-finding [34] and entertainment [13]. Re-finding
the visited information has been shown to be a regular activity by searchers [34, 12]. Teevan et al. [34], by analysing
Yahoo! Web query logs showed that up to 40% of the submitted queries were re-finding queries. Elsweiler et al. [12],
in an empirical study, showed that it is possible to isolate
re-finding behaviour in search logs through various qualitative and quantitative analyses. Given this evidence, it is
important to study the effect of affective, physiological and
behavioural signals in the re-finding intention.
In addition, recently there is an increased effort in the
research community to explore search processes with entertainment intents, e.g. “Entertain Me” [5] and “Search4FUN”
[14] workshops. Besides the works published in these workshops, [13] attempted to understand the needs and motivation underlying leisure-based activities in the context of
television viewing. They reported that the nature of the
need and motivation are different between leisure-based and
work-based situations where most of the needs reported for
leisure-based situation are motivated by a desire to change
mood, emotion, or arousal level. Further research related to
leisure-based activity is the work by [32] which studies pleasure reading behaviour. Their results show that pleasure
readers find information without having any prior information need. In addition, [40] and in a more in-depth study,
[15], reported that in casual search, the motivation is not to
resolve an information need, but hedonistic, e.g. entertainment driven.
Given the emergence of entertainment seeking intentions
on the Web and their unexplored characteristics, it is also
important to study the effect of affective, physiological and
behavioural signals in entertainment seeking intentions. The
defined entertainment seeking intentions in this paper are
based on research done by Zillmann [36], who was instrumental in the establishing a range of theories in this domain. Zillmann and his colleagues posit that entertainment
choices are a reflection of a basic human need to enhance or
retain positive states, and to lessen or steer clear of negative
ones [27]. They have suggested two possible states in which
there may be a need for regulation: physiological arousal and
affect. In the case of physiological arousal, their study suggests that users might be over-stimulated (i.e., stressed) or
under-stimulated (i.e., boredom). In the case of affect states,
it suggests that users might be in negative (i.e. dysphoric)
or positive (i.e. upbeat) moods. An individual experiencing
such states will choose their entertainment content according to their expectations of what would lead them back to an
optimal state [27]. Thus, we investigate two entertainment
seeking intentions: entertainment by adjusting arousal level
and entertainment by adjusting mood.

135

3.
3.1

the previous one is that in both cases searchers have an information need. For this search task, we prepared a number
of videos that covered a variety of contexts in order to capture participants’ interests as best as possible. The videos
were intentionally selected as they would likely be very hard
to find because they lacked textual description. The motivation was to simulate a challenging information re-finding
process where the recalled terms are very ambiguous and do
not lead directly to the relevant item. This would better
represent many realistic re-finding tasks, when the user cannot recall the exact description of the item they are looking
for. The simulated search scenario for INF task was as follows: “Imagine you are discussing a video which you have
seen few days ago with your friends. They are interested in
seeing the video and have asked you to send them a link to it.
You can remember the content of the video but you cannot
remember its title or any textual information which can help
you in retrieving it.” Each participant was then asked to
select and watch one of the three videos presented to them
(an animal1 , martial arts2 , or a science video3 ) that they
are unfamiliar with and consider interesting. Once participants watched the video, they were asked to find it as fast
as possible.
ENA Task: This search task simulates the entertainmentbased search intent where searchers adjust their arousal level.
The main difference between this task and the two tasks before is that the primary need is hedonistic rather than informational. Therefore, to accurately simulate such search processes, we avoid introducing any explicit information need.
Thus, for this search task, we did not prepare any specific
search topic. This decision is motivated by the literature in
sociology [27] and the information seeking and retrieval domain on entertainment [13]. The simulated search scenario
for the ENA task was as follows: “Imagine you are working
in a factory as a night-guard. You have just finished your
routine checks and will be taking out more checks shortly.
You are tired so you decide to watch some videos to wake
yourself up and make yourself ready for your next round of
checks.” Each participant was asked to find as many relevant
videos as possible that make them feel excited.
ENM Task: This search task simulates the entertainmentbased search intent where searchers adjust their mood. Similar to ENA task, searchers engage in such search processes
with hedonistic rather than informational needs. Therefore,
to accurately simulate this search process, we avoid introducing any explicit information need. Thus, similar to the
ENA task, we did not provide any pre-prepared search topic.
The simulated search scenario for the ENM task was as follows: “Imagine your boyfriend/girlfriend is travelling and
communication access is very limited. It is now a few days
since he/she has gone and you are missing him/her very
much. You are feeling very sad and in order to change your
mood, you have decided to watch some videos.” Each participant was asked to find as many relevant videos as possible
that make them feel happy.
Questionnaires: At the beginning of the experiment,
the participants were given an entry questionnaire. This
gathered background and demographic information, and inquired about previous experience with online videos, in particular, browsing and searching habits including their inten-

EXPERIMENTAL METHODOLOGY
Experiment Design

This study used a within subject design. The independent
variable was the search intents (with four levels: “Information Seeking” (INS), “Information Re-finding” (INF), “Entertainment by adjusting Arousal” (ENA), “Entertainment
by adjusting Mood” (ENM)), which was controlled by the
simulated search task given to the participants (see Section
3.2). We did not perform any control on the number of relevant and irrelevant results, in order to simulate a real search
scenario situation as much as possible. The dependent variables were: (i) task perception and (ii) relevance judgment
prediction accuracy.

3.2

Tasks

Four search tasks were devised, each simulating a search
intent. The structural framework of simulated need situations [6] were used to present search tasks. By doing so,
short cover stories were introduced that helped us describe
to our participants the source of their information need, the
problem to be solved and the environment of the situation.
This facilitated a better understanding of the search objective and introduced a layer of realism, while preserving welldefined relevance criteria. In the following, each of the search
tasks is explained in detail.
INS Task: This search task simulates the information
seeking search intent. Information seeking is the most studied search intent in the information seeking and retrieval
domain. This task is designed as a control group since the
majority of studies on relevance judgement prediction in the
past were based on a similar search task intent. For this
search task, we prepared a number of search topics that covered a variety of contexts in order to capture participants’
interests as best as possible. The topics, presented in Table 1, were all checked manually prior to the experiment,
to ensure the availability of relevant documents. The simulated search scenario for INS task was as follows: “Imagine
you have graduated recently and are going to interview for
a job in a local company. As part of the interview process,
you are asked to explain and expand on the area you will be
working on. You feel very enthusiastic about the interview;
however, due to your lack of knowledge you would like to find
out more about this particular topic before taking part in the
interview.” Each participant was then asked to choose one
of the topics that they were unfamiliar with but consider
interesting. Using the video retrieval system, they had to
find as many relevant videos as possible so that they could
acquire a good knowledge about their selected topic.
Table 1:

Search topics for the information seeking task scenario.

Obtaining information regarding contraception methods.
Investigating new knowledge on global warming.
Formulating an opinion about existing social networking sites.

INF Task: This search task simulates the information
re-finding search intent. There are two differences between
this task and the previous one: (i) there is only one document that can satisfy the information need of the searcher,
and (ii) the searcher has seen the relevant documents at
some point before initiating the search process and is now attempting to re-find it. The similarity between this task and

1

www.youtube.com/v/wx7rY11qb0k&showinfo=0
www.youtube.com/v/pEUeP7hx8fs&showinfo=0
3
www.youtube.com/v/P8Cs2tO5w74&showinfo=0
2

136

tions. At the end of each task, the participants completed
a post-task questionnaire to elicit the subject’s viewpoint
on certain aspects of the search process, in particular their
perception of the encountered task. All of the questions
included in these questionnaires were a forced-choice type.
Finally, an exit questionnaire was introduced at the end of
the study. In this questionnaire we gathered information
about the encountered system as well as the user study in
general: which task they preferred and why, and their general comments about the user study.
Procedure: The user study was carried out in the following manner. The formal meeting with the participants
took place in a laboratory setting. At the beginning of
the session the participants were given an information sheet
which explained the conditions of the experiment. They
were then asked to sign a Consent Form and were notified
about their right to withdraw at any point during the study,
without affecting their legal rights or benefits. Then, they
were given an Entry Questionnaire to fill in.
The session proceeded with a brief tutorial on the use of
the search interface. Then participants were helped to wear
the sensory devices, followed by a calibration of the webcamera. After completion of this step, each participant had
to complete four search tasks (explained in Section 3.2), one
for each level of search process intentions (see Section 3.1).
To negate the order and fatigue effects we counter-balanced
the task distribution using a Latin Square design. The subjects were asked every time to provide judgment for any
video that they watch, and were given 10 minutes to complete their task, during which they were left unattended to
work. At the end of each task, the subjects were asked to
complete a post-task questionnaire. Questions in the posttask questionnaire were randomised to avoid the effect of
fatigue. Between each task, a cooling-off period was applied
to minimise the carry-over effect. An exit questionnaire was
administered at the end of the session. Finally, the participants were asked to sign a payment form, prior to receiving
the payment of £12.
Each study took approximately 120 minutes to complete;
this is from the time they accepted the conditions until they
signed the payment receipt. Users could only participate
once in the study. The total cost of the evaluation was £348,
including the cost of the pilot studies. A user study with the
procedure explained above was conduced over a period of 10
days from 16th to 26th of July 2012. The results of these
studies are presented in Section 4.
Apparatus: For our experiment we used one desktop
computer, equipped with a monitor, keyboard, and mouse.
The computer provided access to a custom-made video retrieval system explained in Section 3.4. The web-camera
(Creative Live! Cam Optia AF with a 2.0 megapixels sensor) was used in combination with eMotion [35], for realtime facial expression analysis. In addition, we used three
unobtrusive wearable devices to capture participants’ physiological signals: (i) Polar RS800 Heart Rate Monitor, and
(ii) BodyMedia SenseWear R Pro3 Armband (iii) NeuroSky
MindKit-EMTM headset. All devices and systems were logging using a common system time. Finally, we used entry,
post and exit questionnaires in each session.

3.3

vated by the findings of previous studies in IR related to
this paper [1] as well as findings from other domains showing that emotions are primarily communicated through facial expressions [28] and facial expressions can be associated
with universally distinguished emotions, e.g. happiness, sadness, anger, fear, disgust, and surprise [11]. We applied a
real-time facial expression analysis using an automatic facial expression recognition system with emotion-detection
capabilities (i.e. eMotion) [35]. The process of recognition
occurred as follows: initially, eMotion would locate certain
facial landmark features (eyebrows, corners of the mouth,
etc.) and construct a 3-dimensional wireframe model of the
face, consisting of surface patches wrapped around it. After construction of the model, head motion or any other
facial deformation would be tracked and measured in terms
of motion-units (MU’s), and, finally, classified into one of
the six detectable emotion categories plus neutral. eMotion applies a generic classifier that has been trained on
a diverse data set, combining data from the Cohn-Kanade
database. The main advantage of this system is its reasonable performance across all individuals, irrespective of
the variation introduced from mixed-ethnicity groups. Results of the person-dependent and person-independent tests
presented in [35] support our performance-related assumptions. For additional information regarding the advantages
and limitations of eMotion the reader is referred to [35].
Physiological Signals: In order to capture physiological signals we consider multiple sensory channels including
heart rate monitoring, skin temperature, and neural activity. This decision was also motivated by the findings of previous studies in IR related to this paper [3]. Emotions can
be expressed through several sensory channels and are reflected by a series of more or less observable cues, such as
localised changes in the electrodermal activity, variations
in skin temperature, neural activity and many more. It
has been shown that transitions between emotional states
are correlated with temporal changes in physiological states
[9], which cannot be easily faked. In this work we monitor
participants’ physiological responses by using three sensory
devices: Polar RS800, BodyMedia SenseWear R Pro3 Armband and NeuroSky’s MindKit-EMTM .
Polar RS800 Heart Rate Monitor consists of the Polar
RS800 Running Computer, a wrist-watch that displays and
records the heart rate data, an elastic strap with two electrodes and Polar WearLink R 31, a wireless transmitter. The
elastic strap is worn around the chest (below the chest muscles), allowing the built-in soft textile electrodes to detect
the heartbeat and then transmit the heart rate signal to the
running computer, via the Polar WearLink R 31, which is
attached to the strap.
BodyMedia SenseWear R Pro3 Armband is an unobtrusive, lightweight, multi-sensor hub, which is worn above the
triceps area of the right arm. It can simultaneously measure five low-level physiological metrics: (i) galvanic skin response, (ii) skin temperature, (iii) near-body ambient temperature, (iv) heat flux, and (v) motion, via a 3-axis accelerometer. From those vital sign streams it can produce
accurate statements about the human body states and behaviours. Moreover, the existence of multiple sensors allows
for the disambiguation of contexts, which a single sensor
would have not interpreted accurately.
NeuroSky MindKit-EMTM consists of hardware and software components for simple integration of bio-sensor tech-

Sensory Channels

Affective Signals: In this study we considered facial
expression as our affective signal. This decision was moti-

137

A

B

Figure 1:

A snapshot of the video retrieval system for query “avengers”.

the output of the BodyMedia SenseWear R Pro3 Armband;
and the Attention or Meditation data (referred to as “NV”)
from the output of the eSense-EMTM software. For our behavioural signal, we considered the dwell time (referred to
as “DT”) logged by the system as our dwell time feature.
Finally the task intention was considered as task feature
(referred to as “Task”).
Preprocessing:
For each visited video, the value of
each sensory feature (for both affective and physiological
features) was calculated by averaging the data logged by
its sensory device during the dwell time period. Since none
of the instruments we used normalised the data, we scaled
signal values before applying any classification method, to
avoid having attributes in greater numeric ranges dominating those in smaller numeric ranges.

nology into consumer and industrial end-applications. NeuroSky MindKit-EMTM features two key technologies: (i)
ThinkGear-EMTM headset and (ii) eSense-EMTM software
(i.e. brainwave interpretation software). The headset is
used to extract, filter, and amplify brainwave (EEG) signals
and convert that information into digital mental state outputs for eSense-EMTM software. The EEG signals read by
the MindKit-EMTM are detected on the forehead via points
Fp1 (electrode placement system by the International Federation in Encephalography and Clinical Neurophysiology).
The headset has three dry active sensors: one sensor located
on the forehead and two sensors are located behind the ears
as ground/reference sensors. It also has electronic circuitry
that filters and amplifies the brainwaves. The eSense-EMTM
software further processes and analyses the obtained brainwave signals into two useful neuro-sensory values: the user’s
Attention4 and Meditation5 levels at any given moment.
The output of eSense-EMTM software has been tested over
a wide population and under different environmental conditions, to work across a wide spectrum of individuals.

3.3.1

3.4

Video Retrieval System

For the completion of the search tasks we used a custommade search environment (named VideoHunt) that was designed to resemble the basic layout of existing video retrieval
services, while retaining a minimum of graphical elements
and distractions. VideoHunt works on top of the Bing Video
Search API. For every query submitted, it returned a list of
100 results (36 results on each page), stripped of their title,
snippet and any other metadata. This layout was intentional to ensure that the participants would judge the relevance of videos only after they have examined them. Even
though this approach introduced our participants into artificial search situations which differ from real-life experiences,
it was a necessary trade-off for capturing affective responses
exhibited towards the viewed content.
Search Interface: The layered architecture approach
of VideoHunt interface is shown in Figure 1. The first layer
of the interface is dedicated to supporting any interaction
that occurs during the early stages of the search process
(such as query formulation and search execution). Any output generated during this phase is presented in the second
layer (shown in Figure 1 - A). From there, participants could

Features

For our affective features, we considered 19 features (referred to as “FX”) extracted from the output of eMotion: 7
features related to the classified emotions (i.e. happiness,
sadness, anger, fear, disgust, and surprise) plus neutral and
12 features related to the motion units (MU’s) data, which
is a low-level category of features very similar to Ekman’s
action-units (AU’s) [10]. For our physiological features, we
considered the heart rate data (referred to as “HR”) from the
output of the Polar RS800 Heart Rate Monitor; the galvanic
skin response, skin temperature, near-body ambient temperature, heat flux, and motion data (referred to as “AB”) from
4

Attention Meter: Combination of waves associated with being in
a state of readiness and thinking. It relates to mental arithmetic,
concentration and creative thinking.
5
Meditation Meter: Combination of waves associated with a state of
calm and tranquility.

138

easily select and preview any of the retrieved clips. The content of a clip is shown on a separate panel, in the foreground
(shown in Figure 1 - B), which corresponds to the third layer
of our system. The main reason behind this layered architecture was to isolate the viewed content from all possible
distractions that reside on the desktop screen; therefore, establishing additional ground truth that allowed us to relate
participants’ affective responses to the source of stimuli (in
our case, the perused videos). This was an important aspect
of our experimental methodology, since we were interested
in isolating content-specific emotions. In addition, in the
context of video retrieval, we would expect less interaction
with the system during the dwell time making the captured
physiological signals particularly valuable in this type of environment. Upon viewing the clip, the participants had to
explicitly indicate the relevance of the video. Finally, the
length of time a user spent watching a clip (dwell time) was
monitored and logged by the search interface.
Pilot Studies: Prior to running the actual user study,
a pilot study was performed using 5 participants to confirm
that the process worked correctly and smoothly. A number
of changes were made to the system based on feedback from
the pilot study. The changes consisted of modifications to
the system to improve logging capabilities and improvements
to the tasks. After the final pilot, it was determined that the
participants were able to complete the user study without
problems and that the system was capturing all necessary
data.
Participants: Participants consisted of 24 healthy people with equal gender distribution (12 female and 12 male)
all under the age of 41, with the largest group between the
ages of 18-23 (45.8%) followed by the group between ages
of 24-29 (36%). Participants tended to have a high school
diploma or equivalent (4.16%), college degree (4.16%), bachelors (41.66%) or graduate degree (50%). They were primarily students (62.5%), though there were a number of
self-employed (16.6%), not employed (4.16%) and employed
by a company or organisation (16.6%).

The task perfomed was

4
3

**Easy

**Stressful

Interesting

Clear

Familiar

Clear

Familiar

Clear

Familiar

Clear

Familiar

4
3
2
1

Agreement Level

5

INF

**Easy

**Stressful

Interesting

4
3

RESULTS AND DISCUSSION

1

2

Agreement Level

5

ENA

In this section we present the experimental findings of
our study, based on the 96 search sessions that were carried
out by 24 subjects. We first discuss the task perception expressed in the questionnaire. Following this, we discuss the
predictability of search intentions based on features derived
from the user interaction with the system in Section 4.1.
Task Perception: Figures 2 shows the box plots for the
qualitative analysis of users’ perception of the four tasks (i.e.
INS, INF, ENA, and ENM). Each box plot reports data aggregated from 24 participants, along with five key statistics:
the minimum, first, second (median), third, and maximum
quartiles.6 We performed an ANOVA test between measures
obtained at each phase, across four search tasks for each user
to check the significance of the difference among them. The
test is suitable for this data as we have four groups of data,
therefore we need to compare four means and variances. We
use (*) and (**) to denote the fact that a measure had results different across four search tasks with the confidence
levels (p < 0.05) and (p < 0.01), respectively.
In the post-task questionnaire we measured participants’
perception of their performed task in terms of the diffi6

2
1

Agreement Level

5

INS

**Easy

**Stressful

Interesting

4
3
1

Agreement Level

5

ENM

2

4.

culty of the task, the familiarity of the participant with the
task, the extent to which they found the task stressful, interesting and clear by asking the following question “The
task we asked you to perform was [easy/stressful/interesting/
clear/familiar] (answer: 1: “Strongly Disagree”, 2: “Disagree”, 3: “Neutral”, 4: “Agree”, 5: “Strongly Agree”)”. The
results shown in Figure 2 indicate that participants found
the INF task difficult and stressful, followed by the INS task,
whereas they found other two tasks easy and not-stressful
(the differences were statistically significant). The difference in the answer provided by the participants for interesting, clear and familiar measures is not statistically significant. In the post-task questionnaire we also asked the opinion of the participants with respect to the following statement “I had enough time to do an effective search.] (answer: 1: “Strongly Disagree”, 2: “Disagree”, 3: “Neutral”,
4: “Agree”, 5: “Strongly Agree”)”. The results show that
they found the time given enough to do an effective search
task, (INS: M=4.0 SD=0.88, INF: M=3.8 SD=0.96, ENA:
M=4.0 SD=0.97, ENM: M=4.3 SD=0.56, the differences are
however not statistically significant across the tasks).

**Easy

**Stressful

Interesting

Figure 2:

Box plot of the task perception based on the information gathered from 24 participants questionnaires. The diamond
represents the mean value. (*) and (**) indicate confidence levels
(p < 0.05) and (p < 0.01) respectively.

4.1

Relevance Prediction

In this section, we investigate our research questions introduced in Section 1. We first study the main effect of
using affective, physiological and behavioural signals for the
relevance judgement prediction task, by combining the relevance judgement obtained from our four search intents. We
then study the interaction effect of these signals with search
intents for the relevant judgment prediction task. For this

Further information can be found in [24].

139

purpose, the features explained in Section 3.3.1 were used.
In total, participants judged 488 videos (with 247 relevant,
50.60%), of which 141 (with 82 relevant, 64.53%), 128 (with
91 relevant, 64.06%), 133 (with 73 relevant, 54.88%), and 86
(with 1 relevant, 1.17%) videos belong to “ENM”, “ENA”,
“INS”, and “INF” respectively. Due to the ratio of relevant
to non-relevant videos for INF task, we consider two combination scenarios: one where the relevance judgments of all
tasks are combined (referred to as “ALL”) and another one
where the relevance judgments of all tasks are combined,
except those for the INF task (referred to as “ALL - INF”).
For relevance prediction, we have a binomial classification problem where the classes are “+1” (relevant) and “-1”
(non-relevant). We used SMO, an implementation of SVM
in Weka,7 to discriminate between the two classes explained
above. We trained our models using a polynomial kernel
which in the majority of cases outperformed other SVM kernels (e.g. radial-basis) based on our analysis, not presented
due to the space limits. For each classification method we
present only the model which achieved the best performance
among the rest in its category.
Table 2 shows the classification performance averaged over
the 24 participants of the study across four different search
tasks. It reports the accuracy of the model (i.e. fraction of
items in the test set for which the models’ predictions were
correct) using 10-fold cross-validation.
For the scenarios where only sensory signals are used to
train a model, the baseline represents an untrained model
where all its predictions are biased to a dominant class (referred to as “BL1”). This baseline is more realistic than
a baseline which is based on a random choice (i.e. where
its accuracy is set to 50%). Any feature set that results in
higher accuracy than what is reported for BL1 is as a result of the discriminative power of the set itself. For the
scenarios where dwell time and sensory signals are used to
train a model, the baseline represents a model trained on
the dwell time feature only (referred to as “BL2”). Finally,
for the scenarios where dwell time, task and sensory signals
are used to train a model, the baseline represents a model
trained based on dwell time and task features (referred to
as “BL3”). We also performed a paired Wilcoxon test between the predictions obtained for each model to check the
significance of the difference with its baseline. We use [(*)
and (**)], [(†) and (††)], and [(‡) and (‡‡)] to denote the fact
that a model trained on a set of features had results different
from that of “BL1”, “BL2”, and “BL3” with the confidence
levels [(p < 0.05) and (p < 0.01)] respectively.
Main Results: The key findings which emerged from
the results are that combining dwell time with sensory signals provides a reliable set of features for IRF techniques.
This means that sensory signals can be used as a good substitution for task information for dwell time based IRF techniques. We also observed that the effectiveness of dwell time
as well as sensory channel signals vary across the search intentions. In the remainder of this section we discuss each of
our research questions in more detail.
RQ1: which combination of affective, physiological and behavioural signals provide a reliable set of
features for IRF techniques? In order to answer this
research question we study the main effect of these signals
for the relevance prediction task. This is an attempt to sim7

ulate a real life scenario where the system has to predict
the relevance judgement for search processes with different
(and in the majority of the cases, unidentified) intentions.
Therefore, we only consider the results obtained for the scenarios where the model has to predict the relevance judgement gathered from multiple search intentions (i.e. “All INF” and “ALL” columns).
Behavioural and Sensory Signals: Considering dwell time
with only one sensory signal at a time as a feature set, the
findings show that the combination of facial expression features and dwell time results in the highest accuracy (i.e.
“DT+FX” row). The findings show that a higher accuracy
can be achieved when the model has been trained on a feature set that has dwell time and all the sensory signals (i.e.
“DT+FX+AB+HR+NV” row). This finding suggests the
complementary nature of the information obtained through
various behavioural and sensory signals. Furthermore, they
also outperform the scenario where dwell time and task features are used to train a model, showing that this set of
features is a reliable substitute for IRF techniques
Sensory Signals: Considering each sensory signal individually as a feature set, the findings show that for the situation where the model has to predict the relevance judgement gathered for all studied search intentions (i.e. “ALL”
column), facial expression features results in the highest accuracy (i.e. “FX” row). However such an observation cannot
be made for the situation where the model has to predict
the relevance judgement gathered for multiple search intentions except those of the re-finding intention (i.e. “All INF” column). The difference in the accuracy of facial expression features between “ALL” and “All - INF” situations
suggest that facial expression features can discriminate relevance judgments of the tasks which have a significant difference in their difficulty. The results indicate that if there is
no significant difference in the task difficulty, the heart rate
feature provides the highest accuracy in relevance judgement
prediction (i.e. “HR” row). The findings show that a higher
accuracy again can be achieved when the model has been
trained on a feature set that is based on all sensory signals
(i.e. “FX+AB+HR+NV” row).
Behavioural Signals: Considering only dwell time, the
findings show that for the situation where the model has
to predict the relevance judgement gathered for all studied
search intentions (i.e. “ALL” column), the dwell time feature
results in a high accuracy level. However, once the model
has to predict the relevance judgements gathered for multiple search intentions except those of re-finding intention (i.e.
“All - INF” column), its effectiveness decreases. Our findings
also show that the combination of dwell time and task information improves the accuracy of the relevance judgement
prediction significantly, only if the performed tasks varies in
some aspect, such as difficulty (as it is in our case). This
finding supports the results obtained by Kelly and Belkin
[19]. We observe that if the performed tasks does not vary
significantly in their characteristics, task information do not
provide any additional value and it can also significantly
harm the accuracy.
In the Presence of Task Information: Finally, we investigated the effect of having task information along with dwell
time and all the sensory signals on the accuracy of relevance
judgment prediction (i.e. “DT+Task+FX+AB+HR+NV”
row). The findings show that the prediction accuracy of a
model trained on these features is significantly higher than

http://www.cs.waikato.ac.nz/ml/weka/

140

Table 2: This table shows the prediction accuracy of a model trained on different sets of features (presented as rows), given different search
intentions (presented as columns). The best performing set of features for each condition and search intention is highlighted in bold.
INS
ENA
ENM
INF
ALL - INF
ALL
Random [BL1](*)
54.88%
64.06%
64.53%
98.83%
61.19%
50.60%
DT [BL2](†)
62.40%
65.62%
66.16%
98.83%
71.31%
72.74%
DT+Task [BL3](‡)
–%
–%
–%
–%
69.65%
76.63%
FX
55.63%*
66.4%**
64.53%
98.83%
62.43%**
64.54%**
(+1.3%)
(+3.6%)
(+0%)
(+0%)
(+2.0%)
(+27.4%)
AB
54.88%
64.06%
64.53%
98.83%
61.19%
50.60%
(+0%)
(+0%)
(+0%)
(+0%)
(+0%)
(+0%)
HR
57.89%** 64.06%
64.53%
98.83%
62.93%**
53.27%**
(+5.4%)
(+0%)
(+0%)
(+0%)
(+2.8%)
(+5.2%)
NV
55.63%*
64.06%
64.53%
98.83%
61.19%
55.73%**
(+1.3%)
(+0%)
(+0%)
(+0%)
(+0%)
(+10.1%)
FX+AB+HR+NV
55.63%*
69.53%** 64.53%
98.83%
67.16%**
65.98%**
(+1.3%)
(+8.5%)
(+0%)
(+0%)
(+9.7%)
(+30.3%)
DT+FX
67.66%††
68.75%††
71.63%††
98.83%
72.88%††
77.04%††
(+8.4%)
(+4.7%)
(+8.2%)
(+0%)
(+2.2%)
(+5.9%)
DT+AB
66.91%††
67.96%††
81.56%††
98.83%
71.64%
76.22%††
(+7.2%)
(+3.5%)
(+23.2%)
(+0%)
(+0.4%)
(+4.5%)
DT+HR
63.15%†
73.43%††
82.26%††
98.83%
72.13%†
76.22%††
(+1.2%)
(+11.9%) (+24.3%) (+0%)
(+1.1%)
(+4.5%)
DT+NV
64.41%††
70.31%††
74.46%††
98.83%
72.13%†
75.40%††
(+3.2%)
(+7.1%)
(+12.5%)
(+0%)
(+1.1%)
(+5.6%)
DT+FX+AB+HR+NV
66.16%††
75%††
80.14%††
98.83%
75.37%††
77.04%††
(+6.0%)
(+14.2%) (+21.1%)
(+0%)
(+5.6%)
(+5.9%)
DT+Task+FX+AB+HR+NV
–%
–%
–%
–%
76.36%‡‡
78.89%‡‡
(+9.6%)
(+2.9%)

the prediction accuracy of a model trained on dwell time and
task features significantly (i.e. “DT+Task” row). The results
also show that the prediction accuracy of such a model is
even higher than a model trained on all features except task
one (i.e. “DT+FX+AB+HR+NV” row). This show that researches on task prediction are complementary to this study
rather than contradictory.
RQ2: What would be the accuracy of these feature sets with respect to different search intentions?
In order to answer this research question we study the interaction effect of these signals with search intentions for
relevance prediction task. Therefore, we consider the results
obtained for the scenarios where the model has to predict
the relevance judgement gathered from one search intention
at a time (i.e. “INS”, “ENM”, “ENA” and “INF” columns).
Overall, the results do not show any general pattern, meaning that the effectiveness of different signals are task dependent. We exclude the “INF” intention from our further
analysis, since for “INF” intention, almost all of the judged
videos were non-relevant, additionally the accuracy of “BL1”
model is too high to be outperformed by any of our models.
The findings suggests that sensory channels combined with
dwell time performs better for the search intentions which do
not have a clear information need (i.e. “ENM” and “ENA”)
than the ones that do (i.e “INS”). For search processes with
entertainment-based intentions, a combination of heart rate
and dwell time features results in the highest accuracy. For
search processes with information seeking intention, combination of facial expression and dwell time feature is the
highest accuracy. In particular, the results suggest that for
the ENM task, heart rate and skin temperature; for ENA
task, heart rate and EEG; for INS task, facial expression and
skin temperature are the best two sensory signals, once they
are combined with dwell time. Search processes for which
the purpose is to satisfy an information need, facial expression and skin temperature are the best two sensory channels
once they are combined with dwell time information.

An interesting finding is that the discriminative power of
sensory signals changes once they are combined with dwell
time, even though they show no such power individually. For
example, a sensory feature that was not discriminative on
its own for a task (e.g. “HR” feature for “ENM” task), when
combined with dwell time, can result in the highest prediction accuracy (i.e. “DT+HR” features for “ENM” task). In
addition, the findings suggest that across all search tasks,
combining each sensory signal with dwell time results in a
higher accuracy compared to the situation where only either
of them is used. The combination of all the sensory channels
does not result in the highest accuracy, except for the “ENA”
task. Similar behaviour is observed when these features are
combined with dwell time one.
Finally, comparison of the accuracy obtained from BL2
(dwell time) and BL1 show that dwell time performs better for tasks based on information needs (i.e. “INS”) than
tasks based on hedonistic needs (i.e. “ENA” and “ENM”).
However, the findings show that the accuracy of the dwell
time feature and sensory channels by themselves is not high
and is task dependent. This finding supports the results obtained by previous studies, i.e. [19] for dwell time feature
and [3] for sensory features.

5.

CONCLUSIONS

In this paper we investigated the discriminative power of
different sensory channel signals as a reliable substitute for
task information for IRF techniques based on dwell time,
given different search intentions. In order to do so, we devised four search tasks, each simulating one of the following
search intentions: information seeking, re-finding, entertainment adjusting arousal level and entertainment adjusting
mood. Using a video retrieval system as a use case, we conducted a user study with 24 participants. We then studied
two research questions: (1) which combination of affective,
physiological and behavioural signals provide a reliable set of
features for IRF techniques? and (2) what would be the ac-

141

curacy of these feature sets with respect to different search
intentions? The findings show that combining dwell time
with sensory signals provides a reliable set of features for
IRF techniques, and the sensory signals can be used as a
good substitution of task information for dwell time based
IRF techniques. We also observed that the effectiveness of
both the dwell time feature as well as sensory channel signals vary across different search intentions. The major implication of our work is in developing real time interactive
systems.
Finally, although the findings of this paper are limited to
the video retrieval domain and not generalizable, they motivate the exploration of similar hypotheses in other domains.
Even though significant improvement in relevance prediction
were achieved by adding affective and physiological measures
across different search task types, we acknowledge the following limitations: first, the experiment was conduced in an
artificial situation (i.e. a laboratory setting) which lacks the
ecological validity of a naturalistic study. Second, only one
behavioural signal (i.e. dwell time) was considered in this
experiment. Our positive findings encourage us to explore
more exhaustively behavioural signals such as click-through
and mouse movement as well as eye movement in combination with affective, physiological signals for future work.
Finally, it will be some time before physiological signals can
be used by practical applications. Therefore, the study in
this area is in its infancy. There are still many research questions left to be answered. The findings of this paper lay the
foundation for further studies to address these areas.

6.

[13] D. Elsweiler, S. Mandl, and B. Kirkegaard Lunn.
Understanding casual-leisure information needs: a diary study
in the context of television viewing. In IIiX, pages 25–34, 2010.
[14] D. Elsweiler, M. Wilson, and M. Harvey. Searching4fun. ECIR
workshop, 2012.
[15] D. Elsweiler, M. Wilson, and B. Lunn. Understanding
casual-leisure information behaviour. Library and Information
Science, 211:241, 2011.
[16] H. A. Feild, J. Allan, and R. Jones. Predicting searcher
frustration. In SIGIR, pages 34–41, 2010.
[17] J. Gwizdka and I. Lopatovska. The role of subjective factors in
the information search process. JASIST, 60(12):2452–2464,
2009.
[18] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay.
Accurately interpreting clickthrough data as implicit feedback.
In SIGIR, pages 154–161, 2005.
[19] D. Kelly and N. J. Belkin. Display time as implicit feedback:
understanding task effects. In SIGIR, pages 377–384, 2004.
[20] J. Koenemann and N. J. Belkin. A case for interaction: a study
of interactive information retrieval behavior and effectiveness.
In SIGCHI, pages 205–212, 1996.
[21] J. Liu, M. J. Cole, C. Liu, R. Bierig, J. Gwizdka, N. J. Belkin,
J. Zhang, and X. Zhang. Search behaviors in different task
types. In JCDL, pages 69–78, 2010.
[22] J. Liu, J. Gwizdka, C. Liu, and N. J. Belkin. Predicting task
difficulty for different task types. In ASIS&T, volume 47,
pages 16:1–16:10, 2010.
[23] I. Lopatovska. Searching for good mood: Examining
relationships between search task and mood. ASIST,
46(1):1–13, 2009.
[24] R. McGill, J. W. Tukey, and W. A. Larsen. Variations of box
plots. American Statistician, 32(1):12–16, 1978.
[25] M. Morita and Y. Shinoda. Information filtering based on user
behavior analysis and best match text retrieval. In SIGIR,
pages 272–281, 1994.
[26] R. Oddy. Information retrieval through man-machine dialogue.
Journal of documentation, 33(1):1–14, 1977.
[27] M. B. Oliver. Mood management and Selective Exposure.
Communication and Emotion: Essays in honor of Dolf
Zillmann, pages 85–106, 2003.
[28] M. Pantic and L. Rothkrantz. Expert system for automatic
analysis of facial expressions. Image and Vision Computing,
18(11):881–905, 2000.
[29] M. Pantic and L. Rothkrantz. Toward an affect-sensitive
multimodal human-computer interaction. IEEE,
91(9):1370–1390, 2003.
[30] A. Poddar and I. Ruthven. The emotional impact of search
tasks. In IIiX, pages 35–44, 2010.
[31] Y.-W. Seo and B.-T. Zhang. Learning user’s preferences by
analyzing web-browsing behaviors. In Agents, pages 381–387,
2000.
[32] C. Sheldrick Ross. Finding without seeking: The information
encounter in the context of reading for pleasure. IPM,
35(6):783–799, 1999.
[33] R. Taylor. Question-negotiation an information-seeking in
libraries. Technical report, DTIC Document, 1967.
[34] J. Teevan, E. Adar, R. Jones, and M. Potts. Information
re-retrieval: repeat queries in yahoo’s logs. In SIGIR, pages
151–158, 2007.
[35] R. Valenti, N. Sebe, and T. Gevers. Facial expression
recognition: A fully integrated approach. In ICIAP Workshop,
pages 125–130, 2007.
[36] P. Vorderer. Entertainment theory. Communication and
emotion: Essays in honor of Dolf Zillmann, pages 131–153,
2003.
[37] R. White and D. Kelly. A study on the effects of
personalization and task information on implicit feedback
performance. In CIKM, pages 297–306, 2006.
[38] R. W. White. Implicit Feedback for Interactive Information
Retrieval. PhD thesis, University of Glasgow, 2004.
[39] R. W. White, J. M. Jose, and I. Ruthven. A task-oriented
study on the influencing effects of query-biased summarisation
in web searching. IPM, 39(5):707–733, 2003.
[40] M. Wilson and D. Elsweiler. Casual-leisure searching: the
exploratory search scenarios that break our current models. In
HCIR, pages 28–31, 2010.

ACKNOWLEDGEMENT

This work was supported by the EU FP7 LiMoSINe project
(288024).

7.

REFERENCES

[1] I. Arapakis, K. Athanasakos, and J. M. Jose. A comparison of
general vs personalised affective models for the prediction of
topical relevance. In SIGIR, pages 371–378, 2010.
[2] I. Arapakis, J. M. Jose, and P. D. Gray. Affective feedback: an
investigation into the role of emotions in the information
seeking process. In SIGIR, pages 395–402, 2008.
[3] I. Arapakis, I. Konstas, and J. M. Jose. Using facial
expressions and peripheral physiological signals as implicit
indicators of topical relevance. In ICMR, pages 461–470, 2009.
[4] I. Arapakis, Y. Moshfeghi, H. Joho, R. Ren, D. Hannah, and
J. M. Jose. Enriching user profiling with affective features for
the improvement of a multimodal recommender system. CIVR,
2009.
[5] N. Belkin, C. Clarke, N. Gao, J. Kamps, and J. Karlgren.
Report on the sigir workshop on entertain me: supporting
complex search tasks. ACM SIGIR Forum, 45(2):51–59, 2012.
[6] P. Borlund and P. Ingwersen. The development of a method for
the evaluation of interactive information retrieval systems.
Journal of documentation, 53(3):225–250, 1997.
[7] A. Broder. A taxonomy of web search. ACM Sigir forum,
36(2):3–10, 2002.
[8] M. Claypool, P. Le, M. Wased, and D. Brown. Implicit interest
indicators. In IUI, pages 33–40, 2001.
[9] R. Cornelius. Theoretical approaches to emotion. In ISCA
Tutorial and Research Workshop (ITRW) on Speech and
Emotion, 2000.
[10] P. Ekman. Facial expressions. Handbook of cognition and
emotion, pages 301–320, 1999.
[11] P. Ekman. Emotions revealed: Recognizing faces and feelings
to improve communication and emotional life. Times Books,
2003.
[12] D. Elsweiler, M. Harvey, and M. Hacker. Understanding
re-finding behavior in naturalistic email interaction logs. In
SIGIR, pages 35–44. ACM, 2011.

142

