Segmentation Strategies for Passage Retrieval
in Audio-Visual Documents
Petra Galuščáková
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Prague, Czech Republic

galuscakova@ufal.mff.cuni.cz
ABSTRACT

Our preliminary results show that even simple semanticbased segmentation outperforms regular segmentation. Our
model is a decision tree incorporating the following features:
shot segments, output of TextTiling algorithm [2], cue words
(well, thanks, so, I, now), sentence breaks, and the length of
the silence after the previous word. In terms of the MASP
measure [1], the relative improvement over regular segmentation is more than 19%.

The importance of Information Retrieval (IR) in audio-visual
recordings has been increasing with steeply growing numbers
of audio-visual documents available on-line. Compared to
traditional IR methods, this task requires specific techniques,
such as Passage Retrieval which can accelerate the search
process by retrieving the exact relevant passage of a recording instead of the full document. In Passage Retrieval, full
recordings are divided into shorter segments which serve as
individual documents for the further IR setup. This technique also allows normalizing document length and applying
positional information. It was shown that it can even improve
retrieval results (e.g. [3]).
In this work, we examine two general strategies for Passage Retrieval: blind segmentation into overlapping regularlength passages and segmentation into variable-length passages based on semantics of their content.
Time-based segmentation was already shown to improve
retrieval of textual documents and audio-visual recordings
(e.g. [3, 5]). Our experiments performed on the test collection
used in the Search subtask of the Search and Hyperlinking
Task in MediaEval Benchmarking 20121 confirm those findings and show that parameters (segment length and shift)
tuning for a specific test collection can further improve the
results. Our best results on this collection were achieved by
using 45-second long segments with 15-second shifts.
Semantic-based segmentation can be divided into three
types: similarity-based (producing segments with high intrasimilarity and low inter-similarity), lexical-chain-based (producing segments with frequent lexically connected words),
and feature-based (combining various features which signalize
a segment break in a machine-learning setting) [4]. In this
work, we mainly focus on feature-based segmentation which
allows exploiting various features from all modalities of the
data (including segment length) in a single trainable model
and produces segments which can eventually overlap.
1

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval—Search process, Selection process

General Terms
Experimentation, Theory

Keywords
Information Retrieval; Passage Retrieval; Semantic Segmentation

Acknowledgements
This research is supported by the 207-10/253571 grant and
SVV project number 267 314.

1.

REFERENCES

[1] M. Eskevich, W. Magdy, and G. J. F. Jones. New
metrics for meaningful evaluation of informally
structured speech retrieval. In ECIR, volume 7224 of
LNCS, pages 170–181. Springer, 2012.
[2] M. A. Hearst. TextTiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33–64, Mar. 1997.
[3] M. Kaszkiel and J. Zobel. Effective ranking with
arbitrary passages. Journal of the Am. Society for IST,
52(4):344–364, Jan. 2001.
[4] D. Kauchak and F. Chen. Feature-based segmentation of
narrative documents. In Proc. of ACL, FeatureEng ’05,
pages 32–39. ACL, 2005.
[5] C. Wartena. Comparing segmentation strategies for
efficient video passage retrieval. In CBMI, pages 1–6.
IEEE, 2012.

http://www.multimediaeval.org/

Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact
the owner/author(s).
Copyright is held by the owner/author(s).
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
ACM 978-1-4503-2034-4/13/07.

1143

