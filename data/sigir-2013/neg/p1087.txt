Live Nuggets Extractor
A Semi-Automated System for Text Extraction and Test Collection Creation
Matthew Ekstrand-Abueg

Virgil Pavlu

Javed A Aslam

College of Computer and Information Science, Northeastern University

{mattea,vip,jaa}@ccs.neu.edu

1.

ABSTRACT

collections which can be used to evaluate both documentcentric and text-centric systems by extracting text fragments
during the judging process. The system has been used for
the English track of NTCIR-10 1CLICK-2 and adapted to
Japanese to compare to a manual methodology.
The demo system presented here builds upon our previous work [3] for test collection creation by creating a live
system which accepts a query from the user, presents documents and nuggets for judgment feedback, then reweights
documents and nuggets according to the feedback. The motivation for this is to create a collection of documents to
pass into summarizer systems and simultaneously find the
text against which to evaluate the output of such systems.

The Live Nugget Extractor system provides users with a
method of efficiently and accurately collecting relevant information for any web query rather than providing a simple ranked lists of documents. The system utilizes an online
learning procedure to infer relevance of unjudged documents
while extracting and ranking information from judged documents. This creates a set of judged and inferred relevance
scores for both documents and text fragments, which can
be used for test collections, summarization, and other tasks
where high accuracy and large collections with minimal human effort are needed.
Categories and Subject Descriptors:
H.3.3 [Information Search and Retrieval]
Keywords: information retrieval; relevance assessment; text
extraction; test collection; nuggets

2.

INTRODUCTION

With Information Retrieval research increasingly moving
from document retrieval to retrieval of text fragments, the
needs is growing for systems which can create new test collections based on smaller text units and give users a way to
guide their search for text answers, rather than ranked lists
of documents. To this end, we have created an interface
that allows a user to input a query and iteratively judge
both documents and text fragments, or nuggets, for relevance 12 . These judgments are actively incorporated into a
relevance model and new documents and nuggets are presented to maximize relevant information returned.
We have previously shown that the nuggets system allows
for test collections to be created with high information recall and accuracy of inferred document relevance using a
relatively small number of judgments [3]. Further, it creates
1
A live demonstration of the interface can be accessed at
http://fiji.ccs.neu.edu/nuggets/demo/.
2
We gratefully acknowledge support provided by NSF IIS1256172.

Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact
the owner/author(s).
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
ACM 978-1-4503-2034-4/13/07.

2.1

Related Work

To the best of our knowledge, there exists no similar tool
for collecting relevant information without fully manual factbase extraction. Pooling methods using ranker systems followed by exhaustive judging or statistical sampling are common, but neither can scale with modern collections. TREC,
a U.S. government effort, coordinates professional assessors
to judge up to 3,000 documents per query, but even this
effort has fatigue-driven errors [1] and many relevant documents not even considered for assessment [2].
Although direct relevance feedback at the document level
is too expensive in web search, our active framework for
judging and refining limits the manual work required and
provides inferred relevance of text fragments. This is a
highly expensive operation if performed manually, more comparable to TREC’s track on document assessment, which required considerably more manual work than our system. In
previous work, we showed that our method also outperforms
both Relevance Feedback and Learning to Rank schemes.
Basic query refinement and expansion exist in popular
search engines, providing a query’s common similar searches
and related entities. This, however, is not based on the
information relevant to the query, and does not allow the
user to see ranked text fragments based on this refinement.
As we will show, our system allows the user to guide the
search both for types of relevant documents and information,
while easily viewing the progress of the inference and the
refined set of relevant information.

3.
3.1

SYSTEM COMPONENTS
User Interface

The interface for the Nugget Extractor system presents
the user with the ability to enter a query and then perform

1087

iterative judgments on documents and nuggets to grow the
space of relevant information.
The left side of the interface allows the user to read a
document or an expandable ranked list of documents, and
judge them for relevance. The right side of the interface
allows the user to view the information that has been extracted so far, sorted by inferred relevance, and optionally
to judge individual nuggets.
Additionally, a user may manually create a nugget. This
can be useful to expand the scope of the search, in a similar
fashion to iterative query reformulation. The nugget can
be in any category, to allow for both positive and negative
examples to aid refinement of the relevance space.
Once a user feels they have completed their search, they
may export lists of relevance for nuggets and documents.
They may also see lists of unjudged items sorted by their
inferred relevance scores.

3.2

QREL(docs)
DOCS_REL
------DOCS_NREL

DOCUMENT
SELECTOR

JUDGE

TEXT MATCHING

Figure 1: The Nugget Extractor Interface. Multiple documents are shown on the left, similar to web search
results, expandable to view the documents. Extracted nuggets are shown on the right, ranked in the top
panel by inferred relevance, with judged nuggets visible in lower panels.

GREL(nuggets)
NUGGET
EXTRACTION
NUGGET
WEIGHT
UPDATE

NUGS_REL
------NUGS_NREL

Figure 2: The overall design of assessment process:
Iteratively, documents are selected and assessed,
and nuggets are extracted and [re]weighted.
nuggets are also matched against all documents, and new
document scores are computed from the new weights on all
nuggets. Judging a nugget reweights the documents which
match it, as does creating a new nugget.

Backend Procedure

4.

The User Interface provides the interaction for the manual portions of the extraction system, but most of the work
is done behind the scenes. We use an iterative procedure
to incorporate judgments on documents and nuggets into a
belief graph of relevance, incorporating the observations and
modifying the weights based on matches between documents
and judgments. This procedure can be seen in full in our
previous work [3].
Figure 2 illustrates the backend system, where our TEXT
MATCHING system creates the links between documents
and nuggets, allowing assessments on either side to influence
the inferred relevance scores for all other objects.
When a user enters a query, a set of candidate documents
are retrieved and cleaned for the interface and then the first
set of documents is displayed to the user. When the user
judges a document, it is placed in the QREL and the backend
procedure extracts nuggets from the document if it is marked
relevant. Then all nuggets which matched the document
are reweighted according to its judgment. Finally, the new

CONCLUSION

Our Nugget Extractor system allows users to cover the
relevant information space of a large number of documents
without having to fully read and extract information from
each one. This is useful in creating lists of facts for a query,
creating document or nugget test collections with both explicit and implicit relevance judgements, and in guided search.

5.

REFERENCES

[1] B. Carterette and I. Soboroff. The effect of assessor
error on IR system evaluation. SIGIR ’10.
[2] V. Pavlu, S. Rajput, P. B. Golbus, and J. A. Aslam. IR
system evaluation using nugget-based test collections.
WSDM ’12.
[3] S. Rajput, M. Ekstrand-Abueg, V. Pavlu, and J. A.
Aslam. Constructing test collections by inferring
document relevance via extracted relevant information.
CIKM ’12.

1088

