Spacious: An Interactive Mental Search Interface
Phong D. Vo

Hichem Sahbi

TELECOM ParisTech
46 rue Barrault, 75013 Paris

CNRS TELECOM ParisTech
46 rue Barrault, 75013 Paris

vo@enst.fr

sahbi@enst.fr

ABSTRACT

tered, and this makes them difficult to exploit. Recent approaches [1] propose attribute based object models in which
a visual object can be retrieved by querying its relative attributes, for example “find image C which is greener than
image A but less open than image B.” These attributes define semantic spaces in which the location of a given image
is determined by the scores of binary classifiers associated
to these attributes. Even though promising, these methods
require training and annotation steps, which are prohibitive
especially when handling large scale datasets.
In our proposed method, we share the same perspective
as [1] but we rather assume that semantics are described
with few training images where each one includes only one
semantic; these images are referred to as endmembers. A
semantic space is then defined by assigning the endmembers
to their corresponding semantic coordinates. By adopting an
appropriate metric, we define the membership (and also the
embedding) of any given image with respect to the axes of
the learned semantic space. Thereby, searching for a mental
visual target simply reduces to scanning and targeting data
according to their coordinates in that semantic space. We
apply this proposed method, using a novel software, referred
to as “Spacious”. We now describe how Spacious performs
interactive visualization and mental querying on real-world
scenarios. The technical details are mentioned in Appendix.

We introduce in this work a novel approach for semantic
indexing and mental image search. Given semantic concepts defined by few training examples, our formulation is
transductive and learns a mapping from an initial ambient
space, related to low level visual features, to an output space
spanned by a well defined semantic basis where data can be
easily explored. With this method, searching for a mental
visual target reduces to scanning data according to their
coordinates in the learned semantic space. We illustrate
the proposed method through our graphical user interface
“Spacious”, for the purpose of visualization and interactive
navigation in generic image databases and satellite images.

Categories and Subject Descriptors
[Users and Interactive IR]: search interface

Keywords
mental search, subspace learning, data visualization

1.

INTRODUCTION

Designing queries, in content based image retrieval, has been
an open problem for years. Ideally, a query should not only
be accurate but also easy to set. As the user quite often
does not know exactly how the image of interest looks like,
a search model with feedback based on visualization and
interaction is necessary in order to have a global view of an
image database and to make the search process effective and
also tractable.
There have been efforts [2] in using visualization as a mean
to query and collect feedback for image search, using hierarchical trees or graphs. While the formers are hardly extensible for images with complex scenes or objects, the latters
make the user getting easily lost due to the complexity of
graph structures. Although spectral based dimensionality
reduction techniques produce embeddings that preserve local similarity, their global shapes are often curved or clus-

2.

SATELLITE IMAGE SEARCH

Nowadays, map services (Google Maps, GeoPortail, Bing,
etc.) are becoming very popular; however, they are powerless to search for locations based on their visual content.
Indeed, with these services, searching large satellite images
is usually systematic and tedious (especially when metadata
are scarce or not intuitive to the user). Spacious is an alternative, based on a novel semantic subspace learning method
(see appendix), that allows the user to visualize, explore and
localize visual targets efficiently. Spacious is built on top of
PartiView 1 , and it allows us to visualize and explore satellite images including more than ten thousands image regions.
This software also provides different navigation modes, including region subset selection and filtering.
Given a satellite image, we split it into approximately
12,000 rectangular patches and we encode each patch using
low level visual features. We also consider a vocabulary of
semantics including building, road, vegetation and water ; for
each semantic, we select 15 image patches and their memberships are set accordingly. Fig. 1(a), 1(d) shows a visualization of the learned embedding with Spacious. When

Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact
the owner/author(s).
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
ACM 978-1-4503-2034-4/13/07.

1

1081

http://www.haydenplanetarium.org/universe/partiview

(a) A mental query “find a location with the same proportions of vegetation, road,
and building” is expressed by positioning the cube selector next to the barycenter
of the simplex. Endmember patches are superimposed on the three coordinates. A
candidate is highlighted in the right map.

(b) Patches shown inside the
cube selector constitute, at
least, two out of the three concepts roads, buildings and vegetation.

(c) A mental query “find photos representing sky and plants” will locate the cube selector
somewhere at the middle of the straight line connecting the two vertices plant and sky.
Search results are displayed in the right window. On the leftmost, sliders are used to move
the cube selector and combo-boxes are associated to the selected semantic axes.

(d) This figure shows photos inside the cube selector.
There are few false alarms
mixing sky and water.

Figure 1: Snapshots of Spacious used to explore satellite images (a-b) and LabelMe database (c-d).

5.

traversing dimensions separately, we observe a gradual variation of the underlying semantics whereas in the span of
these dimensions, patches mix several semantics.

3.

GENERIC IMAGE SEARCH

Similarly, we consider a vocabulary of semantics which is
more diverse than in satellite images. This diversity results
from the large number of concepts present into generic images. We learn a semantic space using endmembers as discussed earlier, and a user searches for a mental target by
specifying and refining the abundance of each axis in the
learned semantic space. In practice, we run experiments on
a subset of the LabelMe database; we segment each image
into non overlapping regions, and we describe each one using visual features including SIFT, color histogram, texton
histogram and GIST. Among 2,688 images, we select a subset of 200 pictures as endmembers in order to learn a twelve
dimensional semantic space. Fig. 1(c), 1(d) shows a visualization of that space in 3D using “Spacious”.

4.

REFERENCES

[1] A. K. et al. Whittlesearch: Image search with relative
attribute feedback. In CVPR, 2012.
[2] D. Heesch. A survey of browsing models for content
based image retrieval. Multimedia Tools Appl., 2008.

APPENDIX
Let X ∈ Rd×n be a matrix of n input data points and Y ∈
[0, 1]K×n its underlying membership matrix where Yki > 0
iff the kth semantic is present into X.i ; K is the number
of semantics. Considering the first ` points as endmembers
(i.e., each one contains only one semantic), the membership
vector Y.i , of each endmember, is set to one of the canonical
base vectors. Now, our problem learns an embedding Z ∈
RK×n where each entry Zki corresponds to a mapping of
X.i into the kth semantic dimension
(
Z.i = Y.i
i = 1, . . . , `
1
0
(1)
min tr ZLZ s.t PK
Z≥0 2
Z
=
1
i
= ` + 1, . . . , n
ki
k=1
here tr stands for the matrix trace operator. The main term,
in (1), is a regularizer that ensures similar embedding for
neighboring data in X and L is a normalized graph Laplacian. The first ` constraints guarantee that the embedding
{Z.i }`i=1 ≡ {Y.i }`i=1 while the last n − ` constraints control
the memberships of data points to different semantic axes.

ACKNOWLEDGMENTS

This work was supported in part by a grant from the
Research Agency ANR (Agence Nationale de la Recherche)
under the MLVIS project and a grant from CNES (Centre
National d’Etudes Spatiales) under the VENISE project.

1082

