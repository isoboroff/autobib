Question Retrieval with User Intent
Long Chen

Dell Zhang

Mark Levene

DCSIS
Birkbeck, University of London

DCSIS
Birkbeck, University of London

DCSIS
Birkbeck, University of London

long@dcs.bbk.ac.uk

dell.z@ieee.org

mark@dcs.bbk.ac.uk

ABSTRACT

1.

Community Question Answering (CQA) services, such as
Yahoo! Answers and WikiAnswers, have become popular
with users as one of the central paradigms for satisfying
users’ information needs. The task of question retrieval in
CQA aims to resolve one’s query directly by finding the
most relevant questions (together with their answers) from
an archive of past questions. However, as users can ask
any question that they like, a large number of questions in
CQA are not about objective (factual) knowledge, but about
subjective (sentiment-based) opinions or social interactions.
The inhomogeneous nature of CQA leads to reduced performance of standard retrieval models. To address this problem, we present a hybrid approach that blends several language modelling techniques for question retrieval, namely,
the classic (query-likelihood) language model, the state-ofthe-art translation-based language model, and our proposed
intent-based language model. The user intent of each candidate question (objective/subjective/social) is given by a
probabilistic classifier which makes use of both textual features and metadata features. Our experiments on two realworld datasets show that our approach can significantly outperform existing ones.

In recent years, Community Question Answering (CQA)
services, such as Yahoo! Answers, WikiAnswers, Quora,
Stack Overflow, and Baidu Zhidao, have become popular
knowledge sources for Internet users to access useful information online. When a user submits a new question (i.e.,
query) in CQA, the system would usually check whether
similar questions have already been asked and answered before. If so, the user’s query could be resolved directly by returning those archive questions (i.e., documents) with their
corresponding answers.
Identifying relevant questions in CQA repositories is a difficult task, as the underlying intents of two questions could
be very different even if they bear a close lexical resemblance.
For example, at the time of writing this paper, when submitting the question “What does licking your finger before
turning a page do?” to Google, the question “What would
you do if I licked my fingers before turning the pages of your
books?” (with answers like “No, I wouldn’t care”) is deemed
as the second best match, since these two questions look
similar. However, the intent behind these two questions are
substantially different: the former seeks facts while the latter
looks for social empathy from other people. In this paper,
we aim to develop an approach for question retrieval that
can strike the optimal balance between a question’s lexical
relevance and intent relevance.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval

2.

INTRODUCTION

RELATED WORK

The technique of language modelling has proved to be
effective for question retrieval in CQA. The state-of-theart approach utilises the classic (query-likelihood) language
model [9] (see Section 3.1) together with the translationbased language model [5,8] (see Section 3.2). Cao et al. have
proposed a category-based language model [2] for question
retrieval, but it requires users to manually assign one topic
category to each of their query questions, which is not always feasible. Furthermore, their category-based language
model cannot be applied straightforwardly to incorporate
user intent into question retrieval, because it assumes that
a question belongs to one and only one category, while we
believe that a question can have multiple intents (each to a
certain degree).

General Terms
Algorithms, Experimentation, Performance

Keywords
Community Question Answering, Question Retrieval, User
Intent, Language Modelling.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

3.
3.1

MODELS
Classic Language Model

Using the classic (query-likelihood) language model [9] for
information retrieval, we can measure the relevance of an

973

archive question d with respect to the query question q as:
Y
Pcla (q|d)=
Pcla (w|d)
(1)
w∈q

assuming that each term w in the query q is generated independently by the unigram model of document d. The
probabilities Pcla (w|d) are estimated from the bag of words
in document d with Dirichlet prior smoothing.

3.2

Translation-based Language Model

There are often lexical gaps between a query question
and archive questions in CQA. For example, “Where can
I see movies for free online” and “Anyone share me a DVD
streaming link?” have the same meaning but are expressed
in quite different words. It has been demonstrated that this
issue could be addressed by the translation-based language
model [5, 8]:
Y
Ptra (q|d) =
Ptra (w|d)
(2)

Figure 1: The question topic feature.

w∈q

Ptra (w|d)=

X

P (w|t)P (t|d)

(3)

t∈d

where P (w|t) represents the probability of a document term
t being translated into a query term w. As in [8], we estimate such word-to-word translation probabilities P (w|t) on
a parallel corpus that consists of 200,000 archived questionanswer pairs from Yahoo! Answers.

3.3

Intent-based Language Model

There could be different user intents underlying different
questions. For example, many questions in CQA are affected
by the users’ individual interests (empathy, support, and
affection, etc.) rather than just informational needs. In
this paper, we propose to take user intent into account for
question retrieval in the language modelling framework:
Y
Pint (q|d) =
Pint (w|d)
(4)

Figure 2: The question time feature.
have been identified and exploited for training the probabilistic classifier. We found that question topic, question
time, and asker experience are particularly useful for our
task of intent-oriented question classification.
Question Topic. Figure 1 shows the distribution of user
intent over the top-10 question topic categories in Yahoo!
Answers. Objective and subjective questions have similar
proportion of presence in most topic categories, except for
“Arts & Humanities” (which contains many questions about
history and genealogy). The distribution of social questions
looks quite different: most social questions are about topics
like “Family Relationships,”“News Events,” and “Entertainment & Music” on which people may be more inclined to
have social interaction. This suggests that question topic
features have discriminative power to separate social questions between objective or subjective ones.
Question Time. Figure 2 shows the distribution of user
intent over the hour of the day when the question was asked
on 2005-05-01. It seems that objective and subjective questions do not have apparent differences in terms of question
time. In contrast, social questions show interesting patterns: the peak time for social questions is at 18:00 (end
of a work day), 15:00 (after lunch), and 03:00 (lonely in the
late night). This suggests that question time features are
capable of distinguishing social questions from objective or
subjective ones.
Question Asker’s Experience. Figure 3 shows the distribution of user intent over the question asker’s experience,
i.e., the number of questions the user has asked before. It

w∈q

Pint (w|d)=

N
X

P (w|Ck )P (Ck |d)

(5)

k=1

where Ck represents a category of user intent, P (w|Ck ) is its
corresponding unigram language model (see Section 3.3.2)
and P (Ck |d) is the probability that the document d belongs
to that category (see Section 3.3.1).
Compared to the category-based language model of Cao
et al. [2], the intent-based model above is more general and
more robust, because, instead of imposing hard mutuallyexclusive classifications, it classifies a question into multiple
(user intent) categories with certain probabilities.

3.3.1

Probabilistic Classification of User Intent

When computing P (Ck |d) in the above, intent-based language model, we adopt the question taxonomy proposed by
Chen et al. [4] which classifies the user intent of a question
as objective, subjective, and social. A number of popular
machine learning algorithms have been tried for question
classification, and finally Support Vector Machine (SVM) is
chosen due to its consistent outstanding performance. The
probabilistic outputs are obtained from SVM via Platt’s
method [7].
In addition to standard textual features (i.e., the bag of
words weighted by TFxIDF), a series of metadata features

974

Table 1: The performance of supervised learning
with different sets of features.
features
objective subjective social
text

0.693

0.689

0.152

metadata

0.609

0.642

0.378

text+metadata

0.731

0.693

0.412

Table 2: The performance of supervised learning vs
semi-supervised learning (co-training).
approach
miF1 maF1
Figure 3: The question asker’s experience feature.

0.510

co-training (text+metadata)

0.757

0.534

Pmix (q|d)=αPcla (q|d) + βPtra (q|d) + γPint (q|d)

(7)

where α, β, and γ are three non-negative weight parameters satisfying α + β + γ = 1. When γ = 0, the complete
mixture model backs off to the current state-of-the-art approach, i.e., the combination of the classic language model
and the translation-based language model only [8].

4.

EXPERIMENTS

We conducted experiments on two real-world CQA datasets.
The first dataset, YA, comes from Yahoo! Answers. It is
part of Yahoo! Labs’ Webscope1 L6 dataset that consists of
4,483,032 questions with their answers from 2005-01-01 to
2006-01-01. The second dataset, WA, comes from WikiAnswers. It contains 824,320 questions with their answers collected from WikiAnswers2 from 2012-01-01 to 2012-05-01.
We first experimented with question classification on 1,539
questions that are randomly selected from the YA dataset
and manually labelled according to their user intents. Those
questions were split into training and testing sets with a
proportion of 2:1.
Table 1 shows the performance (miF1 ) of [binary] question
classification through supervised learning (linear SVM) with
different sets of features. One can find that using both textual features and metadata features works better than using
either kind of features alone, for all question categories.
Table 2 depicts the performances (miF1 and maF1 ) of
question classification via supervised learning and also semisupervised learning (co-training) based on both textual and
metadata features. It is clear that co-training that regards
text features and metadata features as two views works better than supervised learning that simply pools these two
types of features together. The performance gain is probably due to the fact that co-training can make use of a large
amount of unlabelled questions in addition to the small set
of labelled questions.
We then experimented with question retrieval using a similar set-up as in [8]: 50 questions were randomly sampled
from the YA and WA datasets respectively for testing (which
were excluded from the CQA retrieval repositories to ensure the impartiality of the evaluation), and the top archive

Estimating Unigram Models for User Intent

Given the probabilistic classification results on all archive
questions, we can obtain the unigram language model for
each user intent category Ck through maximum-likelihood
estimation:
P
d∈Ck tf (w, d)P (Ck |d)
P
(6)
P (w|Ck )= P
0
w0 ∈d
d∈Ck tf (w , d)P (Ck |d)
where tf (w, d) is the term frequency of word w in document d. It is possible to employ more advanced estimation
methods, which is left for future work.

3.4

0.712

linear combination:

seems that subjective and social questions are more likely
to come from experienced users than new ones, probably
because experienced users are relatively familiar with each
other. This suggests that the question asker’s experience
features allow objective questions to separate from subjective or social ones.
It is time-consuming and error-prone to manually label
archive questions according to their user intents. Usually,
we only have a small set of labelled questions, which would
significantly limit the success of supervised learning for question classification. However, obtaining unlabelled questions
is quite easy and cheap. So it is promising to apply semisupervised learning [3] which can make use of a large amount
of unlabelled data in addition to the small set of labelled
data.
For this particular problem of user intent oriented question classification, we argue that the semi-supervised learning framework co-training [1] is particularly suitable. In the
co-training process, each example is described by two different feature sets (views) which provide different and complementary information about it. Ideally, those two views are
conditionally independent (given the class) and each view is
sufficient (to be used for classification alone). The textual
and metadata features mentioned above are both effective in
detecting the user intent of questions but with quite different discriminative powers for different question categories;
therefore, they can be considered as the two views for cotraining.

3.3.2

supervised (text+metadata)

Mixture Model

1

To exploit evidences from different perspectives for question retrieval, we can mix the above language models via

2

975

http://webscope.sandbox.yahoo.com/
http://wiki.answers.com/

Figure 4: The experimental results on Yahoo! Anaswers (left) and WikiAnswers (right) respectively
category-based language model since one question can belong to multiple (user intent) categories to different degrees,
and since a probabilistic question classifier is built automatically by taking into consideration of both textual features
and metadata features in the co-training framework.

Table 3: The model parameters for different question retrieval approaches.
C C+T C+T+I
.

α

1

0.3

0.18

β

0

0.7

0.42

γ

0

0.0

0.40

6.

questions (i.e., search results) returned for each test query
question were manually labelled as either relevant or not.
In order to see whether user intent relevance can improve
question retrieval performance, we compared the following
three approaches:

7.

REFERENCES

[1] A. Blum and T. Mitchell. Combining labeled and unlabeled data
with co-training. In Proceedings of the 11th Annual Conference
on Computational Learning Theory (COLT), pages 92–100,
Madison, WI, USA, 1998.
[2] X. Cao, G. Cong, B. Cui, C. S. Jensen, and C. Zhang. The use
of categorization information in language models for question
retrieval. In Proceedings of the 18th ACM Conference on
Information and Knowledge Management (CIKM), pages
265–274, Hong Kong, China, 2009.
[3] O. Chapelle, B. Schölkopf, and A. Zien, editors.
Semi-Supervised Learning. MIT Press, 2006.
[4] L. Chen, D. Zhang, and M. Levene. Understanding user intent in
community question answering. In Proceedings of the 21st
World Wide Web Conference (WWW), Companion Volume,
pages 823–828, Lyon, France, 2012.
[5] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar questions in
large question and answer archives. In Proceedings of the 14th
ACM International Conference on Information and Knowledge
Management (CIKM), pages 84–90, Bremen, Germany, 2005.
[6] C. D. Manning, P. Raghavan, and H. Schtze. Introduction to
Information Retrieval. Cambridge University Press, 2008.
[7] J. C. Platt. Probabilistic outputs for support vector machines
and comparisons to regularized likelihood methods. In Advances
in Large Margin Classifiers, pages 61–74. MIT Press, 1999.
[8] X. Xue, J. Jeon, and W. B. Croft. Retrieval models for question
and answer archives. In Proceedings of the 31st Annual
International ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR), pages 475–482,
Singapore, 2008.
[9] C. Zhai. Statistical Language Models for Information Retrieval.
Morgan & Claypool Publishers, 2008.

• the baseline approach which only employs the classic
language model (C);
• the state-of-the-art approach which combines the classic language model and the translation-based language
model (C+T) [8];
• the proposed hybrid approach which blends the classic
language model, the translation-based language model,
and the intent-based language model (C+T+I).
The model parameters were tuned on the training data to
achieve optimal results, as shown in Table 3. In the mixture
models (C+T) and (C+T+I), the ratio between parameter
values α and β was same as that in [8].
The retrieval performances of those approaches, measured
by Precision at 10 (P@10) [6] and Mean Average Precision
(MAP) [6], are reported in Figure 4. Consistent to the observation in [8], adding the translation-based language model
(C+T) brings substantial performance improvement to the
classic language model (C). More importantly, it is clear
that our proposed hybrid approach incorporating the intentbased language model (C+T+I) outperforms the state-ofthe-art approach (C+T) significantly, according to both P@10
and MAP on YA and WA.

5.

ACKNOWLEDGEMENTS

We thank Xiaobing Xue (UMass) who kindly shared his
code for the translation-based language model with us. We
appreciate the efforts of the three anonymous reviewers and
their useful comments and suggestions for improving this
work.

CONCLUSIONS

The main contribution of this paper is to show the usefulness of user intent for question retrieval in CQA. Our proposed intent-based language model supersedes the existing

976

