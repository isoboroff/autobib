The Knowing Camera: Recognizing Places-of-Interest in
Smartphone Photos
Pai Peng †

Lidan Shou †♮

Ke Chen †

Gang Chen †

Sai Wu†

♮

†

State Key Lab of CAD&CG
College of Computer Science and Technology
Zhejiang University
Hangzhou, China

{pengpai_sh, should, chenk, cg, wusai}@zju.edu.cn
ABSTRACT

of the place, probably with a URL directing to further information about it; (2) Afterwards, her friends would be
able to watch her online photos, automatically annotated
with respective place names, without requesting for timeconsuming and error-prone tagging. This paper reports our
recent work on the Knowing Camera (KC) project, which
aims at developing a system for recognizing outdoor Placesof-Interest (POIs) captured in smartphone photos, relying
on geotagged photo sharing Web services (e.g. Flickr).
Speciﬁcally, the key technical problem of the system can
be deﬁned as follows. Given (1) a database of POIs denoted
by P , where each POI has a name and geo-location, (2) a set
of geotagged images S, and (3) a query photo with its GPS
location and camera geometries, ﬁnd the most prominent
POI (target POI) captured in that photo.
Generally, there exist two approaches to this problem,
namely the spatial approach and the visual one. The spatial approach usually takes the query location and camera
geometries (also known as the field-of-view or FOV) to ﬁnd
the closest and most visible POIs in P . Unfortunately, such
solution fails to discriminate POIs in the save FOV, not to
mention the errors prevalent in the geotags and camera sensors.
The visual approach requires that each photo in S is associated with one or more POIs. By computing the visual
similarity between the query image and those in S, it is
possible to ﬁnd the best matching photos, and subsequently
retrieve the target POI which receives the most contributions from the matching photos.
However, the online geotagged photos are known to be
both visually and semantically noisy. The “truly” visually
similar ones (judged by human perception) comprise only
a small percentage, as the photos of the same semantics
(a POI) could have very diversiﬁed visual appearances. In
addition, each POI might be associated with other photos
which are just semantically irrelevant, probably due to (i)
users’ mistakes, (ii) the physical distances between the cameras and the objects, and (iii) humans or events recorded in
the photos.
We propose novel techniques to address the above problems. Our method relies on the following observations: First,
the camera geometries (FOV) are easy to obtain when a
photo is shot by a smartphone. However, those of the online geotagged photos are typically unavailable. Second, the
sensors in cameras are erroneous. The sensory readings of
phone direction, viewing angles, and camera location may
all contain uncertainty. Third, as online photos associated

This paper presents a framework called Knowing Camera for
real-time recognizing places-of-interest in smartphone photos, with the availability of online geotagged images of such
places. We propose a probabilistic ﬁeld-of-view model which
captures the uncertainty in camera sensor data. This model
can be used to retrieve a set of candidate images. The visual similarity computation of the candidate images relies
on the sparse coding technique. We also propose an ANN
ﬁltering technique to speedup the sparse coding. The ﬁnal
ranking combines an uncertain geometric relevance with the
visual similarity. Our preliminary experiments conducted in
an urban area of a large city show promising results. The
most distinguishing feature of our framework is its ability
to perform well in contaminated, real-world online image
database. Besides, our framework is highly scalable as it
does not incur any complex data structure.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information
Search and Retrieval

Keywords
places-of-interest, image retrieval, bag-of-visual-words

1. INTRODUCTION
The global population of smartphone users has hit one
billion in the past year. Every day and night, the world
is being captured through millions of phone cameras, and
then displayed in images via Internet social applications (e.g.
Facebook and Flickr) to an enormous audience. These images, ever increasing in numbers at an unprecedented rate,
comprise a rich and useful data source for the physical world.
Think of a tourist visiting an unfamiliar city holding a
GPS-equipped smartphone. Some of the greatest conveniences she could have are: (1) Upon taking a photo of a
place (e.g. a tower or a coﬀee shop) using her mobile phone,
the gadget displays in seconds a pop-up, showing the name
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

969

with each POI contain high impurity, simply computing the
visual similarity between the query and POI-tagged photos
leads to indiscriminating results – similarity values which
are indistinct among several POIs.
Our ﬁrst and second observations motivate the design of
a probabilistic FOV model. Based on this model, all POIs
relevant to the FOV are given a likelihood of being captured
by the camera. Our third observation leads to a solution of
visual similarity comparison based on sparse coding, a technique rooted in the signal processing domain. This technique
allows a query image to be given as a linear function of the
“bases”, which stand for a few representatives in the original candidate images. The noisy images are not accounted
in this case, making the resultant similarity (which we call
SC-similarity) reasonably discriminative. As a result, our
scheme can tolerate impure images containing multiple human faces and bodies, in contrast to most of the existing
works [1, 4, 3] which require clean and uncontaminated image database.
Our method only extracts “bag-of-visual-words” features
from each image, and does not require any additional visual data structure. The only structure being used is a
conventional spatial index (an R-tree in our implementation), which accesses the visual feature vectors of the photos by their geotag locations. This makes our scheme highly
scalable, eﬃcient, and easily adaptable to the existing geotagged photo sharing services. Another desirable feature of
the scheme is the possibility to balance the local recognition
accuracy and visual similarity computation cost, by varying
the range of the probabilistic FOV.
Summary of contributions:
• A simple and scalable framework for recognizing POIs
from smartphone pictures;
• A probabilistic FOV (pFOV) model to adopt the uncertainty in phone sensor readings;
• A visual similarity computing technique using sparse
coding, and an approximate nearest-neighbor ﬁltering
procedure to expedite the computation;
• Experiments on real phone-captured photos achieve
92% accuracy for POIs including landmarks and nonlandmarks.

v

R
p

a

q

r
q

di

qi

qi
Figure 1: The probabilistic FOV model consisting of 4 parameters, namely the camera location q, the camera direction ⃗v , the camera viewing angle α, and the maximum visible
distance R.
Next, the POIs associated with the local images undergo
a probabilistic FOV culling procedure, which removes the
POIs that are geometrically impossible to appear in the
query image (detailed in Section 2.1.2). Images associated
with any culled POI are deleted from the local image set.
The remaining local images now become the candidate images, and their associated POIs are considered the candidate
POIs.
Subsequently, we compute for each candidate POI p a
geometric-relevance value (denoted by geo-relevance), which
indicates the geographical probability that p appears in the
query pFOV (detailed in Section 2.1.3).

2.1.1

The Probabilistic FOV Model

Given a query q, ﬁgure 1 illustrates its probabilistic FOV
(pFOV) model, which is derived from the conventional FOV
(ﬁeld-of-view). The conventional FOV model consists of 4
parameters, namely the camera location q, the camera direction ⃗v , the viewing angle α, and the maximum visible
distance R. The ﬁrst three quantities can be obtained in
real-time from the sensory readings while the last parameter R is determined by the camera speciﬁcation.
It is important to note that the sensory readings may contain an abundance of uncertainty due to device errors. Using
the GPS data of the camera location as an example, the actual camera location might be a hundred meters away from
the GPS readings. Thus, we use a 2D Gaussian distribution
centered at the current location reading q to model the actual camera location. The shaded circle in Figure 1 indicates
the probable camera locations, with an uncertain radius of
r. Likewise, the direction ⃗v , and the viewing angle α are
all uncertain variables with uncertainty margins. For clarity
of illustration we do not plot these margins in the Figure.
Such probability distribution of FOV parameters deﬁne the
probabilistic FOV of the camera.

2. THE RECOGNITION FRAMEWORK
Our POI recognition framework consists of (1) a POI
database P , where each point p ∈ P has a geo-location p.loc,
and (2) an image database S, where each photo s ∈ S has
a geotag location s.loc and is also associated with a POI in
P . To expedite spatial accesses, we also generate an R-tree
which indexes all images in S by their locations s.loc.
Generally, a query q contains a query image q.img, and
its camera geometries stored in q.f ov, including its GPS location, the angles of view, the maximum visible distance,
and the direction of the camera. Each query is processed
in three consecutive phases, namely the spatial phase, the
visual phase, and the ranking phase, as described in the following.

2.1.2

Probabilistic FOV Culling

It can be seen that for any POI p appearing in the query
FOV, there must exist a probabilistic FOV instance (at qi )
which contains p. In other words, the union of all pFOV
instances must cover all possible candidate POIs. This observation leads to a pFOV culling algorithm, which relies on
the following Lemma.
Lemma 1: All candidate POIs whose locations are outside
the union of all pFOV instances can be discarded.
A sample process of pFOV culling is given in Figure 2,

2.1 The Spatial Phase
The spatial phase takes the camera location as the center
and makes a 2D square range query on the R-tree to retrieve
images which are geotagged in the query box. These images
are called “local images”.

970

query box

2.2.1

pd
pb
sb

pe

Camera
Direction

FOV

pc

q

se

sc

q¢¢
r

sd

o

To compute the visual similarity, each candidate image is
represented as a bag-of-visual-words column vector. Let D
be the matrix where each column is the vector for each candidate image in C. Then the problem can be described as:
Given a query image x, can we represent it as a linear combination of other candidate images(columns in D). This is a
typical problem of sparse coding and the objection function
is given by
1
(3)
min ∥Dw − x∥2 + α ∥w∥1
ω 2
where α controls the sparsity in w and ∥w∥1 is the l1 -norm
of the parameter vector. Furthermore, the original query
image x can be reconstructed by multiplying D by the sparse
code w.
This optimization problem is called Lasso and can be
solved by the least angle regression(LAR) approach [2]. The
optimization result is a sparse code as w = (ω0 , ω1 , ..., ωp )T ,
where ωi indicates the contribution of the ith candidate
image to reproduce the query image. Weight value ωi is
then used as the visual similarity between candidate image
i and the query. Finally, these weight values are aggregated
by diﬀerent POIs, so that each POI receives the sum of
all the weights from its associated candidate images. The
aggregated weights are called sparse coding similarity (SCsimilarity) for each POI.
∑
di ∈P OI ωi
∑
SC-similarity =
(4)
ωi

pa

q¢

sf

pf

Figure 2: Illustration of pFOV culling and candidate images/POIs. The blue markers indicate the geotagged locations of the images {si }, while the red ones {pi } indicate
their respective POIs. The circular sectors centered at q, q ′ ,
and q ′′ illustrate the pFOV instances of q.
where POIs located in the shaded region can be culled.
Thus, POIs pe , pf can be culled, because their actual FOVs
cannot be any pFOV instance for q. However, pd cannot
be culled because it is within the reach of a possible pFOV
instance (q ′ ), even when it is outside the query box.
Given the current camera parameters the pFOV culling
algorithm can quickly determine the candidate POIs which
may possibly appear in the image. As a result, the cost
of subsequent evaluation of geo-relevance and SC-similarity
can be reduced signiﬁcantly.

2.2.2

2.1.3 Computing Geo-Relevance

−

∥θ∥2
2σ1 2

−

·e

∥d∥2
2σ2 2

(1)

where d = ∥pq∥. σ1 and σ2 are parameters which ensure
that P (θ, d) is a negligible value (≈ 0) if p is outside the
FOV region. Thus, the probability of a POI p captured by
a query FOV is a cumulative distribution function given by
∫
geo-relevance =

−

e

∥cθ∥2
2σ1 2

−

·e

∥d∥2
2σ2 2

dq

ANN Filtering

So far, we have proposed the method of acquiring visual
similarity between a query image and a certain candidate
image via sparse coding (SC). However, the solution to SC
is typically time-consuming when the number of columns of
D is large.
To improve the eﬃciency without compromising the SC
eﬀect, we conduct an Approximate Nearest Neighbor (ANN)
ﬁltering procedure on the candidate columns before conducting sparse coding. ANN is able to retrieve the top-k approximate nearest-neighbours at signiﬁcantly reduced cost compared to exact NN search in high dimensions. Since the SC
process aims at reconstructing the query image with similar
ones in D, columns which are dissimilar to the query mostly
do not contribute in the reconstruction. Therefore, the reason for using ANN is to discard the dissimilar columns before
invoking the expensive SC process.

Now let us evaluate the probability that a POI p is captured by an exact FOV. As shown in Figure 1, given an
exact FOV at q and a point p whose distance to q is d and
→
its viewing angle (the angle between ⃗v and −
qp) is θ, it is easy
to see that p has a higher probability to be captured by the
camera if d is smaller or angle θ is smaller. Speciﬁcally, we
model the probability distribution function of POI p being
captured by an exact FOV at q as the following function:
P (θ, d) = e

Computing SC-similarity

2.3

(2)

The Ranking Phase

Finally, for each candidate POI, we compute a vote score
as a linear combination of the geo-relevance and SC-similarity
of each candidate POI
score = λ · geo-relevance + (1 − λ) · SC-similarity (5)

Q

where Q is the circular region for the Gaussian distribution
of the camera location with radius r, c > 0 is a factor simulating the 2D uncertainty of the angular parameters, namely
⃗ In practice, the geo-relevance is evaluated in a disα and d.
cretized form of Equation 2. This is omitted to save space.

where 0 ≤ λ ≤ 1 is a balancing factor. The POI with the
top ranking score is taken as the ﬁnal result.

2.2 The Visual Phase

3.

Given a list of candidate images C, we consider each of
their respective visual feature vectors as a basis signal, and
employ the sparse coding technique to obtain a linear combination of the basis vectors, which maximally reproduces
the original signal (the feature vector of the original image).

Image Dataset
Given the same availability of image
data, our framework is expected to perform better in sparsely
populated regions, as the geo-relevance and visual similarity
could be more discriminative. Therefore, we test the framework for urban area only. We download from Flickr 151,193

971

EXPERIMENT RESULTS

1.0

City Hall

0.9
0.8
accuracy

Hotel ***

KC(λ =1)
KC(λ =0)
KC(λ =0.5)
BOW(λ =0.5)
BOW(λ =0)

0.7

0.6

Mosque ***

0.5

0.4
0.3

Figure 3: Noisy dataset downloaded from Flickr. The red
frames indicate the “noisy” images, which do not show outdoor appearance of the places.
Table 1: Results of Computational Cost

0.2
0.0

0.1

0.2

0.3
0.4
0.5
query box size(km)

0.6

0.7

Figure 4: Results of Recognition Accuracy
Method
KC (no culling)
KC (culling, no ANN)
KC (culling, ANN)
BOW (culling, no ANN)
BOW (culling, ANN)

Query time
31.56s
7.03s
0.68s
0.79 s
0.31s

# candidate images
12748
3634
412
3634
412

image set. However, as the query box size increases further,
the accuracy of pure SC-similarity (λ = 0) declines considerably. This is a clear indication that the recognition problem
becomes harder as the search space increases. A more apparent decline can be observed for the BOW method, due to
the same reason. Fortunately, the problem in SC-similarity
can be compensated by the geo-relevance in our scheme, as
when λ = 0.5, the accuracy is very stable and remains to
be above 90%. Such results conﬁrm the eﬀectiveness of our
spatio-visual ranking score.

geotagged images containing 2,256 distinct POIs in a densely
populated area of a large city. The POI information are acquired from other sources. Although the dataset has limited
size, it can suﬃciently simulate the density of photos in urban areas of the world. A larger dataset (spanned over a
greater region) is not expected to impact the recognition
performance, due to the locality introduced by the spatial
range query. It is also important to note that the majority
of the dataset are noisy images which do not capture the
outdoor appearances of the POIs, as depicted in Figure 3.

4.

CONCLUSION

We presented the Knowing Camera framework for realtime recognizing places-of-interest in smartphone photos.
The framework captured the device uncertainty in a probabilistic ﬁeld-of-view model, which could be used to cull candidate images from the database. The visual similarity computation of the candidate images relied on the sparse coding
technique. We also performed an ANN ﬁltering technique
to speedup the sparse coding. The ﬁnal ranking score combined the geometric relevance and the visual similarity. Our
experiments in an urban area showed promising results.

Query Set We develop an Android APP to capture query
photos of the POIs in the dataset. The APP records the
camera FOV parameters at the shooting time. The true
POI of each query image (ground truth) is determined by
the user when the photo is captured. We employ 4 users
who shoot totally 170 POIs, including 48 landmarks and
122 non-landmarks. Each POI is captured for 4 times by
diﬀerent users. The results presented below are averaged
among the users.

Acknowledgments
The work is supported by the National Science Foundation
of China (GrantNo. 61170034).

Baseline Approach
Besides our proposed framework
KC, we also implement a state-of-the-art approach bag-ofvisual-words(BOW ) as a baseline.

5.

Performance Results
Table 1 shows the results of computational costs with
pFOV culling and ANN ﬁltering enabled or disabled. It
can be seen that pFOV culls a signiﬁcant number of images
from the original query box. ANN can further reduce the
number of candidate images (columns) by almost an order of
magnitude. Therefore, these two techniques can eﬀectively
reduce the query processing cost.
The POI recognition accuracy of the proposed approach is
illustrated in Figure 4, where we vary the spatial query box
size and plot the results of diﬀerent λ values. When λ = 1,
the KC scheme degenerates to a pure spatial method. For
comparison, we also plot the accuracy of the BOW. It can
be seen that (i) The pure spatial method alone does not perform well; (ii) As the query box expands, the accuracy of all
schemes is signiﬁcantly improved (when box size < 120m)
because more relevant photos are added into the candidate

REFERENCES

[1] Y. S. Avrithis, Y. Kalantidis, G. Tolias, and E. Spyrou.
Retrieving landmark and non-landmark images from
community photo collections. In ACM Multimedia,
pages 153–162. ACM, 2010.
[2] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani.
Least angle regression. The Annals of Statistics,
32(2):407–451, 2004.
[3] X. Li, C. Wu, C. Zach, S. Lazebnik, and J.-M. Frahm.
Modeling and recognition of landmark image collections
using iconic scene graphs. In ECCV, pages 427–440.
Springer, 2008.
[4] M. Z. Zheng, Yan-Tao, Y. Song, H. Adam,
U. Buddemeier, A. Bissacco, F. Brucher, T.-S. Chua,
and H. Neven. Tour the world: Building a web-scale
landmark recognition engine. In CVPR, pages
1085–1092. IEEE, 2009.

972

