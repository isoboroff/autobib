Topic Hierarchy Construction for the Organization of
Multi-Source User Generated Contents
1

Xingwei Zhu1∗, Zhao-Yan Ming2†, Xiaoyan Zhu1 , Tat-Seng Chua2

State Key Laboratory of Intelligent Technology and Systems, Tsinghua National Laboratory
for Information Science and Technology, Department of Computer Sci. and Tech., Tsinghua University
2
Department of Computer Science, School of Computing, National University of Singapore, Singapore

etzhu192@hotmail.com, mingzhaoyan@nus.edu.sg, zxy-dcs@tsinghua.edu.cn,
chuats@nus.edu.sg
ABSTRACT

Obama’s performance in the 2012 presidential campaign,
he may refer to blogs on Blog sites like Blogger 1 for authoritative criticisms, ask questions on community question
answering (cQA) sites like Yahoo! Answers 2 for speciﬁc information and read tweets from Twitter 3 to follow up with
his friends’ opinions.
However, the volume of UGCs is huge and increasing every day. Even for a speciﬁc topic, it is usually impossible for
users to go through all the contents and manually identify
the newly emerging and important sub-topics. On the other
hand, though in some UGC sources like Wikipedia 4 , the data
is well organized into structured format which can be easily accessed, they cannot catch up with the ever changing
internet since they rely on human to compile and update.
The characteristics of diﬀerent kinds of UGCs are diversiﬁed. Information in any single source is always limited and
domain speciﬁc. For example, if a user requires a technical report for “IPhone 5”, it is better to refer to blogs or
cQAs instead of tweets in Twitter. But when he wants to
know how his friends like “IPhone 5”, Twitter turns out to
be a better place. As a result, in order to have an overall
picture of the topic, users have to refer to multiple UGC
sources, which further increases their burdens to integrate
these heterogeneous contents together.
To address the above problems, in this paper, we propose
to organize information from multiple UGC sources using
an automatically generated topic hierarchy. The task is not
trivial due to the following three challenges:
• Instead of merely constructing a hierarchy [8] or organizing contents according to an existing hierarchy [12],
our task requires the two problems to be solved at the
same time, which requires a seamless integration of
state-of-the-art techniques in both ﬁelds.

User generated contents (UGCs) carry a huge amount of
high quality information. However, the information overload
and diversity of UGC sources limit their potential uses. In
this research, we propose a framework to organize information from multiple UGC sources by a topic hierarchy which is
automatically generated and updated using the UGCs. We
explore the unique characteristics of UGCs like blogs, cQAs,
microblogs, etc., and introduce a novel scheme to combine
them. We also propose a graph-based method to enable
incremental update of the generated topic hierarchy. Using the hierarchy, users can easily obtain a comprehensive,
in-depth and up-to-date picture of their topics of interests.
The experiment results demonstrate how information from
multiple heterogeneous sources improves the resultant topic
hierarchies. It also shows that the proposed method achieves
better F1 scores in hierarchy generation as compared to the
state-of-the-art methods.

Categories and Subject Descriptors
H.0 [Information Systems]: General; H.3.2 [ Information Storage and Retrieval]: Information Storage

Keywords
User Generated Contents, Information Organization, Topic
Hierarchy

1. INTRODUCTION
With the rapid development of Web 2.0, user generated
contents (UGCs) on the internet are becoming important
sources for information navigation and knowledge acquisition. For example, when a user wants to know Barack

• Though there have been many works proposed to organize information from individual UGC sources like
blogs [4], cQAs [12] or tweets [14], etc., integrating information from multiple sources is a new and diﬃcult
problem since they are heterogeneous and may contain
diﬀerent kinds of noise and errors.

∗

This work was done when the ﬁrst author was a visiting
student in National University of Singapore.
†
Corresponding author.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

• In order to keep up with the ever changing internet, a
novel framework is required to enable real time update
on the hierarchies with newly obtained data. The real
1

http://www.blogger.com/
http://answers.yahoo.com/
3
https://twitter.com/
4
http://en.wikipedia.org/wiki/Main Page
2

233

the three main modules, i.e., topic term identiﬁcation, topic
relation identiﬁcation and topic hierarchy generation including the real time hierarchy updating algorithm. In section 5,
we discuss our evaluation method, experiments and results.
Finally Section 6 concludes the paper.

[BLOG] Watch Presidential Debate
with Barack Obama & Mitt
Romney
……
[CQA] When will the Ultimate
debate between Barack Obama
and John McCain take place?
……

Benghazi Attack

Libya

[TWEET] There will be 4
participants in the Debates
tomorrow night

2.

Poll

Tax
Policy
licy

Debate

Barack Obama
Barac
Ideology
ogy
Democrat
Islam
Socialism
Soci

Figure 1: The topic hierarchy for “Barack Obama”.
It organizes 119 blogs, 476 cQAs and 49749 tweets.
Due to the space limit, we only illustrate the title
of one blog, one cQA and one tweet for one of its
sub-topics, debate.
time requirement is very diﬀerent from previous approaches [8] [13], where a hierarchy is built using static
data set and requires no further update.
In view of the above challenges, a three step framework
is proposed to organize UGCs using automatically generated topic hierarchies. Given a collection of UGCs such as
blogs, cQAs and tweets on a speciﬁc topic, we ﬁrst identify
potential topic terms from these sources. Then assisted by
external knowledge from Wikipedia, WordNet 5 and search
engine results, we propose a novel scheme to identify subtopic relations between topic terms using multiple evidences.
Finally, by treating topic terms as nodes and sub-topic relations between them as edges, a graph-based algorithm is
used to incrementally generate a topic hierarchy. Each time
new data is available, the same framework can also be used
to update the previously generated hierarchy with newly
emerging sub-topics. Using the resultant topic hierarchy,
the UGCs can be organized to nodes on the hierarchy according to their relevant sub-topics. In Figure 1, we show an
example topic hierarchy for the topic “Barack Obama”. It
contains 11 sub-topics, under which a total of 119 blogs, 476
cQAs and 49749 tweets are organized. The contributions of
our work can be summarized as:
• We propose a novel framework to organize UGCs by
automatically generated topic hierarchies with real time
update. The experimental results demonstrate its superior performance over the state-of-the-art methods.

3.

• We leverage heterogeneous information from multiple
sources by exploring their unique characteristics, and
propose a novel scheme to combine the evidences extracted from these sources to enhance the framework.

PROBLEM FORMULATION

We deﬁne the root topic C as a word or phrase which indicates the users’ search intends. It can be an entity (e.g.,
“Barack Obama”), an event (e.g., “Benghazi Attack”) or
other informative concept. Given a root topic, we deﬁne
its information source set Sc as Sc = {si }N
i=1 , in which si
indicates a collection of documents from the ith information
source. They are collected for C and can be automatically
updated when new data is available.

The remainder of this paper is organized as follows: In section 2 we present the related work and in Section 3 we formally deﬁne the problem. Section 4 presents the framework of our approach, followed by detailed description on
5

RELATED WORK

Topic Term Detection: TF-IDF weighting has been found
to be very eﬀective [2] [6] for topic term extraction from
documents. To avoid the need to estimation IDF, which
requires huge document collections, Matsuo et al.[11] employed a sentence-level word co-occurrence matrix to ﬁnd
the keywords in a single document. NLP tools such as TextRunner [20] has also been applied to extract keyword terms,
however, since these tools are linguistics-based, it is hard
to use them to analyze UGCs which usually contain multiple languages and are ill grammar. In some recent works
[10], the hierarchical cluster structure of documents (e.g.,
Wikipedia) is also adopted to improve the keyword selection performance.
Taxonomy Induction: Given a small document set, Lawrie
et al. [9] proposed to extract its intrinsic topic hierarchy by
estimating the topicality of terms and co-occurrence probability between them using only the given documents. Given
a candidate term set, Navigli et al. [13] trained classiﬁers
to detect is-a relations between terms and utilized a graphbased algorithm to optimize the term taxonomy. Besides,
Snow et al.[16] introduced a probabilistic model to determine
the most possible hierarchy for a set of concepts. Using both
statistics-based and pattern-based features, Yang et al.[19]
further proposed the metrics of information function and
created hierarchies by an insert process, in which nodes are
inserted onto the hierarchy to minimize the change of information functions. A recent work [21] extended this approach
by employing more objective functions, i.e., minimum Hierarchy Discrepancy and minimum Semantic Inconsistency to
achieve a better insertion decision.
Approaches using Multiple Sources: Information from
multiple sources provides researchers clues in diﬀerent views
and helps to achieve better results by overcoming the bias
of any single information source [3] [5] [17]. Han et al.[3]
used information from Wikipedia, WordNet and a NE cooccurrence corpus to measure the semantic relatedness between words. Although they just chose the conditionally
most conﬁdent source to estimate the relatedness, instead
of integrating the three sources together, the results have
already outperformed the methods which only used single
source. On the other hand, Hoﬀart et al.[5] proposed to integrate information from Wikipedia, WordNet, Geo-Name
corpus, etc., to build Yago2, an open domain structured
knowledge base.

http://wordnet.princeton.edu

234

The data in Sc is usually unstructured and contains noise
and errors. We deﬁne a topic hierarchy as H = {T, M, R},
in which all useful information in Sc is organized using the
following three components:

Topic Term
Identification

Generate/Update

• Topic Set T = {t1 , t2 , ..., ti , ...}, where ti indicates
a topic term. T includes the root topic C and the
potential sub-topics of C for the documents in Sc .

Hierarchy
Information
Source Set

• Document-Topic Mapping M : d → 2T , where d
indicates a document in Sc . M assigns each document
with terms in T as its relevant topics. For some noisy
documents, the mapping results may be ϕ.

Topic Relation
Identification

Topic Hierarchy
Generation

The proposed framework

• Sub-topic Relation Set R: Denote r(tA , tB ) as a
sub-topic relation, which means tB is a sub-topic of
tA . R = {r1 , r2 , ..., ri , ...} is a set of sub-topic relations
between the topics in T . It links all the topics into a
hierarchy rooted at C.

Figure 2: The proposed method: maintaining a
Topic Hierarchy using real-time data from multiple
heterogeneous UGC sources by a three step framework.
Heuristic
POS-tag

Details
A topic term must be a NP6 (“battery”)
or a NP phrase(“battery life”).
Length
A topic term contains at least 1 word
and at most 3 words.
Frequency A topic term must occurs in at least 3
documents in the corpus.
If a phrase is like “N P1 of N P2 ”(“life
of battery”) or “N P2 ’s N P1 ”(“battery’s
life”), it will be converted to “N P2
LinguisticN P1 ”(“battery life”).
Alteration
Rules about plural and abbreviation are
also used.

Formally, we deﬁne our task as follows:
Information Organization Task : Given a root topic C
and its information source set Sc , we aim to build and continuously update a topic hierarchy H for C in order to organize the information in Sc according to their relevant topics.

4. APPROACH
4.1 Information Sources
Inevitably, every single UGC source has ﬂaws. Take blog
as an example, although well-written, blog usually focuses
on narrow points (e.g., technical reports for “IPhone 5” or
big events for “Barack Obama”) and takes a long time work
before being published online. On the other hand, tweets can
provide timely information (e.g., release date for “IPhone 5”
in Europe), while they also contain a huge amount of noises
(e.g., “should I buy a IPhone 4S or IPhone 5? ”). These
drawbacks limit the potential uses of the UGCs.
In this paper, we will tackle the above problem by combining the power of three prevailing UGCs, i.e., Blogs, cQA and
Twitter as our information sources. we also utilize domainindependent factoid knowledge from Wikipedia, WordNet,
etc. to supplement the limited and speciﬁc UGC sources.

Table 1: Heuristic rules used in topic identiﬁcation
interests, we denote them as grounding topics, tg . To characterize a potential grounding topic, we ﬁrst adopt four heuristic rules from [7], i.e.,the pos-tag heuristic, length heuristic,
frequency heuristic and linguistic-alteration heuristic. In Table 1, we present a brief introduction of them.
Next, for each source in Sc , we separately estimate the
TF-IDF scores for the potential grounding topics and obtain
topic terms for each document as follows.
Blogs: We use the blog title and content to compute the
TF-IDF scores for terms in blogs. Since the title of a blog
usually indicates its main topics, we double the weights of
terms in titles. For each blog, the top 5 ranked terms by
TF-IDF are selected as its topic terms.
cQAs: We use the question title, description and the best
answers to compute the TF-IDF scores. For each cQA, the
top 5 ranked terms are selected as its topic terms.
Tweets: We use the content and the words surrounded by
hash tags to compute the scores. Since tweets are short and
cover fewer topics, we only collect the top ranked term as
their topic terms.
Simply putting the topic terms of documents together may
bring lots of noise. In order to select high quality and popular topic terms for the root topic, we adopt the following
two assumptions:

4.2 Framework
To address the information organization task, we introduce a three step framework as illustrated in Figure 2. For a
given root topic C, we ﬁrst identify potential sub-topic terms
from Sc . Then we identify the sub-topic relations between
the sub-topics. Finally we generate the topic hierarchy and
assign UGCs in Sc onto the hierarchy. The following three
sections will describe the details for each module. We will
also show how the framework can be used to update an existing hierarchy when new data is available.

4.3 Topic Term Identification
We collect potential sub-topics for C in two steps: (1) we
extract keywords from documents in Sc to obtain an initial
topic set; and (2) we extend the topic set with more abstract
and general topics using knowledge from external sources.

Assumption 1: A topic term is of high quality only if
it can be extracted from documents in diﬀerent information
sources.
Assumption 2: A topic term is popular only if it can be
extracted from many diﬀerent documents.

4.3.1 Grounding Topic Extraction

6
We use the Stanford CoreNLP toolkit (http://nlp.stanfo
-rd.edu/software/corenlp.shtml) to get the POS-tags.

The documents in Sc contains many potential sub-topic
terms. Since these terms directly reﬂect people’s focuses and

235

We thus only collect the topic terms that are among the
top 200 frequent topic terms in at least 2 diﬀerent information sources into the grounding topic set, TG = {tg1 , tg2 , ...}.

We also use the category and sub-title evidences from
Wikipedia, i.e., ewcate(tA ,tB ) and ewtitle (tA , tB ), which can
be estimated as follows, respectively:
{
1 : wikipage tB has category tag tA
ewcate (tA , tB ) =
0 : otherwise.
(1)
{
1 : tB is a subdirectory of tA on a wikipage
ewtitle (tA , tB ) =
0 : otherwise.
(2)

4.3.2 Topic Set Extension
Although TG contains many low-level topic terms (e.g.,
siri , battery for “IPhone 5”), it lacks middle level topic
terms (e.g., software and device) partly because they are
too abstract and general for UGCs. Since these middle level
terms are also useful in topic hierarchy generation, we investigate external information sources to obtain them.
Search Engine: For each term in TG , we use two patterns, i.e., “* such as <slot>” and “<slot> of * ” to obtain
its higher level topic terms. We ﬁrst ﬁll the term into the
slot and submit it to a search engine like Bing 7 . Then we
collect the noun phrases in the position of the wildcard on
the returned search pages as middle level topics.
WordNet: WordNet contains hypernym relations between
concepts. If a topic term in TG is included in WordNet, we
collect the terms in its direct hypernym synset as middle
level topics.
Wikipedia: If a topic term in TG is a Wikipedia title, its
category tags are also collected as middle level topics.
The middle level topics shared by at least 2 sources are
collected into the extended topic set, TE . Then the ﬁnal
candidate topic set T = {C} ∪ TG ∪ TE .

Evidences from WordNet: If tA and tB can be found
in WordNet and tA is an ancestor of tB , then it is very
probable that tB is a sub-topic of tA . In practice, if tA is an
ancestor of tB in WordNet, we use the WordNet similarity
[15] to estimate the WordNet evidence as ewnet (tA , tB ) =
1
, where W N Dis(tA , tB ) denotes the length of
W N Dis(tA ,tB )
the shortest path between tA and tB in WordNet. Otherwise
we just let ewnet (tA , tB ) = 0.
Evidences from Search Engine Results: We also use
patterns like “<topic> such as <subtopic> and” to collect
evidences from the internet. For each term pair tA and tB ,
we generate a query by ﬁlling them into the pattern (e.g., “tA
such as tB and”) and submit it together with the root topic 9
to the search engine. Denote patterni as the ith pattern, we
can obtain a 0/1 pattern-based evidence espatterni (tA , tB ),
whose value is set to 1 only if the search engine returns more
than ζ results that contain this query; otherwise it is set to
0. In practice, we select 6 patterns as listed in Appendix A
and ζ is set to 10 empirically.

4.4 Topic Relation Identification
The sub-topic relation between topic terms provides essential information to organize the topics. Take “Barack
Obama” as an example, if we know that there is a sub-topic
relation between two of its sub-topics, e.g., policy → tax ,
which indicates that tax is a sub-topic of policy , it will be
very probable that there is a path between the two topic
terms on the topic hierarchy.
To infer a sub-topic relation r(tA , tB ), we need to estimate the probability that the topic tB is a sub-topic of tA .
Inspired by [18], we approximate this probability with an
empirical score e(r(tA , tB )) which is estimated by evidences
from multiple sources. The evidences are summarized in Table 2 and the details are listed as follows:
Evidences from the Information Source Set: If there
is a sub-topic relation between two topic terms, they must
be highly related, where contextual distribution can be used
to measure this relatedness. More speciﬁcally, for each topic
term, the nouns, verbs and adjectives that co-occur with it
in the documents/sentences in Sc are collected as its document/sentence level contexts. For each term pair tA and tB ,
the document and sentence level contextual evidences, i.e.,
edistrdoc (tA , tB )8 and edistrsen (tA , tB ) are deﬁned as the cosine similarity between the corresponding contexts of them.
Evidences from Wikipedia: Pointwise Mutual Information (PMI) is also a measurement for the relatedness between two terms. We compute the PMI over a subset of
Wikipedia corpus, which only includes pages that are not
redirect pages and contain more than 1000 words. For tA
and tB , we compute the normalized PMI as npmi(tA , tB ) =
pmi(tA ,tB )
p(tA ,tB )
, where pmi(tA , tB ) = log p(t
. Then the
−log[p(tA ,tB )]
A )p(tB )
pmi evidence epmi (tA , tB ) =

To combine the above evidences, instead of simply adding
or multiplying them together like [19][21], we ﬁrst divide
the evidences into two sets as shown in Table 2, i.e., the
directed-evidence set Edir which includes the evidences
that can determine the direction of the sub-topic relation
and the undirected-evidence set Eund in which the evidences bear no directionality information.
For the directed-evidences, we use a linear combination to
combine them together as follows:
∑
edir (tA , tB ) =
wk · ek (tA , tB )
(3)
ek ∈Edir

where wk is the weight of the kth evidence∑in determining
the direction of the sub-topic relation and k wk = 1.
Since the training data is not always available for open
domain topics, supervised methods used in previous works
[21] on weight estimation cannot be used to calculate wk .
In this paper, we propose an unsupervised method to meet
this challenge. The basic idea of our method is that since
sub-topic relation is directed, it is not possible that both
r(tA , tB ) and r(tB , tA ) exist. Therefore, if a directed-evidence
ek supports both r(tA , tB ) and r(tB , tA ), which happens
when the diﬀerence between ek (tA , tB ) and ek (tB , tA ) is not
signiﬁcant, ek should be less weighted. More speciﬁcally,
we estimate the weight wk for each directed-evidence ek as
k
wk = ∑difdif
, in which difk is calculated as follows:
k
k

difk =

1+npim(tA ,tB )
.
2

1
pk

∑

|ek (tA , tB ) − ek (tB , tA )| · lk,tA ,tB

tA ,tB ∈T
tA ̸=tB

7

http://www.bing.com
We do not use tweets in generating document level contextual evidence since they are relatively too short.
8

9

236

The root topic is used to ﬁlter out irrelevant search results.

directed-evidences in Edir
espattern0 - espattern5
ewtitle
ewcate
ewnet
undirected-evidences in Eund
edistrdoc
edistrsen
epmi

Source
search engine
wikipedia
wikipedia
wordnet
Source
information set
information set
wikipedia

Algorithm 1 Hierarchy Generation Algorithm
Input:
T : the candidate topic set;
Output:
Rret : the sub-topic relation set of the resultant hierarchy;
Tret : the topic set of the resultant hierarchy;
Initialize T0 = {C}, R0 = ϕ
for i = 1 TO ∞ do
t ← selectTermFrom(T − Ti−1 )
if t is NIL then
Rret ← Ri−1
Tret ← Ti−1
break
end if
Ti ← t ∪ Ti−1
Ri ← Ri−1
for all tk IN Ti−1 do
if edgeBetween(tk , t) exists then
Ri ← Ri ∪ edgeBetween(tk , t)
end if
end for
edgeWeighting(Ri )
Ri = hierarhcyPruning(Ri )
end for

Table 2: The sources of evidences we use to estimate
the probability of a sub-topic relation
where lk,tA ,tB = max{ek (tA , tB ), ek (tB , tA )} gives higher
weights
to the evidences with high values and pk =
∑
tA ,tB ∈T,tA ̸=tB |ek (tA , tB ) + ek (tB , tA )| punishes the evidences that are too general. Then the ﬁnal score e((r(tA , tB ))
is estimated using the following equation:
∏
e(r(tA , tB )) = edir (tA , tB ) ·
es (tA , tB )
(4)
es ∈Eund

∏

where es ∈Eund es (tA , tB ) combines all the undirected-evidences for the two topic terms.

4.5 Topic Hierarchy Generation
latedness between ts and topic terms in Ti−1 . Note that NIL
will be returned if T − Ti−1 = ϕ or score(t) is 0.
Once we have added t into Ti−1 to form Ti , we also add all
the edges between t and topics in Ti−1 into Ri−1 , resulting
in Ri (line 10 - 15). In order to guarantee that edges in
Ri make up a valid hierarchy, next we use a graph based
method to prune the edges in Ri as follows.

By treating the sub-topics in T as nodes, sub-topic relations between them as edges, we can generate a sub-topic
graph. Next, we estimate the weight for each edge using the
scores estimated in section 4.4 using the following equation:

|e(r(tA , tB )) − e(r(tB , tA ))| · e(r(tA , tB )),



if e(r(tA , tB )) > e(r(tB , tA ))



and tA ̸= tB
w(r(tA , tB )) =
0,
if e(r(tA , tB )) < e(r(tB , tA ))




and tA ̸= tB


1,
if tA = tB
(5)
in which w(r(tA , tB )) is proportional to both e(r(tA , tB ))
and the diﬀerence between e(r(tA , tB )) and e(r(tB , tA )), indicating the strength of the edge from the node tA to tB . By
removing the zero weighted edges, equation 5 also guarantees that there is only one directed edge between two nodes,
which is essential for a valid hierarchy.
Next, we need to prune this graph into a hierarchy. Inspired by [13], we employ a graph based method to tackle
this problem. Diﬀerent from this work, we take an iterative approach: at each step we only add one sub-topic into
the hierarchy, which makes our method amendable to incrementally update the hierarchies. In general, the proposed
hierarchy generation algorithm can be formalized as Algorithm 1.
Given a candidate topic set T for C, the algorithm functions as follows. Let Ti and Ri be the resultant topic set and
sub-topic relation set of the hierarchy after the ith iteration,
we initialize them as T0 = {C} and R0 = ϕ (line 1). In the
ith iteration, we ﬁrst add a topic term t in T − Ti−1 into
Ti−1 (line 3 - 9) using the following function:
∑
t = argmax
(w(r(tk , ts )) + w(r(ts , tk )))
(6)
ts ∈T −Ti−1

Edge Weighting using Topic Relatedness (line 16):
Edge weighting assigns score to each edge in Ri , indicating
its importance in constructing the hierarchy. Our method
generalizes that of [13] by weighting each edge in Ri with the
estimated weights instead of simple 0/1 values. The process
is as follows:
• If a topic term is related to many grounding topics
in TG , it could be important in the hierarchy. Let
wt (tk ) denotes the weight of ∑
a topic term tk in Ti ,
we estimate it as wt (tk ) =
tg ∈TG w(r(troot , tg )) ·
w(r(tk , tg )), in which w(r(troot , tg )) gives the importance of the grounding topics, and w(r(tk , tg )) reﬂexes
the relatedness between tk and the grounding topics.
• Then for each node tk ∈ Ti , by denoting L = {tu →
|L|
tu+1 }u=0 as a path that connects troot and tk , its score
∑
is calculated as: scoreL = |L|−1
u=0 wt (tu )·w(r(tu , tu+1 )).
Next, for each edge ts → tk , its weight is estimated as
follows:
wr (ts → tk ) =

max

L ends
with ts →tk

scoreL

(7)

Hierarchy Pruning using Optimum Branching (line
17): Given the edge weighting results, we can use the ChuLiu/Edmond’s optimum branching algorithm [1] to ﬁnd a
subset of the current edge set Ri , which is the optimized
hierarchy for the given topic terms where every non-root
node has only one parent and the sum of the edge weights
are maximized.

tk ∈Ti−1

∑
where t is the topic term that maximizes score(t) = tk ∈Ti−1
(w(r(tk , ts )) + w(r(ts , tk ))), which indicates the overall re-

237

Category

When the iteration terminates at the N th iternation, the
resultant topic set TN −1 and sub-topic relation set RN −1
will be collected for the ﬁnal topic hierarchy. To guarantee
that the hierarchy is speciﬁcally related to the given topic C
and the documents in Sc , we further remove (1) the nodes
that are not reachable for the root topic and (2) the leaf
nodes that are not in the grounding topic set. Finally, for
each topic term t in the topic set, we assign the documents
in Sc whose topic terms contain t to the corresponding node,
resulting in M , the document-topic mapping function of the
resultant topic hierarchy.

Digital
Products
Politicians
Cosmetics
Corporations

Topic
IPhone 5
IPad mini
Xbox kinect
Barack Obama
Mitt Romney
Hillary Clinton
Chanel
Estee Lauder
Facebook Inc.
Microsoft Corp.
Blizzard Inc.

blog
182
192
195
193
188
190
179
194
190
175
192

cQA
1,523
797
1,476
1,043
1,024
1,050
985
1,049
973
1,034
999

tweet
411,826
2,782
30,261
426,811
2,759
811
2,782
17,816
429,979
429,483
2,022

Table 3: Statistics on the collected data sets in each
information source
Topic
IPhone 5

4.6 Topic Hierarchy Update
For a given topic, the information on the internet is everchanging. Diﬀerent from many previous approaches that
work on static corpus, we ﬁnd it useful and necessary to
dynamically sketch evolving topic hierarchies for users. For
example, assume we have built a topic hierarchy for “Barack
Obama” before the presidential campaign; when people start
to talk about his inauguration ceremony on the internet, we
need to detect the corresponding new sub-topic inauguration and insert it and its relevant documents to the correct
place on “Barack Obama”s topic hierarchy.
To this end, we make it a key function for our proposed
framework to be able to incrementally update the topic hierarchy by using the newly obtained data. Let Hold =
{Told , Mold , Rold } be the existing hierarchy and Snew the
newly obtained data set, a new hierarchy Hnew = {Tnew , Mnew ,
Rnew } can be obtained by updating Hold using the following
process:
Update the candidate topic set:
Tadd ← topic term identiﬁed from Snew ;
Tnew ← Told ∪ Tadd ;
Update the topic hierarchy:
Rnew , Tnew ← generate hierarchy using Algorithm 1 in
which R0 = Rold , T0 = Told and T = Tnew − Told , thus
adding new nodes in Tadd and edges between terms in
Tadd and Told into Told and Rold .
Update the document-topic mapping function:
Madd ← new mapping results between terms in Tnew and
documents in Snew ;
Mnew ← Mold ∪ Madd ;

Barack Obama
Chanel
Microsoft Corp.

Sub-topics
game, siri, battery, cost, software,
jailbreak, wiﬁ, update, ...
policy, debate, law, islam, benghazi,
poll, democrat, tax, ...
perfume, watch, bags, sunglass, toiletry, mall, event, advert, ...
bing, surface, os, partner, outlook,
windows phone 8, oﬃce 2013, ...

Table 4: Resultant candidate topic sets for four exemplar root topics
Corporations. In each category, two to three topics are selected, resulting in 11 root topics. For each topic, blogs,
cQAs and tweets are collected. We crawl blogs for a topic
by submitting the topic name as the query into the Google
Blog search engine11 and collect the ﬁrst 200 returned blogs.
For cQAs and tweets, we use the Yahoo! Answer API and
Twitter API to obtain the data stream on the target topic.
A brief statistics of our corpus can be found in Table 3.
Note that the data in all the three information sources are
tagged with time stamps. For blogs, they can be obtained
from the snippets in Google Blog search results which indicate when the blogs are published. The data from Yahoo!
Answer and Twitter APIs also contains time stamps on when
the questions are asked or the tweets are created.

5.2

Topic Term Identification

In this section, we will ﬁrst analysis the candidate topic
sets generated by the method described in section 4.3. Next
we will demonstrate how the use of diﬀerent UGC sources
improve its performance.
Table 4 shows a portion of identiﬁed topic terms for four
exemplar root topics. From the results we can see that UGCs
contain very rich and diversiﬁed information. For “Barack
Obama”, sub-topics on diﬀerent aspects like politics (e.g.,
law ), personal information (e.g., islam) and latest events
(e.g., debates) can be precisely extracted. We even obtain
more comprehensive product-related topics (e.g., sunglass)
for “Chanel” than those in Wikipedia.
Deﬁne the coverage of the candidate topic set T as the
ratio of the documents in the information source set that
contain at least one term in T as its relevant topics, the average coverage is 18.4% for all root topics. Although twitter
contains a huge amount of noise (more than 85% are noise
from statistics on a randomly selected data set that contains
550 tweets), the proposed method can eﬀectively ﬁlter out
most of them, hence only 18.2% high-quality tweets are covered. On the other hand, higher coverage is observed for
blogs (54.7%) and cQAs (40.4%).
From the covered documents, we sample 20 blogs, 20 cQAs
and 50 tweets for each topic. Three annotators are asked to

This update process is robust. This is evidenced from the
fact that, when there are mistakes in the existing hierarchy, the newly obtained information can be used to correct
the wrong relations. For example, the sub-topic relation set
Rold = {barack obama → tax } contains a very ambiguous relation10 . When a new topic term policy is discovered
from the new data set, instead of just adding it as a child to
any of the two existing nodes, our method can further break
the original ambiguous relation and create a better structure
for the three topic terms; thus Rnew = {barack obama →
policy , policy → tax }.

5. EVALUATION
5.1 Experimental Setup
We collect UGCs from the internet to form a corpus of
four categories: Digital Products, Politicians, Cosmetics and
10

It may means barack obama’s policy on tax or barack
obama’s own tax .

11

238

http://www.google.com/blogsearch

determine whether the topic terms we found are relevant
to the corresponding documents. Only the documents that
have agreement among all annotators are used for the statistics. The result shows that for all UGC sources and topic
categories, our method can achieve a precision of 73.2% to
90.4%.

0,7
0,6

F1-score

0,5
0,4

Noblog

0,3

Nocqa
0,2

Notweet

0,1

Ours

0
1,0

Percentage

0,8
0,6

Blog
0,4
0,2

cQA

Figure 4: Performance on topic hierarchy generation
when using diﬀerent information source set

Tweet
Extern

0,0

|R ∩ Rgold |
|R|)
|R ∩ Rgold |
recall (rec.) =
|Rgold |
precision (pre.) =

Figure 3: The percentage of topic terms supported
by each information source on the topic set

F1 score (F1 ) = 2 ·

Figure 3 shows the contribution of diﬀerent UGC sources
in topic term identiﬁcation. We can see that all sources are
indispensable. Although cQAs and blogs provide the majority of the topic terms, tweet tends to contribute timely and
popular topics such as release date of iphone 5 and price
of ipad mini . Though very limited, external sources provide middle-level topics such as os of microsoft and policy
of barack obama, which are essential when organizing the
topics into a hierarchy.

5.3.2

precision · recall
precision + recall

Ablation Study on the Contribution of
Information Sources

In Figure 4, we ﬁrst compare the performance of our proposed methods when using diﬀerent information source set,
which consists of (1) only cQAs and tweets (noBlog), (2)
blogs and tweets (noCQA), (3) blogs and cQAs (noTweet),
and (4) all the three sources (All ). The results show that the
more the sources we use, the better the performance we can
achieve, where the improvements of around 31.2% - 74.8%
are observed on the average F1 scores of All against those
of the other three combinations. There are mainly two reasons. First, since multiple sources provide more topic terms
(as shown in Figure 3), the recall is improved by 39% - 124%.
Second, though not for all the topics, the average precision
is also improved. This is partly because more UGCs help
to estimate better evidences which result in building more
optimal topic hierarchies.
On the other hand, when comparing the results of noBlog,
noCQA and noTweet, we can see that noBlog performs the
poorest for most topics (for topics like “Xbox Kinect”, the F1
score is even 0.). It indicates that blogs play a critical role
in our method, partly because blogs are usually well-written
and contain rich contents. For topics like “Chanel”, the performance does not drop that much when removing the blog
data. This is partly because many of the crawled blogs are
about fashion events and product comparison instead of focusing on real sub-topics like perfume and lipstick . Once
we can collect more appropriate blogs for these topics, even
higher performances can be expected.

5.3 Topic Hierarchy Generation
5.3.1 Evaluation Metrics
We evaluate our results against manually created gold
standards. For each topic, three annotators are employed to
create hierarchies independently using terms from the candidate topic set according to the following three rules.
Rule 1: Relevancy. All nodes on the hierarchy must be
reasonable sub-topics of the root topic. It guarantees that
users won’t get noisy information from the topic hierarchy.
Rule 2: Maximum coverage. All relevant nodes should be
included into the hierarchy. Hence users won’t miss any
useful information about the root topic.
Rule 3: Hierarchical structure. Each two connected nodes
must be directly related that no other nodes in the candidate topic set can be inserted between them. This makes
the hierarchy completely structured for users to quickly ﬁnd
their interested contents.
For example, given the candidate topic set {barack obama,
policy, tax, medic care, friday } for “Barack Obama”. First,
only friday will be removed according to rule 1 and 2. Then
according to rule 3, the gold standard hierarchy should be
{barack obama → policy, policy → tax, policy → medic
care}, which provides users a completely hierarchical view
of the available UGCs about “Barack Obama”.
Next, the three annotators compare their resultant candidate hierarchies and come up with the gold standards
through discussions. On average, the gold standard hierarchies contain 22.3 nodes and 21.3 edges with an average
depth of 3.64.
We use the precision, recall and F1 scores to measure the
hierarchy generation performance. Denote R and Rgold as
the sub-topic relation sets of our output and the gold standard, respectively, the metrics can be calculated as follows:

5.3.3

Evaluation on Effectiveness of Evidence
Combination Schemes

The proposed evidence combination scheme plays an important role in our method. In this section, we compare it
with three baseline methods: (1) Linear, which simply combines all evidences linearly and has been used in [19] [21].
For each topic, we adopt ridge regression [19] to optimize the
weight vector using the training data from the other topics
in the topic category. (2) No undirect, a variation of the
proposed scheme which does not use undirected-evidences.
(3) Equal direct, a variation of the proposed scheme in which

239

concepts in a given concept set. For fair comparison, we use
the probability of a sub-topic relation approximated for our
method here.
In Table 6, we can see that our proposed method achieves
the best performance on all metrics for most topics and signiﬁcantly outperforms the state-of-the-art methods on the
averaged F1 score (t-test, p-value<0.05). Compared to Yang’s
method, the proposed graph based method makes better
use of the relations among three or more topics. Taking
the root topic “Chanel” as an example, given its sub-topic
set {chanel , product, perfume, ...}, while Yang’s method
returns the relation set {chanel → product, chanel →
perfume, ...} by considering only the strongness of pairwise sub-topic relations, our method can further detect the
sub-topic chain along the three topics, i.e., chanel → product → perfume, ,which better captures the global relation.
Moreover, the proposed method outperforms Snow’s method
in term of F1 score by about 12%. It is partly because the results of Snow’s method are strongly aﬀected by the insertion
order of the topics. Once an insertion error occurs in one
step, it cannot be corrected in the following steps. But our
incremental updating mechanism can naturally solve this
problem. Navigli’s method tends to generate very deep hierarchies [13], where errors occur sometimes (e.g., iphone
5 → camera → cost, where the cost of IPhone 5’s camera
does not make sense). This type of errors can be solved by
our method through the use of multiple evidences. As for
the iphone 5 example, we will determine that the relatedness
between iphone 5 and cost is much stronger than that between camera and cost in term of evidences like PMI, thus
a direct connection is established between iphone 5 and
cost.

0,7
0,6

F1-score

0,5
0,4

Linear

0,3

No undirect
0,2

Equal direct

0,1

Ours

0

Figure 5: Performance on topic hierarchy generation
when using diﬀerent evidence combination schemes
the weights of all the directed-evidences are set to be equal.
All methods run on the same topic sets as those used in
generating the gold standards 12 .
The result is shown in Figure 5. Compared to the Linear method, all the three methods that adopt our proposed
scheme perform signiﬁcantly better on the F1 scores (t-test,
p-value<0.05). This indicates that considering the directed
and undirected evidences separately is a better way to estimate the probability of sub-topic relations than putting
all the evidences together undistinguishedly. The use of
undirected-evidences also signiﬁcantly improve the performance (t-test, p-value<0.05). Since we only adopt nine carefully selected directed-evidences, the eﬀect of weights is limited. However, as shown in Table 5, the weight vector indeed
reﬂexes the quality of diﬀerent evidences. For example, the
Wikipedia-based evidences (e.g., ewcate , ewtitle ) usually obtain higher weights since they are from high quality sources.
Stricter patterns (e.g., espattern0 ) are also higher weighted
than general patterns (e.g., espattern2 ). As the result, more
improvement could be observed when more evidences of varied qualities [21] are introduced.
evidence
weight
a
b

espattern0 a espattern2 b ewcate
0.116
0.099
0.129

5.3.5

Case Study on Topic Hierarchy Generation

In this section, we analyse the strength and weakness of
our method using a few examples. First, rather than simply
assigning all sub-topics as direct children of the root topic,
our method tends to generate completely structured hierarchies (average depth is 5.27). For example, (facebook →
application → business → marketing → ads) is a subtopic chain in the resultant hierarchy of “Facebook Inc.”.
Evidences from all sources contribute to this result. Patternbased evidences support the relations that frequently occurs
on web pages like facebook → application. On the other
hand, Wikipedia-based and WordNet-based evidences help
to ﬁnd many novel relations ( rather than traditional isa or has-a relation), such as marketing → ads for the
given example and policy → tax for “Barack Obama”. The
undirected-evidences are also important. For the given example, both contextual-based and pmi-based evidences suggest that marketing and ads are more relevant than business and ads. Based on all these evidences, our method
can best capture the relations among all the sub-topics of a
given root topic.
However, multiple sources also bring in diﬀerent errors.
For pattern-based evidences, when a pattern “<topic> such
as <subtopic> and” matches a string “ ... as well as other
companies/brands/games such as Bing and Gears of War
...”, a sub-topic relation between game and bing will be
detected. This is obviously wrong but is hard to be corrected
by NLP tools. On the other hand, for an error sub-topic
relation ( event → lipstick ) for “Chanel”, we found that

ewtitle
0.129

<topic> such as <subtopic> and
<subtopic> on <topic> and

Table 5: Part of the weight vector of directedevidences for “IPhone 5”

5.3.4 Comparison with State-of-the-art Methods
Based on the results in previous two sections, our method
performs the best when using all information sources and
adopting the proposed combination scheme. We now compare our best method with three state-of-the-art methods:
(1) Yang’s Method [19], which organizes concepts into a hierarchy according to a information function. For each topic,
the information function employs all the evidences in Table 2 and is trained using the gold standard of this topic.
(2) Navigli’s Method [13], a graph based method which only
employs a classiﬁer-based 0/1 evidence and does not support real time update. Since the evidence used in this paper
only provides clues for is-a relation, we extend it with extra
directed-evidences from our evidence set for the sake of fair
comparison. (3) Snow’s Method [16], which uses a probability model to obtain the most probable hierarchy for the
12

In fact, since these methods (including those presented in
the following section) use the same topic identiﬁcation process, they all share the same candidate topic sets.

240

Topic
Iphone 5
Ipad mini
Xbox Kinect
Barack Obama
Mitt Romney
Hillary Clinton
Chanel
Estee Lauder
Facebook Inc.
Microsoft Corp.
Blizzard Inc.

Yang’s Method
pre. rec.
F1
0.16 0.15 0.16
0.14 0.16 0.15
0.10 0.14 0.12
0.19 0.23 0.20
0.07 0.10 0.08
0.21 0.40 0.28
0.16 0.22 0.18
0.23 0.33 0.27
0.20 0.22 0.21
0.18 0.19 0.18
0.18 0.22 0.20

Navigli’s Method
pre. rec.
F1
0.36 0.30 0.33
0.41 0.20 0.27
0.50 0.23 0.32
0.50 0.45 0.47
0.29 0.23 0.26
0.33 0.27 0.30
0.55 0.55 0.55
0.40 0.44 0.42
0.32 0.36 0.34
0.18 0.19 0.21
0.26 0.27 0.26

Snow’s Method
pre. rec.
F1
0.30 0.27 0.28
0.26 0.20 0.22
0.36 0.24 0.28
0.55 0.50 0.52
0.26 0.23 0.25
0.41 0.33 0.37
0.61 0.61 0.61
0.35 0.38 0.37
0.32 0.36 0.34
0.14 0.18 0.15
0.22 0.22 0.22

Our Method
pre. rec.
F1
0.36 0.31 0.33
0.50 0.24 0.33
0.50 0.24 0.32
0.55 0.50 0.52
0.30 0.24 0.26
0.42 0.33 0.37
0.61 0.61 0.61
0.40 0.44 0.42
0.36 0.41 0.39
0.14 0.15 0.18
0.30 0.32 0.31

Table 6: Performance comparison between our method and state-of-the-art methods. The bold face indicates
the best F1 performance for each topic. Our method achieves signiﬁcant improvements on F1 scores (t-test,
p-value<0.05) compared to all three baseline methods.

10
1st Oct

15th Oct

1st Nov

15th Nov

1st Dec

Topic Number
F1 score

0.38
0.36

28

0.34
24

0.32

20
1st Oct

15th Dec

F1 score

14

32
Topic Number

Topic Number

18

F1 score

0.5
0.4
0.3
0.2
0.1
0

22

1st Nov

15th Nov

1st Dec

15th Dec

Date

1st Oct ~ 15th Oct: debate !"#"$%&'(")"&*+,+$-./0&*debate), Benghazi, …

1st Oct ~ 15th Oct: N/A

15th Oct ~ 1st 1/23&policy (Barack Obama *policy *-"4+56

15th Oct ~ 1st Nov: decision !"#$%&'()*++&,,$-.')/')0&12+&') decision)

1st

1/23&democrats (Barack Obama *2/-+#5&*&democrats), …

1st Nov ~ 15th Nov: N/A

7+$3&Israel !"#"$%&'(")"&* Israel), medicare, …

15th Nov ~ 1st Dec: jailbreak (IPhone 5 )jailbreak), Itune, …

1/2&

15th

1/2& ~

1st

F1 score

0.3
15th Oct

Date

~15th

Topic Number

1st 7+$& ~ 15th 7+$3&inauguration (Barack Obama * inauguration)

1st Dec ~ 15th Dec: siri (IPhone 5)'*++&,,$-.')/')',$345*-&') siri)

Figure 6: Hierarchy update on “IPhone 5” and “Barack Obama” between 1st Oct and 15th Dec: The trend
map shows how the F1 score changes when the topic set grows along time; the table illustrates some examples
of the newly detected topics (indicated by Bold Italic face) and their position in the hierarchies (shown in
brackets) in each period.
the only evidence that supports it is from WordNet, since
there is a WordNet path13 between the two words.

period. On the contrary, the change of topics on “IPhone 5”
is very small during the two and a half months.

5.4 Hierarchy Update

6.

Online UGCs increase every minute. In this section, we
demonstrate how the proposed method can be used to update the topic hierarchies incrementally with real time information. We use the data of blogs, cQAs and tweets published before 15th Dec, 2012 on two example topics, i.e.,
“Barack Obama” and “IPhone 5”. According to their published date, we split the data into 7 sub sets. We initiate
the hierarchy using the data before 1st Oct and update it
using the data in the other 6 sub sets, each indicates a duration of 15 or 16 days. As our baselines are not designed for
real time data, we only show our results for the hierarchy
updating experiments here.
The result in Figure 6 oﬀers us a close look of this process.
We can see that as time passes and new topics emerge, our
method can eﬀectively detect these topics and merge them
into the hierarchy. As the results, topics like jailbreak for
“IPhone 5”, debate, inauguration for “Barack Obama” are
updated onto the appropriate positions of the topic hierarchy shortly after they become popular on the internet. From
the results, we can also ﬁnd that “Barack Obama” is a more
time-sensitive topic, which brings in new sub-topics in each
13

CONCLUSION

In this paper, we proposed an automatic method for incremental information organization for multiple UGC sources.
Given a root topic, we used evidences from multiple UGCs to
identify topic terms and sub-topic relations between them.
With these topic terms, a graph-based algorithm was applied
to generate and update the topic hierarchies, on which the
UGCs can be organized according to their relevant topics.
Comprehensive experiments on 11 root topics demonstrated
the eﬀectiveness of our method. For future work, we will
explore more UGC sources such as forums and try to ﬁnd
available initial topic hierarchies to enhance our system. It
is also interesting to apply the generated topic hierarchy in
more sophisticated text analysis tasks.

7.

ACKNOWLEDGMENTS

This research is supported by the Singapore National Research Foundation under its International Research Centre
@ Singapore Funding Initiative and administered by the
IDM Programme Oﬃce and The National Key Basic Research Program (also called 973 Program), No.2012CB316301
and NSFC under project No.61272227.

The WordNet path: event → makeup → lipstick .

241

8. REFERENCES

[14] K. Nishida, R. Banno, K. Fujimura, and T. Hoshide.
Tweet classiﬁcation by data compression. DETECT
’11, pages 29–34. ACM, 2011.
[15] T. Pedersen, S. Patwardhan, and J. Michelizzi.
Wordnet:: Similarity: measuring the relatedness of
concepts. In Demonstration Papers at HLT-NAACL
2004, pages 38–41. ACL, 2004.
[16] R. Snow, D. Jurafsky, and A. Ng. Semantic taxonomy
induction from heterogenous evidence. In Proceedings
of the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the
Association for Computational Linguistics, pages
801–808. ACL, 2006.
[17] X. Wang, K. Zhang, X. Jin, and D. Shen. Mining
common topics from multiple asynchronous text
streams. In Proceedings of the Second ACM
International Conference on Web Search and Data
Mining, pages 192–201. ACM, 2009.
[18] H. Yang and J. Callan. Feature selection for automatic
taxonomy induction. In Proceedings of the 32nd
international ACM SIGIR conference, pages 684–685.
ACM, 2009.
[19] H. Yang and J. Callan. A metric-based framework for
automatic taxonomy induction. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
1-Volume 1, pages 271–279. ACL, 2009.
[20] A. Yates, M. Cafarella, M. Banko, O. Etzioni,
M. Broadhead, and S. Soderland. Textrunner: Open
information extraction on the web. In Proceedings of
Human Language Technologies: The Annual
Conference of NAACL: Demonstrations, pages 25–26.
ACL, 2007.
[21] J. Yu, Z. Zha, M. Wang, K. Wang, and T. Chua.
Domain-assisted product aspect hierarchy generation:
towards hierarchical organization of unstructured
consumer reviews. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 140–150. ACL, 2011.

[1] J. Edmonds. Optimum branchings. Journal of
Research of the National Bureau of Standards B,
71:233–240, 1967.
[2] Y. HaCohen-Kerner, Z. Gross, and A. Masa.
Automatic extraction and learning of keyphrases from
scientiﬁc articles. Computational Linguistics and
Intelligent Text Processing, pages 657–669, 2005.
[3] X. Han and J. Zhao. Structural semantic relatedness:
a knowledge-based method to named entity
disambiguation. In Proceedings of the 48th Annual
Meeting of the Association for Computational
Linguistics, pages 50–59. Association for
Computational Linguistics, 2010.
[4] C. Hayes, P. Avesani, and U. Bojars. An analysis of
bloggers, topics and tags for a blog recommender
system. From Web to Social Web: Discovering and
Deploying User and Content Proﬁles, pages 1–20,
2007.
[5] J. Hoﬀart, F. Suchanek, K. Berberich, and
G. Weikum. Yago2: a spatially and temporally
enhanced knowledge base from wikipedia. Artiﬁcial
Intelligence, 2012.
[6] A. Hulth. Improved automatic keyword extraction
given more linguistic knowledge. In Proceedings of the
2003 conference on Empirical methods in natural
language processing, pages 216–223. ACL, 2003.
[7] S. N. Kim and M.-Y. Kan. Re-examining automatic
keyphrase extraction approaches in scientiﬁc articles.
In Proceedings of the Workshop on Multiword
Expressions: Identiﬁcation, Interpretation,
Disambiguation and Applications, pages 9–16.
Association for Computational Linguistics, 2009.
[8] Z. Kozareva and E. Hovy. A semi-supervised method
to learn and construct taxonomies using the web. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1110–1118. ACL, 2010.
[9] D. J. Lawrie and W. B. Croft. Generating hierarchical
summaries for web searches. In Proceedings of the 26th
annual international ACM SIGIR conference, pages
457–458. ACM, 2003.
[10] X. Mao, Z. Ming, Z. Zha, T. Chua, H. Yan, and X. Li.
Automatic labeling hierarchical topics. 2012.
[11] Y. Matsuo and M. Ishizuka. Keyword extraction from
a single document using word co-occurrence statistical
information. International Journal on Artiﬁcial
Intelligence Tools, 13(01):157–169, 2004.
[12] Z. Ming, K. Wang, and T. Chua. Prototype hierarchy
based clustering for the categorization and navigation
of web collections. In Proceedings of the 33rd
international ACM SIGIR conference, pages 2–9.
ACM, 2010.
[13] R. Navigli, P. Velardi, and S. Faralli. A graph-based
algorithm for inducing lexical taxonomies from
scratch. In Proceedings of the Twenty-Second
international joint conference on Artiﬁcial Intelligence
- Volume Three, pages 1872–1877. AAAI Press, 2011.

APPENDIX
A.

SEARCH ENGINE-BASED EVIDENCES

In Table 7, we list the patterns we use in estimating the
pattern-based evidences.
<topic> such as <subtopic> and
<topic>’s <subtopic>
<subtopic> on <topic> and
<topic> including <subtopic> and
<subtopic> of <topic>
<subtopic> and other <topic>
Table 7: Patterns used to estimate the patternbased evidences

242

