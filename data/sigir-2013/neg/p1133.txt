Workshop on Benchmarking Adaptive Retrieval
and Recommender Systems – BARS 2013
Pablo Castells

Frank Hopfgartner

Alan Said

Mounia Lalmas

Univ. Autónoma de Madrid
Technische Univ. Berlin
Cent. Wiskunde & Informatica Yahoo! Labs Barcelona
Fco. Tomás y Valiente 11
Ernst-Reuter-Platz 7
Science Park 123, 1098 XG Avinguda Diagonal 177
28049 Madrid, Spain
10587 Berlin, Germany
Amsterdam, Netherlands
08018 Barcelona, Spain
pablo.castells@uam.es frank.hopfgartner@tu-berlin.de
alan.said@cwi.nl
mounia@acm.org

and metrics suiting the goals and task models of real applications
is still a prominent open issue.

ABSTRACT
Evaluating adaptive and personalized information retrieval techniques is known to be a difficult endeavor. The rapid evolution of
novel technologies in this scope raises additional challenges that
further stress the need for new evaluation approaches and methodologies. The BARS 2013 workshop seeks to provide a specific
venue for work on novel, personalization-centric benchmarking
approaches to evaluate adaptive retrieval and recommender systems.

The BARS 2013 workshop aimed to serve as a venue for work on
novel, personalization-centric benchmarking approaches to evaluate adaptive retrieval and recommender systems. The workshop
was set to revise and leverage the latest advances in this area,
identify the main issues to be addressed, and share ideas for continued progress. BARS sought, in particular, to join forces and
provide a meeting point for researchers working on largely overlapping and connected areas such as adaptive IR and recommender systems, dealing with closely related problems but often from
different backgrounds.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: search process,
information filtering.

2. SCOPE

Keywords

The workshop gathered researchers and practitioners interested in
developing better, clearer, and/or more complete evaluation
methodologies, and addressing the challenges involved in the
evaluation of adaptive retrieval and recommender systems. It
provided an informal setting for exchanging and discussing ideas,
sharing experiences and viewpoints. The participants worked
together in the identification and better understanding of the current gaps in the evaluation methodologies, seeking to lay directions for progress in addressing them, and the consolidation and
convergence of experimental methods and practice.

Evaluation, adaptive information retrieval, recommender systems,
benchmarking, metrics, methodology.

1. INTRODUCTION
Great progress has been made in recent years in the development
of recommendation, retrieval and personalization techniques. Yet
the evaluation of these systems is still based on traditional metrics, e.g. precision, recall or RMSE, often not taking the use-case
and situation of the system into consideration, and failing to provide a suitable proxy of user satisfaction and business goals.
Moreover, the rapid evolution of novel information retrieval (IR)
and recommender systems foster the need for new evaluation
paradigms.

The accepted papers and the discussions held at the workshop
addressed, among others, the following topics:

New evaluation approaches of adaptive systems should evaluate
both functional and non-functional requirements. Functional requirements go beyond traditional relevance metrics and focus on
user-centered utility metrics, such as novelty, diversity and serendipity. Non-functional requirements focus on performance and
technical aspects, e.g. scalability and reactivity.
The evaluation of adaptive IR systems has been acknowledged to
find difficulty in fitting in established evaluation paradigms and
methodologies, which can be identified as a hurdle to research
and progress in this area. Active research efforts and open discussion are currently taking place in parallel in the Recommender
Systems and Adaptive IR fields, where devising methodologies
Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact
the owner/author(s).
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
ACM 978-1-4503-2034-4/13/07.



New metrics and methods for quality estimation of recommender and adaptive IR systems.



Novel frameworks for the user-centric evaluation of adaptive systems.



Validation of off-line methods with online studies.



Comparison of evaluation metrics and methods.



Comparison of recommender and IR approaches across
multiple systems and domains.



Measuring technical constraints vs. accuracy.



New datasets for the evaluation of recommender and adaptive IR systems.



Benchmarking frameworks.



Multiple-objective benchmarking.

The accepted papers and a summary of discussions are available
in the workshop proceedings, which can be reached from the
workshop website at http://www.bars-workshop.org.

1133

