Document Classification by Topic Labeling
Swapnil Hingmire*†
Sandeep Chougule*
Girish K. Palshikar*
swapnil.hingmire@tcs.com sandeep.chougule@tcs.com gk.palshikar@tcs.com
Sutanu Chakraborti†
sutanuc@cse.iitm.ac.in
*
Systems Research Lab
Tata Research Development and Design Center
Tata Consultancy Services
Pune-13, India

ABSTRACT

Department of Computer
Science and Engineering
IIT Madras
Chennai-36, India

number of labeled dataset. Many times obtaining such a labeled dataset is expensive. Nigam et al. [5] proposed semisupervised approaches for document classification based on
labeled and unlabeled datasets. McCallum and Nigam [4]
proposed a semi-supervised approach based on labeling of
keywords. In keyword based approaches, finding right set of
keywords is a challenge.
In this paper, we propose Latent Dirichlet Allocation (LDA)
[1] based document classification algorithm. Our algorithm
does not require any labeled dataset. In our algorithm, we
construct a topic model using LDA, assign one topic to one
of the class labels, aggregate all the same class label topics into a single topic using the aggregation property of the
Dirichlet distribution and then automatically assign a class
label to each unlabeled document depending on its “closeness” to one of the aggregated topics.
In our algorithm an expert assigns one topic to one of the
class labels, also as LDA topics correlate with human assigned class labels [6], our algorithm exerts a low cognitive
load on the expert.
Class labels predicted by our algorithm may be approximate or noisy. In order to reduce the influence of such
an approximate or noisily labeled documents, we present
an extension to our algorithm based on the combination of
the Expectation-Maximization (EM) algorithm and a naive
Bayes classifier. We show effectiveness of our algorithm on
three real world datasets.
The paper is organized as follows: In section 2 we give brief
introduction to LDA and the Dirichlet distribution. Section
3 contains our document classification algorithm. Section
4 demonstrates effectiveness of our algorithm with experiments on three real world datasets. We end our paper with
conclusions and future prospects of our work in section 5.

In this paper, we propose Latent Dirichlet Allocation (LDA)
[1] based document classification algorithm which does not
require any labeled dataset. In our algorithm, we construct
a topic model using LDA, assign one topic to one of the class
labels, aggregate all the same class label topics into a single
topic using the aggregation property of the Dirichlet distribution and then automatically assign a class label to each
unlabeled document depending on its “closeness” to one of
the aggregated topics.
We present an extension to our algorithm based on the combination of Expectation-Maximization (EM) algorithm and
a naive Bayes classifier. We show effectiveness of our algorithm on three real world datasets.

Categories and Subject Descriptors
I.2.7 [Artificial Intelligence]: Natural Language Processing—Text analysis

General Terms
Experimentation, Performance, Theory, Verification

Keywords
Expectation-Maximization, Text classification, Topic Modelling

1.

†

INTRODUCTION

With the advent of cheap and fast storage, there is an explosive growth in the size and number of documents available
in electronic format. Document classification is a technique
which helps users to make effective use of the knowledge
hidden in the documents.
Traditional supervised document classifiers require a large

2.

LATENT DIRICHLET ALLOCATION
(LDA)

LDA is an unsupervised generative probabilistic model for
collections of discrete data such as text documents. In LDA,
each document is generated by choosing a distribution over
topics and then choosing each word in the document from a
topic selected according to the distribution [3]. Generative
process of LDA can be described as follows:

c 2013 Association for Computing Machinery. ACM acknowledges that
this contribution was authored or co-authored by an employee, contractor
or affiliate of the national government of India. As such, the government of
India retains a nonexclusive, royalty-free right to publish or reproduce this
article, or to allow others to do so, for Government purposes only.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

1. for t = 1...T
(a) φt ∼ Dirichlet(β)

877

3.1

2. for each document d ∈ D

Our algorithm is based on generative property of LDA
and the aggregation property of the Dirichlet distribution.
Let us assume, we want to classify each document to one of
the class labels from C = {1, 2, ..., m}. Using Collapsed
Gibbs sampling for LDA, Z = {z1 , z2 , ..., zT } topics are
learnt on the document corpus D. Now an expert will assign
a class label, i ∈ C to each topic
based on its most
S zt ∈ Z
i
Z
,
the
partition of Z
prominent words. Create Z 0 = m
i=1
such that Z i = {zt |zt ∈ Z and class label of zt is i}.
If for a document d in the corpus D, θd
=
(θ1,d , θ2,d , ..., θT,d ) ∼ Dirichlet(α1 , α2 , ..., αT ) then using the
aggregation property of the Dirichlet distribution define θd0
as:
P
P
P
θd0 = (
θt,d ,
θt,d , ...,
θt,d )

(a) θd ∼ Dirichlet(α)
(b) for each word w at position n in d
i. zdn ∼ Multinomial(θd )
ii. wdn ∼ Multinomial(zdn )
Where, T is the number of topics, φt is the word probabilities for topic t, θd is the topic probability distribution, zdn
is topic assignment and wdn is word assignment for nth word
position in document d respectively. α and β are topic and
word Dirichlet priors respectively.
Training an LDA model is estimation of the word-topic distributions and the topic distributions for all documents in
the corpus. Direct and exact estimation of these parameters is intractable. Collapsed Gibbs sampling is one of the
techniques used for the parameter estimation of LDA [3]. After performing collapsed Gibbs sampling, probability of the
word w assigned to the topic t (φw,t ) and the probability of
the topic t assigned to document the d (θt,d ) is estimated as:
φw,t =

ψw,t +βw
"

#
P

θt,d =

zt ∈Z 1

ψv,t +βv

T
P

i=1

v∈W

#
Ωi,d +αi

(1)

ΩZ i ,d =

t∈A2

P
t∈A1

3.

zt ∈Z 2

αt )

P

[Ωt,d + αt ]

0
θZ
i ,d =

(5)

zt ∈Z m

v∈W

t∈Z i

v,Z i

Ω i
P Z ,d
ΩZ k ,d

(6)

(7)

k∈C

Using Collapsed Gibbs sampling for LDA, update φ0t and θd0 .
A class label c ∈ C is assigned to document d ∈ D such that:
0
c = arg max θZ
i ,d .
i

Algorithm 1 describes our for document classification algorithm .
Algorithm 1: ClassifyLDA
input : D = {d} : Document corpus
output: Class label (dc ) from C = {1, 2, ..., m} for
each document d ∈ D
1 begin
2
Use LDA to learn Z = {z1 , ..., zT } topics on D;
3
Compute φt and θd using equations 1 ;
4
Expert will assign a class label, i ∈ C to each
topic zt ∈ Z based on its most prominent words;
S
i
5
Create a partition Z 0 = m
i=1 Z such that:
i
Z = {zt |zt ∈ Z and class label of zt is i};
6
Initialize φ0t and θd0 using equations 6 and 7;
7
Update φ0t and θd0 using Collapsed Gibbs
sampling;
8
for d ∈ D do
0
9
Infer θZ
i ,d ; ∀i ∈ C using equation 7 ;
0
10
dc = arg max θZ
i ,d ;

(2)

t=1

t∈A1

P

αt , ...,

t∈Z i

where, θ = {θ1 , ..., θt , ..., θT } is a point on the (T −1) simplex
P
(i.e. 0 < θt < 1 and Tt=1 θt = 1) and α = (α1 , ..., αt , ..., αT )
is a set of parameters with αt > 0. So,
θ = (θ1 , ..., θt , ..., θT ) ∼ Dirichlet(α1 , ..., αt , ..., αT )
(3)
Aggregation property of the Dirichlet distribution:
The Dirichlet distribution has a fractal like aggregation
property [2]. It is defined as the aggregation of any subset
of Dirichlet distribution variable yields a Dirichlet distribution, with corresponding aggregation of the parameters. If
{A1 , A2 , ..., Ar } is a partition of {1, 2, ..., T } then,
P
P
P
θ0 = (
θt ,
θt , ...,
θt )
∼ Dirichlet(

αt ,

P

Initialize φ0t and θd0 using following equations:
P
ψ
i
[ψw,t + βw ] φ0w,Z i = P w,Z
ψw,Z i =
ψ

The Dirichlet distribution

The Dirichlet distribution is defined as:
T
P
Γ(
αt ) Y
T
t=1
p(θ|α) = T
θtαt −1
Q
Γ(αt ) t=1

P
zt ∈Z 1

Where ψw,t is the count of the word w assigned to the topic
t, Ωt,d is the count of the topic t assigned to words in the
document d and W is the vocabulary of the corpus.
LDA discovers a set of topics present in the documents and
gives probabilities of observing each word in each topic.
Most prominent words in a topic frequently co-occur with
each other in the documents so one can infer context of
the words in a topic. Using the word probabilities one can
interpret meaning of topics and find major themes in the
documents. The topic probabilities of a document provide
its explicit representation and these probabilities can be embedded in more complex model.

2.1

zt ∈Z m

zt ∈Z 2

∼ Dirichlet(

Ωt,d +αt
"

ClassifyLDA

i

11
end
12 end

t∈Ar

αt ,

P
t∈A2

αt , ...,

P

αt )

(4)

t∈Ar

3.2

DOCUMENT CLASSIFICATION

ClassifyLDA-EM

In ClassifyLDA-EM algorithm, we build a classifier using the combination of EM and a naive Bayes classifier. In
this algorithm we use EM iterations along with the relation
between word co-occurrence knowledge and class labels to
improve the parameters of a naive Bayes classifier.

In this section, we propose our document classification
algorithm based on LDA (ClassifyLDA) and an extension of
the algorithm based on the combination of EM algorithm
and a naive Bayes classifier (ClassifyLDA-EM).

878

Initially, we label all the unlabeled documents in the corpus using ClassifyLDA algorithm described in algorithm 1.
Then, we build a naive Bayes classifier using these labeled
documents and estimate class probabilities for each document. Using these estimated class probabilities we reassign
a class label to each document and rebuild a new naive Bayes
classifier.
We iterate this process of reassigning class labels to the documents and rebuilding a naive Bayes classifier until it converges to a stable classifier. We say a classier is stable when
the change in log likelihood of the parameters of the classier
is below a threshold. ClassifyLDA-EM can be described as:

four discussion groups, for simulated auto racing (sim auto),
simulated aviation (sim aviation), real autos (real auto),
real aviation (real aviation). Following are the three classification tasks associated with this dataset.
1. sim auto vs sim aviation vs real auto vs real aviation
2. auto (sim auto + real auto) vs aviation (sim aviation
+ real aviation)
3. simulated (sim auto + sim aviation) vs real (real auto
+ real aviation)
3. WebKB 3 : This dataset contains 4199 university webpages. The task is to classify the webpages as student,
course, faculty or project.
We randomly split SRAA and WebKB datasets such that
80% is used as training data and remaining 20% is used as
test data.

• Input: D = {d} : Unlabeled document corpus
• Initialization: Let Ĉ be an initial classifier, built
using ClassifyLDA algorithm. Assign a class label to
each unlabeled document in D using ClassifyLDA.
• Loop while Ĉ converges:
– E-step: Use the current classifier, Ĉ, to estimate
the probability of a document belonging to each
class.
– M-step: Re-estimate the classifier, Ĉ using naive
Bayes model based on the document-class probabilities computed in E-step

4.2

• Use Ĉ to classify an unlabeled document.

4.

EXPERIMENTAL EVALUATION

We determine the effectiveness of our algorithm in relation
to semi-supervised text classification algorithm proposed in
[5] (NB-EM). We report the minimum number of labeled
documents at which the performance of ClassifyLDA-EM
and NB-EM are almost similar.

4.1

4.3

Datasets

1. comp:
comp.graphics, comp.os.ms-windows.misc,
comp.sys.ibm.pc.hardware, comp.sys.mac.hardware,
comp.windows.x

4.4

3. rec: rec.autos, rec.motorcycles, rec.sport.baseball,
rec.sport.hockey
alt.atheism,

We experimented with all possible combinations of these major categories.
2. SRAA: Simulated/Real/Aviation/Auto UseNet
data2 : This dataset contains 73,218 UseNet articles from
1
2

Example

Table 2 shows topics learnt and classification of the topics on “politics vs rec” dataset. With the help of most
prominent words in a topic an expert can assign a class
label to the topic. Due to generative property of LDA,
topics labeled with class label “politics” will generate politics related documents with high probability. Now we will
create the partition Z 0 = {{z0 , z1 }, {z2 , z3 }} for the topics
Z = {z0 , z1 , z2 , z3 }. Using the aggregation property of the
Dirichlet distribution, all the same class label topics are aggregated into a single topic. Now, we can use algorithm 1
and the combination of EM algorithm and a naive Bayes
classifier to estimate class labels for unseen documents.
We also explored how well a topic correlates with the class

talk.politics.guns,

4. religion:
talk.religion.misc,
soc.religion.christian

Results

Table 1 shows experimental results. We can observe that,
ClassifyLDA-EM algorithm can achieve almost similar performance in relation to NB-EM with significant reduction in
labeling efforts and for most of the datasets performance of
our algorithm is above 0.9. In table 1, we can also observe
improvement in the performance of ClassifyLDA-EM over
ClassifyLDA which proves that the combination of EM and
a naive Bayes classifier reduces the influence of approximate
or noisily labeled documents.
We observed that, performance of NB-EM depends on initial
labeled documents.

We evaluate the effectiveness of ClassifyLDA and
ClassifyLDA-EM on following three real world text classification datasets.
1. 20Newsgroup: This dataset contains messages across
twenty newsgroups. In our experiments, we use bydate version of the 20Newsgroup dataset1 . This version contains
separate train and test datasets of 20 newsgroups which are
grouped into 6 major categories. We selected 4 major categories: comp, politics, rec, and religion. Following are the
newsgroups in each selected category.

2. politics:
talk.politics.misc,
talk.politics.mideast

Experimental settings

We did preprocessing on the dataset by removing headers
and stopwords. We evaluated effectiveness of our algorithm
by computing the Macro F-measure (F1 ).
For classification tasks of 20Newsgroup related dataset we
choose number of topics (T) equal to two times number of
classes. For SRAA dataset we learnt 10 topics on the complete dataset and labeled these 10 topics for all the three
classification tasks. For WebKB dataset we learnt 10 topics.
The Dirichlet parameter β was chosen to be 0.01 and α was
50/T. We used Mallet4 to run LDA on the documents.
In NB-EM algorithm, we do 10 trails per number of labeled
documents and report average Macro F1.

3

http://qwone.com/~jason/20Newsgroups/
http://people.cs.umass.edu/~mccallum/data.html

4

879

http://www.cs.cmu.edu/~webkb/
http://mallet.cs.umass.edu/

Data set
20Newsgroup
comp vs politics
comp vs rec
comp vs religion
politics vs rec
politics vs religion
rec vs religion
comp vs politics vs rec
comp vs politics vs religion
comp vs rec vs religion
politics vs rec vs religion
comp vs politics vs rec vs religion
SRAA
sim auto vs sim aviation vs real auto
vs real aviation
auto vs aviation
simulated vs real
WebKB
student vs course vs faculty vs project

ClassifyLDA ClassifyLDA-EM
(Macro-F1) (Macro-F1)

# Topics

NB-EM
(Macro-F1)

# Labeled documents for NB-EM

0.960
0.903
0.953
0.957
0.872
0.959
0.932
0.896
0.936
0.889
0.891

0.976
0.949
0.979
0.980
0.929
0.988
0.960
0.932
0.965
0.937
0.936

4
4
4
4
4
4
6
6
6
6
8

0.974
0.947
0.981
0.978
0.927
0.986
0.960
0.929
0.964
0.935
0.934

20
25
25
70
65
105
125
115
105
190
480

0.732

0.770

10

0.786

10250

0.908
0.917

0.929
0.933

10
10

0.927
0.931

300
5250

0.711

0.719

10

0.730

1150

Table 1: Experimental results (Macro-F1) of document classification on 20Newsgroup, SRAA and WebKB datasets
ID Most prominent words in the topic
0 gun armenian turkish didn guns killed file
weapons armenia
1 israel government president jews american
fact question law case rights
2 team game play season hockey players win
league baseball
3 car bike front road buy drive speed engine

Class
politics

and the aggregation property of the Dirichlet distribution.
We also show effectiveness of our algorithm with the help of
experiments. Our approach is specifically suited for domains
where establishing a mapping from topics to class labels is
easier than acquiring a labeled collection of documents. In
future we would like to carry out experiments on datasets
like Reuters-21578 and a more detailed investigation on how
the topic-class mapping influences the classification effectiveness. We will also explore tools that help experts arrive
at the most appropriate topic-class mapping.

politics
rec
rec

Table 2: Topic labeling on the politics vs rec dataset

6.

assigned to it. We represented each class as probability distribution over words. We computed P (w|cj ), the probability
of the word w belonging to the class cj as the fraction of the
number of times word w appears among all the words in
documents of class cj . Then we computed Kullback-Leibler
(K-L) divergence between each class and a topic. Table 3
shows K-L divergence between each class and a topic for the
same dataset. We can observe that the K-L divergence is
least for the class assigned to a topic by the expert.

ID
0
1
2
3

Topic-class mapping
Expert assigned class label
politics
politics
rec
rec

Class labels
politics
rec
3.89
6.12
3.57
6.12
6.64
3.73
6.13
4.32

Table 3: K-L Divergence between each class and a topic
for the politics vs rec dataset

5.

REFERENCES

[1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent
dirichlet allocation. The Journal of Machine Learning
Research, 3:993–1022, March 2003.
[2] B. A. Frigyik, A. Kapila, and M. R. Gupta.
Introduction to the dirichlet distribution and related
processes. Technical report. University of Washington,
Seattle, 2012. https://www.ee.washington.edu/
techsite/papers/documents/UWEETR-2010-0006.pdf
[3] T. L. Griffiths and M. Steyvers. Finding scientific
topics. PNAS, 101(suppl. 1):5228–5235, April 2004.
[4] A. Mccallum and K. Nigam. Text classification by
bootstrapping with keywords, EM and shrinkage. In
ACL-99 Workshop for Unsupervised Learning in
Natural Language Processing, pages 52–58, 1999.
[5] K. Nigam, A. K. McCallum, S. Thrun, and T. Mitchell.
Text classification from labeled and unlabeled
documents using EM. Machine Learning - Special issue
on information retrieval, 39(2-3), May-June 2000.
[6] A. Chanen and J. Patrick. Measuring Correlation
Between Linguists’ Judgments and Latent Dirichlet
Allocation Topics. Proceedings of the Australasian
Language Technology Workshop, pages 13–20, 2007.

CONCLUSIONS

In this paper, we propose a novel, inexpensive document
classification algorithm which requires minimal supervision.
Our algorithm is based on the generative property of LDA

880

