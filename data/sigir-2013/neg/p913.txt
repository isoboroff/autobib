Relevance Dimensions in Preference-based IR Evaluation
Jinyoung Kim

Gabriella Kazai

Imed Zitouni

Microsoft
Bellevue, WA, USA
jink@microsoft.com

Microsoft Research
Cambridge, UK
a-gabkaz@microsoft.com

Microsoft
Bellevue, WA, USA
izitouni@microsoft.com

as an important aspect of search engine evaluation and has been
investigated, for example, by the TREC Web Track’s diversity
task [6]. The track tackled the issue of evaluating diversity by
structuring the test topics into subtopics, reflecting different
intents, and obtaining relevance labels for each intent separately.

ABSTRACT
Evaluation of information retrieval (IR) systems has recently been
exploring the use of preference judgments over two search result
lists. Unlike the traditional method of collecting relevance labels
per single result, this method allows to consider the interaction
between search results as part of the judging criteria. For example,
one result list may be preferred over another if it has a more
diverse set of relevant results, covering a wider range of user
intents. In this paper, we investigate how assessors determine their
preference for one list of results over another with the aim to
understand the role of various relevance dimensions in preferencebased evaluation. We run a series of experiments and collect
preference judgments over different relevance dimensions in sideby-side comparisons of two search result lists, as well as
relevance judgments for the individual documents. Our analysis of
the collected judgments reveals that preference judgments
combine multiple dimensions of relevance that go beyond the
traditional notion of relevance centered on topicality. Measuring
performance based on single document judgments and NDCG
aligns well with topicality based preferences, but shows
misalignment with judges’ overall preferences, largely due to the
diversity dimension. As a judging method, dimensional preference
judging is found to lead to improved judgment quality.

Recently, a number of studies explored a new way of comparing
IR systems [13,11]. Instead of judging retrieved documents in
isolation, the use of preference judgments over two search result
lists, produced by two IR systems, has been proposed. Unlike the
traditional method of assessing relevance per single result, this
method enables comparing search results in context [13]. Using
this method, it becomes possible to consider the interaction
between search results as part of the evaluation criteria. Moreover,
the benefits of user preference based evaluation has been
demonstrated by Sanderson et al. who found high correlations
between user preferences and diversity measures such as intentaware precision or -NDCG [11].
However, comparing two lists of search results is a complex
cognitive task for assessors, where multiple dimensions of
relevance intermix and contribute to varying degree to the final
preference decision. Some of the well-studied aspects of relevance
include topicality, freshness, and authority, which characterize
individual documents, and novelty or diversity, which considers a
set or list of documents [8,15]. These aspects have been studied
extensively in the traditional relevance evaluation setting, but not
within a user preference based evaluation methodology.

Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Search
and Retrieval

In this paper, we adopt a user preference based evaluation method
and investigate how users determine their preference for one list
of results over another. To this end, we propose a method to
collect dimensional preference labels and conduct experiments in
which we ask assessors to compare two search result lists and
indicate their overall preference as well as their preferences along
five relevance dimensions: Relevance (topicality), Freshness,
Authority, Caption quality and Diversity. We employ professional
judges and obtain preference judgments for 523 queries extracted
from a commercial search engine’s query logs.

Keywords
Diversity, relevance criteria, user preferences,

1. INTRODUCTION
When collecting relevance judgments for the Cranfield [7] style
evaluation of IR systems, the established method is to assess the
relevance of individual documents to a given query independently
of other documents retrieved for the same query [14]. While this
method eliminates several sources of variability that may
influence assessors' relevance criteria, it ignores aspects that may
better reflect user satisfaction with search engine result pages
(SERP) in the real-world. For example, a SERP that contains
diverse relevant results satisfying multiple aspects of a user's need
may be preferred by users compared to a SERP with relevant, but
redundant individual results. Diversity has in fact been recognized

Our goal is to investigate:
 How do different dimensions of relevance relate to user
preference judgments?
 How do the different dimensions relate to traditional measures
such as NDCG?
 How does dimensional preference judging affect the judging
experience?

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for
components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to
post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright © 2013 ACM 978-1-4503-2034-4/13/07…$15.00.

Our contributions are: 1) a method for collecting dimensional
preference labels that can improve judgment quality, 2) analyses
that provide insights into the different relevance dimensions at
play in preference based IR evaluation, 3) finding showing that
preferences go beyond topicality relevance that is not captured
with traditional single document relevance judgments and
evaluation metrics, like NDCG.

913

top 10 results from the two engines along with the query
comprises one sample that is used as the input of one assessment
task in our experiments. We randomly assign the two engines to
the left or right, unbeknown to the assessors, such that they occur
on both sides with the same probability. Each search result is
represented by the web page's title, URL, and a snippet.

2. RELATED WORK
A range of alternative methods to IR evaluation has been
proposed in recent years that aim to go beyond the traditional
methods that treat each retrieved document in isolation. For
example, Bailey et al. [2] proposed a method that allows
investigating aspects such as coherence, diversity and redundancy
among the search results displayed in a SERP. Carterette et al. [4]
studied methods to evaluate search engines using preference
judgments over pairs of documents retrieved for a query. They
found significant improvement in inter-assessor agreement when
collecting preference labels as opposed to independent absolute
labels. Chandar and Carterette [5] employed a new preference
based design for the evaluation of novel document retrieval
methods, in which users gave preference judgments, given an
already observed document. Thomas and Hawking [13] proposed
a preference method that displays two sets of search results sideby-side and asks users to indicate which side they preferred. In
their experiments, comparing Google's first and second page
results, they reported high levels of accuracy in users preferring
the top ranked results. User preference based evaluation has also
been shown to correlate highly with diversity based evaluation
measures. Sanderson et al. [11] in a large-scale study, involving
nearly 300 users, 30 search topics and 19 TREC runs, provided
compelling evidence for the utility and reliability of user
preferences. The study in [10] demonstrated that even subtle
differences in retrieval results could be measured with preference
based comparisons. Following this line of work, we adopt a user
preference based evaluation method and collect preference labels
for a range of relevance dimensions over pairs of search result
lists. Unlike Bailey et al. [2], we allow assessors to interact with
the search results and visit the result web pages. Extending the
work of [11] we investigate the underlying criteria and relevance
dimensions upon which user preference decisions may rest.

We run two parallel experiments on the same data set, differing
only in the kind of judgment we ask assessors to make. In one
experiment (control), we only require assessors to indicate their
overall preference over the two result lists for a given query. In
the other experiment (treatment), in addition to the overall
preference judgment, we collect additional preferences along five
dimensions: Relevance, Diversity, Authority, Freshness and
Caption quality. We ask assessors to consider each of these
aspects in turn and indicate which side they prefer according to
that single dimension. Assessors were given judging guidelines
with explanations of each of the dimensions and the interface also
provided tooltip reminders. We collected 2 judgments per sample
in both conditions. Both groups of assessors were from a pool of
trained professional judges with similar judging experience and
performance quality.
All the preference judgments were collected on a 7 point scale,
with the mid-point reflecting no preference between the two sides,
and with increasing levels of preference for the left or right side
towards the two ends of the scale. We map the collected
preference judgments to the following scale {-3,-2,-1,0,1,2,3},
where the negative values indicate preference for the Baseline
engine’s results and positive values reflect preferences for the
Experiment engine’s results.
In addition to the two preference judging experiments, we collect
graded relevance labels, based on a 5 point scale, for the
individual query and search result pairs (query-URL pairs) using a
traditional relevance judging method (isolated, absolute judging).

The concept of relevance has been extensively studied in IR. It is
widely recognized for being multi-faceted and subjective. It is
also well known that human judgments are influenced by various
situational, cognitive, perceptual and motivational biases [3,9] as
well as by document variables, judgment conditions and scales,
and personal factors [12]. In spite of these, relevance has been
proven to be a reliable quantity in comparative IR evaluation [14].
Among the numerous aspects of relevance identified in the
literature, the most dominant dimensions are topicality, authority,
and, more recently, freshness [15]. Other aspects, which consider
the interaction of multiple results, include novelty and diversity
[5,1]. The latter has gained increased attention with the launch of
the diversity task at the TREC Web track [6]. The task required
participating systems to retrieve a ranked list of documents that
collectively satisfied multiple information needs, explicitly
defined by the subtopics of a given test topic. The retrieved
documents were assessed separately for each subtopic using a
traditional judging procedure and binary relevance. In this paper,
we propose a method to collect dimensional preference labels,
including diversity, and examine how these dimensions relate to
the overall preference over two search result lists.

4. RESULTS AND FINDINGS
4.1 Collected Data
From our original sample of 550 queries, 27 were discarded either
due to errors in the scrapes or judges later selecting the ``can’t
judge’’ option. For the remaining 523 queries, a total of 20,862
search results (with 9,306 unique URLs), forming 9,522 unique
query-URL pairs, were obtained from the two engines: 5,134
unique query-URL pairs (4,963 unique URLs) from the Baseline
engine and 5,153 unique query-URL pairs (5,054 unique URLs)
from the Experiment engine. The number of unique query-URL
pairs common to the two engines is 765 (711 unique URLs).
Table 1. Data sets in the Control and Treatment conditions
Queries
Judgments (2 per query)
Assessors
Avg. time spent per task (sec)
Avg. comment length (word)

3. EXPERIMENT SETUP

Control
523
1046
10
105
19

Treatment
523
1046
18
104
27

Table 1 summarizes the collected judgments. We can see that both
the judging tasks were completed in around 105 seconds on
average, even though the treatment group judges had to make
additional dimensional judgments and left longer comments. We
may hypothesize that the dimensions allowed judges to better
structure their preference decision process, which is confirmed in
the judges’ feedback comments, see Section 4.5.

We conduct experiments in which we gather user preference
judgments from professional assessors over pairs of search result
lists shown side-by-side, similarly to [13]. We randomly sample
550 queries from the query log of a commercial search engine.
From this query-set that is a mix of head and tail queries, we
scrape the SERPs returned by two commercial search engines that
we will refer to as Engine Baseline and Engine Experiment. From
each SERP, we extract the top 10 search results. A pairing of the

914

Diversity. This means that while these notions may be inherently
related to some degree, they are also distinguishably different
dimensions.

4.2 Win-Loss Analysis
In this section, we report Win-Loss analysis results for the two
engines, calculated based on the different preference judgments.
We define our WinLoss measure as the number of times engine A
is preferred over engine B minus the number of times engine B is
preferred, divided by the total number of comparisons:
,

#
#

So far, we have seen that preference judgments are largely
predicated on Relevance, but that Relevance alone does not
explain the overall preference decisions. This makes sense, since
if the retrieved results are not relevant in the first place, then no
matter how diverse or fresh the results are, users are not likely to
be satisfied. On the other hand, once Relevance cannot
differentiate between two search engines, then other aspects such
as diversity become important. This is confirmed by the WinLoss
scores calculated for cases when Relevance was judged as tie: the
overall preference based WinLoss score of 11.06 is matched with
a score of 22.86 for Diversity, 0.78 for Authority, -3.33 for
Freshness and 0 for Caption quality. From this we can conclude
that Diversity has the main impact in users’ overall preference
when both sides are equally relevant.

#
#

#

Results in Table 2 show the WinLoss statistics for the Experiment
engine over the Baseline for the different preference judgments.
Note that there are some missing dimensional judgments (holes),
as judges were asked to only provide them when they were
confident in their judgment.
We can observe that the overall preference in both conditions is
for the Experiment engine. However, we also see that the different
dimensions show different behaviors and trends. For example, the
winning engine outperforms the baseline in terms of diversity, but
loses on freshness. The overall WinLoss value lies somewhere inbetween the different dimensional scores.

Table 3. Correlation between overall preference and
dimensional preference labels in the Treatment group

Table 2. WinLoss statistics for the different preference
judgments in the Control and Treatment conditions
WinLoss
8.89
8.32
5.15
19.88
3.1
-0.95
4.19

Control Overall Preference
Treatment Overall Preference
Relevance
Diversity
Authority
Freshness
Caption Quality

Relevance
Diversity
Authority
Freshness
Caption Quality

%Holes
0%
0%
5.3%
34.6%
41.4%
49.8%
47.5%

Overall Preference
0.93
0.71
0.675
0.447
0.435

Relevance
1
0.614
0.647
0.427
0.404

4.3 Inter-judge Agreement
In this section, we report on the extent to which judges agree in
their preferences. Since we collected two judgments per sample,
we use Cohen’s Kappa () as our inter-judge agreement measure.
We calculate kappa values both for the original 7 point scale (7P)
and for the 3 point win-loss-tie scale (3P). In addition, the raw
Jaccard agreement ratio for the 3P labels are also shown, see
Table 4. Statistical significance (p<0.05) is indicated with *.

To investigate the relationship between the different preferences,
we plot their distribution in Figure 1. It is clear that Relevance
aligns the closest with the overall preference labels, followed by
Diversity. On the other hand, Authority, Freshness and Caption
quality behave very differently and are much less discriminating.
Note that this does not mean that they are not important criteria of
user satisfaction, but that in this case, both engines performed
similarly along these dimensions.

Table 4. Inter-judge agreement statistics per preference
Control Overall
Treatment Overall
Relevance
Diversity
Authority
Freshness
Caption quality

40

20

 7P
0.046*
0.088*
0.076*
0.063
0.058
0.200*
0.018

 3P
0.19*
0.22*
0.17*
0.10*
0.11*
0.25*
0.11

Jaccard
0.55
0.55
0.47
0.42
0.46
0.68
0.50

Results show that the treatment group has better inter-assessor
agreement levels, suggesting a benefit of using dimensional
preferences for improving judgment quality.

0
‐3

‐2
‐1
Overall (control)
Relevance
Authority
Caption Quality

0

1

2
3
Overall (treatment)
Diversity
Freshness

4.4 Correlation with NDCG
Although user preference judgments can capture different aspects
of search result quality, in this section, we compare the different
preference judgments with the established measure of NDCG,
calculated over the individual judgments collected separately for
the query-URL pairs in our data set. While a similar comparison
was conducted in [11], here we also consider how the individual
relevance dimension based preferences correlate with NDCG.
More specifically, we calculate the difference between the two
engine’s NDCG scores up to a given rank i, dNDCGi, and then

Figure 1. Distribution of the different preference labels in
the Control and Treatment conditions
Spearman correlation tests confirm that Relevance is the most
correlated dimension with judges’ overall preferences, followed
by the Diversity and Authority dimensions; see Table 3. Looking
at the correlation between Relevance and the other dimensions,
we see that Relevance and Authority are the closest, followed by

915

Our main finding is that overall preferences capture a range of
dimensions, most dominantly Relevance and Diversity. Authority,
Freshness and Caption quality were much less discriminating in
our data set. Thus, we see that preference judgments combine
multiple dimensions of relevance that go beyond the traditional
notion of relevance that is centered on topicality. This is further
confirmed by our finding that search engine performance scores,
calculated using traditional single document judgments and
NDCG correlates with our Relevance dimension preferences, but
shows misalignment with the overall preferences, largely due to
the diversity dimension. This is critical and demonstrates the need
to consider additional measures that go beyond relevance in IR
evaluation.

calculate the Spearman correlation between the obtained dNDCGi
scores and the 7P preference judgments. The obtained correlations
are summarized in Table 5.
We can see that the correlation between the different preference
judgments and dNDCG increases when dNDCG is calculated for
lower ranks. This suggests that preference labels are more holistic,
reflecting the (relative) quality of the whole result set. Comparing
the control and treatment groups’ overall preferences and the
dNDCGi scores, we observe higher correlations in the treatment
condition. This could be a result of the fact that judges had to
explicitly consider the Relevance (topicality) dimension in these
experiments, which is the dominating criterion for the judgments
that NDCG is calculated over (individual query-document pairs
judged independently of each other).

REFERENCES

Table 5. Spearman correlation statistics between the
preference judgments and dNDCGi scores
Control Overall
Treatment Overall
Relevance
Diversity
Authority
Freshness
Caption Quality

dNDCG1
0.081
0.117
0.111
0.036
0.11
0.071
0.006

[1] Azzah Al-Maskari, Mark Sanderson, and Paul Clough, "The
relationship between IR effectiveness measures and user
satisfaction," in Proceedings of the 30th ACM SIGIR Conference,
2007, pp. 773-774.

dNDCG3
0.221
0.269
0.284
0.149
0.221
0.229
0.086

[2] Peter Bailey et al., "Evaluating search systems using result page
context," in Proceedings of the 3rd Symposium on Information
Interaction in Context, 2010, pp. 105-114.
[3] P. Borlund, "The concept of relevance in IR," Journal of the
American Society for Information Science and Technology, vol. 54,
no. 10, pp. 913-925, 2003.
[4] Ben Carterette, Paul N. Bennett, David Maxwell Chickering, and
Susan T. Dumais, "Here or There," in ECIR, 2008, pp. 16-27.
[5] Praveen Chandar and Ben Carterette, "Using Preference Judgments
for Novel Document Retrieval," in Proceedings of the 35th ACM
SIGIR Conference, Portland, Oregon, 2012.

Looking at the dimensional preferences, we see that the Relevance
dimension shows the highest correlation with dNDCGi, even
above that for the overall preference. Diversity and Caption
quality are the least correlated with dNDCGi. This is interesting,
especially since the overall preference is less correlated with
dNDCGi than the Relevance preference, confirming that other
dimensions that contribute to the overall preference cannot be
measured this way. This indicates a potential gap between real
user satisfaction and search engine performance when optimized
for relevance (and NDCG) alone.

[6] C. Clarke, N. Craswell, and I. Soboroff, "Overview of the TREC
2009 Web Track," in TREC Proceedings, 2009.
[7] C. W. Cleverdon, "The Cranfield tests on index language devices,"
Aslib, vol. 19, pp. 173-192, 1967.
[8] E. Cosijn and P. Ingwersen, "Dimensions of Relevance," Information
Processing & Management, vol. 36, no. 4, pp. 533-550, 2000.
[9] E. Pronin, "Perception and misperception of bias in human
judgment," Trends in cognitive sciences, vol. 11, no. 1, pp. 37-43,
2007.

4.5 Assessor Feedback
One consideration when collecting relevance judgments is the
assessors’ experience with the task. Since in a preference based
evaluation, judges need to evaluate many different signals, an
interface that provides a shortlist of possible criteria to consider
may be helpful. Indeed some of the judges in our treatment
experiments commented on this very fact. For example, one judge
noted that “It helped me to remember the different aspects that I
should take into consideration while judging”. Another said that
“[the dimensions] were helpful, because I take all the factors into
consideration. It helped put focus on some of the smaller
differences such as captions…”

[10] Filip Radlinski and Nick Craswell, "Comparing the sensitivity of
Information Retrieval metrics," in Proceedings of the 33rd ACM
SIGIR Conference, 2010, pp. 667-674.

5. CONCLUSIONS

[13] Paul Thomas and David Hawking, "Evaluation by comparing result
sets in context," in Proceedings of the 15th ACM CIKM Conference,
2006, pp. 94-101.

[11] Mark Sanderson, Monica Lestari Paramita, Paul Clough, and
Evangelos Kanoulas, "Do user preferences and evaluation measures
line up?," in Proceedings of the 33rd ACM SIGIR Conference, 2010,
pp. 555-562.
[12] T. Saracevic, "Relevance: A review of the literature and a framework
for thinking on the notion in information science. Part III: Behavior
and effects of relevance," Journal of the American Society for
Information Science and Technology, vol. 58, no. 13, pp. 2126-2144,
2007.

In this paper, we proposed a preference-based evaluation method
to collect both overall and per relevance dimension based
preferences (Relevance, Diversity, Authority, Freshness and
Caption quality). We found that considering the dimensions
separately increases judgment quality, while also allows
experimenters to measure different aspects of a system. Based on
judges’ comments, it seems that the addition of dimensions
provided some structure to their judging process, which resulted
in better quality. It is plausible that the process of considering
dimensional preferences allowed judges to better think through
their decisions, which may have contributed to the observed
higher inter-assessor agreement levels.

[14] Ellen M. Voorhees and Donna K. Harman, Eds., TREC:
Experimentation and Evaluation in Information Retrieval.: MIT
Press, 2005.
[15] Y. Xu and Z. Chen, "Relevance judgment: What do information users
consider beyond topicality?," Journal of the American Society for
Information Science and Technology, vol. 57, no. 7, pp. 961-973,
2006.

916

