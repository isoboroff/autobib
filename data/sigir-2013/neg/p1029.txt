Fresh BrowseRank
Maxim Zhukovskiy

Andrei Khropov

Gleb Gusev

Pavel Serdyukov

Yandex
16 Leo Tolstoy St., Moscow, 119021 Russia

{zhukmax, akhropov, gleb57, pavser}@yandex-team.ru

ABSTRACT
In the last years, a lot of attention was attracted by the problem of page authority computation based on user browsing
behavior. However, the proposed methods have a number of
limitations. In particular, they run on a single snapshot of a
user browsing graph ignoring substantially dynamic nature
of user browsing activity, which makes such methods recency
unaware. This paper proposes a new method for computing page importance, referred to as Fresh BrowseRank. The
score of a page by our algorithm equals to the weight in a
stationary distribution of a flexible random walk, which is
controlled by recency-sensitive weights of vertices and edges.
Our method generalizes some previous approaches, provides
better capability for capturing the dynamics of the Web
and users behavior, and overcomes essential limitations of
BrowseRank. The experimental results demonstrate that
our method enables to achieve more relevant and fresh ranking results than the classic BrowseRank.
Categories and Subject Descriptions: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval
General Terms: Algorithms, Experimentation, Performance
Keywords: Page authority, BrowseRank, freshness, web
search, learning

1.

INTRODUCTION

Web page authority scores are regarded as one of the most
useful features for ranking algorithms. There are many approaches to evaluation of page importance. Among them
there are those which analyze the user browsing histories.
Particularly, BrowseRank algorithm [1] measures the page’s
importance by its probability in the stationary distribution
of a continuous-time Markov process on the user browsing
graph. Despite undeniable advantages of BrowseRank and
its generalizations (see Section 2), these algorithms miss the
temporal aspect of the Web. Newer pages are probably more
relevant to recency sensitive queries than the older ones, and
therefore the temporal aspect of document relevance relates

to the to the ability of a browsing based page authority
measure to distinguish between relevant and irrelevant documents. With regard to web pages devoted to periodical
events such as news, government law issues, sport reviews
or conference homepages, a surfer looks for content likely
related to newer facts and events. By this reason the problem of constructing recency sensitive (or “fresh”) BrowseRank should be of great interest for commercial web search
engines.
In this work we present the algorithm called Fresh BrowseRank. We make our algorithm fresh-aware by weighting pages
according to their freshness. In order to show the effectiveness of Fresh BrowseRank, we compare it with the classic
BrowseRank on a large sample of user browsing graph and
demonstrate that our algorithm is better performing than
BrowseRank. To the best of our knowledge the proposed
algorithm is the first fresh-aware ranking algorithm based
on the user browsing history.
The remainder of the paper is organized as follows. Section 2 contains a short review of related approaches to measuring page importance. In Section 3 we describe a framework necessary to introduce our algorithm. We define Fresh
BrowseRank in Section 4. In Section 5 we introduce the
method we employ for tuning of parameters of our algorithm. The experimental results are reported in Section 6.
In Section 7 we discuss potential applications and future
work.

2.

RELATED WORK

A widely used approach to page importance computation
is analysis of the web as a graph of pages connected with
hyperlinks. The most acknowledged link-based method is
PageRank (Page et al. [2]). According to this algorithm
the score of a page p equals its weight in the stationary distribution of Discrete-Time Markov process which models a
random walk on the hyperlink graph. Such approaches of
computing web page authority have some weaknesses. Particulary, a significant portion of pages and hyperlinks are
not reliable and not used by real users. The other items are
used with essentially different level of activity. Therefore, a
random surfer modeled by using a plain hyperlink graph is
not realistic. In 2008 Liu et al. [1] proposed BrowseRank,
which computes the page importance using user browsing
behavior data. The browsing graph contains richer information such as staying times on web pages by users, number of
transitions between web pages by users. Unlike PageRank
algorithm based on a discrete random work, BrowseRank
exploits a Continuous-Time Markov Process driven by user

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.

1029

browsing statistics.This model provides a more realistic view
on pages’ importance. However, a proper web ranking algorithm should possess some properties that BrowseRank
algorithm still does not. Liu et al. in [3], [4] proposed different generalizations of the algorithm which overcome some of
its imperfections. In [3] the authors introduce different ways
of estimating the distribution of the staying time. In [4] a
Markov Skeleton Process is used to model the random walk
conducted by the web surfer. They show that this framework
can cover many existing algorithms (among them PageRank, Usage-based PageRank [5], TrustRank [6], BrowseRank
Plus, MobileRank [7] as its special cases. The parameters of
these algorithms can be chosen in such a way that they become classical BrowseRank. There are also other approaches
of computing page authority using user behavior data which
do not exploit random walk (see, for example, [8]).
An important problem which is not solved by all the mentioned algorithms is the recency ranking problem. As it is
shown in [9], the user browsing graph is changing significantly day by day. Therefore, pages with a high BrowseRank score a few days ago can be not authoritative in the
present.
Yu et al. [10] were among the first to study algorithms
of link analysis sensitive to the temporal aspects of the
Web. The authors modified PageRank by weighting each
hyperlink according to its age. Other algorithms of recencysensitive link-based ranking were studied by Amitay et al. in
[11], Berberich et al. in [12] and [13], Yang et al. in [14], Dai
and Davison in [15]. The algorithm T-Fresh from the last
paper generalizes the ideas from the other above-mentioned
papers. However, T-Fresh has a number of shortcomings
which are overcome by the algorithm APR introduced by
Zhukovskii et al. in [16]. Both methods are based on graph
weightings that capture freshness of pages and hyperlinks.
Dai and Davison constructed two different factors of page
importance that depend on pages’ freshness and links’ freshness respectively. In contrast to T-fresh, the freshness measure of APR depends on combined pages’ and links’ activities. This approach allows to control influence of both types
of activities on the assigned score. Following similar principles, we propose a solution to overcome the problem of recency insensitivity of BrowseRank as we show in Section 6.
To the best of our knowledge this paper is the first to propose a recency-sensitive variation of BrowseRank.

3.

We define the user browsing graph G = (V, E) as follows.
The set of vertices V consists of all the web pages mentioned
in the browsing log and contains one additional vertex x.
The set of directed edges E represents all the ordered pairs
of neighboring elements {p1 , p2 }. The set E contains also
additional edges from the last pages of all the sessions to
the vertex x. Let σ(x) = 0. For any page p such that
p → x ∈ E denote the number of sessions finished with p by
I(p, x).
Let us remind the definition of BrowseRank. Reset probability σ(p) is a probability of choosing page p while we start
a new browsing session. It is proportional to the number of
sessions s(p) starting from the page p. Therefore σ(x) = 0.
Transition probability w(p1 → p2 ) is a probability of clicking!
P
a hyperlink p1 → p2 . It equals to I(p1 , p2 )/
I(p1 , p) .
p1 →p∈E

The estimated staying time Q(p) of the page p is defined in
[1]. BrowseRank of p equals Q(p)π(p), where
X
π(p) = α̃(p)σ(p) + (1 − α)
ω(p̃ → p)π(p̃)
p̃6=x: p̃→p∈E

(the last equations hold for p = x as well), α̃(p) = α(1 −
π(x)) + π(x).
Note that the transition probabilities do not depend on
the freshness of links and the user chooses old and fresh
links with the same probabilities. Therefore, we make the
transition probabilities fresh by weighting the links according to the freshness scores of the destination pages. In the
next session we introduce this freshness measure.

4.

FRESH BROWSERANK

Let us define the freshness measure of a page. We use the
framework from [16]. Consider two moments of time τ, T ,
τ < T . We divide the interval [τ, T ] into K parts: [ti−1 , ti ],
−τ
. For each page
i ∈ {1, 2, . . . , K}, t0 = τ , ti − ti−1 = TK
p from V denote by t(p) the date when it was created. We
consider vertex x to be created at moment τ .
Let i ∈ {1, 2, . . . , K}. Let p ∈ V be a vertex created
before moment ti . We define the freshness score Fi (p) of p
in the i-th period of time in the following way.
1) We define the initial value F0i (p) capturing freshness of
page p and its links as follows:
F0i (p) = a0 ni (p) + b0 mi (p), p 6= x,

FRAMEWORK AND NOTATION

Popular search engines suggest users to install browser
toolbars that, among other useful features, monitor user
browsing sessions in order to use this aggregated information
later on to improve their search quality. The anonymized
information describing user behavior (visited pages, times
of visiting, submitted queries etc.) is stored in the browsing log. We define the sessions and the user’s browsing
graph in the same way as Liu et al. [1]. Let S be a user
session. We denote its pages by p1 (S), p2 (S), ..., pk(S) (S).
For each i ∈ {1, 2, ..., k(S) − 1} toolbar makes a record
pi (S) → pi+1 (S). We call pages pi (S), pi+1 (S) the neighboring elements of the session S.
For each page p from the browsing log, we denote the number of sessions this page starts with by s(p). For each pair
of neighboring elements {pi , pi+1 } from a session we denote
the number of sessions containing such a pair of neighboring
elements by I(pi , pi+1 ).

(1)

where a0 , b0 , are nonnegative parameters (which are learned
according to the method described in Section 6); ni (p) = 1 if
the vertex p is created in the i-th period, otherwise ni (p) =
0; mi (p) is the number of visits of page in the i-th period.
We set Fi0 (x) = 0.
2) Initial measure F0i (p) is spreading over vertices by outgoing edges:
∆Fi (p) = µF0i (p)+
X
(1 − µ)
p̃6=x: p̃→p∈E

W (p)
P i

Wi (p0 )

∆Fi (p̃), (2)

p0 ∈V : p̃→p0 ∈E

where µ ∈ [0, 1], Wi (p) is a score assigned by the “local”
freshness measure to the vertex p in the i-th period. This
measure is defined in the same way as initial measure F0i

1030

(the values of the parameters may be different from those of
the parameters in equation (1)),
X
Wi (p) = a1 ni (p) + b1 mi (p) +
nj (p), a1 , b1 > 0. (3)

page p2 but i < j. The function h is a loss function. We
consider loss with margins bij > 0, where 1 6 i < j 6 k:
h(i, j, x) = max{x+bij , 0}2 . Let ω be a vector of parameters
of FBR. We minimize
X X
X
h(i, j, fq (p2 ) − fq (p1 ))
F (ω) =

j6i

We want to “spread freshness measure” through outgoing
links from a page even if there are no fresh links among
them. Therefore we increase the weight of the page by 1
(the last summand in 3) if it was created before moment ti .
The system described in Equation 2 illustrates the influence
of neighbors on the freshness measure of the page.
3) Finally, freshness measure Fi is defined as follows: Fi (p)
= βFi−1 (p) + ∆Fi (p). The measure decreases exponentially
as time goes if there are no activities concerned with the
vertex p (the parameter β is from (0, 1)). Indeed Fi (p) =
β i ∆F0 (p) if there were no such activities during the period
[τ, ti ].

q

16i<j6k p ∈V i ,p ∈V j
2
1
q
q

by a gradient based optimization method. A fresh ranking
algorithm requires frequent tuning of parameters. A simple
choice of values of parameters from a large set takes a lot of
time. Thus applying such simple choice makes it impossible
to get a fresh score improving a search quality. Let us inF
. To the best
troduce the way we found the derivative ∂π
∂ωi
of our knowledge we are the first to get derivatives of the
stationary distribution of Markov process when its transition probabilities are functions of a stationary distribution
of another Markov process (such nested Markov processes
are widely used, see [15], [16]). It is easy to find the derivaresh
F
, ∂π
as the solutions of the following system
tives ∂πF∂α
∂β
of linear equations.


∂πF
∂πF
(p) = σ(p) 1 − πF (x) + (1 − α)
(x) +
∂α
∂α


(4)
X
∂πF
ωF (p̃ → p) (1 − α)
(p̃) − πF (p̃) ;
∂α

In Equation 2 all considered vertices and edges are supposed to be created before the time moment ti .
Let the freshness measure assign to page p in the graph
G the score FK (p). We make transition probability fresh
by replacing I(p1 , p2 ) with I(p1 , p2 )FK (p2 ). In other words,
Fresh transition probability
ωF (p1 → p2 ) of edge p1 → p2
P
equals to I(p1 , p2 )FK (p2 )/
p: p1 →p∈E I(p1 , p)FK (p) and

p̃6=x: p̃→p∈E

Fresh BrowseRank of p equals Q(p)πF (p), where
X
πF (p) = α̃(p)σ(p) + (1 − α)
ωF (p̃ → p)πF (p̃).

∂πF
∂πF
(p) = σ(p)(1 − α)
(x) + (1 − α)×
∂β
∂β


X
∂πF
∂ωF
ωF (p̃ → p)
(p̃) +
(p̃ → p)πF (p̃) , (5)
∂β
∂β

p̃6=x: p̃→p∈E

p̃6=x: p̃→p∈E

Parameter
[τ, T ]
K
a0
b0
a1
b1
µ
α
β

Description
the considered period of time
the number of time intervals
the gain F0i (p) receives if t(p) = i
the gain Wi (p) receives if t(p) = i
the gain F0i (p) receives if
a user clicks to p in the i-th period
the gain Wi (p) receives if
a user clicks to p in the i-th period
damping factor for Fi (p) calculating
damping factor for FBR score calculating
the rate of decreasing of Fi (p)

we get the derivative
following equation:

→ p) by finding

∂Fk
(p)
∂β

from the

k−1

X
∂FK
(p) =
(i + 1)β i ∆Fi+1 (p).
∂β
i=0
Let us introduce the systems of linear equations with the
F
F
F
F
F
, ∂π
solutions ∂π
, ∂π
(derivatives ∂π
, ∂π
are the so∂µ
∂a0
∂a1
∂b0
∂b1
lutions of the same equations). The first equations of the
systems are the same with the equations of (5). We just
should choose the considered parameter instead of β. It rei
i
i
, ∂∆F
, ∂∆F
:
mains to find ∂∆F
∂µ
∂a0
∂a1

Table 1: Parameters of Fresh BrowseRank.

∂∆Fi
(p) = Fi0 (p)+
∂µ

 (6)
X
∂∆Fi
Wi (p̃ → p) (1 − µ)
(p̃) − ∆Fi (p̃) ,
∂µ

In Table 1 we give the description of all the parameters of
the algorithm.

5.

∂w
(q
∂β

LEARNING

p̃6=x: p̃→p∈E

Let fq (p) be the value of our feature for a page p and a
query q. We make it query-dependent by combining FBR
linearly with a query-dependent feature.
The training data contains a collection of queries and
sets of pages Vq1 , Vq2 , ..., Vqk for each query q which are ordered from the most relevant (or fresh) to irrelevant (or not
fresh) pages. In other words, Vq1 is the set of all pages
with the highest score selected from among k labels, pages
from the set Vqk have the lowest score. For any two pages
p1 ∈ Vqi , p2 ∈ Vqj let h(i, j, fq (p2 ) − fq (p1 )) be a value of
a penalty we get if the position of the page p1 according
to our ranking algorithm is higher then the position of the

!

,

Wi (p0 ) ;

P

where Wi (p̃ → p) = Wi (p)

p0 ∈V : p̃→p0 ∈E

∂∆Fi
(p) = µni (p) + (1 − µ)
∂a0
∂∆Fi
(p) = (1 − µ)
∂a1

1031

X

Wi (p̃ → p)

p̃6=x: p̃→p∈E

X
p̃6=x: p̃→p∈E



∂∆Fi
(p̃);
∂a0

∂∆Fi
(p̃)+
∂a1
 (7)
∂Wi
(p̃ → p)∆Fi (p̃) .
∂a1
Wi (p̃ → p)

Parameters τ, T, K are chosen among a small number of variants. We consider the period of time [τ, T ] of the length
which is equal to 1 week. The parameter K is chosen in
such a way that the length of one period [ti−1 , ti ] is from
among 1 day, 6 hours, 3 hours and 1 hour.

6.

EXPERIMENTAL RESULTS

All experiments are performed with pages and links crawled
in December 2012 by a popular commercial search engine.
We utilize all the records from the toolbar that were made
from 10 December 2012 to 16 December 2012. There are
113M pages and 478M transitions in the log. For ranking
evaluation we sampled a set of queries from the queries submitted by real users during the period from 14th till 17th
December. A query is a pair <text of query, time of submitting>. For each query a set of URLs was judged by professional assessors hired by the search engine. When an assessor judged a pair <query, URL> he assigned a label based
on both the freshness of the page in respect to the query
time and the topical relevance of the page to the query. Due
to specifics of the task, we consider only recency-sensitive
queries, which our algorithm is focused on. Hence, in order
to be relevant for such queries, the documents need to be
both fresh and topically relevant at the same time. Assessors
are instructed to consider documents to be fresh at least to
some degree if only the last access to the document according
to the toolbar logs took place not earlier than 3 days before
the time of the judged query. Finally, the above-described
procedure resulted into 200 judged recency-sensitive queries
(with 3000 judged query-URL pairs) for our evaluation.
The relevance score is selected from among the editorial
labels: Perfect, Excellent, Good, Fair, Bad. We divide our
data into two parts. On the first part (75% of the dataset)
we train the parameters and on the second part we test
the algorithms. We compare Fresh BrowseRank with classic
BrowseRank in the following way used also by Dai and Davison [15] and Zhukovskiy et al. [16]. These algorithms are
combined linearly by ranks with BM25. Then parameters
for Fresh BrowseRank are chosen by maximizing the lossfunction in the way described in Section 5. The parameter
of linear combination with BM25 is chosen by maximizing
the Normalized Discounted Cumulative Gain (NDCG) metric. We obtain the following parameters.
K = 24, a0 ≈ 5.2, b0 ≈ 1.0, a1 ≈ 6.9, b1 ≈ 1.1,
µ = 0.2, α = 0.18, β = 0.9.
The parameter K is chosen from the set {7, 28, 56, 168}. In
these cases the lengths of periods [ti−1 , ti ] equal to 1 day, 6
hours, 3 hours and 1 hour correspondingly.
Table 2 demonstrates the ranking performance on metrics
NDCG@5 and NDCG@10 over the algorithms.
Algorithm
FBR
BR

NDCG@5
0.71256
0.68312

NDCG@10
0.784
0.75188

Table 2: Performance.

7.

CONCLUSION

In this paper we introduce the new recency-sensitive algorithm of user’s behavior analysis Fresh BrowseRank. We

1032

compare it with BrowseRank. We test our algorithm on a
large data set and demonstrate that our algorithm is better than BrowseRank. Besides our results being interesting
on their own, they may be used by commercial web search
engines for improving a search quality. It seems natural to
propose other recency-sensitive algorithms of user’s behavior
analysis based on our framework. By this reason we believe
that our approach will be the basis for the new research in
this area.
It would be interesting to study the other aspects of the
Web the user’s behavior analysis algorithms are missing. For
example, it is natural to introduce the layer of query nodes
into the graph.
We also introduce the method of training parameters of
nested Markov processes. It would be interesting to exploit
these algorithms for other algorithms based on such processes.

8.

REFERENCES

[1] Y. Liu, B. Gao, T.-Y. Liu, Y. Zhang, Z. Ma, S. He, H. Li,
BrowseRank: Letting Web Users Vote for Page
Importance. Proc. SIGIR’08, pp. 451–458 , 2008.
[2] L. Page, S. Brin, R. Motwani, and T. Winograd, The
PageRank citation ranking: Bringing order to the web.
http://dbpubs.stanford.edu/pub/1999-66, 1999.
[3] Y. Liu. T.-Y. Liu, B. Gao, Z. Ma, H. Li, A framework to
compute page importance based on user behaviors, Inf
Retrieval, 13: 22–45, 2010.
[4] B. Gao, T.-Y. Liu, Z. Ma, T. Wang, H. Li, A General
Markov Framework for Page Importance Computation,
Proc. CIKM’09, pp. 1835–1838, 2009.
[5] M. Eirinaki, M. Vazirgiannis, UPR: Usage-based Page
Ranking for Web Personalization, Proc. on Fifth IEEE
International Conference on Data Mining, pp. 130–137,
2005.
[6] Z. Gyongyi, H. Garcia-Molina, J. Pedersen, Combating web
spam with trustrank, Proc. InVLDB’04, pp. 576–587, 2004.
[7] B. Gao, T.-Y. Liu, Yu. Liu, T. Wang, Z.-M. Ma, H. Li,
Page Importance Computation Based on Markov Processes,
Inf. Retrieval, 14 (5), pp. 488–514, 2011.
[8] G. Zhu, G. Mishne, Mining Rich Session Context to
Improve Web Search, Proc. KDD’09, pp. 1037–1046 , 2009.
[9] Y. Liu, Y. Jin, M. Zhang, S. Ma, L. Ru, User Browsing
Graph: Structure, Evolution and Application, WSDM 2009,
2009.
[10] P. S. Yu, X. Li, and B. Liu, On the temporal dimension of
search. Proc. WWW’04, pp. 448–449, 2004.
[11] E. Amitay, D. Carmel, M. Herscovici, R. Lempel, and
A. Soffer. Trend detection through temporal link analysis.
Journal of the American Society for Information Science
and Technology, 55(14), pp. 1270–1281, 2004.
[12] K. Berberich, S. Bedathur, M. Vazirgiannis, G. Weikum.
Buzzrank... and the trend is your friend. Proc. WWW’06,
pp. 937–938, 2006.
[13] K. Berberich, M. Vazirgiannis, G. Weikum, Time-aware
authority ranking. Int. Math., 2(3), pp. 301–332, 2005.
[14] L. Yang, L. Qi, Y. P. Zhao, B. Gao, and T. Y. Liu. Link
analysis using time series of web graphs. Proc. CIKM’07,
pp. 1011–1014, 2007.
[15] Na Dai and Brian D. Davison, Freshness Matters: In
Flowers, Food, and Web Authority. Proc. SIGIR’10,
pp. 114–121, 2010.
[16] M. Zhukovskii, D. Vinogradov, G. Gusev, P. Serdyukov,
A. Raigorodkii, Recency-sensitive model of web page
authority. Proc. CIKM’12, pp. 2627–2630, 2012.

