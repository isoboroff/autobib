Ranking Explanatory Sentences for Opinion
Summarization
Hyun Duk Kima , Malu G Castellanosb , Meichun Hsub ,
ChengXiang Zhaia , Umeshwar Dayalb , Riddhiman Ghoshb
a

University of Illinois at Urbana-Champaign, USA; b HP Laboratories, USA

{hkim277, czhai}@illinois.edu,
{malu.castellanos, meichun.hsu, umeshwar.dayal, riddhiman.ghosh}@hp.com
a

b

ABSTRACT

to be summarized are already known to be about ‘positive opinions about iPhone screen’, such a summary is obviously redundant
and does not give any additional information to explain the reason
why the positive opinion about iPhone screen is held. A more explanatory summary, such as “retina display is very clear”, would be
much more useful to the users.
In this paper, we introduce and study a novel sentence ranking
problem called explanatory sentence extraction (ESE) which aims
to rank sentences in opinionated text based on their usefulness for
helping users understand the reasons of sentiments. As can be seen
in the previous example, explanatory sentences should not only be
relevant to the target topic we are interested in, but also include details explaining reasons of sentiments; generic positive or negative
sentences are generally not explanatory.
The main technical challenge in solving this problem is to assess
the explanatoriness of a sentence in explaining sentiment. We focus on studying how to solve this problem in an unsupervised way.
Compared with a supervised approach which requires manually labeled training data, an unsupervised approach has the practical advantage of not requiring any manual effort, and being applicable to
many different domains. Moreover, in case when we have labeled
data available, we can always plug in an unsupervised approach
into any supervised learning approach as a feature.
We propose three heuristics for scoring explanatoriness of a sentence (i.e., length, popularity, and discriminativeness). In addition
to the representativeness of information which is a main criterion
used in the existing summarization work, we also consider discriminativeness with respect to background information and lengths of
sentences. We propose two general new methods for scoring explanatoriness of a sentence based on these heuristics, including a
method adapted from TF-IDF weighting and a probabilistic model
based on word-level likelihood ratios.
We created two new data sets and proposed a novel weighted
Mean Average Precision measure to evaluate the proposed explanatoriness scoring methods. Experiment results show that all the proposed methods are effective in selecting explanatory sentences, outperforming a state of the art sentence ranking method taken from a
regular text summarization method.

We introduce a novel sentence ranking problem called explanatory
sentence extraction (ESE) which aims to rank sentences in opinionated text based on their usefulness for helping users understand the
detailed reasons of sentiments (i.e., “explanatoriness"). We propose
and study several general methods for scoring the explanatoriness
of a sentence. We create new data sets and propose a new measure
for evaluation. Experiment results show that the proposed methods are effective, outperforming a state of the art sentence ranking
method for standard text summarization.

Categories and Subject Descriptors
I.2.7 [Artificial Intelligence]: Natural Language Processing—Text
analysis; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval—Information filtering

Keywords
Explanatory sentence ranking, Explanatoriness scoring, Opinion
summarization

1.

INTRODUCTION

Most previous studies on opinion mining and summarization have
focused on predicting sentiments of entities and aspect-based rating for the entities [4, 5, 6]. Although these existing techniques
can show the general opinion distribution (e.g. 70% positive and
30% negative opinions about battery life), they cannot provide the
underlying reasons why people have positive or negative opinions
about the product. Therefore, even if such an opinion summarization technique is available, people would still need to read through
the classified opinionated text collection to find out why people expressed those opinions.
General automatic summarization techniques [3, 8, 9] can be
used to shrink the size of text to read, but they generally extract sentences based on ‘popularity’, and as a result, the output summary
tends to cover already known information. For example, to summarize ‘positive opinions about iPhone screen’, a pure popularitybased summary may be “screen is good”. Given that the sentences

2. PROBLEM FORMULATION
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2034-4/13/07 ...$15.00.

Our problem formulation is based on the assumption that existing techniques can be used to (1) classify review sentences into
different aspects (i.e., subtopics); and (2) identify the sentiment
polarity of an opinionated sentence (i.e., either positive or negative). Thus, as a computational problem, the assumed input is (1)
a topic T as described by a phrase (e.g., iPhone), (2) an aspect A
as described by a phrase (e.g., ‘screen’ of iPhone), (3) a polarity
of sentiment P (on the specified aspect A of topic T ), which is ei-

1069

ther ‘positive’ or ‘negative’, and (4) a set of opinionated sentences
O = {S1 , ..., Sn } of the sentiment polarity P .
Given T , A, P , and O as input, the desired output is a ranked list
of all sentences in O by their explanatoriness, L = (S1′ , ..., Sn′ ),
where Si′ ∈ O and explanatory sentences would be ranked on top
of non-explanatory ones. Such a ranked list can be directly useful
to help users digest opinions or fed into any existing summarization
algorithm to generate an explanatory opinion summary.

3.

tive basic information retrieval functions. Indeed, our popularity
heuristic can be captured through Term Frequency (TF) weighting,
while the discriminativeness can be captured through Inverse Document Frequency (IDF) weighting. We thus propose the following
modified BM25 for explanatoriness scoring (BM25E ).
For explanatoriness ranking, we can consider a sentence as a
query and measure explanatoriness of each word of the sentence
based on how frequent the word is in the input data set (O) and
the background data set (B). Specifically, given a sentence S =
w1 , w2 , ..., wn , the modified BM25E would be defined as:

EXPLANATORINESS SCORING

3.1 Basic Heuristics

BM 25E (S, O, B) =

X

IDF (w, B)

w∈S

We first propose three heuristics that may be potentially helpful
for designing an explanatoriness scoring function.
1. Sentence length: A longer sentence is more likely explanatory
than a shorter one since a longer sentence in general conveys more
information.
2. Popularity and representativeness: A sentence is more likely
explanatory if it contains more terms that occur frequently in all
the sentences in O. This intuition is essentially the main idea used
in the current standard extractive summarization techniques. We
thus can reuse an existing summarization scoring function such as
LexRank for scoring explanatoriness. However, as we will show
later, there are more effective ways to capture popularity than an
existing standard summarization method; probabilistic models are
especially effective.
3. Discriminativeness relative to background: A sentence with
more discriminative terms that can distinguish O from background
information is more likely explanatory. As illustrated by the example in Section 1, over emphasis on representativeness would give
us redundant information. Explanatory sentences should provide
us more specific information about the given topic. Therefore, intuitively, an explanatory sentence would more likely contain terms
that can help distinguish the set of sentences to be summarized O
from more general background sets which contain opinions that are
not as specific as those in O. That is, we can reward a sentence that
has more discriminative terms, i.e., terms that are frequent in O,
but not well covered in a background set.
There may be different ways to construct the background set,
which in general would be a superset of O. In our problem setting,
the set O consists of sentences satisfying the constraints that they
cover aspect A of topic T with sentiment polarity P . We can construct a background set by relaxing any of these constraints. For
example, we may drop the requirement on polarity P to construct
a background set consisting of sentences about aspect A of topic T
(regardless sentiment polarity), or drop the requirement on aspect
A to obtain a background set with sentences covering broadly topic
T with sentiment polarity P . The most general background set
would consist of all sentences about topic T , which can be easily
obtained and has been used in our experiments.
While all the three heuristics make sense, how to combine them
optimally remains a challenging question. Over empahsis on any
single factor would likely lead to unsatisfactory results. For example, if we only focus on popularity, we would likely rank noninformative sentences high. On the contrary, if we only focus on
discriminativeness, the rare opinions which are mentioned only a
few times by eccentric users may be ranked very high. Below we
present two general methods to combine these heuristics for explanatoriness scoring.

IDF (w, B) = log

c(w, O)(k1 + 1)
|O|
c(w, O) + k1 (1 − b + b avgdl
)

|B| − c(w, B) + 0.5
c(w, B) + 0.5

where c(w, O) is the count of w in data set O, |O| and |B| is the
total number of term occurrences in data set O and B respectively,
and avgdl is the average number of total term occurrences of subclusters in T that O is extracted from. k1 and b are parameters to
be empirically set. The length heuristic is implicitly captured by
this scoring function since the sum is taken over all the words in S.

3.3 Probabilistic Explanatoriness Scoring
The basic idea of our second method is to define the explanatoriness of a sentence as the sum of the explanatoriness of each
word in the sentence, and model the explanatoriness of each word
with a probabilistic model. Indeed, if we treat each word as a unit
for modeling explanatoriness, it would be natural to define the explanatoriness of a sentence based on how explanatory each word in
the sentence is. Thus if we use ES(S) and ES(w) to denote explanatoriness score of sentence S and that of word w respectively,
we can define ES(S)
P as the sum of ES(w) over all the words in
S, i.e. ES(S) = w∈S ES(w). Note that with this strategy, we
explicitly encode the length heuristic, and a longer sentence would
be generally favored.
Now the question is how to model and estimate ES(w). For this
purpose, we assume that each word w can be either explanatory or
not, denoted by E ∈ {0, 1} and score the explanatoriness of word
w based on the conditional probability p(E = 1|w), which can be
interpreted as the posterior probability that word w is explanatory.
That is, ES(w) = p(E = 1|w). Since ranking sentences based on
p(E=1|w)
p(E = 1|w) is equivalent to ranking them based on p(E=0|w)
, we
will use the latter. According to Bayes rule, the score is
p(E = 1|w)
p(w|E = 1)p(E = 1)
p(w|E = 1)
=
∝
p(E = 0|w)
p(w|E = 0)p(E = 0)
p(w|E = 0)
With these estimates, the scoring function would be:
X p(w|E = 1)
ES(S) =
p(w|E = 0)
w∈S
We call this function Sum of Word Likelihood Ratio (SumWordLR).
The explanatoriness score of a word is now seen as the ratio of the
probability that word w is observed from an explanatory source
(i.e., E = 1) to that it is observed from a non-explanatory source
(i.e., E = 0). We have two sets of parameters that have to be
estimated, i.e., p(w|E = 1) and p(w|E = 0).
In general, our estimate of p(w|E = 1) and p(w|E = 0) would
depend on what kind of words we would regard as explanatory. For
estimating p(w|E = 1), without additional knowledge, a reasonable assumption is that the set of sentences to be summarized O can
be used as an approximate of sample of words that are explanatory,
which is to say that we will use O to approximate the explanatory

3.2 TF-IDF Explanatoriness Scoring
The first method is to adapt an existing ranking function of information retrieval such as BM25 [7], which is one of the most effec-

1070

source. With maximum likelihood estimate, we have
c(w, O)
p(w|E = 1) = p(w|O) =
|O|
With this estimate, a word that is popular in O would tend to have
a higher explanatoriness score, capturing the popularity heuristic.
For estimating p(w|E = 0), we use all the text available to us
about this topic T (denoted by Btopic for background of all text
about the topic). In this case, the maximum likelihood estimator
would give
c(w, Btopic )
p(w|E = 0) =
|Btopic |
Though we did not explore in this paper, these estimates may be
further extended using various smoothing techniques.

4.

Intuitively, it is more interesting to see the performance of a
method on a topic where the percentage of explanatory sentences is
low (i.e., ‘hard’ topics) than that on a topic where most sentences in
O are explanatory (i.e., ‘easy’ topics) since even a random ranking
of sentences may perform well for an easy topic. Thus, when we
take the average of performance on individual topics, it would be
reasonable to place more weight on a hard topic than on an easy
topic. To implement this idea, we propose a general way to quantify the ‘room for improvement’ of a topic based on the gap between the expected performance of a random ranking and the best
performance achieved by an ideal ranking.
Formally, suppose p is a performance measure. Given a topic
Q, let R(Q) be the ranking results for Q. Let prandom (Q) be the
expected performance of a random ranking for Q, and pideal (Q)
be the performance of an ideal ranking for Q. We can measure the
‘room for improvement’ by:
gap(Q) = pideal (Q) − prandom (Q)
(1)
Intuitively, gap(Q) would be large if the performance of a random
ranking is much lower than that of the performance of the ideal
ranking, while if the random ranking has perfect performance (as,
e.g., in the case that all the sentences in O are explanatory), gap(Q)
would be zero.
We can then use gap(Q) as a weight on each topic to combine
the performance on different topics. Formally, the aggregated performance on a set of queries {Q1 , ..., Qm } is given by:
Pm
gap(Qi )p(R(Qi))
i=1
Pm
(2)
W eighted Average(p) =
i=1 gap(Qi )
where R(Qi ) is the ranking results on topic Qi and p(R(Qi )) is
the performance value of R(Qi ) as measured using measure p. It is
easy to see that we put more weight on a query with more room for
improvement, and if the random ranking gives ideal performance,
we would essentially eliminate the topic from the evaluation. We
apply this weighting strategy to M AP and denote it by wM AP
(for weighted M AP ).
The M AP score of an ideal ranking is always 1.0. To compute
the M AP score of a random ranking, we note that the probability that an explanatory sentence would be retrieved at each rank is
equal to the ratio of explanatory sentences in the input sentence set.
Thus, for given topics {Q1 , ..., Qm }, suppose ni is the number of
sentences, and ki is the number of explanatory sentences of topic
Qi , we will have
ki
M AP (Qi )random =
ni
Thus according to Equation 1 and 2, we obtain wM AP as
Pm
gapM AP (Qi )M AP (Qi )
i=1P
wM AP {Q1 , ..., Qm } =
m
i=1 gapM AP (Qi )

EXPERIMENTS

4.1 Data Set
Because there is no data set for evaluation of explanatory opinion extraction, we created two new data sets. The first is based
on a publicly available product review data set used in previous
works [1, 4], which is one of the most popular data sets for opinion mining. The sentences in this data set are already tagged with
sentiment polarities and classified into different aspects, thus we
can take them as the input data for evaluating our algorithm. The
second data set is in a different domain, i.e., hotel. We crawled
the hotel reviews from trip advisor 1 and generated aspect and sentiment labels for each sentence in the data set with three human
labelers. We filtered out disagreed labels.
Our key evaluation questions are whether the proposed methods
can really find explanatory sentences, and which of the two proposed methods is more effective. To answer these questions, we
need to further create gold standard labels about the explanatoriness of all the sentences. To this end, we first clustered sentences
in each data set based on their aspect and sentiment labels. Each
cluster thus consists of a set of opinionated sentences (i.e., O) corresponding to a unique (T, A, P ). We discarded clusters with fewer
than 10 sentences because such a small cluster does not really need
summarization. We then provided the topic label of each cluster,
(T, A, P ), to two human labelers and asked them to judge whether
each sentence is explanatory or not in each cluster. A sentence is
explanatory if it helps explain why the reviewer holds a particular
polarity (P ) of opinions about aspect A of topic T . Human labelers
generated a binary label for each sentence in this way. The detailed
labeling instructions, labeling examples provided to labelers, and
the generated labels for the product review data set are available at
a public website 2 .
The generated test data sets include totally 89 topic clusters with
3799 sentences. The label agreement, defined as the ratio of number of sentences having the same label to the total number of sentences, is 0.69. To ensure reliable evaluation, we discarded sentences which have no agreed labels, as well as topics that have no
explanatory sentences. The input data set is pre-processed by a basic stemmer to alleviate the problem of data sparseness.

gapM AP (Qi ) = 1.0 − M AP (Qi )random

4.3 Baselines
We hypothesize that the proposed sentence ranking methods would
work better than that of standard text summarization method which
mostly implements the popularity heuristic. As a baseline to test
this hypothesis, we use LexRank [2] sentence ranking method, which
is popularly used in a general summarization algorithm and can
generate ranking score of sentences.

4.2 Measure: Weighted MAP
Our task is ranking sentences based on the predicted explanatoriness of a sentence. Thus we can use the standard information
retrieval measure, Mean Average Precision (M AP ), to quantitatively evaluate the ranking accuracy.
1
2

4.4 Results
In Table 1 we compare the wMAP values of the LexRank baseline and the proposed methods on both data sets. LexRank and
SumWordLR do not have any parameter to tune, so their performance figures are on all the test topics in the two data sets. BM 25E

http://www.tripadvisor.com
http://sifaka.cs.uiuc.edu/~hkim277/expSum

1071

has two parameters, and we used two-fold cross validation to set the
parameters and evaluate its performance. The wMAP values shown
in the table are the average over all the test instances in the two data
sets, respectively.
As a reference point, the wM AP score of random ranking is
0.4494 for the product data set, and 0.4764 for the hotel data set.
It is clear that all methods perform better than the random ranking
as we should expect. We further observe that all our methods outperform the LexRank baseline. This confirms our hypothesis that
a combination of both representativeness and discriminativeness in
the proposed methods is more effective than mostly relying on the
representativeness (i.e., popularity) as in the case of LexRank. Ttest shows that the improvement of our methods over LexRank is
statistically significant with 95% confidence level (marked with a
∗
). For the hotel data set, although the performance values of our
methods are higher than those from LexRank, the difference is not
statistically significant. We suspect that this is because hotel data
set has only 23 topics, not big enough to show statistically significant differences. Another possible explanation is that the centralitybased method LexRank suffers less when there are more sentences
in one input sentence set where the redundancy may have helped
the popularity heuristic to pick up explanatory sentences.

head-phones as well as external speakers”. On the other hand, in
the baseline summary, despite of redundancy removal, we can see
many repetitions of “good sound” because it is popular in the input data. In general, the baseline summary seems to have many
generic sentiment words, which are not informative, and useless
for explaining specific reasons why a reviewer has held a particular
opinion.

5. CONCLUSIONS
In this paper, we introduced a novel sentence ranking problem
called explanatory sentence extraction (ESE), which can extend
the capabilities of the current opinion summarization methods. We
proposed two general methods, modified TF-IDF weighting and
the probabilistic model for scoring explanatoriness. Experiment
results showed that proposed methods are effective in ranking sentences by explanatoriness outperforming a state of the art sentence
ranking method for a standard text summarization method.
Our work is the first step toward studying the new problem of
measuring explanatoriness. With the two data sets and appropriate
measures, we have paved the way for further studying this problem. As a future work, we can further explore different ways of
estimating the proposed probabilistic models, especially through
exploiting pseudo feedback to refine the estimate of the models.
In the current paper, we chose to focus on language-independent
methods, which have the advantage of being applicable to any language without requiring any manual effort. For the future work, we
can further improve these methods by doing deeper linguistic and
semantic analysis on characteristics of explanatory sentences.

Table 1: Comparison of scoring methods in wMap.
S CORING M ETHOD
LexRank
BM25E
SumWordLR

P RODUCT
0.4612
0.7498∗
0.7730∗

H OTEL
0.5869
0.6060
0.6143

Table 2: Sample summaries (T:MP3player1, A:sound, P:+).

6. ACKNOWLEDGMENTS

E XPLANATORY S UMMARY (S UM W ORD LR)
when i played back a symphony orchestra you would swear that you
were seated dead center in front of the orchestra - it ’s that good ! 1 )
great sound ( > 98db signal-to-noise ratio beats ipod ’s " unspecified
" ratio ) and good power output allow the zen xtra to drive large
head-phones as well as external speakers . once again , the sound is
awesome , the batterly life is only 6-8 hours , and that is because all
my music is 320 kbps which does affect the battery life . the sound is
awesome ,
B ASELINE S UMMARY (L EX R ANK )
the sound from the player is ok . - best in class sound the sound is
excellent as one would suspect from a creative product . the sound is
great even with the supplied earbuds – but i find earbuds uncomfortable so i use different headphones . plusses are the easy to remove
battery and the terrific sound produced by the nomad . on the positive
side , the sound of the player is pretty good , once you have everything configured . the sound is great , and the volume is more than
satisfactory for

Thanks to human labelers for data set labeling in our experiments. This material is based upon work supported in part by the
National Science Foundation under Grant Number CNS-1027965
and by an HP Innovation Research Award.

7. REFERENCES
[1] X. Ding, B. Liu, and P. S. Yu. A holistic lexicon-based approach to
opinion mining. In WSDM ’08: Proceedings of the 1st international
conference on Web search and web data mining, pages 231–240, New
York, NY, USA, 2008. ACM.
[2] G. Erkan and D. R. Radev. Lexrank: graph-based lexical centrality as
salience in text summarization. J. Artif. Int. Res., 22(1):457–479, 2004.
[3] E. Hovy and C.-Y. Lin. Automated text summarization in
SUMMARIST. In I. Mani and M. T. Maybury, editors, Advances in
Automatic Text Summarization. MIT Press, 1999.
[4] M. Hu and B. Liu. Mining and summarizing customer reviews. In
KDD ’04: Proceedings of the 10th international conference on
Knowledge discovery and data mining, pages 168–177, New York,
NY, USA, 2004. ACM.
[5] M. Hu and B. Liu. Mining opinion features in customer reviews. In
AAAI’04: Proceedings of the 19th national conference on Artifical
intelligence, pages 755–760. AAAI Press, 2004.
[6] M. Hu and B. Liu. Opinion extraction and summarization on the web.
In AAAI’06: proceedings of the 21st national conference on Artificial
intelligence, pages 1621–1624. AAAI Press, 2006.
[7] K. S. Jones, S. Walker, and S. E. Robertson. A probabilistic model of
information retrieval: development and comparative experiments. Inf.
Process. Manage., 36(6):779–808, Nov. 2000.
[8] J. Kupiec, J. Pedersen, and F. Chen. A trainable document
summarizer. In SIGIR ’95: Proceedings of the 18th annual
international ACM SIGIR conference on Research and development in
information retrieval, pages 68–73, New York, NY, USA, 1995. ACM.
[9] C. D. Paice. Constructing literature abstracts by computer: techniques
and prospects. Inf. Process. Manage., 26(1):171–186, 1990.

In Table 2, we show an example of explanatory summary generated by a proposed method and a baseline summary generated
by LexRank about ‘positive opinion about MP3 player sound’. We
generated a summary by picking sentences one by one from the top
of the ranked list generated by the explanatory ranking algorithm
until the length limit (500 characters) is reached. To simulate actual
usage of the scoring function in summarization setup, we also apply simple redundancy removal technique. That is, when we pick
sentences from the top of the explanatoriness ordered list, if more
than 50% of the content of the next candidate sentence is covered
by already selected sentences, we would skip, and do not include
the candidate sentence in the summary.
We can easily see the benefits of the explanatory summary over
the general summary from the results in Table 2. Specifically,
the explanatory summary shows useful details about ‘sound’ of
the target product, such as “seated dead center in front of the orchestra”, “> 98db signal-to-noise ratio”, “beats ipod”, “allow large

1072

