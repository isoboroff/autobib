SIGIR 2013 Workshop on Modeling User Behavior for
Information Retrieval Evaluation
Charles L. A. Clarke1 , Luanne Freund2 , Mark D. Smucker3 , and Emine Yilmaz4
1
2

School of Computer Science, University of Waterloo, Canada

School of Library, Archival and Information Studies, University of British Columbia, Canada
3

Department of Management Sciences, University of Waterloo, Canada
4

Department of Computer Science, University College London, UK

ABSTRACT

into Cranﬁeld-style eﬀectiveness measures or to understand
existing metrics in terms of user models. At the same time,
another body of research has focused on the simulation of
user interaction with IR user interfaces to predict the value
of new user interfaces to retrieval systems. Researchers have
also used simulation of user behavior to better understand
information retrieval. Space limitations prevent us from citing all applicable work here; for a good starting point to
exploring the relevant literature, please see the references in
the following recent papers: [1, 2, 3].
These varied approaches to the evaluation of IR systems
all rely on the quality of their user models. The better the
user models, the better the predictions made by the evaluation measures.
While there is some overlap between the researchers pursuing these diﬀerent approaches, we believe that it makes
sense to bring these groups together to enable fruitful crosspollination of ideas and methods. This workshop brings together people interested in discussing new approaches and
new ways to collaborate and create shared resources for the
development and application of user models for information
retrieval evaluation.
The workshop solicited two-page poster papers, which were
reviewed by a program committee. Accepted papers will be
presented during boaster and poster sessions. In addition
to the accepted papers, the tentative schedule for the workshop includes invited speakers, breakout groups, and plenty
of time for discussion and presentation of breakout group
reports.

The SIGIR 2013 Workshop on Modeling User Behavior for
Information Retrieval Evaluation (MUBE 2013) brings together people to discuss existing and new approaches, ways
to collaborate, and other ideas and issues involved in improving information retrieval evaluation through the modeling of
user behavior.
Categories and Subject Descriptors: H.3.4 [Information Storage and Retrieval]: Systems and Software — Performance evaluation (eﬃciency and eﬀectiveness)
Keywords: Information retrieval, search evaluation, user
models

1. OVERVIEW
Information retrieval (IR) evaluation is concerned with
producing accurate estimates of the performance of retrieval
systems. Excluding in-situ studies of user behavior, IR evaluation is conducted via simulation of the usage of a retrieval
system. Oﬀering the best ﬁdelity to reality, laboratory-based
user studies provide simulated search tasks to users and then
measure outcomes, such as user satisfaction or success. Unfortunately, user studies can be costly and diﬃcult to scale
to the large number of evaluations required during the development of new retrieval systems. Cranﬁeld-style evaluation
retains the simulated search tasks of laboratory-based user
studies, but replaces the user with a model of user behavior
and recorded user relevance judgments. After the initial cost
of obtaining the relevance judgments, Cranﬁeld-style evaluation has a very low cost and scales easily. The quality of the
eﬀectiveness estimates produced by Cranﬁeld-style evaluation is limited by the user model. The better the user model,
the closer Cranﬁeld-style evaluation is to the standard of the
laboratory-based user study.
During the past decade, there has been an increasing body
of research on ways to incorporate models of user behavior

Acknowledgments
This work was supported in part by the Natural Sciences
and Engineering Research Council of Canada (NSERC), in
part by GRAND NCE, and in part by the University of
Waterloo.

References
[1] L. Azzopardi, K. Järvelin, J. Kamps, and M. D. Smucker.
Report on the SIGIR 2010 workshop on the simulation of
interaction. SIGIR Forum, 44:35–47, January 2011.

Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses, contact
the owner/author(s).
SIGIR’13, July 28–August 1, 2013, Dublin, Ireland.
ACM 978-1-4503-2034-4/13/07.

[2] B. Carterette, E. Kanoulas, and E. Yilmaz. Simulating simple user behavior for system eﬀectiveness evaluation. In
CIKM’11, ACM, 2011.
[3] M. D. Smucker and C. L. A. Clarke. Time-based calibration
of eﬀectiveness measures. In SIGIR’12, ACM, 2012.

1134

