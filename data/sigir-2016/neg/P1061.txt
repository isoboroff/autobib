Utilizing Focused Relevance Feedback
Elinor Brondwine
elinor@tx.technion.ac.il

Anna Shtok
Oren Kurland
annabel@tx.technion.ac.il kurland@ie.technion.ac.il

Faculty of Industrial Engineering and Management
Technion, Israel

ABSTRACT

using information induced from relevant passages and nonrelevant documents.
Our main novel contribution is the development of methods that (i) use relevance feedback at both document and
passage levels, and/or (ii) utilize non-relevant passages.

We present a novel study of ad hoc retrieval methods utilizing document-level relevance feedback and/or focused relevance feedback; namely, passages marked as (non-)relevant.
The first method uses a novel mixture model that integrates
relevant and non-relevant information at the language model
level. The second method fuses retrieval scores produced by
using relevant and non-relevant information separately. Empirical exploration attests to the merits of our methods, and
sheds light on the effectiveness of using and integrating relevance feedback for textual units of varying granularities.

2.

Keywords: focused relevance feedback

1.

RELATED WORK

Methods using information induced from both relevant
and non-relevant documents emphasize terms that appear
in the former and downplay the importance of those appearing in the latter (e.g., [6, 12, 13, 16]). A similar principle,
although implemented using different techniques, is applied
in our methods that utilize also (non-)relevant passages.
Findings about the merits of using information induced
from non-relevant documents have largely been inconclusive
[6, 5, 13]. Notable exceptions are work on addressing very
difficult queries [16, 7] and on the document routing task
[15]. In contrast to our work, information induced from
(non-)relevant passages in relevant documents was not used.
Shen and Zhai [14] showed the merits of using relevant
(but not non-relevant) passages in the mixture model [17].
We extend the mixture model by using both relevant and
non-relevant feedback units (documents and/or passages).
Automatically identifying (non-)effective passages in (non)relevant documents and using these for retrieval has shown
no merit [11, 9]. In contrast, our methods utilize true relevance feedback for passages and documents.
A mixture model, different than ours, was used to induce
a query model from pseudo relevant documents using nonrelevant documents [10]. In contrast to our work, relevant
documents and passage-level feedback were not used.

INTRODUCTION

Most previous work on using relevance feedback for ad
hoc (query-based) document retrieval has focused on utilizing feedback provided at the document level. Utilizing information induced from relevant documents can significantly
improve retrieval effectiveness [12, 13]. The effective utilization of non-relevant documents, on the other hand, has been
demonstrated mainly for very difficult queries [16, 7].
Relevant documents can also contain non-relevant information. Thus, utilizing focused relevance feedback, that is,
feedback for passages in relevant documents, can be of merit.
For example, using information induced from relevant passages can improve retrieval effectiveness [14].
We present a study of methods that utilize positive and/or
negative relevance feedback for documents and/or for passages. Our first method uses a novel mixture model that
integrates, at the language model level, information induced
from relevant and non-relevant units (documents and/or passages). Our second method fuses retrieval scores attained by
using, separately, relevant and non-relevant units.
Empirical evaluation sheds light on the effectiveness of
using information induced from relevant and non-relevant
units of different granularities and their integration. For
example, the best performance of our methods was attained

3.

RETRIEVAL FRAMEWORK

Let Dinit denote an initial list of documents retrieved from
corpus C in response to query q by some retrieval method.
Suppose that relevance feedback is provided for documents
in DF (⊂ Dinit ); specifically, Rd and N Rd are the sets of
relevant and non-relevant documents in DF , respectively.
Relevant documents can also contain non relevant information. Accordingly, we further assume that focused relevance feedback is provided for relevant documents. Namely,
non-overlapping variable length passages of documents in
Rd are marked as relevant to q; unmarked passages are considered non-relevant. We concatenate all relevant and nonrelevant passages in each relevant document into a single
relevant pseudo passage and a single non-relevant pseudo
passage, respectively; the order of concatenation has no effect since we use unigram language models that assume term

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

SIGIR ’16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2914695

1061

independence. Rp and N Rp are the sets of pseudo relevant
and non-relevant passages in documents in Rd , henceforth
simply referred to as relevant and non-relevant passages, respectively. Herein, R refers to the set of either relevant documents Rd or relevant passages Rp ; N R is the set of nonrelevant documents (N Rd ) or non-relevant passages (N Rp ).
Below we present two retrieval methods, based on unigram language models, that utilize the relevance feedback.
The maximum likelihood estimate (MLE) of term
w with reP
def

LE
spect to a set S of texts is pM
(w) =
S

P

x∈S

in R (∈ {Rd , Rp }) using some approach. Then, the document corpus is ranked using −CE(pr (·)||pDir
(·)). Second,
d
the top n documents are re-ranked by the similarity of their
language models with pr (·) and dissimilarity from the language models induced from non-relevant units in N R. Formally, documents d are ranked in descending order of the
following score-based fusion:
LE
−αCE(pr (·)||pDir
(·)) + (1 − α) min CE(pM
(·)||pDir
(·));
d
x
d
x∈N R

c(w,x)
;
′
w′ ∈x c(w ,x)

(4)
α is a free parameter. As in the distillation model, for query
def
LE
term w and non-relevant unit x (∈ N R): pM
(w) = 0 and
x
1
the probabilities are re-normalized .
Various methods can be used to induce pr (·) from a set
of relevant units, R (∈ {Rd , Rp }). We use the standard
mixture model [17] which is a special case of our distillation model from Equation 3 when setting λ1 = 0 in Equation 2.2 Equation 4 is then instantiated using a choice of
N R (∈ {N Rd , N Rp }). The four resultant score-based fusion methods are denoted SF(R,NR).

x∈S
P

c(w, x) is the count of w in text x. We use pDir
x (w) to denote
the probability assigned to term w by a Dirichlet smoothed
unigram language model induced from text x [17]. The similarity between two language models, py (·) and pz (·), is measured using cross entropy, where higher values correspond to
decreased similarity:
X
CE(py (·) || pz (·)) = −
py (w) log pz (w).
(1)
w

3.1

Distillation model

The goal of our first method is to “distill” the aspects most
relevant to the information need from the feedback. For
example, the premise of the mixture model [17] is that terms
in relevant documents are generated either by a relevance
topic language model or by the corpus language model. We
generalize the mixture model to utilize both relevant and
non-relevant feedback units (documents and/or passages).
We assume that terms in units in R (i.e., all relevant documents or all relevant passages) are generated by a mixture
of (i) the relevance topic model, prel (·), which we want to
LE
estimate; (ii) the corpus language model, pM
(·), which is
C
assumed to represent a general non-relevant document; and,
LE
(iii) a query-specific irrelevance topic model, pM
N R (·), induced from the non-relevant units (N R). Following work on
using negative relevance feedback, if w is a query term we
def
LE
set pM
N R (w) = 0 and re-normalize the probabilities [16].
We estimate prel (·) by using the EM algorithm to maximize the log likelihood of units in R:
XX
`
c(w, x) log (1 − λ1 − λ2 )prel (w)+

4.

x∈R w

´
LE
M LE
λ1 pM
(w) ; (2)
N R (w)+λ2 pC
λ1 and λ2 are free parameters. As is common in work on
query language models [17, 1], we interpolate prel (·) with
the original query model:
def

LE
pdistill (w) = λq pM
(w) + (1 − λq )prel (w);
q

(3)

λq is a free parameter. We then rank the documents in the
corpus using −CE(pdistill (·)||pDir
(·)).
d
We instantiate Equation 2 using R (∈ {Rd , Rp }) and N R
(∈ {N Rd , N Rp }). The resultant four models attained from
Equation 3 are denoted Distill(R,NR).

3.2

EXPERIMENTAL SETUP

For experiments we used the INEX corpus3 which contains 2,666,190 Wikipedia articles. We used the 120 queries
from the ad hoc tracks of 2009 and 2010 for which binary
document-level and (arbitrary-length) passage-level relevance
judgments are available; unmarked text in relevant documents is considered non-relevant [2]. The average number
of relevant documents per query is 86. The average percentage of relevant text in a relevant document is 41.5%;
i.e., most text in relevant documents does not pertain to the
query. We re-visit this important point below.
Krovetz stemming was applied to documents and queries
and stopwords on the INQUERY list were removed. Indri
5.3 (http://www.lemurproject.org/indri) was used for experiments. The initial ranking from which Dinit is derived is
induced using standard language-model-based retrieval [17]:
LE
document d is scored by −CE(pM
(·)||pDir
(·)) (see Equaq
d
tion 1). The Dirichlet smoothing parameter in document
language models, µ, was set to 1000 in all methods [18].
The document feedback set, DF , contains 2k documents:
the k highest ranked relevant documents (Rd ) and the k
highest ranked non-relevant documents (N Rd ) in Dinit ; k ∈
{1, 2, . . . , 5}; each value entails an experimental setting. The
goal was to ameliorate across-query effects that are due to
varying numbers of (non-)relevant documents at top ranks.
Mean average precision at cutoff 1000 (MAP) serves as
the retrieval evaluation measure. Two evaluation paradigms
were employed: standard (regular) and residual collection. In the residual paradigm [3], all documents in DF
1
Equation 4 is conceptually reminiscent of the MultiNeg
method from [16] that utilizes the query and non-relevant
documents for re-ranking. Experiments — numbers are
omitted due to space considerations — reveal the following.
The approach in Equation 4 yields better performance in our
setting when using min rather than average. The approach
LE
is also superior to using a single model, pM
N R (·), induced
from the non-relevant units; cf., the SingleNeg method [16].
2
We found that using relevance model #3 (RM3) [8, 1] results in similar conclusions to those we present below. Actual results are omitted due to space considerations.
3
http://www.mpi-inf.mpg.de/departments/
databases-and-information-systems/software/inex

Score-based fusion

The second retrieval model is based on the principle that
documents similar to the relevant units and dissimilar from
the non-relevant units should be rewarded. Specifically, we
apply a two-step approach inspired by work on using only
the query and non-relevant documents [16]. First, a relevance topic model, pr (·), is induced from the relevant units

1062

were removed from result lists and relevance judgments files;
MAP is measured on result lists of 1000 documents. Statistically significant performance differences are determined
using the paired two-tailed t-test with p < 0.05.
Free-parameter values are set using leave-one-out crossvalidation performed over queries per experimental setting;
MAP is the optimization measure. The value ranges are
as follows: λq (Equation 3) is in {0.2, 0.5, 0.8}; λ1 and λ2
(Equation 2) are in {0, 0.1, 0.5, 0.9}; λ1 + λ2 < 1; α (Equation 4) is in {0, 0.2, . . . , 1}; the number of documents reranked in the score-based fusion method, n, is set to 1000.
As is common [17, 1], language models induced using relevance feedback are clipped to ν (∈ {10, 25, 50}) terms.
As noted, the standard mixture model [17], henceforth
MM, is a special instance of our distillation model when
setting λ1 = 0 in Equation 2 (i.e., non-relevant units are
not used) and applying Equation 3. MM is also used to induce the relevance topic model, pr (·), used in Equation 4.
Hence, we use MM(Rd ) which utilizes the relevant documents (Rd ) and MM(Rp ) (also used in [14]) which utilizes
relevant passages (Rp ) as reference comparisons. The EM
algorithm used in the mixture and distillation models converged in 13-14 iterations.

5.

much non-query-pertaining text (see Section 4). However,
using the relevant passages alone, MM(Rp ), is more effective
than using relevant documents and non-relevant passages,
Distill(Rd ,N Rp ), and is as effective as using non-relevant
passages in addition to relevant passages, Distill(Rp ,N Rp ).
That is, using non-relevant passages to distill a relevance
topic model from either relevant documents or relevant passages has no merit over using only the relevant passages.
Distill(Rp ,N Rd ) is the most effective distillation model
in a vast majority of cases; most improvements over other
distillation models and the mixture models are statistically
significant for both evaluation paradigms. Thus, in contrast
to non-relevant passages in relevant documents, non-relevant
documents can be effectively used to distill a relevance topic
model from relevant passages4 .

The score-based fusion model. The performance of the
score-based fusion model (Equation 4) is presented in Figures 1(b) and 1(d). The curves of SF(R,N Rp ) (almost) coincide with the curves of MM(R) regardless of the choice of R.
This means that using the similarity of a document to nonrelevant passages has little merit. In contrast, SF(R,N Rd )
outperforms MM(R), regardless of the choice of R. The
improvements are statistically significant for all 10 cases (5
values of k × 2 choices of R) for the standard evaluation,
and in 4 out of 10 cases for the residual evaluation.
Overall, the most effective score-based fusion model for
both evaluation paradigms is SF(Rp ,N Rd ). The improvements it posts over the other methods (specifically, MM)
are statistically significant in a vast majority of the cases5 .

EXPERIMENTAL RESULTS

Figure 1 depicts the performance results. We see that
the more feedback documents are used (i.e., higher k), the
more effective the retrieval. Specifically, for the residual
evaluation paradigm (Figures 1(c) and 1(d)), where MAP
values decrease with increasing k due to removing the given
relevant documents from all rankings [4], the relative performance improvements of the feedback-based methods with
respect to the initial ranking increase as a function of k.

6.

SUMMARY

Our distillation and score-based fusion methods use relevance feedback for documents and passages in different
ways. The distillation model utilizes both relevant and nonrelevant units in a mixture model to rank the entire corpus.
The score-based fusion model re-ranks a list retrieved using
information induced only from relevant units by using, in
addition, dissimilarities with non-relevant units.
Despite these differences, the conclusions regarding the
merits of using the different types of (non-)relevant units
are similar in most cases. That is, using relevant passages is
superior to using relevant documents regardless of the nonrelevant units used. Yet, using non-relevant documents in
addition to relevant passages is of much merit and results in
the best performance for both methods. A noticeable difference between the two methods is the effectiveness of using
non-relevant passages in addition to relevant documents in
the distillation model. No such merits were observed for the
score-based fusion method.
Acknowledgments. We thank the reviewers for their comments. This paper is based upon work supported in part by
the German Research Foundation (DFG) via the GermanIsraeli Project Cooperation (DIP, grant DA 1600/1-1), the
Israel Science Foundation under grant no. 433/12, and the
Technion-Microsoft Electronic Commerce Research Center.

Relevant units. Figure 1 shows that in all cases, using relevant passages (R = Rp ) yields better performance than
using relevant documents (R = Rd ): compare a solid curve
with white markers (R = Rp ) to a dotted curve with gray
markers of the same type (R = Rd ). This finding can be attributed to the fact that relevant documents contain much
non-relevant information as mentioned in Section 4.

The distillation model. Figures 1(a) and 1(c) show that in
comparison to MM(R), which does not utilize non-relevant
units, and regardless of the choice of R, using also nonrelevant documents in our distillation model improves retrieval effectiveness in a vast majority of cases. For the
standard evaluation, the distillation method yields improvements in 9 out of 10 cases (Distill(Rd ,N Rd ) vs. MM(Rd )
and Distill(Rp ,N Rd ) vs. MM(Rp ) over 5 values of k); in 7
cases the improvements are statistically significant. For the
residual evaluation, effectiveness is improved in 7 out of 10
cases with 4 improvements being statistically significant.
The effectiveness of using N R = N Rp depends on the
relevant units used. Distill(Rd ,N Rp ) outperforms MM(Rd )
in all cases (often statistically significantly) for both evaluation paradigms. Moreover, Distill(Rd ,N Rp ) outperforms
Distill(Rd ,N Rd ) in all cases for the residual evaluation, although few improvements are statistically significant. The
merits of using non-relevant passages (N R = N Rp ) to distill a relevance topic model from relevant documents can
be attributed to the fact that relevant documents contain

4
Extending the distillation model to use both N Rp and N Rd
showed merit over using each alone when relevant documents
are used but not when relevant passages are used.
5
We found that a score-based fusion method that extends
Equation 4 by using both non-relevant documents and nonrelevant passages does not statistically significantly outperform SF(Rd ,N Rd ) and SF(Rp ,N Rd ).

1063

(a) Distillation models (standard evaluation)

(b) Score-based fusion models (standard evaluation)

0.534
0.506

0.520

0.500

0.500
0.480

MAP

MAP

0.480
0.460

0.460
0.440
0.440

Distill(Rd,NRd)

0.420
0.416

Distill(Rp,NRp)
MM(Rd)

Distill(Rd,NRp)
3

2

4

MM(Rp)

SF(Rp,NRd)
5

1

3

2

4

5

k

k

(c) Distillation models (residual evaluation)

(d) Score-based fusion models (residual evaluation)

0.440

0.440

0.429
0.420

0.426
0.420

0.400

0.400

0.380

0.380

MAP

MAP

SF(Rp,NRp)
MM(Rd)

SF(Rd,NRp)

MM(Rp)

Distill(Rp,NRd)
1

SF(Rd,NRd)

0.420
0.417

0.360

0.340

0.360

0.340

0.320

0.300

Distill(Rd,NRd)

MM(Rd)

0.320

Distill(Rd,NRp)

MM(Rp)
Dinit

0.300

Distill(Rp,NRd)
Distill(Rp,NRp)

0.286
1

2

4

5

MM(Rd)

SF(Rd,NRp)

MM(Rp)
Dinit

SF(Rp,NRd)
SF(Rp,NRp)

0.286
3

1

k

SF(Rd,NRd)

2

3

4

5

k

Figure 1: MAP as a function of k. The performance of MM(R) is presented for reference. The MAP of the
initial result list (Dinit ) in the standard evaluation paradigm, which does not depend on k, is 0.368; the MAP
in the residual evaluation is displayed. The color of the markers (white or gray) and curve style indicate the
relevant units used (Rd or Rp ). The type of the markers indicates the non-relevant units (N Rd or N Rp ).

7.

REFERENCES

[10] Y. H. Peng Zhang and D. Song. Approximating true relevance
distribution from a mixture model based on irrelevance data. In
Proc. of SIGIR, pages 107–114, 2009.
[11] S. E. Robertson, H. Zaragoza, and M. J. Taylor. Microsoft
cambridge at TREC-12: HARD track. In Proc. of TREC,
pages 418–425, 2003.
[12] J. J. Rocchio. Relevance feedback in information retrieval. In
G. Salton, editor, The SMART Retrieval System:
Experiments in Automatic Document Processing, pages
313–323. Prentice Hall, 1971.
[13] I. Ruthven and M. Lalmas. A survey on the use of relevance
feedback for information access systems. Knowledge
Engineering Review, 18(2):95–145, 2003.
[14] X. Shen and C. Zhai. Active feedback-UIUC TREC-2003
HARD experiments. In Proc. of TREC, pages 662–666, 2003.
[15] A. Singhal, M. Mitra, and C. Buckley. Learning routing queries
in a query zone. In Proc. of SIGIR, pages 25–32, 1997.
[16] X. Wang, H. Fang, and C. Zhai. A study of methods for
negative relevance feedback. In Proc. of SIGIR, pages 219–226,
2008.
[17] C. Zhai and J. D. Lafferty. Model-based feedback in the
language modeling approach to information retrieval. In Proc.
of CIKM, pages 403–410, 2001.
[18] C. Zhai and J. D. Lafferty. A study of smoothing methods for
language models applied to ad hoc information retrieval. In
Proc. of SIGIR, pages 334–342, 2001.

[1] N. Abdul-Jaleel, J. Allan, W. B. Croft, F. Diaz, L. Larkey,
X. Li, M. D. Smucker, and C. Wade. UMASS at TREC 2004 —
novelty and hard. In Proc. of TREC-13, 2004.
[2] P. Arvola, S. Geva, J. Kamps, R. Schenkel, A. Trotman, and
J. Vainio. Overview of the inex 2010 ad hoc track. In
Comparative Evaluation of Focused Retrieval - INEX, pages
1–32. 2010.
[3] C. Buckley and S. Robertson. Relevance feedback track
overview: TREC 2008. In Proc. of TREC-17, 2008.
[4] C. Cirillo, Y. K. Chang, and J. Razon. Evaluation of feedback
retrieval using modified freezing, residual collection and test
and control groups. The SMART retrieval system-experiments
in automatic document processing, pages 355–370, 1971.
[5] M. D. Dunlop. The effect of accessing nonmatching documents
on relevance feedback. ACM Transactions on Information
Systems, 15(2):137–153, 1997.
[6] E. Ide. New experiments in relevance feedback. The SMART
retrieval system, pages 337–354, 1971.
[7] M. Karimzadehgan and C. Zhai. Improving retrieval accuracy
of difficult queries through generalizing negative document
language models. In Proc. of SIGIR, pages 27–36, 2011.
[8] V. Lavrenko and W. B. Croft. Relevance-based language
models. In Proc. of SIGIR, pages 120–127, 2001.
[9] Y. Li, X. Tao, A. Algarni, and S. Wu. Mining specific and
general features in both positive and negative relevance
feedback. In Proc. of TREC, 2009.

1064

