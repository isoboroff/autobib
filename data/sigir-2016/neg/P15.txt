Bayesian Performance Comparison of Text Classiﬁers
Dell Zhang

DCSIS
Birkbeck, University of London
Malet Street
London WC1E 7HX, UK

Jun Wang

Dept of Computing Science
University College London
Gower Street
London WC1E 6BT, UK

Emine Yilmaz

Dept of Computing Science
University College London
Gower Street
London WC1E 6BT, UK

dell.z@ieee.org
j.wang@cs.ucl.ac.uk
e.yilmaz@cs.ucl.ac.uk
Xiaoling Wang
Yuxin Zhou
Software Engineering Institute
East China Normal University
3663 North Zhongshan Road
Shanghai 200062, China

Software Engineering Institute
East China Normal University
3663 North Zhongshan Road
Shanghai 200062, China

xlwang@sei.ecnu.edu.cn

10122510216@ecnu.cn

ABSTRACT

tiﬁcation, genre detection, authorship attribution, and so on.
In fact, most modern IR systems for search, recommendation, or advertising contain multiple components that use
some form of text classiﬁcation.
How can we know whether one classiﬁer is really better
than the other? Is it possible that they perform equally
well? Sure we should be able to evaluate their classiﬁcation
performances on some benchmark datasets using some performance measures. However, given any ﬁnite amount of test
results, we can never be completely certain that one classiﬁer works better than the other or vice versa: the observed
diﬀerence between their performance scores do not necessarily reﬂect their intrinsic qualities. The central question
here is how to reliably tell if classiﬁer A indeed outperforms
classiﬁer B, given a set of test results. Perhaps the simplest
solution is to apply k-fold cross-validation [21] and then calculate the sample variance of performance scores over multiple “folds” of the dataset. This method tends to yield poor
estimations though: the sample variance can approximate
the true variance well only if we have a large number of
folds, but when the dataset is divided into many folds, the
size of each fold is likely to be too small to give a meaningful performance score (especially for complex multivariate
performance measures like F1 [26]). Hence it is desirable to
derive the uncertainty of performance scores directly from
all the atomic document-category classiﬁcation results.
To address this problem, Yang and Liu deﬁned in their
seminal SIGIR-1999 paper [27] a suite of null-hypothesis signiﬁcance testing (NHST) methods which aim to verify how
strongly the experimental results support the claim that one
particular classiﬁer is more accurate than another classiﬁer.
That paper has been inﬂuential within and beyond the realm
of text classiﬁcation. Since its publication, it has received
about 3,000 citations (according to Google Scholar). Today,
it is almost compulsory for researchers to validate the superiority of their proposed text classiﬁcation algorithms by
means of NHST and report the p-values in their papers.
Although NHST has proven to be useful in assessing text
classiﬁers and its adoption has greatly improved the rigour
of performance evaluation in IR, such a frequentist approach
has many inherent deﬁciencies and limitations which we
shall elaborate on later. In this paper, we propose a novel
approach to performance comparison of text classiﬁers based

How can we know whether one classiﬁer is really better than
the other? In the area of text classiﬁcation, since the publication of Yang and Liu’s seminal SIGIR-1999 paper, it has
become a standard practice for researchers to apply nullhypothesis signiﬁcance testing (NHST) on their experimental results in order to establish the superiority of a classiﬁer.
However, such a frequentist approach has a number of inherent deﬁciencies and limitations, e.g., the inability to accept
the null hypothesis (that the two classiﬁers perform equally
well), the diﬃculty to compare commonly-used multivariate
performance measures like F1 scores instead of accuracy, and
so on. In this paper, we propose a novel Bayesian approach
to the performance comparison of text classiﬁers, and argue its advantages over the traditional frequentist approach
based on t-test etc. In contrast to the existing probabilistic
model for F1 scores which is unpaired, our proposed model
takes the correlation between classiﬁers into account and
thus achieves greater statistical power. Using several typical
text classiﬁcation algorithms and a benchmark dataset, we
demonstrate that the our approach provides rich information
about the diﬀerence between two classiﬁers’ performances.

Keywords
Bayesian inference; hypothesis testing; performance evaluation; text classiﬁcation

1.

INTRODUCTION

Text classiﬁcation (aka categorisation) [25] is a fundamental technique in information retrieval (IR) [19]. It has many
important applications, including topic categorisation, spam
ﬁltering, sentiment analysis, message routing, language idenPermission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.

SIGIR ’16, July 17–21, 2016, Pisa, Italy
c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.

ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2911547

15

on Bayesian estimation [15], and argue its advantages over
the traditional frequentist approach based on t-test etc. Using a few representative text classiﬁcation algorithms and a
benchmark dataset, we demonstrate that the our approach
provides rich information about the diﬀerence between two
classiﬁers’ performances.

The other NHST methods that have been applied to compare classiﬁers include ANOVA test [10], Friedman test [24],
McNemar’s test [6], and Wilcoxon signed ranks test [4]. Due
to their frequentist nature, no matter which speciﬁc test they
use, more or less they suﬀer from the above mentioned perils
(especially the ﬁrst three).

2.

2.2

2.1

RELATED WORK

Bayesian Performance Comparison

It has been loudly advocated in recent years that the
Bayesian approach to comparing two groups of data has
many advantages over the frequentist NHST [14, 15]. However, to our knowledge, almost all the existing models of
Bayesian performance comparison deal with continuous values (that can be described by Gaussian or t distributions)
but not discrete classiﬁcation outcomes, and they produce
estimations for simple statistics (such as the average diﬀerence between the two given groups) but not complex performance measures (such as the F1 score).
Probably the most closely related work is that of Goutte
and Gaussier [8]. Their F1 score model constructed using a
couple of Gamma variates is not as expressive and ﬂexible
as ours. For example, generalising their model to the Fβ
measure (β ≥ 0) [19, 26] with β = 1 would end up with a
complex equation involving three Gamma variates, but that
would be trivial in our approach. It seems that their model
is restricted to a single F1 score for binary classiﬁcation with
two classes only, due to its reliance upon the special properties of the Gamma distribution. In contrast, our approach
is a probabilistic graphical model [13] which opens up many
possibilities for adaptation or extension (see Section 5).
Our previous work on this topic [28, 29] has ignored any
possible connection between the predictions from the two
classiﬁers in comparison. Although this is totally ﬁne when
those two classiﬁers are evaluated separately each on a diﬀerent test dataset, it is not the optimal solution in the common
situation when those two classiﬁers are evaluated on exactly
the same test dataset. In this paper, we extend such a simplistic “unpaired” model to the more general “paired” model
which takes the correlation between classiﬁers into account,
and demonstrate that the former has much less statistical
power than the latter (see Section 4.1).

Frequentist Performance Comparison

The traditional frequentist approach to comparing classiﬁers is to use NHST [21]. The usual process of NHST
consists of four steps: (1) formulate the null hypothesis H0
that the observations are the result of pure chance and the
alternative hypothesis H1 that the observations show a real
eﬀect combined with a component of chance variation; (2)
identify a test statistic that can be used to assess the truth of
H0 ; (3) compute the p-value, which is the probability that a
test statistic equal to or more extreme than the one observed
would be obtained under the assumption of hypothesis H0 ;
(4) if the p-value is less than an acceptable signiﬁcance level,
the observed eﬀect is statistically signiﬁcant, i.e., H0 is ruled
out and H1 is valid.
Speciﬁcally for performance comparison of text classiﬁers,
the usage of NHST has been presented in detail by Yang and
Liu in their SIGIR-1999 paper [27]. In summary, on the document level (micro level), sign-test can be used to compare
two classiﬁers’ accuracy scores (called s-test), while unpaired
t-test can be used to compare two classiﬁers’ performance
measures in the form of proportions, e.g., precision, recall,
error, and accuracy (called p-test); on the category level
(macro level), sign-test and paired t-test can both be used
to compare two classiﬁers’ F1 scores [26] (which are called
S-test and T-test respectively).
In spite of being useful and inﬂuential, such a frequentist approach unfortunately has many inherent deﬁciencies
and limitations [14, 15]. First, NHST is only able to tell us
whether the experimental data are suﬃcient to reject the
null hypothesis (that the performance diﬀerence is zero) or
not, but there is no way to accept the null hypothesis. If we
fail to reject the null hypothesis, we cannot conclude that it
is true, but only recognise that the null hypothesis is a possibility. That is to say, it is impossible for us to use NHST to
conﬁdently claim that two classiﬁers perform equally well .
Second, NHST will reject the null hypothesis as long as the
experimental data suggest that the performance diﬀerence is
non-zero, even if the performance diﬀerence is too slight to
have any real eﬀect in practice. Third, complex performance
measures such as the F1 score can only be compared on the
category level but not on the document level, which seriously restricts the statistical power of NHST as the number
of categories is usually much much smaller than the number
of documents. Fourth, using sign-test, those pairs of identical classiﬁcation outcomes are completely discarded, which
is undesirable because the probability that the two classiﬁers are essentially equal would be substantially underestimated. Fifth, using unpaired t-test, the correlation between
the classiﬁers in comparison are totally ignored, which is
unreasonable because in reality both classiﬁers are likely to
do well on “easy” test documents, and badly on “diﬃcult”
test documents, not to mention that those classiﬁers could
be just diﬀerent versions of the same machine learning algorithm.

3.
3.1

OUR APPROACH
Probabilistic Models

Let us consider a text classiﬁer which has been tested
on a collection of N labelled test documents, D. For each
document xi (i = 1, . . . , N ), we have its true class label yi
as well as the predicted class label ŷi .
If this classiﬁer is actually a Bayesian model, in principle there should be a direct way to assess the suitability of
model M in explaining
 the experimental data by computing
Pr[M|D] ∝ Pr[M] Θ Pr[D|Θ, M] Pr[Θ|M]dΘ. However,
here we would like to consider the general situation where
the true and predicted class labels are the only information
presumed to be available.
In the most basic setting, binary classiﬁcation, a document belongs to either the positive class or the negative
class. Without loss of generality, we use integer 1 as the ID
of the positive class and integer 0 as the ID of the negative
class. Furthermore, for the sake of clarity, we will also denote
the true positive and negative classes using notations + and

16

Table 1: The classiﬁcation results from one binary classiﬁer.
yi

β

+

μ

1
0

ρ+
1 − ρ+

−

1−μ

1
0

ρ−
1 − ρ−

ρ+

yi

+
Γ(α+
1 ,α0 )

β

ρ+



ŷi

μ

α−

N

ψ
α+

N

n+

ρ+

ρ−

c+

c−

α−

n−

(b) compact

Figure 1: The probabilistic graphical model for a binary text
classiﬁer’s performance.
ρ+ . The equations to calculate the contingency table for a
classiﬁer are listed as follows.
tp = N μρ+
f n = N μ(1 − ρ+ )

f p = N (1 − μ)ρ−
tn = N (1 − μ)(1 − ρ− )

With the contingency table for a classiﬁer available, we
can compute not only the accuracy, but also more complex
performance measures such as the F1 score for that classiﬁer.
The precision P , recall R, and their harmonic mean F1 score
could be computed as follows.
tp
μρ+
=
tp + f p
μρ+ + (1 − μ)ρ−
tp
μρ+
R=
=
= ρ+
+
tp + f n
μρ + μ(1 − ρ+ )
2P R
F1 =
P +R
P =

It can be seen that N is cancelled out in the calculation of
the precision, the recall, and the F1 score.
Such a model is quite general to accommodate various
performance measures (see Section 5), though in this paper
we focus on the F1 score only to illustrate the usage of our
model. Let ψ denote the chosen performance measure, then
it is simply a function that depends on μ, ρ+ and ρ− only:
ψ = f (μ, ρ+ , ρ− ).
This model describes a generative mechanism of a classiﬁer’s test results. It is summarised as follows, and also
depicted in Figure 1a as a probabilistic graphical model [13]
using common notations.

α+
1 −1

(1 −

ρ+ )
where the hyper-parameter α+ = α1+ , α0+ encodes our prior belief about the classiﬁer’s prediction accuracy on positive test documents.
 In the same way, we have
ρ− ∼ Beta(α− ), where α− = α1− , α0− . If we do not have
any prior knowledge, we can simply set α+ = α− = (1, 1)
that yields a uniform distribution, as we did in our experiments.
Once the parameters μ, ρ+ and ρ− have been estimated, it
will be easy to calculate the contingency table of “expected”
classiﬁcation results: true positive (tp), false positive (f p),
true negative (tn), and false negative (f n). For example, the
anticipated number of true positive predictions of the classiﬁer should be the number of positive test documents N μ
times the rate of being predicted by the classiﬁer as positive
+
Γ(α+
1 )Γ(α0 )

ρ−

(a) original

parameter β = (β + , β − ) encodes our prior belief about each
class’s proportion. If we do not have such knowledge, we
can simply set β = (1, 1) that yields a uniform distribution,
as we did in our experiments.
When a test document xi with true class label yi is classiﬁed, we anticipate that it will be classiﬁed as positive with
a certain probability ρyi , i.e., Pr[ŷi = 1|ρyi ] = ρyi . For
example, ρ− is the probability that a negative (−) document is classiﬁed to be positive (1). Hence we can say that
ŷi follows a Bernoulli distribution with parameter ρ+ when
yi is positive and ρ− when yi is negative. In other words,
ŷi ∼ Bern(ρ+ ) if yi = + and ŷi ∼ Bern(ρ− ) if yi = −.
It would then be convenient to use the Beta distribution as
the prior distribution of parameter ρ+ and ρ− . More speciﬁcally, ρ+ ∼ Beta(α+ ), i.e., Pr[ρ+ ] =

ψ
α+

− respectively which should be regarded as interchangeable
synonyms of class IDs 1 and 0.
The test documents can usually be considered as “independent trials”, so we regard both their true class labels yi
and their predicted class labels ŷi as independent and identically distributed (i.i.d.) random variables.
Table 1 lists all the possible classiﬁcation results and their
corresponding probabilities for a test document using one
binary classiﬁer. It is worth noting that in our model a
classiﬁer is allowed to exhibit diﬀerent prediction accuracies
on documents from diﬀerent true classes. This ﬂexibility is
necessary to reﬂect the reality and facilitate the estimation
of complex performance measures that take class imbalance
into account.
Given a test document xi , we use μ to represent the probability that its true class label yi is positive. Obviously the
probability that yi is negative would therefore be 1 − μ.
This means that yi follows a Bernoulli distribution with parameter μ: yi ∼ Bern(μ), i.e., Pr[yi |μ] = μyi (1 − μ)1−yi . It
would then be convenient to use the Beta distribution (which
is conjugate to the Bernoulli distribution) as the prior distribution of parameter μ. More speciﬁcally, μ ∼ Beta(β),
−
Γ(β + ,β − )
β + −1
(1 − μ)β −1 where the hyperi.e., Pr[μ] = Γ(β
+ )Γ(β − ) μ

α+
0 −1

μ

ŷi

μ ∼ Beta(β)
yi ∼ Bern(μ) for i = 1, . . . , N
ρ− ∼ Beta(α− )
ρ+ ∼ Beta(α+ )

Bern(ρ+ ) for i = 1, . . . , N if yi = +
ŷi ∼
Bern(ρ− ) for i = 1, . . . , N if yi = −
ψ = f (μ, ρ+ , ρ− )

17

In the above model, each true class label yi is regarded
as an individual sampling event, and each prediction ŷi is
treated as an individual sampling event too. If we aggregate the occurrences of such individual sampling events into
the counts of their occurrences, the model could be greatly
simpliﬁed.
Let n+ represent the total number of positive test documents and n− = N − n+ represent the total number of
negative test documents, then n+ is known to follow the
Binomial distribution with parameters N and μ: n+ ∼
  +
+
Bin(N, μ), i.e., Pr[n+ |N, μ] = nN+ μn (1 − μ)N −n where
N 
!
= n+ !(NN−n
+ )! is the Binomial coeﬃcient.
n+

···

···

···

ψB

β

μ

δ
ψA
α+A

NA

n+A

ρ+A

ρ−A

c+A

c−A

α−A

n−A

+

Let c represent the count of positive predictions (ŷi = 1)
produced on positive test documents (yi = +), then c+
is known to follow the Binomial distribution with parameters n+ and ρ+ : c+ ∼ Bin(n+ , ρ+ ), i.e., Pr[c+ |n+ , ρ+ ] =
n+  + c+
+
+
(1 − ρ+ )n −c . In the same way, we have c− ∼
ρ
c+
Bin(n− , ρ− ).
The parameters μ, ρ+ and ρ− are the same as before and
their prior distributions remain the same. The deterministic
variable ψ also stays unchanged.
This compact model is equivalent to the original model,
but it will be computationally much more eﬃcient due to
the drastic reduction of sampling events. So hereafter the
compact model will be used instead of the original model for
our work on performance comparison.
The compact model is summarised as follows and depicted
in Figure 1b.
μ
n+
ρ+
c+
ψ

∼ Beta(β)
∼ Bin(N, μ)
∼ Beta(α+ )
∼ Bin(n+ , ρ+ )
= f (μ, ρ+ , ρ− )

Figure 2: The unpaired model for performance comparison.
β

δ

α+
N

n+

ψA

ψB

θ+

θ−

c+

c−

α−

n−

Figure 3: The paired model for performance comparison.

The unpaired model consisting of two separate sub-models
for two classiﬁers A and B is depicted in Figure 2, where
most of the sub-model for B is omitted as it is symmetric to
that of A.

n− = N − n+
ρ− ∼ Beta(α− )
c− ∼ Bin(n− , ρ− )

3.1.2

Paired Model

Although the unpaired model is simple and eﬀective, its
underlying assumption that the predictions from two classiﬁers A and B are independent of each other is unrealistic
when those two classiﬁers are evaluated on the same test
dataset. In contrast to the existing work for classiﬁcation
performance comparison (see Section 2), we would like to
avoid this unrealistic assumption by modelling the two classiﬁers’ predictions jointly as pairs. This is indeed crucial to
assessing the real signiﬁcance of the two classiﬁers’ performance diﬀerence, as we demonstrate later in our experiments
(see Section 4.1).
Considering two classiﬁers A and B evaluated on the same
document collection, we have for each document xi (i =
1, . . . , N ) a prediction outcome pair oi = (ŷiA , ŷiB ) where
ŷiA and ŷiB are the predicted class labels given by A and B
respectively.
Table 2 lists all the possible classiﬁcation results and their
corresponding probabilities for a test document using two
binary classiﬁers. Since for each of the two possible yi values
there are four possible oi values {(1, 1), (1, 0), (0, 1), (0, 0)},
this table has 2 × 4 = 8 entries in total.
When a test document xi with true class label yi is classiﬁed by the two classiﬁers A and B, we anticipate that each
possible prediction outcome pair oi will occur with a certain
+
is
probability θoyii , i.e., Pr[oi |θ yi ] = θoyii . For example, θ(0,1)
the probability that a positive (+) document is classiﬁed
to be negative (0) by the classiﬁer A and positive (1) by

The usage of conjugate priors (e.g., Beta for Bernoulli or
Binomial) is not obligatory in our model. Actually any reasonable probability distribution can be used as the prior of
μ, ρ+ or ρ− . If we insist on using conjugate priors, it is possible to simplify the model even further by computing the
posterior probability distributions of our model parameters
analytically and then sampling from the posterior probability distributions directly. However, this will only bring moderate improvement to computational eﬃciency, and more
importantly it will make the model less ﬂexible as some extensions to the model (such as hierarchical modelling) will
be obstructed. So we shall not go down that direction in
this paper.

3.1.1

μ

Unpaired Model

In the unpaired model for performance comparison, the
predictions from the two classiﬁers A and B being compared
are assumed to be independent with each other [28, 29]. Actually the two classiﬁers could be evaluated each on a different test dataset as long as the data come from the same
distribution (e.g., with the same proportion of positive test
examples). So we can simply pool the two probabilistic models for those two classiﬁers together, and introduce a deterministic variable δ to capture the diﬀerence between their
performance scores ψ A and ψ B .
δ = ψA − ψB

18

Table 2: The classiﬁcation results from two binary classiﬁers.
yi
+

−

μ

1−μ

ŷiA

ŷiB

oi

1

1

(1,1)

1

0

(1,0)

0

1

(0,1)

0

0

(0,0)

+
θ(0,0)

1

1

(1,1)

−
θ(1,1)

1

0

(1,0)

0

1

(0,1)

0

0

(0,0)

3.2
3.2.1

+
θ(1,1)

−
−
α(1,1)
, . . . , α(0,0)
.

Bayes Factor

Given a probabilistic model of the chosen performance
measure, we can consider the comparison of two classiﬁers
as a model selection problem and utilise the Bayes factor to
address it [1, 2].
In our context, the Bayes factor is the marginal likelihood
of classiﬁcation results data for the null model Pr[D|M0 ]
(where two classiﬁers perform equally well) relative to the
marginal likelihood of classiﬁcation results data for the alternative model Pr[D|M1 ] (where one classiﬁer works better
than the other classiﬁer): BF = Pr[D|M0 ]/ Pr[D|M1 ]. As
the BF becomes larger, the evidence increases in favour of
model M0 over model M1 . The rule of thumb for interpreting the magnitude of the BF is that there is “substantial”
evidence for the null model M0 when the BF exceeds 3, and
similarly, “substantial” evidence for the alternative model
M1 when the BF is less than 13 [11].
Although for simple models the value of Bayes factor can
be derived analytically as shown by [1, 2], for complex models it can only be computed numerically using for example the Savage-Dickey (SD) method [5]. The SD method
assumes that the prior on the variance in the null model
equals the prior on the variance in the alternative model
at the null value: Pr[σ 2 |M0 ] = Pr[σ 2 |M1 , δ = 0]. From
this it follows that the likelihood of the data in the null
model equals the likelihood of the data in the alternative
model at the null value: Pr[D|M0 ] = Pr[D|M1 , δ = 0].
Thus, the Bayes factor can be determined by considering
the alternative hypothesis alone, because it is just the ratio of the probability density at δ = 0 in the posterior
relative to the probability density at δ = 0 in the prior:
BF = Pr[δ = 0|M1 , D]/ Pr[δ = 0|M1 ].

+
θ(1,0)
+
θ(0,1)

−
θ(1,0)
−
θ(0,1)
−
θ(0,0)

the classiﬁer B. If we let θ + denote the vector of parameters θo+i and similarly let θ − denote the vector of parameters
θo−i , then we can say that oi follows a Categorical distribution with parameter θ + when yi is positive and θ − when
yi is negative. In other words, oi ∼ Cat(θ + ) if yi = +
and oi ∼ Cat(θ − ) if yi = −. It would then be convenient to use the Dirichlet distribution (which is conjugate to
the Categorical distribution) as the prior distribution of parameter θ + or θ − . More speciﬁcally, θ + ∼ Dir(α+ ), i.e.,

α+ −1
Γ(
α+
) 
k
θk k
where the hyper-parameter
Pr[θ + ] =  k +
k
k Γ(αk )


+
+
, . . . , α(0,0)
encodes our prior belief about
α+ = α(1,1)
the classiﬁer’s prediction accuracy on positive test documents. In the same way, we have θ − ∼ Dir(α− ), where
α− =

Decision Making

If we do not have any prior

knowledge, we can simply set α+ = α− = (1, . . . , 1) that
yields a uniform
distribution, as we did in our experiments.

+
+
Let c = c(1,1) , . . . , c+
(0,0) represent the counts of diﬀer3.2.2 Bayesian Estimation
ent types of prediction outcome pairs produced on positive
test documents, then c+ is known to follow the Multinomial
Instead of relying on the Bayes factor which is a single
distribution with parameters n+ and θ + : c+ ∼ Mult(n+ , θ + ),
value, we can make use of the entire posterior probability

 c+
+
Γ(( k c+
+1) 
c+
distribution of δ, the performance diﬀerence between two
k)
k
k
i.e., Pr[c+ |N, θ + ] = + n ! +
k θk =  Γ(c+ +1)
k θk .
c(1,1) !...c(0,0) !
k
classiﬁers, for their comparison. This Bayesian (parameter)
k
In the same way, we have c− ∼ Mult(n− , θ − ).
estimation approach to performance comparison is said to
Once the parameters μ, θ + and θ − have been estimated, it
be more informative and more robust than using the Bayes
will be easy to calculate, for each classiﬁer, the contingency
factor [14, 15].
table of “expected” classiﬁcation results as before by noticing
Given the posterior probability distribution of δ, we can
the following facts:
then reach a discrete judgement (decision) about how those
two classiﬁers A and B compare with each other by exam+
+
−
−
+ θ(1,0)
ρ−A =θ(1,1)
+ θ(1,0)
ρ+A =θ(1,1)
ining the relationship between the 95% Highest Density In+
+
−
−
terval (HDI) of δ and the user-deﬁned Region of Practical
ρ+B =θ(1,1)
+ θ(0,1)
ρ−B =θ(1,1)
+ θ(0,1)
Equivalence (ROPE) of δ [14, 15]. The 95% HDI is a useful
Thus the performance scores ψ A and ψ B , as well as their
summary of where the bulk of the most credible values of
diﬀerence δ could be estimated.
δ falls: by deﬁnition, every value inside the HDI has higher
The paired model is summarised as follows and depicted
probability density than any value outside the HDI, and the
in Figure 3.
total mass of points inside the 95% HDI is 95% of the distribution. The ROPE of δ, e.g., [−0.05, +0.05], encloses those
μ ∼ Beta(β)
values of δ deemed to be negligibly diﬀerent from its null
n− = N − n+
n+ ∼ Bin(N, μ)
value for practical purposes. Using the HDI together with
θ + ∼ Dir(α+ )
θ − ∼ Dir(α− )
the ROPE, the performance comparison decisions could be
+
+
+
−
−
−
made as follows:
c ∼ Mult(n , θ )
c ∼ Mult(n , θ )
A
+
−
B

+
−
• if the HDI sits fully within the ROPE (as illustrated
ψ = f (μ, θ , θ )
ψ = f (μ, θ , θ )
in Figure 6), A is practically equivalent (≈) to B;
δ = ψA − ψB
• if the HDI sits fully at the left or right side of the
ROPE, A is signiﬁcantly worse () or better ()
than B respectively;

19

PyMC31 [22] for MCMC based Bayesian model ﬁtting. The
source code is made open to the research community as online supplementary material2 . It is free, easy to use, and
extensible to more sophisticated models (see Section 5).
We should mention that this program for Bayesian performance comparison runs much slower than standard frequentist NHST techniques. On a machine with Intel x64 Core i7
CPU 2.30GHz, a sign-test or t-test would normally ﬁnish in
less than 0.02 seconds, but our program could take up to 20
seconds for one comparison. Most of the time is spent on the
computationally expensive MCMC sampling as it does require a decent number of samples to achieve high-ﬁdelity approximation of probability distributions. Nevertheless, such
a speed should be perfectly acceptable for the purpose of
comparing classiﬁers because the classiﬁcation experiments
would usually take much longer time. Therefore the program is still very practical. Moreover, the program would
be greatly accelerated if GPUs could be used by Theano, the
underlying computational engine for PyMC3.

Figure 4: An example trace plot.
• if the HDI sits mainly though not fully at the left or
right side of the ROPE, A is slightly worse (<) or
better (>) than B respectively, but more experimental
data would be needed to make a reliable judgement.
The need to specify the ROPE may sound like an extra burden on users compared to NHST, but in fact it is only making a hidden problem — how much performance diﬀerence
would really matter for practical purposes (such as customer
satisfaction and business proﬁt) — explicit. The determination of the ROPE requires only knowledge about the application domain but not expertise in statistics. When the HDI
is far away from or tightly surrounding the null value, the exact ROPE is inconsequential as any reasonable ROPE would
lead to the same decision. Furthermore, in many situations,
the exact ROPE can be left indeterminate. By reporting
the HDI and other summary information about the full posterior distribution of δ, readers can apply whatever ROPE
appropriate for them to make their own decisions.

3.3

4.

EXPERIMENTS

4.1

Synthetic Data

To demonstrate the advantage of our paired model over
unpaired model, we perform power analysis using simulations. The statistical power is the probability of achieving
the goal of a planned empirical study, if a suspected underlying state of the world is true [15]. As the power increases,
there are decreasing chances of a Type II error aka the false
negative rate β since the power is equal to 1 − β.
We consider the following two scenarios where the two
hypothetical classiﬁers A and B are somewhat correlated.
The scenario (a):
Pr[+] = μ
= 0.5
+
+
= 0.3, Pr[(1, 0)|+] = θ(1,0)
= 0.3,
Pr[(1, 1)|+] = θ(1,1)
+
+
Pr[(0, 1)|+] = θ(0,1) = 0.2, Pr[(0, 0)|+] = θ(0,0)
= 0.2,
Pr[−] = 1 − μ = 0.5
−
−
= 0.2, Pr[(1, 0)|−] = θ(1,0)
= 0.2,
Pr[(1, 1)|−] = θ(1,1)
−
−
Pr[(0, 1)|−] = θ(0,1) = 0.3, Pr[(0, 0)|−] = θ(0,0)
= 0.3,
It is easy to see that F1A = 0.6 while F1B = 0.5, so the goal
here is to detect “A  B”.
The scenario (b):
Pr[+] = μ
= 0.5
+
+
= 0.3, Pr[(1, 0)|+] = θ(1,0)
= 0.2,
Pr[(1, 1)|+] = θ(1,1)
+
+
Pr[(0, 1)|+] = θ(0,1) = 0.2, Pr[(0, 0)|+] = θ(0,0)
= 0.3,
Pr[−] = 1 − μ = 0.5
−
−
= 0.3, Pr[(1, 0)|−] = θ(1,0)
= 0.2,
Pr[(1, 1)|−] = θ(1,1)
−
−
Pr[(0, 1)|−] = θ(0,1) = 0.2, Pr[(0, 0)|−] = θ(0,0)
= 0.3,
It is easy to see that F1A = 0.5 and F1B = 0.5, so the goal here
is to detect “A ≈ B”. Please note that this goal is infeasible
using the frequentist NHST.
The power analysis results are shown in Table 3 and also
Figure 5, which clearly indicate the superiority of the paired
model to the unpaired model in terms of statistical power.
The reason why the unpaired model does not have as
much statistical power as the paired model is because the
former cannot tell whether the prediction diﬀerences (or

Software Implementation

The purpose of building these models for classiﬁcation results is to assess the Bayesian posterior probability of δ —
the performance diﬀerence between two classiﬁers A and B.
An approximate estimation of δ can be obtained by sampling from its posterior probability distribution via Markov
Chain Monte Carlo (MCMC) [15] techniques.
We have implemented our models with an MCMC method
Metropolis-Hastings sampling [15]. The default conﬁguration is to generate 50,000 samples, with no “burn-in”, “lag”,
or “multiple-chains”. It has been argued in the MCMC literature that those tricks are often unnecessary: it is perfectly
right to do a single long sampling run and keep all samples [13, 18]. In fact, the approximation accuracy of our
program is very high: its Monte Carlo error (MC error)
was usually close to 0 and never went beyond 0.002 in all
our experiments (see Section 4). Figure 4 shows an example MCMC trace of our program in the experiments which
clearly demonstrates the convergence of MCMC sampling.
In order to calculate the Bayes factor using the SD method
(see Section 3.2.1), we approximate the posterior density
Pr[δ = 0|M1 , D] and the prior density Pr[δ = 0|M1 ] by ﬁtting a smooth function to the corresponding MCMC samples
via kernel density estimation (KDE).
The program is written in Python 3 utilising the module

1

http://pymc-devs.github.io/pymc3/
http://www.dcs.bbk.ac.uk/˜dell/publications/dellzhang
sigir2016 supp.html
2

20

Table 3: The power analysis of our models.
scenario

(a)

(b)

already been transformed by TF-IDF term weighting and
document length normalisation.
We have used the oﬀ-the-shelf implementation of these
classiﬁcation algorithms provided by a Python machine learning library scikit-learn4 in our experiments, again for the
reproducibility reasons. The smoothing parameter α for the
NB algorithm and the regularisation parameter C for the
linear SVM algorithm have been tuned via grid search with
5-fold cross-validation on the training data for the macroaveraged F1 score. The optimal parameters found are: NBBern
with α = 10−14 , NBM ult with α = 10−3 , SVML1 with
C = 22 , SVML2 with C = 21 .
Table 4 shows the results of performance comparison between NBBern and NBM ult , based on which we can conﬁdently say that for most of the target categories, NBBern is
outperformed by NBM ult . Such results conﬁrm the ﬁnding
of [20] on this harder dataset.
Table 5 shows the results of performance comparison between SVML1 and SVML2 , based on which we can conﬁdently say that for most of the target categories, SVML1
and SVML2 have no practical diﬀerence on classiﬁcation effectiveness as measured by the F1 score (given the ROPE
[−0.05, +0.05]), though the former may have its advantages
in terms of sparsity. Such results are complementary to
those reported in [30].
Table 6 shows the results of performance comparison between NBM ult and SVML2 — the better performing classiﬁers from the NB and SVM camps. It can be clearly seen
that for most of the target categories, the competition between NBM ult and SVML2 is too close to call: more test
data would be needed to make a reliable judgement which
one works better. Nevertheless, for six out of the eight target
categories on which we can indeed make reliable judgements,
NBM ult and SVML2 are practically equivalent (given the
ROPE [−0.05, +0.05]). This phenomenon somewhat supports the claim of [23] that NBM ult , if properly enhanced
by TF-IDF term weighting and document length normalisation, can reach a comparable performance as SVML2 .
On the micro (document) level, no NHST method exists
for the comparison of F1 scores. So in the above tables we
show the results of using NHST to compare classiﬁcation
accuracies instead: the column “sign-test” and “t-test” contain the two-sided p-values of micro level sign-test (called
s-test in [27]) and unpaired t-test (called p-test in [27]) respectively. The symbol indicates that the accuracy diﬀerence between A and B is statistically signiﬁcant (p < 0.05)
according to NHST. When NHST fails to reject the null hypothesis that the two classiﬁers work equally well, no conclusion can be drawn from the comparison.
In all those tables, our proposed Bayesian performance
comparison method has oﬀered rich information about the
diﬀerence between two classiﬁers’ F1 scores: in addition to
the ﬁnal judgement (“decision”), we have shown the posterior “mean”, standard deviation (“std”), the Bayes factor
estimated by the SD method (“BFSD ”), the percentage lower
or greater than the null value 0 (“LG pct”), the percentage
covered by the ROPE (“ROPE pct”), and the 95% “HDI”.
By contrast, the frequentist NHST would lead to a far less
complete picture: it has only the p-values (and maybe also
the conﬁdence intervals) to oﬀer. Furthermore, note that
the judgements made by the Bayesian estimation on sev-

power
unpaired paired

goal

dataset-size

AB

500
1000
1500
2000
2500
3000
3500

0.26
0.41
0.70
0.79
0.87
0.92
0.96

0.30
0.52
0.76
0.84
0.90
0.94
0.97

A≈B

500
1000
1500
2000
2500
3000
3500

0.00
0.01
0.26
0.63
0.72
0.88
0.92

0.00
0.22
0.58
0.81
0.87
0.96
0.99

agreements) between the two classiﬁers are consistent or not
while the latter can. Inconsistent prediction diﬀerences yield
a larger variability than consistent ones, and consequently
more are required to exhibit statistical signiﬁcance. Suppose that classiﬁer A has a higher F1 score than classiﬁer B.
If A almost always makes better predictions than B when
they disagree, a relatively small amount of such consistent
diﬀerences could give us enough conﬁdence to assert statistical signiﬁcance, which is recognised by the paired model
but not the unpaired model.

4.2

Real-World Data

We have conducted experiments on a standard benchmark
dataset for text classiﬁcation, 20newsgroups [16], of which
the results are reported here. In order to ensure the reproducibility of our experimental results, we choose to use
not the raw document collection, but a publicly-available
ready-made “vectorised” version3 , as in [28, 29]. We have
also done experiments on other “vectorised” datasets including the classic Reuters-21578 [27], but due to the space
limit those experimental results are reported only as online
supplementary material together with our program’s source
code (see Section 3.3).
In the experiments, we have applied our proposed approach to carefully analyse the performances of two wellknown supervised machine learning algorithms that are widely
used for real-world text classiﬁcation tasks: Naive Bayes
(NB) and linear Support Vector Machine (SVM) [19]. For
the former, we consider its two common variations: one with
the Bernoulli event model (NBBern ) and the other with the
Multinomial event model (NBM ult ) [20]. For the latter, we
consider its two common variations: one with the L1 norm
penalty (SVML1 ) and the other with the L2 norm penalty
(SVML2 ) [7, 30]. Thus we have four diﬀerent classiﬁers in
total. Obviously, the classiﬁcation results of NBBern and
NBM ult would be highly correlated, and those of SVML1
and SVML2 as well. Among them, SVML2 is widely regarded as the state-of-the-art text classiﬁer [17, 25, 27]. It is
also worth to notice that the NB algorithms will be applied
not to the raw bag-of-words text datasets as people usually
do, but on the vectorised 20newsgroups dataset which has
3
http://scikit-learn.org/stable/datasets/twenty
newsgroups.html

4

21

http://scikit-learn.org/stable/

(a) example scenario: A  B.

(b) example scenario: A ≈ B.

Figure 5: Comparing the statistical power of paired and unpaired models.
Table 4: The results of performance comparison between NBBern and NBM ult .
category
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

frequentist
sign-test
t-test




















0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.064
0.024
0.000
0.000
0.000

















0.008
0.000
0.000
0.003
0.000
0.000
0.000
0.001
0.000
0.001
0.000
0.002
0.002
0.000
0.000
0.178
0.151
 0.000
 0.034
 0.011

mean

std

−0.081
−0.114
−0.400
−0.095
−0.249
−0.101
−0.136
−0.092
−0.144
−0.080
+0.108
−0.092
−0.097
−0.115
−0.109
−0.027
−0.105
−0.102
−0.088
−0.040

0.021
0.017
0.028
0.016
0.019
0.017
0.016
0.016
0.017
0.013
0.017
0.016
0.020
0.016
0.016
0.016
0.019
0.016
0.020
0.030

Bayesian
LG pct
ROPE pct

BFSD




















0.003
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
3.132
0.000
0.000
0.007
2.389

100.0%<0<0.0%
100.0%<0<0.0%
100.0%<0<0.0%
100.0%<0<0.0%
100.0%<0<0.0%
100.0%<0<0.0%
100.0%<0<0.0%
100.0%<0<0.0%
100.0%<0<0.0%
100.0%<0<0.0%
0.0%<0<100.0%
100.0%<0<0.0%
100.0%<0<0.0%
100.0%<0<0.0%
100.0%<0<0.0%
95.7%<0<4.3%
100.0%<0<0.0%
100.0%<0<0.0%
100.0%<0<0.0%
91.3%<0<8.7%

HDI

decision

[−0.125, −0.041]
[−0.148, −0.080]
[−0.456, −0.345]
[−0.126, −0.062]
[−0.286, −0.211]
[−0.135, −0.069]
[−0.168, −0.105]
[−0.123, −0.061]
[−0.178, −0.111]
[−0.105, −0.055]
[+0.074, +0.142]
[−0.125, −0.061]
[−0.137, −0.058]
[−0.147, −0.084]
[−0.139, −0.079]
[−0.059, +0.004]
[−0.141, −0.068]
[−0.134, −0.070]
[−0.128, −0.049]
[−0.097, +0.022]

<














<


<
<

multi-label): we will need one μ parameter and a pair of
θ parameters for each class. Thus we are able to measure
each classiﬁer’s overall performance using micro-averaged or
macro-averaged F1 scores [27], and compute their diﬀerence
as the deterministic variable δ in the model [28]. Note that
here δ is estimated using a large number of prediction outcomes for all test documents, rather than just a small number of F1 scores for test categories as in [27] (see Section 2.1).
It would be promising to go further to develop a Bayesian hierarchical model [15] where the classiﬁer’s parameters θ j for
diﬀerent classes are governed by a higher-level overarching
hyper-parameter η (e.g., representing the overall probability of making correct predictions) and thus able to “share
statistical strength” [29]. A potential problem, though, is
the explosive growth of possible prediction outcome combinations along with the increase of class numbers, which in
the worst situation may force us into backing oﬀ to the assumption of independence between classiﬁers so as to keep
the model computationally tractable.
Other performance measures. To compare classiﬁers

eral cases are diﬀerent from those made by the frequentist
NHST (e.g., at the signiﬁcance level 0.05). So even if in
some researchers’ opinion the superiority of the former over
the latter is still debatable, there is no doubt that the former
can at least be complementary to the latter.
Figure 6 illustrates the visualisation of Bayesian performance comparison results produced by our program: the
“posterior plot” sub-graph shows the posterior probability
distribution of the performance diﬀerence variable δ; and
the “factor plot” sub-graph shows the estimation of the Bayes
factor by the SD method.

5.

6.6%
0.0%
0.0%
0.2%
0.0%
0.1%
0.0%
0.4%
0.0%
0.8%
0.0%
0.4%
0.8%
0.0%
0.0%
92.2%
0.1%
0.1%
2.9%
62.9%

EXTENSIONS

The proposed Bayesian approach to performance comparison has been described above in the most basic setting for
concreteness and simplicity, but it is in fact readily extensible to the following more general scenarios.
Multiple classes. It would be straightforward to extend
our model to multi-class classiﬁcation (either single-label or

22

Table 5: The results of performance comparison between SVML1 and SVML2 .
category
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

frequentist
sign-test
t-test
 0.049
0.523
0.632
0.270
0.247
1.000
 0.000
 0.000
0.221
 0.000
0.515
0.771
0.061
0.264
0.733
0.192
0.065
0.105
 0.014
 0.033

0.299
0.706
0.774
0.542
0.524
0.960
 0.031
 0.000
0.457
 0.000
0.657
0.837
0.298
0.463
0.814
0.448
0.355
0.333
0.192
0.265

mean

std

−0.027
−0.011
−0.007
−0.020
−0.022
−0.009
−0.053
−0.133
−0.015
+0.126
−0.013
−0.005
−0.033
−0.019
−0.011
−0.035
−0.030
+0.014
−0.014
+0.008

0.018
0.013
0.014
0.014
0.014
0.014
0.013
0.017
0.014
0.017
0.011
0.013
0.016
0.015
0.014
0.013
0.014
0.014
0.016
0.022

Bayesian
LG pct
ROPE pct

BFSD
 3.399
 9.427
 12.058
 4.593
 3.781
 12.206
 0.011
 0.000
 8.165
 0.000
 9.261
 12.937
1.209
 5.513
 10.671
 0.330
1.225
 6.589
 8.065
 7.488

93.6%<0<6.4%
80.6%<0<19.4%
68.3%<0<31.7%
92.3%<0<7.7%
94.8%<0<5.2%
73.2%<0<26.8%
100.0%<0<0.0%
100.0%<0<0.0%
84.4%<0<15.6%
0.0%<0<100.0%
87.7%<0<12.3%
64.0%<0<36.0%
98.2%<0<1.8%
90.2%<0<9.8%
79.3%<0<20.7%
99.6%<0<0.4%
98.7%<0<1.3%
15.7%<0<84.3%
82.0%<0<18.0%
36.5%<0<63.5%

89.9%
99.9%
99.8%
98.5%
97.4%
99.8%
41.9%
0.0%
99.3%
0.0%
99.9%
100.0%
84.7%
97.9%
99.7%
86.9%
92.1%
99.3%
98.7%
96.9%

HDI

decision

[−0.063, +0.008]
[−0.038, +0.014]
[−0.035, +0.020]
[−0.047, +0.007]
[−0.049, +0.005]
[−0.035, +0.019]
[−0.078, −0.029]
[−0.166, −0.098]
[−0.042, +0.014]
[+0.094, +0.160]
[−0.035, +0.009]
[−0.030, +0.021]
[−0.066, −0.003]
[−0.050, +0.009]
[−0.036, +0.018]
[−0.062, −0.011]
[−0.057, −0.003]
[−0.015, +0.041]
[−0.045, +0.017]
[−0.035, +0.051]

<
≈
≈
≈
≈
≈
<

≈

≈
≈
<
≈
≈
<
<
≈
≈
>

Table 6: The results of performance comparison between NBM ult and SVML2 .
category
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

frequentist
sign-test
t-test













0.314
0.386
0.056
1.000
0.840
0.017
0.057
0.180
1.000
0.000
0.000
0.024
0.000
0.000
0.038
0.009
0.213
0.010
0.119
0.021






0.536
0.521
0.205
1.000
0.853
0.089
0.188
0.312
1.000
0.000
0.000
0.102
0.001
0.009
0.129
0.064
0.430
0.070
0.349
0.179

mean

std

−0.001
+0.028
−0.028
+0.030
+0.006
+0.051
+0.013
+0.031
+0.007
+0.215
−0.115
−0.016
+0.073
+0.060
+0.050
−0.009
+0.041
+0.053
+0.015
−0.061

0.022
0.018
0.021
0.018
0.018
0.017
0.016
0.018
0.018
0.019
0.017
0.017
0.020
0.016
0.017
0.014
0.017
0.016
0.019
0.031

Bayesian
LG pct
ROPE pct

BFSD
 8.542
 3.128
 3.739
2.966
 9.533
 0.075
 8.805
2.566
 9.551
 0.000
 0.000
 7.016
 0.008
 0.004
 0.194
 10.473
0.650
 0.058
 6.927
0.825

50.5%<0<49.5%
6.2%<0<93.8%
91.2%<0<8.8%
5.1%<0<94.9%
37.6%<0<62.4%
0.2%<0<99.8%
20.6%<0<79.4%
4.4%<0<95.6%
35.9%<0<64.1%
0.0%<0<100.0%
100.0%<0<0.0%
84.0%<0<16.0%
0.0%<0<100.0%
0.0%<0<100.0%
0.2%<0<99.8%
74.6%<0<25.4%
0.8%<0<99.2%
0.0%<0<100.0%
22.2%<0<77.8%
97.6%<0<2.4%

6.

using a performance measure diﬀerent from the F1 score,
we would only need to replace the function f (μ, θ + , θ − ) for
computing ψ, as long as that performance measure could be
calculated based on the classiﬁcation contingency table alone
[12]. For example, it would be straightforward to extend our
model to handle the more general Fβ measure (β ≥ 0) [19,26]
with β = 1: we just need to substitute the Fβ formula for
the F1 formula in the function of ψ. For another example,
the Area Under the ROC Curve (AUC) is essentially the
proportion of correctly ranked document pairs [9, 12], so it
could be modelled in a similar way.
Other tasks. More generally, the idea of building a
Bayesian probabilistic graphical model to make comprehensive performance comparison could be applied to not just
classiﬁers, but also search systems (see the ICTIR-2015 best
paper [3]), recommender systems, and advertising systems.

97.7%
89.0%
84.5%
86.8%
99.0%
48.0%
99.1%
85.1%
98.9%
0.0%
0.0%
98.1%
12.4%
26.1%
50.7%
99.8%
71.1%
43.2%
96.5%
36.0%

HDI

decision

[−0.043, +0.042]
[−0.007, +0.063]
[−0.069, +0.013]
[−0.006, +0.066]
[−0.031, +0.040]
[+0.017, +0.083]
[−0.017, +0.046]
[−0.005, +0.067]
[−0.028, +0.042]
[+0.180, +0.253]
[−0.149, −0.082]
[−0.048, +0.017]
[+0.034, +0.113]
[+0.029, +0.090]
[+0.016, +0.082]
[−0.037, +0.021]
[+0.007, +0.074]
[+0.023, +0.085]
[−0.023, +0.053]
[−0.119, +0.001]

≈
>
<
>
≈
>
≈
>
≈


≈
>
>
>
≈
>
>
>
<

CONCLUSIONS

This paper tries to address the problem of comparing text
classiﬁers’ performances by appealing to Bayesian reasoning. Although we ourselves believe that Bayesian statistics
is “the way it should be”, we understand that not everyone is
a Bayesian or wants to become a Bayesian. Our argument
is not whether being a Bayesian is philosophically better
than being a frequentist, but that our Bayesian estimation
based approach to performance comparison of text classiﬁers
avoids all the aforementioned practical weaknesses of NHST
(see Section 2.1) and it provides much richer information
about the diﬀerence between two classiﬁers’ performances
than NHST does, therefore it can supersede or at least complement the currently popular frequentist approach.

23

(a) posterior plot

(b) factor plot

Figure 6: A ≈ B — NBM ult is practically equivalent to SVML2 for target category 8.

7.

ACKNOWLEDGEMENTS

[15] J. K. Kruschke. Doing Bayesian Data Analysis: A Tutorial
with R, JAGS, and Stan. Academic Press, 2nd edition, 2014.
[16] K. Lang. Newsweeder: Learning to ﬁlter netnews. In
Proceedings of the 12th International Conference on Machine
Learning (ICML), pages 331–339, Tahoe City, CA, USA, 1995.
[17] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. RCV1: A new
benchmark collection for text categorization research. Journal
of Machine Learning Research (JMLR), 5:361–397, 2004.
[18] D. J. C. MacKay. Information Theory, Inference, and
Learning Algorithms. Cambridge University Press, 2003.
[19] C. D. Manning, P. Raghavan, and H. Schütze. Introduction to
Information Retrieval. Cambridge University Press, 2008.
[20] A. McCallum and K. Nigam. A comparison of event models for
Naive Bayes text classiﬁcation. In AAAI-98 Workshop on
Learning for Text Categorization, pages 41–48, Madison, WI,
1998.
[21] T. Mitchell. Machine Learning. McGraw Hill, 1997.
[22] A. Patil, D. Huard, and C. J. Fonnesbeck. PyMC: Bayesian
stochastic modelling in Python. Journal of Statistical
Software, 35(4):1–81, 2010.
[23] J. D. Rennie, L. Shih, J. Teevan, and D. R. Karger. Tackling
the poor assumptions of Naive Bayes text classiﬁers. In
Proceedings of the 20th International Conference on Machine
Learning (ICML), pages 616–623, Washington, DC, USA,
2003.
[24] H. Schutze, D. A. Hull, and J. O. Pedersen. A comparison of
classiﬁers and document representations for the routing
problem. In Proceedings of the 18th Annual International
ACM SIGIR Conference on Research and Development in
Information Retrieval (SIGIR), pages 229–237, Seattle, WA,
USA, 1995.
[25] F. Sebastiani. Machine learning in automated text
categorization. ACM Computing Surveys (CSUR), 34(1):1–47,
2002.
[26] C. J. van Rijsbergen. Information Retrieval. Butterworths,
London, UK, 2nd edition, 1979.
[27] Y. Yang and X. Liu. A re-examination of text categorization
methods. In Proceedings of the 22nd Annual International
ACM SIGIR Conference on Research and Development in
Information Retrieval (SIGIR), pages 42–49, Berkeley, CA,
USA, 1999.
[28] D. Zhang, J. Wang, and X. Zhao. Estimating the uncertainty
of average F1 scores. In Proceedings of the 2015 International
Conference on the Theory of Information Retrieval (ICTIR),
pages 317–320, Northampton, MA, USA, 2015.
[29] D. Zhang, J. Wang, X. Zhao, and X. Wang. A bayesian
hierarchical model for comparing average F1 scores. In
Proceedings of the 2015 IEEE International Conference on
Data Mining (ICDM), pages 589–598, Atlantic City, NJ, USA,
2015.
[30] J. Zhu, S. Rosset, T. Hastie, and R. Tibshirani. 1-norm
support vector machines. In Advances in Neural Information
Processing Systems (NIPS) 16, volume 16, pages 49–56,
Vancouver and Whistler, Canada, 2003.

We thank the anonymous reviewers for their very helpful
comments. This work was partly supported by the NSFC
grants (61472141 and 61321064) as well as the Shanghai
Knowledge Service Platform Project (ZF1213).

8.

REFERENCES

[1] D. Barber. Are two classiﬁers performing equally? A treatment
using Bayesian hypothesis testing. Technical report, IDIAP,
2004.
[2] D. Barber. Bayesian Reasoning and Machine Learning.
Cambridge University Press, 2012.
[3] B. Carterette. Bayesian inference for information retrieval
evaluation. In Proceedings of the 2015 International
Conference on the Theory of Information Retrieval (ICTIR),
pages 31–40, Northampton, MA, USA, 2015.
[4] J. Demšar. Statistical comparisons of classiﬁers over multiple
data sets. The Journal of Machine Learning Research
(JMLR), 7:1–30, 2006.
[5] J. M. Dickey and B. P. Lientz. The weighted likelihood ratio,
sharp hypotheses about chances, the order of a markov chain.
The Annals of Mathematical Statistics, 41(1):214–226, 1970.
[6] T. G. Dietterich. Approximate statistical tests for comparing
supervised classiﬁcation learning algorithms. Neural
Computation, 10(7):1895–1923, 1998.
[7] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J.
Lin. LIBLINEAR: A library for large linear classiﬁcation.
Journal of Machine Learning Research, 9:1871–1874, 2008.
[8] C. Goutte and E. Gaussier. A probabilistic interpretation of
precision, recall and F -score, with implication for evaluation.
In Proceedings of the 27th European Conference on IR
Research (ECIR), pages 345–359, Santiago de Compostela,
Spain, 2005.
[9] J. A. Hanley and B. J. McNeil. The meaning and use of the
area under a receiver operating characteristic (ROC) curve.
Radiology, 143(1):29–36, 1982.
[10] D. A. Hull. Improving text retrieval for the routing problem
using latent semantic indexing. In Proceedings of the 17th
Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR), pages
282–291, Dublin, Ireland, 1994.
[11] H. Jeﬀreys. Theory of Probability. Oxford University Press,
3rd edition, 2000.
[12] T. Joachims. A support vector method for multivariate
performance measures. In Proceedings of the 22nd
International Conference on Machine Learning (ICML),
pages 377–384, Bonn, Germany, 2005.
[13] D. Koller and N. Friedman. Probabilistic Graphical Models Principles and Techniques. MIT Press, 2009.
[14] J. K. Kruschke. Bayesian estimation supersedes the t test.
Journal of Experimental Psychology: General, 142(2):573,
2013.

24

