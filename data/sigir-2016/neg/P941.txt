Pearson Rank: A Head-Weighted Gap-Sensitive
Score-Based Correlation Coefficient
Ning Gao, Mossaab Bagdouri, and Douglas W. Oard
University of Maryland, College Park
{ninggao, mossaab, oard}@umd.edu

ABSTRACT

tween two ranked lists of system scores (e.g., the reference and the
approximated).
When making comparisons in the aforementioned cases, we focus on two considerations. First, we prefer the comparison to be on
an interval rather than ordinal scale. In shared tasks, the systems are
ranked according to their measured effectiveness on a test collection. In addition to system ranks, it is important to know whether
some systems are substantially better than the others. Moreover,
for formative evaluation it can be useful to characterize small incremental improvements, even when the new evaluation scores do
not alter a systemâ€™s rank with regard to other systems. For this reason, we prefer a correlation coefficient that is sensitive to the size
of the gaps between system scores. Second, we care more about
comparisons among the best systems, so we prefer a correlation
coefficient that is influenced more by differences near the top of a
ranked list of systems. We call such measures head-weighted.
Several statistics have been proposed to quantify the correlation
between two ranked lists of scores. Pearsonâ€™s Ï [4] assumes that
the scores are on an interval scale (i.e., one in which all score differences of the same size have the same meaning). Mean Average Precision can properly be treated as being on an interval scale
because precision is interval and expected values computed on interval scales are interval. Kendallâ€™s Ï„ [3] and Yilmaz et al.â€™s Ï„AP
[6], by contrast, make the weaker assumption of an ordinal scale
in which score differences are not necessarily informative, but the
relative ordering of systems by those scores is informative.
Gao and Oard suggest a head-weighted gap-sensitive correlation
coefficient called Ï„GAP [2], giving greater penalty to larger gaps
when a swap occurs, but assigning no penalty for misestimating
the gap size if no swap occurs. Consider, for example, the situation
depicted in Figure 1, which occurred in an unpublished systemablation study in which we were studying the reuse of a test collection by systems that did not contribute to a judgment pool. The
x-axis is the reference system score (measured in this case as mean
precision at rank one) obtained using relevance judgments for the
unique contributions of every system, while the y-axis is the score
that we estimated for that same system without using any relevance
judgments for documents uniquely contributed by that system. As
Pearsonâ€™s Ï = 0.89 shows, the estimates exhibit a strong linear correlation with the reference scores. Note, however, the substantial
gap in the reference scores between the best and second-best systems; the estimate does not preserve that gap. Pearsonâ€™s correlation
coefficient is dominated by the many other well approximated gaps,
however, and does not emphasize the problem. Kendallâ€™s Ï„ = 0.74
is affected by several swaps in the preference order, but as the high
value of Ï indicates, most of these swaps are among systems with
small gaps that should not trouble us much. There are several reversals near the top of the ranked list that affect Ï„AP = 0.64, although

One way of evaluating the reusability of a test collection is to determine whether removing the unique contributions of some system
would alter the preference order between that system and others.
Rank correlation measures such as Kendallâ€™s Ï„ are often used for
this purpose. Rank correlation measures are appropriate for ordinal measures in which only preference order is important, but many
evaluation measures produce system scores in which both the preference order and the magnitude of the score difference are important. Such measures are referred to as interval. Pearsonâ€™s Ï offers
one way in which correlation can be computed over results from
an interval measure such that smaller errors in the gap size are preferred. When seeking to improve over existing systems, we care the
most about comparisons among the best systems. For that purpose
we prefer head-weighed measures such as Ï„AP , which is designed
for ordinal data. No present head weighted measure fully leverages
the information present in interval effectiveness measures. This paper introduces such a measure, referred to as Pearson Rank.

Keywords
Evaluation Metric; Correlation Coefficient

1. INTRODUCTION
In information retrieval evaluation, we often wish to compare the
effectiveness of alternative systems by using some single-valued
evaluation metric (e.g., F1 or Mean Average Precision) on some
publicly available collections. Judgments of all the items in the
collections are required to get the ground-truth evaluation of the
systems, which is often infeasible. Hence, sampling or pooling
techniques are used in shared tasks (e.g., TREC, CLEF) to choose
the documents that will be assessed. Naturally, we want to quantify
the adequacy of test collections created this way for assessing the
effectiveness of different systems, especially systems that did not
participate to the creation of the pool [5]. We might also want to
know whether we can approximate the effectiveness of these systems by reducing the number of relevance judgments [1]. We can
attempt to answer these questions by measuring the correlation bePermission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

SIGIR â€™16, July 17 - 21, 2016, Pisa, Italy
c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
âƒ
ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2914728

941

Approximated P@1

1.0
0.8
0.6
0.4

Ï  
Ï„
 
Ï„"1  
Ï„("1  

0.2
0.0
0.0

0.2

0.4

0.6

0.8

Ïr,3 =

2.2

2 iâˆ’1
2
âˆ‘iâˆ’1
j=1 (x j âˆ’ xi ) âˆ‘ j=1 (y j âˆ’ yi )

âˆ‘ (x j âˆ’ xi ) Â· (y j âˆ’ yi )

j=1

â‰¤

iâˆ’1

iâˆ’1

j=1

j=1

âˆ‘ (x j âˆ’ xi )2 âˆ‘ (y j âˆ’ yi )2 ,

(2)

where the two sides are equal if and only if X and Y are linearly
dependent (or, in a geometrical sense, they are parallel). Since the
scores are scaled xi , yi âˆˆ [0, 1], we have
âˆ’1 â‰¤ !

âˆ‘iâˆ’1
j=1 (x j âˆ’ xi ) Â· (y j âˆ’ yi )

2 iâˆ’1
2
âˆ‘iâˆ’1
j=1 (x j âˆ’ xi ) âˆ‘ j=1 (y j âˆ’ yi )

â‰¤ 1,

(3)

where the upper bound 1 is reached when Y = X, and lower bound
-1 is reached when Y = 1 âˆ’ X. Therefore,
m

âˆ’1 =

m
xi
âˆ’xi
â‰¤ Ïr â‰¤ âˆ‘ m
= 1.
i=2 xi
i=2 âˆ‘i=2 xi

âˆ‘ âˆ‘m

i=2

(4)

T HEOREM 2. Let X = {x1 , Â· Â· Â· , xiâˆ’1 , xi , Â· Â· Â· , x pâˆ’1 , x p , Â· Â· Â· , xm } be
a reference score list; Y 1 = {x1 , Â· Â· Â· , xi , xiâˆ’1 , Â· Â· Â· , x pâˆ’1 , x p , Â· Â· Â· , xm }
and Y 2 = {x1 , Â· Â· Â· , xiâˆ’1 , xi , Â· Â· Â· , x p , x pâˆ’1 , Â· Â· Â· , xm } be two lists of approximated scores, where Y 1 has only one swapped adjacent item
pair siâˆ’1 and si near the head of the lists; Y 2 has only one swapped
adjacent item pair s pâˆ’1 and s p near the end of the list. If the score
difference between the swapped pairs are identical (xiâˆ’1 âˆ’ xi ) =
(x pâˆ’1 âˆ’ x p ), the Ïr score for Y 1 with error near the head will be
lower than Y 2 with error near the end.
P ROOF. Let Îµ = 1 âˆ’ Ïr be the loss of a list of approximated
scores, therefore, Îµ = âˆ‘m
i=2 Îµi where Îµi is the loss for each position
from 2 to m. For Y 1 , we have Îµ2 = Â· Â· Â· = Îµiâˆ’2 = 0;

. (1)

â›
â
âˆ‘iâˆ’2
xiâˆ’1 â
j=1 (x j âˆ’ xiâˆ’1 )(y j âˆ’ yiâˆ’1 )
â 
Îµiâˆ’1 = m
Â· 1âˆ’ !
âˆ‘i=2 xi
2 âˆ‘iâˆ’2 (y âˆ’ y
2
(x
âˆ’
x
)
)
âˆ‘iâˆ’2
j
j
iâˆ’1
iâˆ’1
j=1
j=1
â›
â
iâˆ’2
âˆ‘ j=1 (x j âˆ’ xiâˆ’1 )(x j âˆ’ xi )
xiâˆ’1 â
â .
= m
Â· 1âˆ’ !
âˆ‘i=2 xi
âˆ‘iâˆ’2 (x âˆ’ x )2 âˆ‘iâˆ’2 (x âˆ’ x )2

Figure 2 is a toy example for calculating Ïr of two lists, where
X = {x1 , x2 , x3 } and Y = {y1 , y2 , y3 }. The solid arrows show the
score differences between item pairs considered when calculating
Ïr at x2 . The value of Ïr at x2 is:

Ïr,2 =

Properties

iâˆ’1

Let S = {s1 , Â· Â· Â· , sm } be a list of m items ranked in descending order by their reference scores X = {x1 , Â· Â· Â· , xm }; and Y = {y1 , Â· Â· Â· , ym }
be their approximated scores, after scaling all of the scores so that
xi , yi âˆˆ [0, 1]. We define the new (asymmetric) Pearson Rank correlation coefficient (Ïr ) X and Y as:
âˆ‘iâˆ’1
j=1 (x j âˆ’ xi ) Â· (y j âˆ’ yi )

x3
(x1 âˆ’ x3 )(y1 âˆ’ y3 ) + (x2 âˆ’ x3 )(y2 âˆ’ y3 )
"
Â·"
.
x2 + x3
(x1 âˆ’ x3 )2 + (x2 âˆ’ x3 )2 (y1 âˆ’ y3 )2 + (y2 âˆ’ y3 )2

P ROOF. Due to the Cauchy-Schwarz inequality, we have:
$2
#

2.1 Definition

!

y3

T HEOREM 1. The value of Ïr is always between âˆ’1 and 1.
Ïr = 1 if and only if Y = X; and Ïr = âˆ’1 if and only if Y = 1 âˆ’ X.

This section defines Pearson Rank, demonstrates its satisfaction
of desired properties, and contrasts it with other correlations.

m

y2

The value of Ïr is the sum of Ïr,i at items from 2 to m = 3.

2. PEARSON RANK

âˆ‘

y1

The dotted arrows show the score differences between item pairs
considered for Ïr at x3 . The value of Ïr at x3 is:

as Ï„GAP = 0.91 shows, these reversals are among systems that had
very small differences in the reference condition.
Intuitively, this seems like the wrong answer, since if we were
to make a system that was considerably better than the second-best
one, we have no reason to believe that the effectiveness score estimates on the y-axis would reflect that. None of the standard measures capture the fact that the difference we should care the most
aboutâ€”the large gap in the reference system scores between the
best two systems, is completely mischaracterized by the approximated scores as being of only negligible gap size. Pearsonâ€™s Ï
misses this because it is not head-weighted, Ï„AP misses this because
it is not gap-sensitive, and Ï„GAP misses it because it is gap-sensitive
only when a swap occurs. None of these measures focus on what
we care about the most when we have an evaluation measure that is
interval, which is that the size of the gaps among the best systems
be correctly estimated.
This example illustrates the need for a new correlation coefficient that is at the same time head weighted and sensitive to both
swapped and unswapped gaps. Section 2 introduces Pearson Rank
(Ïr ), our novel correlation coefficient, and shows that it has several desirable properties. Through extensive simulation, Section 3
contrasts some behaviors of Ïr with those of rank-based correlation
coefficients. We conclude in Section 4.

1

x3

1.0

Figure 1: Different correlation coefficient values when the gap
between the best system and a lower one is mischaracterized.

Â· xi Â·
âˆ‘m
i=2 xi i=2

x2

Figure 2: Example for calculating Ïr correlation coefficient.

Reference P@1

Ïr (Y |X) =

x1

j=1

x2
(x1 âˆ’ x2 )(y1 âˆ’ y2 )
Â·"
.
x2 + x3
(x1 âˆ’ x2 )2 (y1 âˆ’ y2 )2

j

iâˆ’1

j=1

j

i

(5)

942

Let n = x j âˆ’ xiâˆ’1 and c = xiâˆ’1 âˆ’ xi , the derivative of
!

âˆ‘iâˆ’2
j=1 (x j âˆ’ xiâˆ’1 )(x j âˆ’ xi )

(6)

2 iâˆ’2
2
âˆ‘iâˆ’2
j=1 (x j âˆ’ xiâˆ’1 ) âˆ‘ j=1 (x j âˆ’ xi )

Ï
Ï„
Ï„AP
Ï„GAP
Ïr

could be represented in an alternative form of
âˆ‚

)

âˆš âˆ‘ n(n+c)
2

âˆ‘ n âˆ‘(n+c)2

âˆ‚n

*

=

3((1 + 2n)(1 + 2n(2 + n)) + c(3 âˆ’ 4n2 (2 + n)))
,
âˆš
( n((2 + n)(1 + 2n)(3 + 2n))3/2 )

2.3

(7)

j

i

j=1

j

iâˆ’1

As can be seen, the value of Îµi decreases with increasing i. The
penalty for Îµtâˆˆ[i+1,m]
Îµtâˆˆ[i+1,m] =

xt

âˆ‘m
i=2 xi

â›

Â· â1 âˆ’ !
â›

âˆ‘tâˆ’1
j=1 (x j âˆ’ xt )(y j âˆ’ yt )

2 tâˆ’1
2
âˆ‘tâˆ’1
j=1 (x j âˆ’ xt ) âˆ‘ j=1 (y j âˆ’ yt )

â

3.

â 

=

xt
âˆ‘m
i=2 xi

Â·

(xiâˆ’1 âˆ’ xi )2
2
âˆ‘tâˆ’1
j=1 (x j âˆ’ xt )

(9)

Îµiâˆ’1 + Îµi + âˆ‘m
i+1 Îµt

depends only t. Therefore, the value of Îµ =
larger when the swapped error appears in the head of the list.

is

T HEOREM 3. Let X = {x1 , Â· Â· Â· , xiâˆ’1 , xi , Â· Â· Â· , xm } be a reference
score vector, and Y = {x1 , Â· Â· Â· , xi , xiâˆ’1 , Â· Â· Â· , xm } be an approximation list with one adjacent swapped pair xiâˆ’1 and xi , then the value
of Ïr is inversely related to the score difference between the swapped
pair (xiâˆ’1 âˆ’ xi ).
n(n+c)
P ROOF. The derivative of âˆš âˆ‘ 2

âˆ‘ n âˆ‘(n+c)2

âˆ‚

)

âˆš âˆ‘ n(n+c)
2

âˆ‘ n âˆ‘(n+c)2

âˆ‚c

*

=

with respect to c

3c(1 + n)(1 + 2n)(1 âˆ’ n2 )
((1 + n)(1 + 2n)(1 + 6c + 6c2 + 3n + 6cn + 2n2 ))1.5

(10)

is negative. Therefore, from equations (5) and (7), we can conclude that the value of Îµiâˆ’1 is positively related to the value of
2
with respect to c
c = xiâˆ’1 âˆ’ xi . The derivative of âˆš 22c
2
âˆ‘ n âˆ‘(n+c)

âˆ‚

)

âˆš

2c2
âˆ‘ n2 âˆ‘(n+c)2

âˆ‚c

*

=

SIMULATION EXPERIMENTS

In Section 2.2 we proved some properties of Ïr when a swap
occurs between two items. Another aspect we explore next is its
reaction to score changes that maintain the ranking of the systems.
One way of characterizing the behavior of Ïr is to simulate cases
in which the rank of each system is held invariant but the gap sizes
are allowed to differ in some systematic way. When we do this,
the rank correlation coefficients we have considered (Ï„ , Ï„AP , Ï„GAP )
show a perfect correlation of 1. Some approximations are better
than others, however, and we can use simulation to characterize the
behavior of Ïr in such cases. We assume that the reference and
approximated scores follow some distributions. To implement this
condition, we sample N systems from the distribution of the reference scores X , and sort them in a descending order of scores. We
scale these scores to the interval [0, 1] by subtracting the minimum
score before dividing by the difference between the maximum and
minimum scores, yielding a vector X. In a similar manner, we generate a vector of the ranked scaled approximated scores Y , from a
distribution Y . We compute the Ïr for this pair of vectors, and
repeat this process 100,000 times.
We study three different distributions, but others could have been
considered as well. In the uniform distribution (U[0,1] ), the gaps
between different systems are equal (in expectation). In the normal
distribution (N(0.5,1) ), a few systems would have a score close to
each end of the ranked list, while the majority of the systems would
be located around the middle of the score distribution. With Zipfâ€™s
distribution (Z(231 âˆ’1,2) ), a few systems would have a score close
to the head of the ranked list, while most of the systems would be
clustered towards its tail. Each scatter plot in Figure 3 corresponds
to the median Ïr , for a pair of distributions, out of the 100,000 pairs
of score vectors corresponding to 50 simulated systems. We also
indicate in that figure the different quartiles of the values of Ïr .
We first observe that Ïr is more likely to have a high value when
both of the reference and approximated scores follow the uniform
distribution (Figure 3(a)). In fact, the differences between the scores
are equal in expectation. The next highest Ïr values appear to
be those of the normal distribution (Figure 3(b)). The gaps there
are bigger (in expectation) closer to either end. Thus, the penalty
(which is weighted towards the head of the list) is expected to be

â
âˆ‘ jâˆˆ[1,n], jÌ¸=(iâˆ’1,i) (x j âˆ’ xt )2 + 2(xiâˆ’1 âˆ’ xt )(xi âˆ’ xt )
â 
!
= m
Â· â1 âˆ’
âˆ‘i=2 xi
2 tâˆ’1
2
âˆ‘tâˆ’1
j=1 (x j âˆ’ xt ) âˆ‘ j=1 (x j âˆ’ xt )
xt

Analysis

Table 1 shows the properties of our proposed Ïr and the other
correlation measures. As can be seen, all the measures can be applied with ordinal effectiveness measures, but only Ï„GAP , Ï and
Ïr leverage the fact that typical information retrieval effectiveness
measures are interval. Ï„GAP , however, is a hybrid that treats the
reference scores as interval but the approximated scores as ordinal.
Ï„AP , Ï„GAP and Ïr are head-weighted measures giving more weights
to the items at the head of the list. As a result, all the head-weighted
measures are asymmetric, yielding different results if the roles of
the reference and the approximation are swapped. If a symmetric
measure is desired, the results in both directions could be averaged.
For example, we could define Ïr (X,Y ) = (Ïr (X|Y ) + Ïr (Y |X))/2.

which is positive when both n and c are positive. Therefore, the
value of equation (6) increases with increasing i; and thus the value
of Îµiâˆ’1 decreases with increasing i. Similarly,
â›
â
(x
âˆ’
x
)(y
âˆ’
y
)
âˆ‘iâˆ’1
j
i
j
i
xi
j=1
â 
Îµi = m
Â· â1 âˆ’ !
âˆ‘i=2 xi
2 âˆ‘iâˆ’1 (y âˆ’ y )2
(x
âˆ’
x
)
âˆ‘iâˆ’1
i
i
j=1 j
j=1 j
(8)
2
2(xiâˆ’1 âˆ’ xi )
xi
Â·!
.
= m
âˆ‘i=2 xi
âˆ‘iâˆ’1 (x âˆ’ x )2 âˆ‘iâˆ’1 (x âˆ’ x )2
j=1

Table 1: Property comparisons.
Ordinal Interval Head Weighted Symmetric
yes
yes
no
yes
yes
no
no
yes
yes
no
yes
no
yes
yes
yes
no
yes
yes
yes
no

12cn2 (n + 1)(2n + 1)(6c2 + 9cn + 9c + 4n2 + 6n + 2)
[n2 (2n2 + 3n + 1)(6c2 + 6cn + 6c + 2n2 + 3n + 1)]1.5

(11)

is positive. Therefore, from equation (8) and (11) we can conclude
that Îµi is positively related to c = xiâˆ’1 âˆ’ xi . Similarly, equation (9)
also shows that the value of Îµt is positively related to c = xiâˆ’1 âˆ’
xi . As a conclusion, when excluding the effect of item weights
(xi / âˆ‘m
i=2 xi ), the approximation list is penalized more when swapping item pairs with larger score differences.

943

0.6
UI
TU
OE
SE
UI

0.2
0.0
0.0

0.2

0.4

0.6

0.8

0.8
0.6

0.2
0.0

1.0

0.0

Reference scores: uniform

0.6
UI
TU
OE
SE
UI
0.0

0.2

0.4

0.6

0.8

Reference scores: zipf

(d) Zipfâ€™s vs. Zipfâ€™s

1.0

Approximated scores: normal

Approximated scores: zipf

0.8

0.0

0.4

0.6

0.8

0.8
0.6

0.2
0.0

1.0

0.0

0.6
UI
TU
OE
SE
UI

0.0
0.0

0.2

0.4

0.6

0.8

0.4

0.6

0.8

1.0

Reference scores: normal

0.8

0.2

0.2

(c) Normal vs. uniform

1.0

0.4

UI
TU
OE
SE
UI

0.4

(b) Normal vs. normal

1.0

0.2

0.2

1.0

Reference scores: normal

(a) Uniform vs. uniform

0.4

UI
TU
OE
SE
UI

0.4

1.0

Reference scores: zipf

(e) Zipfâ€™s vs. normal

Approximated scores: uniform

0.4

1.0

Approximated scores: uniform

0.8

Approximated scores: normal

Approximated scores: uniform

1.0

1.0
0.8
0.6
UI
TU
OE
SE
UI

0.4
0.2
0.0
0.0

0.2

0.4

0.6

0.8

1.0

Reference scores: zipf

(f) Zipfâ€™s vs. uniform

Figure 3: Pearson Rank quartile values for simulated scores of 50 systems, and actual simulated scores for the median case.
larger than that of the uniform distribution. Figure 3(c) shows lower
values of Pearson Rank. In this case, the reference scores are drawn
from the normal distribution, which means there is (in expectation)
a high gap towards either end of the reference scores. However,
these gaps are lost in the approximated scoring space, as successive scores are expected to be equally distant.
We now turn to the bottom row of Figure 3, where at least one
of the axes follows Zipfâ€™s distribution. In this distribution, we expect that large gaps would appear only close to the head of the
ranked list. Figure 3(d) shows that Ïr can have a low value of 0.55
and a median of 0.95, even when both the reference and approximated scores are drawn from an identical distribution. This can
be explained by the high variance of the gaps near the head of the
scores. In Figure 3(e) we observe even lower Ïr scores (e.g., the
median value is 0.91). Although, both of the normal and Zipfâ€™s
distributions have a tendency for high gaps close to the head of
the score list, those of the Zipfâ€™s are (in expectation) much larger.
Thus, the gaps appear to get lost when the Zipfâ€™s scores are â€œconvertedâ€ into normal ones. Finally, the worst Ïr values are shown in
Figure 3(f), when very large gaps are expected to appear at the head
of the reference scores, while those of the approximated scores are
expected to be much smaller. In this case, we observe the lowest Ïr
value of 0.51, and a median of 0.87.

even when (as happened by construction in our simulation) rankbased metrics fail to detect any difference between reference and
approximated scores due to the consistent ranking of all systems.

ACKNOWLEDGMENT
This work was made possible by NSF award 1065250, and NPRP
grant # NPRP 6-1377-1-257 from the Qatar National Research Fund
(a member of Qatar Foundation). The statements made herein are
solely the responsibility of the authors.

References
[1] B. Carterette. Robust test collections for retrieval evaluation.
In SIGIR, pages 55â€“62, 2007.
[2] N. Gao and D. Oard. A head-weighted gap-sensitive
correlation coefficient. In SIGIR, pages 799â€“802, 2015.
[3] M. G. Kendall. A new measure of rank correlation.
Biometrika, pages 81â€“93, 1938.
[4] K. Pearson. Note on regression and inheritance in the case of
two parents. Proceedings of the Royal Society of London,
58(347-352):240â€“242, 1895.

4. CONCLUSION
We have proposed a novel head-weighted gap-sensitive scorebased correlation coefficient Ïr . By construction, Ïr gives more
weight to the items with higher reference scores. We have also
shown that Ïr more severely penalizes the swaps occurring near the
head of the list, and those with larger reference score gaps. Simulation experiments illustrate the sensitivity of Ïr to score values,

[5] E. M. Voorhees. Variations in relevance judgments and the
measurement of retrieval effectiveness. Information
processing & management, 36(5):697â€“716, 2000.
[6] E. Yilmaz et al. A new rank correlation coefficient for
information retrieval. In SIGIR, pages 587â€“594, 2008.

944

