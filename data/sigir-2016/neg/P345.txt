User Behavior in Asynchronous Slow Search
Ryan Burton

Kevyn Collins-Thompson

University of Michigan School of Information
105 S. State St.,
Ann Arbor MI, 48109

University of Michigan School of Information
105 S. State St.,
Ann Arbor MI, 48109

ryb@umich.edu

kevynct@umich.edu

ABSTRACT

[16, 12]. As a result, much semantic richness is discarded in
the process of retrieval, and much of the potential in terms
of relevance quality may not be realized. The implicit time
budget to which system developers must adhere also limits
the scope and eﬀectiveness of creative and useful extensions
that may be considered for search processing and interfaces,
such as enhanced personalization or novel ways of diversifying or summarizing results [10].
Slow search – the notion that a system may be able to
“take its time” to process results for increased eﬀectiveness
– has been proposed, but only at the level of advancing the
concept and exploring user attitudes to waiting for queries
[16, 17, 8]. In this paper, we present a study that investigates the eﬀect that an actual slow search system that
supports asynchronous (background) query processing has
on user behavior.
Search that focuses on speed, sometimes at the expense
of quality, may be underserving users with particular needs
or devices. For instance, the growth of mobile phone usage
is outpacing that of desktop PCs—especially in developing
countries—but there is a capability gap not only between
phones and PCs, but between diﬀerent phones as well. This
may lead to lower levels of information seeking and engagement [14]. This study would therefore be useful to search
engine implementers and interface designers targeting developing regions. Demonstrating the feasibility of this new slow
search paradigm would also encourage implementers of conventional search engines to further explore the importance
of the time–quality tradeoﬀ, potentially leading to more systems that can automatically adjust their performance along
a scale that eﬀectively trades oﬀ urgency and quality.
The contributions we present in this paper include an extensive analysis of a search system that embodies characteristics of slow search. We are primarily interested in the
practical value of trading speed for quality. To that end, we
developed a novel system which improves the topic relevance
of a query asynchronously over time while the user continues
to work. This allows us to investigate the types of tasks for
which users are willing to tolerate a delay in processing for
more relevant search results. Using log data, we show how
users behave when given asynchronous slow search capabilities and compare it to a baseline without these features.
We also trained a logistic regression classiﬁer to predict task
success depending on the capabilities given to the user and
interaction features. We also contribute an anonymized data
set1 to allow for analysis by other researchers.
As the primary purpose of this work is concerned with
understanding patterns of interaction behavior when users
have the ability to run a slow search in the background, we
consider the following research questions:

Conventional Web search is predicated on returning results
to users as quickly as possible. However, for some search
tasks, users have reported a willingness to wait for the perfect set of results. In this work, we present the ﬁrst study
to analyze users’ willingness to wait and their search success, when given a Web search system that embodies characteristics of slow search, where speed can be traded for an
improvement in quality. We conducted a between-subjects
user study involving tasks that required multiple queries to
complete, providing a Web search system that gave users
the option to additionally issue asynchronous queries for
which results improve in relevance over time as users continued working. We analyze the resulting survey results
and interaction log data to investigate how users spent their
time while waiting, and how behavior and search outcomes
changes when users are given the option of using a system
with asynchronous slow search capabilities. We ﬁnd that
when given a slow search system, users are able to perceive
the improvement in quality over time, and ﬁnd tasks to be
easier compared to a baseline conventional Web search system. Additionally, we ﬁnd that users continue to issue their
own queries and examine additional documents while the
slow search queries are processed in the background, and
use the slow search feature more eﬀectively as they gain exposure to its behavior across tasks. Our study signiﬁcantly
advances our understanding of the beneﬁts and tradeoﬀs involved in providing slow search scenarios for Web search.

Keywords
Search behavior; interactive information retrieval; user interfaces; slow search

1. INTRODUCTION
Current search systems are heavily optimized for speed:
commercial search engines often conspicuously display the
fraction of a second that it takes to return the list of results
to a query. Traditional systems take numerous shortcuts
for eﬃciency, such as making simplifying linguistic assumptions for query processing, document matching and ranking
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

SIGIR ’16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00


1

DOI: http://dx.doi.org/10.1145/2911451.2911541

345

http://umich.edu/˜ryb/slow

RQ1: What are the types of queries for which users initially report they would have a willingness to wait?
RQ2: How much time will users typically wait for results
from a slow query?
RQ3: In terms of search activity, how do users spend their
time while waiting for a slow query to ﬁnish?
RQ4: How does typical user behavior change when provided with the ability to run a slow query?
RQ5: Do users perform search tasks more eﬀectively with
slow search?

diﬀerences in ms/query can be quite appreciable. This shows
that there is often a beneﬁt to extra processing time, and a
system that takes advantage of this time when appropriate
could satisfy users better, provided they are willing to wait.
There is also increasing recognition of time as an important factor in the evaluation of search systems. Clarke and
Smucker [6] proposed a metric of time-based gain to measure
an information retrieval system’s eﬀectiveness to reﬂect the
value that a user gains over time in interacting with the system. For slow search, this metric is applicable to the value
gained from waiting as the system works to provide better
results. A recent user study by Crescenzi et al. looked at
a design somewhat contrary to ours, namely, the eﬀect on
search behavior when users were given less time to search [7].
Compared to the existing literature, this work presents a
working system that embodies the principles of slow search
and directly improves the relevance of search results, while
investigating the relationship between types of tasks, user
impatience, and quality improvement over time.

We address RQ1 in Section 3.2, and RQ2–RQ5 in Section 4.

2.

RELATED WORK

The concept of slow search was introduced to the literature by Dörk et al. [8] and Teevan et al. [16]. With inspiration from other “slow” movements, including slow food, slow
travel, and slow technology, the authors posit the changes in
how individuals and groups approach the process of search
if a system emphasized slowness over speed. Poirier and
Robinson [15] described a model of how slow principles may
be applicable to information behavior. These initial papers
provide insight in how proposed slow systems might be built,
or survey-based results on how users might be willing to use
such systems.
Previous work has shown that users would be willing to
engage in slow search for certain kinds of queries and tasks.
A study by Teevan et al. [16] based on user surveys and
empirical analysis of search query logs found that, while increased load times for search results led to increased abandonment for typical queries, for tasks in which result quality
was poor, users were willing to wait for better results or try
alternative methods of ﬁnding information. However, none
of this previous work built or studied a working slow search
system with real users: to our knowledge, our paper represents the ﬁrst study of how users interact with an actual
Web search scenario providing slow search features.
Asynchronous search has been studied previously, but primarily in the context of bandwidth limitations and without recognition to the notion of improving search results
[5]. Prior work has examined the relationship between time
delays and user behavior [3, 16, 13], but these were in the
context of conventional search systems, where users have
the expectation of rapid responses, and with no beneﬁt to
waiting.
Slow search also has parallels with question answering systems. Aperjis et al. [2] found that users wait longer to get
an additional answer after receiving a small number of responses on Yahoo! Answers. By analogy, a user performing
a search may be willing to repeatedly check in on the results
of a slow search as it builds a ﬁnal result set, and decide
whether to stop the search or continue waiting. However,
the authors do not show how the number of answers for a
question relate to their quality or the times in which they
arrive. Liu et al. [11] demonstrated in a ﬁeld experiment
that the frequency, quality and time of solutions to tasks on
the crowdsourcing site Taskcn reﬂect strategic decisions depending on the reward level, the existence of a reserve (i.e., a
prior high-quality solution), and expertise of crowdworkers.
The authors however do not investigate the waiting behavior of the requester in light of the solutions. In other words,
for slow search, we are interested in a mix of the two—
examining when a user believes the information received is
“good enough” to stop waiting for additional information.
Büttcher et al. [4] compared the eﬀectiveness of diﬀerent
systems while accounting for the CPU time involved in query
processing. Generally, the systems that used more CPU
time showed better results in eﬀectiveness. In the eﬃciency
task, comparing each system’s best run to its fastest run, the

3.

METHOD

To measure user behavior characteristics, we designed an
extension for the Chrome Web browser that works in conjunction with Web search engines to capture the current
query and send it to a server for extended processing when
the user clicks a “Work Harder” button to the right of the
main search editbox on the search engine page, as shown in
Figure 1. Doing this adds the query to a sidebar (a) on the
search engine result page, which shows a progress bar (b) as
well as the top three results at any given time (c). We call
this extended-time background query a ‘slow’ query. The
user may click on the “(more results)” link (d) at the bottom of the sidebar to view the full list of re-ranked results,
as they are improved and updated asynchronously by potentially adding new documents to the list and re-ranking them.
This page also displays a progress bar, and may be left open
while the user continues to search on the main search page.
This ‘slow’ query processing occurs as a background process, during which users are free to continue performing their
own searching and query reformulations in the main interface while the ‘slow’ query completes. In this study, we allow
at most one slow query at a time, which may be cancelled
before its progress is complete and removed from the sidebar. The extension also serves to log interaction through
queries, clicks, and mouse movements.

3.1

Study participants

Our study consisted of 44 participants (18 Male, 26 Female; mean age = 23.5 SD = 5.9), recruited through the
University of Michigan School of Information. Most were
undergraduates (n = 18) or holders of an undergraduate
degree (n = 12). The majority reported being very experienced with search engines: we asked about their familiarity
on a scale from one to ﬁve; the mean response was 4.6 (SD
= 0.6). Additionally, 38 reported using search engines more
than once per day, while 5 reported using them more than
once per week. The remaining participant reported using
them more than once per month. We also asked participants to report their conﬁdence in their abilities to ﬁnd the
information they need while searching on a scale from one
to ﬁve; the mean response was 4.36 (SD = 0.65).

3.2

Background Survey

To better understand tasks for which people might be willing to wait for a better answer (RQ1), we asked participants
to provide a description of the last search task they performed in which they failed in satisfying their information
need. We report these tasks as well as their anticipated
willingness to wait for the perfect results below.

346

We also categorized participants’ reported tasks according to the nature of information they were seeking. Overall,
16/44 (36%) of diﬃcult/unsatisﬁed needs involved searching
for speciﬁc items or facts that satisﬁed multiple attributes;
10/44 (22%) were questions seeking a speciﬁc factual answer;
4/44 (9%) needs were for the latest version of information;
4/44 (9%) involved searching for a person. The remaining needs involved more vaguely-deﬁned needs, more exploratory research needs, or procedural information on how
to solve a problem. This predominance of multi-attribute
search needs, the nature of which we can ﬁnd examined in
[10], motivated our design of tasks for slow search as described in Sec. 3.3.1.

3.2.2 User willingness to wait
As part of studying the time-quality tradeoﬀs that users
might ﬁnd acceptable in a search engine (RQ2), we asked
participants:
Given your experience, if a search system was able to provide
the perfect results, how long would you be willing to wait for
the search engine to process your query while you did other
tasks and you were notiﬁed when it found these results?

Figure 1: Interface with “Work Harder” button and
sidebar (a). Colors added for illustration. Clicking
the “Work Harder” button in the upper right adds
the current query to the queue (b). The top three
results at any moment are presented below (c), and
a full list of re-ranked results is available by clicking
on (d). These interface additions are always present.

3.2.1

We are interested in what users self-report as acceptable,
not only to calibrate our experiment, but also to see if their
actual behavior matches the expected behavior (which we
compare in Sec. 4.1). Users reported a willingness to wait
9.5 minutes on average (SD = 13.2). An exponential decay in acceptable waiting time is evident from our analysis:
with the survey response data, we ﬁtted an exponential decay model w = exp(−at) to estimate the empirical probability w that a typical user would be willing to wait at least
t minutes. The ﬁtted exponential parameter was a = 0.11,
meaning that for every additional minute of waiting time,
about 10% of remaining users were not willing to continue
waiting. We note that this rate of decay ‘impatience factor’ is in accord with that reported by Teevan et al. [16]
that asked a similar question about willingness to wait for
perfect results.

Prior tasks users reported as difficult

We ﬁrst asked participants the following question:
Think back to the last time you had trouble ﬁnding information with a Web search engine. What was the information
that you were trying to ﬁnd? Please be as speciﬁc as you
can, as best as you can remember.
We coded responses by topic, summarized in Table 1.
As most participants were students, the majority had issues ﬁnding information for classes or assignments. The
common trends for diﬃcult education-related needs involve
ﬁnding new and novel information (e.g., ﬁnding articles on a
topic that has not been seen before), ﬁnding reliable scholarly articles on a topic, and expressing the problem in the
right way for the search engine to yield useful results (“It
was diﬃcult to search for because I wasn’t sure what I was
searching for.”).
For other topics, users had diﬃculty ﬁnding a speciﬁc
item, such as a person, product, or song, most commonly
among the Career, Entertainment, and Shopping topics. The
main issue in these cases involved expressing the right criteria to ﬁnd these items. For instance, one subject tried to ﬁnd
a particular drawer slide, but was not able to use the right
search terms. Instead, he had to iteratively search related
topics in order to pick up more useful search terms.
Topic
Education
Shopping
Entertainment
Health
Career
Technology/Troubleshooting
Food
Sports

3.3

Experiment Design

For the purpose of this study, we focused on investigating
the eﬀects of an improvement in relevance for multi-attribute
tasks. To that end, we implemented a server that communicates with the Chrome extension to simulate an improvement in relevance over the course of ﬁve minutes for each
slow query submitted. For each task that a user may choose
to tackle, we manually selected ﬁve to ten high-quality documents and snippets that, collectively, allow a participant
to correctly solve the problem posed by the task. When the
“Work Harder” button is used, the server selects documents
from the pool to insert into the ranking every twenty seconds. Similarly, another process on the server periodically
moves high-quality documents closer to the top of the ranking over the course of the ﬁve minute period, until these
documents reach the top of the ranking.
We randomly assigned participants to one of three conditions. In the baseline condition (n = 16), the interface
resembles a conventional Web search engine, with no “Work
Harder” button or sidebar. In the “static gain” condition
(n = 15), the interface adds a persistent “Work Harder” button and sidebar to the conventional interface. Furthermore,
the system inserts highly-relevant documents in the middle
of the ranking “below the fold” of the re-ranked results page
and the rank position of these documents stays the same
over the course of the ﬁve minutes. Finally, in the “dynamic
gain” condition (n = 13), the interface is the same as in the
“static gain” condition, but the system inserts documents at

Count
16
6
5
5
3
3
2
2

Table 1: Topics of tasks reported as diﬃcult.

347

the last position of the re-ranked list and then continuously
increases the position of documents at 20 second intervals,
over the ﬁve minute time window, until they ﬁnish at the top
of the ranking. In this study, we used a dynamic gain that
was linear with respect to time. With this design, we introduce the two new capabilities of an improved result list and
a dynamic ranking. We chose to contrast the “static gain”
condition with the “dynamic gain” condition to determine
whether users actually perceived the improved relevance as
well as to study the eﬀect of the dynamic ranking.

3.3.1

As motivation to ﬁnish the task within the allotted thirty
minutes, we compensated users based on their performance
in answering each question, which called for an answer that
addressed each attribute of the problem, as well as three
relevant documents that they found useful in solving the
problem. This gave us a way of verifying whether participants found the documents inserted into the ranking, and
explicit relevance feedback of these documents. An answer
that perfectly met the criteria of the task led to a bonus
of $2 with partial credit being possible, and giving relevant
documents led to a bonus of $1 per URL – this served as
motivation to give explicit relevance feedback.

Description of Search Tasks

Participants were presented with a list of four topics, with
each topic having three tasks within it. Each participant
was required to select one task from two separate topics. We
allowed participants to choose tasks and topics of interest to
them with the goal of increasing their intrinsic motivation
to complete each task.
We prepared the total set of twelve tasks such that each
task was presented in the form of a question to be answered,
and each task called for the participant to ﬁnd ﬁve items that
satisfy multiple attributes speciﬁed within the problem. We
did this to control the cognitive eﬀort required for each task
– users were expected to ﬁnd a set of candidate answers and
verify that each of them satisﬁed all constraints in order to
receive the full reward. For any particular item that the
user submitted in their answer, we considered it “correct”
if and only if it satisﬁed all of the required constraints. We
believed that having a slow search system which reduced this
high expected eﬀort would encourage use of that capability
when available.

3.4 Data Preparation
Missing Data. For each request made by the extension
to log interaction data, the system associated a session ID
with a particular interaction event, with this session ID being linked to the user’s ID, which is randomly generated
when a user begins the study. After the data collection was
complete, there were 34 out of 1149 clicks without session
IDs in our log database, and hence, we were unable to associate these clicks with a task, user ID, or condition. We
therefore manually inspected the click data in an attempt
to re-associate each click with a session ID. We were able to
re-associate all clicks but one due to ambiguity in candidate
tasks: 13 of 34 clicks were recoverable from session IDs included in page URLs, and 20 of 34 clicks could be manually
recovered based on analyzing clicks with session IDs from
closely associated contemporaneous queries.
Relevance Judgements. As a part of completing the
task, we asked users to provide three relevant documents
that helped in answering the task’s question. We made ﬁnal
judgements on these documents in the process of calculating bonuses – if the document indeed provides information
relevant to answering the question correctly, then the document was deemed relevant for the bonus. Otherwise, we
considered the document not relevant.2

Local Businesses (32 tasks completed)
Name ﬁve I.T. companies in Ann Arbor with at least 50 employees.
Entertainment (28 tasks completed)
Name ﬁve video games in which Pharrell Williams’s music has
been featured.
Education (21 tasks completed)
Who are the ﬁve most inﬂuential professors in the United
States in the ﬁeld of sociology?
Shopping (7 tasks completed)
What are ﬁve smartphones that are thinner than a standard
No. 2 pencil and usable on AT&T?

4.

4.1 How Long Participants Waited For Results
Our second research question (RQ2) concerns the amount
of time users typically wait for results. In our background
questionnaire described in Section 3.2.2, our participants expressed a mean willingness to wait 9.5 minutes for the perfect results for their diﬃcult information needs. Our system
imposes a maximum wait time of ﬁve minutes, which was
not explicitly communicated to our participants. Five minutes was selected in order to give users more time to submit multiple slow queries within a single task session. 2/15
participants in the static gain condition and 2/13 participants in the dynamic gain condition explicitly mentioned
without prompting that the waiting time was too long when
asked about their impressions of using the system in posttask questionnaires.
Analyzing the behavioral data, we ﬁnd that in both the
dynamic gain and static gain conditions, approximately twice
as many participants used slow search in the second task
(n = 11) compared to the ﬁrst task (n = 5). Table 3 presents
expected wait times for each condition and task, representing the time for which each slow query is processed until

Table 2: Examples of search tasks and their topics.
In Table 2 we present examples of search tasks for each of
the four topics, along with the number of task completions
by topic. Local Businesses had the most interest, with its
tasks being chosen 32 times in total. Conversely, Shopping
received the least attention, which users choosing these tasks
only 7 times.

3.3.2

EXPERIMENT RESULTS

In this section we conduct an analysis of user activity, addressing the remaining research questions (RQ2–RQ5) and
examining users’ behavior in more detail.

Study Procedure

The user study took place in a laboratory setting at the
University of Michigan School of Information. Participants
volunteered to attend one of eight study sessions, with each
session lasting a maximum of 90 minutes. Each participant
was placed at a computer set up with the Chrome extension, which in turn was randomly associated with one of the
three study conditions (Baseline, static gain, and dynamic
gain). Participants completed two search tasks, with each
task lasting a maximum of thirty minutes.
To introduce participants to the capabilities of the system
before they began the ﬁrst task, users were asked to perform
an exploratory search task—in this case to explore the topic
‘snow leopards’ —as a warmup for ﬁve to ten minutes.

2
If the document only provides information that would result in answering the question incorrectly, as, for example,
only providing a Web page for a coﬀee shop outside of the
location we ask, the page was considered not relevant.

348













& 
$ 

#



$#



$



% 
" 




! 



 
 


















'()*






+,-./01 2.,13,4
5/67 2.,13,4
5/67 2.,13,4 809:,//,;

<69=+,/,>09? 8/3:@4
+,/,>09? 8/3:@4
A3-B/C +,/,>09? 8/3:@4





D7,// EF?,1 8/3:@
G03? EF?,1 2.,1C
D7,// EF?,1 <69=:/3:@ H9?,10:?369

Figure 2: The actions that users perform over the course of the two tasks by condition. The black lines show
the proportion of remaining participants in the session.
Condition
dynamic
dynamic
static
static

gain
gain
gain
gain

Task
1
2
1
2

Query Processing
Time (sec)
227.3
227.2
136.1
266.6

SD

Users

109.8
82.4
145.6
85.0

4/13
3/13
5/15
1/15

users performed and averaged across users for each minute
of activity. The resulting plot of these aggregated actions
is shown in Figure 2. Generally, participants took slightly
longer to complete their ﬁrst task than their second (944
seconds vs. 804 seconds on average). For the two tasks, the
users in the dynamic gain condition had the shortest completion times (879 seconds for the ﬁrst task, and 735 seconds
for the second task). These diﬀerences were not statistically
signiﬁcant.
Diﬀerences between ﬁrst and second session. In
general, there appeared to be a period of slight acclimatisation as users in the slow search conditions made and cancelled slow queries throughout the session. By comparison,
in the second task, users started by making slow queries and
committed more to this decision rather than cancelling and
restarting. More precisely, in the static gain condition, users
made an average of 0.53 slow queries and cancelled 0.33 of
them in the ﬁrst task. By the second task, they made 0.93
slow queries and cancelled 0.07 of them. Similarly, in the
dynamic gain condition, they made an average of 0.62 slow
queries in their ﬁrst task and cancelled 0.23 of them; by their
second task, they made 1 slow query and cancelled 0.08 of
them. Potentially, this small number of slow queries could
reﬂect an optimal interaction strategy, which we will discuss
further in Section 4.4. We present the results for comparison
in Table 4.
Document assessment over time. In Figure 3, we plot
the mean cumulative clicks performed during each task as
time progressed. We can see that the conditions began with

Table 3: Mean slow query processing times by task,
with standard deviation (SD) and fraction of users
who cancelled their slow query.

either completion, or cancellation by a user. Users in the
dynamic gain condition waited an average of 227 seconds in
both tasks 1 and 2. By comparison, for users in the static
gain condition, their wait times increased from an expected
136.1 seconds to 266.6 seconds: participants cancelled more
queries in the ﬁrst task than in the dynamic gain condition,
but by the second task, these users cancelled fewer than in
the dynamic gain condition, which likely contributed to the
increase in mean slow processing time observed for the static
gain condition. An independent two-group t-test shows that
the diﬀerence between the mean wait time in the ﬁrst and
second tasks of the static gain condition is statistically signiﬁcant (t(9.7949) = −2.318, p < 0.05).

4.2

How Participants Spent Their Time

To obtain an overview of user activity as participants progressed through each task, we aggregated the actions that

349

Condition
dynamic gain
dynamic gain
static gain
static gain

Task
1
2
1
2

Submitted
0.62
1.00
0.53
0.93

Cancelled
0.23
0.08
0.33
0.07
















Table 4: Slow queries submitted/cancelled by task.

+ 
*
) 
(

&



!



$



#



 
#


"


!


















!









 
 






 











"%







"#$%&%$'














!









 
 





















"#$%&%$'

,-,./01023,4.516578
/01023,4.516578

96:91;./01023,4.516578
/0:<13/.=<0/608

516578




 


Figure 4: How users spend their time while waiting
for slow queries to ﬁnish.









 
' ( 




 



)*+,-./,
01/*2.3 4*./
56*6.3 4*./

Most activity in this interval is focused on examining documents: the number of clicks is relatively high in the ﬁrst
task (6.6 for static gain and 7.6 for dynamic gain). In
comparison, the number of queries is relatively low (2.2 for
static gain and 3.8 for dynamic gain). By the second task,
users do less in this interval, perhaps relying on the system
more than examining documents and conducting additional
queries themselves. In both conditions, the average number
of queries and clicks both decrease (static gain: Queries =
1.55, Clicks = 3; dynamic gain: Queries = 0.91, Clicks =
3.09). Of additional note is that for static gain, the number of relevant clicks dropped from 1.2 to 0.18, while the
number of highly relevant clicks stayed relatively consistent
(0.6 in task 1 and 0.55 in task 2). This may indicate that
users’ time was better spent in the second task with regards to ﬁnding the most relevant documents to solving the
given task. We can compare this to dynamic gain, where
the number of relevant clicks increased (0.4 to 0.73), and
also the number of highly relevant clicks increased from 0
to 0.36. Thus, while the number of non-relevant clicks and
queries decreased, users made better use of their time in
ﬁnding helpful documents.

Figure 3: Average time-click curves by task. This
includes non-relevant clicks.

rather diﬀerent click trajectories at the beginning of each
task, but eventually converged as the task continued. Users
in the dynamic gain condition begin examining documents
slightly later than in the baseline and static gain conditions
in the ﬁrst task. Comparing this to the second task, users
in the three conditions examined fewer documents in the
corresponding period at the beginning of the ﬁrst task. Additionally, users in the baseline condition began examining
more documents sooner than in the slow conditions.

4.2.1 Behavior While Waiting
Our third research question (RQ3) pertains to activity
while waiting for slow search results to ﬁnish processing. To
answer this question, we looked at how users continued to
interact with the system after submitting a slow query. We
note that for the conditions with the slow search button,
many users spent their time waiting after submitting a slow
query. This was especially pronounced in their ﬁrst ﬁve minutes of each task, where more users waited on average after
making a query than at subsequent time periods, Comparing the two slow search conditions in the ﬁrst task, we see
in Figure 2 that more users spend time waiting after querying in the ﬁrst ﬁve minutes of dynamic gain than in static
gain. However, the proﬁles are more similar by the second
task: in both groups, users made heavy use of the “Work
Harder” button initially, waiting before eventually clicking
on results.
Figure 4 shows the activities that users performed while
slow queries were processing for the dynamic gain and static
gain conditions, i.e. after a slow query was submitted, and
before the query either ﬁnished processing or was cancelled.

4.3

Feature Analysis of Search Behavior

To investigate research questions RQ4 and RQ5, we computed a list of features characterizing search behavior, as
inspired by previous studies such as [1]. The features we
calculated are outlined in Table 5 and Table 6.
Dwell time (CIT ) was determined by calculating the time
between a click and any subsequent interaction with a search
page (mouse movements, scrolling, keyboard events, queries,
or clicks). As we ask users to provide ﬁve correct items
that satisfy multiple attributes for each task, we calculate
Precision as the proportion of items included in their answer
for a task that satisfy all attributes.

4.3.1

Behavioral Analysis of Searchers by Condition

For our fourth research question (RQ4), we investigate the
types of changes seen in users’ behavior when given asynchronous slow search capabilities. Having randomly assigned

350

Feature

SG

DG

Baseline features
Session length (ΣΔt*, 839.47 807.23
sec.)
Regular queries (CntQR ) 6.50
4.58
Regular
queries/sec 0.01
0.01
(QR P S)
Slow features
Slow queries (CntQS )
0.73
0.81

Slow queries per second
(QS P S)
Slow queries cancelled
(CntQS C)
Slow queries cancelled
per second (QS CP S)

9.03×
10−4
0.33

3.86×
10−4

9.99×
10−4
0.35

3.65×
10−4

Query features
length 4.48
5.20

Query
word
(QW L)
Query character length 27.14
31.20
(QCL)
Click features
Pages in session (CntR)
13.27
11.62
Clicks per query (CP Q)
3.14
3.23
Time to ﬁrst click (sec.) 24.93
25.53
(QDT )
Dwell Time (sec.) (CIT )
234.36 276.28
Outcomes
Reward ($)
4.21
4.04
Reward Variance ($)
1.13
1.12
Precision
0.85
0.72
Click Relevance
4.03
3.08

Baseline

U
Test

961.00

-

8.81
0.01

-

0.00

0.00
0.00

0.00

Feature

SG >
B*;
DG >
B*
SG
B*;
DG
B*
SG
B*;
DG
B*

5.72

-

34.88

-

14.48
2.37
24.46

-

358.86

-

4.16
2.14
0.83
3.55

-

>
>
>
>

NS
Baseline features
Session length (ΣΔt*, sec.)
847.69

S

U Test

891.10

Regular queries (CntQR )
7.53
Regular queries/sec (QR P S)
0.01
Slow query features
Slow queries (CntQS )
0.39
Slow queries per second (QS P S)
5.22 ×
10−4
Slow queries cancelled (CntQS C) 0.14
Slow
queries
cancelled/sec 2.97 ×
(QS CP S)
10−4
Query features
Query word length (QW L)
5.01
Query character length (QCL)
28.90
Click features
Pages in session (CntR)
12.44
Clicks per query (CP Q)
2.55
Dwell Time (sec.) (CIT )
284.20
Time to ﬁrst click (sec.) (QDT )
13.65

6.20
0.01

NS <
S*
-

Reward ($)
Reward Variance ($)
Precision
Click Relevance

0.57
6.72 ×
10−4
0.27
1.65 ×
10−4

-

5.23
32.67

-

13.75
3.14
296.36
32.86

NS <
S*

Outcomes
3.23
1.70
0.52

4.74
0.36
1.00

3.42

3.69

NS <
S***
-

-

Table 6: Comparison of behavioral features by success level. NS = Not Successful; S = Successful.
*p<0.05; **p<0.01; ***p<0.001.
result quality for the slow search conditions may contribute
to these cross-condition diﬀerences and exploring these is a
topic for future work.

4.3.2

Behavioral Analysis of Successful Searchers

Our ﬁfth and ﬁnal research question (RQ5) investigates
whether users perform tasks more eﬀectively with the help
of slow search. As Table 5 shows, users in the dynamic gain
condition received the smallest reward, and had the lowest
precision. This raised the question of what factors played a
part in increased performance. We compared user features
for successfully completed tasks to those of the remaining
tasks. We deﬁne success as a precision of 1 for a particular
task, such that the answers given satisﬁed all the criteria set
by the task’s question. We present the features computed
based on success in Table 6. We performed Mann-Whitney
U tests to determine whether there were signiﬁcant diﬀerences by success.
We found signiﬁcant diﬀerences in session length (ΣΔt;
p < 0.05) and time to ﬁrst click (p < 0.05). The average
session length for successful tasks (891.10 seconds) was signiﬁcantly higher than that for less successful tasks (847.69
seconds). Despite this, the number of queries issued is not
signiﬁcantly diﬀerent, though the average time to ﬁrst click
is signiﬁcantly higher for the successful (343 seconds) than
for the less successful (260.44 seconds). This indicates that
the time spent examining the search results was a major factor in success, as we also notice that the dwell times were
not signiﬁcantly diﬀerent. We also investigated click relevance, as described in Section 4.2. Users who were successful had an expected click relevance of 3.69, compared to a
click relevance of 3.42 for the rest of the users. However,
this diﬀerence was not statistically signiﬁcant.
We also performed logistic regression to predict user success using the above feature set. We found that the intercept
(β = −6.524) and the clicks per query (β = 0.887) were sig-

Table 5: Comparison of behavioral features across
conditions. SG = Static Gain; DG = Dynamic Gain.
*p<0.05; **p<0.01; ***p<0.001.

users into a condition either with or without such capabilities, we compare the session-level features for each condition.
We present the values of these features in Table 5.
Compared to the two slow search conditions, users in
the baseline condition on average were the slowest in completing a session (ΣΔt = 961 seconds), issued the highest
number of queries (CntQR = 8.81) and the longest queries
(QW L = 5.72; QCL = 34.88), and had the longest dwell
time (CIT = 358.86). These diﬀerences were not statistically
signiﬁcant, but they may reﬂect a greater degree of eﬀort for
users in this condition, as users take more time to examine
and possibly evaluate documents, and conduct more queries
to address the various facets of the problem. We also found
that users did indeed make use of slow search when given
the option: features that quantify the use of slow search such
as CntQS , QS P S, CntQS C, and QS CP S were signiﬁcantly
greater than zero in the dynamic gain and static gain conditions (p < 0.05). For most of these features (that is, CntQS ,
QS P S, and CntQS C), the values were highest in dynamic
gain, though not signiﬁcantly more so than in static gain. In
contrast, QS CP S was highest in the static gain condition,
though this was not statistically signiﬁcant. No other differences were signiﬁcant (with all tests here based on paired
Mann-Whitney U tests with Bonferroni correction). Cognitive factors such as the evolving degree of user trust in

351

Component
Queries
Time to “Work Harder” (sec.)
Wait time (sec.)
Snippets examined

niﬁcant predictors of success (p < 0.05). For the intercept,
a participant is not likely to be successful, with all other
predictors held constant. An additional click per query increases the odds of success by 143%. Other predictors that
were marginally signiﬁcant (p < 0.1) include the rate of regular queries (QR P S, β = −0.009, p < 0.1) and the eﬀect of
session length when the condition is static gain (β = −0.009,
p = 0.1). In the case of the query rate, an increase in this
rate predicts an increase in the odds of success, whereas an
increase in the session length in the static gain condition
predicts a decrease in the odds of success. No other terms
were signiﬁcant predictors.
Overall, the logistic regression analysis shows that making
good use of one’s time is the main factor in success. That
is, searching and examining documents in a short period of
time usually means that the user will be successful. The
interaction between the session length and using the static
gain system also suggests that, as a longer session length
implies diﬃculty in satisfying an information need, not being
able to take adequate advantage of the system’s assistance
decreases the likelihood of success.

4.4

Task 1
6.4
386
136.1
NA4

Task 2
6.6
130.1
277.3
22.5

p-value
0.479
0.005 **
0.002 **

Table 7: Changes in interaction in relation to optimal strategies for static gain. The number of snippets examined is estimated. *p<0.05; **p<0.01;
***p<0.001.
Component
Queries
Time to “Work Harder” (sec.)
Wait time (sec.)
Snippets examined

Task 1
5.38
190.8
266.6
21.2

Task 2
3.68
135.9
277.1
20.1

p-value
0.131
0.04 *
0.048 *
0.5

Table 8: Changes in interaction in relation to optimal strategies for dynamic gain. The number of
snippets examined is estimated. *p<0.05; **p<0.01;
***p<0.001.

Analysis of Interaction Strategies

Because the ability to perform a slow search was a new
feature for participants – the training period built into the
start of the study not withstanding – we examined how participants’ choice of search strategies changed across sessions
as users became more familiar with the feature.
In particular, we were interested in how users in the two
slow conditions adapted their decision-making and use of
the feature in relation to more optimal strategies. Each of
the slow conditions could be considered to have an optimal
strategy in terms of the number of regular queries issued,
the number of snippets examined, the time taken to invoke
the “Work Harder” button for the ﬁrst time, and the waiting
time for slow processing.
For the dynamic gain condition, we consider one optimal
strategy to be the following: 1. Issue a query; 2. Click “Work
Harder”; 3. Wait for 5 minutes as the results automatically
improve to maximum eﬀectiveness; 4. Examine the ﬁrst 10
slow results3 . In comparison, the static gain condition has
a very diﬀerent strategy: 1. Issue a query; 2. Click “Work
Harder” 3. Examine the ﬁrst 30 slow results immediately.
The diﬀerences stem from the fact that the static gain condition happens to improve relevance immediately, but to a
much lesser degree than in the dynamic gain condition at
100% completion. Thus, for the static gain condition, it is
in the user’s best interest not to wait, but this is not evident
from the interface.
To examine how behavior changed relative to these strategies, we analyzed whether these strategic components shifted
toward optimality from the ﬁrst task to the second task, in
each condition. To do this, we estimated the number of
snippets examined by using time on page from our baseline condition to determine the time to examine one snippet
(8s), and used the time on page from the improved results
page with slow results with the assumption that the times
to examine a snippet are comparable. This value is capped
at the number of results on the page (50). We employed a
bootstrap hypothesis testing procedure [9], and present our
ﬁndings in Tables 7 and 8.
Table 7 shows for that for static gain users, the time it
took for users to ﬁrst use slow search signiﬁcantly decreased
from the ﬁrst to second task: from 386 s to 130 s (p < 0.01).

Wait time increased signiﬁcantly from 136.1 s to 277.3 s (p <
0.01). We see users moving closer to the optimal strategy
for when to invoke slow search, but not for wait time. The
number of queries did not change signiﬁcantly, but increased
slightly from 6.4 to 6.6. This could suggest that users did
not know to take advantage of the fact that the preselected
documents were always in the middle of the ranking, and
continued to search on their own even as they waited more
for an eﬀect.
Table 8 shows that in the dynamic gain condition, users
invoked slow search much sooner for the second task (190.8
s to 135.9 s; p < 0.05), and signiﬁcantly increased their
waiting time (266.6 s to 277.1 s; p < 0.05). Additionally,
although not statistically signiﬁcant, we notice a decrease in
the number of queries issued (5.38 to 3.68). This seems to
suggest that these users had begun adjusting their behaviors toward the optimal strategy, as they developed a better
mental model of how the system responded to their use of
the “Work Harder” button.

4.5

Post-task Survey Results

After each task, we asked users about their experience
using the system. We also asked participants in conditions
having the “Work Harder” button to give their impressions
on whether the button made the task easier, whether they
noticed an improvement in the quality of results, as well as
to write about their thoughts on the usefulness and ease of
use of the system.
Figure 5 shows the mean ratings on a ﬁve point Likert scale of participants’ experiences of using the system
by condition. The error bars represent the standard errors of the means. We performed ANOVAs on these results to see if exposure to the diﬀerent conditions aﬀected
the ratings given to whether they were able to ﬁnd the information they were looking for, their productivity, the effort extended, if they liked using the system, if the button
made the task easier, if the button improved the quality of
results, if the progress bars were useful, and if the ability
to check the intermediate results was useful. Of these, we
saw signiﬁcant diﬀerences between conditions in response
to the button making the task easier (F (1, 54) = 5.324,
p < 0.05), the button improving the quality of the results
(F (1, 54) = 4.529, p < 0.05), and the progress bars being
useful (F (1, 54) = 5.146, p < 0.05).
In general, users’ perceptions of the slow search features
(the latter four in Figure 5) were higher in the dynamic gain

3
The times taken for each step would be as short as possible,
and a user might elect to do other things, including regular
searches, during the waiting interval.
4
Insuﬃcient data due to lack of use of this feature during
the session.

352

celled fewer slow queries by the second task. A future study
may manipulate the processing time for these slow queries
to examine users’ tolerance for waiting, and whether users
will wait under tighter time constraints. Future studies may
also look at impatience under greater uncertainty.
Users spent time looking for additional documents while
waiting (RQ3). As illustrated in Section 4.2, users in both
the dynamic gain and static gain conditions spent their time
performing queries and clicking on documents in the interval while a slow query was processing. By the second task,
these activities were reduced, but not in a statistically signiﬁcant sense. We also showed that users in both of these
conditions performed more slow queries in the second task
and also waited more after performing these queries instead
of clicking on documents or cancelling. This may indicate
that users could still have been learning to use the system
by the second task despite the training period and using the
system for the ﬁrst task. The reasons why users appeared to
make more eﬀective use of slow search by the second task require further study: the change could be due simply to their
experience with the system in the ﬁrst task, or it could be
due to their increased awareness of the feature due to our
explicitly asking users about their experience in using the
“Work Harder” button between tasks. A future study may
extend the training period to ensure that users are not only
familiar with the system, but that they are also conﬁdent
in predicting what the system will do. We also plan to do
a longer-term online study in which users interact with the
system for an extended period of time, which will help us
to determine how long it takes for user behavior to stabilize and what it looks like when it does. Such a study will
also help to understand usage in diﬀerent scenarios without
artiﬁcial constraints.
User search behavior did not signiﬁcantly change with additional slow search capabilities (RQ4). Our analysis in Section 4.3.1 showed that user behavior in terms of search interactions was similar across conditions, with users in the two
slow conditions making signiﬁcant use of the “Work Harder”
button. We observed that users in the baseline condition
took longer to complete sessions, conducted more and longer
queries, and clicked on fewer documents per query (Table 5).
Users, by the end of the study, may have still not yet fully
understood the capabilities of the system. However, these
results may also indicate that slow search systems should
cater to similar types of queries as current search systems,
and support the kinds of interactions that users have grown
accustomed to. A future study may serve to tease out these
diﬀerences by looking at users who have become familiar
with the system and users without such a system.
Users did not achieve higher ﬁnal eﬀectiveness with slow
search, but showed evidence of higher eﬃciency (RQ5). For
the tasks we evaluated, users achieved comparable ﬁnal rewards across the three conditions, with the baseline condition showing slightly higher average reward, but overall
diﬀerences were not statistically signiﬁcant. However, as we
note above, users obtained these rewards in less overall time
for both slow search conditions compared to the baseline
condition, giving some evidence of higher eﬃciency. In addition, users in the dynamic gain condition did indeed report
that they noticed more of a diﬀerence in the improvement
in search results than users in the static gain condition, and
gave higher ratings for the usefulness of the progress bars.
This may have been because it would have been clearer in
the dynamic gain condition that the results were changing,
and continued to change during the ﬁve minute duration. In
contrast, users in the static gain condition may have not noticed the change between the unmodiﬁed and the modiﬁed
results. Regardless, this shows that users are able to notice the diﬀerence when the results change, suggesting there


























 









 






!"#$%&'$

















()")&* +"&'


































,-'".&* +"&'

Figure 5: Post-task survey scores by condition.
condition than in the static gain condition. Among the more
general experiential questions (the former four), we see that
users in the static gain condition gave the highest rating
for whether they found the information they were looking
for (M = 4.43, SD = 0.90), while in the dynamic gain
condition they gave the lowest rating (M = 4, SD = 1.17).
For productivity, we see that having the button improved
users’ perceptions over the baseline (M = 3.88, SD = 1.04),
with the static gain condition having the slight edge (M =
4.1, SD = 0.84) over the dynamic gain condition (M = 4.07,
SD = 1.01). Users in the baseline condition also reported
exerting the most eﬀort (M = 3.19, SD = 1.31), which
might have been reﬂected in their interactions, with users in
this condition taking longer on average to complete tasks,
perform more queries, and examine documents. Compared
to the other conditions, users in the static gain condition
reported liking the system the most (M = 3.57, SD = 0.97).
Indeed, among these former four questions, the static gain
condition has the highest ratings, though, once again, the
diﬀerences were not statistically signiﬁcant.

5.

DISCUSSION AND IMPLICATIONS

Our study provides insights about how users engaged with
a slow search system that provided an asynchronous query
capability with improvements in search result quality over
time. We now discuss our main ﬁndings and implications.
Users are willing to wait for multi-attribute queries (RQ1).
We found through our background survey that many of the
tasks and queries that users typically have trouble with
are multi-attribute queries in which various constraints of
a query must be satisﬁed (Section 3.2.1). This justiﬁes our
use of such queries in our study, and the use of slow search
after users gain familiarity with the system shows that multiattribute queries are a good ﬁt for a slow search system.
Users will typically wait for results (RQ2). Our background survey revealed that users reportedly are willing to
wait for a mean of 9.5 minutes for “perfect” results (Section 3.2.2). Placing users under time pressure and imposing
a maximum time of ﬁve minutes for query processing also
led to users waiting, as was seen in Section 4.1. Interestingly,
in both dynamic gain and static gain conditions, users typically submitted more slow queries, waited more, and can-

353

is some utility in having future systems expose progressive
improvements in ranking to users.
We note that users found slightly fewer relevant documents on average in the dynamic gain condition compared
to the static gain condition. One explanation for this diﬀerence is that in the static gain condition, the system inserts
all of the same documents that would have been inserted in
the dynamic gain condition, but in the lower quarter of the
ranking. As a result, static gain users had the opportunity
to gain access to these documents more quickly if they were
willing to look for them in the ranking. In the dynamic gain
condition, however, users had to wait longer to see the same
highly relevant results, since the system begins with those
documents at the bottom of the initial ranking and improves
their position steadily over the course of ﬁve minutes.
While our dataset and corresponding analysis has allowed
us to gain insight into the research questions we posed, we
also recognize a number of limitations in our current study.
Our ﬁndings, particularly that of RQ1, would be more robust with a larger sample of users. A future study in a more
natural setting may also reduce experimental demand eﬀects
that might have inﬂuenced user behavior, and users’ choices
of tasks may have also aﬀected their performance.
For future work, there are multiple possible avenues in exploring user interaction with slow search systems The ‘Work
Harder’ button might be removed altogether and replaced
with a background process that can automatically ﬁnd and
attempt to improve results for failed or abandoned search
sessions. A user with low time pressure and a high degree
of trust in a slow search system may submit a query to be
processed in the background while performing non-search
tasks, especially in the transition between devices, in which
case supporting the examination of intermediate results or
performing more queries in the interim would not be vital.
In another instance, a user may use the system as as a supporting agent in a search task: the system would gather
additional relevant documents and present them to the user
as they continue to search. The participants’ tendency in
this study to continue searching shows that this is a useful
capability to have.
The concept of “slowness” could additionally be applied to
many diﬀerent scenarios in which other aspects of retrieval
‘quality’ may be improved. This study focused on improving relevance for multi-attribute queries—a diﬃcult class of
queries for many existing systems, but in principle a system
could also improve intrinsic diversity, employ crowdsourcing
to augment algorithms, or summarize and organize results.

on the tasks we evaluated. Followup studies may explore
diﬀerent mechanisms to improve quality for slow search and
further investigate the nature of time-quality tradeoﬀs and
user choice.
Acknowledgements. We thank Eytan Adar, Yan Chen,
and the anonymous reviewers for their feedback.

References
[1] M. Ageev, Q. Guo, D. Lagun, and E. Agichtein. Find it if
you can: a game for modeling diﬀerent types of web search
success using interaction data. In Proceedings of SIGIR
2011, pages 345–354. ACM, 2011.
[2] C. Aperjis, B. A. Huberman, and F. Wu. Human
speed-accuracy tradeoﬀs in search. In System Sciences
(HICSS), 2011 44th Hawaii International Conference on,
pages 1–10. IEEE, 2011.
[3] J. Brutlag. Speed matters for Google web search. Google,
June 2009. URL http://bit.ly/1Oonkyz.
[4] S. Büttcher, C. L. Clarke, and I. Soboroﬀ. The TREC 2006
Terabyte track. In TREC 2006 Notebook, volume 6,
page 39. NIST Special Publication, 2006.
[5] J. Chen, S. Amershi, A. Dhananjay, and L. Subramanian.
Comparing web interaction models in developing regions.
In Proceedings of the First ACM Symposium on Computing
for Development, page 6. ACM, 2010.
[6] C. L. Clarke and M. D. Smucker. Time well spent. In
Proceedings of the 5th Information Interaction in Context
Symposium, pages 205–214. ACM, 2014.
[7] A. Crescenzi, D. Kelly, and L. Azzopardi. Time pressure
and system delays in information search. In Proceedings of
SIGIR 2015, pages 767–770, New York, NY, USA, 2015.
ACM.
[8] M. Dörk, P. Bennett, and R. Davies. Taking our sweet time
to search. In Proceedings of CHI 2013 Workshop on
Changing Perspectives of Time in HCI, 2013.
[9] R. Gould. Bootstrap hypothesis testing. Stats 110A, 2002.
URL http://bit.ly/1ncz67z.
[10] Y. Kim, K. Collins-Thompson, and J. Teevan. Using the
crowd to improve search result ranking and the search
experience. ACM TIST: Special Issue on the Crowd in
Intelligent Systems, 7(4):50, 2016.
[11] T. X. Liu, J. Yang, L. A. Adamic, and Y. Chen.
Crowdsourcing with all-pay auctions: A ﬁeld experiment on
Taskcn. Proceedings of the American Society for
Information Science and Technology, 48(1):1–4, 2011.
[12] M. L. Mauldin. Retrieval performance in FERRET: A
conceptual information retrieval system. In Proceedings of
SIGIR 1991, pages 347–355. ACM, 1991.

6. CONCLUSION
We reported on a user study that investigated ﬁve research
questions about user interaction with a slow search system
that oﬀered users the option of running a ‘slow’ query in the
background, showing progressive results in a sidebar. Using surveys and log data, we analyzed users who interacted
with the system in one of three between-subjects conditions:
a ‘dynamic gain’ condition that steadily improved search result quality of the optional slow query over the course of
ﬁve minutes, a ‘static gain’ slow query that inserted relevant documents immediately with no additional ranking improvements over time, or a baseline condition giving conventional Web search results. Our ﬁndings suggest that users
elected to perform slow search queries when given the opportunity. Additionally, we show that users are willing to wait
for multi-attribute queries (RQ1), users will indeed wait for
results when using slow search (RQ2), and users continued
to search while waiting for results (RQ3). User behavior did
not signiﬁcantly change with additional slow search capabilities (RQ4), and users did not achieve higher ﬁnal eﬀectiveness with slow search, but did ﬁnish in less time (RQ5)

[13] D. Maxwell and L. Azzopardi. Stuck in traﬃc: how
temporal delays aﬀect search behaviour. In Proceedings of
the 5th Information Interaction in Context Symposium,
pages 155–164. ACM, 2014.
[14] P. M. Napoli and J. A. Obar. The emerging mobile internet
underclass: A critique of mobile internet access. The
Information Society, 30(5):323–334, 2014.
[15] L. Poirier and L. Robinson. Informational balance: slow
principles in the theory and practice of information
behaviour. Journal of Documentation, 70(4):687–707, 2014.
. URL http://dx.doi.org/10.1108/JD-08-2013-0111.
[16] J. Teevan, K. Collins-Thompson, R. W. White, S. T.
Dumais, and Y. Kim. Slow search: Information retrieval
without time constraints. In Proceedings of HCIR 2013,
page 1. ACM, 2013.
[17] J. Teevan, K. Collins-Thompson, R. W. White, and
S. Dumais. Slow search. Communications of the ACM, 57
(8):36–38, 2014.

354

