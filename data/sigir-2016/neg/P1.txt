Understanding Human Language:
Can NLP and Deep Learning Help?
Christopher Manning
Stanford University

manning@stanford.edu
ABSTRACT
There is a lot of overlap between the core problems of information
retrieval (IR) and natural language processing (NLP). An IR
system gains from understanding a user need and from
understanding documents, and hence being able to determine
whether a document has information that satisfies the user need.
Much of NLP is about the same thing: Natural language
understanding aims to understand the meaning of questions and
documents and meaning relationships. The exciting recent
application of deep learning approaches in NLP has brought new
tools for effectively understanding language semantics. In
principle, there should be a lot of synergy, though in practice the
concerns of IR on large systems and macro-scale understanding
have tended to contrast with the emphasis in NLP on language
structure and micro-scale understanding.
My talk will emphasize the two topics of how NLP can contribute
to understanding textual relationships and how deep learning
approaches substantially aid in this goal. One basic – and very
successful tool – has been the new generation of distributed word
representations: neural word embeddings. However, beyond just
word meanings, we need to understand how to compose the
meanings of larger pieces of text. Two requirements for that are
good ways to understand the structure of human language
utterances and ways to compose their meanings. Deep learning
methods can help for both tasks. Finally, we need to understand
relationships between pieces of text, to be able to do tasks such as
Natural Language Inference (or Recognizing Textual Entailment)
and Question Answering, and I will look at some of our recent
work in these areas, both with and without the help of neural
networks.

Short Biography
Christopher Manning is a professor of computer science and
linguistics at Stanford University. His Ph.D. is from Stanford in
1995, and he held faculty positions at Carnegie Mellon University
and the University of Sydney before returning to Stanford. His
research goal is computers that can intelligently process,
understand, and generate human language material. Manning
concentrates on machine learning approaches to computational
linguistic problems, including syntactic parsing, computational
semantics and pragmatics, textual inference, machine translation,
and using deep learning for NLP. He is an ACM Fellow, a AAAI
Fellow, and an ACL Fellow, and has coauthored leading
textbooks on statistical natural language processing and
information retrieval. He is a member of the Stanford NLP group
(@stanfordnlp).

Keywords
Natural language processing, deep learning, word vectors,
compositionality, natural language inference, recognizing textual
entailment, question answering

Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. Copyrights
for third-party components of this work must be honored. For all other
uses, contact the Owner/Author.
Copyright is held by the owner/author(s).
SIGIR ’16, July 17 - 21, 2016, Pisa, Italy.
ACM 978-1-4503-4069-4/16/07.

DOI: http://dx.doi.org/10.1145/2911451.2926732

1

