A Complete & Comprehensive Movie Review Dataset
(CCMR)
Xuezhi Cao , Weiyue Huang, Yong Yu
Apex Data and Knowledge Management Lab
Shanghai Jiao Tong University

cxz,hwy,yyu@apex.sjtu.edu.cn
ABSTRACT

the goal is to improve user experiences by implementing corresponding recommender systems [8]. Matrix factorization
is the most widely used technique for this task [3]. For user
behavior analysis, researches mainly focus on detecting malicious users and bots in review sites (or fake review detection)
[7]. Because higher rating in review sites may lead to more
purchases, some merchants hire malicious users or bots to
give untruthful ratings. These fake reviews highly jeopardize
the user experiences and bring review sites into disrepute.
There are research works that tackle this by rating pattern
analysis [2], linguistic analysis [5] and other techniques.
Despite the satisfying research outcomes, there are still
research problems remain to be solved. For example, most
existing works on fake review detection focus on identifying each malicious user or fake review instance instead of
targeting directly at the final aggregated ratings, which are
the most important information the review sites provide for
users. Also, there is no work on analyzing the influences of
public opinions towards users (users are aware of the aggregated rating for the item before rating it in most sites). For
these directions, one major obstacle is the lack of suitable
dataset. To model the aggregated ratings, we requires complete rating logs. To analyze the influences of public opinions, we need snapshots of aggregated ratings right before
each rating action. However, currently there is no dataset
that satisfies these requirements.
Therefore, we assemble and publish a complete and comprehensive dataset CCMR for the community. Here complete indicates complete logs over all users instead of only a
subset of users, and comprehensive indicates the availability
of side information such as timestamps, item-side information, review texts and snapshots of the aggregated ratings.
In the following sections, we first describe the dataset and
its collecting methodology, and then compare it with existing datasets. Further, we propose two potential topics that
benefit from CCMR. We also conduct preliminary analysis
and experiments according to these topics.

Online review sites are widely used for various domains including movies and restaurants. These sites now have strong
influences towards users during purchasing processes. There
exist plenty of research works for review sites on various aspects, including item recommendation, user behavior analysis, etc. However, due to the lack of complete and comprehensive dataset, there are still problems that remain to
be solved. Therefore, in this paper we assemble and publish
such dataset (CCMR) for the community. CCMR outruns
existing datasets in terms of completeness, comprehensiveness and scale. Besides describing the dataset and its collecting methodology, we also propose several potential research topics that are made possible by having this dataset.
Such topics include: (i) a statistical approach to reduce the
impacts from fake reviews and (ii) analyzing and modeling
the influences of public opinions towards users during rating
actions. We further conduct preliminary analysis and experiments for both directions to show that they are promising.

CCS Concepts
•Information systems → Test collections; Spam detection; Personalization;

Keywords
Test Collection, User Behavior, Review Sites

1.

INTRODUCTION

Online review sites are now widely used for various domains. For example we have IMDb and MovieLens for movies,
Yelp for restaurants, TripAdvisor for hotels and attractions.
When making choices, nowadays lots of people refer to these
review sites for guidance. These sites have significant influences towards users during their purchasing processes.
Due to the importance, there are plenty of research works
in this direction. Most of existing works focus on rating prediction and user behavior analysis. For rating predictions,

2.

CCMR DATASET

The CCMR dataset can be freely accessed online1 . For
privacy concerns, we reorder the user ids as well as the movie
ids in CCMR. For review texts, we replace each word with
corresponding word id.
CCMR consists of movie rating logs from one of China’s
largest movie review sites. The data is collected and dumped
in May, 2015. All rating logs before the collection time are

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

SIGIR ’16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00

1

DOI: http://dx.doi.org/10.1145/2911451.2914669

661

http://dataset.apexlab.org/ccmr

Table 1: CCMR Compare with Existing Datasets
EachMovie
72,916
1,628
2,811,983
√

# Users
# Items
# Rating Actions
Timestamp
Item Info
Full Log
Aggregated Stats

×
×
×

Book-Crossing [10]
278,858
271,379
1,149,780
×
√
×
×

MovieLens
240,000
33,000
22,000,000
√

Netflix [1]
480,000
18,000
100,000,000
√

√
(partial)
×
×

×
×
×

CCMR
4,920,695
190,129
283,775,314
√
√
√
√

included. In total, it covers 190,129 movies and 4,920,695
active users (with at least one rating action). Among those
users and movies, we have 283,775,314 rating actions. On
average we have 1,471.92 rating logs per movie and 58.10
per active user. Besides the rating matrix, we also include
the corresponding review text, the date of the rating action
as well as the snapshot of aggregated statistics for the target
movie right before the rating action. For aggregated statistics, -1 is given when no statistic is available due to insufficient rating histories. We list and explain all data columns
for rating logs in Table 2. For each movie, we also provide director(s), actors, movie genre (action/romance/etc.),
release date and nation.

One of the most well known task based on rating logs
is rating prediction, which is also the core for recommender
systems in item-based sites. Matrix factorization is the most
widely accepted and used technique for this task [3]. As
this task has been heavily studied, there also exist several
well assembled datasets, e.g. Book-Crossing, EachMovie
and MovieLens from GroupLens2 and the Netflix dataset
[1]. Comparing to these datasets, CCMR’s advantage is
its scale, completeness and comprehensiveness. Therefore,
CCMR can be employed for wider scenarios and research
topics. A detailed comparison is listed in Table 1.

Table 2: Data Columns in CCMR - Rating Logs

Because review sites now play important parts during
user’s purchasing processes, the ratings are critical for the
product’s revenues. Driven by benefits, some merchants hire
malicious users or bots to generate fake reviews to increase
the rating of their products or to discredit their competitors.
As analyzed previously (Figure 1), malicious users widely
exist and have noticeable influences on the overall statistics.
Fake reviews not only have negative impact on user experiences but also bring the review sites disrepute. Therefore, this problem has aroused people’s attention and heated
discussion. There are works aiming at identifying the fake
reviews or malicious users. Jindal et al. focus on unusual
review patterns in [2]. Linguistic features in the review text
are also considered in [5]. Mukherjee et al. further go beyond single user and target at group spammers [6].
Despite the promising performance of existing works, they
only focus on identifying specific user or review instance. On
the other hand, users normally do not go through each review instance in detail. Instead, most users only focus on the
aggregated ratings. So we can eliminate the fake reviews’ influences by directly recovering the unbiased aggregated ratings. Another advantage of this is that spammers can not
easily find the rules and evolve accordingly while they can
for traditional approaches. Therefore, this would be a great
future direction for fake review detection. However, to the
best of our knowledge, currently there is no research work
following this direction.
One major reason is that currently we have no suitable
dataset to conduct this topic. We need the complete rating
logs to model the aggregated ratings, while current datasets
only provide logs over a subset of users instead of complete
logs.
Another problem is the lack of ground truth (the unbiased public opinion towards the item). By analyzing the
identified fake reviews (by analysis in Figure 1 and existing
techniques), we find that most fake reviews are close to the

Column
user id
movie id
rating
date
review text
stat avg
stat distrib 1
···
stat distrib 5

3.

Description
Re-ordered to 0-4,920,694.
Re-ordered to 0-190,128.
1-5 scale, and 0 indicates user mark it as
watched but no rating is given.
Date of the rating action
Review text of the rating action.
Average rating of the movie right before
the rating action.
Rating distribution of the movie right
before the rating action.
(Given in percentage)

We plot the distribution of user’s rating counts as well as
movie’s with log-log plots in Figure 1, from which we can
notice the power law or the long tail phenomenon. We can
also notice that in log-log plot for user, there is an unusual
streak above the main streak, i.e. an unusual large number
of users have exactly the same number of rating logs. By
detailed analysis, ratings from each of these users are mostly
conducted on the same day. So they are highly likely the
malicious users or bots.
0

0

10
Percentage of User

Percentage of Movie

10

−2

10

−4

10

−6

10

−2

10

−4

10

−6

10

−8

0

5

10

10
# Rating Actions

10

0

10

2

10
# Rating Actions

4

10

Figure 1: Log-Log Plot for Movie/User Action Frequency

2

662

FAKE REVIEWS

http://grouplens.org/datasets/movielens/

4.

movie’s release date (few weeks). Specifically, we define fk
to be the ratio of fake reviews posted within k days after
the release date (f+∞ =1), and nk accordingly for normal
reviews. We plot fk /nk in Figure 2 to show that fake reviews significantly shift towards the release date comparing
to normal reviews. This finding also matches with intuition
because only the ratings in first few weeks are crucial for the
movie’s box office. Therefore we consider the aggregated rating at a long time (years) after the movie’s release date as a
good approximation of the unbiased public opinion.
So the task can be formally stated as: given rating logs
within k days after the movie’s release date, estimate the unbiased public rating (approximated by the aggregated rating
at least one year after release date). We model the task as
regression problem and employ linear regression for it. For
features, we include the average ratings on day 1, day 2 up
to day k, and also movie-side information including directors, actors, genre and nations. Except for the real-valued
average ratings, one-hot encoding is used for the categorical
information.
For comparison, we implement baseline method by directly use the average rating as the estimation, which is the
method used in most review sites. By using Mean Absolute
Error (MAE) as the metric, we show the results in Figure 3.
As we can notice, performance of linear regression is noticeably better than the baseline. We believe there still exist
much space for improvement by employing more advanced
techniques. The results also indicate that the task is rather
challenging, especially for the very first few days.

In this section we focus on behavior analysis over legitimate users instead of malicious users. We propose another
future direction: to analyze and model the influences of public opinions towards each individual user in review sites.
The users are aware of the public opinion towards the item
before rating it. Recall the online rating process. In most
scenarios, users conduct the rating actions on the item’s detail page, in which the aggregated rating (given by average rating and rating distribution) is normally highlighted.
However, there is no research analyzing whether and to what
extent are users influenced by such public opinions.
There are several works in similar directions. Krishnan
et al. define this influence as Social Influence Bias (SIB)
in [4], and find statistically significant evidence of SIB on a
political-related rating system (a rather small dataset and
specific scenario). Behavior patterns behind voting actions
is targeted in [9]. They claimed that user actually votes for
whether the instance is ranked lower/higher than it deserves
according to his/her opinion.
However, there is no work modeling this in large-scale review sites. The main reason is that no existing dataset provides the snapshots of the aggregated ratings before each rating action. By assembling CCMR, we provide such dataset
for the community.
To show the existence of such influence (or SIB) in review
sites, we conduct the following preliminary analysis. We
randomly select popular movies with rather large variation
in their aggregated rating. For each movie, we represent
their rating logs as (ri , pi ) where ri indicates the rating of
ith action and pi indicates the public opinion (the aggregated rating) right before the action. We group the logs by
similar pi , and report the rating results from each group by
averaging. If users are not effected by public opinion, then
ri and pi should be independent and the mean of ri within
each group should be the same, which represents the true
public opinion towards the movie. We plot the results for
three movies in Figure 4. As we can see, dependency exists.
The phenomenon widely exists in most movies. Therefore,
we claim that the public opinion do have an noticeable influence on users.
To gain more insight, we model the user’s final rating by
linear combination over user’s actual preferences and the
public opinion. The combination parameter for each user
varies as some users are susceptible while the others are
not. We further integrate the model with traditional matrix
factorization (MF) [3]. The overall model as well as the loss
function are now as follows:
rij = (1 − wi )Ui · Vj + wi pij
X
(1)
L(w, U, V ) =
(Rij − rij )2 + γ(|w| + |U | + |v|)

1.16
Fake vs Normal

Ratio

1.14
1.12
1.1
1.08
1.06
0

10
20
30
40
50
# Days After Movie’s Release Date

Figure 2: Review Distribution, Fake vs Normal

Baseline
Linear Regression

Mean Absolute Error

0.8
0.7
0.6

ij

where rij is the predicted rating for user i on movie j. Ui ·Vj
models the user’s true preferences by user and item’s latent
factor according to MF. pij is the public opinion for movie j
just before the user i rates it. wi is the parameter indicating
to what extent user i is influenced by public opinion. Large
wi indicates the user to be more susceptible.
Evaluating it as rate prediction model, it has slight improvement comparing to MF. A relative improvement of
1.74% is achieved according to RMSE metric. As our goal
is to understand the influence of public opinion instead of
rating prediction, we focus on parameter wi . We plot the

0.5
0.4
0.3
0.2
0.1

INFLUENCE OF PUBLIC OPINION

2

4
6
8
10
12
# Days After Movie’s Release Date

14

Figure 3: Preliminary Result for Recovering Truthful Rating

663

4.2
4
3.8
3.6

movie − 469
3.8

4
4.2
Public Opinion

4.6
4.5
4.4
4.3
4.2

movie − 78512

4.1

4.4

4.4

Rating Result (Average)

4.4

Rating Result (Average)

Rating Result (Average)

4.8
4.6

4.7
4.6
4.5
movie − 80174
4.4
4.55

4.5
4.6
Public Opinion

4.6
4.65
4.7
Public Opinion

4.75

0.3

0.4

0.5

0.2

0.3

0

0.1

0.2

−0.5

w

1

w

w

Figure 4: Influence of Public Opinion on User Ratings

0

0.1
Average

−1

200

400
600
800
# Rating Histories

1000

−0.1

0

200
400
600
# Rating Histories

800

Variance
0

0

200
400
600
# Rating Histories

800

Figure 5: w vs # Rating Histories
analytical results regarding wi and the number of rating actions made by user i in Figure 5. On the left we plot each
user instance. For users with few actions, the distribution
seems like Gaussian. However, when shifting to users with
heavy activities, the distribution begins to fan out, especially to higher w. To be clear, we also plot the mean and
variance of w for users with different activity level. From
which we conclude that users with heavy activities are tend
to be more conservative and susceptible to public opinions.
And for normal users, the susceptible level is mostly due to
the user’s personality.
By these preliminary analysis, we show that influences
of public opinion do exist and there are interesting phenomenons in user behaviors. We believe that following this
direction, there are plenty of behavior patterns to be revealed. By understanding such patterns, we can model user
preferences more accurately and further improve personalized services such as recommender systems.

5.

dataset. For any suggestions to extend the dataset, please
contact the first author.

6.

REFERENCES

[1] J. Bennett and S. Lanning. The netflix prize. In
Proceedings of KDD cup and workshop, volume 2007,
page 35, 2007.
[2] N. Jindal, B. Liu, and E.-P. Lim. Finding unusual
review patterns using unexpected rules. In CIKM,
pages 1549–1552. ACM, 2010.
[3] Y. Koren, R. Bell, and C. Volinsky. Matrix
factorization techniques for recommender systems.
Computer, (8):30–37, 2009.
[4] S. Krishnan, J. Patel, M. J. Franklin, and
K. Goldberg. A methodology for learning, analyzing,
and mitigating social influence bias in recommender
systems. In RecSys, pages 137–144. ACM, 2014.
[5] T. Lappas. Fake reviews: The malicious perspective.
In Natural Language Processing and Information
Systems, pages 23–34. Springer, 2012.
[6] A. Mukherjee, B. Liu, J. Wang, N. Glance, and
N. Jindal. Detecting group review spam. In WWW
companion, pages 93–94. ACM, 2011.
[7] A. Mukherjee, V. Venkataraman, B. Liu, and N. S.
Glance. What yelp fake review filter might be doing?
In ICWSM, 2013.
[8] P. Resnick and H. R. Varian. Recommender systems.
Communications of the ACM, 40(3):56–58, 1997.
[9] R. Sipos, A. Ghosh, and T. Joachims. Was this review
helpful to you?: it depends! context and voting
patterns in online content. In WWW, pages 337–348.
ACM, 2014.
[10] C.-N. Ziegler, S. M. McNee, J. A. Konstan, and
G. Lausen. Improving recommendation lists through
topic diversification. In WWW, pages 22–32. ACM,
2005.

CONCLUSION

In this paper we assemble and publish a benchmark dataset
CCMR for the community. CCMR is a complete and comprehensive movie review dataset with over 283 million rating actions made by 4.9 million users over 190k movies. The
data is complete as it includes actions from all users instead
of only users in sampled subset. For comprehensiveness,
we provide timestamp, item-side information as well as the
snapshot of aggregated statistics for the item before each
rating action. Therefore, our dataset CCMR can support a
wider range of tasks. Further, we propose two novel tasks
that are made possible by employing CCMR dataset. One is
to eliminate effect of fake reviews by directly recovering the
unbiased aggregated rating, and the other is to analyze and
model the influences of public opinions towards each user.
We also conduct preliminary analysis and experiments for
these tasks. We will keep maintaining and updating this

664

