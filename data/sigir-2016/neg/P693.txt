GNMID14: A Collection of 110 Million Global Music
Identification Matches
Cameron Summers, Greg Tronel, Jason Cramer, Aneesh Vartakavi, Phillip Popp
Gracenote, Emeryville, CA, USA
csummers, gtronel, jcramer, avartakavi, ppopp@gracenote.com

ABSTRACT

formation is available [10]. However, no significant public
datasets exist for music identification, which provides a different perspective on music consumption behavior. Here we
introduce a dataset to fill this gap called the Gracenote Music Identification 2014 dataset, or GNMID14. Its unique
signal, large size and rich metadata such as mood, date, and
location make it a valuable resource on which to perform
IR experiments for music. In the following section, we describe existing related datasets. Section 3 characterizes the
dataset and proposes potential uses. Section 4 presents suggested applications. Section 5 presents results of an example
application. And Section 6 summarizes and discusses future
work.

A new dataset is presented composed of music identification matches from Gracenote, a leading global music metadata company. Matches from January 1, 2014 to December 31, 2014 have been curated and made available as a
public dataset called Gracenote Music Identification 2014,
or GNMID14, at the following address: https://developer.
gracenote.com/mid2014. This collection is the first significant music identification dataset and one of the largest music related datasets available containing more than 110M
matches in 224 countries for 3M unique tracks, and 509K
unique artists. It features geotemporal information (i.e.
country and match date), genre and mood metadata. In
this paper, we characterize the dataset and demonstrate its
utility for Information Retrieval (IR) research.

2.

Keywords
dataset; collection; music; geotemporal; content identification; fingerprint; mood; genre; music similarity; music consumption behavior

1.

RELATED DATASETS

While no significant public datasets exist for music identification at the time of this publication, there are several
datasets worth comparing in terms of content, size, geotemporal context, and track metadata. 30Music [12] - 31M
Last.fm user listening events - and #nowPlaying [14] - 58M
Twitter logs of listening streams - compare most closely in
content and size of MID14 but lack geographical information. Additionally, the structure inherent to music identification arguably makes the associated track metadata cleaner
because fingerprints uniquely identify a track with its metadata in a database. In contrast, matching large amounts of
text from Twitter logs has a number of challenges [14] and
Last.fm metadata comes from user tags, which can be noisy
[6].
The Million Musical Tweet Dataset (MMTD) [4] is another listening event dataset from Twitter and includes geotemporal data like that of GNMID14. However, it is about 100
times smaller in size and is subject to similar issues with
Twitter text matching. The Million Song Dataset (MSD) [1]
uniquely provides audio features among large music datasets
as well as some complementary datasets for satellite applications. Compared to GNMID14, MSD differs most notably
in its absence of geotemporal information and smaller scale.
Each of these other datasets provide user information where
GNMID14 does not, however, making them well-suited for
personalization applications. But for content-based information retrieval applications, GNMID14 fills a gap as a large
dataset with a unique music consumption signal and contextual information.
Beyond the music domain several text-based datasets exist that include geographical and temporal information such
as those used in tasks at Text Retrieval Conference (TREC)
and Semantic Evaluation (SemEval) [8, 13, 2]. Geotemporal

INTRODUCTION

The marriage of the internet and audio fingerprinting technology allowed, for the first time, massive numbers of people
to identify a recorded music track. Using a device such as
a smartphone, a person can easily identify a recorded track
being played and discover information like the artist’s home
country or even purchase the recording. A byproduct of
this identification process is a record that includes information such as timestamp, location on geo-enabled devices, and
track metadata of the identified music, and in aggregation,
these records create a unique music consumption signal.
Datasets of other music consumption signals, such as internet radio or microblogs (e.g. Twitter), have been successfully used in a variety of information retrieval applications
[9, 5]. They link contextual information with music content,
enriching retrieval interactions in search and recommendation systems, and they enable personalization when user inPermission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

SIGIR ’16, July 17 - 21, 2016, Pisa, Italy
c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2914679

693

information has fueled great strides in general information
retrieval tasks[11], and we hope that this dataset can similarly move research forward in the music domain.

3.

Figure 1: Distribution of genres across matches

THE GNMID14 DATASET

A music identification event consists of a query against
a database containing music fingerprints. A typical query
scenario involves a user hearing music being played aloud,
recording that music with a device such as cell phone, sending the query for identification, and then reviewing information about the identified music. The data in GNMID14 is
the resulting positive matches from these music identification queries for the whole year 2014. We refer the reader to
[3] for a closer review audio fingerprinting technology.

3.1

Curation

Figure 2: Distribution of moods across matches

GNMID14 was created through curation of a raw export
of matches from 2014 of the music identification database.
As this data is not a current product of the company, it
is provided to the research community with fewer curation
steps than is typically applied when data is licensed to another company. However, the authors believe that the resulting data is still cleaner than what is typically available
to researchers using large datasets in the music domain.
The initial export included the following fields: Date,
Country, Track ID, User ID, and from this several curation steps were performed. First, entries with incomplete
or missing fields were removed. All entries from users with
greater than 1000 total matches were also removed to reduce
the likelihood of non-human users. Next, the User ID was
dropped in its entirety to avoid privacy issues encountered in
other large datasets [7]. Next, duplicate tracks were removed
as extensively as possible using a combination of text and
fingerprint analysis. Finally metadata - track name, artist
name, 25 artist-level genres and 26 track-level moods - was
mapped to each Track ID.

3.2

Figure 3: Distribution of genres across tracks

Statistics

Table 1 provides some basic statistics on GNMID14. As
outlined in the table, cover for genre and mood 1 are less
than 100% of the total tracks. While this is typical of music
metadata at this scale, this coverage is relatively high.
Figure 1 and Figure 2 show the distribution of genre and
mood across matches. Additionally, Figure 3 and Figure
4 show the distribution of genre and mood across tracks.
Lastly, Table 2 shows the top 10 countries with the most
matches in the dataset.

Figure 4: Distribution of moods across tracks

Table 1: GNMID14 Statistics
Countries
224
Unique Tracks
3,061,121
Unique Artists
509,005
Matches
111,105,707
Matches with Genres 90%
Matches with Mood
94%

4.
1

SUGGESTED APPLICATIONS

This dataset offers a rich environment for variety of studies of the world’s music. General analysis of elements in the
dataset such as temporal genre distributions or geographic

Genre is editorially labeled and mood is machine generated.

694

distributions between the seed track ”Dark Horse” by Katy
Perry and the retrieved track with the smallest distance,
”Black Ice” by AC/DC. Countries with low counts have been
omitted for clearer visualization. As is clearly seen in the
Figure 5, these tracks exhibit very similar distributions with
heavy activity in the United States and secondary activity
in Brazil and Mexico. Additionally, Table 3 shows the top
5 tracks with most similar listening distributions.
Similarly, Figure 6 and Table 4 show the track distribution
comparison for the most similar track and top 5 most similar
tracks, respectively, for the seed track ”Yamulemaeu” by Joe
Arroyo y La Verdad. And Figure 7 and Table 5 show the
track distribution comparison for the most similar track and
top 5 most similar tracks, respectively for the seed track
”Y(JR GROOVE Remix)” by MBLAQ.

Table 2: Countries by Number of Matches
Number of Matches Country Code
18,026,281
USA
4,828,753
MEX
3,176,338
DEU
2,886,882
BRA
1,975,424
RUS
1,974,358
ESP
1,752,407
FRA
1,658,587
GBR
1,642,353
COL
1,511,542
CHL

artist distributions may yield useful results for IR applications. The temporal nature of the data allows for prediction
of artist or genre popularity or geographical movement of a
track. Also, an understanding of the relationship between a
music identification signal and a user listening signal would
help the IR community select the appropriate music consumption dataset for their research.

5.

Figure 5:
Geographical distributions of ”Dark
Horse” and most similar track

GEOGRAPHIC SIMILARITY

To demonstrate the utility of GNMID14 we present a
novel similarity metric for retrieving tracks in the dataset.
Due to various characteristics such as language, origin, and
genre, a track will have a distinct geographic popularity
distribution. For example, a track sung in Japanese by a
Japanese artist may have many listens in Japan and fewer
listens elsewhere. It can be useful and interesting in a search
or recommendation system to find songs with a similar popularity distribution across countries. This provides another
way of exploring the data beyond that of a more traditional
similarity metric such as retrieving tracks of the same genre.

5.1

Figure 6: Geographical distributions of ”Yamulemaeu” and most similar track

Methods

To find similar tracks we first represent each track in a
vectorized ”bag-of-countries” form and use a vector similarity metric, in this case Euclidean distance, to compute the
similarity of the tracks.
Specifically, for track xi with total listens in countries
[c1 , c2 , ..., cn ], we calculate a normalized geographic listening
distribution, xˆi , to account for differences in track popularity in (1).
c1
xˆi =< Pn

ck

c2
, Pn

ck

cn
, ..., Pn
>
ck

(1)

We then calculate the Euclidean distance, dij , between
track x̂i and x̂j as
dij = x̂i − x̂j

2

(2)

5.3

2

For a given seed track xs , we rank the remaining tracks
in ascending order by distance to the seed track dsj and
retrieve the N highest ranking results.

5.2

Discussion

The most similar tracks retrieved for the first two seed
tracks, whose most similar tracks are shows in Table 3 and
Table 4, give a sense of geography and language and yet
cross boundaries in other content dimensions such as artist
origin and genre. The third seed track by MBLAQ, a South
Korean artist, presents an interesting result where there is
no activity in South Korea. Still, this retrieves other tracks
from Korean pop artists with similar popularity in other
parts of the world.

.

Results

Three seed tracks were chosen from GNMID14 to show a
variety of behaviors of retrieval using the similarity metric in
(2). Figure 5 shows a comparison of sparse country listening

695

7.

Figure 7: Geographical distributions of ”Y(JR
GROOVE Remix)” and most similar track

[1] T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and
P. Lamere. The million song dataset. In ISMIR 2011:
Proceedings of the 12th International Society for
Music Information Retrieval Conference, October
24-28, 2011, Miami, Florida, pages 591–596.
University of Miami, 2011.
[2] R. Campos, G. Dias, A. M. Jorge, and A. Jatowt.
Survey of temporal information retrieval and related
applications. ACM Computing Surveys (CSUR),
47(2):15, 2014.
[3] P. Cano, E. Batlle, T. Kalker, and J. Haitsma. A
review of audio fingerprinting. Journal of VLSI signal
processing systems for signal, image and video
technology, 41(3):271–284, 2005.
[4] D. Hauger, M. Schedl, A. Košir, and M. Tkalcic. The
million musical tweets dataset: What can we learn
from microblogs. In Proceedings of the 14th
International Society for Music Information Retrieval
Conference (ISMIR 2013), 2013.
[5] Y. Kim, B. Suh, and K. Lee. # nowplaying the future
billboard: mining music listening behaviors of twitter
users for hit song prediction. In Proceedings of the first
international workshop on Social media retrieval and
analysis, pages 51–56. ACM, 2014.
[6] P. Lamere. Social tagging and music information
retrieval. Journal of new music research,
37(2):101–114, 2008.
[7] A. Narayanan and V. Shmatikov. How to break
anonymity of the netflix prize dataset. arXiv preprint
cs/0610105, 2006.
[8] M. Sanderson. Test collection based evaluation of
information retrieval systems. Now Publishers Inc,
2010.
[9] M. Schedl. Leveraging microblogs for spatiotemporal
music information retrieval. In Advances in
Information Retrieval, pages 796–799. Springer, 2013.
[10] M. Schedl, A. Vall, and K. Farrahi. User geospatial
context for music recommendation in microblogs. In
Proceedings of the 37th international ACM SIGIR
conference on Research & development in information
retrieval, pages 987–990. ACM, 2014.
[11] G. Tassey, B. R. Rowe, D. W. Wood, A. N. Link, and
D. A. Simoni. Economic impact assessment of nist’s
text retrieval conference (trec) program. Report
prepared for National Institute of Technology (NIST),
2010.
[12] R. Turrin, M. Quadrana, A. Condorelli, R. Pagano,
and P. Cremonesi. 30music listening and playlists
dataset.
[13] P. S. Yu, X. Li, and B. Liu. On the temporal
dimension of search. In Proceedings of the 13th
international World Wide Web conference on
Alternate track papers & posters, pages 448–449.
ACM, 2004.
[14] E. Zangerle, M. Pichl, W. Gassler, and G. Specht. #
nowplaying music dataset: Extracting listening
behavior from twitter. In Proceedings of the First
International Workshop on Internet-Scale Multimedia
Management, pages 21–26. ACM, 2014.

Table 3: Top 5 Similar Tracks for ”Dark Horse” by
Katy Perry feat. Juicy J
Rank
1
2
3
4
5

Track
Black Ice
Even In Death
Try
By The Way
Duality

Artist
AC/DC
Evanescence
P!nk
Red Hot Chili Peppers
Slipknot

Table 4: Top 5 Similar Tracks for ”Yamulemau” by
Joe Arroyo
Rank
1
2
3
4
5

Track
El Amor De Mi Vida Eres Tú
Cuatro Cirios.
Sola Con Mi Soledad
Te Llamo Para Despedirme
Necesito Una Compañera

Artist
Kent Leroy
Javier Solı́s
Maricela
Sergio Denis
Los 3 Grandes
Compositores

Table 5: Top 5 Similar Tracks for ”Y(JR GROOVE
Remix)” by MBLAQ
Rank
1
2
3
4
5

6.

Track
I’ll Be Back (Club Mix)
I REMEMBER
Purple Line
ᅣᄂ
ᄋ
ᅮᄉ
ᅳ (Janus)
Catch Me

REFERENCES

Artist
2PM
B.A.P, Dae Hyen
東方神起
ᅩᄋ
ᄇ
ᅵᄑ
ᅳᄅ
ᆫᄃ
ᅦ
ᅳ
東方神起

CONCLUSIONS

This paper presented a novel dataset of music identification matches that fills a gap in available datasets for music
information retrieval. The results of a small investigation
into a geographical similarity metric for music tracks showed
the utility of the dataset for IR related tasks. Moving forward, additional metadata may be added to enhance the
utility of the dataset, and pending interest from the research
community, additional data may be released.

696

