Cluster-based Joint Matrix Factorization Hashing
for Cross-Modal Retrieval
Dimitrios Rafailidis

Fabio Crestani

Department of Informatics
Aristotle University of Thessaloniki
Thessaloniki, Greece

Faculty of Informatics
Università della Svizzera italiana (USI)
Lugano, Switzerland

draf@csd.auth.gr

fabio.crestani@usi.ch

ABSTRACT
Cross-modal retrieval has been an emerging topic over the
last years, as modern applications have to eﬃciently search
for multimedia documents with diﬀerent modalities. In this
study, we propose a cross-modal hashing method by following a cluster-based joint matrix factorization strategy. Our
method ﬁrst builds clusters for each modality separately and
then generates a cross-modal cluster representation for each
document. We formulate a joint matrix factorization process with the constraint that pushes the documents’ representations of the diﬀerent modalities and the cross-modal
cluster representations into a common consensus matrix. In
doing so, we capture the inter-modality, intra-modality and
cluster-based similarities in a uniﬁed latent space. Finally,
we present an eﬃcient way to generate the hash codes using the maximum entropy principle and compute the binary
codes for external queries. In our experiments with two publicly available data sets, we show that the proposed method
outperforms state-of-the-art hashing methods for diﬀerent
cross-modal retrieval tasks.

CCS Concepts
•Information systems → Multimedia information systems;

Keywords
Hashing; cross-modal retrieval; matrix factorization

1. INTRODUCTION
In multimedia applications, hashing techniques have been
widely used for large-scale similarity search, such as locality
sensitive hashing [4], iterative quantization [5] and spectral
hashing [8]. The key idea is to design hash functions and
learn similarity preserving binary codes for data representation with low storage cost and fast query speed. In the
aforementioned hashing methods, given a query from one
modality, for example an image-query, results are eﬃciently
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

SIGIR ’16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
⃝
DOI: http://dx.doi.org/10.1145/2911451.2914710

781

retrieved from an image database. In this study, we focus
on cross-modal hashing, for example, given a query from
an image modality, how to return the most relevant results
from a textual modality. The rich representation of a document with diﬀerent modalities has many applications; for
instance, in a real-world multimedia application, given an
image-query, a search engine can return relevant text documents to describe its details.
To handle the large amount of available multimedia content with diﬀerent modalities in modern applications, several
cross-modal hashing methods have been proposed [1, 6, 11,
13]. The main challenge of cross-modal hashing is how to
learn the binary codes by capturing both the inter-modality
and intra-modality similarities [2, 12]. For instance, [1] constructs a uniﬁed space and learns groups of hash functions
with eigendecomposition and AdaBoost to ensure that if two
documents with diﬀerent modalities are relevant, then their
corresponding hash codes are similar. [6] extends spectral
hashing to cross-modal retrieval by formulating the learning of binary codes as a tractable eigenvalue problem. [11]
presents an iterative scheme to learn a shared hamming
space using a graph regularization formulation and a set
of binary classiﬁers. [13] constructs heterogeneous hamming
spaces and then connect them, while preserving the local
structure using an anchor-based representation. Finally, [12]
considers inter-modality and intra-modality consistency to
generate a common hamming space, and integrates a linear
regression model to learn hash functions so that the binary
codes for new documents can be eﬃciently generated.
In [2], authors make the ﬁrst attempt to compute hash
codes using joint/collective matrix factorization with a latent factor model. Each matrix corresponds to the documents’ representations for each modality, and then by jointly
factorizing the matrices uniﬁed hash codes are generated.
By revealing the associations between the diﬀerent modalities and computing the similarities in each modality, the
joint matrix factorization hashing method of [2] achieves
high cross-modal retrieval accuracy. Nonetheless, the baseline joint matrix factorization that is used in [2] does not
preserve the similarities that documents have in their neighborhoods/clusters in the same modality, nor considers the
associations that the neighborhoods have in the diﬀerent
modalities.
To capture the inter-modality and intra-modality similarities at a neighborhood-based level, in this study we introduce a cluster-based joint matrix factorization method for
cross-modal hashing. Our method consists of three steps,
ﬁrstly we propose an eﬃcient way to generate cross-modal

cluster representations for the documents. In the second
step, we incorporate the generated cross-modal cluster representations into a joint factorization technique to compute
the inter-modality and intra-modality similarities of the documents in the diﬀerent modalities. This is achieved by formulating a joint matrix factorization process with the constraint that pushes the documents’ representations of the
diﬀerent modalities and the cross-modal cluster representations into a common consensus matrix. Finally, we calculate the uniﬁed hash codes based on the maximum entropy
principle [8], as well as the projection matrices to generate
hash codes for external documents that do not belong to
the database. Our experiments on two benchmark data sets
demonstrate the superiority of the proposed hashing method
over competitive hashing methods for diﬀerent cross-modal
retrieval tasks.

2. PROBLEM FORMULATION
In the remainder of the paper, we use the following notation, numbers are denoted by lower case letters e.g., a;
matrices by plain upper case letters e.g., A; sets by calligraphic upper case letters e.g., A; and vectors by lower case
bold letters e.g., a.
Let n be the number of documents and m the number
of modalities. Given j = 1 . . . m and i = 1 . . . n, each ith document is represented in the j-th modality by a dj dimensional feature vector:
(j)

xi

(j)

(j)

(j)

= [x1 , x2 , . . . , xdj ] ∈ ℜ1×dj

with dj ̸= dj ′ for two diﬀerent modalities j and j ′ . In the
j-th modality, the n feature vectors are stored in a matrix
X (j) ∈ ℜdj ×n , where the i-th column is the feature vector
(j)
xi . In our method, we assume that the inter-modality and
intra-modality similarities are calculated in a uniﬁed hash
code with length k for each document i:

where qj is the number of clusters in the j-th modality, with
(j)
qj ̸= qj ′ for two modalities j, j ′ . Also, Cw denotes the
(j)
w-th cluster of the j-th modality, with w ∈ 1, . . . qj . Let
q×n
Z ∈ℜ
be the cross-modal cluster matrix, with q=q1 +
q2 +, . . . , qm . ∀ c=1, . . . , q we set Z(c, i)=1, if a document i
belongs to cluster c, and 0 otherwise. The i-th column of
Z is a representation (feature vector) of document i based
on all the generated clusters in the m modalities. Hence, if
two documents, e.g. i and p, belong to the same clusters in
most of the m modalities, then the respective i-th and p-th
columns (representations) of the cross-modal cluster matrix
Z will be similar.

3.2

Joint Matrix Factorization

3.2.1

To capture the intra-modality similarities of the n documents, ∀ j=1, . . . , m we consider the following matrix factorization of X(j):
X (j) ≈ U (j) V (j)

In the following Section, we detail each step of our method.

3. PROPOSED APPROACH
3.1 Cross-Modal Cluster Representations
∀ j=1, . . . , m we cluster the n feature vectors x(j) with
power iteration [7]. Thus, we have m diﬀerent clusterings:
 (1) (1)
(1)
C1 , C2 , . . . , Cq1





C (2) , C (2) , . . . , Cq(2)
2
1
2
..



.


 (m) (m)
(m)
C1 , C2 , . . . , Cqm

(1)

(j)

(3)

As presented in Section 3.1, the cluster assignments are nonnegative (Z ≥ 0), with the feature vectors also being nonnegative in each matrix X (j) ≥ 0; consequently, each matrix
factorization in Eq. (3) is subject to:
U (1) , . . . U (m) ≥ 0,

3.2.2

V (1) , . . . V (m) ≥ 0,

Uz , Vz ≥ 0

Inter-modality, Intra-modality and Cluster-based
Similarities in Joint Matrix Factorization

All matrices in Eq. (3) have to be jointly factorized to simultaneously compute the inter-modality and intra-modality
similarities of the documents in the m modalities, and to
capture the associations with the cross-modal cluster matrix. We formulate a joint matrix factorization process with
the constraint that pushes the matrices X (j) of the m modalities and the cross-modal cluster matrix Z into a common
consensus matrix B ∗ ∈ ℜk×n , with B ∗ corresponding to a
shared k-dimensional latent space of the n internal documents. To consider the case of having the representation of
an external document in modality j, we denote the projection of each modality j into the shared k-dimensional space
as a projection matrix P (j) ∈ ℜk×dj on condition that:
V (j) = P (j) X (j) ,

782

(2)

k×n

 (1)
X ≈ U (1) V (1)




X (2) ≈ U (2) V (2)
..


.


 (m)
X
≈ U (m) V (m)
Z ≈ Uz Vz

The problems that our cross-modal hashing method faces
are formally deﬁned as follows:

Definition 2. (External Documents’ Hash Codes)
“To calculate m projection matrices P (j) ∈ ℜk×dj , with j =
1, . . . , m, so as for any external document i that does not
(j)
belong to B, given its representation xi in modality j to
generate the respective hash code bi .”

dj ×k

with U
∈ ℜ
, V
∈ ℜ
and k being the number of latent factors in matrix factorization, equal to the
length of the hash codes bi . In addition, we consider the
matrix factorization of the cross-modal cluster matrix Z as
follows: Z ≈ Uz Vz , with Uz ∈ ℜq×k and V (j) ∈ ℜk×n , to
capture the associations of the n documents at the clusterbased level. Given the m matrices X (j) and the cross-modal
cluster matrix Z, in total we have m + 1 individual matrix
factorizations:
(j)

bi = [y1 , y2 , . . . , yk ] ∈ {0, 1}1×k

Definition 1. (Internal Documents’ Hash Codes)
“To compute a consensus binary matrix B ∈ {0, 1}k×n , where
the i-th column corresponds to the unified hash code bi of an
internal document i.”

Intra-modality and Cluster-based Similarities
in Individual Matrix Factorization

with

j = 1, . . . , m

(4)

where V (j) ∈ ℜk×n and X (j) ∈ ℜdj ×n . To calculate the
m projection matrices P (j) and the consensus matrix B ∗ ,
we formulate the problem of joint matrix factorization as a
minimization problem [3]. Based on Eqs. (3) and (4), we
have to minimize the following loss function L for the joint
matrix factorization:
L(U (1) , . . . , U (m) , P (1) , . . . , P (m) , Uz , Vz , B ∗ ) =
}
m {
∑
||X (j) − U (j) P (j) X (j) ||2F + λj ||P (j) X (j) − B ∗ ||2F
j=1

+||Z − Uz Vz ||2F + λz ||Vz − B ∗ ||2F
(5)
where || · ||F denotes the Frobenius norm. In the second line
of Eq. (5), the term ||X (j) − U (j) P (j) X (j) ||2F denotes the approximation error of each factorized matrix X (j) , while the
term ||P (j) X (j) − B ∗ ||2F is the disagreement measurement
between the product P (j) X (j) and the consensus matrix
B ∗ . Accordingly, in the third line of Eq. (5), the ﬁrst term
||Z − Uz Vz ||2F denotes the approximation error of the factorized cross-modal cluster matrix Z; and ||Vz −B ∗ ||2F is the respective disagreement measurement. The product P (j) X (j) ,
matrix Vz and the consensus matrix B ∗ are comparable, as
they have the same dimensionality (k × n). ∀ j = 1, . . . , m
each parameter λj tunes the weight of modality j over the
joint factorization, and the respective disagreement term;
accordingly, parameter λz for the cross-modal cluster matrix. For reasons of simplicity we set λ1 = . . . = λm = λz =
λ. In our approach, we solve the minimization problem of L,
using the “multiplicative rules” [3], an iterative update procedure, where in each iteration we ﬁx B ∗ and minimize L
over the rest of matrices, and then we ﬁx the rest of matrices
and minimize L over B ∗ ; The outcome of the joint factorization when minimizing the loss function L of Eq. (5) is computing matrices U (1) , . . . , U (m) , P (1) , . . . , P (m) , Uz , Vz , B ∗ .

3.3 Hash Codes
We generate the uniﬁed hash codes in B ∈ {0, 1}k×n by binarizing the real values of the consensus matrix B ∗ ∈ ℜk×n ,
thus generating a binary vector bi = [y1 , y2 , . . . , yk ] for each
internal document i. According to [8], eﬃcient hash codes
should also maximize the entropy; based on the maximum
entropy principle, a binary bit that gives balanced partitioning of the whole data set should provide maximum information. Hence, given the code length k, then ∀a = 1, . . . , k
we set a threshold thresa for binarizing the a-th bit, with
thresa being the median1 of the a-th row in B ∗ . If a bit
ya > thresa , then we set ya = 1 and 0 otherwise. In doing
so, the binary code achieves the best balance. To generate
the hash code of an external document i that does not belong to matrix B, we use the projection matrices, computed
by the joint matrix factorization when minimizing the loss
(j)
function in Eq (5). Given the feature vector xi ∈ ℜ1×dj
of the external document i in the j-th modality, we use the
respective projection matrix P (j) ∈ ℜk×dj to calculate the
following real-value vector:
(j)

T

bi = xi P (j) ∈ ℜ1×k

(6)

1

We use the median as a threshold, because mean values are
sensitive to extremely high or low values in B ∗ .

783

which is then binarized as in the internal case, creating the
hash code bi for the external document i.

4.
4.1

EXPERIMENTAL EVALUATION
Data Sets

We use two real-world data sets Wiki 2 and NUS-WIDE 3 .
The Wiki data set consists of 2,866 Wikipedia documents,
with each document containing a text and one corresponding relevant image. In addition, images are represented
by 128-dimensional SIFT feature vectors [10] and text by
10-dimensional topics vectors. All documents (image-text
pairs) are labeled by 10 semantic categories. We split the
data set as follows, 20% as query set; 5% as cross-validation
set to tune the parameters of each method; while the remaining 75% of the data set is considered as the database,
from which the hash codes are learnt and the results are
retrieved. NUS-WIDE consists of 269,648 image-tag pairs
from Flickr, where we keep the image-tag pairs that belong
to one of the 10 largest concepts [11, 14]. Images are represented by 500-dimensional SIFT vectors and text as 1000dimensional vectors, by performing PCA on the original tag
occurrences [14]. The query set consists of 1% of the dataset;
1% is used as cross-validation set; while the remaining data
set is the database to retrieve the results. The hash codes
are learned from 5K image-text pairs from the database [2].

4.2

Evaluation Protocol

As both data sets are bi-modal with images and texts, we
evaluate our hashing method on the following cross-modal
retrieval tasks: (i) image-query → text results and (ii) textquery → image results. Following the evaluation protocol of
relevant studies [2, 11, 14], we measure the performance of
cross-modal hashing in terms of mAP . Given a query and
the top result set R, Average Precision (AP ) is deﬁned as:
1∑
AP =
P (r)δ(r)
(7)
l r∈R
where l is the number of true neighbors in the retrieved set
R, that is, the results which belong to the same category
with the query; P (r) denotes the precision of the top retrieved results in set R and δ(r)=1 if the r-th result is a
true neighbor and 0 otherwise. In the experiments we set
|R|=50, and mAP is computed by averaging the AP values
over all the queries. We repeated our experiments ﬁve times
and we report mean mAP values and standard deviations
over the runs.

4.3

Results

In the proposed Cluster-based Joint Matrix Factorization
Hashing method (C-JMFH), we varied the parameter λ
in [10−4 10−1 ], concluding in 10−2 and 10−3 for Wiki and
NUS-WIDE, respectively. To evaluate the impact of the
cross-modal cluster matrix when computing the consensus
matrix B ∗ in Eq. (5), and consequently the binary codes
in B, we use as baseline a variant of our method, namely
JMFH, which does not consider the cross-modal cluster
matrix in the joint factorization process. We use IMH4 [12]
as baseline, and we compare the proposed C-JMFH method
2

http://www.svcl.ucsd.edu/projects/crossmodal/
http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm
4
http://staﬀ.itee.uq.edu.au/shenht/publications.htm
3

IMH

CMFH

JMFH

C−JMFH

IMH

CMFH

JMFH

C−JMFH

32

64

128

0.7

0.3

0.65

mAP

mAP

0.25

0.6
0.55

0.2

0.5
0.15

16

32

64

Code length

image → text

128

0.45

16

Code length

text → image

Figure 1: Performance evaluation on Wiki for the
cross-modal retrieval tasks.

5. CONCLUSION
In this study, we presented a cross-modal hashing method
using a cluster-based representation into a joint factorization
process. In our experiments, we showed that the proposed
method outperforms other state-of-the-art hashing methods
in terms of cross-modal retrieval accuracy. Although we
focused on the case of cross-modal retrieval, that is, given
a query from one modality to retrieve results from another
5

http://ise.thss.tsinghua.edu.cn/MIG/publications.jsp

784

0.65

IMH

CMFH

JMFH

IMH

C−JMFH

CMFH

JMFH

C−JMFH

32

64

128

0.75
0.7

0.6

mAP

0.65

mAP

with CMFH5 [2], a hashing method that also follows a joint
matrix factorization strategy when generating the binary
codes. The parameter values in the baselines were determined by cross-validation, using the publicly available implementations and we report the best results.
Figures 1 and 2 show the results for the cross-modal retrieval tasks in Wiki and NUS-WIDE, respectively. The
baseline IMI method has poor performance, compared with
the rest of cross-modal hashing methods, by not following a
joint matrix factorization strategy and thus not computing
the inter-modality and intra-modality similarities as well as
CMFH, JMFH and C-JMFH do. As also observed in relevant studies [2, 6], the retrieval accuracy does not necessarily improve when increasing the number of bits in many
baseline methods such as IMI, as it does not follow a joint
matrix factorization strategy. Meanwhile, we observe that
CMFH and JMFH achieve comparable performance, as both
hashing methods jointly factorize the representations in the
diﬀerent modalities, with the retrieval accuracy increasing
when a larger number of bits is selected. This happens because in these methods the hamming space corresponds to
the latent space of the joint matrix factorization process,
consequently encoding more information for a large number
of bits. The proposed C-JMFH method boosts the mAP
retrieval accuracy by incorporating the cross-modal cluster
representations of the documents when learning the hash
codes. Compared to the second best method, the proposed
C-JMFH method achieves a relative improvement of 3.112.9% in all the cross-modal retrieval tasks, with the exceptional case of 1.9% relative improvement in the case of a
low selection of number of bits, that is, the case of 16 bits
in NUS-WIDE for the text → image cross-modal retrieval
task. To verify the superiority of the proposed C-JMFH
method, we used the paired t-test and we found that the differences between the reported results for C-JMFH against
the competitive hashing methods were statistically signiﬁcant for p<0.05.

0.55

0.6
0.55

0.5
0.5

0.45

16

32

64

Code length

image → text

128

0.45

16

Code length

text → image

Figure 2: Performance evaluation on NUS-WIDE.
modality, an interesting future direction is to extend our
method to multimodal retrieval, where given a query from
one or more modalities to retrieve documents from all the
diﬀerent modalities [9].

6.

REFERENCES

[1] M. M. Bronstein, A. M. Bronstein, F. Michel, and
N. Paragios. Data fusion through cross-modality
metric learning using similarity-sensitive hashing. In
CVPR, pages 3594–3601, 2010.
[2] G. Ding, Y. Guo, and J. Zhou. Collective matrix
factorization hashing for multimodal data. In CVPR,
pages 2083–2090, 2014.
[3] J. Gao, J. Han, J. Liu, and C. Wang. Multi-view
clustering via joint nonnegative matrix factorization.
In SDM, pages 252–260, 2013.
[4] A. Gionis, P. Indyk, and R. Motwani. Similarity
search in high dimensions via hashing. In VLDB,
pages 518–529, 1999.
[5] Y. Gong and S. Lazebnik. Iterative quantization: A
procrustean approach to learning binary codes. In
CVPR, pages 817–824, 2011.
[6] S. Kumar and R. Udupa. Learning hash functions for
cross-view similarity search. In IJCAI, pages
1360–1365, 2011.
[7] F. Lin and W. W. Cohen. Power iteration clustering.
In ICML, pages 655–662, 2010.
[8] R. Lin, D. A. Ross, and J. Yagnik. SPEC hashing:
Similarity preserving algorithm for entropy-based
coding. In CVPR, pages 848–854, 2010.
[9] X. Liu, Y. Mu, B. Lang, and S. Chang. Mixed
image-keyword query adaptive hashing over multilabel
images. TOMCCAP, 10(2):22, 2014.
[10] D. G. Lowe. Distinctive image features from
scale-invariant keypoints. IJCV, 60(2):91–110, 2004.
[11] S. Moran and V. Lavrenko. Regularised cross-modal
hashing. In SIGIR, pages 907–910, 2015.
[12] J. Song, Y. Yang, Y. Yang, Z. Huang, and H. T. Shen.
Inter-media hashing for large-scale retrieval from
heterogeneous data sources. In SIGMOD, pages
785–796, 2013.
[13] Y. Wang, X. Lin, L. Wu, W. Zhang, and Q. Zhang.
Lbmch: Learning bridging mapping for cross-modal
hashing. In SIGIR, pages 999–1002, 2015.
[14] Y. Zhen and D. Yeung. Co-regularized hashing for
multimodal data. In NIPS, pages 1385–1393, 2012.

