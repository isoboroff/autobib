Going Beyond Relevance: Incorporating effort in
Information Retrieval
Manisha Verma

University College London

manisha.verma.13@ucl.ac.uk

ABSTRACT

how much time and effort it may take to consume it. While
a judge can take significant time to evaluate a document, an
end user may not be willing to spend as much time consuming it, even if it is relevant. They report that the primary
reason of mismatch between explicit and implicit document
judgments is a result of effort needed to find and consume
required information from a given document.
Thus, we need to design systems and evaluation methodologies that consider both user effort and relevance to rank
documents or compare systems. Effort would becomes crucial with change in device of information access. In mobile,
where interaction is limited by screen size and restricted
text-input, users are more likely to give up finding information via search engines. Thus, we need to characterize effort
across devices. Finally, effort based evaluation of search engines would have to be designed around user’s search tasks.
For instance, systems that rank documents high effort documents on the top for easy tasks should be penalized more
than those that rank low effort documents (on top) for relatively difficult task.
With the above mentioned motivations we wish to explore ways to accommodate user effort in IR evaluation.
We shall focus on three research questions regarding effort.
RQ1: What constitutes user effort in IR. What parameters are most representative of user effort. Given a document and user query, what features will be useful to predict
effort based judgments. We identified three factors [3] to
be associated with effort, gathered judgments (both explicit
and preference based) and investigated their correlation with
user satisfaction. Our findings suggest that ease of finding
information dominates all other factors. RQ2: How can existing learning to rank framework be modified to optimize
for both relevance and effort. RQ3: How does effort vary
with user device. What factors characterize effort on mobile
and desktop. We conducted a user study [2] to understand
variation in relevance with device. We shall use this data
to control for relevance and investigate role of effort on mobiles. RQ4: Finally, how can we design effort based evaluation methodology which takes into account user’ search task
while rewarding or penalizing systems.

Primary focus of Information retrieval (IR) systems has been
to optimize for Relevance. Existing approaches used to rank
documents or evaluate IR systems do not account for “user
effort”. At present, relevance captures topical overlap between document and user query. This mechanism does not
take into consideration either time or effort of end user to
satisfy information need. While a judge may spend time
assessing a document, an end user may not thoroughly examine a document. We identified factors that are associated
with effort for a single document and gathered judgments
for same. We also investigated the role of several features in
predicting effort on webpage. In future, we shall investigate
role of effort on mobile and investigate effort based evaluation methodology that also takes into account user’s search
task.

1.

INTRODUCTION

Search engines measure the estimate document relevance
on basis of Topicality. A document is considered relevant
to user’s query if its content topically overlaps with user’s
information need. Information Retrieval (IR) systems are
designed to optimize for relevance. It is assumed that relevant documents shall answer user’s information need, which
in turn will yield higher user satisfaction. Evaluation metrics are also designed to compare systems on basis of how
many relevant documents are retrieved and where are they
shown on SERPs.
At present, researchers rely on pre-designed small-scale
test collections to evaluate effectiveness of IR systems known
as batch evaluation. These test collections consist of some
documents manually judged for relevance for pre-defined
queries. Live users are also used to evaluate system’s search
effectiveness. User based evaluation relies on observing and
measuring user’s interaction with document to determine its
relevance. One would expect that batch evaluation would
agree with user-based evaluation of systems. However it has
been shown in the past [1] that these two forms of evaluation do not agree with each other. Recently, Yilmaz et al.
[4] proposed that this mismatch is due to the disagreement
between what judges and users consider relevant. Trained
judges are asked to identify document relevance regardless of

2.

REFERENCES

[1] M. Sanderson, M. L. Paramita, P. Clough, and
E. Kanoulas. Do user preferences and evaluation
measures line up? SIGIR ’10. ACM.
[2] M. Verma and E. Yilmaz. Characterizing relevance on
mobile and desktop. ECIR ’16. Springer.
[3] M. Verma, E. Yilmaz, and N. Craswell. On obtaining
effort based judgements for information retrieval.
WSDM ’16. ACM.
[4] E. Yilmaz, M. Verma, N. Craswell, F. Radlinski, and
P. Bailey. Relevance and effort: An analysis of
document utility. CIKM ’14. ACM.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’16 July 17-21, 2016, Pisa, Italy
c 2016 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-4069-4/16/07.
DOI: http://dx.doi.org/10.1145/2911451.2911487

1173

