Simulation of Interaction: A Tutorial on Modelling and
Simulating User Interaction and Search Behaviour
Leif Azzopardi
School of Computing Science, University of Glasgow
Glasgow, United Kingdom

Leif.Azzopardi@glasgow.ac.uk
ABSTRACT

augment these traditional forms of evaluation [31] in order
to explore more possibilities and draw deeper insights into
search and search behaviour [12, 25]. This is because simulation provides a low cost means of experimentation that
enables repeatable and reproducible evaluations to be conducted that include interaction. However for the results of
such simulations to be credible the models used need to be
validated. And so it is important and timely to discuss the
state of the art in simulation and how to build credible and
valid simulations. In this tutorial, we will cover: (i) the
what, why, when of simulation, (ii) the high level interaction models used for simulation, (iii) simulating the different
components: (a) query generation, (b) stopping strategy, (c)
decision making, etc, and (iv) how to build and evaluate
valid simulations and what-if simulations.

Search is an inherently interactive, non-deterministic and
user-dependent process. This means that there are many
different possible sequences of interactions which could be
taken (some ending in success and others ending in failure).
Simulation provides a low cost, repeatable and reproducible
way to explore a large range of different possibilities. This
makes simulation very appealing, but it also requires care
and consideration in developing, implementing and instantiating models of user behaviour for the purposes of experimentation.
In this tutorial, we aim to provide researchers with an
overview of simulation, detailing the various types of simulation, models of search behavior used to simulate interaction,
along with an overview of the various models of querying,
stopping, selecting and marking. Through the course of the
tutorial we will describe various studies and how they have
used simulation to explore different behaviours and aspects
of the search process. The final section of the tutorial will
be dedicated to “best practice” and how to build, ground
and validate simulations. The tutorial will conclude with a
demonstration of an open source simulation framework that
can be used develop various kinds of simulations.

1.1

Tutorial Overview

First we will provide an overview of simulation in IR and
describe the different ways in which simulation has been used
(e.g. simulated work tasks and tracks [22, 47], simulated
and synthetic data collections [1, 10, 29, 44], and simulated
interaction [12, 24, 25, 34, 48].) The focus of this tutorial
will be on how to use simulation to undertake interactivebased evaluations and explorations to determine:

CCS Concepts

1. how well an IR system performs, and

•Information systems → Task models; Search interfaces; Test collections; •Human-centered computing
→ User models; Web-based interaction;

2. how performance changes under different conditions
and behaviours.
We will describe and explain the benefits and drawbacks
of simulation and how it fits with other evaluations type (i.e.
TREC style, user studies, etc).

Keywords
Simulation, Evaluation, Performance, Information Retrieval

1.

High Level Interaction Models

INTRODUCTION

In the next part of the tutorial, we will focus on explaining
the different high level models used to simulate the interaction between the user and the search system [21, 37, 38, 39,
45, 24].
Here we will cover the state-based stochastic model of the
user [21] and the process oriented: Searcher Model [39, 45]
and the Complex Searcher Model [37, 38] as shown in Figure 1,. This will provide the necessary background for understanding how the search process is modelled, and what
components need to be instantiated when developing a simulation. We will point out the types of tasks that are currently modelled and what we can evaluate with them along
with pointing out their limitations and describing avenues
for further development.

Simulation provides a complementary approach to traditional forms of evaluation i.e. TREC style batch evaluations
and user studies. Recently, there has been a growing interest in developing user models and developing simulations to
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

SIGIR ’16, July 17 - 21, 2016, Pisa, Italy
c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2914799

1227

1
Examine Topic

2
Generate Queries

3

4
Issue Query

View SERP
Yes

No, issue next query
Yes

Examine Snippet

No, abandon SERP

Stop?

Continue?

No

Snippet
Relevant?
Yes

No

6
Mark Document

Yes

5

Document
Relevant?

Read Document

Click Document

Figure 1: A flow chart of the high level interactions that are simulated.

Components

1.2

The core actions/components in most high level interaction
models are:

This tutorial will be focused at an introductory to intermediate level. We assume the participant has a good
knowledge of Information Retrieval i.e. knowledge about
the TREC style evaluation and the IR process. The tutorial
is aimed at masters, graduate students and researchers wanting to know more about the variety of simulation methods,
techniques and tools and how they can use them to enhance
their research.

1. the application of query (re)formulation strategies;
2. snippet scanning and assessment;
3. snippet clicking;
4. document reading;

1.3

5. document assessment; and

Intended Audience

Intended Learning Outcomes

By the end of the tutorial, participants should be able to:

6. session stopping.
• Explain the benefits and drawbacks of simulations
So in this part of the tutorial, we will explore in detail each of
the components within the high level interaction models and
describe and explain the different methods and models used
to: (i) generate/formulate queries and query suggestions [1,
10, 21, 23, 29, 33, 46], (ii) determine when users stop and
how they browse [24, 23, 37, 38, 45], (iii) interact and provide
feedback [19, 27, 28, 32, 33, 35, 41] and (iv) encode the cost
of actions [13, 20, 40, 42, 43]. Here we will explain a number
of these different studies, and how they have examined the
influence of the different components on search performance
and/or search behaviors.

• Describe the different high level interaction models
• Discuss and compare the different querying, stopping
and decision making components
• Evaluate and measure the performance of systems and
behaviours using simulation
• Design and construct grounded simulations and whatif simulations

Putting it all together

1.4

The final part of the tutorial is how to put all the components together to build a credible and valid simulation, or
to create reasonable “what-if” simulations. To build a simulation, we will explain the typical method undertaken:

Leif Azzopardi is a Senior Lecturer within the School
of Computing Science at the University of Glasgow, within
the Glasgow Information Retrieval Group. His research focuses on building formal models for Information Retrieval
- usually drawing upon different disciplines for inspiration,
such as Quantum Mechanics, Operations Research, Microeconomics, Transportation Planning and Gamification. Central to his research is the theoretical development models
for Information Seeking and Retrieval, where his research
interests include:

• specify the search goal and stopping criteria, i.e. search
until the simulated user reaches a certain level of gain,
reaches a time limit, or meets some other stopping condition,
• specify the high level interaction model, and specify
each of the different low level components,

Biography

• Models for the retrieval of documents, sentences, experts and other information objects [18, 26];

• then, vary the parameters and components of interest
in order to ask particular research questions.

• Probabilistic models of user interaction and the simulation of users for evaluation [1, 9, 11];

This will be followed up by discussion on how to ground and
validate simulations - and how simulations can guide empirical research and illuminate theory. We will conclude the
day by providing a demonstration of a toolkit for simulating
interaction [36].

• Economic and optimization of models of interaction [3,
5, 6, 16], specifically how cost and effort affect interaction and performance with search systems [13, 30];

1228

• Methods which assess the impact of search technology on society in application areas such as, search engine bias and the accessibility of e-Government information [14, 15], and;

[13] L. Azzopardi, D. Kelly, and K. Brennan. How query
cost affects search behavior. In Proceedings of 36th
ACM SIGIR Conference, pages 23–32, 2013.
[14] L. Azzopardi and V. Vinay. Accessibility in
information retrieval. In Proceedings of the 30th
European Conference on Advances in Information
Retrieval, pages 482–489, 2008.
[15] L. Azzopardi and V. Vinay. Retrievability: An
evaluation measure for higher order information access
tasks. In Proc. of the 17th ACM CIKM Conference,
pages 561–570, 2008.
[16] L. Azzopardi and G. Zuccon. An analysis of theories
of search and search behavior. In Proceedings of the
2015 International Conference on The Theory of
Information Retrieval, ICTIR ’15, pages 81–90, New
York, NY, USA, 2015. ACM.
[17] L. Azzopardi and G. Zuccon. Building and using
models of information seeking, search and retrieval:
Full day tutorial. In Proceedings of the 38th
International ACM SIGIR Conference on Research
and Development in Information Retrieval, SIGIR ’15,
pages 1107–1110, 2015.
[18] K. Balog, L. Azzopardi, and M. de Rijke. Formal
models for expert finding in enterprise corpora. In
Proceedings of the 29th Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, SIGIR ’06, pages 43–50, 2006.
[19] F. Baskaya, H. Keskustalo, and K. Järvelin.
Simulating simple and fallible relevance feedback. In
Advances in Information Retrieval, pages 593–604.
Springer Berlin Heidelberg, 2011.
[20] F. Baskaya, H. Keskustalo, and K. Järvelin. Time
drives interaction: Simulating sessions in diverse
searching environments. In Proc. 35th ACM SIGIR,
pages 105–114, 2012.
[21] F. Baskaya, H. Keskustalo, and K. Järvelin. Modeling
behavioral factors in interactive information retrieval.
In Proc. 22nd ACM CIKM, pages 2297–2302, 2013.
[22] P. Borlund. The iir evaluation model: a framework for
evaluation of iir systems. Info. research, 8(3), 2003.
[23] B. Carterette, A. Bah, and M. Zengin. Dynamic test
collections for retrieval evaluation. In Proc. 5th ACM
ICTIR, pages 91–100, 2015.
[24] B. Carterette, E. Kanoulas, and E. Yilmaz. Simulating
simple user behavior for system effectiveness
evaluation. In Proc. 20th ACM CIKM, pages 611–620,
2011.
[25] C. Clarke, L. Freund, M. Smucker, and E. Yilmaz.
Report on the sigir 2013 mube workshop. SIGIR
Forum, 47(2):84–95, 2013.
[26] R. T. Fernández, D. E. Losada, and L. A. Azzopardi.
Extending the language modeling framework for
sentence retrieval to include local context. Information
Retrieval, 14(4):355–389, 2011.
[27] D. Harman. Relevance feedback revisited. In Proc.
15th ACM SIGIR Conference, pages 1–10, 1992.
[28] K. Järvelin. Interactive relevance feedback with
graded relevance and sentence extraction: Simulated
user experiments. In Proc. 18th ACM CIKM
Conference, pages 2053–2056, 2009.

• Searching for fun (i.e. the SINS of users) [4].
Over the past ten years, he has developed a number of
techniques, tools and methods for simulated evaluations, including: (i) query generation methods [9, 11, 1], (ii) stopping
strategies [37, 38] (iii) interaction styles and search strategies [3, 6, 2]. In 2010, he co-organized an ACM SIGIR
Workshop on the Simulation of Interaction and delivered
a keynote on the ‘Assimilation of Users’ at the ACM SIGIR
2013 Workshop Modeling User Behaviour for Evaluation. He
has also given numerous lectures and invited talks on simulation at various universities and at the Information Foraging
Summer School (2010-2012). He has also given a series of
tutorials on Retrievability [7, 8] (ACM SIGIR 2014, ECIR
2015, ACM ICTIR 2015) and on Formal Models of Information Seeking, Search and Retrieval [17] (ACM SIGIR 2015,
ACM CIKM 2015).

2.

REFERENCES

[1] L. Azzopardi. Query side evaluation: An empirical
analysis of effectiveness and effort. In Proceedings of
the 32nd ACM SIGIR, pages 556–563, 2009.
[2] L. Azzopardi. Usage based effectiveness measures:
Monitoring application performance in information
retrieval. In Proceedings of the 18th ACM CIKM
Conference, pages 631–640, 2009.
[3] L. Azzopardi. The economics in interactive
information retrieval. In Proc. 34th ACM SIGIR,
pages 15–24, 2011.
[4] L. Azzopardi. Searching for unlawful carnal
knowledge. In Proceedings of the SIGIR Workshop:
Search for Fun, volume 11, pages 17–18, 2011.
[5] L. Azzopardi. Economic models of search. In
Proceedings of the 18th Australasian Document
Computing Symposium, pages 1–1, 2013.
[6] L. Azzopardi. Modelling interaction with economic
models of search. In Proc. 37th ACM SIGIR, pages
3–12, 2014.
[7] L. Azzopardi. The retrievability of documents. In
Proceedings of the 37th International ACM SIGIR
Conference, SIGIR ’14, pages 1291–1291, 2014.
[8] L. Azzopardi. Theory of retrieval: The retrievability of
information. In Proceedings of the 2015 International
Conference on The Theory of Information Retrieval,
ICTIR ’15, pages 3–6, 2015.
[9] L. Azzopardi and M. de Rijke. Automatic construction
of known-item finding test beds. In Proceedings of
SIGIR ’06, pages 603–604, 2006.
[10] L. Azzopardi, M. de Rijke, and K. Balog. Building
simulated queries for known-item topics: An analysis
using six european languages. In Proc. 30th ACM
SIGIR, pages 455–462, 2007.
[11] L. Azzopardi, M. de Rijke, and K. Balog. Building
simulated queries for known-item topics: an analysis
using six european languages. In Proceedings of the
30th annual international ACM SIGIR conference,
pages 455–462, 2007.
[12] L. Azzopardi, K. Järvelin, J. Kamps, and M. Smucker.
Report on the sigir 2010 workshop on the simulation
of interaction. SIGIR Forum, 44(2):35–47, 2011.

1229

[29] C. Jordan, C. Watters, and Q. Gao. Using controlled
query generation to evaluate blind relevance feedback
algorithms. In Proc. 6th ACM/IEEE-CS JCDL, pages
286–295, 2006.
[30] D. Kelly and L. Azzopardi. How many results per
page?: A study of serp size, search behavior and user
experience. In Proceedings of the 38th International
ACM SIGIR Conference, pages 183–192, 2015.
[31] H. Keskustalo and K. Järvelin. Simulations as a means
to address some limitations of laboratory-based ir
evaluation. The Janus Faced Scholar: A Festschrift in
Honour of Peter Ingwersen,
Informationsvidenskabelige Akademi, Copenhagen,
pages 69–86, 2010.
[32] H. Keskustalo, K. Järvelin, and A. Pirkola. The effects
of relevance feedback quality and quantity in
interactive relevance feedback: A simulation based on
user modeling. In Advances in IR, volume 3936 of
LNCS, pages 191–204. 2006.
[33] H. Keskustalo, K. Järvelin, and A. Pirkola. Evaluating
the effectiveness of relevance feedback based on a user
simulation model: Effects of a user scenario on
cumulated gain value. Information Retrieval,
11(3):209–228, 2008.
[34] A. Leuski. Relevance and reinforcement in interactive
browsing. In Proc. 9th ACM CIKM, pages 119–126,
2000.
[35] J. Lin and M. D. Smucker. How do users find things
with pubmed?: towards automatic utility evaluation
with user simulations. In SIGIR’08 Proceedings of the
31st annual international ACM SIGIR conference,
pages 19–26, 2008.
[36] D. Maxwell and L. Azzopardi. Simulating interactive
information retrieval with simiir: A framework for the
simulation of interaction. In Proceedings of the 39th
annual international ACM SIGIR conference, 2016.
[37] D. Maxwell, L. Azzopardi, K. Järvelin, and
H. Keskustalo. An initial investigation into fixed and
adaptive stopping strategies. In Proc. 38th ACM
SIGIR, pages 903–906, 2015.

[38] D. Maxwell, L. Azzopardi, K. Järvelin, and
H. Keskustalo. Searching and stopping: An analysis of
stopping rules and strategies. In Proc. 24th ACM
CIKM, pages 313–322, 2015.
[39] A. Moffat, P. Thomas, and F. Scholer. Users versus
models: What observation tells us about effectiveness
metrics. In Proc. 22nd ACM CIKM, pages 659–668,
2013.
[40] T. Pääkkönen, K. Järvelin, J. Kekäläinen,
H. Keskustalo, F. Baskaya, D. Maxwell, and
L. Azzopardi. Exploring behavioral dimensions in
session effectiveness. In Proc. 6th CLEF, pages
178–189, 2015.
[41] I. Ruthven. Re-examining the potential effectiveness of
interactive query expansion. In Proc. 26th ACM
SIGIR Conference, pages 213–220, 2003.
[42] M. Smucker. An analysis of user strategies for
examining and processing ranked lists of documents.
In Proc. of 5th HCIR, 2011.
[43] M. D. Smucker and C. L. Clarke. Modeling optimal
switching behavior. In Proceedings of the 2016 ACM
on Conference on Human Information Interaction and
Retrieval, pages 317–320. ACM, 2016.
[44] J. Tague, M. Nelson, and H. Wu. Problems in the
simulation of bibliographic retrieval systems. In Proc.
3rd ACM SIGIR, pages 236–255, 1980.
[45] P. Thomas, A. Moffat, P. Bailey, and F. Scholer.
Modeling decision points in user search behavior. In
Proc. 5th IIiX, pages 239–242, 2014.
[46] S. Verberne, M. Sappelli, K. Järvelin, and W. Kraaij.
User simulations for interactive search: Evaluating
personalized query suggestion. In Advances in
Information Retrieval, volume 9022 of LNCS. 2015.
[47] E. Voorhees and D. Harman. TREC: Experiment and
Evaluation in Information Retrieval. The MIT press,
2005.
[48] R. White, J. Jose, C. van Rijsbergen, and I. Ruthven.
A simulated study of implicit feedback models. In
Advances in Information Retrieval, volume 2997 of
LNCS, pages 311–326. 2004.

1230

