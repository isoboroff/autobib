Discovering Author Interest Evolution in Topic Modeling
Min Yang

Jincheng Mei

University of Hong Kong
Pokfulam Road, Hong Kong

University of Alberta
Edmonton, AB, Canada

Fei Xu

Chinese Academy of Sciences
Haidian District, Beijing, China

ljmei2@ualberta.ca
myang@cs.hku.hk
Wenting Tu
Ziyu Lu

xufei@iie.ac.cn

University of Hong Kong
Pokfulam Road, Hong Kong

University of Hong Kong
Pokfulam Road, Hong Kong

wttu@cs.hku.hk

zylu@cs.hku.hk

ABSTRACT

∗

where the speed of the interest drifting is controlled by the
similarity of consecutive vectors. The IDM model also captures the fact that co-authors usually share common interests, by making their interest vectors positively correlated.
The parameters of the model are learnt through a prediction
task: given vectors associated with the context, the goal is
to predict each word in the document. This scheme has been
proven eﬀective in discovering topics from documents [7, 6,
11].
Our model is conceptually related to the work of [11] that
incorporates the ordering of words and the semantic meaning of sentences into topic modeling. However, [11] does
not consider the authorship and temporal information of
the text. In contrast, the goal of this paper is to address the
challenges of modeling author interest evolution. Our model
has three advantages over the traditional LDA model. First,
given the document’s authors, the document’s topic is determined by the authors’ interests, which may change over
time. Second, our model provides a natural way to measure
the similarity of interest, which helps controlling the speed
of interest drift, as well as measuring the similarity between
authors. Third, our model takes the ordering of words, sentences and paragraphs into account, so that the topics reflect
the semantics of the context.

Discovering the author’s interest over time from documents
has important applications in recommendation systems, authorship identification and opinion extraction. In this paper, we propose an interest drift model (IDM), which monitors the evolution of author interests in time-stamped documents. The model further uses the discovered author interest information to help finding better topics. Unlike traditional topic models, our model is sensitive to the ordering
of words, thus it extracts more information from the semantic meaning of the context. The experiment results show
that the IDM model learns better topics than state-of-theart topic models.

1. INTRODUCTION
Author interests play a crucial role in a variety of natural language processing (NLP) application. Motivated by
the demand of understanding people’s preferences and behavioral pattern, a flurry of researches devote to discovering
author interests. The common approach is to build a generative model that represents the author’s interests as a latent
variable (see Section 2). Most of these models (LDA-style
models) make the bag-of-word assumption, which states that
the meaning of the document is fully characterized by the
unigram statistics of words. The bag-of-word assumption
neglects the ordering of words, sentences and paragraphs,
though these factors are important in defining the semantics meaning of the context.
We propose a Interest Drift Model (IDM) for modeling the
dynamic evolution of individual author’s interest. The IDM
learns the representation of words, sentences and document
as vectors. It also learns the interests of authors as vectors.
Since the interests of authors may change over time, each
author’s interests is represented by a sequence of vectors,

2.

RELATED WORK

The Author-Topic model [8] is the first generative model
that simultaneously models the content of documents and
the interests of authors. There are also some variants of
Author-Topic models such as [5]. These models are devoted
to discovering static latent topics and author’s interests,
without considering temporal information.
To characterize topics and their evolution over time, [2]
propose a dynamic topic model (DTM) which jointly models word co-occurrence and time. [9] propose a non-Markov
continuous-time model (ToT). These models capture the
evolution of topics, but they completely ignore the authorship information.
There are recent works taking both timestamp and authorship information into account, including the TemporalAuthor-Topic (TAT) model [4], the Author-topic over time
(AToT) model [10]. Nevertheless, these models do not characterize the drift of the individual author’s interests.
All of the aforementioned topic models employ the bagof-words assumption, which is rarely true in practice. [11]

∗Fei Xu is the corresponding author.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

SIGIR ’16, July 17 - 21, 2016, Pisa, Italy
c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
⃝
ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2914723

801

propose a generative topic model that represent each topic
as a cluster of multi-dimensional vectors. Nevertheless, this
model doesn’t use temporal information or models the interest of authors.

that there is no correlation between co-authors. If ρ = 1,
then the drift vectors vec(ai , d) − vec(ai , d′i ) are in the same
direction for all co-authors, meaning that the interest drift
is perfectly correlated. In summary, the density function of
vector vec(a, d) is defined by
( (v − µ )T Σ −1 (v − µ ) )
d
d
d
(4)
p(vec(a, d) = v) ∝ exp −
2
The vector µd and the matrix Σd follow the definition above.

3. INTEREST DRIFT MODEL
In this section, we describe the IDM model as a generative model. Then we illustrate the inference algorithm for
estimating the model parameters.

3.1.2

3.1 Model description
We assume that there are W diﬀerent words in the vocabulary and there are D documents in corpus. In addition,
these documents belong to T topics, where T is a hyperparameter specified by the user. For a specific document d,
suppose that it has m authors a1 ,..., am . We use an interest
vector vec(ai , d) ∈ Rp to represent the interests of author
ai when he/she writes document d. Two authors have similar interests if the distance between their interest vectors
is small. Our goal is to learn these interest vectors from
the content of the corpus. It is clear that the interests of
co-authors should be correlated. The interests of the same
author in writing diﬀerent documents should consistent.

3.1.1 Author interests
We now specify the generative model for vectors vec(ai , d) ∈
Rp (i = 1, . . . , m), conditioning on the interest vectors of all
earlier documents. This defines a valid model, since interest vectors can be generated sequentially from the earliest
document to the latest document. Let t be the time that
document d was written. Let d′i be the last document that
author ai has written before she writes the current document, and let the timestamps for d′i be t′i . We have t′i ≤ t for
all i = 1, ...., m. We define a joint distribution on the vectors vec(a, d) := (vec(a1 , d), ..., vec(am , d)). It is a multivariate normal distribution on Rmp taking the form N (µd , Σd ).
Firstly, we specify the mean µd . Let
µd := (vec(a1 ,d′1 ), ...., vec(am ,d′m )),

(1)

vec(ai ; d′i )

where
is the interest vector for author ai when
she wrote document d′i . This definition implies that the new
interests of authors have connections to the history.
Second, we define the covariance matrix Σd . Note that
Σd is an mp × mp matrix, so that it can be partitioned into
m2 sub-matrices, each with dimension p×p. Let Σij ∈ Rp×p
be the sub-matrix of Σd on the i-th row and j-th column.
If i = j, then the sub-matrix characterize the covariance of
the author i’s interest. Let
(
)2
Σii := σ(t − t′i ) I,

3.1.3

Word tokens

Given the vectors representing author interests, words,
sentences and documents, we describe how the word tokens
are generated. Let V be the collection of all latent vectors:
V := {vec(w)} ∪ {vec(d)} ∪ {vec(s)} ∪ {vec(a, d)}
(5)
For the i-th word in the sentence, we represent its word
realization by wi , which is generated by
p (wi = w|d, a, s, wi−n , . . . , wi−1 )
n
∑
w
w
∝ exp(aw
aw
(6)
author + adoc + asen +
t )
t=1
w
w
w
where aw
author , adoc , asen and at are linear transformations
of the vectors of the author interest, the document, the sentence and the previous n words. These linear transformations are defined by
w
aw
(7)
author = ⟨uauthor , vec(a, d)⟩

(2)

w
aw
doc = ⟨udoc , vec(d)⟩
w
aw
sen = ⟨usen , vec(s)⟩

where σ(x) is an increasing function of x. It means that
as time passing, the covariance matrix entries get bigger,
indicating that the interest of author i is less likely to concentrate on its mean – the interest vector when she wrote
the earlier document d′ . More concretely, we adapt a linear
function σ(x) = α + βx with hyper-parameters α > 0 and
β > 0. The values of α and β are chosen by cross-validation.
If i ̸= j, then the sub-matrix Σij characterizes the correlation between the interest of the author i and the author j.
Let
Σij := ρσ(t − t′i )σ(t − t′j )I,

Words, sentences and documents

The words, sentences and documents are represented by
vectors so that their semantic representations are naturally
connected with that of the author interests. For each word
w ∈ {1, . . . , W }, there is a p-dimensional vector representation vec(w) ∈ Rp . Each document with index d ∈ {1, . . . , D}
has a vector representation vec(d) ∈ Rp . There are S sentences in the corpus, each sentence indexed by s ∈ {1, . . . , S}.
The sentence with index s is associated with a vector representation vec(s) ∈ Rp .
We assume that all these vectors are generated from a
multi-variate normal distribution conditioning on the topic.
Given the a topic k, the conditional distribution of vectors are defined by vec(w) ∼ N (µword
, Σword
), vec(s) ∼
k
k
doc
sen
sen
doc
N (µk , Σk ) and vec(d) ∼ N (µk , Σk ) where µk and Σk
represent the mean and the covariance matrix. We assume
that
∑T the topic k is chosen with probability πk such that
k=1 πk = 1. Thus, the vectors are generated from a Gaussian mixture model with unknown mixture weights and unknown Gaussian parameters. We use Ψ to collectively represent these parameters.

w
aw
t = ⟨ut , vec(wi−t )⟩

(8)
(9)
(10)

w
w
w
p
where uw
author , udoc , usen , ut ∈ R are transformation coefficient. These coeﬃcients are unknown parameters of the
model. They are shared across all word slots in the corpus.
We use U to represent this collection of these coeﬃcients.
Finally, we summarize the set of unknown parameters to
be learned: the set V containing the vector of author interests, words, sentences and documents, the set U containing
the unknown linear transformation coeﬃcients, and the set
Ψ containing parameters that defines the Gaussian mixture
model. We will see that given arbitrary two of these sets,
the remaining set of parameters can be solved by eﬃcient
algorithms.

(3)

where ρ ∈ [0, 1] is another hyper-parameter measuring the
correlation of the interest of co-authors. if ρ = 0, then
vec(a1 , d),...,vec(am , d) are mutually independent, meaning

802

3.2 Topic Inference

Data Set
NIPS
Enron

Given the model parameters U , Ψ and the vectors V , we
can infer the posterior probability distribution of topics. In
particular, for a word w with vector representation vec(w),
the posterior distribution of its topic, namely q(z(w)), is
easy to derive given the generative model. For any topic
z ∈ 1, 2, . . . , T , we have
πz N (vec(w)|µz , Σz )
.
q(z(w) = z) = ∑T
k=1 πk N (vec(w)|µk , Σk )

(11)

(12)

4.

4.1

3.3.1 Stage I: Estimating λ
In this stage, the latent vector of words, sentences and
documents are fixed. We estimate the parameters of the
Gaussian mixture model λ = {πk , µk , Σk }. This is a classical statistical estimation problem which can be solved by
running the EM algorithm. The reader can refer to the book
[1] for the implementation details.

3.3.2 Stage II: estimating U and Ψ
When λ is known and fixed, we estimate the model parameters U and the latent vectors Ψ by maximizing the loglikelihood of the generative model. In particular, we iteratively sample a location in the corpus, and consider the
log-likelihood of the observed word at this location. Let the
word realization at location i be represented by w. The
log-likelihood of this location is equal to

(∑

w
w
exp(aw
author + adoc + asen +

4.2

aw
t +b

)
aw
t + b) ,

EXPERIMENTS

Datasets

Baseline methods

t=1

4.3

where log(p(Ψ |λ)) is the log-prior of parameters Ψ . The
w
w
w
quantities aw
doc , asen , aa and at are defined in equations (7)(10). Using stochastic gradient descent, we update
∂Ji (U, Ψ )
∂vec(wi )
∂Ji (U, Ψ )
+α
i
∂uw
t

vec(wi ) ← vec(wi ) + α
wi
i
uw
t ← ut

IDM
2442
1016

In the experiments, the proposed IDM model is compared
with several baseline methods, including Latent Dirichlet
Allocation (LDA) model[3], Author-Topic (AT) model [8],
Dynamic Topic model (DTM) [2], Gaussian Mixture Neural
Topic Model (GMNTM) [11] and Author-Topic over Time
(AToT) model [10]. We use the same settings for all hyper
parameters as in their original papers.

t=1
m
∑

w

AToT
2580
1066

We use the NIPS paper data and the Enron emails data
in our experiment. Data preprocessing is executed before
training the models. We remove non-alphabet characters,
numbers, pronoun, punctuation and stop words from the
text. Then, stemming is applied to reduce the vocabulary
size and settle the issue of data spareness. The detailed
properties of the datasets are described as follow.
NIPS papers (NIPS): This dataset is a collection of papers from the NIPS conference between 1987 and 19991 . After data preprocessing, the data set contains 1,740 research
papers with 2,037 authors. Each timestamp is determined
by the year of the proceedings. Following the preprocessing
in [8, 12], the 1,740 papers are further divided into a training
set of 1,557 papers and a test set of 183 papers of which 102
are single-authored papers.
Enron emails (Enron): This data was originally published by the FERC during its investigations2 . We further
clean the corpus by removing computer generated files from
each user and remove “quoted original messages” in replies.
Finally, there are 75,686 messages in our Enron data set. We
further divide the data into a training set of 65,686 messages
and a test set of 10,000 messages. Each author has at least
one document in the training set.

We estimate the model parameters and latent vectors λ, U
and Ψ by maximizing the likelihood of the generative model.
The parameter estimation consists of two stages. In Stage I,
we maximize the likelihood of the model with respect to λ.
Since λ characterizes a Gaussian mixture model, this procedure can be implemented by the Expectation Maximization
(EM) algorithm. In Stage II, we maximize the model likelihood with respect to U and Ψ , this procedure can be implemented by stochastic gradient descent. We alternatively
execute Stage I and Stage II until the parameters converge.

− log

GMNTM
2490
1049

In this section, we test the IDM model on two publicly
available datasets. We compare our model with state-ofthe-art topic models with both quantitative and qualitative
evaluations.

3.3 Estimating model parameters

m
∑

DTM
2685
1095

i
for the word vector vec(wi ) and the associated weight uw
t .
w
w
Similarly, we update vec(s), vec(d), vec(a, d) and udoc , usen ,
uw
author using the same gradient step. Once the gradient
update is complete, we randomly sample another location
to continue the update. The procedure terminates when
there are suﬃcient number of updates performed, so that
both U and Ψ converge to stationary values.

The term q(z(a, d) = z) reflects the ratio of the author a’s
interests in topic z when she writes document d. In the
experiments, we demonstrate that such a scheme yields reasonable representation for the author’s interests.

w
w
Ji (U, Ψ ) = log(p(Ψ |λ)) + aw
author + adoc + asen +

AT
2766
1134

Table 1: Comparison of test perplexity per word

The topics of sentences, documents and author interest can
be inferred in the same way. Similarly, we can use the same
formula for inferring the author interest vectors’ associated
topics, i.e.
πz N (vec(a, d)|µz , Σz )
q(z(a, d) = z) = ∑T
.
k=1 πk N (vec(a, d)|µk , Σk )

LDA
2820
1208

Implementation details

In our model, the learning rate α is set to 0.05 and gradually reduced to 0.0001. For each word, the previous six
words in the same sentence are included in the context. For
easy comparison with other models, the word vector size
is set to p = 100. Increasing the word vector size further

(13)

1

(14)

2

803

Available at http://www.cs.nyu.edu/˜roweis/data.html
http://www.cs.cmu.edu/˜enron/

Time
1995
1996
1997
1998
1999

Neural networks
Bengio Y, Sejnowski T, LeCun Y, Giles C, Ruppin E
Giles C, Horn D, Bengio Y, LeCun Y, Giles C
Bengio Y, Tresp V, Niranjan M, LeCun Y, Giles C
LeCun Y, Spence C, Niranjan M, Bengio Y, Tresp V
Bengio Y, Tresp V, Bengio S, Doucet A, LeCun Y

We can also extract individual author’s interests over time.
In this paper, we report M. I. Jordan, who has published a
suﬃcient number of paper on the NIPS conference. We show
the top 3 most likely topics of the corresponding author (by
equation (12)). The research interest evolution for the selected authors between 1995 and 1999 are reported in Table
3. As shown by Table 3, the most likely topics of the author
are varying during the experimental period. For instance,
Jordan’s research interest is mainly focused on “expert networks” and “dynamical model” in the early years, then it
expanded to Reinforcement learning and Graphical models.
This can be verified from his homepage.

Table 2: Top 5 most likely authors
Time
1995
1996
1997
1998
1999

M. I. Jordan
Mixtures model, HMM, Probabilistic model
Belief networks, Graphical models, Mixture model
Belief networks, Graphical models, EM
EM, Mixture model, Unsupervised learning
Bayesian learning, Graphical models, Probabilistic model

5.

CONCLUSION

In this paper, we have proposed a interest drift model
(IDM) for topic modeling. The IDM model achieves significantly lower perplexity compared to the state-of-the-art
methods, indicating that our model finds high-quality topics.

Table 3: Top 5 most likely topics
improves the quality of the topics. To perform comparable
experiments with restricted vocabulary, words outside of the
vocabulary is replaced by a special token and doesn’t count
into the word perplexity calculation.

6.

REFERENCES

[1] C. M. Bishop. Pattern recognition and machine
learning, volume 1. springer New York, 2006.
[2] D. M. Blei and J. D. Laﬀerty. Dynamic topic models.
In Proceedings of the 23rd ICML, pages 113–120.
ACM, 2006.
[3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent
dirichlet allocation. the Journal of machine Learning
research, 3:993–1022, 2003.
[4] A. Daud. Using time topic modeling for
semantics-based dynamic research interest finding.
Knowledge-Based Systems, 26:154–163, 2012.
[5] N. Kawamae. Author interest topic model. In
Proceedings of the 33rd international ACM SIGIR,
pages 887–888. ACM, 2010.
[6] Q. V. Le and T. Mikolov. Distributed representations
of sentences and documents. arXiv preprint
arXiv:1405.4053, 2014.
[7] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and
J. Dean. Distributed representations of words and
phrases and their compositionality. In Advances in
Neural Information Processing Systems, pages
3111–3119, 2013.
[8] M. Rosen-Zvi, T. Griﬃths, M. Steyvers, and
P. Smyth. The author-topic model for authors and
documents. In Proceedings of the 20th conference on
Uncertainty in artificial intelligence, pages 487–494.
AUAI Press, 2004.
[9] X. Wang and A. McCallum. Topics over time: a
non-markov continuous-time model of topical trends.
In Proceedings of the 12th ACM SIGKDD, pages
424–433. ACM, 2006.
[10] S. Xu, Q. Shi, X. Qiao, L. Zhu, H. Jung, S. Lee, and
S.-P. Choi. Author-topic over time (atot) a dynamic
users interest model. In Mobile, Ubiquitous, and
Intelligent Computing, pages 239–245. Springer, 2014.
[11] M. Yang, T. Cui, and W. Tu. Ordering-sensitive and
semantic-aware topic modeling. In Proceedings of the
28th AAAI, 2015.
[12] M. Yang, D. Zhu, and K.-P. Chow. A topic model for
building fine-grained domain-specific emotion lexicon.
In ACL (2), pages 421–426, 2014.

4.4 Quantitative evaluation
To evaluate the predictive power of the proposed model,
we compare test-set perplexity of our model with that of the
baselines on both NIPS and Enron data sets. Following the
same evaluation as in [8], for the NIPS corpus we calculate
the perplexity for the 102 single-authored test document.
Whereas, for the Enron corpus, we calculate the perplexity
for the 1000 held-out documents that are sampled from the
test sets. After running the algorithm that estimates the
vector representation of words, sentences, documents and
author interests, the averaged test perplexity is defined as
)
(
1 ∑
log p(wd |ad , Dtrain ) ,
exp −
Ntest w
where Ntest are the total number of words in the held-out
test documents and p(wd |ad , Dtrain ) is the probability assigned to the words wd in the test document, conditioned
on the known authors ad of the test document and training
data Dtrain .
Table 1 shows the results of the perplexity comparison on
both data sets, with T = 100 topics. The IDM model is
clearly better than the other models, as illustrated by its
relatively low perplexity score on both data sets. This is
because that the IDM model take the authorship information and the timestamps information into consideration. In
addition, it also use the ordering of the words.

4.5 Author interest evolution analysis
An interesting feature of our model is the capability of
discovering those authors who have drifting interests. This
interest-tracking feature is crucial in recommender systems
and in assisting investigators finding criminal activities such
as insider threats. Due to the limited space, in this experiment, we test our model on the NIPS dataset between 19951999. We take Neural Networks topic as an example. For
the selected topic, we choose the top 5 most likely authors
according to equation (12). The results are illustrated in
Table 2. By examining the author’s homepage, our results
are consistent with the truth. For example, Bengio was interested in neural network over the whole period, while Sejnowski shown great interest in neural network only in the
early phase.

804

