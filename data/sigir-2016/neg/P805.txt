Distributional Random Oversampling
for Imbalanced Text Classification
Alejandro Moreo, Andrea Esuli

Fabrizio Sebastiani

Istituto di Scienza e Tecnologie dell’Informazione
Consiglio Nazionale delle Ricerche
56124 Pisa, Italy
E-mail: {firstname.lastname}@isti.cnr.it

Qatar Computing Research Institute
Hamad bin Khalifa University
Doha, Qatar
E-mail: fsebastiani@qf.org.qa

ABSTRACT

haviour, with few classes having high prevalence and very
many classes having low or very low prevalence.
Severe imbalance is known to degrade the performance of
a number of inductive learning algorithms, such as decision
trees, neural networks, or support vector machines [8]. The
main approaches previously proposed for solving this problem may be grouped into the following classes: [7, 9]: (i)
data-level approaches, which perform a random resampling
of the dataset in order to rebalance class prevalences (ii) algorithmic approaches, which focus on adapting traditional
classification methods to scenarios where data are imbalanced; and (iii) cost-sensitive learning approaches, that combine the data-level and algorithmic approaches by imposing
a higher cost on the misclassification of examples from the
minority class. We here focus on approaches of type (i), most
of which rely on oversampling the minority class (i.e., adding
new minority-class training examples, typically duplicates or
quasi-duplicates of the existing ones) and/or undersampling
the majority class (i.e., removing some majority-class examples from the training set), with the goal of rebalancing the
class distribution in the training set.
We propose a novel method based on oversampling the
minority class, and specifically designed to deal with types
of data (such as text) where the distributional hypothesis
(according to which the meaning of a feature is somehow
determined by its distribution in large corpora of data – see
[6]) may be assumed to hold. Our method, dubbed Distributional Random Oversampling (DRO), consists of extending the standard vector representation (based on the bagof-words model) with random latent dimensions based on
distributional properties of the observed features. We assign to each document a discrete probabilistic function that
operates in a latent space and is queried as many times as
desired in order to oversample a given document (i.e., to
produce distributionally similar versions of it). Since this
generative function is based on the distributional hypothesis, the expectation is that the variability introduced in
the newly generated examples reflects semantic properties
of the terms that occur in the document being oversampled.
We present the results of experiments conducted on popular text classification benchmarks such as Reuters-21578,
OHSUMED-S, and RCV1-v2.
Our method is presented in Section 2; Section 3 discusses
our empirical results, while Section 4 concludes.

The accuracy of many classification algorithms is known to
suffer when the data are imbalanced (i.e., when the distribution of the examples across the classes is severely skewed).
Many applications of binary text classification are of this
type, with the positive examples of the class of interest far
outnumbered by the negative examples. Oversampling (i.e.,
generating synthetic training examples of the minority class)
is an often used strategy to counter this problem. We present
a new oversampling method specifically designed for classifying data (such as text) for which the distributional hypothesis holds, according to which the meaning of a feature
is somehow determined by its distribution in large corpora
of data. Our Distributional Random Oversampling method
generates new random minority-class synthetic documents
by exploiting the distributional properties of the terms in
the collection. We discuss results we have obtained on the
Reuters-21578, OHSUMED-S, and RCV1-v2 datasets.

1.

INTRODUCTION

Many applications of binary text classification exhibit severe data imbalance, i.e., are characterized by sets of data
in which the examples of one class are far outnumbered by
the examples of the other. Such cases are especially frequent
in information retrieval and related tasks, where the binary
distinction to be captured is between a class of interest and
“the rest”, i.e., between the (typically few) documents relevant to a certain concept (e.g., as expressed by a query) and
the (typically many) documents unrelated to it. This phenomenon is exacerbated in applications of multi-label multiclass (MLMC) text classification, i.e., applications where,
given a set C = {c1 , ..., c|C| } of classes, each document may
be labelled by several classes at the same time1 . In these
applications the average prevalence (i.e., relative frequency)
of a class is low, since C typically exhibits a power-law be1
MLMC classification is typically solved by training |C| independent binary classifiers, one for each class of interest.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

2.

SIGIR ’16, July 17-21, 2016, Pisa, Italy

THE LATENT SPACE OVERSAMPLING
FRAMEWORK

We assume a binary classification context, with classes
C = {c, c}. Let T r = {d1 , . . . , d|T r| } be a set of training

c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2914722

805

documents and F = {t1 , . . . , t|F | } its vocabulary. We use
W|T r|×|F | to denote the document-term matrix, where wij ∈
R is the weight of term tj in document di as computed by
#
a weighting function. By d i ∈ R|F | we denote the vectorial
representation of document di .
We present a general framework for oversampling, that we
dub Latent Space Oversampling (LSO); our Distributional
Random Oversampling method will be a specific instantiation of it. In LSO we oversample minority-class documents
by extending the original feature space F with an additional
latent space L. Each new synthetic example ok for a doc#
ument di will be expressed as #
o k = [ d i ; #
v k ] ∈ R|F |+|L| ,
#
|F |
where d i ∈ R
is the (fixed) observed part in the original feature space (i.e., a copy of the i-th row of W ), and
#
v k ∈ R|L| is the variable part in the latent space L, which
is generated by some stochastic function.
The vector expansion involves a two-step process for each
document di , i.e., (i) the estimation of model parameters Θi
for di via a parameter estimation criterion Ψ(W, di ), such
that Θi ← Ψ(W, di ) is calculated only once for each example
di ; and (ii) the generation of the variable part #
v k ← G(Θi ),
obtained by means of a generation function G. This function is called several times for each minority-class example
until the desired level of balance is reached, and exactly once
for each majority-class example, since we neither oversample
nor undersample majority-class examples. The oversampled
matrix is then re-weighted (e.g., in order to bring to bear
updated idf values, and in order to perform correct length
normalization) before training the classifier. Each test document dt is also expanded to the enlarged vector space before
being fed to the classifier; the only difference with the expansion process we carry out for training documents is that any
global knowledge involved in the estimation of parameters
Θt comes from the training data.
Different oversampling strategies could thus be defined by
considering different parameter estimation criteria Ψ and
different generation functions G. In the following sections
we first illustrate one possible such strategy, based on probabilistic topic models (Section 2.1); we then present our DRO
method based on the distributional hypothesis (Section 2.2).

2.1

a dedicated latent dimension for each term in the vocabulary,
i.e., |L| = |F |. LDO assumes each minority-class document
to be governed by similar topic distributions, causing the
variable part of oversampled documents to exhibit topically
similar patterns.

2.2

Distributional Random Oversampling

We propose Distributional Random Oversampling (DRO), a
different instantiation of LSO. DRO is based on the hypothesis that related documents (such as, e.g., the minority-class
documents) may be expected to contain semantically similar
words, and relies on a direct application of the distributional
hypothesis, by virtue of which the meaning of feature tj is
#
embedded in column t Tj ∈ R|T r| of matrix W . Unlike in
LDO, we here take weight wij to be generated by a realvalued weighting function such as, e.g., tfidf or BM25.
As the parameter estimation criterion we take a function
ΨDRO that returns Θi = (pi1 , . . . , pi|T r| ), where pik will be
used as parameters of a multinomial distribution for document di . Parameter pik is computed as
X #T
t j [k] · wij · s(tj )
pik =

tj ∈di
|T r|

(1)

X X #T
t j [k] · wij · s(tj )

k=1 tj ∈di

#
i.e., by (i) summing together the k-th components t Tj [k] of
#T
the (length-normalized) feature vectors t j (i.e., the columns
of the W matrix) corresponding to all unique terms tj ∈ di ,
weighted by (a) their relative importance with respect to
the document (the wij component) and by (b) their relative
importance with respect to the classification task (the s(tj )
P|T r|
component)2 , and (ii) normalizing to satisfy k=1 pik = 1.
We will choose a generation function GDRO that returns
a vectorial representation of a bag of n (latent) words, each
of which is drawn from lk ∼ Multinomial (Θi ). Note that
in this case |L| = |T r|. Similarly to the case of LDO we
set n = length(di ), so that sparsity is preserved in the enlarged feature space. In contrast to LDO, the multinomial
distribution of DRO is deterministically obtained from the
training collection, thus avoiding the need for computationally expensive statistical inference methods.
Each test document is also expanded to the enlarged vector space before being fed to the classifier. In this case note
#
that, in Equation 1, t Tj – which encodes the distributional
knowledge – and s(tj ) are the supervised components, i.e.,
they are obtained from T r. Instead, wij is computed partly
from the document itself (e.g., the tf component) and partly
from the training set (e.g., the idf component).
In sum, the rationale of our method is to generate synthetic minority-class vectors where the part corresponding
to the latent space is the result of a generative process that
brings to bear the distributional properties of the words contained in the document being oversampled.

Latent Dirichlet Oversampling

One possible instantiation of the LSO framework is what we
will here call Latent Dirichlet Oversampling (LDO). LDO
relies on Latent Dirichlet Allocation (LDA – [1]), a probabilistic topic model that assumes, in order to define the
model parameters and the generative function, that each
(observed) document in a collection is generated by a mixture of (unobserved) topics. As the weight wij we here take
the raw number of occurrences of term tj in document di .
As the parameter estimation criterion ΨLDO we may choose
any Bayesian inference method (such as Variational Bayes
or Gibbs Sampling). The document-specific model parameters are Θi = [θi ; ϕ], where θi is the topic distribution of di
and ϕ is the per-topic word distribution obtained from T r.
We will choose a generation function GLDO that returns a
vectorial representation of a bag of n words, each of which is
drawn by first choosing a topic zk ∼ M ultinomial(θi ), and
then choosing a term tj ∼ M ultinomial(ϕzk ). We set n =
length(di ) (i.e., to the total number of word occurrences in
di ) so that the synthetic bag of words will allocate the same
number of term occurrences as the original document (thus
preserving sparsity in the new space). Note that, in this case,
the latent space is mirroring the original feature space, with

3.

EXPERIMENTS

As the datasets for our experiments we use Reuters-21578,
OHSUMED-S, and RCV1-v2. All these collections are multilabel, i.e., each document may be labelled by zero, one, or
2
In this paper we compute s as the mutual information between the feature and C = {c, c}.

806

3

Available from http://bit.ly/1F8AFcO
Available from http://1.usa.gov/1mp7RGr
5
For LDO we used the Gensim implementation of LDA
(see http://bit.ly/1Rl7pFV) which also allows estimating
the document-topic distribution of test examples.
6
For this method, as suggested in [3], we used the MATLAB
implementation of Gibbs sampling available at http://bit.
ly/1Rl7DNl
7
http://svmlight.joachims.org/
4

Dataset
Reuters-21578
Ohsumed-S
RCV1-v2

Training
9,603
12,358
23,149

Test
3,299
3,652
60,074

Features
23,563
26,382
37,211

Classes
115
97
101

HP
3
9
16

LP
50
60
73

VLP
62
28
12

LDO

DRO

VLP

DECOM

LP

BSMOTE

HP

SMOTE

VLP

RO

LP

BoW

F1M

HP

α

Prev.

Table 1: Details on the 3 datasets used.

F1µ

several classes at the same time, which gives rise to |C| binary classification problems, with C the set of classes in the
dataset. For Reuters-215783 we use the standard (“ModApté”)
split, which identifies 9,603 training documents and 3,299
test documents. We restrict our attention to the 115 classes
with at least one positive training example. OHSUMED-S
[4] consists instead of 12,358 training and 3,652 test MEDLINE textual records from 1987 to 1991, classified according
to 97 MeSH index terms. RCV1-v24 comprises 804,414 news
stories generated by Reuters from Aug 20, 1996, to Aug 19,
1997. In our experiments we use the entire training set, containing all 23,149 news stories written in Aug 1996; for the
test set we pick the 60,074 news stories written during Sep
1996. We restrict our attention to the 101 classes with at
least one positive training example.
As the evaluation measures we use microaveraged F1 (F1µ )
and macroaveraged F1 (F1M ).
We compare the performance of LDO5 and DRO with
the following baselines: (i) Random Oversampling (RO),
a method that performs oversampling by simply duplicating random minority-class examples; (ii) Synthetic Minority Oversampling Technique (SMOTE – [2]), a method that
generates new synthetic minority-class examples as convex
linear combinations of the document di being sampled and a
document randomly picked among the k minority-class nearest neighbours of di (typically using k = 5); (iii) BorderlineSMOTE (BSMOTE – [5]), a more recent version of SMOTE
that only oversamples those borderline minority-class examples that would be misclassified as negatives by a k-NN
classifier; (iv) DECOM [3], a probabilistic topic model that
assumes all documents belonging to the same class to follow
the same topic distribution that, once determined, is used to
oversample minority-class examples following the LDA generation procedure6 ; (v) a bag-of-words model (BoW) where
no oversampling is performed. For LDA-based methods we
follow the related literature and set the number of topics to
30; in order to favour convergence, i.e., to allow the system
to find a stationary point for the distribution parameters Θ
that maximize the posterior probability of the corpus, we
set the number of iterations to 3,000 and perform 10 passes
over the corpus in each iteration.
As the learner of our experiments we adopt linear-kernel
SVMs (in the popular SVM-light implementation7 ); in all
our experiments we use the default SVM-light parameters.
All methods are fed with the same preprocessed version of
the datasets where, for each distinct binary decision problem, the top 10% most informative words have been selected,
using mutual information as the selection function and tfidf
as the weighting function. We perform oversampling of the
minority class until a desired prevalence α for the minorityclass is reached; we let α range on {0.05, 0.10, 0.15, 0.20}.
We do not consider undersampling in this paper, i.e., all
negative examples are picked exactly once. The results we
present are all averages across 5 random trials we have run
for each setting. For each dataset we partition the classes

.05
.10
.15
.20
.05
.10
.15
.20
.05
.10
.15
.20
.05
.10
.15
.20
.05
.10
.15
.20
.05
.10
.15
.20

.907
.907
.907
.907
.633
.633
.633
.633
.426
.426
.426
.426
.954
.954
.954
.954
.767
.767
.767
.767
.132
.132
.132
.132

.907
.907
.910
.909
.700
.682
.662
.648
.485
.456
.473
.473
.954
.952
.953
.953
.788
.778
.770
.764
.319
.272
.269
.269

.907
.911
.911
.911
.754
.718
.684
.654
.478
.416
.395
.387
.954
.953
.953
.953
.809
.784
.756
.731
.428
.357
.302
.277

.907
.904
.902
.899
.678
.678
.678
.678
.426
.426
.426
.426
.954
.952
.951
.950
.782
.783
.783
.782
.212
.212
.212
.212

.907
.912
.911
.911
.650
.639
.629
.629
.441
.418
.398
.398
.954
.954
.953
.955
.773
.762
.750
.738
.315
.280
.250
.240

.907
.909
.908
.911
.706
.690
.679
.664
.484
.482
.476
.474
.954
.953
.952
.952
.790
.786
.777
.774
.310
.308
.289
.287

.907
.897
.905
.899
.761
.766†
.759†
.764†
.568†
.568†
.567†
.570†
.954
.950
.951
.947
.810
.812†
.807†
.805†
.509†
.515†
.519†
.507†

Table 2: Results on Reuters-21578.
into (i) HighPrevalence (HP), the classes with a prevalence
higher than 0.050; (ii) LowPrevalence (LP), the classes with
a prevalence in the range [0.015, 0.050]; and (iii) VeryLowPrevalence (VLP), the classes with a prevalence smaller than
0.015. The reason for partitioning the classes according to
prevalence is to allow the results to provide insights as to
which classes benefit from oversampling and which do not.
Table 1 shows some details of the document collections
used in the experiments. Tables 2 to 4 report the results
of our experiments in terms of F1M and F1µ , for Reuters21578, OHSUMED-S, and RCV1-v2, respectively. Results
are reported at different levels α of oversampling; we use
boldface to highlight the best performing method, while
symbol “†” indicates that the method outperforms all others in a statistically significant sense8 . Note that, for each
block of 4 rows identifying a certain set of classes (HP, LP,
VLP), the results for BoW are always the same; this is obvious since there is no oversampling in BoW, which thus
does not depend on the value of α. Note also that, in all
three tables, the first row of the HP results for α = 0.05
always contains identical values, since the HP classes have
a prevalence ≥ 0.05.
Overall, the results of these experiments indicate that
DRO is superior to the other six methods presented (including LDO). In the low-prevalence groups (LP and VLP)
DRO is superior in most cases, across the different datasets
and the different degrees α of oversampling, and especially
so in terms of F1M ; when DRO is not superior, the differences in performance with the top-performing method are
8
Two-tailed t-test on paired examples at 0.05 confidence
level.

807

LP

VLP

DRO

HP

LDO

VLP

.05
.10
.15
.20
.05
.10
.15
.20
.05
.10
.15
.20
.05
.10
.15
.20
.05
.10
.15
.20
.05
.10
.15
.20

.843
.843
.843
.843
.489
.489
.489
.489
.048
.048
.048
.048
.877
.877
.877
.877
.638
.638
.638
.638
.106
.106
.106
.106

.843
.848
.848
.846
.600
.602
.597
.594
.249
.237
.228
.220
.877
.878
.877
.876
.676
.685
.680
.676
.408
.391
.379
.367

.843
.848
.848
.845
.616
.603
.584
.563
.257
.210
.186
.172
.877
.878
.877
.875
.674
.672
.656
.637
.408
.343
.303
.283

.843
.846
.848
.848†
.573
.582
.583
.584
.148
.148
.148
.148
.877
.878
.878
.878
.666
.678
.680
.680
.268
.268
.268
.268

.843
.845
.845
.844
.577
.587
.591
.593
.263
.271
.265
.267
.877
.877
.877
.876
.663
.673
.675
.675
.426
.431
.429
.430

.843
.847
.847
.846
.613
.619
.617
.614
.269
.261
.252
.245
.877
.877
.876
.875
.677
.691†
.688†
.683†
.446
.437
.424
.415

.843
.839
.838
.838
.617
.631†
.632†
.629†
.295
.294†
.297†
.295†
.877
.873
.871
.869
.664
.683
.680
.674
.489†
.489†
.497†
.493†

Table 4: Results on RCV1-v2.

Table 3: Results on OHSUMED-S.

5.

fairly small. This superiority is more pronounced for the
VLP classes, where DRO obtained 23 out of 24 best results,
almost always with very large margins. In the HP classes,
instead, our results do not reveal any clear winner, since the
best results are haphazardly distributed among all of the
baselines. Moreover, the best system is not substantially
better to BoW in the vast majority of cases, which makes
the idea of oversampling such classes questionable.
In sum, the results seem to indicate that the smaller the
prevalence of the minority class is, the higher is the gain
that can be obtained due to the use of DRO. This is an appealing feature for an oversampling method. We attribute
this behaviour to DRO’s distributional nature, which enables the information of the entire collection to contribute
in the generation of each synthetic example (whereas RO
and SMOTE-based methods are limited to local information provided by one or two examples, respectively). This
could be advantageous for ill-defined classes (as those belonging to LP and VLP). It may instead introduce noise,
or even some redundancy, for well-defined ones (i.e., those
in HP); this suggests that the best policy may be that of
applying DRO to low- or very-low prevalence classes only,
while leaving high-prevalence classes untouched.

4.

LP

DECOM

HP

BSMOTE

.753
.752
.753
.756
.588
.588†
.578
.576
.451
.469
.455
.476
.801
.798
.795
.795
.647
.640
.626
.620
.552†
.570†
.553†
.553†

SMOTE

.753
.757
.765
.769
.569
.565
.555
.542
.396
.378
.373
.376
.801
.803
.804
.805
.668
.665
.658
.652
.313
.262
.243
.251

RO

.753
.755
.760
.763
.538
.532
.525
.523
.354
.352
.330
.314
.801
.802
.804
.805
.647
.644
.640
.633
.365
.328
.311
.291

BoW

.753
.754
.763
.767
.571
.570
.569
.568
.458
.448
.448
.448
.801
.802
.805
.806
.666
.669
.666†
.666†
.437
.416
.416
.416

α

.753
.756
.767
.771
.603
.578
.550
.524
.455
.433
.440
.427
.801
.803
.806
.807
.672
.654
.625
.595
.518
.484
.446
.415

Prev.

DRO

.753
.758
.764
.769
.557
.552
.526
.514
.385
.372
.363
.364
.801
.803
.804
.805
.662
.657
.645
.642
.299
.241
.198
.200

F1M

LDO

.753
.753
.753
.753
.479
.479
.479
.479
.354
.354
.354
.354
.801
.801
.801
.801
.616
.616
.616
.616
.282
.282
.282
.282

F1µ

DECOM

VLP

BSMOTE

LP

SMOTE

F1µ

HP

RO

VLP

BoW

LP

α

Prev.
F1M

HP

.05
.10
.15
.20
.05
.10
.15
.20
.05
.10
.15
.20
.05
.10
.15
.20
.05
.10
.15
.20
.05
.10
.15
.20

REFERENCES

[1] David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
Latent Dirichlet allocation. Journal of Machine
Learning Research, 3:993–1022, 2003.
[2] Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O.
Hall, and W. Philip Kegelmeyer. SMOTE: Synthetic
minority over-sampling technique. Journal of Artificial
Intelligence Research, 16(1):321–357, 2002.
[3] Enhong Chen, Yanggang Lin, Hui Xiong, Qiming Luo,
and Haiping Ma. Exploiting probabilistic topic models
to improve text categorization under class imbalance.
Information Processing & Management, 47(2):202–214,
2011.
[4] Andrea Esuli and Fabrizio Sebastiani. Improving text
classification accuracy by training label cleaning. ACM
Transactions on Information Systems, 31(4):Article 19,
2013.
[5] Hui Han, Wen-Yuan Wang, and Bing-Huan Mao.
Borderline-SMOTE: A new over-sampling method in
imbalanced data sets learning. In Proceedings of the 1st
International Conference on Intelligent Computing
(ICIC 2005), pages 878–887, Hefei, CN, 2005.
[6] Zellig S. Harris. Distributional structure. Word,
10(23):146–162, 1954.
[7] Haibo He and Edwardo A. Garcia. Learning from
imbalanced data. IEEE Transactions on Knowledge and
Data Engineering, 21(9):1263–1284, 2009.
[8] Nathalie Japkowicz and Shaju Stephen. The class
imbalance problem: A systematic study. Intelligent
Data Analysis, 6(5):429–449, 2002.
[9] Yanmin Sun, Andrew K. Wong, and Mohamed S.
Kamel. Classification of imbalanced data: A review.
International Journal of Pattern Recognition and
Artificial Intelligence, 23(4):687–719, 2009.

CONCLUSIONS

We have presented a new oversampling method for imbalanced text classification, based on the idea of assigning a
probabilistic generative function to each minority-class document in the training set, a function that can be iteratively
queried until the desired level of balance is reached. This
probabilistic function is built upon distributional representations of the words contained in the document being oversampled, which allows the model to introduce some random
variability in the new examples while preserving the underlying semantic properties motivated by the distributional
hypothesis.

808

