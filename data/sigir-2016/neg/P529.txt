Contextual Bandits in A Collaborative Environment
Qingyun Wu1 , Huazheng Wang1 , Quanquan Gu2 , Hongning Wang1
1
Department of Computer Science
Department of Systems and Information Engineering
University of Virginia, Charlottesville VA, 22904 USA

2

{qw2ky,hw7ww,qg5w,hw5x}@virginia.edu

ABSTRACT

cus on information that raises user interest and, simultaneously, the
need to explore new information for globally improving user experience create an explore-exploit dilemma. Significant research attention has been paid on multi-armed bandit algorithms [16, 19, 4,
5], which provide a principled solution of the dilemma. Intuitively,
bandit algorithms designate a small amount of traffic to collect user
feedback while improving their estimation qualities in realtime.
With the available side information about users or items to be
presented, contextual bandits have become a reference solution [3,
10, 21, 14]. Specifically, contextual bandits assume the expected
payoff is determined by a conjecture of unknown bandit parameters and given context, which is represented as a set of features
extracted from both users and recommendation candidates. Such
algorithms are especially advantageous when the space of recommendation is large but the payoffs are interrelated. They have been
successfully applied in many important applications, e.g., content
recommendation [21, 6] and display advertising [11, 23].
However, a common practice in contextual bandit algorithms estimates the unknown bandit parameters pertaining to each user independently. This unfortunately ignores dependency among users.
Due to the existence of social influence [13], e.g., content and
opinions sharing among friends in a social network, exploiting the
dependency among users raises new challenges and opportunities
in personalized information services. For example, in many realworld applications, e.g., content recommendation in Facebook or
Twitter, because of the mutual influence among friends and acquaintances, one user’s click decision on the recommended items
might be greatly influenced by his/her peers. This indicates the
knowledge gathered about the interest of a given user can be leveraged to improve the recommendation to his/her friends, i.e., collaborative learning. In other words, the observed payoffs from a
user’s feedback might be a compound of his/her own preference
and social influence he/she receives, e.g., social norms, conformity
and compliance. As a result, propagating the knowledge collected
about the preference of one user to his/her related peers can not
only capitalize on additional information embedded in the dependency among users, which is not available in the context vectors;
but also helps conquer data sparsity issue by reducing the sample
complexity of preference learning (e.g., known as cold-start in recommender systems [26]). Failing to recognize such information
among users will inevitably lead to a suboptimal solution.
In this work, we develop a collaborative contextual bandit algorithm that explicitly models the underlying dependency among
users. In our solution, a weighted adjacency graph is constructed,
where each node represents a contextual bandit deployed for a single user and the weight on each edge indicates the influence between a pair of users. Based on this dependency structure, the
observed payoffs on each user are assumed to be determined by
a mixture of neighboring users in the graph. We then estimate the
bandit parameters over all the users in a collaborative manner: both

Contextual bandit algorithms provide principled online learning solutions to find optimal trade-offs between exploration and exploitation with companion side-information. They have been extensively
used in many important practical scenarios, such as display advertising and content recommendation. A common practice estimates
the unknown bandit parameters pertaining to each user independently. This unfortunately ignores dependency among users and
thus leads to suboptimal solutions, especially for the applications
that have strong social components.
In this paper, we develop a collaborative contextual bandit algorithm, in which the adjacency graph among users is leveraged to
share context and payoffs among neighboring users while online
updating. We rigorously prove an improved upper regret bound
of the proposed collaborative bandit algorithm comparing to conventional independent bandit algorithms. Extensive experiments
on both synthetic and three large-scale real-world datasets verified
the improvement of our proposed algorithm against several stateof-the-art contextual bandit algorithms.

CCS Concepts
•Information systems → Recommender systems; •Theory of
computation → Regret bounds; •Computing methodologies →
Sequential decision making;

Keywords
Collaborative contextual bandits; online recommendations; reinforcement learning

1.

INTRODUCTION

Satisfying users with personalized information plays a crucial
role for online service providers to succeed in market. However,
the rapid appearance of new information and new users together
with the ever-changing nature of content popularity make traditional recommendation approaches, e.g., collaborative filtering [7,
25], incompetent. Modern information service systems now adopt
online learning solutions to adaptively find good mappings between
available content and users. During online learning, the need to foPermission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

SIGIR ’16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2911528

529

3.

context and received payoffs from one user are prorogated across
the whole graph in the process of online updating. The proposed
collaborative bandit algorithm establishes a bridge to share information among heterogenous users and thus reduce the sample complexity of preference learning. We rigorously prove that our collaborative bandit algorithm achieves a remarkable reduction of upper
regret bound with high probability, comparing to the linear regret
with respect to the number of users if one simply runs independent
bandits on them. Extensive experiment results on both simulations
and large-scale real-world datasets verified the improvement of the
proposed algorithm compared with several state-of-the-art contextual bandit algorithms. In particular, our algorithm greatly alleviates the cold-start challenge, in which encouraging performance
improvement is achieved on new users and new items.

2.

METHODOLOGY

We develop a contextual bandit algorithm in a collaborative environment, where the adjacency graph among users is leveraged
to share context and payoffs between neighboring bandits during
online update. We provide rigorous proof of the resulting upper regret bound, which has a significant regret reduction comparing to
running independent bandit algorithms on the same collection of
users. In the following discussions, we will first describe the notations and our model assumptions about the collaborative bandit
problem, then carefully illustrate our developed bandit algorithm
and corresponding regret analysis.

3.1

Bandits in a Collaborative Setting

We consider a contextual bandit problem with a finite, but possibly large, number of arms, which correspond to the candidate item
set to be presented (e.g., articles in a content recommendation system). We denote the arm set as A and the cardinality of A as K.
There are N different users in this collection. At each trial t, a
learner observes a given user ut from user collection and a subset of arms from A, where each arm a is associated with a feature
vector xat ,ut ∈ Rd summarizing the information of both user ut
and arm at at trial t. The learner displays an arm at to ut and receives the corresponding payoff rat ,ut from the user. The goal of
the learner is to update its arm-selection strategy with respect to the
new observations, such that after T trials its regret with respect to
the oracle arm selection strategy is minimized. In particular, the
accumulated T -trial regret for all users is defined formally as,

RELATED WORK

Multi-armed bandit algorithms provide principled solutions to
the explore/exploit dilemma, which exists in many real-world applications, such as display advertisement selection [11, 23], recommender systems [21, 6], and search engine systems [24, 28]. As
opposed to the traditional K-armed bandit problems [5, 4, 16, 19],
feature vectors in contextual bandits are created to infer the conditional expected payoff of an action [3, 12, 20, 21]. The setting
for contextual bandit with linear payoffs was first introduced in [3],
where the expectation of payoff for each action is assumed to be a
linear function of its context vector. In the follow-up research [21,
12], LinUCB is introduced to use ridge regression to compute the
expected payoff of each action and corresponding confidence interval. Later on, generalized linear models are introduced to parameterize bandit algorithms for non-linear payoffs [14]. Comparing
to their context-free counterparts, contextual bandits have achieved
superior performance in various application scenarios [21, 14].
The idea of modeling dependency among bandits has been explored in prior research [8, 9, 10, 15, 17]. Studies in [2, 27] explore
contextual bandits with assumptions about probabilistic dependencies on the product space of context and actions. Hybrid-LinUCB
[21] is such an instance, which uses a hybrid linear model to share
observations across users. Social network structures are explored
in bandit algorithms for introducing possible dependencies [9, 15].
In [8], parallel context-free K-armed bandits are coupled by the
social network structure among the users, where the observed payoffs from neighboring nodes are shared as side-observations to help
estimate individual bandits. Besides utilizing existing social networks for modeling relatedness among bandits, there is also work
automatically estimates the bandit parameters together with the dependency relation among them, such as clustering the bandits via
the learned model parameters during online updating [15]. Some
recent work incorporates collaboration among bandits via matrix
factorization based collaborative filtering techniques: Kawale et al.
preformed online matrix factorization based recommendation via
Thompson sampling [18], and Zhao et al. studied interactive collaborative filtering via probabilistic matrix factorization [29].
The most similar work to ours studied in this paper is the GOB.Lin
algorithm introduced in [10]. GOB.Lin requires connected users in
a network to have similar bandit parameters via a graph Laplacian
based model regularization. As a result, GOB.Lin explicitly requires the learned bandit parameters across related users to be close
to each other. In our algorithm, we do not have such strong assumption about each individual bandit, but we make explicit assumptions
about the reward generation via an additive model: neighboring
users’ judgements of the recommendations will be shared across,
i.e., word-of-mouth, to explain the observed payoffs in different
users. This gives us the flexibility in capturing the heterogeneity
of preferences among different users in practice, and leads to both
theoretically and empirically improved results.

R(T ) =

T
X
t=1

Rt =

T
X

(ra∗t ,ut − rat ,ut )

(1)

t=1

where a∗t is the best arm to display to user ut at trial t according to
the oracle strategy, ra∗t ,ut is the corresponding optimal payoff, and
Rt is the regret from all users at trial t.
In standard linear contextual bandit problems, the payoffs of
each arm with respect to different users are assumed to be governed
by a noisy version of an unknown linear function of the context vectors [3, 21]. Specifically, each user i is assumed to associate with an
unknown parameter θi ∈ Rd (with kθi k ≤ 1), which determines
the payoff of at by rat,i = xTat,i θi + t , where t is drawn from
a Guassian distribution N (0, σ 2 ). θs are independently estimated
based on the observations from each individual user. However, due
to the existence of mutual influence among users, an isolated bandit
can hardly explain all the observed payoffs even for a single user.
For example, the context vectors fail to encode such dependency.
To capitalize on the additional information embedded in the dependency structure among users (i.e., θ for different users), we propose
to study contextual bandit problems in a collaborative setting.
In this collaborative environment, we place the bandit algorithms
on a weighted graph G = (V, E), which encodes the affinity relationship among users. Specifically, each node vi ∈ {V1 , ..., VN }
in G hosts a bandit parameterized by θi for user i; and the edges
in E represent the affinity relation over pairs of users. This graph
can be described as an N × N stochastic matrix W. In this matrix,
each element wij is nonnegative and proportional to the influence
that user j has on user i in determining the payoffs of different
arms. wij = 0 if and only if user j has no influence on user
P
i. W is column-wise normalized such that N
j=1 wij = 1 for
i ∈ {1, ...., N }. In this work, we assume W is time-invariant and
known to the learner beforehand.
Based on the graph G, collaboration among bandits happens
when determining the payoff of a particular arm with respect to a
given user. To denote this, we define a d × N matrix Θ, which
consists of parameters from all the bandits in the graph: Θ =
(θ1 , . . . , θN ). Accordingly, we define a context feature matrix

530

Algorithm 1 Collaborative LinUCB

With the collaborative assumption about the expected payoffs
defined in Eq (2), we appeal to ridge regression for estimating the
unknown bandit parameter θ for each user. In particular, we simultaneously estimate the global bandit parameter matrix Θ for all the
users as follows,

N ×N

1: Inputs: α ∈ R+ ,λ ∈ [0, 1], W ∈ R
T
2: Initialize: A1 ← λI, b1 ← 0, ϑ̂1 ← A−1
1 b1 , C1 ← (W ⊗
I)A−1
(W
⊗
I),
1
3: for t = 1 to T do
4:
Receive user ut
5:
Observe context vectors, xat ,ut ∈ Rd for ∀a ∈ A
b t W) +
6:
Take action at = arg maxa∈A X̊aTt ,ut vec(Θ
q
α X̊aTt ,ut Ct X̊at ,ut
7:
Observe payoff rat ,ut
8:
At+1 ← At + vec(X̊at ,ut WT )vec(X̊at ,ut WT )T
9:
bt+1 ← bt + vec(X̊at ,ut WT )rat ,ut
10:
Ct+1 ← (WT ⊗ I)A−1
t+1 (W ⊗ I)
−1
11:
ϑ̂t+1 ← At+1 bt+1
12: end for

T
X
λ
b = arg max 1
Θ
(X̊aTt ,ut vec(Θt W) − rat ,ut )2 + tr(ΘT Θ)
2 t=1
2
Θ
(3)
where λ ∈ [0, 1] is a trade-off parameter of l2 regularization in
ridge regression.
Since the objective function defined in Eq (3) is quadratic with
respect to Θ, we have a closed-form estimation of Θ as ϑ̂t =
b
A−1
t bt , in which ϑ̂ = vec(Θ) and At and bt are computed as,

At =λI +

t
X

vec(X̊at0 ,ut0 WT )vec(X̊at0 ,ut0 WT )T

(4)

t0 =1

bt =

Xt = (xat,1 , . . . , xat,N ), where the ith column is the context vector xat,i for arm a at trial t selected for user i. The collaboration
among bandits characterized by the influence matrix W results in
a new bandit parameter matrix Θ̄ = ΘW, which determines the
payoff rat ,ut of arm at for user ut at trial t by,
rat ,ut − diagut (XTt ΘW) ∼ N (0, σ 2 )

vec(X̊at0 ,ut0 WT )rat0 ,ut0

(5)

t0 =1

where I is a dN × dN identity matrix.
The effect of collaboration among bandits is clearly depicted
in the above estimation of Θ. Matrix At and vector bt store
global information shared among all the bandits in the graph. More
specifically, the context vector xat ,ut and payoff rat ,ut observed
in user ut at trial t can be propagated through the whole graph
via the relational matrix W to other users. To understand this,
note that vec(X̊at ,ut WT ) is a dense vector with projected context vectors on every user, while the original X̊at ,ut is a sparse
vector with observations only at active users ut . Because of this
information sharing, at certain trial t, although some users might
have not generate any observation yet (i.e., cold-start), they can
already start from a non-random initialization of their bandit parameters θi . It is easy to verify that when W is an identity matrix,
i.e., users have no influence among each other, the estimation of
Θ degenerates to independently computing N different θs (since
vec(X̊at ,ut WT ) = X̊at ,ut ). And the mutual influence will be
maximized when W is a uniform matrix, i.e., all the users have
equivalent influence to each other. We have to emphasize that the
benefit of this collaborative estimation of Θ is not to just simply
compute the θs in an integrated manner; but because of the collaboration among users, the estimation uncertainty of all θs can be
quickly reduced comparing to simply running N independent bandit algorithms. This in turn leads to an improved regret bound. We
will elaborate the effect of collaboration in online bandit learning
with more theoretical justifications in Section 3.3.
b predict the expected payoff
The estimated bandit parameters Θ
of a particular arm for each user according to the observed context
feature matrix Xt . To complete an adaptive bandit algorithm, we
need to design the exploration strategy for each user. Our collaborative assumption in Eq (2) implies that rat ,ut across users are
independent given Xt and W. As a result, for any σ, i.e., the standard deviation of Gaussian noise in Eq (2), the following inequality
holds with probability at least 1 − δ,
q
T
|ra∗t ,ut − rat ,ut | ≤ αt vec(X̊ut WT )T A−1
t vec(X̊ut W ) (6)

(2)

where diagut (X) is the operation returning the ut -th element in
the diagonal of matrix X. Eq (2) postulates our additive assumption
about reward generation in this collaborative environment: the reward rat ,ut is not only determined by user ut ’s own preference on
the arm at (i.e., wut ut xTat ,ut θut ), but also by the judgements from
P
the neighbors who have influence on ut (i.e., j6=ut wut j xTat,j θj ).
This enables us to distinguish a user’s intrinsic preference of the
recommended content from his/her neighbors’ influence, i.e., personalization. In addition, the linear payoff assumption in our model
is to simplify the discussion in this paper; and it can be relaxed via
a generalized linear model [14] to deal with nonlinear rewards.
We should note that our model assumption about the collaborative bandits is different from that specified in the GOB.Lin model
[10]. In GOB.Lin, connected users in the graph are required to
have similar underlying bandit parameters, i.e., via graph Laplacian regularization over the learned bandit parameters. And their
assumption about reward generation follows conventional contextual bandit settings, i.e., rewards are independent across users. In
our setting, neighboring users do not have to share similar bandit
parameters, but they will generate influence on their neighbors’ decisions. This assumption is arguably more general, and it leads to
an improved upper regret bound and practical performance. Theoretical comparison between these two algorithms will be rigorously
discussed in Section 3.3.

3.2

t
X

Collaborative Linear Bandit Algorithm

To simplify the notations in our following discussions, we define two long context feature vectors and a long bandit parameter vector based on the vectorize operation vec(·). We define
Xat = vec(Xat ) = (xTat,1 , . . . , xTat,N )T , which is a concatenation of context feature vectors of the chosen arm at at trial t for all
the users. And we define X̊at ,ut = vec(X̊at ,ut ) , in which X̊at ,ut
is a special case of Xat : only the column corresponding to the user
ut at time t is set to xTat ,ut , and all the other columns are set to
zero. This corresponds to the situation that at trial t the learner
only needs to interact with one user. Correspondingly, we define
T T
ϑ = vec(Θ) = (θ1T , θ2T , ..., θN
) ∈ RdN as the concatenation of
bandit parameter vectors over all the users.

where αt is a parameter in our algorithm defined in Lemma 1 of
Section 3.3 and δ is embedded in the computation of αt . The proof
of this inequality can be found in the Appendix.
The inequality specified in Eq (6) gives us a reasonably tight upper confidence bound (UCB) for the expected payoff of a particular
arm over all the users in the graph G, from which a UCB-style
arm-selection strategy can be derived. In particular, at trial t, we

531

choose an arm for user ut according to the following arm-selection
strategy,

b t W)
at,ut = arg max X̊aTt ,ut vec(Θ
(7)

relationship between the proposed collaborative bandit algorithm
and conventional independent bandit algorithms in a more intuitive
way, we can make a very specific assumption about how a sequential learner interacts with a set of users. Assuming all the users are
evenly served by CoLin, i.e., each user interacts with the learner
T
times. When W is an identity matrix, the regret bound of
T̄ = N
CoLin degenerates to the case of running
N independent LinUCB,
√
whose upper regret bound is O(N√ T̄ ln T̄ ). When W is uniform,
T̄
the regret bound reduces to O(N T̄ ln N
), where we achieves an
√
O( T̄ ln N ) regret reduction comparing to running N independent LinUCBs on each single user. The proof of regret bound in
this special case is given in the Appendix.
It is necessary to compare the derived upper regret bound of
CoLin with that in the GOB.Lin algorithm [10], which also exploits
the relatedness among a set of users. In GOB.Lin, the divergence
among every pair of bandits (if connected in the graph) is measured
by Euclidean distance between the learned bandit parameters. In
its upper regret bound, such divergence is accumulated throughout
the iterations. In extreme case where users are all connected but
associate with totally distinct bandit parameters, GOB.Lin’s upper
regret bound could be much worse than running N independent
bandits, due to this additive pairwise divergence. While in our algorithm, such divergence is controlled by the multiplicative factor
PT P 2
t=1
j wut j ≤ T . We can rigorously prove the following inequalities between the upper regret bound of CoLin (RC (T )) and
GOB.Lin (RG (T )) always holds,
X
2T
2
2
)
kθi∗ − θj∗ k2
0 ≤ RG
(T ) − RC
(T ) ≤ 16T N ln(1 +
dN 2

a∈ A

+αt

q

T
vec(X̊ut WT )T A−1
t vec(X̊ut W )

We name this resulting algorithm as Collaborative Linear Bandit,
or CoLin in short. The detailed description of CoLin is illustrated
in Algorithm 1, where we use the property that vec(X̊ut WT ) =
(W ⊗ I)vec(X̊ut ) = (W ⊗ I)X̊t to simplify Eq (7).
Another evidence of the benefit from collaboration among users
is demonstrated in Algorithm 1. When estimating the confidence
interval of the expected payoff for action at in user ut at trial t,
CoLin not only considers the prediction confidence from bandit ut ,
but also that from its neighboring bandits (as described by the Kronecker product between W and I). When W is an identity matrix,
such effect disappears. Clearly, this collaborative confidence interval estimation will help the algorithm quickly reduce estimation
uncertainty, and thus leads to the optimal solution more rapidly.
One potential issue with CoLin is its computational complexity:
matrix inverse has to be performed on At at every trial. First, because of the rank one update of matrix At (8th step in Algorithm
1), quadratic computation complexity is possible via applying the
Sherman-Morrison formula. Second, we may compute A−1
in a
t
mini-batch manner to further reduce computation with some extra
penalty in regret. We will leave this as our future research.

3.3

Regret Analysis

In this section, we provide detailed regret analysis of our proposed CoLin algorithm. We first prove that the estimation error of
b is upper bounded in Lemma 1.
bandit parameters Θ
Lemma 1: For any δ > 0, with probability at least 1 − δ, the
estimation error of bandit parameters in CoLin is bounded by,
s
Pt
PN
2


√
t0 =1
j=1 wut0 j
∗
−2 ln(δ)+ λkϑ∗ k
kϑ̂t −ϑ kAt ≤ dN ln 1+
λdN
q
in which kϑ̂t − ϑ∗ kAt = (ϑ̂t − ϑ∗ )T At (ϑ̂t − ϑ∗ ), i.e., the
matrix norm induced by the positive semidefinite matrix At
Based on Lemma 1, we have the following theorem about the
upper regret bound of the CoLin algorithm.
Theorem 1: With probability at least 1 − δ, the cumulated regret of
CoLin algorithm satisfies,
s
PT PN
2 

t=1
j=1 wut j
R(T ) ≤ 2αT 2dN T ln 1 +
(8)
λdN
in which αT is the upper bound of kϑ̂T − ϑ∗ kAT and it can be
explicitly calculated based on Lemma 1. The detailed proof of this
theorem is provided in the Appendix.
As shown in Theorem 1, the graph structure plays an important role in the upper regret bound of our CoLin algorithm. Consider two extreme cases. First, when W is an identity matrix,
i.e., no influence
among users, the upper regret bound degener√
T
ates to O(N T ln N
). Second, when the graph is fully connected
and uniform, i.e., ∀i, j, wij = N1 , such that users have homogeneous influence among each
√ other, and the upper regret bound of
CoLin decreases to O(N T ln NT2 ). That means via collabora√
tion, CoLin achieves an O( T ln N ) regret reduction for every
single user in the graph comparing to the independent case.
Note that the our regret analysis in Theorem 1 is in a very general form, in which we did not make any assumption about the order or frequency that each user will be served. To illustrate the

532

(i,j)∈E

It is clear to notice that if there is no influence between the users
in the collection, i.e., no edge in G, these two algorithms’ regret
bound touches (both degenerate to N independent contextual bandits). Otherwise, GOB.Lin will always lead to a worse and faster
growing regret bound than our CoLin algorithm.
In addition, limited by the use of graph Laplacian, GOB.Lin can
only capture the binary connectivity relation among users. CoLin
differentiates the strength of connections with a stochastic relational graph. This makes CoLin more general when modeling the
relatedness among bandits and provides a tighter upper regret bound.
This effect is also empirically verified by our extensive experiments
on both synthetic and real-world data sets.

4.

EXPERIMENTS

We performed empirical evaluations of our CoLin algorithm against
several state-of-the-art contextual bandit algorithms, including N
independent LinUCB [21], hybrid LinUCB with user features [21],
GOB.Lin [10], and online cluster of Bandits (CLUB) [15]. Among
these algorithms, hybrid LinUCB exploits user dependency via a
set of hybrid linear models over user features, GOB.Lin encodes the
user dependency via graph-based regularization over the learned
bandit parameters, and CLUB clusters users during online learning
to enable model sharing. In addition, we also compared with several popularly used context-free bandit algorithms, including EXP3
[5], UCB1 [4] and -greedy [4]. But their performance is much
worse than the contextual bandits, and thus we do not include their
performance in the following discussions.
We tested all the algorithms on a synthetic data set via simulations, a large collection of click stream from Yahoo! Today Module
dataset [21], and two real-world dataset extracted from the social
bookmarking web service Delicious and music streaming service
LastFM [10]. Extensive experiment comparisons confirmed the advantage of our proposed CoLin algorithm against all the baselines.
More importantly, comparing to the baselines that also exploit user
dependencies, CoLin performs significantly better in identifying

10

0

200

LinUCB Θ̄

∗

Hybrid-LinUCB Θ̄
GOB.Lin Θ̄

180

∗

∗

CLUB Θ̄

∗

CoLin Θ̄

∗

400

CoLin Θ ∗

350

160

120
100

250

10

Regret

L2 Difference

300

140

Regret

LinUCB
Hybrid-LinUCB
GOB.Lin
CoLin

-1

200

150

80

100

60
LinUCB
Hybrid-LinUCB
GOB.Lin
CoLin

40
20
0

5000

10000

15000

20000

25000

50

10

30000

-2

0

5000

10000

20000

25000

30000

0

α = 0.2

α = 0.4

α = 0.6

α = 0.8

αt

Iteration

Iteration

(a) Convergence of cumulated regret

15000

(b) Accuracy of bandit parameter estimation

(c) The effect of α

Figure 1: Analysis of regret, bandit parameter estimation and parameter tuning.
in this experiment. We report the cumulated regret as defined in
Eq (1) and the Euclidean distance between the learnt bandit parameters from different algorithms and the ground-truth in Figure 1.
To reduce randomness in simulation-based evaluation, we reported
the mean and standard deviation of final regret from different algorithms after 30,000 iterations over 5 independent runs for results
in all following experiments. To increase visibility, we did not plot
error bars in Figure 1 (a) and (b).
As we can find in Figure 1 (a), simply running N independent
LinUCB algorithm gives us the worst regret, which is expected.
Hybrid LinUCB, which exploits user dependency via a set of hybrid linear models over user features performed better than LinUCB, but still much worse than CoLin. Although GOB.Lin also
exploits the graph structure when estimating the bandit parameters,
its assumption about the dependency among bandits is too restrictive to well capture the information embedded in the interaction
with users. We should note that in our simulation, by multiplying
the relational matrix W with the ground-truth bandit parameter matrix Θ∗ , the resulting bandit parameters Θ̄∗ align with GOB.Lin’s
assumption, i.e., neighboring bandits are similar. And Θ̄∗ is used in
reward generation. Therefore, our simulation does not produce any
bias against GOB.Lin. In Figure 1 (a) we did not include CLUB,
whose regret grew linearly. After looking into the selected arms
from CLUB, we found because of the aggregated decisions from
users in the automatically constructed user clusters, CLUB always
chose suboptimal arms, which led to a linearly increasing regret.
In Figure 1 (b), we compared accuracy of the learnt bandit parameters from different algorithms. Because of their distinct modeling assumptions, LinUCB, hybrid LinUCB, CLUB and GOB.Lin
cannot directly estimate Θ∗ , i.e., the true bandit parameters for
each user. Instead, they can only estimate Θ̄∗ , which directly governs the generation of observed payoffs. Only CoLin can estimate
both Θ̄∗ and Θ∗ . As we can find in the results, CoLin gave the
most accurate estimation of Θ̄∗ , which partially explains its superior performance in regret. We also find that LinUCB actually
achieved a more accurate estimation of Θ̄∗ than GOB.Lin, but its
regret is much worse. To understand this, we looked into the actual execution of LinUCB and GOB.Lin, and found that because of
the graph Laplacian regularization in GOB.Lin, it better controlled
exploration in arm selection and therefore picked the optimal arm
more often than LinUCB. Hyrbid LinUCB’s estimation of Θ̄∗ is
the worst, but it is expected: hybird LinUCB uses a shared model
and a personalized model to fit the observations. Comparing to
CoLin’s estimation quality of Θ̄∗ , its estimation of Θ∗ is much
worse. The main reason is that CoLin has to decipher Θ from the
estimated Θ̄ based on W, where noise is accumulated to prevent
accurate estimation. Nevertheless, this result demonstrates the possibility of discovering each individual user’s true preference from
their compound feedback. This is meaningful for many practical
applications, such as user modeling and social influence analysis.
We also notice that although CLUB’s estimated Θ̄∗ is almost
as good as LinUCB’s (as shown in Figure 1 (b)), its regret is the

users’ preference on less popular items (items that are only observed among a small group of users). This validates that with the
proposed collaborative learning among users, CoLin better alleviates the cold-start challenge comparing to the baselines.

4.1

Experiments on synthetic dataset

In this experiment, we compare the bandit algorithms based on
simulations and use the accumulated regret and bandit parameter
estimation accuracy as the performance metrics.

4.1.1

Simulation Setting

In simulation, we generate N users, each of which is associated
∗
).
with a d-dimensional parameter vector θ ∗ , i.e., Θ∗ = (θ1∗ , . . . , θN
Each dimension of θi∗ is drawn from a uniform distribution U (0, 1)
and normalized to kθi∗ k = 1. Θ∗ is treated as the ground-truth
bandit parameters for reward generation, and they are unknown to
bandit algorithms. We then construct the golden relational stochastic matrix W for the graph of users by defining wij ∝ hθi∗ , θj∗ i,
and normalize each column of W by its L1 norm. The resulting
W is disclosed to the bandit algorithms. In the end, we generate a
size-K action pool A. Each action a in A is associated with a ddimensional feature vector xa , each dimension of which is drawn
from U (0, 1). We also normalize xa by its L1 norm. To construct
user features for hybrid LinUCB algorithm, we perform Principle
Component Analysis (PCA) on the relational matrix W, and use
the first 5 principle components to construct the user features.
To simulate the collaborative reward generation process among
users, we first compute Θ̄∗ = Θ∗ W and then compute the payoff
of action a for user i at trial t as rat,i = diagi (XTt Θ̄∗ ) + t , where
t ∼ N (0, σ 2 ). To increase the learning complexity, at each trial t,
our simulator only discloses a subset of actions in A to the learning
algorithms for selection, e.g., randomly select 10 actions from A
without replacement. In simulation, based on the known bandit parameters Θ̄∗ , the optimal action a∗t,i and the corresponding payoff
ra∗t,i for each bandit i at trial t can be explicitly computed.
Under this simulation setting, we compared hybrid LinUCB, N
independent LinUCB, GOB.Lin, CLUB and CoLin. At each trial,
the same set of actions are presented to all the algorithms; and the
Gaussian noise t is fixed for all those actions at trial t. In our
experiments, we fixed the feature dimension d to 5, article pool size
K to 1000, and set the trade-off parameter λ for L2 regularization
to 0.2 in all the algorithms. We compared the regret of different
bandit algorithms during adaptive learning. Furthermore, since the
ground-truth bandit parameters are available in the simulator, we
also compared the quality of learned parameters in each contextual
bandit algorithm. This unveils the nature of each bandit algorithm,
e.g., how accurately they can recover a user’s true preference.

4.1.2

Results & Analysis

We first set the user size N to 100 and fix the standard deviation
σ to 0.1 in the Gaussian noise for reward generation. All the contextual bandit algorithms are executed up to 300 iterations per user

533

Table 1:
Bandit Size (N )
LinUCB
HybridLinUCB
GOB.Lin
CoLin

Cumulated regret with different bandit size (σ=0.1).
40
80
100
200
75.31±5.11
168.42±9.90 191.53±6.18 355.56±7.23
59.12±2.11
150.09±5.29 164.11±9.19 311.43±11.59
58.49±5.04
143.42±5.28 141.96±6.36 275.32±10.51
21.78±12.84 47.73±4.31
49.83±6.55
77.38±20.59

Table 2:
Noise (σ)
LinUCB
HybridLinUCB
GOB.Lin
CoLin

Cumulated regret with different noise level (N =100).
0.01
0.05
0.1
0.3
92.72±2.56 116.53±3.07 191.53 ±6.18 830.47±69.48
69.47±2.12
91.48±1.94
164.11±9.19 759.93±39.15
58.85±6.25
82.33±1.53
141.96±6.36 708.13±43.73
41.69±6.95 40.95±4.43
49.83±6.55
83.98±8.57

Table 3: Cumulated regret with different noise level on matrix W (N =100).
matrix noise (δ)
0
0.01
0.03
0.05
HybridLinUCB 164.11±9.19 171.74±7.67 171.51±13.31 163.93±9.19
GOB.Lin
141.96±6.36 163.28±5.63 164.36±6.66 169.87±11.89
CoLin
49.83±6.55
54.42±2.45
101.39±6.01 239.88±13.86
Table 4: Cumulated regret with different matrix sparsity level.
Sparsity (M/N )
20/100
40/100
60/100
80/100
HybridLinUCB 135.98±5.11 141.15±4.82 150.49±4.58 160.12±7.55
GOB.Lin
133.30±3.98 126.13±5.59 143.29±6.49 143.42±5.82
CoLin
39.74±8.80
30.76±3.66
37.29±3.55
49.56±8.88
be interesting to incorporate this factor in regret analysis to provide
more insight of collaborative bandit algorithms.
Thirdly, in CoLin, we have assumed the adjacency matrix W is
known to the algorithm beforehand. However, in reality one might
not precisely recover this matrix from noisy observations, e.g., via
social network analysis. It is thus important to investigate the robustness of collaborative bandit algorithms to a noisy W. We fixed
the user size N to 100 and corrupted the ground-truth adjacency
matrix W: add Gaussian noise N (0, δ) to wij and normalize the
resulting matrix. We refer to this noisy adjacency matrix as W0 .
The simulator still uses the true adjacency matrix W to compute
the reward of each action for a given user; while the noisy matrix W0 will be provided to the bandit algorithms, i.e., CoLin and
GOB.Lin. This newly introduced Gaussian noise is different from
the noise in generating the rewards as described in Eq (2).
From the cumulated regret shown in Table 3, we can find that
under moderate noise level, CoLin performed much better than
GOB.Lin; but CoLin is more sensitive to noise in W than GOB.Lin.
Because CoLin utilizes a weighted adjacency graph to capture the
dependency among users, it becomes more sensitive to the estimation error in W. While in GOB.Lin, because only the graph connectivity is used and the random noise is very unlikely to change the
graph connectivity, its performance is more stable. Further theoretical analysis of how an inaccurate estimation of W would affect the
resulting regret will be an interesting future work yet to explore.
Finally, the regret analysis of CoLin shows that its upper regret bound is related with the graph structure through the term
PT PN
2
t=1
j=1 wut j and GOB.Lin’s regret bound is related to the
graph connectivity [10]. We designed another set of simulation
experiments to verify the effect of graph structure on CoLin and
GOB.Lin. In this experiment, we set the user size N to 100 and
controlled the graph sparsity as follows: for each user in graph G,
we only included the edges from his/her top M most influential
neighbors (measured by the edge weight in W) in the adjacency
matrix, and normalized the resulting adjacency matrix to a stochastic matrix. No noise is added to W in this experiment (i.e., δ = 0).
As shown in Table 4, the regret of all bandit algorithms increases
as W becomes sparser, i.e., less information can be shared across

worst. As we described earlier, CLUB’s aggregated decision at
user cluster level constantly forced the algorithm to choose suboptimal arms; but the reward generation for each arm in our simulator follows that defined in Eq (2), which still provides validate
information for CLUB to estimate Θ̄∗ with reasonable accuracy.
In Figure 1 (c), we investigated the effect of exploration parameter αt ’s setting in different algorithms. The last column indexed
by αt represents the theoretical values of α computed from the algorithms’ corresponding regret analysis. As shown from results,
the empirically tuned α yields comparable performance to the theoretical values, and makes online computation more efficient. As a
result, in all our following experiments we use a fixed α instead of
a computed αt .
To further investigate the convergence property of different bandit algorithms, we examined the following four scenarios: 1) various user sizes N , 2) different noise level σ, 3) a corrupted affinity
matrix W, and 4) a sparse affinity matrix W, in reward generation. We report the results in Table 1 to 4. Because of its poor
performance, we did not include CLUB in those tables.
Firstly, in Table 1, we fixed the noise level σ to 0.1 and varied
the user size N from 40 to 200. We should note in this experiment
the total number of iterations varies as every user will interact with
the bandit learner 300 times. The regret in LinUCB goes linearly
with respect to the number of users, since no information is shared
across them. Via model sharing, hybrid LinUCB achieved some regret reduction compared with LinUCB; but its regret still increases
linearly with the number of users. Compared with the independent
bandits, we can clearly observe the regret reduction in CoLin with
increasing number of users. As we discussed in Section 3.3, although GOB.Lin exploits the dependency among users, its regret
might be even worse than running N independent LinUCBs, especially when the divergence between users is large.
Secondly, in Table 2, we fixed N to 100 and varied the noise level
σ from 0.01 to 0.3. We can notice that CoLin is more robust to noise
in the feedback: its regret grows much slower than all baselines.
Our current regret analysis does not consider the effect of noise in
reward, as long as it has a zero mean and finite variance. It would

534

1.5

2.0

Normalized Payoff

1.3

CTR-Ratio

9

M-LinUCB
Uniform-LinUCB
Hybrid-LinUCB

1.2

1.1

1.0

8
7

1.5

1.0

M-LinUCB
LinUCB
Uniform-LinUCB
Hybrid-LinUCB

0.5

0.9

0.8
0

5000

10000

15000

time

(a) Yahoo dataset

20000

0.0
0

500

1000

CoLin
GOB.Lin
CLUB

1500

2000

Normalized Payoff

CoLin
GOB.Lin
CLUB

1.4

6
5
4
3
2

M-LinUCB
N-LinUCB
Uniform-LinUCB
Hybrid-LinUCB

1
0
0

time

(b) Delicious dataset

500

1000

1500

CoLin
GOB.Lin
CLUB

2000

time

(c) LastFM dataset

Figure 2: Normalized reward on three real-world datasets
putational complexity: because the dimension of global statistic
matrix At defined in Eq (4) is dN ×dN , the running time of CoLin
scales quadratically with the number of users. It makes CoLin less
attractive in practical applications where the size of users is large.
One potential solution is to enforce sparsity in the estimated W matrix such that distributed model update is possible, i.e., only share
information within the connected users. The simulation study in
Table 4 confirms the feasibility of this direction and we will explore it in our future work.

users. We can observe that the regret of CoLin increases faster
than that in GOB.Lin, since more information becomes unavailable to CoLin. The results empirically verified that CoLin’s regret
bound is directly related to the graph structure described by the
P
P
2
term Tt=1 N
j=1 wut j and GOB.Lin’s regret bound is only related
to the graph connectivity.

4.2

Experiments on Yahoo! Today Module

In this experiment, we compared our CoLin algorithm with LinUCB, hybrid LinUCB, GOB.Lin and CLUB on a large collection
of ten days’ real traffic data from Yahoo! Today Module [21] using
the unbiased offline evaluation protocol proposed in [22].
The dataset contains 45,811,883 user visits to the Today Module in a ten-day period in May 2009. For each visit, both the user
and each of the 10 candidate articles are associated with a feature
vector of six dimensions (including a constant bias feature), constructed by a conjoint analysis with a bilinear model [21]. However,
this data set does not contain any user identity. This forbids us to
associate the observations with individual users. To address this
limitation, we first clustered all users into user groups by applying
K-means algorithm on the given user features. Each observation is
assigned to its closest user group. The weight in the adjacency matrix W is estimated by the dot product between the centroids from
K-means’ output, i.e., wij ∝ hui , uj i. The CoLin and GOB.Lin
algorithms are then executed over those identified user groups. For
LinUCB baseline, we tested two variants: one is individual LinUCBs running over the identified user groups and it is denoted as
M-LinUCB; another one is a uniform LinUCB shared by all the
users, i.e., it does not distinguish individual users, and it is denoted
as Uniform-LinUCB.
In this experiment, click-through-rate (CTR) was used to evaluate the performance of all bandit algorithms. Average CTR is
computed in every 2000 observations (not the cumulated CTR) for
each algorithm as the performance metric based on the unbiased offline evaluation protocol proposed in [22, 21]. Following the same
evaluation principle used in [21], we normalized the resulting CTR
from different bandit algorithms by the logged random strategy’s
CTR. We report the normalized CTR from different contextual bandit algorithms over the 160 derived user groups in Figure 2 (a).
CoLin outperformed all baselines on this real-world data set, except CLUB on the first day. Results from other user cluster sizes
(40 and 80) showed consistent improvement as demonstrated in
Figure 2 (a) with 160 user clusters; but due to space limit, we did
not include those results on cluster size of 40 and 80. As we can
find CLUB achieved the best CTR on the first day; but as some popular news articles became out-of-date, CLUB cannot correctly recognize their decreased popularity, and thus provided degenerated
recommendations. But in CoLin, because of the collaborative preference learning, it better controlled exploration-exploitation tradeoff and thus timely recognized the change of items’ popularity.
However, one potential limitation of CoLin algorithm is its com-

4.3

Experiments on LastFM & Delicious

The LastFM dataset is extracted from the music streaming service Last.fm (http://www.last.fm), and the Delicious data set is extracted the social bookmark sharing service website Delicious (https:
//delicious.com). These two datasets were generated by the Information Retrieval group at Universidad Autonomade Madrid for the
HetRec 2011 workshop with the goal of investigating the usage
of heterogeneous information in recommendation system1 . The
LastFM dataset contains 1892 users and 17632 items (artists). We
used the information of “listened artists” of each user to create payoffs for bandit algorithms: if a user listened to an artist at least once,
the payoff is 1, otherwise 0. The Delicious dataset contains 1861
users and 69226 items (URLs). We generated the payoffs using the
information about the bookmarked URLs for each user: the payoff
is 1 is the user bookmarked a particular URL, otherwise 0. Both of
these two data sets contain the users’ social network graph, which
makes them a perfect testbed for collaborative bandits.
Following the same settings in [10], we pre-processed these two
datasets in order to fit them into the contextual bandit setting. Firstly,
we used all tags associated with a single item to create a TF-IDF
feature vector, which uniquely represents the item. Then we used
PCA to reduce the dimensionality of the feature vectors. In both
datasets, we only retained the first 25 principle components to construct the context vectors, i.e., the feature dimension d = 25. We
then generate the candidate arm pool as follows: we fix the size of
candidate arm pool to be K = 25; for a particular user u, we pick
one item from those nonzero payoff items for user u according to
the whole observations in the dataset, and randomly pick the other
24 from those zero-payoff items for user u.
User relation graph is naturally extracted from the social network
information provided by the datasets. In order to make the graph
denser and make the algorithms computationally feasible, we performed graph-cut to cluster users into M clusters (following the
same setting as in [10]). Users in the same clusters are assumed to
have the same bandit parameters. In our experiments, M was set
to be 50, 100, and 200. Our reported results are from the setting of
M = 200, and similar results were achieved in other settings of M.
After user clustering, a weighted graph can be generated: the nodes
are the clusters of the original graph; and the edges between differ1
Datasets and their full description
http://grouplens.org/datasets/hetrec-2011

535

is

available

at

0.35

0.9
CoLin
GOB.Lin
M-LinUCB
CLUB
Hybrid-LinUCB

0.30
0.25

10 2

10 1

CoLin
GOB.Lin
M-LinUCB
CLUB
Hybrid-LinUCB

0.8
0.7

Precision

Delicious Dataset
LastFM Dataset

Precision

Number of Preferences

10 3

0.20
0.15

0.6
0.5
0.4
0.3

0.10

0.2
0.05

10 0 0
10

10

1

10

2

10

3

10

4

10

5

Item popularity Rank

0.00
0

0.1
10

20

30

40

50

60

Ranked item(by popularity) batch

(a) Item popularity of the two datasets

(b) Item-based Precision on Delicious

0.0
0

5

10

15

20

25

Ranked item(by popularity) batch

(c) Item-based Precision on LastFM

Figure 3: Item-based analysis on Delicious and LastFM datasets
distribution, less popular items are still challenging for all the compared algorithms.
This analysis motivates us to further analyze the user-level recommendation performance of different bandit algorithms, especially
to understand the effectiveness of collaboration among bandits in
alleviating the cold-start problem. To quantitatively evaluate this,
we first ranked the user clusters in a descending order with respect
to the number of observations in it. We then selected top 50 clusters as group 1. From the bottom 100 user clusters, we select 50
of them who are mostly connected to the users in group 1, as refer to them as group 2. The first group of users is called “learning
bucket” and the second group is called “testing bucket”. Based on
this separation of user clusters, we designed two different experiment settings: one is warm-start, another one is cold-start. In the
warm-start setting, we first run all the algorithms on the learning
bucket to estimate parameters for both group of users, such as At
and bt in CoLin. However, because user group 2 does not have
any observation in the learning bucket, their model parameters can
only be updated via collaboration among bandits, i.e., in CoLin
and GOBLin. Then with the model parameters estimated from the
learning bucket, we run and evaluate different algorithms on the
deployment bucket. Correspondingly, in the cold-start setting, we
directly run and evaluate the bandit algorithms on the deployment
bucket. It is obvious that since LinUCB assume users are independent and there is no sharing parameters among users, LinUCB’s
performance will not change under warm-start and cold start setting. While in CoLin, GOB.Lin and CLUB, because of the collaboration among users, model parameters are shared among users.
In this case, user preference information learned from the learning
bucket can be propagated to the deployment bucket.
We report the performance on the first 10% observations in the
deployment bucket instead of the whole observations in order to
better simulate the cold-start situation (i.e., all the algorithms do
not have sufficient observations to confidently estimate model parameters). In Figure 4 (a) and (b) we report the gap of cumulated
rewards from CoLin GOB.Lin, and CLUB between warm-start setting and cold-start setting, normalized by rewards obtained from
LinUCB. From Figure 4(a) we can notice that on Delicious dataset,
although at the very beginning of the warm-start setting both GOBLin and CoLin performed worse than the cold-start setting, both
algorithms in warm-start quickly improved and outperformed the
cold-start setting. One possible explanation is that the algorithms
might take the first several iterations to adapt the model propagated
from user group 1 to user group 2. In particular, from Figure 4 (a),
it is clear that once both algorithm are adapted, the improvement
between warm-start and cold-start on CoLin is larger than that on
GOB.Lin. This verified CoLin’s effectiveness in address the coldstart challenge. From Figure 4 (b), we can find that warm-start
helps both algorithms immediately at the first several iterations on
LastFM dataset. This might be caused by the flat distribution of
item popularity in this data set: users in group 2 also prefer the

ent clusters are weighted by the number of inter-cluster edges in the
original graph. In CoLin, we also need the diagonal elements in W,
which is undefined in a graph-cut based clustering algorithm. We
computed the diagonal elements based on the derived regret bound
of CoLin. Specifically, we first set wij ∝ c(i, j), where c(i, j)
is the number of edges between cluster i and j; then we optimize
PN PN 2
{wii }N
i=1 which minimizes the term
j wij .
i
We included three variants of LinUCB, hybrid LinUCB, GOB.Lin
and CLUB as our baselines. The three variants of LinUCB include:
1) LinUCB that runs independently on each user, denoted as NLinUCB; 2) LinUCB that is shared in each user cluster, denoted as
M-LinUCB (M is the number of clusters); 3) LinUCB that is shared
by all the users, denoted as Uniform-LinUCB. Following the setting it [10], GOB.Lin also operates at the user cluster level and
it takes the connectivity among clusters as input. We normalized
the cumulated reward in each algorithm by a random strategy’s cumulated reward, and compute the average accumulated normalized
reward in every 50 iterations. Note that user features required by
hybrid LinUCB are not given in these two datasets. We construct
the user features by applying PCA on CoLin’s adjacency matrix W
and retained the first 25 principle components.
From the results shown in Figure 2 (b) and (c), we can find that
CoLin outperforms all the baselines on both Delicious and LastFM
datasets. It’s worth noting that these two datasets are structurally
different, as shown in Figure 3 (a), the popularity of items on these
two data sets differs significantly: on LastFM dataset, there are a
lot more popular artists whom everybody listens to than the popular
websites which everyone bookmarks on Delicious dataset. Thus the
highly skewed distribution of item popularity makes recommendation on Delicious dataset much more challenging. Because most of
items are only bookmarked by a handful of users, exploiting the relatedness among users to propagate feedback become vital. While
on LastFM since there are much more popular items that most users
would like, most algorithms can easily recognize the quality of
items. In order to better understand this difference, we performed
detailed item-level analysis to examine the effectiveness of different algorithms on items with varied popularity. Specifically, we
first ranked all the items in these two datasets in a descending order
of item popularity and then examined the item-based recommendation precision from all the bandit algorithms, e.g., percentage of
item recommendations that are accepted by the users. In order to
better visualize the results, we further grouped ranked items into
different batches and report the average recommendation precision
over each batch in Figure 3 (b) and Figure 3 (c).
From the item-based recommendation precision results, we can
clearly find that on the LastFM dataset, CoLin achieved superior
performance against all the baselines in every category of items,
given the popularity of items in this data set is more balanced. On
Delicious dataset, CoLin achieved superior performance on topranked items, however, because of the skewness of item popularity

536

3
2
1
0

0.0

0.5

1.0

CoLin: Warm-Cold
GOB.Lin: Warm-Cold
CLUB: Warm-Cold

1.5

1
2
0

500

1000

1500

2000

2.0
0

500

1000

1500

2000

time

time

(a) Delicious

Percentage of users got improved.

Percentage of users got improved.

0.5

CoLin: Warm-Cold
GOB.Lin: Warm-Cold
CLUB: Warm-Cold

4

NormalizedReward

NormalizedReward

5

50%
Warm_CoLin
Warm_GOB.Lin
Warm_CLUB

40%

Cold_CoLin
Cold_GOB.Lin
Cold_CLUB

30%

20%

10%

0%

1%

2%

3%

5%

10%

First k% recommendations.

(b) LastFM

(c) Delicious

70%
Warm_CoLin
Warm_GOB.Lin
Warm_CLUB

60%

Cold_CoLin
Cold_GOB.Lin
Cold_CLUB

50%

40%

30%

20%

10%

0%

1%

2%

3%

5%

10%

First k% recommendations.

(d) LastFM

Figure 4: Effectiveness of collaboration and User-based analysis

7.

items liked by user group 1. We should note that the larger gap
from GOB.Lin than that from CoLin between warm-start and coldstart settings does not mean CoLin is worse than GOB.Lin; but
it indicates the cold-start CoLin learned faster than the cold-start
GOB.Lin on this data set. And the final performance of both coldstart and warm-start CoLin was better than GOB.Lin in the corresponding settings. We can also notice that cold-start CLUB performed very similarly as warm-start CLUB. It means the user clusters automatically constructed in CLUB does not help collaborative
learning. This experiment confirms that appropriate observation
sharing among users indeed helps address the cold-start problem in
recommendation.
Furthermore, we performed a user-based analysis to examine
how many users will benefit from the collaborative bandits setting. We define an improved user as the user who is served with
improved recommendations from a collaborative bandit algorithm
(i.e. CoLin, GOB.Lin and CLUB) than those from isolated LinUCBs. We report the percentage of improved users in the first 1%,
2%, 3%, 5%, and 10% observations during online learning process.
Figure 4 (c) and (d) demonstrate that on all collaborative bandit algorithms, the warm-start setting benefits much more users than in
the cold-start setting. This result further supports our motivation
of this research that collaboration among bandits can help alleviate
the cold-start challenge.

5.

[1] Y. Abbasi-yadkori, D. Pál, and C. Szepesvári. Improved algorithms
for linear stochastic bandits. In NIPS, pages 2312–2320. 2011.
[2] K. Amin, M. Kearns, and U. Syed. Graphical models for bandit
problems. Proceedings of UAI 2011, 2011.
[3] P. Auer. Using confidence bounds for exploitation-exploration
trade-offs. Journal of Machine Learning Research, 3:397–422, 2002.
[4] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the
multiarmed bandit problem. Mach. Learn., 47(2-3):235–256, May
2002.
[5] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. E. Schapire. Gambling
in a rigged casino: The adversarial multi-armed bandit problem. In
Foundations of Computer Science, 1995. Proceedings., 36th Annual
Symposium on, pages 322–331, 1995.
[6] D. Bouneffouf, A. Bouzeghoub, and A. L. Gançarski. A
contextual-bandit algorithm for mobile context-aware recommender
system. In Neural Information Processing, pages 324–331. 2012.
[7] J. S. Breese, D. Heckerman, and C. Kadie. Empirical analysis of
predictive algorithms for collaborative filtering. Technical Report
MSR-TR-98-12, Microsoft Research, May 1998.
[8] S. Buccapatnam, A. Eryilmaz, and N. B. Shroff. Multi-armed
bandits in the presence of side observations in social networks. In
Decision and Control (CDC), 2013 IEEE 52nd Annual Conference
on, pages 7309–7314. IEEE, 2013.
[9] S. Buccapatnam, A. Eryilmaz, and N. B. Shroff. Stochastic bandits
with side observations on networks. SIGMETRICS Perform. Eval.
Rev., 42(1):289–300, June 2014.
[10] N. Cesa-Bianchi, C. Gentile, and G. Zappella. A gang of bandits. In
Pro. NIPS, 2013.
[11] O. Chapelle and L. Li. An empirical evaluation of thompson
sampling. In Advances in Neural Information Processing Systems,
pages 2249–2257, 2011.
[12] W. Chu, L. Li, L. Reyzin, and R. E. Schapire. Contextual bandits
with linear payoff functions. In International Conference on
Artificial Intelligence and Statistics, pages 208–214, 2011.
[13] R. B. Cialdini and M. R. Trost. Social influence: Social norms,
conformity and compliance. 1998.
[14] S. Filippi, O. Cappe, A. Garivier, and C. Szepesvári. Parametric
bandits: The generalized linear case. In NIPS, pages 586–594, 2010.
[15] C. Gentile, S. Li, and G. Zappella. Online clustering of bandits. In
Proceedings of the 31st International Conference on Machine
Learning (ICML-14), pages 757–765, 2014.
[16] J. C. Gittins. Bandit processes and dynamic allocation indices.
Journal of the Royal Statistical Society. Series B (Methodological),
pages 148–177, 1979.
[17] S. Kar, H. Poor, and S. Cui. Bandit problems in networks:
Asymptotically efficient distributed allocation rules. In Decision and
Control and European Control Conference (CDC-ECC), 2011 50th
IEEE Conference on, pages 1771–1778, Dec 2011.
[18] J. Kawale, H. H. Bui, B. Kveton, L. Tran-Thanh, and S. Chawla.
Efficient thompson sampling for online matrix-factorization
recommendation. In Advances in Neural Information Processing
Systems, pages 1297–1305, 2015.
[19] T. L. Lai and H. Robbins. Asymptotically efficient adaptive
allocation rules. Advances in applied mathematics, 6(1):4–22, 1985.
[20] J. Langford and T. Zhang. The epoch-greedy algorithm for
multi-armed bandits with side information. In NIPS, pages 817–824.
2008.

CONCLUSIONS

In this paper, we developed CoLin algorithm to study contextual
bandit problem in a collaborative environment. In CoLin, context
and payoffs are shared among the neighboring bandits during online update, and it helps reduce the preference learning complexity
(i.e., requires less observations to achieve satisfactory prediction
performance) and leads to reduced overall regret. We religiously
proved a reduced upper regret bound comparing to independent
bandit algorithms. Experimental results based on extensive simulations and three real-world datasets consistently confirmed the improvement of the proposed collaborative bandit algorithm against
several state-of-the-art contextual bandit algorithms.
In our current setting, we have assumed the graph structure among
users is known to the algorithm beforehand. It is meaningful to
study how to dynamically estimate the graph structure during online update with provable regret bound. The computation complexity in CoLin is now expensive. Utilizing the sparse graph structure
to decentralize the computation is an important and interesting research direction. Besides, CoLin should not be only limited to linear payoffs; various types of link functions and regret definitions
can be explored as our future work.

6.

REFERENCES

ACKNOWLEDGMENTS

We thank the anonymous reviewers for their insightful comments.
This paper is based upon work supported by the National Science
Foundation under grant IIS-1553568.

537

(1) Based on CoLin’s arm selection strategy, if arm Xt is chosen at time t,
it must satisfy,

[21] L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit
approach to personalized news article recommendation. In
Proceedings of 19th WWW, pages 661–670. ACM, 2010.
[22] L. Li, W. Chu, J. Langford, and X. Wang. Unbiased offline
evaluation of contextual-bandit-based news article recommendation
algorithms. In Proceedings of 4th WSDM, pages 297–306. ACM,
2011.
[23] W. Li, X. Wang, R. Zhang, Y. Cui, J. Mao, and R. Jin. Exploitation
and exploration in a performance based contextual advertising
system. In Proceedings of 16th SIGKDD, pages 27–36. ACM, 2010.
[24] F. Radlinski, R. Kleinberg, and T. Joachims. Learning diverse
rankings with multi-armed bandits. In Proceedings of 25th ICML,
pages 784–791. ACM, 2008.
[25] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Item-based
collaborative filtering recommendation algorithms. In Proceedings
of 10th WWW, pages 285–295. ACM, 2001.
[26] A. I. Schein, A. Popescul, L. H. Ungar, and D. M. Pennock. Methods
and metrics for cold-start recommendations. In Proceedings of 25th
SIGIR, pages 253–260. ACM, 2002.
[27] A. Slivkins. Contextual bandits with similarity information. The
Journal of Machine Learning Research, 15(1):2533–2568, 2014.
[28] Y. Yue and T. Joachims. Interactively optimizing information
retrieval systems as a dueling bandits problem. In Proceedings of
26th ICML, pages 1201–1208. ACM, 2009.
[29] X. Zhao, W. Zhang, and J. Wang. Interactive collaborative filtering.
In Proceedings of the 22nd CIKM, pages 1411–1420. ACM, 2013.

vec(X̊ut WT )T ϑ̂t−1 + αt kvec(X̊ut WT )kA−1

t−1

≥ vec(X̊∗ut WT )T ϑ̂t−1 + αt kvec(X̊∗ut WT )kA−1

t−1

(2) Based on Cauchy-Schwarz inequality, we have,
vec(X̊∗ut WT )T ϑ̂t−1 + αt kvec(X̊∗ut WT )kA−1 − vec(X̊∗ut WT )T ϑ∗
t−1

≥ −kvec(X̊∗ut WT )T kA−1 kϑ̂t−1 − ϑ∗ kAt−1 + αt kvec(X̊∗ut WT )kA−1

t−1

t−1

≥ −αt−1 kvec(X̊∗ut WT )kA−1 + αt kvec(X̊∗ut WT )kA−1
t−1

and therefore, we have
vec(X̊∗ut WT )T ϑ̂t−1 +αt kvec(X̊∗ut WT )kA−1 ≥ vec(X̊∗ut WT )T ϑ∗
t−1

And according to Lemma 11 in [1], we have,
T

ln(

X
det(AT )
det(AT )
)≤
kvec(X̊ut WT )k2 −1 ≤ 2 ln(
)
At−1
det(λI)
det(λI)
t=1

Thus the accumulated regret at time T in CoLin can be bounded by,
v
v
u
u T
T
u
u X
X
t
R2 ≤ tT 4α2
kvec(X̊ WT )k2
R(T ) ≤ T
t

APPENDIX

vec(X̊a

t0 =1

0
t0 ,ut

 det(A ) 
T
det(λI)
s
P
PN
2
 T

t=1
j=1 wut j
≤ 2αT 2dN T ln
+1
λdN

WT )t0 − λϑ∗

Analysis for the special case of evenly served users :
If we assume users are evenly served, say each user is served T̄ = T /N
times. With this assumption, the regret of CoLin can be further written as,
v
u
t0X
=T̄ i
N u
X
u
Rt2
R(T ) ≤
tT̄

in which t0 is the Gaussian noise at time in reward generation.
P
Define St = tt0 =1 vec(X̊a 0 ,u0 WT )t0 , we have,
t

∗
ϑ̂t − ϑ∗ = A−1
t (St − λϑ )

i=1

Because St is a martingale, according to Theorem 1 and 2 in [1],
s
√
det(At )1/2 det(λI)−1
∗
kϑ̂t − ϑ kAt ≤ 2 ln(
) + λkϑ∗ k
δ
Since kxat,i k ≤ 1, trace(At ) ≤ λdN +

Pt

Pt
trace(At ) dN
)
det(At ) ≤ (
dN
we have det(λIdN ) ≤ λdN .

kϑ̂t −ϑ∗ kAt ≤

v
u
u
t

t0 =1

t0 =1
PN

PN

j=1

2
j=1 wu0 j
t

≤

N
X

t=T̄ (i−1)+1

s


T̄ ln2

i=1

(9)

(10)

det(At0 ) 
det(At0 ) 
+ λkϑ∗ k ln
det(At )
det(At )

(1) In the extreme case that W is identity matrix, the A matrix in CoLin
becomes a block diagonal matrix, which consists of Aui , i ∈ 1, .., N .
Q
ui
Using the block diagonal matrix’s property that det(A) = N
i det(A ),
the regret can be further simplified as,
N r 
X

R(T ) ≤
T̄ ln2 det(Aui ) + λkϑ∗ k ln(det(Aui )

2 , we have
wu
0j
t

)dN .

≤ (λ +
Similarly,
dN
Putting all these into Eq (9), we have,

i=1

Pt
dN ln(1 +

t0 =1

PN

2
j=1 wu0 j
t

λdN

) − 2 ln(δ)+ λkϑ∗ k

A0t = At + T̄ vec(X̊WT )vec(X̊WT )T
such that,

Rt = ra∗t ,ut − rat ,ut
vec(X̊∗ut WT )T ϑ∗

(11)


T̄
T̄
≤ N T̄ d2 ln2 (1 + ) + λkϑ∗ kd ln(1 + )
d
d
√ 
which is O N ln(T̄ ) T̄ and is identical with N independent LinUCB.
(2) In the extreme case where W is uniform, we have,
s

√

Proof of Theorem 1:
According to the definition of regret in Eq (1), the regret of CoLin at time
t can be written as,

=

T 8α2T ln

=

t0

t

A−1
t−1

t=1

s

Proof of Lemma 1:
Consider the objective function of ridge regression defined in Eq (3). By
taking the gradient of L(Θ) with respect to Θ and applying our model
assumption specified in Eq (2), we have,
t
X

ut

T

t=1

At (ϑ̂t − ϑ∗ ) =

t−1

≥0

det(A0t ) = det(At )(1 + T̄ kvec(X̊WT )k2
T T

− vec(X̊ut W ) ϑ

A−1
t

∗

)

(12)

By applying Eq(12) and according to Eq (10) the regret bound of CoLin
≤ vec(X̊ut WT )T ϑ̂t−1 + αt kvec(X̊ut WT )kA−1 − vec(X̊ut WT )T ϑ∗can be written as,
s
t−1

T̄
T̄ 
≤ kvec(X̊ut WT )kA−1 kϑ̂t−1 − ϑ∗ kAt−1 + αt kvec(X̊ut WT )kA−1
R(T ) ≤ N T̄ ln2 (1 +
) + λkϑ∗ k ln(1 +
)
t−1
t−1
λN
λN
T
≤ 2αt kvec(X̊ut W )kA−1
t−1
√ 
T̄
) T̄ .
which is O N ln( N
where the first inequality is based on the following two inequalities,

538

