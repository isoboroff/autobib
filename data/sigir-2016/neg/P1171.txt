Fairness in Information Retrieval
Aldo Lipani
Institute of Software Technology & Interactive Systems
Vienna University of Technology
Vienna, Austria

lipani@ifs.tuwien.ac.at

ABSTRACT

ing (low recall). But this last interpretation, due to the fact
that recall is inversely proportional to the number of relevant documents per topic, is very difficult to be addressed
if to be judged is just a portion of the collection of documents, as it is done when using the pooling method. To
tackle this problem, another kind of evaluation has been developed, based on measuring how much an IR system makes
documents accessible [1]. Accessibility measures can be seen
as a complementary evaluation to recall because they provide information on whether some relevant documents are
not retrieved due to an unfairness in accessibility.
The main goal of this Ph.D. is to increase the stability and
reusability of existing test collections, when to be evaluated
are systems in terms of precision, recall, and accessibility.
The outcome will be: the development of a novel estimator
to tackle the pool bias issue for P @n [4], and R@n, a comprehensive analysis of the effect of the estimator on varying
pooling strategies [3], and finally, to support the evaluation
of recall, an analytic approach to the evaluation of accessibility measures [2].

The offline evaluation of Information Retrieval (IR) systems
is performed through the use of test collections. A test collection, in its essence, is composed of: a collection of documents, a set of topics and, a set of relevance assessments
for each topic, derived from the collection of documents.
Ideally, for each topic, all the documents of the test collection should be judged, but due to the dimensions of the
collections of documents, and their exponential growth over
the years, this practice soon became impractical. Therefore, early in IR history, this problem has been addressed
through the use of the pooling method [5]. The pooling
method consists of optimizing the relevance assessment process by pooling the documents retrieved by different search
engines following a particular pooling strategy. The most
common one consists on pooling the top d documents of
each run. The pool is constructed from systems taking part
in a challenge for which the collection was made, at a specific
point in time, after which the collection is generally frozen in
terms of relevance judgments. This method leads to a bias
called pool bias, which is the effect that documents that were
not selected in the pool created from the original runs will
never be considered relevant. Thereby, this bias affects the
evaluation of a system that has not been part of the pool,
with any IR evaluation measures, making the comparison
with pooled systems unfair.
IR measures have evolved over the years and become more
and more complex and difficult to interpret. Witnessing a
need in industry for measures that ‘make sense’, I focus on
the problematics of the two fundamental IR evaluation measures, Precision at cut-off P @n and Recall at cut-off R@n.
There are two reasons to consider such ‘simple’ metrics: first,
they are cornerstones for many other developed metrics and,
second, they are easy to understand by all users. To the eyes
of a practitioner, these two evaluation measures are interesting because they lead to more intuitive interpretations like,
how much time people are reading useless documents (low
precision), or how many relevant documents they are miss-

CCS Concepts
•Information systems → Retrieval effectiveness; Test
collections;

Keywords
Pool Bias, Pooling Method, P@n, R@n, Accessibility

Acknowledgements
This research was supported by the Austrian Science Fund
(FWF) project number P25905-N23 (ADmIRE).

REFERENCES
[1] L. Azzopardi and V. Vinay. Accessibility in information
retrieval. In Proc. of ECIR’08, pages 482–489.
[2] A. Lipani, M. Lupu, A. Aizawa, and A. Hanbury. An
initial analytical exploration of retrievability. In Proc.
of ICTIR’15, pages 329–332.
[3] A. Lipani, M. Lupu, and A. Hanbury. The curious
incidence of bias corrections in the pool. In Proc. of
ECIR’16, pages 267–279.
[4] A. Lipani, M. Lupu, and A. Hanbury. Splitting water:
Precision and anti-precision to reduce pool bias. In
Proc. of SIGIR’15, pages 103–112.
[5] K. Spärck Jones and C. J. van Rijsbergen. Report on
the need for and provision of an “ideal” information
retrieval test collection. British Library Research and
Development Report No. 5266, page 44.

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).

SIGIR ’16 July 17-21, 2016, Pisa, Italy
c 2016 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-4069-4/16/07.
DOI: http://dx.doi.org/10.1145/2911451.2911473

1171

