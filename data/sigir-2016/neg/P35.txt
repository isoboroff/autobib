Searching by Talking:
Analysis of Voice Queries on Mobile Web Search
Ido Guy
Information Systems Engineering Department
Ben Gurion University of the Negev, Beer Sheva, Israel
Yahoo Research, Haifa, Israel

idoguy@acm.org

ABSTRACT

use of voice has also been promoted by the increasing popularity of voice-activated intelligent assistants, such as Siri,
Google Now, and Cortana. These assistants provide contextbased query-less personalized advice for mobile users, but
also enable web search [18]. A recent survey of 1400 U.S.
smartphone users found that 55% of the teenagers use voice
search every day [3, 14]. It is therefore becoming important
for information retrieval researchers and practitioners to understand this new medium of search and its differences from
traditional text search.
Using voice as a means to search holds various potential
advantages. Although typing usability has improved in recent years, querying by voice is still likely to be substantially
easier and faster for the vast majority of mobile users. For
users with visual or manual impairment, or with limited literacy skills, voice search may break down the entry barrier
into web search. In addition, as searching by voice does not
require visual attention or the use of hands, it can be performed in situations such as driving, cooking, or exercising,
where typed search might be especially cumbersome, errorprone, and even dangerous. In the aforementioned survey,
78% of the teens who used voice search pointed out its usefulness for multitasking as a key motivating factor [14].
In spite of its growing popularity, the area of voice search
has not received much attention in the IR literature. Early
work compared voice and text queries in a laboratory study,
however these did not represent typical web search queries,
but rather complex long questions [10]. More recent work
has mostly focused on voice recognition [1, 8, 27, 31, 38, 43]
and query reformulation [3, 19, 33]. A few studies revealed
more details about how voice search is performed on commercial search engines [30, 40], however we are not aware of
a systematic log analysis of voice queries as of yet.
In this work, we perform a query log analysis of half a million voice queries, issued to the mobile application of a commercial web search engine, over a period of six months. The
log includes English-only queries, from the United States,
transcribed from voice to text using high-quality ASR. We
compare the voice queries with a similar-size sample of mobile text queries, typed on the same mobile application. Our
comparison inspects characteristics of context, clicks, sessions, and, primarily, the query text itself. We examine
both semantic and syntactic features and compare them for
voice versus text queries. In the final part of our analysis, we directly compare the similarity of the voice and text
query language to natural language corpora, which include
traditional news articles and the titles of questions in a large
community question answering (CQA) website.

The growing popularity of mobile search and the advancement in voice recognition technologies have opened the door
for web search users to speak their queries, rather than type
them. While this kind of voice search is still in its infancy,
it is gradually becoming more widespread. In this paper,
we examine the logs of a commercial search engine‚Äôs mobile
interface, and compare the spoken queries to the typed-in
queries. We place special emphasis on the semantic and
syntactic characteristics of the two types of queries. We
also conduct an empirical evaluation showing that the language of voice queries is closer to natural language than
typed queries. Our analysis reveals further differences between voice and text search, which have implications for the
design of future voice-enabled search tools.
Keywords: mobile search; spoken queries; voice search

1.

INTRODUCTION

The popularity of search from mobile devices (mobile search)
has rapidly increased in recent years [34]. In fact, the number of mobile queries has already exceeded the number of
those submitted from desktop devices in the United States
and other countries [32]. A prominent characteristic of the
advancement in mobile search is the emergence of voice search,
allowing users to input queries in a spoken language and then
retrieve the relevant entries based on system-generated transcriptions of the voice queries [19]. Recent developments in
speech recognition, backed by high bandwidth coverage and
high-quality speech signal acquisition, are enabling higher
quality voice search [8]. Already in 2010, Google presented
a case study stating that their goal is to make voice search
ubiquitously available and that a level of performance was
achieved such that usage is growing [30]. Since then, further enhancements to automatic speech recognition (ASR)
for web search have been reported [31, 43], taking advantage
of the large data that started to accumulate on voice search
logs [8], and applying advanced learning methods [17]. The
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

SIGIR ‚Äô16, July 17-21, 2016, Pisa, Italy
¬© 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2911525

35

Our work has the following key contributions:
‚Ä¢ To the best of our knowledge, we present the most comprehensive analysis of a web search engine voice query
log.
‚Ä¢ We combine a semantic analysis using novel methods,
such as analyzing a broad set of triggered cards, with
an in-depth syntactic analysis, to shed more light on the
common and different between voice and text queries.
‚Ä¢ We provide empirical evidence, based on language modeling, that voice queries are closer to natural language
than text queries, yet are still distant from natural question language.
Our findings suggest different ways for search systems to enhance their support and take advantage of the unique characteristics of voice queries. We conclude the paper by summarizing the key findings and discussing their implications
and future research directions.

2.

of accents and acoustic conditions.‚Äù Zweig and Chang [43]
found that the use of Model M (exponential n-gram language
model) with personalization features improved the speech
recognition performance on Bing voice search. In this work,
we take advantage of the advancement in speech recognition,
to explore a high-quality transcribed query log, but do not
delve into speech recognition aspects.
Some of the recent work has focused on voice query reformulation, showing that users sometimes respond to voice
recognition errors by different reformulation patterns, such
as repeating a query or refining it [19]. Classifiers were built
to predict and categorize voice reformulations, extending
text-based approaches with features such as voice recognition time and confidence [3]. Researchers also found that
users do not tend to switch between voice and text when
reformulating queries [33]. While our study does not focus
on query reformulation, we report related statistics for voice
versus text queries in our session analysis.
Most closely-related to our research are three studies that
directly referred to the comparison between voice and text
queries. The first described a case study of the development of ‚ÄúGoogle Search by Voice‚Äù [30]. While most of the
report is focused on describing the technology and the evaluation of the voice recognition component, a section is dedicated to evaluating the user experience based on a 4-week
query log analysis. It was found that the query categories
‚Äúfood & drink‚Äù and ‚Äúlocal‚Äù (e.g., place names or business
listings) were more popular with voice searches. Also, short
queries, in particular 1- and 2-word queries, were relatively
more frequent in voice searches than in typed searches, while
longer queries (5+ words) were far rarer. A poster by Yi
and Maghoul [40] inspected the change in mobile search on
Yahoo from 2007 to 2010 and provided a short comparison
of 79K voice queries to typed mobile and desktop queries,
which examined query length and categories. The most comprehensive comparison between voice and text queries was
performed in a lab study from a decade ago (pre-smartphone
era) [10]. The 12 participants were students from the local
research lab, as voice search was in its infancy and required
IR experience. They were asked to formulate 10 TREC topics as queries in a process that took 1 to 3 minutes per query.
The resulting queries did not reflect typical web queries and
were complex and long (23.1 words for an average voice
query, 9.5 for text). Moreover, the study did not involve
a search system and participants were not exposed to search
results. In addition to query characteristics, such as length
and typing duration, the retrieval effectiveness of typed versus spoken queries was evaluated. In our analysis, when
relevant, we tie to the results reported in these three studies
and discuss the common and different with our own findings.

RELATED WORK

Studies of mobile query log analysis have been published
throughout the past decade, ever since mobile devices became ubiquitous. One of the early studies [20] compared
search patterns on 12-key keypad cellphones, PDAs, and
desktop (PC) computers. It found that the diversity of
queries on mobile was substantially lower than on desktop
and that the most popular query in each of the three device
types was different. Baeza-Yates et al. [4] compared mobile
and desktop search queries on Yahoo Japan and found that
mobile queries included fewer characters, more queries in the
Business category, and fewer in Art. Yi et al. [41] performed
a large-scale query log analysis of the Yahoo OneSearch mobile service, and found that mobile query patterns were dynamic, as users were exploring how to use the devices. With
the evolution of mobile devices into smartphones, mobile
search has also been shown to change. Kamvar et al. [21]
examined search behavior on iPhones and found it was more
similar to desktop search than to search on basic mobile
phones. Song et al. [34] performed a broad 3-month log analysis of Bing search on desktop, iPad, and iPhone. Due to
the significant differences between user search patterns on
the three platforms, they proposed a ranking system that
considered platform-specific features.
With the advancement of speech recognition technologies,
studies of mobile search using voice started to emerge. Many
of the studies focused on voice recognition challenges. Wang
et al. [38] defined voice search as ‚Äúthe technology underlying
many spoken dialog systems that provide users with the information they request with a spoken query‚Äù, and reviewed
key challenges, such as environmental noise, pronunciation
variance, and linguistic issues. Acero et al. [1] described
the architecture of the speech recognition interface of ‚ÄúLive
Search for Mobile‚Äù. Moreno-Daniel et al. [27] discussed the
interleaving of ASR with IR systems and suggested to combine acoustic and semantic models to enhance performance.
In recent years, alongside the enhancement of ASR technologies with deep learning [17], various studies suggested
advanced methods for voice search ASR and reported further performance enhancements. Chelba et al. [8] leveraged
the data on Google‚Äôs voice search logs to enhance language
modeling and achieved ‚Äúsmall but significant‚Äù gains in speech
recognition performance. Shan et al. [31] described a system
for Mandarin Chinese voice search and reported ‚Äúexcellent
performance on typical spoken search queries under a variety

3.

RESEARCH SETTINGS

Our analysis is based on a random sample of 500,000
queries from the Yahoo mobile search application, performed
by over 50,000 unique users of the voice interface along a
period of exactly six months (April-October 2015) in the
United States. The mobile search application transcribes a
voice query into a text query using state-of-the-art ASR, and
from this point onward treats it as a text query. In other
words, the multi-modal interface allows inputting queries
by voice, but returns results using the same information retrieval techniques and the standard mobile search user interface. For comparison, we collected an identical number

36

of queries performed using the ‚Äúregular‚Äù keyboard-based interface of the same mobile application. We refer to the former set of queries as voice queries and to the latter as text
queries. The text queries were collected along the same period of six months for a similar number of users. Moreover,
we sampled an identical number of voice and text queries in
each day of the experimental period. When inspecting dayof-week distribution and session statistics, we compared all
queries from all users in our voice sample with all queries
from all users in our text sample, during two months of the
experimental period, to allow suitable analysis.
Each query in the log, either voice or text, included, in
addition to the query itself, a timestamp (adapted to the
timezone in which it was performed), a location in the form
of city and state, and, for logged-in users, the user‚Äôs age
and gender. In addition, for each query we had information
about its associated clicks, if any were performed, including
the corresponding URLs and ranks within the search results
page (SERP).
Our analysis is organized as follows. Section 4 compares
basic characteristics of voice and text queries, including context, query length, and session characteristics. Section 5
examines the query semantics by inspecting different categories as well as specific queries and terms. Section 6 looks
into click behavior and distribution of clicked domains, reflecting on the findings in the query semantics analysis. Section 7 examines query syntax as reflected in characteristics
of parsing and distribution of part-of-speech tags. The semantic and syntactic analyses reveal various differences between voice and text queries, of which many indicate that
voice queries are phrased closer to natural language than
text queries. The final part of our evaluation therefore explicitly compares the similarity of both types of queries to
natural language corpora, and is described in Section 8.

4.

Figure 1: Query distribution by hour of the day.

Figure 2: Query length distribution.
for text (std: 2.38, median: 3, max: 308). Query length was
measured by the number of words, using white-space tokenization. The substantially higher maximum value in text
queries is likely due to the use of the copy-paste feature,
which does not exist for voice. Figure 2 shows the detailed
distribution of voice versus text queries by length. It can be
seen that one-word queries were particularly rarer on voice
(12.2% vs. 21% for text), perhaps implying a lower portion
of navigational queries. Voice queries were more common
starting at queries of 5 words, which have been recently referred to as ‚Äúverbose‚Äù queries [15]. Overall, 34.5% of the
voice queries were of 5 words or more, compared to only
21.2% of the text queries. The length difference between
voice and text queries has a major effect on query syntax,
which we examine more closely in Section 7.
Previous work was somewhat inconsistent with regards
to voice versus text query length. While a Google case
study found that voice queries tend to be shorter than typed
queries (2.5 versus 2.9 on average, respectively) [30], other
studies found voice queries to be longer (e.g., 3.4 versus 2.2
on a Yahoo study) [10, 40]. Jiang et al. [19] pointed out
this discrepancy and stated that ‚Äúfurther studies are needed
to identify the characteristics of queries in voice search.‚Äù.
Our findings evidently support voice queries being substantially longer than text queries (while also indicating a general trend of queries becoming longer on mobile search).
The portion of unique queries for voice was 73.9%, compared to 77.6% for text. This stands somewhat in contrast
to voice queries being longer, as we would expect more repetition for shorter queries. We believe this finding stems from
two main reasons. First, the lower query diversity for voice
search characterizes search in its earlier stages, as has been
the case for web and mobile search [40]. Second, the use
of abbreviations, spelling variants, and punctuation marks,
makes text queries more diverse. We will further demonstrate this in Section 5.

BASIC CHARACTERISTICS

In this section, we compare basic characteristics of voice
and text queries, including context aspects, basic query features, and session characteristics.

4.1

Context

We found similar contextual characteristics for voice and
text queries in terms of searcher‚Äôs age and geography (cities
and states). There was a slight tendency towards male
searchers in the voice log compared to the text log (up 3%).
The distribution across day-of-week was similar for voice
and text queries: in both, there was a slight peak on weekends compared to weekdays (5% more searches on average).
In contrast, there was a noticeable difference between voice
and text queries with regards to time-of-day, as depicted in
Figure 1. Voice queries were more frequent during day hours
(from 8am to 8pm), while higher portions of the text queries
(relative to voice) were performed during evening, night, and
early morning hours (8pm to 8am). These differences were
consistent during weekdays and weekends.
In our analysis, we inspected the results while controlling
for factors that were found to be different between voice and
text queries, including time-of-day and gender. When relevant, we report the influence of these factors on the results.

4.2

4.3

Sessions

A session is a series of queries issued by an individual
user in close succession, often with all queries being related
to the same topic. Using a common approach for defining
sessions [35], we considered queries that occur in a sequence
without 15 minutes of inactivity as part of the same session.

Queries

The average query length was significantly higher for voice
queries at 4.2 (std: 2.96, median: 4, max: 109) versus 3.2

37

which it was presented). We excluded very broad cards
(news, shopping, and digital magazines) and cards that re% 1-query sessions
65.1%
67.1%
flect a vague or ambiguous intent (e.g., company, which also
Avg (std) session length
1.74(1.61)
1.77(2.03)
includes websites such as Facebook or Amazon). The left
Avg (std) idle in seconds
174.4(207.9)
121.8(182.7)
side of the table shows the ratio for frequent cards, trigMedian idle in seconds
84
63.5
gered for at least 0.5% of both voice and text queries, while
% identical queries
13.7%
15.1%
the right side focuses on narrower less common cards that
% refining queries
10%
7.6%
still appeared for at least 0.1% of either voice or text queries.
Among the frequent cards, music videos, which are triggered
Table 1: Session characteristics.
by queries such as song names and singer names, were the
most common for voice compared to text (highest triggerTable 1 shows session statistics. The average session length
ing ratio). CQA cards, which include direct (inline) answers
on voice and text queries was very similar, with higher stanfrom community question answering sites, recipe cards, and
dard deviation for voice. About two thirds of the sessions,
map cards, were also more common for voice. On the other
on both voice and text, included only one query. Average
hand, sports and people cards were considerably more comand median idle times were shorter on voice queries, likely as
mon for text queries. The latter is a particularly common
a result of the faster inputting enabled by voice. This gap
card, triggered for celebrity names and sometimes a refining
may also reflect the fact that voice queries often focus on
keyword such as age or height.
topics that require little interaction with the results, as our
Inspecting specific queries that refer to popular celebrities,
analysis will later demonstrate. Finally, we inspected the
we indeed observed a ratio smaller than 1 between voice and
relation of a query in a session to its previous query (when
text, which becomes particularly low for queries that only
such exists): the bottom of Table 1 shows the portion of
include the celebrity‚Äôs name, without any refinements. For
identical and refining queries (a query q2 refines a query q1
example, the voice/text ratio for queries that included ‚Äúleif q1 is a prefix of q2, but not identical to q2). Overall,
bron james‚Äù was 0.85, but this ratio decreases to 0.57 when
the numbers are similar, with somewhat higher portions of
only considering the exact query ‚Äúlebron james‚Äù. Similarly,
identical queries on voice and refining queries on text.
for ‚Äúbruce jenner‚Äù these ratios were 0.97 and 0.47, respectively; for ‚Äúkim kardashian‚Äù 0.63 and 0.48; and for ‚Äúdonald
5. QUERY SEMANTICS
trump‚Äù, 0.83 and 0.42.
In this section, we compare voice and text query semanReviewing the less frequent cards, on the right side of table
tics. We first examine higher level query categories by in2, the time card was largely more common on voice queries,
specting the triggering of vertical cards. We then examine
with a triggering ratio of over 5.5. This card is typically
the query themselves more closely, by comparing the most
triggered by queries that ask for the time in a specific locapopular queries and the most distinctive query terms.
tion (e.g., ‚Äúsavanna time‚Äù or ‚Äúwhat is the hour in chicago‚Äù).
Countries (country names, sometimes with refinements such
5.1 Triggered Cards
as ‚Äúcapital of‚Äù or ‚Äúpopulation‚Äù), dictionary, and weather,
The recent evolution of web search has introduced richer
were also more popular on voice, whereas lottery and even
experience of the SERP, with cards (also referred to as ‚Äòoneboxes‚Äô
more so horoscope were more popular on text. Overall, we
or ‚Äòdirect displays‚Äô) showing results of verticals such as weather,
see that many of the infrequent cards are more commonly
direct factual answers, or live sport scores [32]. These cards
triggered for voice. Additionally, it appears that cards with
extend organic search results (‚Äòten blue links‚Äô) by addressing
concise answers (time, definition, weather) are more coma user‚Äôs specific information need directly on the SERP [9],
monly triggered for voice queries, while cards that require
often sparing the need for a click [23]. Commercial search enhigher user engagement, as they present richer content or
gines trigger cards upon the identification of a relevant user‚Äôs
more likely to require interaction (horoscope, lottery), are
intent. The card triggering technology largely relies on query
more commonly triggered for text queries.
pattern matching that provides high precision, since the preSome of the findings in this section coincide with the
sentation of a wrong card might substantially degrade the
Google case study [30], which identified ‚Äúfood & drink‚Äù and
user experience and is therefore highly undesirable. In the
‚Äúlocal‚Äù as the more popular categories for voice (out of a tofollowing analysis, we examine the distribution of presented
tal of 8), corresponding with our findings w.r.t the ‚Äúrecipe‚Äù
cards for voice versus text queries, relying on the search
and ‚Äúmaps‚Äù cards. That study also found that the ‚Äúonline
engine‚Äôs technology for card triggering to shed light on secommunities‚Äù and ‚Äúadult‚Äù categories were less frequent for
mantic differences between the two types of queries. The
voice, to which we show support in the next section.
card triggering technology allows us to build on the ability
to capture clear and specific user intent based on different
5.2 Popular Queries
patterns of language. For example, the dictionary card is
triggered by queries such as ‚Äúwhat is <term>‚Äù, ‚Äú<term>
Table 3 shows the most popular queries for text versus
definition‚Äù, or ‚Äúmeaning of <term>‚Äù.
voice (popularity is measured by the number of unique users
Overall, the portion of queries for which at least one card
who issued the query at least once). The top query in each
was triggered was 43.3% for voice queries and 40.6% for text.
list is already different: ‚Äúfacebook‚Äù for text versus ‚Äúyoutube‚Äù
The higher portion for voice may reflect a more frequent use
for voice. The difference between the two is substantial: on
for information needs that can be directly satisfied on the
text queries, ‚Äúfacebook‚Äù was issued by a number of users
SERP and require less interaction from the user.
larger by a factor of 1.7 than the number of users who
Table 2 presents the ‚Äútriggering ratio‚Äù for 16 different cards
queried for ‚Äúyoutube‚Äù, while on voice it was issued by less
(i.e., the ratio between the portion of voice queries for which
than half (0.44). It can also be seen that adult site queries
the card was presented and the portion of text queries for
(‚Äúporn‚Äù, ‚Äúxnxx‚Äù, ‚Äúredtube‚Äù) appear only on the top text list.
Text

Voice

38

Frequent Cards
Card Name
V/T Ratio
Music Videos
1.32
CQA
1.3
Recipe
1.3
Maps
1.27
Movies
1.13
Finance
1.02
People
0.75
Sports
0.73

Unigrams
Text
Voice
pornhub the
2015
is
xnxx
a
tumblr what
facebook
in
redtube you
tx
to
ca
of
st
how
login
end
vs
i
nc
for
craigslist on
ny
do
dr
number

Infrequent Cards
Card Name
V/T Ratio
Time
5.65
Countries
1.52
Dictionary
1.43
Weather
1.19
Cars
0.95
TV
0.9
Lottery
0.56
Horoscope
0.36

Table 2: Voice(V)/text(T) card triggering ratio.

1. facebook
2. youtube
3. pornhub
4. google
5. yahoo mail
6. craiglist

Text
7. porn
8. xnxx
9. yahoo
10. amazon
11. redtube
12. facebook login

1. youtube
2. yahoo mail
3. facebook
4. google
5. hello
6. craiglist

On the other hand, the voice list includes more retail brands
(‚Äúwalmart‚Äù, ‚Äúhome depot‚Äù) and the query ‚Äúhello‚Äù, which is
likely used for experimenting with the voice system. In addition, the voice list includes the use of the suffix ‚Äò.com‚Äô
for popular navigational queries, e.g., both ‚Äúyahoo‚Äù and ‚Äúyahoo.com‚Äù are on the top list for voice (and similarly for ‚Äúamazon‚Äù). Across all queries, however, the ‚Äò.com‚Äô suffix was less
common on voice than on text (ratio of 0.6), likely due to
substantially sparser use of full URLs on voice queries: the
prefixes ‚Äòwww‚Äô and ‚Äòhttp‚Äô appeared much more commonly
on text queries, with a voice/text ratio of 0.07 and 0.01,
respectively.

Distinctive Query Terms

To further inspect semantic differences, we set out to explore which terms mostly characterize voice versus text queries.
To this end, we used Kullback-Leibler (KL) divergence, which
is an asymmetric distance measure between two given distributions [6]. Specifically, we calculated the terms that contribute the most to the KL divergence between the voice
and text query language models, for unigrams, bigrams, and
trigrams1 . Table 4 reports the terms with the highest KL
divergence for text queries (w.r.t voice queries) and for voice
queries (w.r.t text). Inspecting the unigrams, the terms on
the voice list mostly include common function words (determiners, prepositions), question words, and pronouns. The
only two nouns on the list are ‚Äúend‚Äù and ‚Äúnumber‚Äù. For
‚Äúend‚Äù, closer inspection verified that this is due to the ASR
often confusing it with the more common ‚Äúand‚Äù (e.g., ‚Äúcan i
eat onions end garlic while breast-feeding‚Äù or ‚Äúthe preacher
end the bear song‚Äù). The text unigram list, on the other
hand, includes site names (especially social media and adult)
and common abbreviations (states, e.g., ‚Äútx‚Äù, ‚Äúca‚Äù, ‚Äúnc‚Äù; and
also ‚Äúst‚Äù, ‚Äúvs‚Äù; ‚Äúdr‚Äù), which are hardly ever used on voice.
For bigrams and trigrams, the text lists include website
and entity names, sometimes with extending keywords such
as ‚Äúsign up‚Äù, ‚Äúlogin‚Äù, ‚Äúonline‚Äù, ‚Äú2015‚Äù, ‚Äúnews‚Äù, or ‚Äúscores‚Äù. On
the other hand, the voice list includes many common parts
of natural language (‚Äúwhat is‚Äù, ‚Äúin the‚Äù), requests phrased in
natural language (‚Äúshow me‚Äù, ‚Äútake me to‚Äù, ‚Äúi‚Äôm looking for‚Äù,
1

Trigrams
Text
Voice
facebook sign up
what is the
credit card login
how do you
fargo bank login phone number for
bobbi kristina brown
do you spell
dicks sporting goods
how old is
online login site
how do i
drudge report 2015
what time is
yahoo mail inbox
where is the
chase online login
i need the
wireless my account
time is it
verizon wireless my
what is a
scrabble word finder
what are the
online banking login
i want to
craigslist los angeles
take me to
toys r us
i‚Äôm looking for

Table 4: Most distinctive query terms.

Voice
7. yahoo
8. walmart
9. amazon
10. home depot
11. yahoo.com
12. amazon.com

Table 3: Most popular queries.

5.3

Bigrams
Text
Voice
yahoo mail
is the
facebook sign
what is
sign up
how do
you tube
do you
dear abby
in the
online login
number for
big tits
of the
near me
phone number
mlb scores north carolina
schedule 2015 pictures of
crossword clue
new york
yahoo news
for the
card login
what‚Äôs the
horoscope 2015
where is
season 2
show me

We elaborate on the smoothing method in Section 8.

39

‚Äúphone number for‚Äù), and also a few state names that appear
in their standard form (‚Äúnew york‚Äù, ‚Äúnorth carolina‚Äù).
Finally, we also used the KL analysis to examine distinctive unigrams positioned at the beginning and at the end of
a query. The most distinctive words to open a voice query
were the question words ‚Äúwhat‚Äù and ‚Äúhow‚Äù and the most distinctive for text queries were the site names ‚Äúpornhub‚Äù and
‚Äúfacebook‚Äù. The most distinctive word to terminate a voice
query was ‚Äúplease‚Äù, again indicating the use of natural language, while for text it was ‚Äú2015‚Äù, perhaps as it is easy to
write but relatively long to pronounce.
The difference in use of question words is one of the most
prominent between voice and text queries. A recent study
examined this form of ‚Äúquestion queries‚Äù, which ‚Äútake the
form of natural language‚Äù [39]. We used a similar methodology to identify this type of queries for voice and text. Overall, 9.9% of the voice queries begin with a wh-word (one of
the 5W1H), compared to only 3.7% of the text queries (ratio 2.67). Adding yes/no questions (start with ‚Äòdoes‚Äô, ‚Äòdid‚Äô,
‚Äòcan‚Äô, etc.), the portions grow to 11.9% and 4.7%, respectively (ratio 2.55). The two most popular question words,
by a large margin, were ‚Äúhow‚Äù (3.6% of all voice queries) and
‚Äúwhat‚Äù (3.5%); while for ‚Äúwhat‚Äù the voice/text ratio was 3.1,
for ‚Äúhow‚Äù it was lower at 2.3. The lowest ratio among the
5W1H was for ‚Äúwhy‚Äù queries at 1.8 (these queries account for
0.4% of all voice queries), probably as these are more openended questions, often characterized by longer answers that
require more exploration on the part of the user [37]. On
the other hand, common prefixes for factoid questions were
substantially more common on voice, e.g., ‚Äúhow old is‚Äù (ratio
5.74) or ‚Äúwho is the‚Äù (5.36) [16].
Thus far, we have seen many quantitative characteristics
by which voice queries differ from text queries. Next, we
show a few anecdotal examples of voice and text queries used
to perform a semantically-similar search. To this end, we inspected queries in our voice sample that landed (i.e., resulted
in a click) on CQA pages, as these often reflect a specific information need. For such queries, we matched text queries
that landed on the same CQA page during a period of one
week (our text sample did not contain such matches, thus we
had to inspect a larger log). Table 5 presents seven examples of voice and text query pairs that landed on the same
CQA page and express a similar information need. These
examples nicely demonstrate some of the findings pointed
out during this section. We note, however, that we cherrypicked examples where the voice query was especially different from the corresponding text query. In other cases, voice
queries were similarly phrased to text queries. For instance,

voice query

text query

looking for a restaurant that serves oysters in san francisco

oysters restaurant sf

how many minutes are played in women‚Äôs
soccer

women soccer duration

what restaurant did colin farrell and
vince vaughn have dinner at in brooklyn

colin
farrell
vince
vaughn joint dinner

need to see old sites i visited

view browsing history

is priority shipping and standard shipping the same thing

priority vs standard
shipping

if you‚Äôre 65 years old do you need a fishing license

fishing license senior
citizen

i need the phone number for walmart in
canton connecticut please

walmart canton connecticut phone

Text
Domain
1
2
3
4
5
6
7
8
9
10
11
12

Table 5: Example voice and text queries that landed on the
same CQA page.

0.78
0.83

MRR for all queries
MRR for clicked queries

0.78
0.97

% Unique domains (hosts)
% Top domain clicks

0.71
1.1

inspecting our original samples, 13.1% of the voice queries
were completely identical to a query in the text sample.

CLICKS

Table 6 shows the ratio for various click characteristics
between voice and text queries2 . It can be seen that the
click-through rate (CTR; the portion of queries for which
at least one click was made) and average number of clicks
per query were substantially lower for voice queries. We
conjecture that voice queries are often conducted in a situation that allows less interaction with the device, including
clicking on search results. The mean reciprocal rank (MRR)
across all queries is also substantially lower for voice queries.
The MRR across clicked queries only is similar for voice and
text queries, indicating that the difference in the general
MRR is mostly due to the lower CTR of voice queries.
Inspecting the clicked domains (a domain is determined by
the ‚Äòhost‚Äô part of the clicked URL), the portion of unique domains out of all clicks is substantially lower for voice queries,
indicating lower diversity. Further analysis indicated that a
higher portion of the voice clicks are performed on top domains, determined using a list of the 100 most commonlyclicked domains during our experiment‚Äôs period.
Table 7 shows the top clicked domains for text and voice
queries. The rightmost column shows, for each of the top
voice domains, the ‚Äúclick ratio‚Äù, i.e., the ratio between its
number of clicks on the voice query sample and number of
clicks on the text query sample. Differences emerge between
the two lists from their top. While the most clicked domains
for text queries are Wikipedia and Facebook, they are only
2nd and 5th, respectively, on the voice click list, with low
click ratios, especially for the social network, at 0.51. Instead, the top of the voice query list is dominated by video
domains: video.search.yahoo.com, with more than double
the clicks as on text queries, and popular video sharing site
2

Ratio
2.05
0.81
1.26
1.26
0.51
0.61
1.18
1.99
1.75
1.76
0.76
0.54

Youtube. Also higher on the voice list are CQA sites, such
as Yahoo Answers at 4th (6th on text) and Answers.com at
10th (not among the top 12 for text). On the top voice list
only, with high click ratios, are also maps.yahoo.com and
Yellowpages, reflecting more specific information needs (we
have already seen ‚Äúphone number‚Äù is a distinctive term for
voice queries). Another evident difference is with adult sites:
while four (Pornhub, Xvideos, Xnxx, and Redtube) are on
the top text domains, only two make the top voice domains,
with low click ratios. As we saw, text queries are more popular during night hours, which could explain the difference.
Inspecting the portions of adult site clicks by the hour of the
day, we indeed observed a sharp increase during night hours
compared to day hours, however, the gap between text and
voice clicks persists throughout all hours of the day.
Further inspecting the lists of top clicked domains revealed
differences towards voice queries in other CQA sites (e.g.
wikiHow with a click ratio of 1.3), maps (MapQuest 1.75; appearances of ‚Äúmaps‚Äù within all clicked domain strings with a
ratio of 1.74), weather (AccuWeather 2.22; ‚Äúweather‚Äù string
1.48), dictionary (dictionary.reference.com 1.43; thefreedictionary.com 1.75, ‚Äúdictionary‚Äù 1.35), recipes (allrecipes.com
1.58, ‚Äúrecipe‚Äù 1.53), video streaming (screen.yahoo.com 1.97;
‚Äúscreen‚Äù 1.77), and music (iTunes 1.88). On the other hand,
more dominant on text queries were shopping sites (eBay
0.92, Craiglist 0.54, ‚Äúshopping‚Äù 0.84), health (WebMD 0.88,
drugs.com 0.77, nih.gov 0.64), news (news.yahoo.com 0.82,
‚Äúnews‚Äù 0.7), sports (sports.yahoo.com 0.72, espn.go.com 0.8,
‚Äúsport‚Äù 0.83), finance (finance.yahoo.com 0.48, ‚Äúfinance‚Äù 0.56,
‚Äúbank‚Äù 0.89), celebs (celebs.yahoo.com 0.63, ‚Äúceleb‚Äù 0.81),
and social network sites with a particularly low ratio between voice and text clicks (Twitter 0.17, Linkedin 0.37).
Despite the differences in favor of voice clicks for audio and
video results, there was no such difference for photo sites
(Photobucket 0.97, Pinterest 0.65, ‚Äúphoto‚Äù 0.87).

Table 6: Click statistics.

6.

video.search.yahoo.com
en.wikipedia.org
youtube.com
answers.yahoo.com
facebook.com
pornhub.com
local.yahoo.com
maps.yahoo.com
yellowpages.com
answers.com
amazon.com
xvideos.com

Table 7: Most clicked domains.

Voice / Text Ratio
Click-through rate
Avg #clicks

en.wikipedia.org
facebook.com
pornhub.com
video.search.yahoo.com
youtube.com
answers.yahoo.com
xvideos.com
local.yahoo.com
xnxx.com
amazon.com
redtube.com
imdb.com

Voice
Domain

7.

QUERY SYNTAX

In Section 4.2, we saw that voice queries tend to be longer
than text queries. In this section, we delve deeper into syntactic analysis of voice versus text queries. Our analysis
includes two parts: we first inspect the characteristics of
syntactic parsing of both types of queries and then examine
the distribution of key part-of-speech (POS) tags. In our
analysis, we used two corpora for additional comparison:
the first is the Wall Street Journal (WSJ) corpus (sections
2-23) [26], which primarily includes business and financial
news articles, broken into sentences (a total of 42,248 sen-

We cannot disclose actual values due to business sensitivity.

40

tences). The second is a collection of 500,000 question titles
in English, randomly sampled from the Yahoo Answers CQA
website. We only considered question titles of one sentence
(over 90% of all titles on the site).

7.1

(52.4% vs. 64.3%), although still considerably higher than
for WSJ (34.5%) and question titles (30.6%). Adjectives are
also somewhat more common for queries, with similar portions for text (9.9%) and voice (9.6%). For the other parts
of speech, it can be seen that the portions for voice queries
are higher than text queries, and closer to the portions for
titles and WSJ, although not quite as high. These differences are consistent across all five lower POS types in the
table, and especially salient, with more than a double ratio
between voice and text, for determiners and pronouns.
The richer language used in voice queries is largely due to
their length: as we saw, voice queries are a token longer on
average than text queries. Yet, differences also emerge when
comparing queries of the same length. Table 10 shows the
POS tag portions for voice versus text across queries of 2-7
tokens. There is a clear general trend in POS distribution
by query length: the portion of nouns decreases as the number of tokens increases, the portion of adjectives remains
stable, while the portion of other POS types increases with
the length of the query. For queries with a fixed length, differences can still be observed between voice and text in the
number of nouns, on the one hand, which is higher for text
queries, and verbs, prepositions, determiners, pronouns, and
adverbs, which are higher for voice queries. These differences
somewhat diminish as query length grows, yet such queries
are quite rare, especially for text (e.g., only 2.7% of the text
queries are of 7 tokens). The differences for determiners and
pronouns remain solid even for queries of 7 tokens. Overall, we see that even for queries of the same length, there
is a difference in POS distribution between voice and text
queries, with more diverse language used in voice.

Parsing Characteristics

For this analysis, individual sentences from each of the
four corpora ‚Äì WSJ, question titles, voice queries, and text
queries ‚Äì were tokenized, pos-tagged, and syntactically parsed
using the Stanford parser3 . The parser first generates an
unlexicalized PCFG parse [22] and then produces typed dependencies by matching patterns on CFG trees [11].
Table 8 reports four measures of syntactic complexity (four
middle columns) for each of the four corpora. The first column shows the median and average number of tokens per
parsed item4 . The second column depicts the median and
mean dependency tree depth, defined as the number of edges
in the longest path from the root node to a leaf in the tree.
The third and fourth columns present the fraction of dependency tree root edges that go to tokens POS-tagged as nouns
or verbs, respectively. We use these two measures as proxies
to the syntactic category of the input text, with noun roots
often indicating simple noun phrases and verb roots often
indicating more complex syntactic forms [28]. Finally, the
rightmost column of Table 8 presents the median and average length-normalized log probability score of the PCFG
parse, which serves as a proxy for grammaticality (a more
negative score reflects a lower probability of the parse).
These results indicate substantial differences between voice
and text queries: voice queries have more tokens, higher tree
depth, higher portion of root nodes that govern a verb, lower
portion of root nodes that govern a noun, and higher parse
score. All of these differences make voice queries more similar to question titles (which are, in turn closer to news articles), relative to text queries. This analysis suggests that on
the scale where text queries are at one extreme (shorter, less
grammatical) and natural-language news articles are at the
other (longer, better-formed), voice queries are positioned
somewhere in-between text queries and question titles.

7.2

8.

NATURAL LANGUAGE RESEMBLANCE

Our analysis so far has revealed semantic and syntactic
differences between voice and text queries. In this section,
we set out to validate that the language of voice queries is
indeed closer to natural language than text queries. To this
end, we built two natural language models (LMs). The first
is based on the WSJ corpus (sections 2-23, 42,248 sentences).
The second, marked QT, was built based on a random sample of 50M question titles from the Yahoo Answers CQA
site, posted between 2006 and 2015. Yahoo answers is a
large and diverse website, acting not only as a medium for
sharing technical knowledge, but as a place where one can
seek advice, gather opinions, and satisfy curiosity about a
wide variety of topics [2]. We opted to use these two corpora
since one (WSJ) represents classic formal language, commonly used in natural language processing research, while
the other (QT) represents a more up-to-date web language
contributed by the ‚Äúcrowd‚Äù in order to ask questions.
We built unigram, bigram, and trigram LMs, with JelinekMercer smoothing [42], Œª=0.8, as learned throughout our
experiments6 . Smoothing unknown unigrams was done using the standard  of 1 over the vocabulary size.
For measuring the similarity to a natural language model
we used perplexity, which is perhaps the most commonly
used measure of model quality in speech, natural language
processing, and information retrieval research [29]. Perplexity quantifies the error between the predicted probability of

POS Tagging

The second part of the query syntax evaluation focuses
on the distribution of part-of-speech tags using the Stanford
POS tagger5 [36]. In our analysis, we removed all punctuation tokens. Mobile queries in general include very few
punctuation marks: only 0.7% of the text tokens and 0.3%
of the voice tokens were punctuation marks, compared to
12.4% and 12.8% for question titles and WSJ, respectively
(largely due to the use of question marks at the end of question titles and periods at the end of WSJ sentences). We
worked with a lower-case version of all four corpora, as all
the voice queries and the vast majority of text queries were
lower case in their original form.
Table 9 displays the portion of primary POS tags (as the
portion out of of all tokens in the corpus) for the four corpora. The first row refers to nouns, which are prevalent in
queries, as previously shown [5]. Yet, for voice queries the
portion of nouns is substantially lower than for text queries
3

http://nlp.stanford.edu/software/lex-parser.shtml
The number of tokens is slightly higher than reported in
Section 4.2, due to the use of the Stanford tokenizer instead
of white-space tokenization.
5
http://nlp.stanford.edu/software/tagger.shtml
4

6
In our experiments, we worked with Œª=0.5, 0.6, .., 0.9. We
only report the results with Œª=0.8 for clarity of presentation,
but note that the respective outcomes for other values of Œª
were very similar.

41

Corpus

Median (mean)
token count

Median (mean)
tree depth

root ‚Üí N N ‚àó
edges (%)

root ‚Üí V B‚àó
edges (%)

Median (mean)
parse score

WSJ
CQA question titles
Voice queries
Text queries

20 (20.41)
10 (10.6)
4.3 (4)
3.3 (3)

6
4
3
2

9.2
16.7
55.7
66.5

84.1
75.0
37.1
28.6

‚àí6.2 (‚àí6.3)
‚àí9.3 (‚àí10.4)
‚àí13.7 (‚àí13.9)
‚àí15.3 (‚àí15.6)

(6.5)
(4.3)
(2.9)
(2.4)

Table 8: Syntactic properties of four corpora.
Text

Voice

Titles

WSJ

% Nouns (NN)

64.3

52.4

30.6

34.5

% Adjectives (JJ)

9.9

9.6

6.8

8.0

% Verbs (VB)

8.7

12.1

21.6

16.2

% Prepositions (IN)

5.5

7.6

8.9

11.8

% Determiners (DT)

2.0

4.5

7.5

9.8

% Pronouns (PR)

1.7

3.6

9.9

3.4

% Adverbs (RB)

2.2

3.5

6.4

4.6

is largely due to the structure of the voice query language,
rather than merely due to its vocabulary.
The general perplexity values for bigrams and trigrams
are substantially higher for WSJ than for QT. This can be
either due to the WSJ language being less similar to query
language than QT, or due to the large volume of training
data for QT, which produced a better language model. To
further explore this, we randomly sampled 42,248 question
titles from the 50M QT corpus ‚Äì identical to the number
of sentences in the WSJ corpus ‚Äì and trained unigram, bigram, and trigram LMs based on this smaller corpus. The
lowest row of Table 11 shows the respective results. It can
be seen that the bigram and trigram perplexity values are
higher than for the massively trained QT model, but are
still lower by roughly an order of magnitude for bigrams,
and two orders of magnitude for trigrams, compared to the
WSJ7 . This gives a stronger indication that the queries are
more likely to be generated from the QT LM than the WSJ.
The differences between voice and text queries remain ‚Äì the
perplexity ratios are similar to the full QT LM.
While being longer is an inherent characteristic of voice
queries, we set out to explore whether for queries of the
same length, there are still differences in the perplexity between text and voice queries. Table 12 shows the results for
both WSJ and QT, as measured for text and voice queries
of length 3, 5, and 7 tokens. Generally, the perplexity values indeed decrease as query length increases, indicating, as
conjectured, that longer queries are closer to natural language. For both WSJ and QT, the perplexity across all
n-grams is still noticeably lower for voice versus text queries
of the same length. While the ratio is not as sharp as for
the general query population, the difference is still clear and
the trend of decreasing ratio from unigrams to bigrams and
from bigrams to trigrams persists. This indicates that when
we take a voice query and a text query of the same length,
the former will have a language closer to natural.

Table 9: Part-of-speech distribution.
2
NN
JJ
VB
IN
DT
PR
RB

T
80.6
9.5
5.8
0.2
0.3
0.5
1.1

3

4

5

6

7

V
T
V
T
V
T
V
T
V
T
V
73.6 72.1 65.2 66.6 60.1 60.5 54.5 53.9 50.1 47.7 45.1
10.8 12.5 12.6 11.5 10.9 10.7 10.2 10.0 9.5 9.3 9.1
7.4 6.3 8.3 7.5 9.8 8.7 11.2 10.5 12.4 12.8 13.5
1.0 2.4 3.7 5.2 6.5 7.5 8.1 9.1 9.3 10.2 10.3
1.1 0.8 1.9 1.3 2.6 2.0 3.6 2.9 4.6 3.6 5.6
1.3 0.6 1.8 1.1 2.5 1.6 3.0 2.1 3.5 3.2 4.2
1.7 1.0 1.7 1.2 2.2 2.1 3.2 3.2 3.7 4.2 4.5

Table 10: Part-of-speech distribution for text (T) vs. voice
(V) by query length (number of tokens).
an event proposed by a language model, compared to the
empirical probability of the event. In our case, we used perplexity to measure the quality of a natural language model
(WSJ or QT) with regards to a corpus of queries (i.e., the
events) from either voice or text. In other words, we measured how likely the set of voice versus text queries is to
originate from the given language model.
More formally, given a language model LM and a set of
observed probabilities P , the perplexity of LM is defined as
2H(P ;LM ) where H(P ; LM ) is the cross entropy of the probability model LM with respect to the observed probabilities
P , summed over all events in P . The closer the estimated
probabilities for each event to the actual probabilities, the
lower the perplexity. In our case, each event is a query q in a
corpus Q (either voice or text), with an observed probability
1
. Therefore, the cross entropy can be calculated as:
|Q|
X
1 X
H(Q; LM )=‚àí
P (q) ¬∑ log2 LM (q)=‚àí
log LM (q)
|Q| q‚ààQ 2
q‚ààQ

9.

DISCUSSION AND IMPLICATIONS

Our study disclosed various differences between voice and
text search. In this section, we summarize the key findings,
discuss implications, and suggest directions for future work.
‚Ä¢Query Categories. Both our query semantics and click
analysis revealed that voice queries are more focused on
audio-video content, such as from music channels or video
sharing websites. It seems that voice search is more often
used when the result is also expected to include voice. In
addition, we saw that higher portions of the voice queries
triggered the ‚Äúdirect answer‚Äù card and yielded clicks on
popular CQA sites. This may be a result of the fact that
higher portions of the voice queries were phrased as ques-

where log2 LM (q) is the length-normalized log probability of
the query q based on the language model LM . The perplexity itself is calculated by using the cross entropy as the exponent ‚Äì the lower its value, the better is the language model
for ‚Äúpredicting‚Äù the generation of the given query corpus.
The first two rows of Table 11 show the perplexity results
for the WSJ and QT LMs. It can be seen that the perplexity
for voice queries is considerably lower than for text queries,
for unigrams, bigrams, and trigrams. The ‚ÄòV/T‚Äô columns
explicitly show the proportion between the two. It can also
be seen that the ratio decreases, i.e., the gap between voice
and text queries grows, when moving from unigrams to bigrams and then to trigrams, indicating that the difference

7
The unigram perplexity is substantially lower for the
smaller training set. We suspect that the large training set
adds many rare unigrams to the vocabulary that do not at
all appear in the query sets, but reduce the probability of
other unigrams that do appear in the queries.

42

Corpus

Text

Unigrams
Voice

V/T

Text

Bigrams
Voice

V/T

Text

Trigrams
Voice

V/T

WSJ
QT

30, 463
35, 843

13, 563
12, 865

0.445
0.359

11.9M
70, 064

792, 952
11, 764

0.066
0.168

17.3B
402, 837

133M
21, 127

0.008
0.067

QT 42K

12, 667

6, 665

0.526

466, 428

83, 924

0.18

49.5M

2.89M

0.058

Table 11: Perplexity of text (T) and voice (V) queries w.r.t natural language models.
len

T

Unigrams
V V/T

T

Bigrams
V V/T

T

Trigrams
V V/T

3
5
WSJ
7

33.1K 17.4K 0.53 1.5M 519K 0.35 139M 33M 0.24
16.2K 9.2K 0.57 169K 63K 0.37 4M
1M 0.26
8.4K 5.4K 0.65 42.9K 19.1K 0.45 544K 181K 0.33

3
5
7

43.4K 19.1K 0.44 40.8K 15.8K 0.39 112K 32.4K 0.29
17.3K 9.5K 0.55 7.2K 3.7K 0.51 12.5K 5.5K 0.44
6.7K 4.6K 0.69 2.3K 1.4K 0.6
3K 1.6K 0.55

QT

‚Ä¢Pronouncing vs. writing. In our analysis, a variety of
issues were observed that relate to the difference between
speaking queries and typing them. We saw more frequent
use of words that are easy to pronounce but hard to write
(e.g., long state names), and on the other hand less frequent
use of abbreviations or calendar years (2015), which are
easy to type but harder to pronounce. Related to this is
the absence of a copy-paste feature, reflected by the rare use
of URLs in voice queries. In addition, some typing styles
are ‚Äústandardized‚Äù by the transcription process, e.g., the
use of apostrophe for possession (on text, ‚Äòs‚Äô is commonly
used both with and without the preceding apostrophe) or
the use of diacritical marks, such as in ‚ÄúbeyonceÃÅ‚Äù. Finally,
the use of punctuation marks and upper case letters, which
is infrequent on mobile search, is even rarer on voice search.
‚Ä¢Voice query language. Our syntactic analysis, based on
parsing and POS tagging, showed that voice queries are
not only longer, by a token on average, than text queries,
but also use richer language. Our semantic analysis demonstrated this with the use of natural language phrases such
as ‚Äúi‚Äôm looking for‚Äù, ‚Äútake me to‚Äù, and ‚Äúplease‚Äù. The perplexitybased analysis showed that the language of voice queries is
indeed closer to natural language, even when controlling for
query length. While it has long been claimed that voice is a
richer, more expressive media than written text [7, 10], our
study demonstrates it for the domain of web search queries.
Having said that, the language of voice queries was found
to still be far from natural-language questions and even farther from news articles. We also saw that voice queries may
often be as short and identically-phrased as text queries.
These findings suggest that voice queries pose their own
type of language, in-between traditional text queries and
natural-language questions.
One question that follows, which we did not explore in
this work, is how the length and richer language of voice
queries can help improve the search process. Previous
studies found that longer queries, closer to natural language, do not necessarily improve the retrieval effectiveness compared to typical keyword-based queries [10, 39].
On the other hand, taking advantage of the general growth
of query length on web search, recent studies proposed various methods for applying linguistic analysis on long queries,
including part-of-speech tagging, dependency parsing, and
entity and relation extraction, in order to enhance search
performance. Linguistic analysis can also be applied on the
document side and matched against the query‚Äôs analysis,
to resolve ambiguity and further enhance query-document
match calculation. A recent book on ‚Äúverbose‚Äù queries (5
words or more; 34.5% of all voice queries) summarizes this
body of research and explains that for such queries, not
only is it more feasible to apply linguistic analysis, but it
is also essential for the understanding of the specific intent
and the relevance of the returned results [15].
Previous work has tried to automatically transform queries
into questions, by adding missing functional words or using
question templates [12, 25], motivated by a variety of rea-

Table 12: Perplexity by query length for text and voice.
tions: White et al. [39] found that such ‚Äúquestion queries‚Äù
typically have informational intent and often result in visits
to CQA sites. We also found evidence for higher portions
of recipe-related queries and clicks, perhaps implying that
voice search is used while cooking. On the other hand,
lower portions of voice queries referred to social networking and adult sites, which may represent more sensitive or
personal content [13]. These noticeable differences in query
categories suggest that search services that build on query
classification, such as vertical selection, card triggering, ad
targeting, query expansion, and even result ranking, may
need to be adapted when used for voice search. For example, as voice queries are often phrased as questions, the
identification of CQA queries (e.g., for presenting a CQA
vertical on the SERP) may need to change.
‚Ä¢Device interaction.
Voice search tends to focus on
topics that require less interaction with the device‚Äôs touchscreen. This was reflected by a substantially lower number
of clicks, higher portions of queries that triggered cards,
and more queries that expressed a narrow information need
(time, dictionary, weather, and phone numbers). We also
saw this trend on celebrity queries, where the more openstyle queries that only include the person‚Äôs name, were relatively less common than queries that refer to a refined aspect, such as age or spouse. On the other hand, queries for
research topics (e.g., health) and, as already mentioned, social network sites, which require higher level of engagement,
were less frequent on voice. These results have two key
implications. First, they suggest that voice search should
enable voice-based result presentation, to support a complete hand-free interaction with the user. While short voice
answers have started to emerge on commercial web search
engines, we believe these capabilities should be further extended, to support interaction with more result types, exploration of different search results, query suggestion, and
ultimately a complete dialogue with the user, as already
done by intelligent assistants for personal advice [18]. The
second implication relates to the IR evaluation process. Recent studies have argued that with modern search, especially on mobile devices, the merit of clicks as a primary
evaluation measure decreases [23]. Other measurements,
such as ‚Äúgood abandonment‚Äù, have been proposed [24]. Our
findings show that voice search clicks, as a form of interaction, are even rarer than text search clicks on mobile
devices. Thus, new metrics for evaluating user satisfaction
of voice queries should be developed.

43

sons, such as helping with intent disambiguation, improving search over CQA archives, enhancing query expansion,
and allowing to post a query directly as a question on a
CQA site when the answer cannot be found. Since voice
search queries are more often phrased as questions, they
may directly enable these benefits.
There is plenty of room (and need) for further research,
such as exploring the use of voice queries‚Äô phonetic characteristics, e.g., the speaker‚Äôs stress, speed, and intonation for
search personalization, or conducting user studies to gain
in-depth understanding of the voice search process. Our
analysis suggests that voice search is still in its early stages,
as reflected by the smaller portion of unique queries and the
lower diversity of clicked domains. Future research should
follow the dynamics of voice search as it is poised to become
ubiquitous and further evolve.

[18]

[19]

[20]
[21]

[22]
[23]

[24]

10.

ACKNOWLEDGMENTS

My thanks to Alex Nus, Dan Pelleg, Yuval Pinter, and
Gilad Tsur for their useful comments. Special thanks to
Avihai Mejer for his useful advice throughout this research.

11.

[25]

[26]

REFERENCES

[1] A. Acero, N. Bernstein, R. Chambers, Y. Ju, X. Li, J. Odell,
P. Nguyen, O. Scholz, and G. Zweig. Live search for mobile:
Web services by voice on the cellphone. In Proc. ICASSP,
pages 5256‚Äì5259, 2008.
[2] L. A. Adamic, J. Zhang, E. Bakshy, and M. S. Ackerman.
Knowledge sharing and yahoo answers: Everyone knows
something. In Proc. WWW, pages 665‚Äì674, 2008.
[3] A. H. Awadallah, R. Gurunath Kulkarni, U. Ozertem, and
R. Jones. Characterizing and predicting voice query
reformulation. In Proc. CIKM, pages 543‚Äì552, 2015.
[4] R. Baeza-Yates, G. Dupret, and J. Velasco. A study of mobile
search queries in japan. In Query Log Analysis (WWW
workshop), 2007.
[5] C. Barr, R. Jones, and M. Regelson. The linguistic structure of
english web-search queries. In Proc. EMNLP, pages 1021‚Äì1030,
2008.
[6] A. Berger and J. Lafferty. Information retrieval as statistical
translation. In Proc. SIGIR, pages 222‚Äì229, 1999.
[7] B. L. Chalfonte, R. S. Fish, and R. E. Kraut. Expressive
richness: A comparison of speech and text as media for
revision. In Proc. CHI, pages 21‚Äì26, 1991.
[8] C. Chelba and J. Schalkwyk. Empirical exploration of language
modeling for the google.com query stream as applied to mobile
voice search. In Mobile Speech and Advanced Natural
Language Solutions, pages 197‚Äì229. 2013.
[9] L. B. Chilton and J. Teevan. Addressing people‚Äôs information
needs directly in a web search result page. In Proc. WWW,
pages 27‚Äì36, 2011.
[10] F. Crestani and H. Du. Written versus spoken queries: A
qualitative and quantitative comparative analysis. JASIST,
57(7):881‚Äì890, 2006.
[11] M.-C. De Marneffe, B. MacCartney, and C. D. Manning.
Generating typed dependency parses from phrase structure
parses. In Proc. LREC, pages 449‚Äì454, 2006.
[12] G. Dror, Y. Maarek, A. Mejer, and I. Szpektor. From query to
question in one click: Suggesting synthetic questions to
searchers. In Proc. WWW, pages 391‚Äì402, 2013.
[13] A. Easwara Moorthy and K.-P. L. Vu. Privacy concerns for use
of voice activated personal assistant in the public space.
International Journal of Human-Computer Interaction,
31(4):307‚Äì335, 2015.
[14] Google official blog. http://googleblog.blogspot.co.il/2014/10/
omg-mobile-voice-survey-reveals-teens.html. [Accessed
2016-05-01].
[15] M. Gupta and M. Bendersky. Information retrieval with
verbose queries. Foundations and Trends in Information
Retrieval, 9(3-4):209‚Äì354, 2015.
[16] I. Guy and D. Pelleg. The factoid queries collection. In PROC.
SIGIR, 2016.
[17] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly,
A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, and
B. Kingsbury. Deep neural networks for acoustic modeling in

[27]

[28]

[29]
[30]

[31]

[32]

[33]

[34]

[35]

[36]

[37]
[38]
[39]

[40]

[41]

[42]

[43]

44

speech recognition: The shared views of four research groups.
Signal Processing Magazine, 29(6):82‚Äì97, 2012.
J. Jiang, A. Hassan Awadallah, R. Jones, U. Ozertem,
I. Zitouni, R. Gurunath Kulkarni, and O. Z. Khan. Automatic
online evaluation of intelligent assistants. In Proc. WWW,
pages 506‚Äì516, 2015.
J. Jiang, W. Jeng, and D. He. How do users respond to voice
input errors? lexical and phonetic query reformulation in voice
search. In Proc. SIGIR, pages 143‚Äì152, 2013.
M. Kamvar and S. Baluja. A large scale study of wireless search
behavior: Google mobile search. In CHI, pages 701‚Äì709, 2006.
M. Kamvar, M. Kellar, R. Patel, and Y. Xu. Computers and
iphones and mobile phones, oh my!: A logs-based comparison of
search users on different devices. In Proc. WWW, pages
801‚Äì810, 2009.
D. Klein and C. D. Manning. Accurate unlexicalized parsing. In
Proc. ACL, pages 423‚Äì430, 2003.
D. Lagun, C.-H. Hsieh, D. Webster, and V. Navalpakkam.
Towards better measurement of attention and satisfaction in
mobile search. In Proc. SIGIR, pages 113‚Äì122, 2014.
J. Li, S. Huffman, and A. Tokuda. Good abandonment in
mobile and pc internet search. In PROC. SIGIR, pages 43‚Äì50,
2009.
C. Y. Lin. Automatic question generation from queries. In
Workshop on the Question Generation Shared Task, pages
156‚Äì164, 2008.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini. Building
a large annotated corpus of english: The penn treebank.
Computational linguistics, 1993.
A. Moreno-Daniel, S. Parthasarathy, B. Juang, and J. Wilpon.
Spoken query processing for information retrieval. In Proc.
ICASSP, volume 4, pages IV‚Äì121‚ÄìIV‚Äì124, 2007.
Y. Pinter, R. Reichart, and I. Szpektor. Syntactic parsing of
web queries with question intent: A distant supervision
approach, 2016. Proc. NAACL.
R. Rosenfield. Two decades of statistical language modeling:
Where do we go from here? Proceedings of the IEEE, 2000.
J. Schalkwyk, D. Beeferman, F. Beaufays, B. Byrne, C. Chelba,
M. Cohen, M. Kamvar, and B. Strope. Your word is my
command: Google search by voice: A case study. In Advances
in Speech Recognition, pages 61‚Äì90. 2010.
J. Shan, G. Wu, Z. Hu, X. Tang, M. Jansche, and P. J. Moreno.
Search by voice in mandarin chinese. In Proc.
INTERSPEECH, pages 354‚Äì357, 2010.
M. Shokouhi and Q. Guo. From queries to cards: Re-ranking
proactive card recommendations based on reactive search
history. In Proc. SIGIR, pages 695‚Äì704, 2015.
M. Shokouhi, R. Jones, U. Ozertem, K. Raghunathan, and
F. Diaz. Mobile query reformulations. In Proc. SIGIR, pages
1011‚Äì1014, 2014.
Y. Song, H. Ma, H. Wang, and K. Wang. Exploring and
exploiting user search behavior on mobile and tablet devices to
improve search relevance. In Proc. WWW, pages 1201‚Äì1212,
2013.
J. Teevan, D. Ramage, and M. R. Morris. #twittersearch: A
comparison of microblog search and web search. In Proc.
WSDM, pages 35‚Äì44, 2011.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
Feature-rich part-of-speech tagging with a cyclic dependency
network. In Proc. NAACL, pages 173‚Äì180, 2003.
S. Verberne. Paragraph retrieval for why-question answering. In
Proc. SIGIR, pages 922‚Äì922, 2007.
Y. Y. Wang, D. Yu, Y.-C. Ju, and A. Acero. An introduction to
voice search. Signal Processing Magazine, 25(3):28‚Äì38, 2008.
R. W. White, M. Richardson, and W. Yih. Questions vs.
queries in informational search tasks. In Proc. WWW, pages
135‚Äì136, 2015.
J. Yi and F. Maghoul. Mobile search pattern evolution: The
trend and the impact of voice queries. In Proc. WWW, pages
165‚Äì166, 2011.
J. Yi, F. Maghoul, and J. Pedersen. Deciphering mobile search
patterns: A study of yahoo! mobile search queries. In Proc.
WWW, pages 257‚Äì266, 2008.
C. Zhai and J. Lafferty. A study of smoothing methods for
language models applied to ad hoc information retrieval. In
Proc. SIGIR, pages 334‚Äì342, 2001.
G. Zweig and S. Chang. Personalizing model m for voice-search.
In Proc. INTERSPEECH, pages 609‚Äì612, 2011.

