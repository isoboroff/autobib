The LExR Collection for Expertise Retrieval in Academia
Vitor Mangaravite, Rodrygo L. T. Santos, Isac S. Ribeiro,
Marcos André Gonçalves, Alberto H. F. Laender
{mangaravite, rodrygo, isacsandin, mgoncalv, laender}@dcc.ufmg.br
Department of Computer Science
Universidade Federal de Minas Gerais
Belo Horizonte, MG, Brazil

ABSTRACT

benchmark test collections. The TREC Enterprise track
contributed two such test collections suitable for expert finding in an enterprise setting. In particular, the World Wide
Web Consortium (W3C) test collection [5] includes W3C
working group members as candidate experts, whereas the
CSIRO Enterprise Research Collection (CERC) [1] includes
CSIRO employees as candidate experts. Test collections developed for expert finding in an academic setting have also
been made publicly available. For instance, the UvT test
collection [2] and its extended version TU [4] include employees of Tilburg University, the Netherlands, as candidate
experts. In turn, the ArnetMiner test collection [10] includes
researchers with a DBLP page as candidate experts. Other
non-public test collections for expertise retrieval have also
been developed [3, see Section 4.2.4]. Table 1 describes core
properties and salient statistics of these test collections.
As shown in the top half of Table 1, most of the existing expertise retrieval test collections encompass candidate
experts from a single organization. The only exception is
the ArnetMiner test collection, which includes computer science researchers from multiple organizations as candidate
experts. On the other hand, both the ArnetMiner as well
as the W3C test collections encompass candidate experts
from a single knowledge area, namely, computer science and
web standards, respectively. In addition, none of these test
collections fully supports both the expert profiling as well
as the expert finding tasks. Indeed, W3C, CERC, and ArnetMiner focus on expert finding, with only CERC including relevance judgments performed by expert judges. In
contrast, UvT and TU primarily focus on expert profiling,
with expert finding supported via pseudo relevance judgments adapted from expert profiling judgments. Moreover,
except for TU’s support of expert profiling, none of these
test collections provides graded relevance judgments, which
could enable more discriminative ranking comparisons [9].
In this paper, we introduce the Lattes Expertise Retrieval
(LExR) test collection, a new test collection for expertise
retrieval in academia. As described in the bottom half of
Table 1, LExR addresses the strengths and weaknesses of
previous test collections in order to provide a comprehensive, large-scale benchmark for evaluating expertise retrieval
approaches. In particular, it is both cross-organization and
cross-area, encompassing candidate experts from all knowledge areas working in research institutions all over Brazil.
In addition, it includes graded relevance judgments performed by the experts themselves as opposed to external
judges. Moreover, additional cross-expert judgments are
included for expert finding, which provide improved cov-

Expertise retrieval has been the subject of intense research
over the past decade, particularly with the public availability
of benchmark test collections for expertise retrieval in enterprises. Another domain which has seen comparatively less
research on expertise retrieval is academic search. In this
paper, we describe the Lattes Expertise Retrieval (LExR)
test collection for research on academic expertise retrieval.
LExR has been designed to provide a large-scale benchmark
for two complementary expertise retrieval tasks, namely, expert profiling and expert finding. Unlike currently available
test collections, which fully support only one of these tasks,
LExR provides graded relevance judgments performed by
expert judges separately for each task. In addition, LExR is
both cross-organization and cross-area, encompassing candidate experts from all areas of knowledge working in research institutions all over Brazil. As a result, it constitutes
a valuable resource for fostering new research directions on
expertise retrieval in an academic setting.

Keywords
Academic search; expertise retrieval

1.

INTRODUCTION

The need for expertise emerges whenever proficient knowledge is required about a particular topic. Two information
retrieval tasks have been proposed to address complementary sides of this need: expert profiling and expert finding [3].
Expert profiling can be seen as a summarization task, aimed
at identifying the topics of expertise of a given person. In
turn, expert finding (often referred to as expert search) can
be seen as a ranking task, aimed at identifying expert persons given a topic of interest. These two complementary
tasks have been the subject of substantial research over the
past decade, which contributed to approaches for summarizing, ranking, and evaluating expertise.
One of the key driving forces for the progress of research
on expertise retrieval has been the availability of public
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

SIGIR ’16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2914678

721

cr
os
scr org
os
a
s- ni
ex are zati
on
pe a
r
fr t p
ee
ro
fil
t
ex opi ing
pe cs
r
gr t ju
ad
dg
e
i
ex d ju ng
pe
dg
r
ex t fi ing
pe nd
in
r
gr t ju g
ad
dg
ed
in
ju g
dg
in
g

domain

year

W3C [5]
CERC [1]
UvT [2]
TU [4]
ArnetMiner [10]

enterprise
enterprise
academia
academia
academia

2004
2007
2006
2008
2008

✔

LExR

academia

2015

✔

✔
✔
✔

✔

✔
✔

✔

✔
✔

✔

✔

✔

✔

✔
✔
✔
✔
✔
✔

#candidates

#documents

#queries

#qrels

size

1,092
3,500
1,168
977
1,033,050

331,037
370,715
36,699
32,567
1,632,440

99
127
743/1,491
239/1,266
13

9,860
2,862
4,318
2,112
1,781

5.70
4.20
0.31
1.11
0.85

206,697

11,942,014

1,450
/235

50,802
/1,635

7.51

✔

✔
✔

✔

Table 1: Core properties and salient statistics of publicly available test collections for expertise retrieval.
Whenever two figures are present for #queries or #qrels, they correspond to the expert profiling and expert
finding tasks, respectively. The uncompressed collection sizes are measured in GB.
erage of relevant experts for a given topic as well as better consensus on the relevance grades assigned to each expert by multiple judges. As a result, LExR coherently supports both expert profiling and expert finding, enabling the
investigation of a variety of research questions related to
expertise retrieval in academia. In the remainder of this
paper, we discuss the methodology behind the construction of the LExR test collection and its salient characteristics. The full test collection is available for download from
http://www.lbd.dcc.ufmg.br/lbd/collections.

Relevant candidates
All candidates

0.35

Proportion

0.30
0.25
0.20
0.15
0.10
0.05

ce

ce

cie
n
lS

Ag
ric
ult

dS

ura

r ts
dA
an

oc
ial

ers
ett

plie
Ap

isti
cs,
L
Lin

gu

Sc
ien

r
ee

ce
s

gin

En

ce
s

cie
n

Sc
ien
r th

ma
Hu

Ex
ac
ta

nd

Ea

alt

LExR TEST COLLECTION

The LExR test collection is based upon the Lattes platform,1 an internationally renowned initiative for managing
information about science, technology, and innovation for
individual researchers and research institutions in Brazil [6].
LExR was developed to provide a unified yet coherent benchmark for research on expert profiling and expert finding in
academia. In the remainder of this section, we describe the
methodology behind the construction of LExR. In particular, Section 2.1 describes the acquisition of documentary
evidence of expertise for a large set of academic experts,
whereas Section 2.2 describes the process of obtaining relevance judgments for expert profiling and finding.

nS

y
log

Bio

hS

cie
n

Ot
he

rs

ce
s

0.00

He

2.

0.40

Figure 1: Candidate experts per area of interest.

The Lattes platform currently stores the academic curricula of nearly 4.5 million individuals involved in research
activities in Brazil. Each curriculum comprises information
about an individual’s academic life, including his or her current and past affiliations, earned degrees, academic services,
student supervisions, research projects, and scientific publications. To start off with a large set of promising experts,
we obtained the curricula of 206,697 individuals with a doctorate degree and at least one publication. Figure 1 shows
the distribution of candidate experts per broad area of interest, as informed in their curriculum (for now, ignore the
red bars). As shown in the figure, exact and earth sciences
contribute the majority of candidate experts in the collection (36%), followed by biology (27%) and health sciences
(16%). The least represented area is linguistics, languages,
and arts, which account for less than 1% of all candidates.

In order to simulate a general-purpose academic search
scenario, we retained only metadata records associated with
scientific publications, which are typically available in online
repositories such as digital libraries and academic social networks. The resulting 11,942,014 documents comprise multiple types of publication, including books and book chapters
(8%), journal articles (20%), conference papers (46%), and
other published material (26%). Most documents are written in Portuguese (61%), followed by English (22%), Spanish
(9%), and other languages (8%). Among relevant candidates
in our ground-truth, English documents are prevalent (51%),
followed by Portuguese (37%), Spanish (6%), and other languages (5%). For each document with a digital object identifier (DOI), we further attempted to recover its abstract by
scraping the landing page associated with its DOI. Given
the enormous amount of publishers in our corpus (precisely,
2,836), we handcrafted scrapers for the 22 most prolific ones,
which account for 80% of all documents with a DOI. Factoring in download failures, we were able to recover the abstract
for a total of 413,356 documents. Another 69,866 abstracts
were recovered using the Mendeley API.2 Figure 2 shows
the distribution of profile sizes in terms of the number of

1

2

2.1 Candidates and Documents

http://lattes.cnpq.br/

722

http://dev.mendeley.com/

Number of experts (log)

documents as well as the number of tokens in each profile.
In both cases, we can observe a long-tail distribution with
most candidates having relatively small profiles and a few
candidates having very large profiles.

perts, 1,450 (27%) responded to our invitation and provided
relevance judgments on the suggested topics of their expertise, with the option to inform us of further topics not among
the suggested ones. Relevance judgments were performed
on a graded scale, with 0, 1, and 2 indicating an irrelevant,
relevant, and highly relevant topic for the expert’s profile,
respectively. Altogether, these 1,450 experts (queries) identified 50,802 somewhat relevant topics (qrels) that make our
benchmark for expert profiling. Table 2 shows a breakdown
of the average number of qrels of various relevance grades
per query. On average, each expert in our benchmark has a
total of 27 relevant topics of expertise in his or her profile.

106
105
104
103
102
101
100
0

1000

2000
3000
4000
Profile size (in documents)

5000

6000

Number of experts (log)

expert profiling
106
105
104
103
102
101
100
0

50000

100000
150000
Profile size (in tokens)

200000

expert finding

grade

avg. qrels

grade

avg. qrels

1
2

10.90 (39%)
16.73 (61%)

1
2
3

1.90 (20%)
3.25 (34%)
4.41 (46%)

Table 2: Average number of relevants of various
grades per expert profiling and expert finding query.

250000

2.2.2 Expert Finding

Figure 2: Distribution of profile sizes.

While relevance judgments for expert profiling could be
trivially converted into judgments for expert finding (and
vice versa), the resulting judgments would likely lack in
coverage and coherence. To illustrate this situation, Figure 3 depicts a direct conversion of expert profiling judgments (represented by directed edges running from experts
to topics, weighted by the relevance grade assigned in each
judgment) into expert finding judgments (represented by
transposed edges running from topics to experts).

2.2 Queries and Relevance Judgments
For a test collection to provide a realistic benchmark for
evaluation, a set of queries should be selected that are representative of real user information needs [9]. Although we
do not have direct access to expert finding users in our scenario, we do have access to expert profiling users, namely,
the experts themselves. In this section, we describe the process of producing queries and relevance judgments for both
the expert profiling and the expert finding tasks.

expert proﬁling

2.2.1 Expert Profiling
Early test collections for expert finding such as the W3C
test collection [1] lacked knowledgeable judges to assess the
relevance of candidate experts [3]. For expert profiling, the
most knowledgeable judge to assess an expert’s profile is arguably the expert him or herself. Indeed, even when expertise profiles are meant to be used by others, as is the case
in academic and professional online social networks, they
must be typically approved by the profiled expert beforehand. With this in mind, we invited a subset of 5,355 of
the most prominent researchers from our set of candidates
to participate in our expert profiling judgments. These included researchers from 123 research consortia awarded by
the Brazilian government with a five-year grant (starting
from 2008) to foster networks of collaborative research in
several areas considered strategic for the country.
In contrast to previous expert profiling test collections,
such as UvT [2] and TU [4], we did not impose a predefined set of topics of expertise for each expert to choose
from. Instead, each of the 5,355 invited experts was presented with a list of suggested topics obtained by pooling
nine standard content-based tag recommenders operating
on different fields of the expert’s publications, namely, titles, abstracts, and keywords [8]. In particular, each expert
was presented with up to 60 suggested topics in no particular order, selected in a round-robin fashion from the top 50
tags returned by each recommender. Of the 5,355 invited ex-

3
e1

e2

1

2

expert ﬁnding
t1
t2
t1

3
t1

t2

2

1

e1
e2
e1

Figure 3: Asymmetric conversion between expert
profiling and expert finding relevance judgments.
From Figure 3, a coverage problem may arise in the resulting expert finding judgments to the extent that further
relevant experts on topics t1 and t2 will be missing, unless
they have been previously included in the expert profiling
judgments. Likewise, in a typical situation where expert profiling judgments are performed by different judges (ideally,
each expert would judge his or her own profile), relevance
grades assigned to the same topic will not be comparable to
one another, thus resulting incoherent expert finding judgments. As an illustrative example, a boastful yet unknowledgeable candidate e1 may assign him or herself a relevance
grade 3 for topic t1 , whereas a modest yet proficient candidate e2 may assign him or herself a grade 2 for the same
topic. When converted into expert finding judgments, these
assignments may convey the wrong impression about these
candidates’ relative expertise on topic t1 .

723

3. CONCLUSIONS

To overcome such an asymmetry, we conducted a second
round of judgments aimed specifically at the expert finding
task. To this end, we invited a subset of the researchers that
had contributed judgments on their own expertise profiles
to judge the expertise of others. To avoid overly specific
topics, we discarded 102 of the 1,450 original respondents
with a single topic of expertise shared by at most one other
researcher. As a result, for each topic of his or her expertise, each of the invited 1,348 researchers (acting as a judge)
was presented with a list of at least two other candidates
that had also declared themselves somewhat experts on the
topic. A link to the full curriculum of each candidate was
also provided to aid in the judgment. The judge was then
asked to determine the expertise of each candidate on the
topic following a graded scale, with 0, 1, 2, and 3 indicating
an unknowledgeable, somewhat knowledgeable, very knowledgeable, and expert person on the topic, respectively. Of
the 1,348 invited researchers, 513 (38%) agreed to act as
judges for this second round of relevance judgments.
To enforce a minimum level of support and an improved
consensus on the judged experts, we retained 235 topics that
had at least three self-declared experts (in the expert profiling judgments) subsequently considered as experts by at
least two judges (in the expert finding judgments). The final
relevance grade for an expert was computed as the rounded
mean of the grades received from all judges. Analogously,
we estimated the disagreement about an expert as the standard deviation around the expert’s unrounded mean grade.
Per-topic disagreement values were then obtained by averaging the disagreements observed for all experts associated
with each topic. Figure 4 shows the distributions of disagreement considering all topics, as well as subsets of topics
with experts from an increasing number of areas of interest.
As shown in the ‘All’ group in Figure 4, disagreement
values vary considerably across topics, with an average of
0.29 ± 0.02. Interestingly, the more knowledge areas covered
by a topic, the higher the disagreement observed for this
topic, with topics covering five or more areas showing the
highest average disagreement (0.37 ± 0.07). In total, the 235
obtained topics (queries) were associated with 1,635 experts
(qrels) in our benchmark for expert finding. As shown in
Table 2, this makes an average of over five experts per query.
In addition, these experts are more evenly distributed across
knowledge areas, as highlighted by the red bars in Figure 1.

The public availability of benchmark test collections has
fostered intense research on expertise retrieval over the past
decade, particularly in the enterprise domain. In this paper,
we described a new test collection for expertise retrieval in
the academic domain, which has not seen as much research
progress in comparison. In particular, the Lattes Expertise Retrieval (LExR) test collection provides a large-scale
benchmark for the evaluation of two complementary expertise retrieval tasks, namely, expert profiling and expert finding.3 Moreover, it is the first publicly available test collection that fully supports both tasks, with dedicated graded
relevance judgments performed by real experts for each task
separately. In addition, it is both cross-organization and
cross-area, encompassing candidate experts from all areas
of knowledge working in research institutions spread all over
Brazil. Together, these features make LExR a unique and
valuable resource for the research community.

Acknowledgments
This work was partially funded by projects InWeb (MCT/
CNPq 573871/2008-6) and MASWeb (FAPEMIG/PRONEX
APQ-01400-14), and by the authors’ individual grants from
CNPq and FAPEMIG.

4. REFERENCES
[1] P. Bailey, N. Craswell, I. Soboroff, and A. P. de Vries.
The CSIRO enterprise search test collection. SIGIR
Forum, 41(2):42–45, 2007.
[2] K. Balog, T. Bogers, L. Azzopardi, M. de Rijke, and
A. van den Bosch. Broad expertise retrieval in sparse
data environments. In Proc. of SIGIR, pages 551–558,
2007.
[3] K. Balog, Y. Fang, M. de Rijke, P. Serdyukov, and
L. Si. Expertise retrieval. Found. Trends Inf. Retr.,
6(2–3):127–256, 2012.
[4] R. Berendsen, M. de Rijke, K. Balog, T. Bogers, and
A. van den Bosch. On the assessment of expertise
profiles. J. Am. Soc. Inf. Sci. Technol.,
64(10):2024–2044, 2013.
[5] N. Craswell, A. P. de Vries, and I. Soboroff. Overview
of the TREC 2005 Enterprise track. In Proc. of
TREC, 2005.
[6] J. Lane. Let’s make science metrics more scientific.
Nature, 464(7288):488–489, 2010.
[7] V. Mangaravite and R. L. T. Santos. On
information-theoretic document-person associations
for expert search in academia. In Proc. of SIGIR,
2016.
[8] I. S. Ribeiro, R. L. T. Santos, M. A. Gonçalves, and
A. H. F. Laender. On tag recommendation for
expertise profiling: A case study in the scientific
domain. In Proc. of WSDM, pages 189–198, 2015.
[9] M. Sanderson. Test collection based evaluation of
information retrieval systems. Found. Trends Inf.
Retr., 4(4):247–375, 2010.
[10] J. Tang, J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su.
ArnetMiner: extraction and mining of academic social
networks. In Proc. of KDD, pages 990–998, 2008.

Disagreement

1.0
0.8
0.6
0.4
0.2
0.0
All

1

2

3

4

5+

Figure 4: Judging disagreement distributions for all
topics and for subsets of topics with experts from an
increasing number of knowledge areas.

3

724

Reference results for both tasks can be found in [7, 8].

