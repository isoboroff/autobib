Principles for the Design of Online A/B Metrics
Widad Machmouchi

Georg Buscher

Bing, Microsoft, USA
widadm@microsoft.com

Bing, Microsoft, USA
georgbu@microsoft.com
When designing an A/B metric, a question of critical importance
is: How do you know you found a good metric?

ABSTRACT
In this paper, we describe principles for designing metrics in the
context of A/B experiments. We share some issues that comes up
in designing such experiments and provide solutions to avoid
such pitfalls.

Traditionally, in research, this question has been framed and
treated as a prediction problem. Given logs with recorded user
behavior, and given ground truth labels for the successfulness /
outcome of the logged user interactions, build a predictor
(classifier) that ingests signals from the logs and optimizes
precision/recall/etc. against the ground truth labels [1,2]. Then,
the assumption often is that the best predictor will also result in
the best A/B metric.

Keywords
A/B measurement; controlled experiments; metric design
principles.

1. INTRODUCTION
Online experimentation is becoming more and more popular.
Controlled experiments with thousands or even millions of users
are applied to establish causal relationships between a new
treatment and a change in user behavior. Such A/B
experimentation is used widely now in industries related to social
media, e-commerce, online publishing, search engines, etc.,
which try to optimize for engagement, revenue, user success,
among other aspects [2]. One of the key factors in evaluating
online controlled experiments are metrics. They help discern
whether the treatment effect on users was desired or not and
therefore guide ship decisions of the teams building the new
treatments. For that reason, good A/B metrics are of critical
importance in order to make sound data-driven decisions.

While this type of evaluation is a reasonable first step in designing
a good A/B metric, it can easily yield something ineffective and
flawed for the following reasons.

Yet, it is very easy to build A/B metrics that suffer from
undetected weaknesses and which eventually point in the wrong
direction leading to – unknowingly – incorrect ship decisions.
Therefore, great care has to be devoted to proper design of A/B
metrics that are expressive, robust, and trustworthy.

2) The usage of endogenous signals: Search engine logs usually
contain information on how the search results page served to the
user looked like, and how the user interacted with the page. A
predictor might use signals based on both types of information,
yet only the latter type of information is generally safe to use
when the application is an A/B metric. We call the former type of
signals “endogenous signals” which encode the system response
from the search engine, and the latter type “exogenous signals”
which encode the user response. Using endogenous signals in the
predictor opens it up for loopholes that can seriously affect its
trustworthiness as an A/B metric. As an obvious example, if a
predictor declares user success whenever the weather forecast
element is displayed on the search results page (e.g., for the query
“weather” that could make sense), then this predictor would be in
favor of showing the weather forecast on every single results
page, no matter what query, and it would not guard against
showing a broken forecast or a forecast for the wrong location to
the user, all of which is certainly undesired.

1) Insensitivity to changes the metric is supposed to measure:
When designing a metric, one needs to be cognizant of the nature
of treatment changes it is supposed to measure and be sensitive
to. Even a very good predictor might not pick up certain user
behavior changes and therefore miss measuring an A/B
difference. This can happen particularly for rare user interaction
patterns that a predictor might, e.g., prune away to avoid overfitting. Yet, a particular treatment might change the occurrence of
this specific rare user interaction pattern, and not much else, and
hence the predictor would be “blind” to the change.

In this paper we report a few important lessons learnt while
designing A/B ship metrics for the large search engine Bing.com.
Based on many years of experience, we elaborate on a number of
important aspects that should be taken into account when
designing online A/B metrics, specifically having user
satisfaction metrics in mind.

2. THE A/B METRIC PROBLEM
The eventual purpose of A/B metrics, particularly OECs (Overall
Evaluation Criterion) is to tell whether the treatment user group
A demonstrated more desirable behavior than the control user
group B. In the context of web search, this often means measuring
whether treatment users had an improved or impaired search
experience in terms of user satisfaction.

Hence, rather than treating the initial question as a prediction
problem, it should be treated as an A/B measurement problem:
given an experiment corpus, i.e., a set of controlled A/B
experiments with ground truth labels specifying whether an entire
experiment is considered an improvement or a regression, design
an A/B metric that directionally aligns best with the ground truth
labels. Ground truth labels for entire experiments can be attained
from a panel of experts that don’t only take a large collection of
metrics into account, but also deep-dive to get a detailed technical
understanding of the treatment change and how it could affect the
user experience. It is critical to cover a diverse set of experiments
in the experiment corpus, and ideally include representative

Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for thirdparty components of this work must be honored. For all other uses,
contact the Owner/Author.
Copyright is held by the owner/author(s).
SIGIR '16, July 17-21, 2016, Pisa, Italy
ACM 978-1-4503-4069-4/16/07.
http://dx.doi.org/10.1145/2911451.2926731

589

(i.e., treating any value > x as x), or functional transformation
(e.g., using the log function reducing effects from outliers).

experiments for all types of changes to the search engine that need
to be evaluated by the metric, since otherwise the metric might
have “blind spots” regarding certain treatments. Based on such an
experiment corpus, the best metric should both have the best
directional alignment with the labels as well as be most sensitive
to the treatment effects (statistical sensitivity in terms of, e.g., tvalue of a t-test).

2) The metric can be invalid if it changes due to denominator
movements. Figure 1 shows an example where the objective is to
measure user engagement in the form of clicks. Depending on
which denominator is used, the metrics can move in contradictory
directions or not at all, and this can be very confusing for
experimenters making ship decisions. Generally, the more finegrained the denominator, the higher the sensitivity of the metric,
yet the higher the chance that the denominator moves on its own.
Our guidance is to use the metric with the most fine-grained
denominator which doesn’t yet move on its own. In the figure
below that would be Clicks/Session.

3. DESIGNING A METRIC SYSTEM
No metric is perfect – all of them have weaknesses or loopholes
and incorrectly move for certain treatments they are not designed
for. Hence, it is important to design a good metric system for
making ship decisions, i.e., a collection of metrics that measure
treatment effects from various different angles, so that based on
their entirety it is possible to get a comprehensive understanding
of the implications of the experiment. The challenge with having
a large number of metrics, though, is that 1) some metrics will
move statistically significantly by chance (for independent
metrics typically 1 in 20 with a pval threshold of 0.05), and 2) for
strong treatment effects many metrics will move, some of them in
seemingly contradictory ways. As a consequence, experimenters
might be allured to cherry-pick the metrics that are most in-line
with their intuition, which can easily lead to seemingly datadriven yet unsound ship decisions.

Figure 1: Denominator changes can lead to invalid and
contradictory metric movements.

To address these challenges, we generally design metric systems
in a hierarchical way. At the top are the most robust metrics, in
our case a metric like “sessions per user”. They are defined at the
user-level, have the fewest built-in assumptions on user behavior
interpretation, yet are usually are the least sensitive. On the next
level come session-level metrics such as Session Success Rate.
They have built-in assumptions on what success looks like,
therefore are less robust, yet more sensitive. On the third level,
there are many feature-specific metrics, often on the impression
level, such as Web Result Success Click Rate, etc. These have
even more assumptions on how user success will show up for
certain experiments, are least robust, yet most sensitive.

5. DEBUGGABLITY
Once an A/B metric system is developed, experimenters will rely
on the movements in the metric to decide whether to ship the
change they are testing. However, even if these changes are
localized to a specific feature, they can affect users’ behavior
outside these features and influence their interaction with the rest
of the page.
Due to these effects, it is important for an online metric system to
be easily debuggable and its movements clearly understood. A
metric need to be easily decomposable to the different signals and
user behavior that contributes to its movements. Simple linear
functions that combine basic features representing users’
interactions guarantee that experimenters are able to pinpoint to
the main drivers behind a metric movement. Compare that with
machine-learned metrics that can easily become a black box,
where the inner workings of the metric are opaque. This would
make debugging the metric movements and identifying the
failings of the change being tested hard for the experimenters.

As part of this hierarchy, such a system should have metrics that
covers different areas of the page. This is what defines the scope
of a metric. A metric that measures the change in user behavior
on a specific feature is a feature-level metric, e.g. Web Result
Success Click Rate. A metric that measures success across all the
page is a page-level metric and has a higher scope than featurelevel metrics, e.g. Page Click Through Rate. Similarly, we can
define metrics that consider success across one session of the
user, or even across all their traffic during the experiment. A
metric system that targets all the different scopes guarantees that
experiments are not missing global effects of their changes that
cannot be captured by the feature-level metrics.

6. ACKNOWLEDGMENTS
The authors would like to thank Aidan Crook, Anand Oka,
Siamak Faridani and Victor Hu for interesting discussions on the
topic of this paper.

When interpreting a scorecard, we would look at metrics in a topdown fashion. Particularly when metrics on the first or second
level are moving already, then metrics on the third level have to
be interpreted very carefully due to, e.g., denomination effects as
explained in the next section.

7. REFERENCES

4. METRIC DEFINITIONS

[1] H. Feild et al. 2010. Predicting searcher frustration. In
SIGIR’10: 34–41.

There are few very important aspects to keep in mind when
working on concrete metric definition formulas.

[2] Hassan. 2012. A semi-supervised approach to modeling
web search satisfaction. In SIGIR’12: 275–284.

1) When counting events, time, or anything that doesn’t have an
upper bound, then using plain counts in the metric can introduce
large amounts of noise making the metric less sensitive. For a
metric like “time to click” where time from the page request to
the first click is measured, variance is usually very high due to
outliers. Effective ways to address this are to apply truncation

[3] Ron Kohavi, Alex Deng, Brian Frasca, Roger Longbotham,
Toby Walker, and Ya Xu. Trustworthy online controlled
experiments: Five puzzling outcomes explained.
Proceedings of the 18th Conference on Knowledge
Discovery and Data Mining, 2012.

590

