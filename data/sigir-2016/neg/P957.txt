Quote Recommendation in Dialogue
using Deep Neural Network
Hanbit Lee‚àó , Yeonchan Ahn‚àó , Haejun Lee‚àó‚àó , Seungdo Ha‚àó , Sang-goo Lee‚àó
‚àó

‚àó

School of Computer Science and Engineering, Seoul National University, Seoul, Korea
‚àó‚àó
ArtiÔ¨Åcial Intelligence Team, Samsung Electronics, Seoul, Korea

{skcheon, acha21, seungtto, sglee}@europa.snu.ac.kr, ‚àó‚àó haejun82.lee@samsung.com

ABSTRACT
Quotes, or quotations, are well known phrases or sentences
that we use for various purposes such as emphasis, elaboration, and humor. In this paper, we introduce a task of
recommending quotes which are suitable for given dialogue
context and we present a deep learning recommender system
which combines recurrent neural network and convolutional
neural network in order to learn semantic representation of
each utterance and construct a sequence model for the dialog thread. We collected a large set of twitter dialogues
with quote occurrences in order to evaluate proposed recommender system. Experimental results show that our approach outperforms not only the other state-of-the-art algorithms in quote recommendation task, but also other neural
network based methods built for similar tasks.

Figure 1: The example of tweet thread that includes
the quote, ‚ÄùBeggars can‚Äôt be choosers.‚Äù

Categories and Subject Descriptors
emails and blogs, convenience in text entry has become a
very important feature for these devices. It is more so for
wearable devices like Apple Watch and Samsung Galaxy
Gear where the input/output spaces are constrained even
further. An eÔ¨Äective quote recommender, after monitoring
an ongoing discourse, could recommend a few useful quotations from which the user can select instead of having to
(laboriously) type in the entire sentence. Moreover, the system can be extended to recommend not only quotes but also
other frequently-used responses as well.
Quote recommendation for writing (composition) has been
introduced in Tan, et al., [10]. In this paper, we extend the
task to dialogs. Dialogs diÔ¨Äer from plain composition in
that they consist of sequences of utterances, each of which
forms a semantic unit. Thus, it is important to construct
a recommendation model that is capable of capturing such
sequential structure of dialogs.
We propose a deep neural network based quote recommendation system, which combines recurrent neural network
(RNN ) and convolutional neural network (ConvNetwork ) in
order to learn semantic representation of each utterance and
construct a sequence model for the dialog thread.
As a dialog database, we use twitter dialog threads (threads
where only two users are exchanging tweets). A twitter
thread, or tweet thread, is a sequence of linked tweets, composed of an initial tweet and subsequent reply tweets. Figure 1 shows an example of a tweet thread that contains the
quote, ‚ÄúBeggars can‚Äôt be choosers‚Äù. We use a large set of
such threads with quote occurrences as the training set for
the recommendation task.
Experimental results show that our approach outperforms

I.2.7.7 [ArtiÔ¨Åcial Intelligence]: Natural Language Processing‚ÄîText Analysis

Keywords
Quote recommendation, Dialogue model, Deep neural network

1.

INTRODUCTION

Quotes, or quotations, are well known phrases or sentences
that we use in our writings and conversations for various purposes such as emphasis, elaboration, and humor. However,
it is not always easy to promptly come up with an adequate
quote that Ô¨Åts the situation at hand. We can try to Ô¨Ånd
one on search engines, but it is also hard to Ô¨Ågure out the
right search keywords because in most cases the quotes are
metaphorical so the words in those quotes do not always
match the words expressing our intentions. A quote recommender can be quite useful in these situations.
Meanwhile, as we rely more and more on mobile devices
in written communications including text messages, tweets,
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proÔ¨Åt or commercial advantage and that copies bear this notice and the full citation on the Ô¨Årst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speciÔ¨Åc permission
and/or a fee. Request permissions from permissions@acm.org.

SIGIR ‚Äô16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00

DOI: http://dx.doi.org/10.1145/2911451.2914734

957

2.

ƒë
7ZHHWYHFWRU

RELATED WORK

Quote recommendation can be viewed as a task of searching quote that Ô¨Åts in the given query texts. Tan et al., 2015
[10] apply learning-to-rank framework using cosine similaritybased features between query and quote context to recommend ranked list of quotes relevant to query. He et al., 2010
[2] also exploit similarity features between candidate paper
and current writing context. Huang et al., 2012 [4] proposes
to use translation model to connect the context of writing
and the cited paper. These approaches use the traditional
bag-of-words model which have a clear drawback: the loss
of semantics. The bag-of-words model performs especially
worse when it comes to dialogues because utterances in dialogues are in short and irregular colloquial forms.
With recent success of distributional word representation
which overcomes such drawback, there are several approaches
trying to learn distributional representation of sentences or
paragraphs. One of the most prominent and recently emerging method is convolutional neural network. [5] Convolutional network based approaches are resulting in state-ofthe-art performance for the tasks like sentence classiÔ¨Åcation
[6] and relevant short text matching [9]. We also take advantages of convolutional network to learn intermediate representation of each tweet.
Our work is related to dialogue models which have been
studied for the purpose of building automatic dialogue systems. [13, 1] Recently with the signiÔ¨Åcant advances in training of neural network, deep neural networks based dialogue
models are actively being proposed. Most of them take recurrent neural network based approaches to model the utterance sequence. [12, 8] Their purpose is to generate natural
language sentences, but our task focuses on recommending
a ranked list of quotes among candidate quotes.

3.

0D[SRROLQJ

&RQYROXWLRQ

‡¢ï‡¨µ
‡¢ï‡¨∂
‡¢ï‡¨∑
‡¢ï‡¨∏
‡¢ï‡¨π
‡¢ï‡¨∫

3UHWUDLQHG
:RUGYHFWRUV

"PGWTCNA0'6 PD\EHIRU
\RXUELUWKGD\&219

Figure 2: Overall architecture of our deep neural
network model for quote recommendation

of information. LSTM retains latent vectors and updates
those vectors by several gated functions with input vector
for each time step. In our model we recurrently feed LSTM
with distributional vector that represent each tweet in the
tweet thread as shown in Figure 2. Formally, the input of
LSTM is a tweet thread which is a sequence of tweet vectors:
[v1 , v2 , . . . , vn ] where n is length of the tweet thread.

3.1.2

Softmax

The output of LSTM layer is passed to fully connected
layer with softmax activation in order to compute the probability of candidate quotes to be recommended. It computes
probability of recommending quote j as follows:
ex ¬∑w j
p(y = j|x ) = K
x ¬∑w k
k=1 e

OUR DEEP NEURAL NETWORK MODEL

where x is the output vector of LSTM, w j is the weight
vector of class j in fully-connected networks, and K is the
number of class, in this case, the number of target quotes.

3.2

Convolutional Neural Networks for mapping tweets to intermediate vectors

We use convolutional neural networks to map tweets to
their intermediate vectors. Convolution operation captures
the signiÔ¨Åcant local semantics, i.e., n-grams, of a tweet, so
convolutional neural network can learn the optimal tweet
representation for our quote recommendation task. Our
ConvNetwork is composed of a single convolution layer and
a max pooling layer.

Recurrent Neural Network for tweet sequence modeling

Since use of a quote in a dialogue highly depends on temporal history of previous utterances, it is important to model
a dialogue as a sequence of utterances. Recurrent neural network is a neural network which is widely used in modeling
sequence, because it eÔ¨Äectively learns signiÔ¨Åcant patterns of
sequential data. So we exploit recurrent neural network to
recommend quotes suitable for given context of dialogue.

3.1.1

‡¢ï‡¨µ
‡¢ï‡¨∂
‡¢ï‡¨∑
‡¢ï‡¨∏
‡¢ï‡¨π
‡¢ï‡¨∫

"EQPXQNWVKQPCN ,ZDQQD
EHWDOOHUFDQ,ERUURZ
VRPHRI\RXUWDOOQHVV"
-XVWIRUDIHZZHHNVVHH
LI,OLNHLW

In this section we describe our deep neural network model
for quote recommendation. Our architecture is a combination of the convolutional neural network (ConvNetwork ) and
the recurrent neural network (RNN ) as shown in Figure 2.
ConvNetwork maps tweets in the thread to their distributional vectors. And then the sequence of tweet distributional
vectors are fed to RNN so as to compute the relevance of
target quotes to the given tweet dialogue. In the following,
we brieÔ¨Çy explain main components of our network.

3.1

6RIWPD[

/670

/670

4827(6

not only the other state-of-the-art algorithms for quote recommendation task, but also other neural network based
methods built for similar tasks.

3.2.1

Convolution

The input to ConvNetwork is a tweet matrix t ‚àà Rd√óm
where m is the number of words in the tweet and d is
the dimensionality of word vectors. i-th column of t is ddimensional word vector oi ‚àà Rd corresponding to the ith word in the tweet. Convolution layer captures the local
patterns in tweet by convolving a Ô¨Ålter matrix of weights
w ‚àà Rd√óh with the input tweet matrix t ‚àà Rd√óm , where
h is the Ô¨Ålter width which is a hyper parameter. A feature

Long short-term memory unit

We use long short-term memory unit (LSTM) [3] which
is a recurrent neural network with several gates which allow networks to learn long-term dependencies without loss

958

Table 1: Statistics of our dataset
Dataset
q
N
t
n
m
ALL
412 222,645
540.4
3.85 13.9
Top100 100 159,302 1593.0 3.94 13.4
Top200 200 202,920 1014.6 3.90 13.6
Top300 300 218,132
727.1
3.89 13.7
Top400 400 222,408
556.0
3.89 13.7

map c ‚àà Rm‚àíh+1 is produced by convolution where each
component is computed as follows:

ci = f (
w ‚ó¶ t[:,i:i+h‚àí1] + bi )
k,j

where ‚ó¶ is element-wise multiplication, t[:,i:i+h‚àí1] is matrix
slice of size h, bi is the bias value, and f is non-linear activation function. Generally, more than one Ô¨Ålter is applied
to convolution layer to yield richer information, so we use
multiple Ô¨Ålters w(1) , w(2) , . . . , w(p) where p is another hyper
parameter which indicates the number of Ô¨Ålters. With multiple Ô¨Ålters, we get multiple feature maps c(1) , c(2) , . . . , c(p) .

3.2.2

n is average number of tweets in a tweet thread, and m is
average number of words in a tweet. We use 10% of dataset
as validation set, another 10% as test set and rest of dataset
as training set.
Originally there is only one gold quote for each query context in test data, but there are many cases in which more
than one quote can be used. For example, the quote ‚ÄúGreat
minds think alike.‚Äù can be substituted with the quote ‚ÄúBirds
of a feather Ô¨Çock together.‚Äù in the situation when two people share an idea. We sampled test queries from the top100
dataset and manually tagged the appropriate quotes that
can be used in each query context. (MultipleGold )

Max-pooling

After obtaining feature maps, we apply max-pooling operation which take the maximum value cÃÇ(i) from each of
feature map c(i) and form a p-length feature vector:
v =< cÃÇ(1) , cÃÇ(2) , . . . , cÃÇ(p) >
Eventually m tweet matrices in tweet thread are mapped to
sequence of m tweet representation vectors.

4.2

ConvNetwork

[t1 , t2 , . . . , tm ] ‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚àí‚Üí [v1 , v2 , . . . , vm ]

3.3

4.2.1

Training of the networks

4.1

Baselines

We compare our approach with three state-of-the-art content based recommendation approaches [2, 4, 10]. Contextaware relevance model (CRM) recommends a quote based
on similarity between the query and the contexts of the
quote using bag-of-words representation. Citatioin translation model (CTM) applies a translation model to quote
recommendation by using the probability that the query
context would be translated into the quote. Learning-torecommend quote (LRQ) is most recent approaches that
use learning-to-rank framework with quote-based features,
quote-context similarity features, and context similarity features. We also experiment with CNN and RNN separately
to compare with our combined networks. Since these simple
neural models cannot accept tweet sequence as input, so we
concatenate all tweets in a tweet thread into one long context tweet and use it as the input for the particular thread.

We initialize the word vectors in our network with those
trained from unsupervised neural language model and keep
them static. It is a popular method to improve performance in case supervised training data is not suÔ¨Écient. We
use publicly available pre-trained word vectors, which were
trained on 27 billion words of 2 billion tweets from Twitter.
[7]
The entire networks are trained to minimize the crossentropy of the predicted and true distributions. The objective function includes an L2 regularization term over the
parameters to prevent overÔ¨Åtting. RMSProp [11] is used
to optimize stochastic gradient decent with a learning rate
of 10‚àí3 and a decay rate of 0.9. Dropout is adapted on
max-pool layer in ConvNetwork and fully-connected layer
in RNN. We set other hyper-parameters as follows: 3 for Ô¨Ålter width (h), 500 for the number of feature maps (p), and
dropout rate as 0.5. And we use ReLU activation function.

4.

Baselines and Evaluation Metrics

4.2.2

Evaluation Metrics

MRR: Mean reciprocal rank (MRR) is the average score
of inverse rank of the gold quote in recommendation list.
1
1 
M RR =
|test|
rank(goldq uote)

EXPERIMENTS
Data Construction

We collected the quotes to be recommended from two
sources of quotes: one is the Wikiquote1 website, and the
other is Oxford Concise Dictionary of Proverbs2 . For each
quote, we collected tweet dialogues that contain at least one
occurrence of the quote.
From each tweet dialogue thread, we only use the tweets
up to the quote occurrence as context. For the tweet that
contains the quote, only the part from the Ô¨Årst word to the
last word before the quote is used as context.
We started out with top 100 quotes in order of quantity
of relevant tweet threads and then expanded to top 200,
300, and 400. Table 1 shows the statistics of datasets. q is
total number of quotes, N is total number of tweet threads,
t is average number of collected tweet threads per quote,

Recall@k : Since there is only one gold quote for each
query in original test set, Recall@k is the number of cases
that the gold quote is recommended in top-k result divided
by total test cases.
NDCG@k : Normalized discounted cumulative gain (NDCG)
is a widely used evaluation metric in various IR tasks. It
takes into account the ranking position of gold quotes in
recommendation list.
Hit@k : For MultipleGold test set, we use Hit@k which
is the number of cases that any of gold quotes are recommended in top-k result divided by total test cases.

4.3

1

Results and Discussion

The comparison results of diÔ¨Äerent methods for the top100
dataset are shown in table 2. As we can see, CRM and LRQ

http://en.wikiquote.org
2
Oxford University Press, 1998

959

Table 2: Recommendation performance of
methods for top100 dataset
Method
MRR Recall@5 NDCG@5
CRM
0.207
0.279
0.198
CTM
0.274
0.359
0.269
LRQ
0.221
0.308
0.214
CNN
0.378
0.486
0.385
RNN
0.375
0.489
0.389
CNNRNN 0.416
0.532
0.422
0.6

CRM
LRQ
RNN

0.5

diÔ¨Äerent

Query context
@convolution Ahh!
I‚Äôm so
excited now looking forward to
September!
@neural NET yay me too!!! I
didn‚Äôt think u would still be in
Orlando in September!!!
@convolution yea...things are
taking longer than expected
@neural NET hey don‚Äôt worry!
[Quote]
Figure 4: An example results
dation

Hit@5
0.470
0.579
0.523
0.668
0.661
0.698

CTM
CNN
CNNRNN

Results
1.Slow and steady
wins the race
2.Business
before
pleasure
3.Patience is a virtue
4.All good things
must come to an
end
5.Better safe than
sorry
of quote recommen-

0.4

on real twitter dialogue data to show that our approach is
eÔ¨Äective for quote recommendation. In the future, we plan
to investigate several useful dialogue features that may not
be captured via neural networks.

0.3
0.2
0.1

6.

0
Top100

Top200

Top300

Figure 3: Results of recall@5 of diÔ¨Äerent methods
when increasing # of target quotes

7.

REFERENCES

[1] M. Gasic, C. Breslin, M. Henderson, D. Kim,
M. Szummer, B. Thomson, P. Tsiakoulis, and
S. Young. On-line policy optimisation of bayesian
spoken dialogue systems via human interaction. In
ICASSP, 2013.
[2] Q. He, J. Pei, D. Kifer, P. Mitra, and C. L. Giles.
Context-aware citation recommendation. In WWW,
2010.
[3] S. Hochreiter and J. Schmidhuber. Long short-term
memory. Neural Computation, 9(8):1735‚Äì1780, 1997.
[4] W. Huang, S. Kataria, C. Caragea, P. Mitra, C. L.
Giles, and L. Rokach. Recommending citations:
Translating papers into references. In CIKM, 2012.
[5] N. Kalchbrenner, E. Grefenstette, and P. Blunsom. A
convolutional neural network for modelling sentences.
In ACL, 2014.
[6] Y. Kim. Convolutional neural networks for sentence
classiÔ¨Åcation. In EMNLP, 2014.
[7] J. Pennington, R. Socher, and C. D. Manning. Glove:
Global vectors for word representation. In EMNLP,
2014.
[8] I. V. Serban, A. Sordoni, Y. Bengio, A. Courville, and
J. Pineau. Building end-to-end dialogue systems using
generative hierarchical neural network models. In
ICASSP, 2013.
[9] A. Severyn and A. Moschitti. Learning to rank short
text pairs with convolutional deep neural networks. In
SIGIR, 2015.
[10] J. Tan, X. Wan, and J. Xiao. Learning to recommend
quotes for writing. In AAAI, 2015.
[11] T. Tieleman and G. Hinton. Lecture 6.5 - rmsprop,
coursera: Neural networks for machine learning. In
Technical report, 2012.
[12] O. Vinyals and Q. V. Le. A neural conversational
model. In arXiv, 2015.
[13] S. Young, M. Gasic, B. Thomson, and J. D. Williams.
Pomdp-based statistical spoken dialog systems: A
review. IEEE, 101(5):1160‚Äì1179, 2013.

which search for quotes based on similarity features between
query and quote contexts, do not result in successful performance. That is because they cannot Ô¨Ånd out contexts
which are semantically similar to the query but expressed
in diÔ¨Äerent words. CTM which use translation model performs slightly better. Deep neural network based approaches
achieve higher scores than the non-neural methods, which
shows that the distributed word representation helps semantic word matching and the deep neural network automatically extract meaningful representation of tweet context in
respect to the quote recommendation. CNN captures significant local semantic features, i.e., n-grams, while RNN captures the overall ordering of words in context. Our approach
outperforms both separate CNN and RNN models showing
that per-tweet feature extraction captures structural context of dialogue while simple concatenation of tweets cannot. Figure 3 shows the results of recall@5 when increasing the number of target quotes. Although overall accuracy
decreases as the number of target quotes increases, our approach consistently outperforms other methods.
Hit@5 evaluation of our model reaches almost 0.7 for top100
dataset which means that our recommendation system recommends at least one relevant quote in top 5 list in seven out
of ten times. This proves that our recommendation model
is practically useful. Figure 4 shows the example of recommendation results for a query context in the test set. As
we can see, our system recommends quotes comforting the
user named ‚Äúconvolution‚Äù and it is quite appropriate to the
situation.

5.

ACKNOWLEDGEMENT

This paper was supported by Samsung Electronics.

Top400

CONCLUSION

In this work, we present a task of quote recommendation
in a dialogue. We propose a deep neural network model
which eÔ¨Éciently extracts meaningful local semantic features
from each tweet using convolutional network and learns each
tweet features in sequence. We evaluate the proposed model

960

