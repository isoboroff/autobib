Self-Paced Cross-Modal Subspace Matching
Jian Liang1 , Zhihang Li1 , Dong Cao1 , Ran He1,2∗, Jingdong Wang3

2

1
Center for Research on Intelligent Perception and Computing,
National Laboratory of Pattern Recognition, Institute of Automation,
Chinese Academy of Sciences (CAS), Beijing 100190, China
Center for Excellence in Brain Science and Intelligence Technology, CAS, Shanghai 200031, China
3
Microsoft Research, Beijing 100190, China

{jian.liang,

zhihang.li, dong.cao, rhe}@nlpr.ia.ac.cn, jingdw@microsoft.com

ABSTRACT
Cross-modal matching methods match data from diﬀerent
modalities according to their similarities. Most existing
methods utilize label information to reduce the semantic
gap between diﬀerent modalities. However, it is usually time-consuming to manually label large-scale data.
This paper proposes a Self-Paced Cross-Modal Subspace
Matching (SCSM) method for unsupervised multimodal
data. We assume that multimodal data are pair-wised and
from several semantic groups, which form hard pair-wised
constraints and soft semantic group constraints respectively.
Then, we formulate the unsupervised cross-modal matching
problem as a non-convex joint feature learning and data
grouping problem. Self-paced learning, which learns samples
from ‘easy’ to ‘complex’, is further introduced to reﬁne the
grouping result. Moreover, a multimodal graph is constructed to preserve the relationship of both inter- and intramodality similarity. An alternating minimization method is
employed to minimize the non-convex optimization problem,
followed by the discussion on its convergence analysis and
computational complexity. Experimental results on four
multimodal databases show that SCSM outperforms stateof-the-art cross-modal subspace learning methods.

Keywords
Cross-Modal Matching; Heterogeneous Data; Unsupervised
Subspace Learning; Self-Paced Learning

1.

INTRODUCTION

Nowadays, multimodal data spring up as the popularity of
social network on the Internet. People would like to upload
personalized content through various forms such as text,
image, audio and video simultaneously, this phenomenon
urgently pushes the need for heterogeneous content retrieval.
Compared with unimodal retrieval, cross-modal retrieval
exploiting heterogeneous feature representations to discover
∗

Corresponding Author.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.

SIGIR ’16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00

DOI: http://dx.doi.org/10.1145/2911451.2911527

569

optimal heterogeneous content for a given user query, is more
and more frequently utilized. Although search engines also
provide such services, e.g., text-image search, they mainly
rely on homogeneous contents associated with heterogeneous
contents within the same web pages or tweets.
The
main challenge in cross-modal matching is how to bridge
the semantic gap between heterogeneous modalities (e.g.,
diﬀerent distributions, diﬀerent dimensionalities). In this
literature, some promising approaches [31, 37, 44] have been
developed to alleviate this gap. Probability based learning
algorithms [28, 16] along with subspace based learning
algorithms [12, 29, 37, 18, 22] are even more favored and
widely applied for cross-modal learning applications, e.g.,
image annotation and cross-language information retrieval.
Topic models, such as Latent Dirichlet Allocation (LDA)
[6], have proved eﬀective at describing the underlying
topics in one single modality. Following researches [5, 28]
focused on correlating topics among diﬀerent modalities, and
extended it to the multi-modal case. While [5] assumed
a one-to-one correspondence between the topics of each
modality, and [28] assumed a regression module between
two sets of topics instead of one-to-one correspondence, and
achieved better performance with more freedom allowed.
[16] further deﬁned a Markov random ﬁeld to capture
the relationships between diﬀerent topic sets in every
modality. Besides traditional topic models, [35] adopted
deep Boltzmann machines to learn a generative model,
and [27] proposed an eﬀective nonparametric Bayesian
framework based on the Indian Buﬀet Process (IBP) for
integrating multimodal data in a latent space. Besides, some
recent work [1, 19] exploited deep models for cross-modal
matching. And [38] further investigated the impact of deep
features on cross-modal retrieval.
In contrast, subspace based cross-modal methods seem
attractive due to its eﬃciency. They aimed to learn a
latent common subspace to make all modalities of data
to be close to each other. In this literature, Canonical
Correlation Analysis (CCA) [12] is the most popular one to
obtain such a common space. CCA tries to ﬁnd two linear
projections to maximally preserve the mutual correlations
among multimodal data. Owing to its simplicity and
eﬀectiveness, CCA has been widely applied to the crossmodal retrieval [7], face recognition [21] and word embedding
[8, 9]. Another popular method is the Partial Least
Squares (PLS) [31] that learns orthogonal score vectors
by maximizing the covariance between diﬀerent multimodal
data. Although both CCA and PLS are able to solve crossmodal subspace matching, the valuable label information

Figure 1: Overview of the proposed SCSM for unsupervised cross-modal matching. Handling hard pair-wised constraints and soft
semantic group constraints (labels are unknown) simultaneously, SCSM is formulated as a non-convex joint feature learning and data
grouping problem. Self-paced learning is further introduced to reﬁne the grouping result so that we can train a model from ‘easy’ pairs
to ‘complex’ pairs. Moreover, a novel multimodal similarity graph is leveraged to preserve the local intra- and inter- neighborhoods in
the common subspace RC . (Best viewed in colors).

has not been fully utilized to reduce the semantic gap
between text and image modalities.
Since label information potentially reduces the semantic
gap between the low-level image features and high-level
document descriptions, supervised methods have drawn
considerable attentions in cross-modal subspace learning.
A generalized multi-view framework [33] was presented to
learn a discriminative common space. Built on CCA, [11]
proposed to incorporate the third view (label information)
to capture the high-level semantics. To select relevant
and discriminative features simultaneously, [37] developed a
coupled linear regression framework. Moreover, the authors
of [13] seek the common structure hidden in heterogeneous
modalities via the pairwise constraint. Hashing methods
have been also used successfully in multimodal problem
[24, 39] and cross-modal problem [40, 43, 41]. While [40]
utilized discriminative coupled dictionaries to learn better
hash functions, [43] exploited matrix factorization to learn
the hash functions. Despite the improvement brought by
these supervised cross-modal methods, they will suﬀer from
unlabeled data exceedingly.
To address these above problems, we propose a self-paced
joint learning framework (as shown in Figure 1) for the
unsupervised cross-modal matching problem by considering
feature learning and data grouping simultaneously, which
result in a non-convex objective.
First, we assume
that multimodal data are pair-wised and from several
semantic groups, which form hard pair-wised constraints
and soft semantic group constraints respectively.
For
these hard pair-wised constraints, two linear projections
are learned to map diﬀerent modalities into a common
space respectively. For the soft semantic group constraints,
self-paced learning strategy [20, 42] is applied to reﬁne
the grouping results so that we can train a model from
‘easy’ pairs to ‘complex’ pairs. Second, the projection
regularization is employed for feature learning. A graphbased regularization term preserves the relationship of both
inter- and intra- modality similarities. Finally, we present
an alternating algorithm to cope with this optimization

570

problem. Extensive experimental results on four benchmark
databases demonstrate that our approach is highly eﬀective.
The contributions of this paper are summarized as follows:
1) We propose a general framework for unsupervised
cross-modal subspace learning that can handle hard pairwised constraints and soft semantic group constraints
simultaneously. A joint feature learning and data grouping
formulation is accordingly developed and results in a nonconvex problem.
2) Self-paced learning is used to alleviate the inaccurate
estimation of soft semantic group so that we can gradually
include sample pair sequences from ‘easy’ to ‘complex’.
Multimodal graph, preserving the inter-modality and intramodality similarity, is then utilized to better explore
unsupervised multimodal data.
3) An alternating minimization method is put forward to
eﬃciently minimize the proposed non-convex optimization
problem. Experimental results validate that our method
outperforms state-of-the-art unsupervised subspace learning
methods or even better than some supervised ones.
The remainder of the paper is organized as follows. In
Section 2, we will illustrate the details of our SCSM method.
Experimental results are presented in Section 3. Finally, the
conclusion is summarized in Section 4.

2.

SELF-PACED CROSS-MODAL SUBSPACE
MATCHING

In this section, we will present our SCSM in details and
use two modalities for example in this paper. Note that
SCSM can be easily extended to cover the case for more
than two modalities.

2.1

Notation and Problem Deﬁnition

We start with a brief introduction to some notations. For
a matrix M ∈ Rn×m , its i-th row, j-th column are denoted
by mi , mj respectively, and Mi,j lies in the i-th row and j-th
column. The
norm of the matrix M is deﬁned as
Frobenius
n
2
||M||F =
i=1 ||mi ||2 , and
 the trace of the square matrix
M is deﬁned as T r(M ) = i Mi,i .

Assume that we have two sets of features from diﬀerent
modalities (e.g., image and text), Xa = [xa1 , xa2 , . . . , xan ] ∈
Rd1 ×n , Xb = [xb1 , xb2 , . . . , xbn ] ∈ Rd2 ×n , where di is the
dimensionality of the i-th modality, n is the amount of
training image-text pairs. And each pair {xai , xbi } has the
same underlying content and belongs to the same class, i.e.,
the hard pair-wised constraint. However, the concrete label
of each pair is unknown here.
The goal of cross-modal matching is to obtain two
subspaces Sp = UTp Xp ∈ Rc×n , p ∈ {a, b} with the same
dimensionality, where Up ∈ Rdp ×c , p ∈ {a, b} denote two
projection matrices for these two modalities Xp , p ∈ {a, b},
respectively. Cross-modal matching tasks usually include:
1) using texts to match the related images, and 2) using
images to match the related texts.
Here, we mainly focus on the case that one retrieved result
is a good matching only when it shares the same semantic
label with the given user query.

2.2

Self-Paced Learning Revisit

Curriculum learning [4] and self-paced learning [20] have
been attracting increasing attention in machine learning,
computer vision and multimedia analysis ﬁelds.
The
philosophy under these concepts is to simulate the learning
process of humans/animals, i.e., humans/animals generally
start with learning easier aspects of a learning task, and then
gradually take more complex examples into consideration
[25]. Instead of using the aforementioned heuristic strategies
in curriculum learning, self-paced learning automatically
includes training samples from ‘easy’ to ‘complex’ in a purely
self-paced way.
Given a training dataset D = {(xi , yi )}n
i=1 , in which
{xi , yi } denotes the i-th observed sample and its label,
then let L(yi , g(xi , w)) be the loss function, w is the model
parameter inside the decision function g. Generally, the
objective of self-paced learning consists of two parts, i.e.,
a weighted loss term on all samples and a general self-paced
regularizer imposed on sample weights, is expressed as:
min
w,v∈[0,1]

E(w, v; λ) =
n

n


(vi L(yi , g(xi , w)) + f (vi , λ)), (1)

i=1

where λ is the age parameter for controlling the learning
rate, and f (v, λ) is the self-paced regularizer. A formal
deﬁnition is given in [17, 42]. This strategy, as supported by
empirical evaluation, has proved helpful in alleviating the
local optimum problem in non-convex optimization [3].

2.3

Model Formulation

In Figure 1, multimodal data are ﬁrstly pair-wised, further
supposed to come from several latent semantic groups, which
form hard pair-wised constraints and soft semantic group
constraints respectively.
Latent Discriminative Subspace Learning For the
cross-modal problem, a CCA-like objective function expressed as
min UTa Xa − UTb Xb 2F + Φ(Ua , Ub ),

Ua ,Ub

(2)

where Φ(·) is a regularizer imposed on the projection matrices, is exploited frequently to discover the optimal common
subspace. Then we consider the following objective,

UTp Xp − Y2F + Φ(Ua , Ub ),
(3)
min
Ua ,Ub ,Y

p∈{a,b}

571

as a relaxed candidate for that in Eq.(2), owing to the basic
A−B2

F
.
inequality A − C2F + B − C2F ≥
2
Since multimodal data are separated into several groups
in the latent subspace, we further strict the latent variable
Y lying in the discrete space {0, 1}c×n , revealing the
group/cluster membership, where c is the amount of latent
groups. Obviously, 1c Y = 1n , where 1c and 1n denote
the constant vectors of all one,  of dimensions c and n
respectively. Let Φ(Ua , Ub ) = β Ua 2F + Ub 2F as [2],
we can write the multimodal feature learning term as follows:

UTp Xp − Y2F + βUp 2F ,
min

Ua ,Ub ,Y

p∈{a,b}

s.t. Y ∈ {0, 1}c×n ,

c


(4)
Yi,j = 1, ∀j ∈ [1, n],

i

where each Xp is the feature representation matrix, Up is the
corresponding projection matrix, and Up 2F is a regularizer
imposed on project matrices to avoid trivial solutions.
Cluster indicator Y is a discrete variable, which easily
involves the optimization method in a local optimum.
Fortunately, the learning process from ‘easy’ to ‘complex’
proposed in self-paced learning can eﬀectively avoid the local
optimum. Thus we incorporate self-paced learning model in
Eq.(1) into Eq.(4), resulting in the following form:
min

n
 

Ua ,Ub ,v,Y



vi  i + β

p∈{a,b} i=1

||Up ||2F + f (v; k)

p∈{a,b}

s.t. Y ∈ {0, 1}

c×n

,

c


(5)

Yi,j = 1, ∀j ∈ [1, n],

i

where the i-th loss of sample is deﬁned as i = ||(UTp xip −
yi )||2F above.
Multimodal Locality Preserving The term in Eq.(4)
mainly focuses on the similarities among pair-wised heterogeneous data. Local similarity in each modality may vanish
in the process of feature learning. We then take locality
preserving into consideration, which has also proven eﬀective
in [14], the projected features in the common subspace RC
should inherit the similar local structure to it in these two
original feature spaces. Then for each projected modality,
we try to minimize the following objective function:
 i
(zp − zpj )2 Wij ,
(6)
i,j

where Zp = UTp Xp are the projected feature matrices, and
Wij is the intra-similarity between the i-th and the j-th
sample. We adopt the Gaussian kernel function d(xip , xjp ) =
e

j 2
−||xi
p −xp ||
2σ 2


Wijp =

to measure the local similarity:
if xip ∈ Nr (xjp ) or xjp ∈ Nr (xip ),
otherwise,

d(xip , xjp )
0

(7)

where Nr (·) is the set of r-nearest samples. The total intrasimilarity preserving term in Eq.(6) for each modality can
be formulated as:

T r(Zp Lp ZTp ),
(8)
Lintra =
p∈{a,b}

matrix and Dp is a
where Lp = D − W is the Laplacian

p
p
diagonal matrix, where Di,i = j Wi,j .
p

p

Moreover, a novel similarity preserving term is introduced
to maximize the similarity between data from one identical
latent cluster. Given the cluster indicator Y instead of the
missing semantic labels in [36], W ab = W ba = Y T Y can be
seen as the inter-similarity matrix. Apparently, only paired
data from the same cluster are treated as similar pair, the
similarity of coupled data from diﬀerent clusters equals to 0,
otherwise. We can deﬁne a inter-similarity preserving term
akin to the intra-similarity term above, expressed as:
Linter = T r(Za Lab ZTb ).



W ab
.
b
γW

γW a
W ba

(10)

Combining these two locality-preserving terms Lintra and
Linter together, we further obtain the following term:
L(Ua , Ub ) = Linter + γLintra

=
T r(UTp Xp Lpq XTq Uq ),

where Lpq is the corresponding block of the Laplacian matrix
L, and γ is a trade-oﬀ parameter to balance the intra- and
inter- similarity preserving terms.
Associating discriminative subspace learning with local
similarity preserving together, we obtain the overall objective function for SCSM:
n
 
vi i + αL(Ua , Ub )
min
p∈{a,b} i=1



+β

||Up ||2F

c


+ f (v; k)

Yi,j = 1, ∀j ∈ [1, n].

Optimization Algorithm

In this subsection, a fast iterative optimization algorithm
is developed to ﬁnd the optimal solution of Eq.(12).
Regarding the self-paced regualrizer f (·),
we adopt the same
strategy utilized in [20]: f (v; k) = − k1 i vi , where v ∈ Rn
denotes the weights imposed on the loss term, which is a
binary vector. When age parameter k is given in each
iteration, the current indicator variable vi can be deﬁned
for each sample xi as:

1 if i ≤ k1 ,
(13)
vi =
0 if i > k1 .
Besides, we rewrite Eq.(12) into the following form:

min
||(UTp Xp − Y)diag(v)||2F
U{a,b},v,Y

+α



p∈{a,b}

T r(UTp Xp Lpq XTq Uq )

p∈{a,b}

+β



1
vi
k i

||Up ||2F −

p∈{a,b}

s.t. Y ∈ {0, 1}c×n ,

c


q

where V is a diagonal matrix whose entries correspond to
the elements in v. Diﬀerentiating the objective function in
Eq.(15) with respect to Up and setting it to zero, we have
the following equation:
(Xp VVT XTp + αXp Lpp XTp + βI)Up
= Xp VVT Y T − αXp Lpq XTq Uq

(16)

Then, the optimal solution of Eq.(16) can be computed via
solving the above linear system problem.
2) Solve Y when Ua , Ub , v are ﬁxed. Optimizing the
objective function in Eq.(14) is equal to optimizing the
following problem:

||(UTp Xp − Y)V||2F + 2αT r(UTa Xa Y T YXTb Ub )
min
Y

p∈{a,b}

s.t. Y ∈ {0, 1}c×n ,

c


Yi,j = 1, ∀j ∈ [1, n].
(17)

(12)

i

2.4

(15)

i

p∈{a,b}

s.t. Y ∈ {0, 1}c×n ,

p

(11)

p,q∈{a,b}

U{a,b},v,Y

min ||(UTp Xp − Y)V||2F
Up

+α
T r(UTp Xp Lpq XTq Uq ) + β||Up ||2F ,

(9)

Then we introduce a new joint similarity matrix as below:
W=

problem. We employ an alternating minimization algorithm
for Eq.(14), the involved parameters include Ua , Ub , v and
Y.
1) Solve Ua , Ub when Y, v are ﬁxed. Optimizing the
objective function(14) is equal to minimizing the following
problem:

(14)

It is challenging to directly minimize the above objective
function w.r.t Y due to the discrete constraints, resulting
in a NP hard problem. Inspired by [34], we can optimize
Y column by column, i.e., optimize one column of Y with
all the other columns ﬁxed. So we can iteratively learn one
column at one time. Then Eq.(17) is equivalent to:
min T r(VT Y T YV) + αT r(EY T YFT )
Y

− T r(GY T ) − T r(HYT )
c

Yi,j = 1, ∀j ∈ [1, n],
s.t. Y ∈ {0, 1}c×n ,

(18)

i

where E = UTa Xa , F = UTb Xb , G = UTa Xa VVT , H =
UTb Xb VVT .
The objective in Eq.(18) can be further attributed to
minimizing two following general cases, i.e., T r(AY T YBT )
and T r(CY T ). Let y be the i-th column of Y, i = 1, · · · , n,
 the matrix Y of excluding y where y is one of all n
and Y
 the
samples. Similarly, denote by a the i-th column of A, A
 the
matrix of A excluding a, b the i-th column of B, and B

matrix of B excluding b and c the i-th column of C and C
the matrix of C excluding c. Then we have the following
form:
Y
 T )(ybT + Y
B
 T ))
T r(AY T YBT ) = T r((ayT + A

Yi,j = 1, ∀j ∈ [1, n].

B
 T a + yT Y
A
Tb
= const + T r(ayT ybT ) + yT Y

i

Since the matrix Y ∈ {0, 1}c×n is limited to discrete
values, i.e., a non-convex set, Eq.(14) is a non-convex

572

B
 T a + yT Y
A
 T b.
= const + yT Y
(19)

Here T r(ayT ybT ) = T r(abT ) = const. Similarly, we have:
 Y ) = const + y c.
T r(CY ) = T r(cy ) + T r(C
T

T

T

T

(20)

According to Eq.(19) and Eq.(20), we can formulate
Eq.(18) as follows:
V
 T v + αY
F
 T e + αY
E
 T f − g − h)
min yT (2Y
y

s.t. y ∈ {0, 1}c×1 ,

c


(21)
yi = 1,

i

where e, f , g, h are the i-th column of E,F,G,H and
 F,
 G,
 H
 are the matrices of E, F, G, H excluding e, f , g, h
E,
respectively.
Thus, this subproblem can be easily solved as:

1 i = h(m)
(22)
yi =
0 otherwise,
V
 T v + αY
F
 T e + αY
E
 T f − g − h, and h(m)
where m = 2Y
returns the index of the minimum value of m. After 2 ∼ 3
inner iterations, we can obtain a optimal complete Y.
3) Solve v when Ua , Ub , Y are ﬁxed. The loss of each
sample can be directly computed, thus we can obtain v by
Eq.(13).
Algorithm 1 Self-Paced Cross-Modal Subspace Matching
(SCSM)
Input: The matrices of unlabeled data Xp ∈ Rdp ×n , p ∈
{a, b};
Output: The projection matrices Up ∈ Rdp ×c , p ∈ {a, b};
1: Initialize Y by K-means clustering, and compute the
Laplacian matrix of the multimodal graph L, and
initialize t = 0, Up , p ∈ {a, b} as identity matrix;
2: repeat
3:
Compute vt according to Eq.(13);
4:
Compute Utp , p ∈ {a, b} by solving the linear system
problems in Eq.(16);
5:
Compute Y t according to Eq.(21) and Eq.(22);
6:
Compute Wt according to Eq.(10);
7:
t = t + 1;
8: until Convergence;
Algorithm 1 summarizes the alternate minimization
procedure to optimize Eq.(14). Firstly, the label information
Y is obtained by K-means clustering on the text modality
and the multimodal Laplacian matrix is constructed in Step
1; Then, Step 3 ∼ 6 is an alternate procedure of iteratively
updating each of U{a, b}, v, Y one by one in a loop.

2.5

Computational Complexity Analysis

Here we try to discuss the time complexity of the proposed
SCSM. For each outer iteration, we need O(nd21 + nd22 ) to
solve Ua and Ub , O(nc2 ) to solve Y, and O(ncd1 +ncd2 ) for
solving v. Thus, the overall time complexity is O(nd2 + n2 c)
for SCSM, where c is the dimension of the common subspace
and d is the larger original dimensionality for each modality.

3.

EXPERIMENT

In this section, we comprehensively compare SCSM and
other existing cross-modal approaches on four benchmark
datasets, the Pascal VOC [15, 10], the Wikipedia [29], and

573

the LabelMe [32, 26] dataset, which are widely adopted for
cross-modal matching tasks.

3.1

Evaluation Criteria & Baseline Methods

There are several widely used evaluation criteria for crossmodal subspace matching algorithms [29, 33, 37]. During
the testing phase, multimodal data are mapped into a
common subspace via the learned projection matrices for
each modality. Then in the subspace, we take one type of
modality as a query set to match another type of modality.
Like [29, 37, 18], the cosine distance is utilized to measure
the similarity of projected features. The simple goal is that
given a query from one modality, we can return the top k
closest matches in another modality via subspace learning.
The mean average precision (MAP) metric is a classical
performance evaluation criterion in the information retrieval
circle. The higher MAP indicates the better performance.
Besides the MAP, we also exploit the precision-scope curve
[30] and precision-recall curve [29] to intensively evaluate
the eﬀectiveness of diﬀerent methods, where the scope K
is speciﬁed by the number of the top-ranked texts/images
presented to users.
With regards to baseline methods, CCA [12], PLS [31] and
GMBLM [33] are three unsupervised methods, which merely
use the hard pair-wised information to learn a common
latent subspace. In contrast, supervised methods such as
CDFE [23], GMLDA [33], GMMFA [33], LCFS [37] and
recent JFSSL [36] all take the semantic class information
into account, and they are also compared here. Despite the
improved performance, semantic labels are usually expensive
and time-consuming to obtain for supervised methods. To
further study the eﬀect of initial clustering result, we further
assume the clustering result as pseudo semantic labels,
and attain the matching performance of above supervised
methods, referred as UCDFE, UGMLDA and UGMMFA.

3.2
3.2.1

Performance on Cross-Modal Matching
Results on the Pascal VOC dataset

The Pascal VOC dataset [15] is a commonly used dataset,
it consists of 9,963 image-text pairs from 20 categories,
including 5,011 training and 4,952 testing image-text pairs.
Since some of the images have multiple labels, we select
the images with only one label [33, 37]. As a result, we
obtain 2808 training and 2841 testing data samples that
correspond to 20 categories. The image and text features
are 512-dimensional Gist [26] features and 399-dimensional
word frequency features, respectively.
We ﬁrst use the Principal Component Analysis (PCA) to
remove the redundancy in features. 95% information energy
is preserved by the PCA before evaluation. The MAP scores
of the cross-modal retrieval results are shown in Table 1.
From Table 1, we can observe that the proposed SCSM
achieves the best performance on both the image-to-text
and text-to-image matching tasks. Obviously, supervised
methods (i.e., CDFE, GMLDA and GMMFA) outperform
their unsupervised versions by 10%, and earn bigger
advantages over CCA and PLS. This is because supervised
methods rely on semantic labels to reduce the semantic
gap of diﬀerent modalities, but unsupervised methods only
use pair-wised information. However, our unsupervised
method not only surpasses the unsupervised methods,

Dataset & Methods

CCA
PLS
GMBLM
UCDFE
UGMMFA
UGMLDA
CDFE
GMMFA
GMLDA
LCFS [36]
JFSSL [36]
SCSM

Pascal VOC
Image
Text
Avg
0.250
0.212
0.231
0.256
0.241
0.249
0.312
0.232
0.272
0.279
0.209
0.244
0.298
0.232
0.265
0.301
0.239
0.270
0.306
0.227
0.267
0.327
0.259
0.293
0.324
0.260
0.292
0.344
0.267
0.306
0.361 0.280 0.320
0.375 0.282 0.329

Image
0.251
0.262
0.255
0.224
0.269
0.272
0.260
0.273
0.273
0.280
0.306
0.274

Wiki
Text
0.199
0.174
0.204
0.184
0.211
0.215
0.209
0.219
0.218
0.214
0.228
0.217

Avg
0.225
0.218
0.229
0.204
0.240
0.244
0.234
0.246
0.246
0.247
0.267
0.245

Image
0.347
0.304
0.347
0.333
0.340
0.340
0.397
0.409
0.409
0.413
0.428
0.423

Wiki++
Text
Avg
0.310
0.329
0.329
0.317
0.318
0.333
0.301
0.317
0.310
0.325
0.318
0.325
0.344
0.370
0.362
0.386
0.362
0.386
0.384
0.404
0.396 0.412
0.381 0.402

Image
0.268
0.522
0.515
0.570
0.512
0.542
0.685
0.719
0.716
0.641

LabelMe
Text
Avg
0.236
0.252
0.435
0.478
0.466
0.490
0.594
0.582
0.499
0.505
0.536
0.539
0.725
0.705
0.724 0.722
0.720
0.718
0.672 0.656

Table 1: MAP scores of unsupervised SCSM and other methods on the Pascal VOC, Wiki, Wiki++ and LabelMe datasets, while CDFE,
GMMFA, GMLDA, LCFS and JFSSL are supervised methods.

but outperforms several supervised methods, achieving the
state-of-the-art performance.
Figure 2 shows the corresponding precision-recall curves
and precision-scope curves. The scope, i.e., the top K
retrieved items, varies from K = 50 to 1000. It can be
observed from Figure 2 that SCSM consistently outperforms
other unsupervised methods for both Image query vs. Text
database and Text query vs. Image database, regardless
of precision-scope curves and precision-recall curves. Note
that GMBLM only performs inferior to SCSM, due to its
consideration of intra-similarity.
Class

aeroplane

boat

Tags
queries

True
Image

Matching Images

aeroplane
grass door
window
wheel
boat
rigging
water sky
wave

bird

bird tree
sky

train

train rope
railroad
pole tree

3.2.3

Table 2: Retrieval examples by user tags queries on the Pascal
VOC database by the proposed SCSM. For each tags query
(second column), the top several retrieved images are shown in the
fourth column. The ﬁrst column represents the ground truth class
of each corresponding tags query, and the third column shows the
ground truth Image with the queries. (Best viewed in colors.)

3.2.2

image and text features. The MAP scores obtained by
SCSM and other approaches are shown in Table 1.
From Table 1, we can see that SCSM achieves similar
results to GMLDA and GMMFA, and is just inferior to
JFSSL. However, it continues to be the best algorithm for
unsupervised cross-modal subspace learning. The reason
for these results is that the dimension of the features in
this database is generally low so that the feature learning
hardly takes eﬀect. In the next subsection, we will show
our improved performances with the help of better feature
representations.
The corresponding precision-recall curves (a) and precisionscope curves (b) are also plotted for both forms of crossmodal retrieval tasks in Figure 3. We observe that, for
an image query, SCSM obtains similar and even inferior
performance, while for the text query, it performs the best
among these unsupervised algorithms.

Results on the Wiki dataset

The Wiki image-text dataset [29] generated from Wikipedia
articles, is a fundamental dataset for cross-modal matching.
It is composed of 2,866 image-text pairs from 10 semantic
classes, with image features being 128-dimensional SIFT
feature and the text feature being 10-dimensional Latent
Dirichlet allocation (LDA) feature. We split it into a
training set of 1,300 pairs and a testing set of 1,566 pairs in
the experiment [37]. We directly carry out the experiment
with the provided datasets owing to the low dimensional

574

Results on the Wiki++ dataset

The Wiki++ dataset [36] is built on the famous Wiki
dataset [29]. They share all but feature representations.
4,096-dimensional Convolutional Neural Network (CNN)
features are extracted for images by Caﬀe1 , and 5,000dimensional BOW feature vectors are learned based on the
basic tf-idf features. We also split the entire dataset into
a 1,300 training set and a 1,566 testing set. Speciﬁcally,
PCA is also performed on the features, and 95% information
energy preserved for both views.
From Table 1, it is obvious that the proposed SCSM
algorithm achieves the best performance with average MAP
40.2% among unsupervised methods. Even though SCSM
performs a little lower than JFSSL, it still can beat other
supervised methods such as GMLDA and LCFS. Compared
with performances obtained in the Wiki dataset, all methods
achieve better results when using CNN visual features in
spite of the image query or the text query. This is because
the CNN features have proven eﬀective for image feature
representation. In the high and sparse dimensional feature
setting, SCSM shrinks the distance with the best-performing
JFSSL from 2.2% to 1%, oﬀers the great potentials when
better features are provided.
SCSM obtains the best performance in terms of precision1

http://caﬀe.berkeleyvision.org/.

PLS
CCAVOC
Image−to−Text

GMBLM

UCDFE

UGMLDA

0.45

0.8

0.4

0.7

0.35

0.6

0.3

0.5

UGMMFA

SCSM

PLS Image−to−Text
CCA VOC GMBLM

UCDFE

0.4

UGMLDA

UGMMFA

SCSM

0.45
0.4

0.35

0.35

0.4

0.2

0.3

0.15

0.2

Precision

Precision

Precision

Precision

0.3

0.25

0.25

0.2
0.2
0.15

0.1
0.05

0.15

0.1

0

0.2

0.4

0.6

0.8

0

1

0.3
0.25

0

0.2

0.4

Recall

0.6

0.8

0.1

1

0

200

400

600

800

0.1

1000

0

200

400

Scope

Recall

(a) precision-recall curve

600

800

1000

Scope

(b) precision-scope curve

Figure 2: Performance of various unsupervised methods on the Pascal VOC dataset, based on precision-recall curve(a) for K = 50 to
1000 and precision-scope curve(b). The performances for image query are shown in the left sub-ﬁgures.
PLS
CCA
Image−to−Text
wiki

GMBLM

UCDFE

UGMLDA

0.4

0.7

0.35

0.6

0.3

0.5

UGMMFA

SCSM

PLSImage−to−Text
CCA wiki

GMBLM

UCDFE

0.26

UGMLDA

UGMMFA

SCSM

0.35

0.24
0.3

0.4

0.2

0.3

0.15

0.2

0.1

0

0.2

0.4

0.6

0.8

0.1

1

0.2

Precision

0.25

Precision

Precision

Precision

0.22

0.18

0.25

0.2

0.16
0.15
0.14

0

0.2

0.4

Recall

0.6

0.8

0.12

1

0

200

400

600

800

0.1

1000

0

200

400

Scope

Recall

(a) precision-recall curve

600

800

1000

Scope

(b) precision-scope curve

Figure 3: Performance of various unsupervised methods on the Wiki dataset, based on precision-recall curve(a) for K = 50 to 1000 and
precision-scope curve(b). The performances for image query are shown in the left sub-ﬁgures.
Image−to−Text
PLS
CCAwiki4

GMBLM

UCDFE

0.6

UGMLDA

UGMMFA

Image−to−Text
PLS
CCAwiki4

SCSM

0.8

0.55

GMBLM

UCDFE

0.45

0.7

0.4

0.6

0.35

0.5

0.3

SCSM

0.45

0.4

Precision

Precision

0.3
0.25

Precision

0.4

0.4
0.35

0.25

0.35
0.3
0.25

0.3

0.2

0.2

0.15

0.2

0.2
0.15
0.1

UGMMFA

0.5

0.5
0.45

Precision

UGMLDA
0.55

0

0.2

0.4

0.6

0.8

0.1

1

0

0.2

0.4

Recall

0.6

0.8

0.1

1

Recall

0.15
0

200

400

600

800

0.1

1000

0

200

400

Scope

(a) precision-recall curve

600

800

1000

Scope

(b) Precision-scope curve

Figure 4: Performance of various unsupervised methods on the Wiki++ dataset, based on precision-recall curve(a) for K = 50 to 1000
and precision-scope curve(b). The performances for image query are shown in the left sub-ﬁgures.
Image−to−Text
PLS
CCAlabelme2 GMBLM

UCDFE

UGMLDA

0.9

1

0.8

0.9

Image−to−Text
labelme2
PLS
CCA

SCSM

GMBLM

UCDFE

UGMLDA

0.8

0.8

0.7

0.7

0.6

0.6

0.5

0.5

UGMMFA

SCSM

Precision

0.5
0.4

Precision

0.7

0.6

0.6
0.5

Precision

0.8

0.7

Precision

UGMMFA

0.4

0.4

0.4

0.3

0.3

0.3

0.2

0.2

0.3

0.2

0.2

0.1

0.1

0

0.2

0.4

0.6

0.8

1

0

0.2

0.4

Recall

0.6

0.8

0.1

1

Recall

0

200

400

600

800

1000

0.1

0

200

400

Scope

(a) precision-recall curve

600

800

1000

Scope

(b) precision-scope curve

Figure 5: Performance of various unsupervised methods on the LabelMe dataset, based on precision-recall curve(a) for K = 50 to 1000
and precision-scope curve(b). The performances for image query are shown in the left sub-ﬁgures.

recall curves for both image query and text query, as shown
in Figure 4(a). From the precision-scope curves in Figure
4(b), SCSM discovers more good matches in the top K
documents than its several counterparts.
To further study the diﬀerence between SCSM and other

575

methods (here we ignore the unsupervised versions of
supervised methods due to the space limit) in Figure 6.
From the ﬁgure, we can ﬁnd that SCSM takes the ﬁrst
space for 5 classes, all these classes are easily distinguished.
For the rest 5 classes, SCSM is a bit lower than supervised

methods but higher than CCA, PLS and GMBLM. It can be
concluded that SCSM can achieve a comprehensively better
performance among unsupervised methods.
Methods

Matching Images

SCSM
GMLDA

for unsupervised methods to do feature learning without
semantic labels. However, the advanced performances from
the Wiki to the Wiki++ database indicate that SCSM
can probably approach the supervised methods with deep
features for the LabelMe database.
Regarding the precision-recall and precision-scope curves,
SCSM still obtains the best performances in both image
query and text query tasks. The gap between SCSM and
second-best performing UCDFE is apparent, thus we can
conclude that SCSM is distinguished, even other algorithms
share the same initial pseudo semantic labels.

CCA
GMBLM
Table 3: Retrieval examples by text query “In 1994, Angus and
Malcolm invited Rudd to several jam sessions ...” on the Wiki
database by the proposed method. Red border indicates an
incorrect matching result. (Best viewed in colors.)

Class

Tags
queries

coast

sky sea
water sand
beach

insidecity

mountain

0.7

street

PLS
CCA
GMBLM
CDFE
GMLDA
GMMFA
SCSM

0.6

True
Image

Matching Images

building
staircase
window
ﬂag
balcony
clouds
mountain
land river
water
island
tree
sidewalk
cars side
road
streetlight

Table 4: Retrieval examples by tags queries on the LabelMe
database by the proposed method. For each tags query (second
column), the top several retrieved images are shown in the fourth
column. The ﬁrst column represents the ground truth class of
each corresponding tags query, and the third column shows the
ground truth Image with the queries. (Best viewed in colors.)

0.5

MAP

0.4

0.3

0.2

0.1

0

art

biology

geography

history

literature
media
Category

music

royalty

sport

Image
queries

warfare

“aeroplane tree grass insect
navy”, “aeroplane leaves tail
building sky light”, “sky
aeroplane cable”, “aeroplane
sky branch”

Figure 6: MAP of diﬀerent methods on the Wiki++ dataset with
respect to each category. (Best viewed in colors.)

3.2.4

“car tree road sky stone
grass”, “pole wire sign
trafficlight road fence”, “car
ground road hill road”, “car
road sky grass tree”

Results on the LabelMe dataset

The LabelMe Outdoor dataset [32, 18] consists of 2,688
fully annotated outdoor images from 8 categories, i.e.,
“coast”, “forest”, “highway”, “inside city”, “mountain”, “open
country” and “tall building”.
For the text modality,
we generate the object account vector via the LabelMe2
toolbox.
We randomly select 200 samples from each
category for training, resulting in 1,600 image-text pairs for
the training set and the remaining 1,0863 image-text pairs
for the testing set. There are total 789 unique words with
frequencies varying from one time to more than 2000 times.
We select the word frequencies that are more than one time.
As a result, the image and text features are 512-dimensional
Gist features and 470-dimensional word frequency features,
respectively.
From Table 1, SCSM obtains the best average score
65.6% among the unsupervised methods, however, it
performs worse than some supervised methods. It may be
because the word frequency features contain massive noises,
such as misspelling and duplicated words, make it hard
2
http://labelme.csail.mit.edu/Release3.0/browserTools/php/mat
labtool box.php.
3
Two instances are dropped due to its missing tags (refrred as
text).

576

SCSM

“foot leg clothes had
tabletop”, “rock water
waterfall person ground”,
“person arm hand foot
clothes”, “person clothes
water shoe glasses wall”

GMLDA
“sky aeroplane cable”,
“aeroplane letter wing moon
star door tail sky”,
“bicyclehandle wheel clothes
pedal leaves ground
stairstep”, “aeroplane
radiotower cart cone”
“house tree sky grass curb
street car tag”, “car street
grass tree house sky”, “car
decoration grill carlight”,
“bus debris trash house
house”
“sign building trash wall
person”, “person ribbon
clothes slide fence”, “clothes
ground junk”, “frame clothes
watch bracelet person”

Table 5: Retrieval examples by image queries on the Pascal
VOC database. For each image query (second column), the
top several retrieved tags are shown in the second column
for the proposed unsupervised SCSM and third column for
supervised GMLDA.

3.3

Convergence & Parameter Analysis

It is obvious to prove that the employed alternative
minimization strategy can converge to a local optimum.
However, under the self-paced framework, our learning algorithm is hard to guarantee the global convergence. On the
other hand, the experimental results in 
Figure 8 on all four
datasets can demonstrate the objective, p∈{a,b} ||(UTp Xp −
Y)diag(v)||2F + αT r(UTp Xp Lpq XTq Uq ), decreases as the
iteration number increases.
With regards to the parameters α and β in SCSM, we
conduct an experiment to study the inﬂuence of diﬀerent

Image
queries

SCSM

GMLDA

“water sea building sky sand
beach mountain rainbow”,
“sand beach water sea sky”,
“sky mountain ocean water
water sea sand beach trees”,
“water sea sky waves”

“sky mountain field river
water trees”, “sea water sky
sand beach building tree”,
“sky buildings sea water”,
“sky buildings sea water”

“sky field road tree shrub”,
“sky desert ground trees
hill”, “sky snowy mountain
hill buildings”, “sky
mountain field grass cow”

“sky path shrub trees stone”,
“sky mountain field river
water trees”, “sky mountain
field desert shrub plant”,
“sky desert ground trees hill”

“sky skyscraper building”,
“building trees sidewalk road
bus”, “sky buildings
skyscraper”, “sky skyscraper
building”

“sky building wall step
person walking”, “sky
building car box plants”,
“sky skyscraper buildings
car side sidewalk”, “city
river water building
skyscraper dock”

0.4

0.4

0.3

0.3

MAP

MAP

Table 6: Retrieval examples by image queries on the LabelMe
database. For each image query (second column), the top
several retrieved tags are shown in the second column for
the proposed unsupervised SCSM and third column for
supervised GMLDA.

0.2
0.1

0.2
0.1

0

0

1e−3
1e−2
1e−1

1

β

10
100

1e−3
1e−2
1e−1

1e−1
1e−2
1e−3
1e−4
1e−5
α
1e−6

1

β

10
100

1
1e−1
1e−2
α
1e−3

10

datasets. As an unsupervised method, the reasons for the
better performance of our SCSM may lie in twofold.
First, images often contain several semantic concepts,
potentially belonging to several semantic groups, their
labels may be inaccurate for some challenging datasets.
Hence the grouping results may be a complementary for
the missing label information when feature representations
are enough powerful. Second, the pseudo group label
obtained by canonical clustering methods is inaccurate.
Since clustering is a non-convex problem, we introduce
self-paced learning to avoid the local minima and reﬁne
the grouping results. Besides, the projection matrices are
computed by two regression-like analyses on the pseudo
label, and the multimodal graph preserving the inter- and
intra- similarities is also constructed.
Moreover, we observe that all methods, especially SCSM,
perform better in the Wiki++ dataset. Because the deep
features in the Wiki++ dataset are high-dimensional and
discriminative, SCSM can more eﬀectively learn features,
and estimate the latent semantic group more accurately.
However, our method in the Wiki and LabelMe dataset is
inferior to the supervised state-of-the-art method proposed
by [36], much eﬀort still needs to be taken to improve the
performance when the features contain massive noises.

100
1.1

× 104

4

7

(b) Wiki

0.8

0.4
0.2

0.8
0.7
0.6

0.4

0.6

MAP

4
3
2
1

0

2

4
6
Iteration Number

8

0

10

0

2

(a) Pascal VOC

0.2

0

5

4
6
Iteration Number

8

10

8

10

0.4

(b) Wiki

0

β

1

10
100

1e−1
1e−2
1e−3
1e−4
1e−5
α
1e−6

(c) Wiki++

1e−3
1e−2
1e−1

β

3.0

1

10
100

1e−4
1e−5
1e−6
1e−7
1e−8
1e−9
α

× 103

3.4

× 103

3.2

2.8

3.0

(d) LabelMe

2.6

Objective Function value

1e−3
1e−2
1e−1

Objective Function value

MAP

0.6

0.9

0.5

0.8

x 10

6
Objective Function value

(a) Pascal VOC

Objective Function value

1

2.4
2.2
2.0
1.8

2.8
2.6
2.4
2.2
2.0
1.8

1.6

Figure 7: Performance variation for the average MAP with
respect to α and β with γ ﬁxed as 1.

1.4

1.6
0

2

4
6
Iteration Number

8

(c) Wiki++
choices. Note that in our experiment, γ is set as 1 while
α varies with the training data size n and feature length
d, and β varies from [1e−3 , 1e−2 , · · · , 1e2 ]. The analysis
on parameter sensitivity shows that SCSM is very robust
to model parameters which can achieve stable and superior
performance under a wide range of parameter values.
For fair comparisons, we tune the parameter of subspace
dimensionality for subspace based cross-modal approaches,
and adopt the best-performing dimensionality for comparison. For LCFS, JFSSL and our SCSM, the subspace
dimensionality is ﬁxed as the number of semantic classes
(e.g., c = 20 for the Pascal VOC dataset). Besides, the
width parameter σ for Gaussian kernel in Eq.(7) is ﬁxed as
1 for following experiments.

3.4

Discussion

Experimental results show that, our SCSM not only
surpasses the unsupervised methods (e.g., CCA, GMBLM),
but also outperforms some supervised methods (e.g., CDFE,
GMLDA, and LCFS), in the Pascal VOC and Wiki++

577

10

1.4

0

2

4
6
Iteration Number

(d) LabelMe

Figure 8: Convergence curve of the proposed SCSM.

4.

CONCLUSION

In this paper, we have proposed a novel unsupervised
method for cross-modal subspace matching. We have
introduced hard pair-wised constraints and soft semantic
group constraints for multi-modal data, which are potentially eﬀective for unsupervised learning. A joint feature
learning and data grouping formulation has been accordingly
developed. For an accurate estimation of the semantic
group, self-paced learning has been incorporated into the
non-convex loss. Moreover, multimodal graph is included
to preserve the inter- and intra- modality similarity. To
minimize this joint learning problem, we have presented
an alternating minimization solution. Experimental results
on four multimodal databases demonstrate that the eﬀectiveness of the proposed method for unsupervised multi-

modal data. For future work, the proposed method will
be extended for multi-label cross-modal matching problem.

Acknowledgments
This work was supported by the National Basic Research
Program of China (Grant No. 2012CB316300), the Youth
Innovation Promotion Association of the Chinese Academy
of Sciences (CAS) (Grant No. 2015190) and the National
Natural Science Foundation of China (Grant No. 61473289).
Jian Liang and Zhihang Li contributed equally to this work
and should be considered co-ﬁrst authors.

5.

REFERENCES

[1] G. Andrew, R. Arora, J. Bilmes, and K. Livescu. Deep
canonical correlation analysis. In ICML, pages 1247–1255,
2013.
[2] F. Bach and Z. Harchaoui. Diﬀrac: a discriminative and
ﬂexible framework for clustering. In NIPS, pages 49–56,
2008.
[3] S. Basu and J. Christensen. Teaching classiﬁcation
boundaries to humans. In AAAI, pages 109–115, 2013.
[4] Y. Bengio, J. Louradour, R. Collobert, and J. Weston.
Curriculum learning. In ICML, pages 41–48. ACM, 2009.
[5] D. M. Blei and M. I. Jordan. Modeling annotated data. In
SIGIR, pages 127–134, 2003.
[6] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet
allocation. JMLR, 3:993–1022, 2003.
[7] J. Costa Pereira, E. Coviello, G. Doyle, N. Rasiwasia, G. R.
Lanckriet, R. Levy, and N. Vasconcelos. On the role of
correlation and abstraction in cross-modal multimedia
retrieval. IEEE TPAMI, 36(3):521–535, 2014.
[8] P. Dhillon, D. P. Foster, and L. H. Ungar. Multi-view
learning of word embeddings via cca. In NIPS, pages
199–207, 2011.
[9] P. S. Dhillon, J. Rodu, D. P. Foster, and L. H. Ungar. Two
step cca: A new spectral method for estimating vector
models of words. In ICML, pages 1551–1558, 2012.
[10] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and
A. Zisserman. The pascal visual object classes (voc)
challenge. IJCV, 88(2):303–338, 2010.
[11] Y. Gong, Q. Ke, M. Isard, and S. Lazebnik. A multi-view
embedding space for modeling internet images, tags, and
their semantics. IJCV, 106(2):210–233, 2014.
[12] D. R. Hardoon, S. Szedmak, and J. Shawe-Taylor.
Canonical correlation analysis: An overview with
application to learning methods. Neural Computation,
16(12):2639–2664, 2004.
[13] R. He, M. Zhang, L. Wang, Y. Ji, and Q. Yin. Cross-modal
subspace learning via pairwise constraints. IEEE TIP,
24(12):5543–5556, 2015.
[14] X. He and P. Niyogi. Locality preserving projections. In
NIPS, pages 153–160, 2003.
[15] S. J. Hwang and K. Grauman. Reading between the lines:
Object localization using implicit cues from image tags.
IEEE TPAMI, 34(6):1145–1158, 2012.
[16] Y. Jia, M. Salzmann, and T. Darrell. Learning
cross-modality similarity for multinomial data. In ICCV,
pages 2407–2414, 2011.
[17] L. Jiang, D. Meng, T. Mitamura, and A. G. Hauptmann.
Easy samples ﬁrst: Self-paced reranking for zero-example
multimedia search. In MM, pages 547–556, 2014.
[18] C. Kang, S. Xiang, S. Liao, C. Xu, and C. Pan. Learning
consistent feature representation for cross-modal
multimedia retrieval. IEEE TMM, 17(3):370–381, 2015.
[19] R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel,
R. Urtasun, A. Torralba, and S. Fidler. Skip-thought
vectors. In NIPS, pages 3276–3284, 2015.
[20] M. P. Kumar, B. Packer, and D. Koller. Self-paced learning
for latent variable models. In NIPS, pages 1189–1197, 2010.

578

[21] A. Li, S. Shan, X. Chen, B. Ma, S. Yan, and W. Gao.
Cross-pose face recognition by canonical correlation
analysis. arXiv preprint: 1507.08076, 2015.
[22] J. Liang, R. He, Z. Sun, and T. Tan. Group-invariant
cross-modal subspace learning. In IJCAI, 2016.
[23] D. Lin and X. Tang. Inter-modality face recognition. In
ECCV, pages 13–26. 2006.
[24] J. Masci, M. M. Bronstein, A. Bronstein, and
J. Schmidhuber. Multimodal similarity-preserving hashing.
IEEE TPAMI, 36(4):824–830, 2014.
[25] D. Meng and Q. Zhao. What objective does self-paced
learning indeed optimize? arXiv preprint:1511.06049, 2015.
[26] A. Oliva and A. Torralba. Modeling the shape of the scene:
A holistic representation of the spatial envelope. IJCV,
42(3):145–175, 2001.
[27] B. Ozdemir and L. S. Davis. A probabilistic framework for
multimodal retrieval using integrative indian buﬀet process.
In NIPS, pages 2384–2392, 2014.
[28] D. Putthividhy, H. T. Attias, and S. S. Nagarajan. Topic
regression multi-modal latent dirichlet allocation for image
annotation. In CVPR, pages 3408–3415, 2010.
[29] N. Rasiwasia, J. Costa Pereira, E. Coviello, G. Doyle, G. R.
Lanckriet, R. Levy, and N. Vasconcelos. A new approach to
cross-modal multimedia retrieval. In MM, pages 251–260,
2010.
[30] N. Rasiwasia, P. J. Moreno, and N. Vasconcelos. Bridging
the gap: Query by semantic example. IEEE TMM,
9(5):923–938, 2007.
[31] R. Rosipal and N. Krämer. Overview and recent advances
in partial least squares. In Subspace, Latent Structure and
Feature Selection, pages 34–51. Springer, 2006.
[32] B. C. Russell, A. Torralba, K. P. Murphy, and W. T.
Freeman. Labelme: a database and web-based tool for
image annotation. IJCV, 77(1-3):157–173, 2008.
[33] A. Sharma, A. Kumar, H. Daume III, and D. W. Jacobs.
Generalized multiview analysis: A discriminative latent
space. In CVPR, pages 2160–2167, 2012.
[34] F. Shen, C. Shen, W. Liu, and H. Tao Shen. Supervised
discrete hashing. In CVPR, pages 37–45, 2015.
[35] N. Srivastava and R. R. Salakhutdinov. Multimodal
learning with deep boltzmann machines. In NIPS, pages
2222–2230, 2012.
[36] K. Wang, R. He, L. Wang, W. Wang, and T. Tan. Joint
feature selection and subspace learning for cross-modal
retrieval. IEEE TPAMI, 2015.
doi:10.1109/TPAMI.2015.2505311.
[37] K. Wang, R. He, W. Wang, L. Wang, and T. Tan. Learning
coupled feature spaces for cross-modal matching. In ICCV,
pages 2088–2095, 2013.
[38] Y. Wei, Y. Zhao, C. Lu, S. Wei, L. Liu, Z. Zhu, and S. Yan.
Cross-modal retrieval with cnn visual features: A new
baseline. IEEE TCYB, 2016.
doi:10.1109/TCYB.2016.2519449.
[39] B. Wu, Q. Yang, W.-S. Zheng, Y. Wang, and J. Wang.
Quantized correlation hashing for fast cross-modal search.
In IJCAI, pages 3946–3952, 2015.
[40] Z. Yu, F. Wu, Y. Yang, Q. Tian, J. Luo, and Y. Zhuang.
Discriminative coupled dictionary hashing for fast
cross-media retrieval. In SIGIR, pages 395–404, 2014.
[41] T. Zhang and J. Wang. Collaborative quantization for
cross-modal similarity search. In CVPR, 2016.
[42] Q. Zhao, D. Meng, L. Jiang, Q. Xie, Z. Xu, and A. G.
Hauptmann. Self-paced learning for matrix factorization. In
AAAI, pages 3196–3202, 2015.
[43] J. Zhou, G. Ding, and Y. Guo. Latent semantic sparse
hashing for cross-modal similarity search. In SIGIR, pages
415–424, 2014.
[44] Y. T. Zhuang, Y. F. Wang, F. Wu, Y. Zhang, and W. M.
Lu. Supervised coupled dictionary learning with group
structures for multi-modal retrieval. In AAAI, pages
1070–1076, 2013.

