Interacting with Financial Data using Natural Language
Vassilis Plachouras1 , Charese Smiley2 , Hiroko Bretz2 , Ola Taylor1 , Jochen L. Leidner1 ,
Dezhao Song2 , Frank Schilder2
1

Thomson Reuters, Research & Development
1 Mark Square
London, EC2A 4EG, UK

2

Thomson Reuters, Research & Development
610 Opperman Drive
Eagan, MN 55123, USA

firstname.lastname@thomsonreuters.com
ABSTRACT
Financial and economic data are typically available in the
form of tables and comprise mostly of monetary amounts,
numeric and other domain-specific fields. They can be very
hard to search and they are often made available out of
context, or in forms which cannot be integrated with systems
where text is required, such as voice-enabled devices.
This work presents a novel system that enables both experts in the finance domain and non-expert users to search financial data with both keyword and natural language queries.
Our system answers the queries with an automatically generated textual description using Natural Language Generation (NLG). The answers are further enriched with derived
information, not explicitly asked in the user query, to provide the context of the answer. The system is designed to
be flexible in order to accommodate new use cases without
significant development effort, thus allowing fast integration
of new datasets.

CCS Concepts

Figure 1: Screenshot of the system.

•Computing methodologies → Natural language generation; •Information systems → Query intent;

1.

need. However, it will not present the wider picture and possibly ignore interesting wider trends or correlations. Providing a more comprehensive answer to the information seeker
will draw in further dimensions that may easily be ignored
if just a number is returned.
In this work, we introduce a system that enables financial
domain experts and non-expert users to search financial data
with natural language queries and uses Natural Language
Generation (NLG) to produce human-readable answers from
the data. The answers are automatically generated from
the retrieved data and are put in context by performing
comparisons with past data to identify trends and interesting
data points. For the query “India’s GDP 2010”, the system
retrieves the record that provides the GDP of India in 2010
and then produces the answer shown in Figure 1. For the
query “India’s GDP versus CO2 emissions 2000-2009”, the
system calculates the correlation between the two indicators
and generates the sentence “For 2000-2009, CO2 emissions
had a very strong correlation with GDP (0.97).”
The method was designed to be robust and avoid some
of the issues that grammar-based Natural Language Interface (NLI) systems face, such as developing grammars for
new domains. Furthermore, beyond just processing terse
keyword-based queries the method is capable of handling

INTRODUCTION

Financial data, such as macro-economic indicator time series for countries, information about mergers and acquisition
(M&A) deals between companies, or stock price time series,
is typically stored in relational databases, requiring domain
expertise to search and retrieve. For example, a financial
analyst who is familiar with a particular database will be
able to locate the information he or she is seeking quickly.
However, for users who lack the necessary familiarity with
the databases, it may be difficult and time-consuming to
obtain the information they are looking for.
The retrieval of specific data points (e.g., India GDP 2010)
may be useful for responding to a very specific information
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

SIGIR ’16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2911457

1121

Offline

many types of well-formed English questions like “China’s
CO2 emissions between 2009 and 2013” or “What is the GDP
of India in 2010?” without complex linguistic processing. In
addition, we have designed the system with the aim to be
flexible and easily adaptable to different use cases, ranging
from retrieving specific points from a time series of financial
data to ranking entities or computing correlations, by supplying different configurations to the system. The output of
the system has been integrated in Thomson Reuters Eikon,
a platform for financial analysis.
The automatically generated textual description of answers enables the system to be used in desktop or smaller
devices, where expressing the answer in a textual form can
provide a succinct summary of multiple diagrams and charts,
or in settings where text is required (e.g., in speech-enabled
devices, where the answer can be spoken back to the user).
To the best of our knowledge, this is the first system combining natural language search and NLG for financial data.

2.

Tabular
Data

Online

Templates

Intent to Query
Plan Mapping

User query

Answer
Generation

Search

Query
Understanding

Index

Answer
Aggregation
& Rendering

Web UI

Figure 2: System architecture.
EKSO [20] which introduces the concept of offline indexing and E-Mesh [11], which also uses offline-generation to
make answer retrieval robust in the light of linguistic variability. However, our system provides textual descriptions
of the answers combined with useful context information,
which has not been explicitly asked in user queries. Mathematica’s producer Wolfram Inc. created WolframAlpha1 ,
a commercial natural language search system over structured data, predominantly for the science and engineering
domains. The technical details of this system have not been
disclosed in any peer-reviewed publications to date. IBM
Watson Analytics is an online platform that analyzes structured datasets to answer questions. Google also retrieves
indicator values for a given country, macro-economic indicator and year. However, existing services such as WolframAlpha, IBM Watson Analytics and Google do not generate a
textual description of the answers.

RELATED WORK

Keyword-Based Search Systems for Databases.
The BANKS system [1] enables keyword search and navigation in structured and semi-structured datasets. The system
performs an in-memory graph search and ranks answers by
relevance and the a priori “prestige”, inspired by PageRank.
The FRISK system [15] uses a dynamic programming-based
method for query segmentation and presents the possible interpretations so that users can select the one that is closest
to the original intention. Discover [7] uses a greedy execution plan over a graph structure to provide efficient search
over relational data without knowledge of the schema. The
Spark system [12] assembles matching tuples from different tables together according to relationships such that the
tuples are collectively relevant. Keymantic [2] and Keyword++ [5] also belong to this family of systems.
Natural Language Interfaces to Databases (NLI).
NLI systems convert user queries to the query language used
by the underlying database. For example, both DBSemSXplorer [3] and DEANNA [21] systems translate a user query
into a SPARQL query which is then processed by a triple
store. TR Discover [19] is a general-purpose and domainadaptable NLI for querying linked data. It uses a featurebased grammar to parse a natural language question into its
first order predicate logic representation (FOL). The FOL
of a question is further transformed into a SPARQL or SQL
query to retrieve answers from a knowledge graph.
Question Answering Systems. Our system also relates
to question answering, for which several systems have been
produced, for example [8][22][10], the top-performing system
entries [13][6][18] at the final (2007) open-domain QA factoid
track of TREC, or IBM’s Watson [4].
Natural Language Generation (NLG). Natural Language Generation has a long tradition in natural language
processing [16]. More recently, statistical approaches where
the text generation is informed by statistical properties of
training corpora have been suggested [9].
Our system is different from keyword-based search systems for databases in the sense that we decouple the search
process from a specific database technology by using a search
engine instead. Compared to NLI systems, we avoid the expensive query execution on databases or knowledge graphs,
possibly involving joins or graph traversals. Perhaps the
systems that most closely resembles the present work are

3.

SYSTEM ARCHITECTURE

The architecture of the system is shown in Figure 2.
At the preprocessing stage, the Answer Generation module transforms the data into virtual documents that can
be indexed by a search engine, using parametric templates.
The virtual documents, which represent the answers that
the system returns, are indexed using Elasticsearch2 . At
query time, the Query Understanding module analyzes a
user query and identifies entities that are relevant to the
indexed data. From the identified entities, the module generates a ranked list of query intents, where each intent represents a group of entities in the query. The Search module iterates over the ranked list of intents and identifies the
top-ranked intent that can be mapped to a predefined set
of query plans. If such an intent is found, the query plan
is populated with entities from the intent and one or more
queries are sent to the underlying search engine. Next, the
results are passed to the Answer Aggregation and Rendering
module, which is responsible for generating a high quality
human-readable description of the results, providing both an
overview and the context of the result, as shown in Figure 1.

3.1

Template-based Answer Generation

The input for the Answer Generation module comprises
the data and a set of templates, which transform the data
into virtual documents that will be indexed by the search
engine. A template consists of: 1) the type of the document,
2) the source of the data, either in the form of a path on
disk or a SQL query that returns data from a database,
and a mapping of the columns to field names in the index,
1
2

1122

http://www.wolframalpha.com/
https://elastic.co

3.3

3) a document identifier, and 4) the caption, which is an
array of constant strings and data values. When the Answer
Generation module applies a template to a dataset, a virtual
document is generated for each database record, comprising
the named fields from the template, the document identifier
and the caption field.
Since the templates control the format and data of the
documents, the design of a template can be guided by the
application requirements. For example, when developing an
application about time-series of financial macro-economic indicators, where users can query for year ranges, two different
design choices may be to create a separate virtual document
per time-series point, or to aggregate k-year ranges of timeseries points and present to the users precomputed results
about trends and changes. In this work, we have precomputed the values for all 5 and 10 year intervals between 1960
and 2015 to speed up online query processing.

3.2

Search

The Search module receives as input the user query and
the ranked list of intents and returns an answer obtained
by querying the virtual document index. The first step in
this process includes the mapping of intents to query plans.
A query plan specifies the computational steps involved in
answering the query and corresponds to a composition of
elementary actions, such as searching, filtering results according to a specified condition, ranking results according
to the values of a given field, aggregating values to compute
the value of a function. Each query plan has one or more
search steps, where a query is sent to the underlying search
engine. The obtained results are compared to the user query,
to identify if there is no or a partial match with the query.
The mapping of an intent I to a query plan, similar to a
rule-based classifier, is performed by checking whether the
entities in I are compatible with the entities required for a
query plan. If no interval entity is provided, then the most
recent result is returned. There is also a query plan which
performs plain text search on the caption field of the virtual
documents in the index in case that the user query cannot
be mapped to a query plan. In this way, our system can
robustly retrieve financial data exploiting either the data
structure or their textual representation.

Query Understanding

Queries in financial search applications tend to be short
and contain abbreviations or codes. While we can directly
use the caption field of virtual documents to match against
user queries, we have built a Query Understanding module
that can extract entities relevant to the application in order
to perform structured search more effectively. The query
understanding module processes a query as follows.
Entity tagging. First, we identify entities in the query
string. The type of entities depend on the application
requirements. In the case of macro-economic indicators,
we identify countries (e.g., US or United States), regions
(e.g., South Asia), groups of countries (e.g., OECD), macroeconomic indicators and their abbreviations (e.g., Gross national income, or GNI). In the case of merger and acquisition
deals, we also identify companies, names of financial advisors such as investment banks, dates, industry sectors. We
tag entities using a regular expression tagger, a trie-based
tagger and a scalable n-gram tagger [14].
Intent generation and ranking. Next, from the set
of all identified entities, we generate sets of non-overlapping
entities, which we call intents. An intent I is a set of entities.
For each intent I, we first apply a set of regular expressions
to identify entities which have been tagged wrongly and remove them. In our setting, tagging errors occur where a token could be a 2-letter country code or not (e.g., the country
code IN or the preposition in).
We also disambiguate entities when the presence of one
entity can be used to disambiguate the tagged entities for
another sequence of tokens. For example, there is more than
one indicator for gross domestic product (GDP), which is
expressed in different units, such as “current US$”, or “constant 2005 US$”. We consider the indicator units to be a
distinct entity type and use them to disambiguate indicator
entities tagged in queries.
P
e∈I s(e)
, where s(e) is
Next, we compute a score s(I) = (1+c)|I|
a score associated with entity e adjusted by the prevalence
of each identified entity, a measure, based on web counts, of
how likely is one definition of an entity over another one with
the same surface form, |I| is the number of entities in I and
c is the number of tokens not covered by any of the entities
in I. The role of (1 + c) and |I| in the denominator is to
decrease the score of intents which consist of many entities
with poor coverage of the query. The output of the query
understanding module is a ranked list of intents.

3.4

Answer Aggregation and Rendering

The Answer Aggregation and Rendering module generates
human readable descriptions of the answers. The module
accepts the output of the Search module as input. It then
uses the intent chosen by the Query Understanding module to select from the template bank an appropriate set of
templates for text generation, and populates the templates
with the accompanying data. In this work, we cover several
use cases for macro-economic indicators, such as countryyear-indicator, country-multi-year-indicator, region-yearindicator, and group-year-indicator, as well as for M&A
deals: company-company-deal, country-sector-year-deals,
sector-year-deals, country-year-deals. Each use case relies on
a separate template bank which contains multiple rephrasings for each sentence plan. Hence, the Answer Aggregation
and Rendering module can generate variations of the same
answer by selecting different templates. Three example templates are shown below.
• in [year], [country] [indicator] stand at [amount].
• [country] finish [year] with a [indicator] of
[amount].
• [country] [indicator] end [year] at [amount].
The templates were generated semi-automatically from
the Reuters News Archive (RNA), a large news corpus containing 14 million articles. We tag named entities (e.g.,
countries, dates), augmented with custom entities (e.g., economic indicators such as GDP ) using variants derived from
the RNA. Candidate sentences from the corpus not containing all pieces of information needed for the generation are
discarded. Remaining sentences containing additional irrelevant information are then manually removed or altered.
Additional variation is achieved through verb selection at
the lexical level. We use the RNA to extract verb choice
patterns and found that verbs associated with smaller percentage changes (inch,ease) can be distinguished from those

1123

associated with larger changes (skyrocket,plummet). This
stands in contrast to previous work (cf. [17]) which found
verb choice to be largely idiosyncratic when analyzing a
much smaller data sample. In the template example below, verb of motion is selected statistically based on the
rate of change.

[3]

[4]

• [country] [indicator] for [year] averaged [amount]
[verb of motion]
[percentage change]
from
in
[year from]
to
[to value]
[from value
in[year to].

4.

[5]

APPLICATIONS

[6]

We have applied our system to two datasets. The first
dataset consists of macro-economic indicator data from the
World Bank3 . There are 1,319 indicators with a time
frame from 1960 to 2015. For the macro-economic indicator dataset, the system processes queries requesting the
indicator values for a specific country and year or range of
years, the top ranked country according to an indicator for
a region or group of countries (e.g., OECD), as well as the
correlation between the indicator values for a given country. To this end, we created 2 answer generation and 36
answer rendering templates. The second dataset comprises
more than 200K records about company M&A deals sampled from a proprietary database. For the M&A dataset,
the system processes queries comprising company names, as
well as queries for deals in a country, an industry sector, or
their combinations for a given year, for which we prepared
4 answer generation and 15 answer rendering templates.

5.

[7]

[8]

[9]

[10]

[11]

[12]

CONCLUSIONS

In this work, we have introduced a system that enables
users to search financial data robustly using both keyword
and natural language queries, and returns the answer in the
form of an automatically generated textual description, enriched with additional derived information about the context of the answer. The textual description of the answers
enables us to use the system both in desktop or smaller devices, including speech-enabled devices, where text may be
the only way to deliver the answer. Moreover, the system
was designed to be flexible and domain-adaptable, by using
a template-based answer generation engine to convert data
to virtual documents, which are indexed by a search engine,
and a configurable mapping of user intents to query plans.

[13]
[14]

[15]
[16]
[17]

[18]

Acknowledgments

[19]

We thank Albert Lojko, Alex Tyrell, Sidd Shenoy, Rohit
Mittal and Jessica Tran from Thomson Reuters F&R, and
Khalid Al-Kofahi from Thomson Reuters R&D for their support and discussions. This work received financial support
by Thomson Reuters Global Resources.

6.

[20]

REFERENCES

[1] B. Aditya, Gaurav Bhalotia, Soumen Chakrabarti, Arvind
Hulgeri, Charuta Nakhe, Parag, and S. Sudarshan.
BANKS: Browsing and keyword searching in relational
databases. VLDB ’02, pages 1083–1086, 2002.
[2] Sonia Bergamaschi, Elton Domnori, Francesco Guerra,
Mirko Orsini, Raquel Trillo Lado, and Yannis Velegrakis.
Keymantic: Semantic keyword-based searching in data

3

[21]

[22]

http://data.worldbank.org/

1124

integration systems. Proc. VLDB Endow.,
3(1-2):1637–1640, 2010.
Sina Fakhraee and Farshad Fotouhi. DBSemSXplorer:
Semantic-based keyword search system over relational
databases for knowledge discovery. KEYS ’12, pages 54–62,
2012.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
Fan, David Gondek, Aditya A. Kalyanpur, Adam Lally,
J. William Murdock, Eric Nyberg, John Prager, Nico
Schlaefer, and Chris Welty. Building Watson: An overview
of the DeepQA project. AI Magazine, 31(3):59–79, 2010.
Venkatesh Ganti, Yeye He, and Dong Xin. Keyword++: A
framework to improve keyword search over entity
databases. Proc. VLDB Endow., 3(1-2):711–722, 2010.
Andrew Hickl, Kirk Roberts, Bryan Rink, Jeremy Bensley,
Tobias Jungen, Ying Shi, and John Williams. Question
answering with LCC’s CHAUCER-2 at TREC 2007. TREC
’07, 2007.
Vagelis Hristidis and Yannis Papakonstantinou.
DISCOVER: Keyword search in relational databases.
VLDB ’02, pages 670–681. Morgan Kaufmann, 2002.
Boris Katz, Gary C. Borchardt, and Sue Felshin. Natural
language annotations for question answering. FLAIRS ’06,
pages 303–306, 2006.
Ravi Kondadadi, Blake Howald, and Frank Schilder. A
statistical nlg framework for aggregated planning and
realization. ACL ’13, pages 1406–1415, 2013.
Jochen L. Leidner, Johan Bos, Tiphaine Dalmas, James R.
Curran, Stephen Clark, Colin J. Bannard, Mark Steedman,
and Bonnie Webber. The QED open-domain answer
retrieval system for TREC 2003. TREC ’03, pages 595–599,
2003.
Jochen L. Leidner and Darya Kamkova. Making structured
data searchable via natural language generation – with an
application to ESG data. FQAS ’13, pages 495–506, 2013.
Y. Luo, W. Wang, and X. Lin. Spark: A keyword search
engine on relational databases. ICDE ’08, pages 1552–1555,
2008.
Dan Moldovan, Christine Clark, and Mitchell Bowden.
Lymba’s PowerAnswer 4 in TREC 2007. TREC ’07, 2007.
Naoaki Okazaki and Jun’ichi Tsujii. Simple and efficient
algorithm for approximate dictionary matching. COLING
’10, pages 851–859, 2010.
K.Q. Pu and Xiaohui Yu. FRISK: Keyword query cleaning
and processing in action. ICDE ’09, pages 1531–1534, 2009.
Ehud Reiter and Robert Dale. Building Natural Language
Generation Systems. Cambridge University Press, 2000.
Ehud Reiter, Somayajulu Sripada, Jim Hunter, Jin Yu, and
Ian Davy. Choosing words in computer-generated weather
forecasts. Artif. Intell., 167(1-2):137–169, September 2005.
Dan Shen, Michael Wiegand, Andreas Merkel, Stefan
Kazalski, Sabine Hunsicker, Jochen L. Leidner, and
Dietrich Klakow. The Alyssa system at TREC QA 2007:
Do we need Blog06? TREC ’07, 2007.
Dezhao Song, Frank Schilder, Charese Smiley, Chris Brew,
Tom Zielund, Hiroko Bretz, Robert Martin, Chris Dale,
John Duprey, Tim Miller, and Johanna Harrison. TR
Discover: A Natural Language Interface for Querying and
Analyzing Interlinked Datasets. ISWC ’15, pages 21–37,
2015.
Qi Su and Jennifer Widom. Indexing relational database
content offline for efficient keyword-based search. IDEAS
’05, pages 297–306, 2005.
Mohamed Yahya, Klaus Berberich, Shady Elbassuoni,
Maya Ramanath, Volker Tresp, and Gerhard Weikum.
Deep answers for naturally asked questions on the Web of
data. WWW ’12 Companion, pages 445–449, 2012.
Zhiping Zheng. Question answering using Web news as
knowledge base. EACL ’03, pages 251–254, 2003.

