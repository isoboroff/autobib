Doc2Sent2Vec: A Novel Two-Phase Approach for Learning
Document Representation
Ganesh J

Manish Gupta

Vasudeva Varma

IIIT, Hyderabad, India
ganesh.j@research.iiit.ac.in

Microsoft, India
gmanish@microsoft.com

IIIT, Hyderabad, India
vv@iiit.ac.in

ABSTRACT

documents as it suffers from data sparsity and curse of high
dimensionality. Latent Dirichlet Allocation (LDA) [2] is another widely adopted distributed document representation.
In an attempt to harness the power of neural networks
for document representations, Le et al. [3] propose a simple approach to learn document embedding from the word
sequence using a standard word-level language model. The
representations learned capture the ordering of words (unlike BOW) and also the semantics of the words in an efficient way (unlike LDA). In their follow-up work [4], the
authors proposed an incremental model by jointly learning
the word embeddings along with its document embedding.
This change leads to learning rich and accurate representation compared to the previous model, which freezes the word
vectors while learning the document vectors.
Inspired by the superior results obtained by the neural language models, we present a two-phase approach, Doc2Sent2Vec,
to learn document embedding. In the first phase, we learn
the sentence embedding using the word sequence generated
from the sentence. Intuitively, the sentence representation
is computed by modeling word-level coherence. In the next
phase, we propose a novel model that learns the document
representation from the sentence sequence generated from
the document. Intuitively, the document embedding is computed by modeling sentence-level coherence. We argue in
this work that the proposed decoupled strategy allows our
model to compute accurate and rich document representations.
We validate the learned document embeddings using two
classification tasks. In the first task, we aim at classifying a
research article (or paper) among one of the eight different
fields of computer science domain. In the second task, we
predict the tag of a Wikipedia page. Doc2Sent2Vec outperforms the existing state-of-the-art model in scientific article
classification task by ∼12.07% and Wikipedia page classification task by ∼6.93%, both in terms of F1 score.
Our main contributions are summarized below.

Doc2Sent2Vec is an unsupervised approach to learn lowdimensional feature vector (or embedding) for a document.
This embedding captures the semantics of the document and
can be fed as input to machine learning algorithms to solve
a myriad number of applications in the field of data mining and information retrieval. Some of these applications
include document classification, retrieval, and ranking.
The proposed approach is two-phased. In the first phase,
the model learns a vector for each sentence in the document
using a standard word-level language model. In the next
phase, it learns the document representation from the sentence sequence using a novel sentence-level language model.
Intuitively, the first phase captures the word-level coherence
to learn sentence embeddings, while the second phase captures the sentence-level coherence to learn document embeddings. Compared to the state-of-the-art models that
learn document vectors directly from the word sequences, we
hypothesize that the proposed decoupled strategy of learning sentence embeddings followed by document embeddings
helps the model learn accurate and rich document representations.
We evaluate the learned document embeddings by considering two classification tasks: scientific article classification
and Wikipedia page classification. Our model outperforms
the current state-of-the-art models in the scientific article
classification task by ∼12.07% and the Wikipedia page classification task by ∼6.93%, both in terms of F1 score. These
results highlight the superior quality of document embeddings learned by the Doc2Sent2Vec approach.

1.

INTRODUCION

Document representations plays a vital role in the performance of several downstream IR applications such as document classification (or tagging), retrieval, ranking and so
on. The most commonly used document representation is
bag-of-words (BOW) or bag-of-n-grams [1]. Despite its simplicity and efficiency, it fails to capture the semantics of the

• We present a novel two-phase approach, Doc2Sent2Vec,
to learn document embeddings. To this end, we introduce a novel sentence-level language model which
effectively constructs document representations by explicitly modeling sentence-level coherence.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

• Experiments on Citation Network and Wikipedia datasets
show that Doc2Sent2Vec learns high quality document
embeddings outperforming the competitive baselines
in both the classification tasks.

SIGIR ’16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2914717

809

sentences within document (sentence-level coherence)
d(m)

s(m, t-cS) . . . s(m, t-1)

s(m, t+1) . . . s(m, t+cS)

w(n,t)

hidden

hidden

s(m, t)

w(n,t-cW) . . . w(n,t-1)

w(n,t+1) . . . w(n,t+cW)

words within sentence (word-level coherence)

Figure 1: Architecture diagram of the Doc2Sent2Vec approach. Sentence embedding weights are shared
between the two tiers.

2.

THE DOC2SENT2VEC APPROACH

w(n,t) given the context words and the sentence is defined
using the following softmax function.

Our hierarchical framework as shown in Figure 1 consists
of two tiers; one learning the sentence representation from
the word context, another learning the document representation from the sentence context.
Problem Formulation: Formally, let us denote a set D of
M documents, D = {d1 , d2 , · · · , dM }, where each document
dm is a sequence of Tm sentences, dm = {s(m,1) , s(m,2) , · · · ,
s(m,Tm ) }. Each sentence is a sequence of Tn words, s(m,n) =
{w(m,n,1) , w(m,n,2) , · · · , w(m,n,Tn ) }. For brevity, we drop the
sentence index m when it is obvious in the context. The
goal of the Doc2Sent2Vec approach is to jointly learn lowdimensional representations of words, sentences and documents as a continuous feature vector of dimensionality Dw ,
Ds and Dd respectively. We will realize this goal in two
phases, as discussed in the following.
Modeling word-level coherence: In the first phase, the
model aims to learn sentence representation from the word
sequence within the sentence. We add the sentence vector
to the standard language model that predicts the next word
given its context word. This sentence vector must capture
the topics of the sentence in a compact form. Each word
is mapped to a unique vector, denoted by a column in the
matrix Vword , whose size is given by Dw ×|Vw | (where |Vw | is
the vocabulary size). Similarly, each sentence in a document
is mapped to a unique vector denoted by a column in the
matrix Vsent with size Ds × |Vs |, where |Vs | is the number of
unique sentences. The model uses the concatenation of word
vectors of context words along with the sentence vector as
features to predict the given word in a sentence.
Formally, consider w(n,t−cw ) , · · · , w(n,t−1) , w(n,t+1) , · · · ,
w(n,t+cw ) as the context words for the target word w(n,t) ,
appearing in the sentence s(m,n) . The objective of the wordlevel language model is to maximize the log likelihood probability.
Lword =

X 

X

P(w(n,t) |w(n,t−cw ) , · · · , w(n,t−1) , w(n,t+1) , · · · , w(n,t+cw ) , s(m,n) )


T
exp v̄1word v0w
(n,t)


(2)
= P
|Vw |
1T
0
exp
v̄
w=1
word vw

where v0w(n,t) is the output representation of w(n,t) and v̄1word
is the concatenation of the input embeddings (ignoring the
central term w(n,t) ), with dimensionality 2 × cw × Dw + Ds .

1
v̄word = vs(m,n) ; vw(n,t−c

X

;



(3)

T

exp(v̄2word v0s
)
(m,n)
P(s(m,n) |w(n,1) , · · · , w(n,Tn ) ) = P|V |
T
s
2
0
s=1 exp(v̄word vs )

(4)

where v0s(m,n) is the output representation of sm,n and v̄2word
is the concatenation of the input embedding of all the words
present in the sentence sm,n , with dimensionality Tn × Dw .

2
v̄word = vw(n,1) ; · · · ; vw(n,T

n)

;



(5)

Modeling sentence-level coherence: In the next phase,
we propose a novel language model which constructs the document representations from the sentence sequence present
in the document. The novel task is to predict the current
sentence using the embeddings of the surrounding sentences
and the document embedding as features. We add the document vector in the input layer of this model, that captures
the topics of the entire document in a compact form. Each
document is mapped to a unique vector denoted by a column in the matrix Vdoc , whose size is given by Dd × |Vd |
(where |Vd | is the number of unique documents).
Formally, consider s(m,t−cs ) , · · · , s(m,t−1) , s(m,t+1) , · · · ,
s(m,t+cs ) as the context sentences for the target sentence
s(m,t) , appearing in the document dm . The objective of
our novel sentence-level language model is to maximize the
following log likelihood probability.

log P(w(n,t) |w(n,t−cw ) , · · · , w(n,t−1) ,

s(m,n) ∈dm w(n,t) ∈s(m,n)



w)

Similarly, we define the probability of observing the sentence s(m,n) , given the words present in it as follows.

log P(s(m,n) |w(n,1) , · · · , w(n,Tn ) )+

w(n,t+1) , · · · , w(n,t+cw ) , s(m,n) )

; · · · ; vw(n,t−1) ;
vw(n,t+1) ; · · · ; vw(n,t+c

dm ∈D s(m,n) ∈dm

X

w)

(1)

Here 2 × cw denotes the length of the context for the word
sequence. The probability of observing the central word

810

Dataset
CND

# Docs
8000

Wiki10+

19740

Labels
information retrieval, data mining, artificial intelligence, machine learning and pattern recognition, natural language
and speech, computer vision, distributed and parallel computing, human-computer interaction
wiki, art, reference, people, culture, books, design, politics, technology, psychology, interesting, wikipedia, research,
religion, music, math, development, theory, philosophy, article, language, science, programming, history, software

Table 1:

Lsent =

X 

Dataset Details

Model
Paragraph2Vec w/o WT [3]
Paragraph2Vec [4]
Doc2Sent2Vec w/o WT
Doc2Sent2Vec

log P(dm |s(m,1) , · · · , s(m,Tm ) )+

dm ∈D

X

log P(s(m,t) |s(m,t−cs ) , · · · , s(m,t−1) , s(m,t+1) , · · · ,

s(m,t) ∈dm

Table 2:

s(m,t+cs ) , dm )

Dataset Description: The dataset details are displayed
in Table 1. Citation Network Dataset (CND) [5] consists of
a collection of research papers (along with abstracts) from
different fields of computer science domain. Inspired by the
recent work [13], we use only a sample of the original dataset
to speed up the training process. We construct a dataset
of 8000 papers by randomly sampling 1000 research papers
from 8 different fields, as mentioned in Table 1. In the second task where we perform Wikipedia page classification,
we make use of Wiki10+ dataset [14], which contains one or
more social tags (along with the number of users who have
annotated this tag) for each Wikipedia page retrieved from
delicious.com. We find the most frequent 25 social tags and
only keep those documents that contain any of these tags.
It results in a collection of 19740 documents as shown in Table 1, with each document associated with the most voted
social tag. For simplicity, we consider only the first paragraph of the Wikipedia article for learning the embeddings.
Experimental Setup: In all our experiments, we consider
the following four models.

P(s(m,t) |s(m,t−cs ) , · · · , s(m,t−1) , s(m,t+1) , · · · , s(m,t+cs ) , dm )
T

exp(v̄1sent v0s
) (7)
(m,t)
= P|V |
s
T
0)
exp(v̄
v
sent s
s=1

where v0s(m,t) is the output representation of s(m,t) and v̄1sent
is the concatenation of the input embeddings (ignoring the
central term s(m,t) ), with dimensionality 2 × cs × Ds + Dd .

1
v̄sent = vdm ; vs(m,t−c

s)

; · · · ; vs(m,t−1) ;
vs(m,t+1) ; · · · ; vs(m,t+c

s)

;



(8)

Similarly, we define the probability of observing the document dm given the sentences present in it as follows.
T

exp(v̄2sent v0dm )
P(dm |s(m,1) , · · · , s(m,Tm ) ) = P|V |
d
2T
0
d=1 exp(v̄sent vd )

(9)

• Paragraph2Vec w/o WT [3]: Paragraph2Vec algorithm
without Word Training (i.e. the word embedding
matrix Vw is freezed during training).

where v0dm is the output representation of dm and v̄2sent is
the concatenation of the input embedding of all the sentences present in the document dm , with dimensionality
Tm × Ds .

2
v̄sent = vs(m,1) ; · · · ; vs(m,T

m)

;



• Paragraph2Vec [4]: Extension of [3] which allows joint
training of both word and document vectors.
• Doc2Sent2Vec w/o WT: Model discussed in Section 2
without word training.

(10)

Training details: The overall objective function of Doc2Sent2Vec
is to maximize the log likelihood probability as follows.
L = Lword + Lsent

Classification Performance on CND Dataset

(6)

Here 2 × cs denotes the length of the context for the sentence sequence. The probability of observing the central
sentence s(m,t) given the context sentences and the document is defined using the softmax function as given below.

• Doc2Sent2Vec: Model discussed in Section 2.

To ensure fair comparison, we empirically set Cw , Cs , Dw ,
Ds , Dd to 5, 1, 100, 100, 100 respectively, for all the models.
We lowercase all the words, remove those which occur less
than 10 (15) times in the CND (Wiki10+) corpus. We use
pre-trained Glove [11] word vectors trained successively on
Wikipedia 20141 and Gigaword 52 corpus, to initialize the
word embeddings (Vw ). It is important to notice that we
use a linear classifier (one-vs-rest logistic regression3 ) to do
prediction. It can be argued that the model performance
can be improved using non-linear models, but this falls out
of scope of our goal. We use 5-fold cross-validation to report
the model performance for both the tasks.

(11)

We employ stochastic gradient descent to learn the parameters, where the gradients are obtained via backpropagation [12], with fixed learning rate of 0.1. However, it
takes O(Vw ), O(Vs ), O(Vs ) and O(Vd ) to compute ∇ log P
from Equations 2, 4, 7 and 9, which is undesirable in practice. Hence, we use hierarchical softmax [6], to facilitate
faster training. The testing phase was excluded as the embeddings for all the documents in the dataset are estimated
during the training phase.

3.

F1
0.1275
0.135
0.1288
0.1513

EXPERIMENTS

1

http://dumps.wikimedia.org/enwiki/20140102/
https://catalog.ldc.upenn.edu/LDC2011T07
3
http://scikit-learn.org/stable/modules/generated/sklearn.
linear model.LogisticRegression.html
2

In this section, we present the experimental results to
show the effectiveness of the learned document embeddings
by considering two classification tasks.

811

Model
Paragraph2Vec w/o WT [3]
Paragraph2Vec [4]
Doc2Sent2Vec w/o WT
Doc2Sent2Vec

Table 3:

F1
0.0476
0.0445
0.0401
0.0509

ment sequence in the stream. The Doc2Sent2Vec approach
is different from HDV because our model does not assume
the existence of a document stream and HDV does not model
sentences.

Classification Performance on Wiki10+ Dataset

5.

We proposed a novel two-phase approach Doc2Sent2Vec
to learn document representations in an unsupervised fashion. To this end, we introduced a novel sentence-level language model which exploits the sentence sequence present
in the document. We validated the document embeddings
by considering two classification tasks. Our classification
results indicate the superiority of the proposed approach,
thereby constituting a step towards learning accurate and
rich document representations.
In the future, we plan to extend the current approach
to a general multi-phase approach where every phase corresponds to a logical sub-division of a document like words,
sentences, paragraphs, subsections, sections and documents.
Also, it will be interesting to investigate how the document
embeddings that are learned through the Doc2Sent2Vec approach can be enhanced by considering the document sequence in a stream such as news click-through streams [7].

Analysis on CND: Scientific article classification results
are shown in Table 2. We observe that it is beneficial to
learn word vectors too while training instead of merely using
pre-initialised word vectors. On incorporating the learning
of word vectors, the improvement of ∼5.88% and ∼17.47%
in F1 score for Paragraph2Vec and Doc2Sent2Vec respectively, justifies this claim. Moreover, we see that our model
outperforms the state-of-the-art model by a significant margin of ∼12.07%. This is mainly because our model is able
to exploit both word-level and sentence-level coherence to
enrich the embeddings.
Analysis on Wiki10+: We present the Wikipedia page
classification results in Table 3. It is interesting to see that
learning the word vectors has a negative impact for Paragraph2Vec algorithm. This is shown by a decline in the
F1 score by ∼6.5%. We believe that the word vectors before training are semantically accurate as they are learned
from the complete Wikipedia corpus. While training them
again, the vectors tend to get distorted leading to poor results. It can be argued that the same trend should follow
for our model when we learn the word vectors. However,
the Doc2Sent2Vec results indicate that the F1 improves by
∼26.93% on learning word vectors. This illustrates that the
proposed strategy of jointly exploiting sentence level and
word level coherence is insensitive to the distortions generated by word vectors, resulting in robust embeddings of the
Wikipedia pages. The performance improvement of ∼6.93%
over the best baselines in terms of F1 score, highlights the
superiority of the Doc2Sent2Vec approach.

4.

CONCLUSION

Acknowledgement
This work is supported by SIGIR Donald B. Crouch Travel
Grant. The authors would like to thank NVIDIA for donating one Tesla K40 GPU card.

6.

REFERENCES

[1] Harris, Z.: Distributional structure. Word, 10(23). (1954)
146–162
[2] Blei, D., Ng, A.Y., Jordan, M.I.: Latent Dirichlet Allocation.
In: JMLR. (2013)
[3] Le, Q., Mikolov, T.: Distributed Representations of Sentences
and Documents. In: ICML. (2014) 1188–1196
[4] Dai, A.M., Olah, C., Le, Q.V., Corrado, G.S.: Document
embedding with paragraph vectors. In: NIPS Deep Learning
Workshop. (2014)
[5] Chakraborty, T., Sikdar, S., Tammana, V., Ganguly, N.,
Mukherjee, A.: Computer Science Fields as Ground-truth
Communities: Their Impact, Rise and Fall. In: ASONAM.
(2013) 426–433
[6] Mikolov, T., Chen, K., Corrado, G., Dean, J.: Efficient
Estimation of Word Representations in Vector Space. In: ICLR
Workshop. (2013) vol. abs/1301.3781
[7] Djuric, N., Wu, H., Radosavljevic, V., Grbovic, M.,
Bhamidipati, N.: Hierarchical Neural Language Models for
Joint Representation of Streaming Documents and their
Content. In: WWW. (2015) 248–255
[8] Bengio, Y., Ducharme, R., Vincent, P., Jauvin, C.: A Neural
Probabilistic Language Model. In: JMLR. (2003) 1137–1155
[9] Collobert, R., Weston, J., Bottou, L., Karlen, M.,
Kavukcuoglu, K., Kuksa, P.: Natural Language Processing
(Almost) from Scratch. In: JMLR. (2011) 2493–2537
[10] Morin, F., Bengio, Y.: Hierarchical Probabilistic Neural
Network Language Model. In: AISTATS. (2005) 246–252
[11] Pennington, J., Socher, R., Manning, C.D.: GloVe: Global
Vectors for Word Representation. In: EMNLP. (2014)
1532–1543
[12] Rumelhart, D., Hinton, G., Williams, R.: Learning
Representations by Back-propagating Errors. In: Nature.
(1986) 533–536
[13] Dos Santos, C.N., Gatti, M.: Deep convolutional neural
networks for sentiment analysis of short texts. In: COLING.
(2014) 69–78
[14] Zubiaga, A.: Enhancing navigation on wikipedia with social
tags. In: arxiv. (2012) vol. abs/1202.5469

RELATED WORK

Our work is inspired by the outburst of representation
learning works using neural networks [6, 8, 9] to solve many
natural language processing tasks. In this work, we focus
on the important problem of capturing the semantics of the
document in a machine-understandable format (or representation).
A recent work Paragraph2Vec [3] provides a simple solution to this problem by extending the popular algorithm
‘Word2Vec’ [6]. Their strategy of adding a memory vector
to the input layer of the neural network exhibits significantly
better results than commonly used representations such as
LDA, BOW and so on. In the original work the authors
freeze the word vectors, while in the extension [4] they let
the gradients update the word vectors too, along with document vectors. This trick further improves the results in
understanding longer documents such as research articles
and Wikipedia pages. Our work is inspired by the superior
results of these neural language models.
Similar to the spirit of Doc2Sent2Vec, Djuric et al. [7]
propose a Hierarchical Document Vector (HDV) model to
learn representations from a document stream. In the first
phase, the model learns document representation from the
word sequence (similar to [3]). In the next phase, the model
enriches the representations further by exploiting the docu-

812

