A Sequential Decision Formulation of the Interface Card
Model for Interactive IR
Yinan Zhang

Chengxiang Zhai

Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801

Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana, IL 61801

yzhng103@illinois.edu

czhai@illinois.edu

ABSTRACT

However, PRP relies on two unrealistic assumptions: (1)
independent utility/relevance of documents, and (2) sequential browsing, making it hard to study issues such as diversity (which requires “dependent relevance”) or study how to
optimize IR interface design within the PRP framework. To
address the limitation due to assumption (1), a more general
PRP for Interactive IR (namely PRP-IIR) was proposed [8],
which captures the the dependency of the utility of documents examined later by a user on that of the documents
already seen by the user in a theoretical framework. Such a
framework also provides a theoretical justification for novel
evaluation measures proposed for novelty and diversity (e.g.,
the α-NDCG [5]), and rank-biased precision [16]. The recent
Interface Card Model (ICM) [26] further generalizes PRPIIR to address the limitation due to assumption (2), and
frames the retrieval problem as to optimize a sequence of
“interface cards” to be presented to a user in an interactive
manner so as to minimize the user’s effort while maximizing
the gain. The framework is shown to not only cover both
PRP and PRP-IIR as special cases, but also enable automatic design of an interactive navigation interface in adaption to the screen sizes and the uncertainty about a user’s
information need [26]. From evaluation perspective, ICM
implies that an evaluation measure should be “aware” of the
actions taken by users on a retrieval result and the effort of
taking an action, thus providing a justification for measures
such as the time-based gain measure [23] where user effort
is measured based on the time spent by the user.
ICM provides a high-level theoretical framework for optimizing interactive retrieval, but it does not specify a systematic way to instantiate it for solving a concrete interface
optimization problem, leaving how to further refine this general framework an open challenge. In this paper, we address
this challenge and propose a novel formulation of the Interface Card Model based on sequential decision theory, which
leads to a general instantiation of ICM that can explicitly
model user states and stopping actions in search in a formal
framework. It naturally connects the optimization of interactive retrieval with Markov Decision Process (MDP) and
Partially Observable Markov Decision Process (POMDP)
[20], thus enabling the use of reinforcement learning algorithms for optimizing interactive retrieval interfaces in general. We refer to the new model as Interface Card Model
with User States (ICM-US).
While the proposed ICM-US model remains a high-level
framework, it has several important advantages over ICM.
Firstly, ICM-US explicitly models user states, which can potentially include many relevant variables about a user that

The Interface Card model is a promising new theoretical
framework for modeling and optimizing interactive retrieval
interfaces, but how to systematically instantiate it to solve
concrete interface optimization problems remains an open
challenge. We propose a novel formulation of the Interface
Card model based on sequential decision theory, leading to
a general framework for formal modeling of user states and
stopping actions. The proposed framework naturally connects optimization of interactive retrieval with Markov Decision Processes and Partially Observable Markov Decision
Processes, and enables the use of reinforcement learning algorithms for optimizing interactive retrieval interfaces. Simulation and user study experiments demonstrate the effectiveness of the proposed model in automatically adjusting
the interface layout in adaptation to inferred user stopping
tendencies in addition to user interaction and screen size.

1.

INTRODUCTION

Formal modeling of information retrieval process is one of
the most important fundamental research problems in information retrieval. It not only enables formalization of the
retrieval task as a well-defined computation problem, which
is required for designing any effective retrieval algorithm,
but also provides a foundation for quantitative evaluation of
a retrieval system.
The Probability Ranking Principle (PRP) was an early attempt to formalize the retrieval task as a problem of ranking
documents for a query. It motivated and laid out a theoretical foundation for studying and deriving many traditional
retrieval models to optimize ranking in the past few decades
[21, 19, 18, 9, 1, 24, 6], as well as more recent works on
learning to rank [11]. PRP also provides a foundation and
justification for designing evaluation measures such as Mean
Average Precision (MAP) and Normalized Discounted Cumulative Gain (nDCG) [10] to measure the accuracy of a
ranked list of retrieval results.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from Permissions@acm.org.
SIGIR ’16, July 17-21, 2016, Pisa, Italy
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07 ...$15.00.
DOI: http://dx.doi.org/10.1145/2911451.2911543 .

85

we want to model (e.g., patience, attention tendency, and
readability) in optimizing retrieval results. In this paper, we
will particularly examine the modeling of stopping actions
which is related to a user’s patience in search, and derive a
framework to optimize the interface design with consideration of users’ stopping tendencies. Secondly, ICM-US opens
up many opportunities to use sequential decision theories
and reinforcement learning algorithms to optimize interactive retrieval. In this paper, we will show that it is possible
to use the ICM-US framework to define and solve the interface optimization problems studied in [26] in more elegant
and more general ways. Specifically, we work out the “plain
card” case in the sequential decision theory context and take
the user stopping tendencies into consideration, and mathematically prove that a more general ranking principle is
the solution to the Bellman Equation. In the “navigational
card” case, we consider user stopping tendencies and define
a more general interface optimization problem. The problem is proved to be NP-Hard and we conduct experiments to
tackle it in two ways: (a) we conduct simulated experiments
to solve the Bellman Equation under reasonable simplification assumptions; (b) we approximately solve the optimization problem in more general settings and conduct user
studies to examine the empirical benefit of ICM-US. The
results demonstrate that ICM-US is effective for automatically adjusting the interface layout in adaptation to inferred
user stopping tendencies in addition to user interaction and
screen size.

to complement each other and collectively lead to more effective search systems for users.

3.

INTERFACE CARD MODEL WITH USER
STATES

In [26], the authors proposed the Interface Card model for
optimizing interactive retrieval interfaces in a general setting
as follows. Let t be the interaction lap under consideration,
q t be an interface card the system could issue to the user,
at+1 ∈ A(q t ) be an action the user takes in the following lap
as a response to q t , 1 and fct be the constraint function for
q t . Let ct be the context accumulated till the current user
action that starts from c0 = i, the prior information the
system has about the user, and is incrementally updated by
the rule ct+1 = (ct , q t , at+1 ). Let p(at+1 |ct , q t ) be the user
action model for characterizing how likely the user issues
action at+1 given context ct and card q t . Let ut be the
estimated user surplus, which equals the difference between
the user’s reward and their cost for issuing an action. Then
the Interface Card optimization problem is defined as:
Definition 3.1 (Interface Card Optimization). In each lap
t, the interface system should play a card q t that maximizes
the expected surplus ut given the current context and under
the current constraint, where the expectation is taken with
respect to the user action model:
maximize
qt

2.

RELATED WORK

E(ut |ct , q t )
X
=
p(at+1 |ct , q t ) u(at+1 |ct , q t )

(1)

at+1 ∈A(q t )

The proposed ICM-US framework is related to several
lines of recent work on POMDP and reinforcement learning (e.g.,[13, 12, 22]) and economic IR models (e.g., [2, 3]),
especially because all these works tend to also model user
interactions formally. The main difference between ICMUS and these work is that ICM-US is based on the Interface
Card Model, which is a very general framework for modeling
retrieval process (framed as choosing optimally a sequence of
interface cards), thus ICM-US can potentially model and optimize very complicated interaction interfaces, whereas the
other works cannot optimize the design of an interface due
to the restriction to mostly a ranking-based formulation of
the retrieval problem. However, the specific techniques and
models proposed in these existing work can all potentially
contribute to further refinement of ICM-US to make it even
more operational. For example, the economic IR work would
enable ICM-US to incorporate a user decision model for refining the user actions (e.g., modeling how the user actions
depend on the user state).
Our user action model characterizes how users make navigational decisions and stopping decisions in an information
seeking process, which is related to Information Foraging
theories [17], where models of user actions driven by information scent were proposed to describe how users navigate
on the web following hyperlinks. Recent works in user search
behavior analysis proposed and evaluated novel models for
users’ navigation actions [25], stopping actions [14], and for
characterizing users’ patience levels [16] in a search session.
Our ICM-US framework is more focused on the “orthogonal” question of how to optimize interactive search interfaces
given a learned user action model; such line of research and
the existing works in user action modeling and analysis serve

subject to

fct (q t ) ≤ 0

Instead of directly extending Equation (1) as in [26], we
first introduce its intrinsic relation to sequential decision theories, and then redefine and expand the instantiations in [26]
in a more systematic way and derive new interesting results.
In contrast to the typical practices adopted by other recent
works in applying sequential decision theories in information retrieval (e.g., [13]), we are not deriving our framework
based on sequential decision theories; all our derivation is
self-contained and solely relies on the Interface Card model
and our own assumptions, and the formalisms in sequential
decision theories we observe at the end are natural consequences of the derivation.
We first relate the interface optimizations in consecutive
laps using the notion of context surplus:
Definition 3.2 (Context Surplus). The context surplus is
the maximum expected surplus across all possible cards q t
subject to the constraint under the given context:
E(ut |ct ) = max E(ut |ct , q t )
qt

(2)

subject to fct (q t ) ≤ 0.
Assumption 3.1 (Accumulative Surplus). The action surplus u(at+1 |ct , q t ) takes the form of an arithmetic sum:
u(at+1 |ct , q t ) = u0 (ct , q t , at+1 ) + E(ut+1 |ct+1 )
1

t

(3)

As in [26], we assume the action set A(q ) is countable; the
case of uncountable action set could be handled via trivial
changes to the model.

86

The two components are: (a) u0 (ct , q t , at+1 ) - the immediate action surplus of action at+1 given card q t and under
context ct , which is the difference between the immediate
action reward r0 (ct , q t , at+1 ) and the immediate action cost
s0 (ct , q t , at+1 ); (b) E(ut+1 |ct+1 ) - the context surplus in the
next lap.

Definition 3.3 forms the basis for all subsequent derivations in this paper. Due to space limitations, we will only
write out derivations for Equation (4) or its equivalent form
Equation (7) in most places; the part for Equation (5) could
be trivially filled out in each case.
Now we propose a fundamental assumption underlying all
our following derivations regarding the modeling of the user:

Conceptually, we assume that the user obtains surplus
in an accumulative fashion: it is usually reasonable in real
world cases, which is also the reason why it became a standard practice in reward modeling in sequential decision theories. However, there might also be cases where the user
surplus takes a non-additive form. For example, certain future reward could be conditioned on some particular card in
advance, e.g. some instructive cards at the beginning to help
the user better understand the interactive interface. In such
cases, our assumption would become invalid and we would
need to step back to the more general form of the original
Interface Card model; we leave it to future work.
There will always be a terminal lap in an interactive retrieval process, e.g. when the user fulfilled their information
need or they could not find anything interesting and give up
the interaction. To make our discussion more concise and
modular, for the moment we assure that our proposed formulation could naturally characterize the case of terminal
laps, and we will come back to this in Section 4.
In contrast to the diminishing reward model adopted in a
large portion of sequential decision theories, where the future reward is multiplied by a discount factor γ < 1, we do
not penalize future reward in the current most general form
of our framework. Nevertheless, we will show in Section 4
that the diminishing reward model could be derived as a specialization of our framework to capture the user’s stopping
actions in the interaction process, which gives the diminishing reward model a much deeper and more principled reason
of existence.
With Equation (2) and (3), we rewrite Equation (1) as:

Assumption 3.2 (User State). In each lap t, the user lies
in a unique, unambiguous user state z t that contains all the
necessary information to determine the user action model
and the immediate action surplus when given any interface
card the system issues.
The user state is a general concept encapsulating many
real world cases. For example, the user’s information need
could be one straightforward type of user state, and there
could also be more subtle user states such as binary states
indicating whether the user is in the exploration or exploitation stage of their information seeking process; it is at the
will of the practitioners of our framework to decide on the
type of user states they would like to model. We will be
define the user state z t in various forms in the following
sections to fulfill diversified modeling needs.
The user state z t is closely related to the context ct in
that they both characterize about the user for the system,
but they are intrinsically different. There are two key insights we could obtain if we look back at Equation (4): (a)
the user action model and the immediate action surplus are
essentially all we need to fully carry out the computations in
our framework; (b) it was the context that determined the
user action model and the immediate action surplus when
given the interface card, but its role is now fulfilled by the
user state. More fundamentally, the context is always explicit to the system and contains all the information the
system has at hand to speculate about the user, whereas
the user state is intrinsic to the user and may or may not be
known to the system, yet it is the essential information the
system ever needs to know about the user. In other words,
the user action model and the immediate action surplus are
independent of the context given the user state - the user
state serves as the sole linkage between the context and all
the computational parts of our framework.
We first assume the user state is hidden from the interface
system, and the interface system could only guess about the
user state relying on the context.

Definition 3.3 (Interface Card Bellman Equation). The
expected surplus in consecutive laps satisfies:

X
E(ut |ct ) = max
p(at+1 |ct , q t ) ·
qt

at+1 ∈A(q t )
t

(4)

t

t+1

u0 (c , q , a

) + E(u

t+1

|c

t+1

)





and the optimal card the system should pick in lap t is:

X
t
qopt
= argmax
p(at+1 |ct , q t ) ·
qt

at+1 ∈A(q t )
t

t

t+1

u0 (c , q , a

) + E(u

t+1

|c

t+1

with q t in both equations subject to fct (q t ) ≤ 0.

)



Assumption 3.3 (Initial User State Distribution). There
exists an initial probability distribution over the user states,
denoted by d0 , specifying how likely the user is in each user
state at lap t = 0 that the system could estimate from the
prior information i about the user, i.e. the initial context
c0 . We write p(z 0 |d0 ) = p(z 0 |i) = p(z 0 |c0 ).

(5)



It is often convenient to define:
X
u0 (ct , q t ) =
p(at+1 |ct , q t ) u0 (ct , q t , at+1 )

In cases where there is no prior information about the
user, the system could naturally assume a uniform distribution over all possible user states.
When the interaction starts, we assume that the user
states may probabilistically transition from lap to lap when
the user observes an interface card:

(6)

at+1 ∈A(q t )

and transform Equation (4) to an equivalent form:

E(ut |ct ) = max u0 (ct , q t )+

Assumption 3.4 (User State Transition Function). In each
lap t, There exists a transition function ptT (z t+1 |z t , q t ) that
specifies the probability of the user transitioning from z t to
z t+1 when the user observes q t .

qt

X

at+1 ∈A(q t )

p(at+1 |ct , q t ) E(ut+1 |ct+1 )



(7)

87

Now, starting from p(z 0 |d0 ) = p(z 0 |c0 ), we could estimate the user state distribution dt inductively on t based on
Bayes’ rule:

aware of the user states for the sake of modeling convenience, either by explicitly asking or confirming about the
user states, or by interpreting that the user actions (e.g.
queries or clicks) have exact mappings to user states. In
such cases, the belief state dt is collapsed into an observable
user state z t . With a series of simplifications (omitted here
due to space limitations), Equation (11) becomes:

p(z t+1 |dt+1 ) = p(z t+1 |ct+1 ) = p(z t+1 |ct , q t , at+1 )
= p(z t+1 |dt , q t , at+1 ) = α p(at+1 |z t+1 , q t ) p(z t+1 |dt , q t )
X
= α p(at+1 |z t+1 , q t )
p(z t |dt ) ptT (z t+1 |z t , q t )

Definition 3.5 (Interface Card Bellman Equation (Fully
Observable User States)).
X  t+1 t t
t t
E(u |z ) = max
p(z |z , q ) ·

zt

(8)

where α−1 = p(at+1 |dt , q t ) is computed by:
X
p(at+1 |dt , q t ) =
p(at+1 |z t+1 , q t ) p(z t+1 |dt , q t )

qt

z t+1

=

X

p(a

t+1

|z

t+1

t

,q )

X

t

p(z |d

t

) ptT

(z

t+1

t

t

(9)

z t+1

t

|z , q )

u0 (z , q , z

zt

z t+1

subject to fct (q t ) ≤ 0.

The context ct now provides no useful information additional to our estimated user state distribution dt for computational purposes, so it could essentially be replaced by dt
in all places. If we define u0 (dt , q t , at+1 ) as:
u0 (dt , q t , at+1 ) = E(u0 |dt , q t , at+1 )
X
=
p(z t |dt ) u0 (z t , q t , at+1 )

t

t+1

) + E(u

t+1

|z

t+1

)





(12)

Equation (12) takes the exact same form as the standard
Bellman Equation for Markov Decision Process (MDP), which
is not a surprise since MDP could be derived as a special
case of POMDP if observations uniquely determine states.
We will see an example of realizing Equation (12) to solve a
concrete interface optimization problem in Section 5.

(10)

zt

then Equation (4) becomes:

4.

Definition 3.4 (Interface Card Bellman Equation (Partially Observable User States)).

X
E(ut |dt ) = max
p(at+1 |dt , q t ) ·

Definition 3.3 and its instantiations in the forms of Equation (11) and (12) have opened up enormous opportunities
for research studies to apply the Interface Card model to
a very wide range of real world problems via tools from sequential decision and reinforcement learning theories. In this
work, as an initial step, we will utilize our new framework
to study a basic yet nontrivial aspect of interactive information retrieval - stopping actions. We claimed in Section 3
that our framework could naturally characterize such cases,
and we now establish the formalism, starting by making the
following assumption:

qt

at+1 ∈A(q t )
t

t

(11)

t+1

u0 (d , q , a
subject to fct (q t ) ≤ 0.

) + E(u

t+1

|d

t+1

)





Equation (11) takes the exact same form as the standard
Bellman Equation for Partially Observable Markov Decision
Process (POMDP), where the user state z t , the user state
distribution dt , the interface card q t and the user action at+1
respectively play the roles of state, belief state, action and
observation (or often called evidence), and u0 (dt , q t , at+1 )
and E(ut |dt ) respectively serve as the the reward function2
and the value function for belief states [20]; Equation (8) is
the standard forward equation used for updating the belief
state based on the action and the observation. In the language of POMDP, the state the interface system is interested
in is the user state, and the actions the system could take
are the interface cards it could issue to the user. However,
since the user state is not fully observable, the system could
only decide on the optimal action to perform according to
the estimated distribution of user states, i.e. the belief state,
and the estimation is computed based on the observations
the system could collect about the user state, which are not
surprisingly the user actions. We will see a concrete example
of realizing Equation (11) to solve an interface optimization
problem in Section 6.
Though the user states would usually be invisible to the
system, we could often assume that the system is actually

STOPPING ACTION MODELING

Assumption 4.1 (User Stops Interaction). The interactive
process is always ended by the user - the system would always respond to user actions with optimized cards whereas
the user may or may not choose to continue the interaction.
Our assumption applies to most real world scenarios where
the user may choose to stop the interaction either because
of satisfaction of their information need or frustration due
to lack of useful information. There might occasionally be
cases where the system appears to be the one terminating
the interaction, e.g. if the system determines that nothing
in its database is interesting to the user and chooses to issue
a “terminal card” that attempts to stop the interaction with
some possible explanations. In such cases, the user is still
free to choose between leaving the system and starting another round of interaction, and indeed many would choose
the latter, so we could also model the termination as the
user’s choice when facing the system’s “terminal card”.
Definition 4.1 (Stopping Action). In each lap t, there is
a stopping action at+1
∈ A(q t ) for any interface card q t ,
B
and its estimated probability under the current context ct ,
t
t
p(at+1
B |c , q ), is called the stopping rate. The expected future surplus of the the new context following the stopping
action E(ut+1 |ct+1 ) = 0, where ct+1 = (ct , q t , at+1
B ).

2

The reward function is defined on state-action-observation
triples instead of on state-action pairs or just states as done
in many studies which are special cases of our definition.

88

In the language of sequential decision theories, observing
a user stopping action is analogous to entering a terminal
state (or belief state for POMDP) for the system.
The stopping rate is typically dependent on the interface
card the user faces: e.g., it would be smaller if the user is
interested in certain content on the card, and larger if the
content on the card does not look appealing to the user for
the past couple of laps. However, for the sake of modeling
and inference convenience, we may sometimes assume that
the stopping rate is constant:

interaction laps for the user. In economic theories, every
cost is intrinsically an opportunity cost, which is formally
defined to be the value of the best alternative [7]. In the
setting of interactive retrieval, when the opportunity cost
is higher than the expected surplus of carrying on the interaction longer, the user is very likely to switch to their
best alternative way of spending their time, e.g. working
on something else or simply relaxing, and from the system’s
perspective, this is exactly the stopping action of the user.
Therefore, our model for user stopping actions turns out to
draw fundamental connections from economic theories to sequential decision theories by providing deeper explanations
to the diminishing reward model. We could look even further into discrete choice models [15] in modern economic
studies to derive more rigorous formalisms for user decision
modeling, and we leave it to future works.

Assumption 4.2 (Constant Stopping Rate). The stopping
rate is a constant determined by the prior information c0 =
i of the user and is independent of the past interactions
0
and the interface card; we denote it by: β 0 = p(at+1
B |c ) =
t+1 t
t
t
t
0
p(aB |c , q ), ∀c , q , and we restrict that 0 < β < 1.
Assumption 4.2 is a “double-sided sword”: in the language
of bias-variance trade-off, it leads to a potentially large bias
in exchange for less variance. In this work, we will be cautious and make use of it only to our own advantage: we will
keep it for the rest of this section to derive one interesting
theoretical result and continue to rely on it in Section 5 to
simplify the computation, but we will discard it and resume
the dependency of the stopping rate on the full context and
the interface card in Section 6 for more realistic scenarios.
Now, with Assumption 4.2 in effect, if we define the user
continuation action model as:
ptK (at+1 |ct , q t ) =

p(at+1 |ct , q t )
p(at+1 |ct , q t )
=
t+1 t
1 − β0
1 − p(aB |c , q t )

5.

PLAIN CARD

As the first concrete extension of our proposed framework,
we revisit the “plain card” setting in [26]. We augment it
with stopping actions, and formally re-define the optimization problem based on our new sequential decision formulation for Markov Decision Process (MDP) defined in Equation (12). We will mathematically prove that a more generalized ranking principle with user stopping tendencies taken
into consideration is the solution to the Bellman Equation.
In the “plain card” setting in [26], the main assumption
is that each interface card q t is an atomic choice et placed
on a ranked list and that the user examines and either accepts or rejects the choices in a sequential manner. Now, we
additionally allow that the user may also stop after examining each choice. We continue to use p(et ), r(et ), s(et ) to
respectively denote the probability of the user’s interest in
accepting et , the immediate reward of accepting et and the
immediate cost of examining et in lap t, and these quantities
are assumed to be independent of the past user actions as
long as they have all been reject actions. An accept action
is regarded as a “satisfying” stopping action - it terminates
the current interaction session and starts a new one with
an updated set of parameters. We now formally define this
framework in the MDP language, starting with defining the
user state and the user action model:

(13)

for all at+1 6= at+1
B , then Equation (7) becomes:
Definition 4.2 (Interface Card Bellman Equation (Constant Stopping Rate)).

E(ut |ct ) = max u0 (ct , q t )+
qt
 (14)
X
(1 − β 0 )
ptK (at+1 |ct , q t ) E(ut+1 |ct+1 )
at+1 6=at+1
B

subject to fct (q t ) ≤ 0.
We could further extend Equation (14) to derive its instantiated forms for the MDP and POMDP cases based on
Equation (12) and (11); we omit such derivations here due to
space limitations, but we will be developing concrete models
as examples of these two forms in Section 5 and 6, respectively.
More interestingly, the term 1−β 0 in Equation (14) clearly
resembles the discount factor λ in a form of the standard
Bellman Equation for MDP and POMDP frequently seen in
sequential decision theories (as well as many recent works
in applying sequential decision theories in information retrieval, e.g., [13]) for modeling diminishing reward, and the
role of the (belief) state transition probabilities is here fulfilled by the user continuation action model. The resemblance is not a coincidence - it reveals a fundamental insight into the diminishing reward model in sequential decision theories. Traditionally, in addition to establishing an
upper bound for the value function for mathematical conveniences, a major purpose for setting the discount factor is
to express to the model our intuitive preference for quicker
reward; in our case, we want to reduce the cost of excessive

Definition 5.1 (Plain User State). The plain user state z t
at lap t is defined to be the set of choices not yet examined
′
by the user till the previous lap: z t = {e : q t 6= e, ∀ t′ < t}.
Definition 5.2 (Plain User Action Model). The plain user
action model for user state z t and choice et is a distribution
t+1
over the accept action at+1
and the
0 , the reject action a1
stopping action at+1
defined
under
the
constant
stopping
B
rate assumption as:

t+1 t t
t
t

 p(a0 |z , e ) = 1zt (e ) p(e )
t+1 t t
0
(15)
p(a1 |z , e ) = 1 − β − 1zt (et ) p(et )

 p(at+1 |z t , et ) = β 0
B
where 1zt (et ) is the indicator function for testing whether
et ∈ z t , and we assume p(et ) ≤ 1 − β 0 , ∀ et .

Essentially, the plain user action model claims that the
user will always reject a choice they have rejected before
(as long as no accept action has taken place in the middle),
which is very reasonable in real world situations.

89

For technical conveniences, we define the choice surplus of
a choice e: u(e) = p(e) r(e) − s(e), and we assume: (a) the
set of all possible choices is finite, so the cardinality of z t ,
||z t ||, is also finite; (b) if the choice shown on the interface
is the only element left in z t , then the user’s reject action
is regarded as a “frustrating” stopping action; (c) s(e) > 0,
∀ e, and (d) u(e) ≥ 0, ∀ e. Now, we could mathematically
solve Equation (12) in closed form:

The definition for η(e) in Equation (16) is identical to that
of ρ(e) in [26] except an additional multiplier involving β 0 :

 

s(e)
p(e)
β0
r(e)
−
=
1
−
ρ(e)
η(e) =
p(e) + β 0
p(e)
p(e) + β 0
(22)
If β 0 → 0, i.e. when the user never abandons the search,
then the two forms become equal. In the more general case
where β 0 > 0, our new form of η(e) may lead to a different ranking result where some choices with larger p(e)
values may be promoted because of the additional multiplier. Intuitively, when the user has a high tendency to stop,
they would examine less choices on average, so by promoting
choices with larger p(e) values to higher places, the system
could hope for a better chance of an accept action, and thus
at least some reward, before the user leaves. Therefore, given
a proper β 0 value (e.g. learned from user interaction logs),
our new ranking principle defined by η(e) could enable ranking with user stopping tendencies taken into consideration
and form a basis for novel ranking algorithms on top of a
richer user model.

Theorem 5.1 (Optimal Plain Card). Let z t 6= ∅. Define:
η(e) =

u(e)
p(e) r(e) − s(e)
=
, e ∈ zt
p(e) + β 0
p(e) + β 0

(16)

Suppose z t = {ej }n
j=1 = {e1 , e2 , . . . , en } and ej ’s satisfy:
η(ej ) ≥ η(ek ), ∀ 1 ≤ j < k ≤ n

(17)

t

Then e1 is an optimal card for z and:
t

t

E(u |z ) =

n
X
j=1

j−1

Y

0

!

(1 − β − p(ek )) u(ej )

k=1

(18)

Further, the complete optimization solution is to sequentially show e ∈ z t to the user in descending order of η(e).

6.

Proof. We prove by induction on ||z t ||:
1. ||z t || = 1. Let z t = {e1 }, then:
(
u(et ),
t t t
E(u |z , e ) =
(1 − β 0 )E(ut |z t ) − s(et ),

if et = e1
otherwise
(19)
Since E(ut |z t ) ≥ E(ut |z t , e1 ) = u(e1 ) ≥ 0, we have
E(ut |z t , et ) < (1 − β 0 )E(ut |z t ) ≤ E(ut |z t ), ∀ et 6= e1 ,
i.e. showing any choice other than e1 is non-optimal.
So, e1 is an optimal card for z t and E(ut |z t ) = u(e1 ).
2. Suppose n > 1 and the theorem holds for all z s.t.
||z|| = n − 1. Suppose z t = {ej }n
j=1 and ej ’s satisfies
Equation (17). If et ∈ z t , let et = eh , 1 ≤ h ≤ n, then:
E(ut |z t , eh ) = u(eh ) + (1 − β 0 − p(eh ))E(ut+1 |z t+1 )
(20)
where z t+1 = z t \ {eh }. By induction hypothesis on
z t+1 , we have E(ut+1 |z t+1 ) ≥ 0 from Equation (18).
Thus, E(ut |z t , eh ) ≥ 0, and E(ut |z t ) ≥ 0. Therefore,
E(ut |z t , et ) = (1 − β 0 )E(ut |z t ) − s(et ) < E(ut |z t ),
∀ et ∈
/ z t , i.e. showing any choice not in z t is again
non-optimal.
E(ut+1 |z t+1 ) in Equation (20) is by induction hypothesis maximized from ranking e ∈ z t+1 in decreasing
order of η(e). Thus, if et = eh 6= e1 , then et+1 = e1 .
Let z t+2 = z t \ {eh , e1 }, z1t+1 = z t \ {e1 }, then:
E(ut |z t , eh ) = u(eh ) + (1 − β 0 − p(eh )) ·
u(e1 ) + (1 − β 0 − p(e1 ))E(ut+2 |z t+2 )
≤ u(e1 ) + (1 − β 0 − p(e1 )) ·



u(eh ) + (1 − β 0 − p(eh ))E(ut+2 |z t+2 )
≤ u(e1 ) + (1 − β 0 − p(e1 ))E(ut+1 |z1t+1 )

NAVIGATIONAL CARD

In this section, we revisit the “navigational card” setting in
[26] and formally re-define and solve the optimization problem based on our sequential decision formulation for Partially Observable Markov Decision Process (POMDP) defined in Equation (11). We will again incorporate stopping
actions, but we will discard the constant stopping rate assumption and resume the more generalized and realistic setting: the stopping rate depends on the card and the full
context. We will demonstrate that our new formalism leads
to automatic interface adjustment based on users’ stopping
tendencies in addition to the context and the screen size.
The “navigational card” setting in [26] assumed the interface is backed by a set of information items denoted again by
e (e.g. websites), each associated with some related tags (e.g.
topics of websites), and the items and tags are respectively
represented on the interface by item and tag blocks denoted
by b. A card q t could contain any combination of item blocks
and/or tag blocks, as long as the total area
P they occupy does
not exceed the screen area: fct (q t ) =
b∈q t w(b) − 1 ≤ 0,
where w(b) represents the space block b occupies relative
to the entire screen size; the system may freely determine
the layout of the interface by increasing or decreasing the
number of item / tag blocks to display. Facing such a card
q t , the user may either select a displayed block or issue the
if nothing on the card interests them,
“next card” action at+1
N
the probabilities of which follow an item action model that
depends on the item the user is interested in. The interface
system estimates the user action model as an expectation of
the item action model taken with respect to the estimated
probability distribution of the user’s interest in each item
based on the latest context.
In this work, we incorporate the stopping action at+1
in
B
all action set A(q t ). If the user finds an interesting item
block and selects it, we consider it as a “satisfying” stopping
action which triggers the interface system to jump to the
item’s corresponding page. We define the user interest as a
hidden user state and assume it doesn’t change across laps:

 (21)

= E(ut |z t , e1 )
where the first inequality comes from η(e1 ) ≥ η(eh )
and the second inequality comes from E(ut+1 |z1t+1 ) ≥
E(ut+1 |z1t+1 , eh ). Therefore, et = e1 is optimal and
E(ut |z t ) = E(ut |z t , e1 ), which expands to the right
hand side of Equation (18).

Definition 6.1 (User Interest State). A user interest state
z t denotes the user’s interested item et in lap t: z t = et .

90

on the card q t and on the belief state dt . With sufficient user
interaction log data, we may apply reinforcement learning
algorithms to learn a more refined user action model in the
real world. In this study, we assume the simple user interest
action model to reduce the learning complication and focus
more on the modeling part.
To apply our model in real problems, the only missing
component now is the actual planning in the POMDP framework, and it is well known that planning in POMDP is NPhard [20]. We reduce the planning problem to manageable
forms via imposing additional assumptions on the cards in
Section 6.1 and via employing planning heuristics in Section
6.2, and we leave explorations of general planning solutions
to future work.

Assumption 6.1 (Persistent User Interest). The user interest state does not change across laps: p(et+1 |et , q t ) = 1
if et+1 = et , 0 otherwise. From now on, we use z 0 = e0 to
denote the user interest state.
Essentially, Definition 6.1 implicitly assumes that the user’s
interest is focused on only one item within each lap, and Assumption 6.1 extends it to the whole interaction process.
Both these two parts are sometimes inaccurate in reflecting
real world scenarios, but they serve to exponentially reduce
the complexity of our optimization problem.
With our simplification assumptions, Equation (8) and (9)
respectively reduce to:
p(e0 |dt+1 ) = α p(at+1 |e0 , q t ) p(e0 |dt )
α−1 = p(at+1 |dt , q t ) =

X

p(at+1 |e0 , q t ) p(e0 |dt )

(23)

6.1

(24)

e0

In order to make the computations more tractable, we
follow [26] and assume a constant immediate action cost s0 ,
a finite item set E = {e1 , e2 , . . . , en }, and assume that we
don’t have any useful prior information so we start with a flat
belief state d0 : p(ej |d0 ) = 1/n, 1 ≤ j ≤ n. We further make
the following two common assumptions before separating
into the two experiment sections:
Assumption 6.2 (Uniform Item Reward). The reward to
the user for selecting any item block in any lap is the same
and is denoted by r0 .

Assumption 6.1.1 (Exclusive Blocks). Any item e is related to at most one block on any card q t : ∀ e, ∃ at most
one b ∈ q t s.t. v(e, b) = 1.

Instead of modeling the actual expected reward of selecting an item block, r0 could be regarded as measuring an
eventual success in the interactive navigation, i.e. as the
value of locating any interesting item to the user versus not
finding anything at all.

Intuitively, this is a reasonable strategy for the system
especially given that the system has access to a complete
tag set: showing multiple blocks related to some item would
not only confuse the user, but the system in turn would also
be less precise in narrowing down into the item the user is
truly interested in, ending up unnecessarily increasing the
number of interaction laps.
More interestingly, this assumption guarantees that the
belief state dt is always a uniform distribution over a subset
of the items, which could be trivially reasoned via induction (omitted here due to space limitations). Furthermore,
due to the assumptions of uniform item reward and complete tag set, all belief states dt over the same number of
items would appear identical to the system in terms of planning: any optimal policy for a belief state dt over e.g. items
{e1 , e2 , e3 } is completely reflective to one for a belief state
dt over items {e4 , e5 , e6 } or any other item subset of size
3. Therefore, the belief state space has now been reduced
from a high-dimensional continuous space all the way to a
one-dimensional discrete space of size n. From now on, if dt
is over a set of n items, we say the size of dt , ||dt ||, is n.
We conducted simulation experiments to employ the standard value iteration algorithm to solve the Bellman Equation via dynamic programming. The experiments were performed for two screen sizes: a medium size (M) holding at
most two item blocks or four tag blocks, and a small size (S)
holding at most one item block or two tag blocks. We tested
two values for δ: 0.02 for simulating a relatively patient (P)
user, and 0.2 for a relatively impatient (I) user. In total,
we had four settings abbreviated as “MP”, “MI”, “SP”, and
“SI”. We found in our experiment runs that varying s0 and
r0 within reasonable ranges did not affect the experiment

Assumption 6.3 (Simple User Interest Action Model). Let
v(e, b) be a measure of the intrinsic relation3 between item
e and block b. Let ε ≥ 0, 0 < δ < 1. Given a user interest
state e0 and an interface card q t , the user issues an action
based on the following simple user interest action model :
1. If the item block for e0 , b0e , is on q t (i.e. b0e ∈ q t ), the
user always selects it: p(b0e |e0 , q t ) = 1, p(b|e0 , q t ) = 0,
t+1 0
0
t
t
∀ b ∈ q t , b 6= b0e , and p(at+1
N |e , q ) = p(aB |e , q ) = 0.
2. Otherwise, if q t contains at least one tag block related
to e0 (i.e. ∃ b ∈ q t s.t. v(e0 , b) > 0), the user will either
select one of these tag block(s), select “next card” or
0
t
stop: p(b|e0 , q t ) = α v(e0 , b), ∀ b ∈ q t , p(at+1
N |e , q ) =
t+1 0
t
−1
|e
,
q
)
=
α
ε
δ,
where
α
=
α
ε
(1
−
δ),
and
p(a
B
P
0
b∈q t v(e , b) + ε.

3. Otherwise, the user either selects “next card” or stops:
t+1 0
0
t
t
t
p(at+1
N |e , q ) = 1−δ, p(aB |e , q ) = δ, and p(b|e, q ) =
0, ∀ b ∈ q t .
The uniform user interest action model denotes the simple
user interest action model where each v(e, b) is either 1 or 0.
Conceptually, ε captures the chance the user misses to
identify a related tag, and δ captures the user’s stopping
tendency given they have not identified any interesting block
- either they missed one or there wasn’t any indeed. Despite
the fact that δ is treated as a constant, we are not assuming a
constant stopping rate; the stopping rate is depending both
3

Simulation Experiments

In this section, we directly use the standard value iteration algorithm for POMDP in sequential decision theories
[20] to explicitly solve our interface optimization problem
defined in Equation (11). We assume the user follows the
uniform user interest action model, and we set ε = 0: the
user never misses a related tag. Similar to [26], we also assume a complete tag set: ∀ item subset E ′ ⊆ E, ∃ block
b s.t. v(e, b) = 1E ′ (e), i.e. 1 if e ∈ E ′ , 0 otherwise. In
addition, we make the following assumption on the card the
interface system could issue in order to reduce to a linear
belief state space:

Please refer to [26] for a detailed explanation.

91

justs the layout to show more tag blocks on both small and
medium screens - with the hope of a better chance to hold
the user onto the interaction when they see a tag related to
their interest. Therefore, our proposed novel formulation of
the Interface Card model in the language of sequential decision theories successfully led to automatic interface layout
optimization results which could not only adapt to user interest and screen size, but also better cater for both patient
and impatient users.

Figure 1: Value function of belief states
10

E(ut |dt )

8
6
MP
MI
SP
SI

4
2
0

0

5

6.2
10

15

20

25

30

||dt ||
Figure 2: Optimal policy for belief states
MP
MI
SP
SI
0

1

2

3

4

5

6

7

8

9

10

||dt ||
only items

items/tags

User Study Experiments

In this section, we apply our theoretical framework to
solve interface optimization problems in real world settings.
The assumptions we made in Section 6.1 would hardly exist
in the real world (e.g. the complete tag set assumption), so
we are again facing a general NP-Hard POMDP planning
problem. We continue to assume that the user follows the
uniform user interest action model, but in contrast to Section 6.1, we now allow ε > 0, which permits the possibility
of the user missing to identify a related tag as is often the
case in the real world.
Instead of using other sophisticated planning algorithms,
we employ a straightforward and widely adopted heuristic in
sequential decision theories, the dual-mode control heuristic
[4], which picks actions (i.e. interface cards in our case)
that lead to a minimal expected entropy value of the belief
state. We slightly modify the heuristic to accommodate user
stopping actions:

only tags

outcome in any fundamental way, and here we report the
results we obtained when we set s0 = 1 and r0 = 10.
Figure 1 shows the value function of belief states as a function of their size. For all four settings, the value function
decreases as the uncertainty of the belief state, i.e. their size,
increases, and the decreasing rates are all diminishing, implying that the interface system could reduce the belief state
size in an exponential manner through interacting with the
user. It is also clear that the value function is higher for
medium screens than for small screens, and higher for patient users than for impatient users: a larger screen naturally
helps the user navigate to their interested items in less laps,
and a more patient user is more likely to stick to the interaction until they obtain the reward. Further, the difference
between the value function for patient and impatient users
is much smaller on the medium screen compared to that on
the small screen, suggesting that a larger screen is helpful in
attracting the user to stick to the interaction by showing a
wide variety of blocks to cater for the user’s interest.
Figure 2 shows the optimal policy our model determined
for each belief state size in terms of the interface layout. The
interface is decided to be full of item blocks when the uncertainty of the belief state is low, and automatically changes to
a combination of item and tag blocks and further to solely
tag blocks as the uncertainty increases, which is a consequence of the tag blocks’ advantage in more quickly narrowing down the belief state size using less screen space as
compared to the item blocks. It is not possible to display
both item and tag blocks in the small screen, so the layout directly “jumps” from all item blocks to all tag blocks,
and the “jump” also takes place when the belief state size is
relatively small as compared to the case on medium screen,
reflecting the more urgent need for more space-efficient tag
blocks on smaller screens. More interestingly, the layout
is also automatically adaptive to user stopping tendencies:
when the user is less patient, our model intelligently ad-

Definition 6.2 (Entropy Heuristic (with stopping actions)).
t
qopt

= argmin
qt

X

at+1 ∈A(q t )

H(d



p(at+1 |dt , q t ) ·

t+1

(25)
) + 1B (a

t+1

)r

0



subject to fct (q t ) ≤ 0. (H(dt+1 ) denotes the entropy of
dt+1 and 1B (at+1 ) is shorthand for the indicator function
of testing whether at+1 = at+1
B .)
The additional term 1B (at+1 ) r0 we put into the dualmode control heuristic measures the eventual reward of finding an interesting item if the user abandons the search (which
we assumed to be uniform for all items and denoted by r0 ).
In order to assess the effectiveness of our proposed model
in automatically optimizing the interface layout of real interactive retrieval systems, we built prototype interface systems similar to those used in [26]: we fetched popular news
articles (as items) from the New York Times Most Popular
API 4 together with their associated keywords (as tags), and
we used Amazon Mechanical Turk (AMT)5 to conduct user
studies. We employed a straightforward randomized algorithm similar to the one used in [26] to select the optimal
interface card in each lap, but based on our new objective
function defined in Equation (25). On the user side, we randomly partitioned the AMT workers into two groups, one
being encouraged to stick to the interaction and thus playing the role of patient users, and the other being encouraged
to freely give up the interaction and thus playing the role of
impatient users; we refer to these two groups respectively as
4
5

92

http://developer.nytimes.com/
http://www.mturk.com

patient (P) and impatient (I) users. On the interface side,
we varied the screen size and developed two sets of interfaces, one for a medium sized screen (M) being able to hold
at most two item blocks or eight tag blocks, and the other
for a very small screen (S) being able to hold at most one
item block or four tag blocks. We again have four settings
in total: “MP”, “MI”, “SP”, and “SI”.
We built two types of interfaces for each of the four settings for comparison. The first type is the baseline interface built based on the Interface Card Model (ICM) [26]
without user stopping tendencies taken into consideration it is essentially always assuming a “perfectly patient” user.
The second type is based on our new Interface Card Model
with User States (ICM-US) with user stopping tendencies
being considered, and it employs a straightforward learning
method to infer users’ stopping tendencies. More specifically, in the uniform user interest action model, if we treat δ
as the only free variable we would make inference about and
all other parameters as given, then the Maximum Likelihood
Estimate (MLE) of δ is:
δM LE =

#(stopping action)
#(stopping action) + #(next page action)

Figure 3: ICM interface after clicking “Colleges and
Universities”.

Figure 4: ICM-US interface after clicking “Colleges
and Universities”.

(26)

where “#()” denotes the number of occurrences of the enclosed action in the interaction log. In our experiment, we
estimated that δM LE = 0.029 and 0.145 respectively for the
group of patient and impatient users. Ideally, we could learn
δM LE for each individual user, but due to the limited amount
of log data we could obtain, we decided to learn its value for
each user group collectively. We noticed that our estimated
δM LE value for both user groups differed within ±8% across
the two screen sizes, implying that our uniform user interest action model is an adequately reasonable assumption for
real world users.
We measure the effectiveness of the two types of interfaces
using two metrics: (a) whether the users end up successfully finding an interesting article or not, referred as “success? ”, and (b) how many laps the user spent for reaching
an interesting article, referred as “#lap”. We use one-sided
McNemar’s test for comparing “success?” and one-sided
Wilcoxon sign-ranked test for comparing “#lap”. Table 1
shows the significance levels of our comparison tests for all
four settings, where we adopt the convention in R 6 and use
“ ”, “⋆” and “⋆⋆” to represent a p-value within the range of
(0.05, 0.1), (0.01, 0.05) and (0.001, 0.01), respectively. It is
clearly observed that our new interface is significantly better than the baseline interface at helping impatient users
navigate to an interesting article without significantly increasing the number of laps they needed; the differences for
patient users are not significant, which is also expected because the two types of interfaces would generate very similar
optimization results due to a very small δ value used in our
new interface.

Interestingly, the differences in “success?” and ”#lap” are
both less significant on the very small screen than on the
medium sized screen for impatient users, contradicting to
our expectation that our new model should benefit smaller
screens more which are in nature less effective in keeping
users engaged with the system. We speculate that, on the
very small screen, both the baseline and the new interface
would decide that the very limited screen space is more suitable for the “keyword layout” in most of the laps where the
system is still very uncertain about the user’s interest, and
the difference only occurs after the system could narrow
down the user’s interest to within a few articles (as was
also observed in Figure 2).
To give better intuitions into the interface optimization
outcomes of our proposed model, we show some example
screens we observed when we used our interfaces ourselves.
Figure 3 and 4 display the second screen we observed on
the very small screen respectively in the baseline interface
and in our new interface (fed with the δM LE value estimated
from the group of impatient users), after we click the “Colleges and Universities” keyword on the first screen (where
both interfaces chose to display four popular keywords due
to the very limited screen space). Given that there are three
news articles associated with the clicked keyword, the baseline interface decided to adopt the “article layout” and display the three articles one by one to the user (of which the
first one is shown in Figure 3), with the hope that the user
will thoroughly go over the three articles with the system.
However, it is possible that an impatient user would stop
if the first shown article does not interest them, and they
would be even more likely to stop if they find the second
shown article again not interesting. The new interface, on
the other hand, considered user stopping tendencies in the
optimization computation and thus chose to adopt the “keyword layout” and display three keywords within the same

·

Table 1: Significance levels of comparison tests.
Setting Sample size “Success?” “#lap”
MP
25
MI
25
⋆⋆
SP
25
SI
24
⋆

·

6

https://www.r-project.org/

93

screen each corresponding to one of the three articles. Even
an impatient user would likely identify a keyword related to
their interest and continue on with the interaction by clicking it, and they would immediately reach their interested
article in the next screen. It is also possible that the user
might fail to recognize a related keyword, but the system
determined that this would be a minor risk and would not
affect the higher overall benefit of attracting an impatient
user to stick longer to the interaction and eventually find
something interesting. Note that all the reasoning above is
only for our purpose to appreciate the intelligent layout decision of the interface system; the interface system is not built
on top of any ad hoc logic to fulfill any part of our reasoning;
its decision relies only on our optimization framework that
elegantly captures all our intuitions in a single formulation.
Similar automatic layout decisions are observed on the
medium sized screen as well, and the screen is even more
capable in interface adaptations thanks to its additional intermediate layout choice of devoting half the space to an
article and half to keywords, but we do not show the example screens here due to space limitations. The results
demonstrate the clear effectiveness of our proposed model
in automatically optimizing interface layouts according to
users’ stopping tendencies.

7.

[6]

[7]
[8]

[9]

[10]

[11]
[12]
[13]

[14]

CONCLUSIONS AND FUTURE WORK

We proposed a novel refinement of the Interface Card
Model based on sequential decision theory, i.e. ICM-US,
that can facilitate formal user modeling and naturally connect optimization of interactive retrieval with Markov Decision Process and Reinforcement Learning (RL) in a general way, thus enabling the use of RL to solve potentially a
wide range of problems of optimal interface design. ICM-US
opens up many opportunities for formally modeling user behavior in interactive retrieval as well as incorporating user
behavior models into an optimal retrieval algorithm, making
the retrieval algorithm “sensitive” to user behavior.
An obvious future direction is to further explore the large
space of specific refinements of ICM-US and apply ICMUS to many applications to optimize interactive search and
interface design. Another very interesting direction is to use
ICM-US to analyze interactive search log data for discovery
of interesting user behavior patterns or testing hypotheses
about users’ search behavior.

[15]

[16]

[17]
[18]

[19]

[20]
[21]

8.

REFERENCES

[1] G. Amati and C. J. Van Rijsbergen. Probabilistic
models of information retrieval based on measuring
the divergence from randomness. ACM Trans. Inf.
Syst., 20(4):357–389, Oct. 2002.
[2] L. Azzopardi. Modelling interaction with economic
models of search. In SIGIR ’14, pages 3–12, 2014.
[3] L. Azzopardi and G. Zuccon. An analysis of theories
of search and search behavior. In ICTIR ’15, pages
81–90, 2015.
[4] A. Cassandra, L. Kaelbling, and J. Kurien. Acting
under uncertainty: discrete bayesian models for
mobile-robot navigation. In IROS ’96, volume 2, pages
963–972 vol.2, Nov 1996.
[5] C. L. Clarke, M. Kolla, G. V. Cormack,
O. Vechtomova, A. Ashkan, S. Büttcher, and
I. MacKinnon. Novelty and diversity in information

[22]

[23]

[24]

[25]

[26]

94

retrieval evaluation. In SIGIR ’08, pages 659–666,
2008.
H. Fang and C. Zhai. An exploration of axiomatic
approaches to information retrieval. In SIGIR ’05,
pages 480–487, 2005.
R. Frank and B. Bernanke. Principles of
Microeconomics. McGraw-Hill, 2009.
N. Fuhr. A probability ranking principle for
interactive information retrieval. Information
Retrieval, 11(3):251–265, 2008.
D. Hiemstra and W. Kraaij. Twenty-one at TREC-7:
ad-hoc and cross-language track. In TREC-7, volume
500-242, pages 227–238, 1999.
K. Järvelin and J. Kekäläinen. Cumulated gain-based
evaluation of ir techniques. ACM Trans. Inf. Syst.,
20(4):422–446, Oct. 2002.
T.-Y. Liu. Learning to rank for information retrieval.
Found. Trends Inf. Retr., 3(3):225–331, Mar. 2009.
J. Luo, X. Dong, and H. Yang. Session search by direct
policy learning. In ICTIR ’15, pages 261–270, 2015.
J. Luo, S. Zhang, and H. Yang. Win-win search:
Dual-agent stochastic game in session search. In
SIGIR ’14, pages 587–596, 2014.
D. Maxwell, L. Azzopardi, K. Järvelin, and
H. Keskustalo. Searching and stopping: An analysis of
stopping rules and strategies. In CIKM ’15, pages
313–322, 2015.
D. McFadden and K. Train. Mixed MNL models for
discrete response. Journal of Applied Econometrics,
15(5):447–470, 2000.
A. Moffat and J. Zobel. Rank-biased precision for
measurement of retrieval effectiveness. ACM Trans.
Inf. Syst., 27(1):2:1–2:27, Dec. 2008.
P. Pirolli and S. Card. Information foraging.
Psychological Review, 106:643–675, 1999.
J. M. Ponte and W. B. Croft. A language modeling
approach to information retrieval. In SIGIR ’98, pages
275–281, 1998.
S. E. Robertson and K. S. Jones. Relevance weighting
of search terms. Journal of the American Society for
Information Science, 27(3):129–146, 1976.
S. Russell and P. Norvig. Artificial Intelligence: A
Modern Approach. Prentice Hall Press, 2009.
G. Salton, A. Wong, and C. S. Yang. A vector space
model for automatic indexing. Commun. ACM,
18(11):613–620, Nov. 1975.
M. Sloan and J. Wang. Dynamic information retrieval:
Theoretical framework and application. In ICTIR ’15,
pages 61–70, 2015.
M. D. Smucker and C. L. Clarke. Time-based
calibration of effectiveness measures. In SIGIR ’12,
pages 95–104, 2012.
H. Turtle and W. B. Croft. Evaluation of an inference
network-based retrieval model. ACM Trans. Inf. Syst.,
9(3):187–222, July 1991.
W.-C. Wu, D. Kelly, and A. Sud. Using information
scent and need for cognition to understand online
search behavior. In SIGIR ’14, pages 557–566, 2014.
Y. Zhang and C. Zhai. Information retrieval as card
playing: A formal model for optimizing interactive
retrieval interface. In SIGIR ’15, pages 685–694, 2015.

