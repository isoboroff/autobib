Generalized BROOF-L2R: A General Framework for
Learning to Rank Based on Boosting and Random Forests
Clebson C. A. de S√°

Marcos A. Gon√ßalves

Daniel X. Sousa

‚àó

Thiago Salles

Federal University of Minas Gerais
Computer Science Department
Belo Horizonte, Brazil

{clebsonc, mgoncalv, danielxs, tsalles}@dcc.ufmg.br
ABSTRACT

able to effectively access relevant information that satisfies
her information needs. Retrieval systems, such as search engines, question and answer, and expert search systems serve
exactly this purpose: given an information need, expressed
in the form of a query, and a set of possible information
units (e.g., documents), the main goal is to provide an ordered list of information units according to their relevance
with relation to the query. The desideratum is to increase
the likelihood of satisfying an user‚Äôs information need in an
effective manner, which translates to maintain the truly relevant results on top of the less relevant ones.
One of the key aspects that influence retrieval systems
is how they determine the relative relevance among candidate results in order to produce a ranked list based on
their relevance with regard to some information need, posed
in the form of a query. The quality of those rankings is
thus paramount to guarantee efficient and effective access
to relevant information (and, hopefully, the satisfaction of
the user‚Äôs information needs). Several approaches to generate such ranked lists do exist, being traditionally performed
by the specification of a function that is able to relate some
user‚Äôs query to the set of known (indexed) information units.
Usually, ranking functions consider several features, such
as those that rely on the relatedness between query and
possible results (e.g., BM25, edit distance, similarities in
vector space models) or on link analysis information (e.g.,
PageRank, HITS). Such features must be somehow combined to provide accurate relevance scores (and, thus, a
properly ranked list of results).
Unfortunately, to specify and tune ranking functions turns
out to be a major problem, specially when the number of features becomes large, with non-trivial interactions. This motivates the use of supervised machine learning techniques to
devise such functions, since machine learning techniques are
effective to combine multiple pieces of evidence towards optimizing some goal. This is the direction pursued by Learning
to Rank (L2R) techniques, the primary focus of this work.
More specifically, based on a set of query-document pairs
with known relevance judgments, the goal is to learn a function f (d, q) that is able to accurately devise the relevance
scores for a document d, with respect to a query q. Due to its
importance, several approaches for L2R have been proposed
in the literature. Ensemble methods, such as RankBoost [7],
AdaRank [32] and Random Forests [1] (and the variations
thereof, such as [11]), are deemed to be the techniques of
choice for L2R, achieving higher effectiveness in published
benchmarks when compared to other algorithms [11, 3]. Both
RankBoost and AdaBoost are based on boosting [26], an

The task of retrieving information that really matters to
the users is considered hard when taking into consideration
the current and increasingly amount of available information. To improve the effectiveness of this information seeking
task, systems have relied on the combination of many predictors by means of machine learning methods, a task also
known as learning to rank (L2R). The most effective learning
methods for this task are based on ensembles of tress (e.g.,
Random Forests) and/or boosting techniques (e.g., RankBoost, MART, LambdaMART). In this paper, we propose a
general framework that smoothly combines ensembles of additive trees, specifically Random Forests, with Boosting in
a original way for the task of L2R. In particular, we exploit
out-of-bag samples as well as a selective weight updating
strategy (according to the out-of-bag samples) to effectively
enhance the ranking performance. We instantiate such a
general framework by considering different loss functions,
different ways of weighting the weak learners as well as different types of weak learners. In our experiments our rankers
were able to outperform all state-of-the-art baselines in all
considered datasets, using just a small percentage of the
original training set and faster convergence rates.

Keywords
Learning to Ranking; Random Forests; Boosting

1.

INTRODUCTION

Today, we live in an era of massive available information,
with a never-seen-before (and increasing) rate of information
production. It is not surprising that such a scenario imposes
hard to tackle challenges. For example, the availability of
massive amounts of data is not of great help if one is not
‚àó
This work was partially funded by projects InWeb (grant
MCT/CNPq 573871/2008- 6) and MASWeb (grant FAPEMIG/PRONEX APQ-01400-14), and by the authors‚Äô individual grants from CNPq, FAPEMIG, Capes and Google
Inc.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ‚Äô16, July 17‚Äì21, 2016, Pisa, Italy.
c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2911540

95

iterative meta-algorithm that combines a series of weaklearners in order to come up with a strong final learner, focusing on hard-to-classify regions of the input space as the
iterations go by. The strategies based on Random Forest
rely on the combination of several decision trees, learned using bootstrapped samples of the training set, together with
additional sources of randomization (such as random feature selection) to produce decorrelated-correlated trees‚Äîa
requirement to guarantee its effectiveness.
In this work, we propose a general framework for L2R,
named Generalized BROOF-L2R that explores the advantages of boosting and Random Forests, by combining them
in a non-trivial fashion. More specifically, at each iteration of the boosting algorithm, a Random Forest model is
learned, considering training examples sampled according to
a probability distribution. Such probability distribution is
updated at the end of each iteration, in order to force the
subsequent learners to focus on hard to classify regions of
the input space. In particular, the use of RF models as
weak learners has its own advantages, since it is capable of
providing robust estimates of expected error through out of
bag error estimates and, by means of selectively updating
the weights of out of bag samples, one can effectively slow
down the tendency of boosting strategies to overfit (a well
known phenomenon that becomes critical as the noise level
of the dataset being analyzed increases).
As we shall detail in the next sections, the key aspects of
the proposed Generalized BROOF-L2R have to do with how
to update the probability distribution and how such update
should be performed, as well as the underlying ranker to be
used to produce the final set of results. In this work, we discuss a set of possible instances of the proposed general framework, in order to highlight the behavior and potential of the
proposed L2R solution. In fact, the instances that makes
use of out-of-bag samples and optimizes through gradient
descent [12] over the residues is able to achieve the strongest
results, in terms of Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG), with significant improvements over the explored adversary algorithms,
considering 5 traditional benchmark datasets. Our alternative instances were also able to achieve competitive (or superior) results when compared to the baselines. Moreover,
as our experimental evaluation shows, our approaches based
on the proposed general framework are able to produce topnotch results with substantially less training samples when
compared to the baselines. Such data efficiency is key to
guarantee practical feasibility as obtaining labeled data is
still a costly process.
To summarize, the contributions of this work are threefold. We provide a general framework for L2R that is able to
combine two strong methods (boosting and Random Forests)
in an original way, which can be specialized in several ways
and produce highly effective L2R solutions. We propose and
discuss a set of alternative instantiations of such a framework, in order to highlight the behavior and effectiveness of
each possible choice. Finally, we advance the state of the
art in L2R by means of some instantiations of our proposed
framework that are able to outperform top-notch solutions,
according to an extensive benchmark evaluation considering
five datasets and seven L2R baseline algorithms.
Roadmap: Section 2 discusses related work. Section 3
presents our proposed Generalized BROOF-L2R framework,
as well as outlines our proposed set of possible instantiations

of the proposed framework. We clarify our experimental
setup and discuss the obtained results in Section 4. Finally,
Section 5 concludes and highlights some future work.

2.

RELATED WORK

Learn to Rank (L2R) [17] is the focus of active developments due to its cross-industry and society importance.
Here, we review some relevant work on this topic, positioning our work in the literature.
L2R attempts to improve traditional strategies for ranking
query results according to some relevance criteria, by exploring supervised machine learning algorithms to combine various relevance related features into a more effective ranking
function, based on a set of queries and associated documents
with relevance judgments. L2R have been successfully applied to a variety of tasks, such as Question and Answer [28],
Recommender [29, 16] and Document Retrieval [14] systems.
Solutions specifically tailored to improve document retrieval have been extensively studied in the past years [4,
19]. In general, there are three major L2R approaches: the
pointwise, pairwise and listwise approaches. Pointwise L2R
algorithms are probably the simplest (yet successful) approaches, directly translating the ranking problem to a classification/regression one. In this case, the training set for the
supervised learning algorithm consists of pairs hqi , (xi,j , yi,j )i
of queries qi and a list of associated documents xi,j , each one
with its relevance judgment yi,j . In this case, each triple
hqi , xi,j , yi,j i is considered to be a single training example.
The goal is to learn a classifier/regressor model capable of
accurately predicting the relevance score of a document x0 ,
with relation to a query qi , thus producing a partial ordering over documents. Pairwise algorithms, on the other
hand, transform the ranking problem into a pairwise classification/regression problem. In this case, learning algorithms
are used to predict orders of document pairs, thus exploring more ground-truth information than the pointwise approaches. Unlike both mentioned strategies, the listwise approaches essentially treat hqi , (xi,j , yi,j )j i as a single training
instance (that is, considering a ranked list of documents for a
query qi as a single training example), capturing more information from the training set (namely, group structure) than
the previous alternatives. Of course, being able to better
capture training data information when learning a ranking
function comes with a price: usually, pairwise and mainly
listwise approaches are harder to train, since they require
more sophisticated (e.g. query-level) loss functions [17].
In terms of the state-of-the-art in L2R, methods based
on Random Forests (RFs) and boosting were shown to be
strong solutions according to already published benchmarks
[22, 11, 3]. More specifically, RFs (and the variations thereof
[11]) as well as boosting algorithms such as Gradient Boosted
Regression Trees (GBRT) [9] and LambdaMART [33], are
considered by many [3, 22, 18] to be the state of the art in
L2R tasks. This work is based on both RFs and boosting
strategies. Thus, in the following we briefly review some
previous literature on them.
The RF algorithm was proposed in [1] as a variation of
bagging of low-correlated decision/regression trees, built with
a series of random procedures, such as bootstrapping of
training data and random attribute selection. The popularity of RFs is highlighted by their successful application in
several domains, such as tag recommendation [3], object segmentation [27], human pose recognition [30] and L2R [3, 22],

96

to name a few. Thus, it is natural to expect several extensions to it, in order to improve its effectiveness even more.
One such extension is the extremely randomized trees (ERT)
model [10] and its application to L2R [11]. The ultimate
goal of ERTs is to reduce the correlation between the trees
composing the ensemble, a requirement to guarantee high
effectiveness of RF models. This is achieved by modifying
the RF algorithm in, essentially, two aspects: each tree is
learned considering the entire training set, instead of bootstrapped samples. Furthermore, in order to determine the
decision splits after the random attribute selection, instead
of selecting a cut-point that optimizes node purity, ERTs
simply select a random cut-point threshold. This ultimately
reduces tree correlation, potentially improving generalization capability of the learned model. As a final remark,
such RF based models can be regarded as nonlinear pointwise approaches for L2R.
Boosting strategies have also been shown to produce state
of the art results on L2R tasks, with GBRT [9] (a.k.a, MART1
(Multiple Additive Regression Trees) and Lambda-MART [33]
as the two perhaps most widely used strategies. Both algorithms are additive ensembles of regression trees. GBRT
learns a ranking function by approximating the root mean
squared error (RMSE) on the training set through gradient
descent. As with typical boosting algorithms, the goal of
GBRT is to focus on regions of the input space where predicting the correct relevance score is a hard task. Since this
algorithm aims at approximating the RMSE on the training data, it can be regarded as a pointwise approach. The
Lambda-MART algorithm, on the other hand, is a listwise
approach that directly optimizes the ranked list of documents according to some retrieval measure, such as NDCG
(instead of simply approximating the RMSE of the training documents relevance scores in isolation). To this end,
Lambda-MART learns a ranking function that generates a
list of relevant documents to a query that is as close as possible to the correct rank. As GBRT, it is based on gradient
descent to optimize such metric.
Due to the successful application of RFs and boosting
in machine learning tasks (such as classification and L2R),
some authors propose to use both strategies in order to come
up with better learned models. For example, in [22] GBRTs
and RFs are independently explored in order to learn better
ranking functions. More specifically, the GBRT model is
initialized with the residues of the RF algorithm, followed
by the traditional iterations of a GBRT model. The main
motivation behind this approach is that RFs are less prone
to overfitting, being ideal to initialize the GBRT algorithm
instead of the usual uniform initialization. According to the
reported benchmark, such strategy was shown to be superior
to the GBRT algorithm.
Unlike [22], in [21] the authors propose an enhanced RF
model for classification by boosting the decision trees composing the ensemble. In this case, each tree is learned with
training examples weighted by wi , resembling boosting by
re-weighting. In particular, training instances with higher
weights influence more when determining the decision nodes
(and cut-point threshold definition). Furthermore, each tree
is evaluated according to this weighted training set, which
enables the ensemble to focus on hard-to-predict regions.
The observed effect of such combination is the ability to
1

come up with high quality models with substantially reduced
training sets. As we shall detail, our proposed framework is
tailored for the L2R task and, instead of introducing boosting into random forests, we apply boosting to several RF
models, which act as weak learners.
Differently from the aforementioned previous work, we
base ourselves in a recent development for text classification,
namely, the BROOF algorithm [25]. In BROOF algorithm,
RF and boosting strategies are tightly coupled in order to
exploit their unique advantages: by exploiting out of bag error estimates as well as selectively updating training weights
according to out of bag samples, the BROOF model is able
to focus on hard-to-classify regions of the input space, without being compromised by the boosting tendency to overfit.
This ultimately leads to competitive results when compared
to state of the art algorithms. In here, we generalize such
approach specifically for L2R tasks in order to come up with
better ranking functions: the Generalized BROOF-L2R. As
we shall see, this general framework is flexible enough so
that it can be instantiated in several ways, exploiting distinct characteristics of the ranking tasks being addressed. In
special, with this general framework we are able to achieve
state of the art results, with rankers superior to the top
notch algorithms proposed so far in all evaluated cases.

3.

GENERALIZED BROOF-L2R

In this section, we detail our proposed Generalized BROOFL2R framework. Briefly speaking, this framework allows the
definition of learners based on the combination of Random
Forests and the Boosting meta-algorithm, in a non-trivial
fashion. As we shall see, this framework establishes a set of
operations to be performed during the boosting iterations,
in a well defined order of application. The goal is to drive
the weak learners towards hard to predict regions of the underlying data representation, in order to come up with an
optimized additive combination of weak learners to form the
final predictor. The extension points of the proposed framework can produce a heterogeneous set of instantiations that
typically produces very competitive results for L2R. In the
following, we present the generalized framework for L2R, as
well as some pointwise instantiations. We stress that the set
of instantiations discussed here is far from exhaustive, being
possible to elaborate even better possibilities in future work.

3.1

Framework Description

Based on the BROOF algorithm, proposed in [25] to solve
text classification tasks, we here extend the proposed ideas
in order to exploit the combination of Random Forests and
Boosting for the specific task of L2R. However, instead of
directly adapting the original algorithm to a single L2R
method, we here generalize it into an extensible framework
that is flexible enough to permit a series of possible instantiations. The proposed framework, named Generalized
BROOF-L2R is an additive model composed of several Random Forest models, which act as weak-learners. Each fitted
model influences the final decision proportionally to its accuracy, focusing ‚Äî as the boosting iterations go by ‚Äî on
ever more complex regions of the input space, in order to
drive down the expected error. As usual in a boosting strategy, two aspects play a key role: (i) the influence Œ≤t of each
learner in the fitted additive model, and (ii) the strategy to
update the sample distribution wi,j in each iteration t of the
boosting meta-algorithm.

From now on, we will use MART and GBRT as synonyms.

97

The basic structure of the framework is outlined in Algorithm 1, together with a brief explanation of what we call
its extension points‚Äîthe general functions exploited by the
framework to determine how the optimization process works.
There are 5 general functions whose purpose is to specify the
weight distribution update process, the error estimation and
the underlying input representation. Particularly, the use of
the Random Forest classifier as a weak learner extends the
range of possible instantiations of the framework, since it
enables us to come up with better error rate estimates and
a more selective approach to update the examples‚Äô weights,
through the use of the so-called out-of-bag samples.
i
Let Qtrn = {(qi , {xi,j , yi,j })|m
j=1 } be the training set, descriptively the set of documents xi,j , with associated graded
relevance judgment yi,j with relation to a query qi . Initially, associate a weight wi,j with each training example
xi,j according to the general function InitializeWeights.
For each boosting iteration t, the input data representation
may be updated, through the general function UpdateExamples. This general function can considerably extend the
range of possible implementations of the framework, allowing us for example, to instantiate a Gradient Boosting Machine algorithm [20, 12]. Then, a Random Forest regressor
model RFt is learned considering this data representation.
In order to evaluate the generalization capabilities of RFt ,
predict yÃÇ for a set of training documents given by ValidationSet. The output of this step is paramount to guide
the optimization process towards hard to classify regions of
the input space. Although being of great importance to
boosting effectiveness, this focus on hard to classify regions
of the input space may also be harmful to the optimization process, specially when dealing with noisy data. As
noted by [8, 13], boosting tends to increase the weights of
few hard-to-classify examples (e.g., noisy ones). Thus, the
decision boundary may only be suitable for those noisy regions of the input space while not necessarily general enough
for general examples. In order to offer a greater robustness
against such a drawback, our framework exposes an intermediary step related to how the examples weights get updates
as the boosting iterations go by. The general function ValidationSet serves the purpose of specifying which training
examples should be used during error estimation and weights
update. The main goal here is to provide some mechanism
to slowdown overfitting as well as provide more robust estimates of error weight (to capture the generalization power
of each weak learner and to determine how they should influence the final predictor).
The selected training examples are then used to compute
both the error rate of the model and the influence Œ≤t of
the weak learner on the final model, through ComputeLearnerWeights. Finally, the training examples‚Äô weight
distribution is updated by UpdateExampleWeights. This
update process should, ideally, take into account the generalization capability of the current weak learner RFt , as well
as how hard is to correctly predict the ranked lists of the
validation examples. Validation examples whose outcome
is hard to predict by an accurate learner should influence
more in the following boosting iterations. An early stopping
strategy is adopted, terminating the boosting iterations if
the current learner has an estimated error rate greater than
0.5. The final prediction rule is then given by an additive
combination of the weak-learners RFt , weighted by Œ≤t .

Instantiation

Description
Extension Point

Variation

BROOFabsolute

InitializeWeights
UpdateExamples
ValidationSet
ComputeLearnerWeights
UpdateExampleWeights

Uniform
Identity
OOB
Absolute
OOB

BROOFmedian

InitializeWeights
UpdateExamples
ValidationSet
ComputeLearnerWeights
UpdateExampleWeights

Uniform
Identity
OOB
Median
OOB

BROOFheight

InitializeWeights
UpdateExamples
ValidationSet
ComputeLearnerWeights
UpdateExampleWeights

Uniform
Identity
OOB
Height
OOB

BROOFgradient

InitializeWeights
UpdateExamples
ValidationSet
ComputeLearnerWeights
UpdateExampleWeights

Uniform
Residue
OOB
Constant
Constant

Table 1: Generalized BROOF-L2R: Possible instantiations.

3.2

Possible Instantiations

In this section, we describe a set of possible instantiations
of the proposed framework. Due to space limitations, we
here focus on four possible instantiations, stressing that this
is far from being an exhaustive list of possibilities. In fact,
we consider some representative alternatives that highlight
the flexibility of the proposed framework to produce L2R
solutions that typically produces very competitive results.
In order to induce a L2R algorithm based on the Generalized BROOF-L2R framework, one needs to specify the 5
generic functions discussed earlier. Our proposed instantiations can be found in Table 1. In that table, we specify which
alternative was chosen for each generic function, providing
details on how they are implemented.
As it can be observed, BROOFabsolute , BROOFmedian and
BROOFheight rely on out-of-bag samples in order to drive
the boosting meta-algorithm further on hard to predict regions of the input space. Such samples are explored when
estimating the weak-learner‚Äôs error rate through out-of-bag
estimates. Recall that in boosting, the usual way of assessing the errors is to use the training to measure the error.
This is too optimistic, since the same data that was used
to train the model is used as a measure of error. By using
the out-of-bag samples we are able to produce better error
estimates, since the out-of-bag are an independent set of
samples that was left apart during the construction of the
model. Thus, it is able to better approximate the expected
error rate of the learner and is a more reliable measure than
the usual training error rate [1].
In addition, the out-of-bag errors estimates are used to
identify the weights‚Äô distribution that should be applied
on following iterations of the boosting procedure; allowing
the model to focus on hard to predict regions of the input space. We hypothesize that such selective update strategy can slowdown the algorithm‚Äôs tendency to overfit. The
major difference between them relates on how each weaklearner influence on the final predictor. The proposed instantiations can be found outlined in Algorithms 2 to 4.
More specifically, we considered the absolute regression loss,
|yi,j ‚àí yÃÇi,j |, computed for the out-of-bag samples. We call

98

Algorithm 1 Generalized BROOF-L2R: Pseudocode
i
1: function Fit(Qtrn = {(qi , {xi,j , yi,j }|m
j=1 }, max iter=M , num trees=N , shrinkage=Œ∑)
2:
wi,j =InitializeWeights(Qtrn )
3:
x0i,j ‚Üê yi,j
4:
for each t = 1 to M do
5:
x0i,j ‚ÜêUpdateExamples(Qtrn , x0i,j )
6:
RFt ‚Üê RFRegressor .Fit({(x0i,j , yi,j )}, N )
t
t
7:
{(yÃÇi,j
, yi,j
)} ‚ÜêValidationSet(RFt , Qtrn )
t
t
t
8:
hei,j , Œ≤t i ‚ÜêComputeLearnerWeights(RFt , {(yÃÇi,j
, yi,j
)})
P
t
9:
if
e
w
‚â•
0.5
then
i,j
i,j i,j
10:
break
11:
end if
t
t
12:
wi,j ‚ÜêUpdateExampleWeights(eti,j , Œ≤t , {(yÃÇi,j
, yi,j
)})
13:
end for
M
14:
return {(RFt , Œ≤t )}|t=1
15: end function
Function

Description

InitializeWeights

Initial weights associated to each example, ressambling boosting by re-weighting.
Uniform: Equal weights for each example, wi,j = P 1m .
i,j

i,j

Random: Randomly initialized weights, wi,j =Random(), 0 ‚â§ wi,j ‚â§ 1.
UpdateExamples

ValidationSet

ComputeLearnerWeights

UpdateExampleWeights

Determines the underlying representation of the input data, directly defining what the algorithm should
optimize for.
Identity: Maintains the original representation of input data, x0i,j = xi,j .
 0
t‚àí1
xi,j ‚àí Œ∑ yÃÇi,j
if t > 1
Residue: Optimizes for the residues: x0i,j =
, where Œ∑ is a shrinkage factor.
yi,j otherwise
Determines which training data will be considered during weight update and error rate estimation, with
direct influence on the algorithm robustness to overfitting.
OOB: The set of out of bag examples OOBt related to RFt .
Train: The entire training set Qtrn .
Determines how to compute the influence
of the current weak learner on the final predictor.
P
t
t

, where  =
Absolute: Œ≤t = Œ∑ 1‚àí
yÃÇ | and Œ∑ is a shrinkage factor.
i,j ei,j wi,j , ei,j = |yi,j ‚àí
P i,j t

Median: Similarly to the above variant, Œ≤t = Œ∑ 1‚àí
and  =
i,j ei,j wi,j . However, the errors are given
by eti,j = |Median(RyÃÇi,j ) ‚àí yÃÇi,j | where Ri denotes the list of predictions yÃÇi,j associated to documents
whose real relevance score is i.
P
t
t

Height: Similarly to the variants above, both Œ≤t = Œ∑ 1‚àí
and  =
i,j ei,j wi,j . Unlike them, ei,j =

# irrelevant documents above xi,j if xi,j is relevant
, in the ordered list of results.
# relevant documents below xi,j otherwise
Constant: Produces constant coefficients, Œ≤t = Œ∑.
Specifies how to update the training examples weights to be used in the next iteration.
OOB: Updates the weights associated to the out of bag samples according to Œ≤t and the difficulty involved
1‚àíet

in predicting the samples‚Äô outcomes. More specifically, wi,j = wi,j Œ≤t i,j
Train: Updates the weights associated to the entire training set. Similarly to the above variant, the
update strategy considers both the coefficient Œ≤t and the error eti,j .
Constant: Keeps the same weights during the boosting iterations, wi,j = wi,j .

this variant BROOFabsolute . We also considered two other
alternatives, that rely on the position of documents in the
predicted ranked lists. One alternative, named BROOFL2Rmedian , relies on the intuition that documents with the
same relevance judgment should be as nearer as possible to
each other on the current ranked list. We thus consider as
loss |Median(RyÃÇi,j ) ‚àí yÃÇi,j | where Ri denotes the list of predictions yÃÇi,j associated to documents whose real relevance
score is i. The second alternative, named BROOFheight ,
is inspired on ideas of [5]. We define the height of a document xi,j as the total number of irrelevant documents ranked
higher then xi,j if xi,j is relevant, or the total number of relevant documents ranked below xi,j if it is an irrelevant one.
Finally, in order to illustrate the generality of our proposed framework, we provide a fourth instantiation, BROOFgradient , that resembles the gradient boosting machines (GBM),
that optimizes through gradient descent [12] over the residues. More specifically, by a suitable combination of alternative implementations for each general function outlined in
Algorithm 1, one can come up with an algorithm that could
be named Gradient Boosted Random Forests (GBRF). This

is achieved by considering an alternative representation of
input data, that optimizes for the residues, such as y ‚àí yÃÇ, instead of the original input representation, updating them according to the negative gradient of the cost function (in this
case, RMSE). Such alternative is outlined in Algorithm 5.
As we shall see in our experimental evaluation (Section 5),
our proposed instantiations achieve very strong results compared to seven state-of-the-art baselines algorithms in five
representative datasets. In particular, BROOFabsolute and
BROOFgradient were shown to be the strongest algorithms,
obtaining significant improvements over the best baselines.

4.

EXPERIMENTAL EVALUATION

We conducted extensive experiments in well-known L2R
benchmarks. In the following, we describe the characteristics of the used datasets, the baseline algorithms, the experimental protocol/setup and the experimental results.

4.1

Datasets

The corpus we use are freely available online for scientific
purposes. Such datasets can be divided into two groups

99

Algorithm 2 BROOFabsolute : Pseudocode
i
1: function Fit({(qi , {xi,j , yi,j }|m
j=1 }, M , N , Œ∑)
2:
wi,j = P 1m
i,j
i,j
3:
x0i,j ‚Üê yi,j
4:
for each t = 1 to M do
5:
x0i,j ‚Üê xi,j
6:
RFt ‚Üê RFRegressor .Fit({(x0i,j , yi,j )}, N )
t
t
7:
{(yÃÇi,j
, yi,j
)} ‚Üê RFt .OOB
t
8:
ei,j ‚Üê |yi,j ‚àí yÃÇi,j |
P
t
9:
‚Üê
i,j ei,j wi,j

10:
Œ≤t ‚Üê Œ∑ 1‚àí
11:
if  ‚â• 0.5 break
1‚àíet
12:
wi,j ‚Üê wi,j Œ≤t i,j
13:
end for
14:
return {(RFt , Œ≤t )}|M
t=1
15: end function

Algorithm 4 BROOF-L2Rheight : Pseudocode
i
1: function Fit({(qi , {xi,j , yi,j }|m
j=1 }, M , N , Œ∑)
2:
wi,j = P 1m
i,j
i,j
3:
x0i,j ‚Üê yi,j
4:
for each t = 1 to M do
5:
x0i,j ‚Üê xi,j
6:
RFt ‚Üê RFRegressor .Fit({(x0i,j , yi,j )}, N )
t
t
7:
{(yÃÇi,j
, yi,j
)} ‚Üê RFt .OOB

8:

9:
10:
11:
12:
13:
14:
15:

Algorithm 3 BROOFmedian : Pseudocode
i
1: function Fit({(qi , {xi,j , yi,j }|m
j=1 }, M , N , Œ∑)
1
P
2:
wi,j =
i,j mi,j
3:
x0i,j ‚Üê yi,j
4:
for each t = 1 to M do
5:
x0i,j ‚Üê xi,j
6:
RFt ‚Üê RFRegressor .Fit({(x0i,j , yi,j )}, N )
t
t
7:
{(yÃÇi,j
, yi,j
)} ‚Üê RFt .OOB
8:
eti,j ‚Üê Median(pos(yi,j ) ‚àí pos(yÃÇi,j ))
P
t
9:
‚Üê
i,j ei,j wi,j

10:
Œ≤t ‚Üê Œ∑ 1‚àí
11:
if  ‚â• 0.5 break
1‚àíet
12:
wi,j ‚Üê wi,j Œ≤t i,j
13:
end for
14:
return {(RFt , Œ≤t )}|M
t=1
15: end function

if xi,j is relevant
otherwise

1‚àíet

wi,j ‚Üê wi,j Œ≤t i,j
end for
return {(RFt , Œ≤t )}|M
t=1
end function

Algorithm 5 BROOFgradient : Pseudocode
i
1: function Fit({(qi , {xi,j , yi,j }|m
j=1 }, M , N , Œ∑)
2:
wi,j = P 1m
i,j
i,j
3:
x0i,j ‚Üê yi,j
4:
for each t 
= 1 to M do
t‚àí1
x0i,j ‚àí Œ∑ yÃÇi,j
if t > 1
5:
x0i,j ‚Üê
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

considering the relevance judgments and their sizes. The two
largest datasets contain hquery, documenti pairs with five
relevance levels, ranging from 0 (completely irrelevant) to 4
(highly relevant). In this group we have one dataset from the
‚ÄúYAHOO! Webscope Learning to Rank Challenge‚Äù, divided
into three partitions for training, validation and test. The
second largest dataset, WEB10K, consists of 10, 000 queries
released by Microsoft. In contrast to the YAHOO! datasets,
the Microsoft dataset is partitioned into 5 folds for crossvalidation purposes, with 3 partitions used for training, 1
for validation and 1 for test.
The second group of datasets corresponds to well-known
LETOR 3.0 Topic distillation tasks, TD2003 and TD2004
(a.k.a., informational queries), of the Web track of the Text
Retrieval Conference 2003 and 2004. These datasets contain binary relevance judgments. Similarly to the WEB10K
benchmark, these datasets are partitioned into 5 folds to be
used in a folded cross-validation procedure.
For comparative purposes, considering that the Microsoft
and LETOR datasets were designed for a folded cross-validation procedure, we applied this same strategy to the YAHOO! dataset by merging the original partitions into a single
set, and splitting the sorted queries into 5 folds, distributed
using the same proportions: 3 folds for training, 1 for validation and 1 for test. We report results for both splits: the
original one (called YAHOOV1S2) and the new 5-fold split
(called YAHOOV1S2-F5).

4.2

# irrelevant documents above xi,j
eti,j ‚Üê
# relevant documents below xi,j
P
t
‚Üê
i,j ei,j wi,j

Œ≤t ‚Üê Œ∑ 1‚àí
if  ‚â• 0.5 break

yi,j otherwise
RFt ‚Üê RFRegressor .Fit({(x0i,j , yi,j )}, N )
t
t
{(yÃÇi,j
, yi,j
)} ‚Üê RFt .OOB
eti,j ‚Üê |yi,j ‚àí Œ∑ yÃÇi,j |
P
t
‚Üê
i,j ei,j wi,j
Œ≤t ‚Üê Œ∑
if  ‚â• 0.5 break
wi,j ‚Üê wi,j
end for
return {(RFt , Œ≤t )}|M
t=1
end function

able implementations of state-of-the-art L2R methods, including AdaRank (with MAP and NDCG as loss functions),
Random Forests, SVMrank , MART, LambdaMART and RankBoost. We used the RankLib2 (under the Lemur project)
implementations of RankBoost, MART and LambdaMART.
For AdaRank we used the implementation freely available
at Microsoft Research3 . For SVMrank , we used the original implementation of [15]4 . Finally, for Random Forests,
we used the implementation available in Scikit-Learn[24] library, which is also the basis of our implementations.

4.3

Experimental Protocol and Setup

To validate the performance of our approaches, we use
two statistical tests to assess the statistical significance of
our results, namely, the Wilcoxon signed-rank test and the
paired Student‚Äôs t-test. We consider the Wilcoxon signedrank test since it is a non-parametric statistical hypothesis
testing procedure that requires no previous knowledge of the
samples distribution. In fact, some authors believe that it is
one of the best choices for the analysis of two independent
samples [6]. However, there is also some discussion in the
literature favoring the Student‚Äôs t-test when comparing L2R
methods [23]. Due to the lack of consensus, we perform our
2

http://sourceforge.net/p/lemur/wiki/RankLib/
http://research.microsoft.com/en-us/downloads/
0eae7224-8c9b-4f1e-b515-515c71675d5c/
4
https://www.cs.cornell.edu/people/tj/svm light/svm
rank.html
3

Baselines

In our experiments we consider as baselines freely avail-

100

analysis with both tests, considering a two-sided hypothesis
with significance level of 0.95% in both tests.
The statistical tests are computed over the values for Mean
Average Precision (MAP) and the Normalized Discounted
Cumulative Gain at the top 10 retrieved documents (hereafter, NDCG@10), the two most important and frequently
used performance metrics to evaluate a given permutation
of a ranked list using binary and multi-relevance order [31].
To compute these metrics we used the standard evaluation
tool available for the LETOR 3.0 benchmark (for binary
datasets), as well the tool available for the Microsoft dataset
for all multi-label relevance judgment datasets 5 . For MAP,
let Q be the set of all queries. These tools simply compute
X AveragePrecision(q)
.
M AP =
|Q|
q‚ààQ

values of 0.025, 0.05, 0.075 and 0.1. The best found values
for the MART and LambdaMART were ensembles of 1000
trees with shrinkage factor Œ∑ of 0.1. For the AdaRankM AP ,
AdaRankN DCG@5 and for the RankBoost algorithm, similar
procedures were performed in the validation set to configure
the number of iterations.
Finally, we performed 5, 10 and 30 runs of the 5-fold cross
validation procedure for WEB10K, YAHOO! and LETOR
datasets, respectively. The differences in the number of repetitions are due to the size of the datasets and the need to
properly address the variance of the results. The reported
results on Tables 2 and 3 are the average of all these runs,
being the statistical tests applied to these results.

4.4

Regarding NDCG, we assume that NDCG@p is 0 (zero)
for empty queries, i.e., queries with no relevant documents.
Some of the available evaluations tools (e.g., the one from
YAHOO!) assume the value of 1 for these cases, which may
lead to higher values of NDCG [2]. We chose to standardize
this issue, using the same criterion used by most evaluation
tools, e.g., those available for the Letor (3.0 and 4.0) and
Microsoft datasets, in order to allow fairer comparisons. Accordingly, let IDCGp be the maximum possible discounted
cumulative gain for a given query. These tools implement
NDCG@p as follows:
p

N DCG@p =

Results

In this section we analyze our proposals in terms of effectiveness, comparing them to the 7 explored baseline algorithms on the 5 described datasets. The results are reported
on Tables 2 and 3.
We start by considering the MAP metric (Table 2). Briefly,
the MAP results show that, overall, our proposed framework
outperforms or ties with the strongest baselines in all cases.
More specifically, with the TD2003 dataset, BROOFheight
outperformed the strongest baseline (RF) considering both
statistical tests, with BROOFabsolute and BROOFmedian as
the winners according to at least one statistical test. In this
dataset, BROOFgradient was statistically tied with the best
baseline. Considering TD2004, BROOFabsolute was considered the top performer amongst the proposed solutions, being tied with the strongest baseline ‚Äì RankBoost ‚Äì in this
dataset. Regarding the WEB10K dataset, we can see that
BROOFgradient was the top performer, according to both
statistical tests, being superior to MART, the strongest baseline. Finally, in the YAHOOV1S2 dataset all four proposed
algorithms were statistically superior to the strongest baseline (RF) according to both statistical tests, whereas in the
YAHOOV1S2-F5 dataset BROOFgradient was the best approach. In sum, according to the MAP metric, our results
clearly show that the proposed instantiations of the Generalized BROOF framework produced very competitive results
as the best algorithm, being superior in the majority of the
cases (and tying in the others) ‚Äì a very significant result.
Turning our attention to the NDCG results, reported on
Table 3, a similar behavior can be observed: our proposed
instantiations are no worse than the strongest baselines in
all cases, being superior in the majority of cases. Considering the TD2003 and TD2004 datasets, our solutions were
no worse than any baseline, being statistically tied with the
strongest one (RF, in both cases). BROOFgradient was the
best algorithm in the three remaining datasets, according to
both employed statistical tests. Furthermore, BROOFmedian
was also superior to the best baseline (MART) in the YAHOOV1S2 dataset (according to the Student‚Äôs t-test), with
BROOFabsolute and BROOFheight tied with the MART algorithm. Again, this set of results highlights the effectiveness
of the proposed approaches.
We now turn our attention to some behavioral aspects
of our algorithms, namely, convergence and learning efficiency. In order to better understand the convergence rate
of our proposals, we provide an empirical evaluation of our
most effective solution (i.e., BROOFgradient ), by analyzing
the obtained NDCG@10 as we vary the number of boosting iterations, contrasting these results with the boosting

X 2reli ‚àí 1
DCGp
, where DCGp =
.
IDCGp
log2 (i + 1)
i=1

In terms of algorithm tuning, we follow the usual procedure of tuning the hyper-parameters using training and
validation sets. Considering the Random Forest based approaches we vary the number of trees ranging from 10 to
1000. We achieved convergence around 300 trees, We also
optimized the percentage of features to be considered as candidates during node splitting, as well as the maximum allowed number of leaf nodes. The optimal values were 0.3
and 100, respectively.
For BROOFabsolute , BROOFmedian and BROOFheight , we
limited the number of iterations to 500, reminding that the
algorithms have an early stopping criterion that prevents
further boosting iterations when the error rate exceeds 0.5.
On average, our strategies converge at about 15 iterations
on the LETOR datasets, and around 5 to 10 iterations on
the multi-relevance judgment datasets. An exception was
BROOFgradient which converged at about 100 iterations for
the largest datasets.
Concerning the SVMrank baseline, we favored the use of
a linear kernel considering the fact that we verified in our
analysis that a polynomial kernel is infeasible on large scale
benchmarks such as WEB10K. The cost parameter C was
calibrated using the training and validation sets with the
explored values: 0.001, 0.01, 0.1, 1, 10, 100 and 1000. For
the boosting methods Mart and LambdaMART, we tuned,
always considering the validation set, the number of iterations ranging from one to a hundred, with a step of 1, and
then scaling it up to 1000 iterations, with steps of 100. For
the shrinkage factor of the predictive models, we tested the
5

Reminding that, at the time of the writing of this paper,
the evaluation tool used in the YAHOO! competition was
not available online.

101

Datasets
YAHOOV1S2

YAHOOV1S2-5F

Baselines

Mart
LambdaMart
RF
RankBoost
AdaRank-MAP
Adarank-NDCG
SVM-Rank

0.192633
0.165181
0.278644
0.235189
0.2003
0.121672
0.257490

0.193744
0.169605
0.2522
0.255467
0.196801
0.132435
0.220392

0.352491
0.350263
0.337702
0.316201
0.294792
0.304359
0.324552

0.559721
0.5545
0.563355
0.544887
0.413846
0.540243
0.544887

0.568821
0.563694
0.559019
0.547524
0.480190
0.538514
0.551333

BROOF

Algorithm
TD2003

TD2004

WEB10K

BROOFabsolute
BROOFmedian
BROOFheight
BROOFgradient

0.288039+
0.282427‚àó
0.285937‚àó
+
0.280634

0.263288
0.259941
0.259058
0.252342

0.342437
0.347665
0.340340
0.36251‚àó
+

0.565486‚àó
+
0.567696‚àó
+
0.564774‚àó
+
0.572918‚àó
+

0.563729
0.567284
0.557727
0.57656‚àó
+

‚Äò‚àó‚Äô: better than the strongest baseline, with statistical significance according to Wilcoxon Test
‚Äò+‚Äô: better than the strongest baseline with statistical significance according to Student‚Äôs t-test
‚Äòn‚Äô: statistically tied results considering both tests

Table 2: Mean Average Precision (MAP): Obtained results.
Datasets
TD2003

TD2004

WEB10K

YAHOOV1S2

YAHOOV1S2-5F

Baselines

Mart
LambdaMart
RF
RankBoost
AdaRank-MAP
AdaRank-NDCG
SVM-Rank

0.271274
0.224536
0.36346
0.31613
0.271921
0.166241
0.344177

0.263926
0.237338
0.350582
0.33399
0.281035
0.182031
0.303471

0.4404
0.445437
0.424498
0.397071
0.35732
0.385761
0.399902

0.703757
0.69619
0.703139
0.682478
0.51767
0.66309
0.682478

0.714763
0.706287
0.702384
0.681796
0.607867
0.664115
0.691064

BROOF

Algorithm

BROOFabsolute
BROOFmedian
BROOFheight
BROOFgradient

0.360802
0.36798
0.368195
0.368695

0.358146
0.350466
0.355356
0.348532

0.434964
0.436284
0.42882
0.456081‚àó
+

0.70633
0.708538+
0.70383
0.717271‚àó
+

0.706954
0.709148
0.701985
0.725129‚àó
+

‚Äò‚àó‚Äô: better than the strongest baseline, with statistical significance according to Wilcoxon Test
‚Äò+‚Äô: better than the strongest baseline, with statistical significance according to Student‚Äôs t-test
‚Äòn‚Äô: statistically tied results considering both tests

Table 3: Normalized Discounted Cumulative Gain (NDCG@10): Obtained results.
baselines. We here focus on the three largest datasets: YAHOOV1S2, YAHOOV1S2-F5 and WEB10K. Results can be
found on Figure 1. As it can be observed, BROOFgradient
share similar behavior with three explored boosting algorithms, namely, MART, RankBoost and AdaRank-NDCG:
the four algorithms show fast convergence rates. The two
key differences are: (i) our approach is able to achieve significantly better results at the initial boosting iterations
and (ii) BROOFgradient converges to a higher asymptote
than the other algorithms. On the other hand, the convergence rate of LambdaMART was significantly slower than
the convergence rate of the mentioned algorithms. In sum,
BROOFgradient enjoys faster convergence rates, with higher
NDCG values at the initial boosting iterations and higher
asymptote. This is paramount to guarantee practical feasibility of our solution: although high effectiveness is a requirement, achieving such high effectiveness with just a few
boosting iterations is key to minimize running time.
Another aspect of direct impact on the practical feasibility
of the solutions is to what extent the algorithms are ‚Äúdata
efficient‚Äù. That is, to what extent each algorithm is capable
of delivering highly effective rankings with reduced training sets. We evaluate the solutions under this dimension
by analyzing each algorithm‚Äôs learning curve. To this end,
we measure the effectiveness of each algorithm as we vary
training set size. We randomly sample s% examples from
the training set, selected at random. We vary s from 10% to
100%, with steps of 10%. The obtained results can be found

on Figure 2. Considering the WEB10K dataset, we can observe a surprising result: BROOFgradient is able to outperform all algorithms with just 20% of the training set, even
considering the other algorithms trained with larger training sets (including the entire training set). Also, it can be
noted that BROOFabsolute is no worse than the baseline algorithms, even with 10% of the training set. In fact, with about
40% of the training set BROOFgradient is able to achieve its
maximum effectiveness, whereas for BROOFabsolute 10% is
enough. For the YAHOO datasets, a similar behavior was
observed: with about 20% to 30% of the training set our
approaches were able to outperform the baseline algorithms
(or match, in the case of BROOFabsolute ), even considering
the baseline algorithms trained with the entire training set.
In these datasets, our algorithms were able to achieve maximum effectiveness at 50% to 80% of the training set. Considering the TD2003 and TD2004 datasets, the RF baseline
was a bit more competitive to our approaches, exhibiting
a similar behavior in terms of effectiveness as the training
set size varies. In these datasets, 50% to 60% of the training set was enough to produce the best effectiveness on the
TD2003, while 40% was enough to surpass all baselines on
TD2004. These findings have also a direct influence on the
practical feasibility of our solutions. First, smaller training
sets translates to smaller runtimes. Second, obtaining labeled data is critical but also costly. Clearly, being able to
produce highly effective models from reduced training sets
is an important characteristic of a successful approach.

102

Figure 1: Convergence analysis: NDCG as the number of boosting iterations increases.

Figure 2: Learning curve analysis for the boosting algorithms.
Finally, we turn our attention to the effect of the use of
out-of-bag samples by our approaches. Due to space restrictions, we here focus on BROOFgradient , considering the
WEB10K dataset. We analyze the effect of weak-learner error rate estimation through out-of-bag samples by contrasting it with a variant whose generic function ValidationSet
equals to Train. The effectiveness of BROOFgradient and
the mentioned variation, as the boosting iterations go by,
can be found on Figure 3. From that figure, it is clear that
the out-of-bag error estimation produces more effective results than the simple training error estimate. In fact, for all
boosting iterations, the BROOFgradient variation with ValidationSet set to OOB produces better results than the
results obtained with ValidationSet set to Train. This
highlights the importance of exploiting the out-of-bag error estimates in our proposed framework instantiations. As
a final remark, as it can be observed in Figure 3, even the
variant that uses the training error rate is able to outperform
the explored baselines. This is also an important aspect that
highlights the quality of the proposed framework.

5.

Figure 3: BROOFgradient : Effect of out-of-bag samples versus entire training set.
dom Forests and Boosting. Such combination, that uses
Random Forests models as weak-learners for the boosting
algorithm, relies on the use of the out of bag samples produced by the Random Forests to (i) determine the influence
of each weak-learner in the final additive model and (ii)
update the sample distribution weights by means of a more
reliable error rate estimate. In fact, the framework is general
enough to provide a rather heterogeneous set of instantiations that, according to our empirical evaluation, are able to

CONCLUSIONS AND FUTURE WORK

In this work, we propose an extensible framework for L2R,
called Generalized BROOF-L2R, which smoothly combines
two successful strategies for Learning to Rank, namely, Ran-

103

achieve competitive results compared to state-of-the-art algorithms for L2R. We proposed four different instantiations.
Three instantiations closely follows the ideas of a recently
proposed algorithm for text classification, namely, BROOF.
The fourth instantiation is based on gradient descent optimization, resembling gradient boosting machines. In fact,
such instantiation can be seen as a gradient boosted random
forests model. As our results show, despite the fact that all
the four algorithms provide very competitive results, two of
them are consistently the top-performers, highlighting the
quality and effectiveness of our proposed framework. Also,
our proposals have two properties that are paramount to
guarantee their practical feasibility, namely, data efficiency
and fast convergence rates.
The space of possible instantiations of the proposed general framework for L2R is rather large. This clearly makes
room for further investigations regarding such possibilities.
In fact, one can come up with improved instantiations of
the framework, by means of extending the set of possible
implementations for each generic function composing the
framework. This is under investigation. We also plan to
study a more comprehensive set of instantiations, in order
to build a substantially larger catalog of algorithms based
on the Generalized BROOF-L2R to better understand the
effects of each choice on model effectiveness.

[14] T. Joachims. Optimizing search engines using clickthrough
data. In ACM SIGKDD, pages 133‚Äì142, 2002.
[15] T. Joachims. Training linear svms in linear time. In ACM
SIGKDD, pages 217‚Äì226, 2006.
[16] A. Karatzoglou, L. Baltrunas, and Y. Shi. Learning to rank for
recommender systems. In ACM RecSys, pages 493‚Äì494, 2013.
[17] T.-Y. Liu. Learning to rank for information retrieval. Found.
Trends Inf. Retr., 3(3):225‚Äì331, 2009.
[18] C. Lucchese, F. M. Nardini, S. Orlando, R. Perego,
N. Tonellotto, and R. Venturini. Quickscorer: A fast algorithm
to rank documents with additive ensembles of regression trees.
In SIGIR, pages 73‚Äì82, 2015.
[19] C. Macdonald, R. L. Santos, and I. Ounis. The whens and hows
of learning to rank for web search. Inf. Retr., 16(5):584‚Äì628,
2013.
[20] L. Mason, J. Baxter, P. Bartlett, and M. Frean. Boosting
algorithms as gradient descent. In In Advances in Neural
Information Processing Systems, pages 512‚Äì518, 2000.
[21] Y. Mishina, R. Murata, Y. Yamauchi, T. Yamashita, and
H. Fujiyoshi. Boosted random forests. IEICE Transactions,
98-D(9):1630‚Äì1636, 2015.
[22] A. Mohan, Z. Chen, and K. Weinberger. Web-search ranking
with initialized gradient boosted regression trees. JMLR
Workshop and Conference Proceedings: Proceedings of the
Yahoo! Learning to Rank Challenge, 14:77‚Äì89, 2011.
[23] L. a. F. Park. Confidence Intervals for Information Retrieval
Evaluation. Australiasian Document Computing Symposium,
2010.

References
[1] L. Breiman. Random forests. Mach. Learn., 45(1):5‚Äì32, 2001.

[24] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,
V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau,
M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:
Machine learning in Python. J. Mach. Learn. Res.,
12:2825‚Äì2830, 2011.

[2] R. Busa-Fekete, B. KeÃÅgl, T. EÃÅltetoÃã, and G. Szarvas. Tune and
mix: learning to rank using ensembles of calibrated multi-class
classifiers. Machine Learning, 93(2):261‚Äì292, 2013.
[3] S. D. Canuto, F. M. BeleÃÅm, J. M. Almeida, and M. A.
GoncÃßalves. A comparative study of learning-to-rank techniques
for tag recommendation. JIDM, 4(3):453‚Äì468, 2013.
[4] O. Chapelle and Y. Chang. Yahoo! learning to rank challenge
overview. JMLR - Proceedings Track, 14:1‚Äì24, 2011.

[25] T. Salles, M. GoncÃßalves, V. Rodrigues, and L. Rocha. Broof:
Exploiting out-of-bag errors, boosting and random forests for
effective automated classification. In SIGIR, pages 353‚Äì362,
2015.

[5] K. Christakopoulou and A. Banerjee. Collaborative ranking
with a push at the top. In WWW, pages 205‚Äì215, 2015.

[26] R. E. Schapire and Y. Freund. Boosting: Foundations and
Algorithms. 2012.

[6] J. DemsÃåar. Statistical comparisons of classifiers over multiple
data sets. J. Mach. Learn. Res., 7:1‚Äì30, 2006.

[27] F. Schroff, A. Criminisi, and A. Zisserman. Object class
segmentation using random forests. In British Machine Vision
Conf., pages 1‚Äì10, 2008.

[7] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An efficient
boosting algorithm for combining preferences. J. Mach. Learn.
Res., 4:933‚Äì969, 2003.

[28] A. Severyn and A. Moschitti. Learning to rank short text pairs
with convolutional deep neural networks. In ACM SIGIR,
pages 373‚Äì382, 2015.

[8] Y. Freund and R. E. Schapire. Experiments with a New
Boosting Algorithm. In ICML, pages 148‚Äì156, 1996.

[29] Y. Shi, M. Larson, and A. Hanjalic. List-wise learning to rank
with matrix factorization for collaborative filtering. In ACM
RecSys, pages 269‚Äì272, 2010.

[9] J. H. Friedman. Greedy function approximation: A gradient
boosting machine. Annals of Statistics, 29:1189‚Äì1232, 2000.

[30] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio,
R. Moore, A. Kipman, and A. Blake. Real-time human pose
recognition in parts from single depth images. In CVPR, pages
1297‚Äì1304, 2011.

[10] P. Geurts, D. Ernst, and L. Wehenkel. Extremely randomized
trees. Mach. Learn., 63(1):3‚Äì42, 2006.
[11] P. Geurts and G. Louppe. Learning to rank with extremely
randomized trees. In Proc. of the Yahoo! L2R Challenge, held
at ICML 2010, Haifa, Israel, June 25, 2010, volume 14 of
JMLR Proceedings Track, pages 49‚Äì61, 2011.

[31] N. Tax, S. Bockting, and D. Hiemstra. A cross-benchmark
comparison of 87 learning to rank. Information Processing &
Management, 51(6):757‚Äì772, 2015.

[12] T. Hastie, R. Tibshirani, and J. H. Friedman. The Elements of
Statistical Learning. Springer, 2009.

[32] J. Xu and H. Li. Adarank: A boosting algorithm for
information retrieval. In ACM SIGIR, pages 391‚Äì398, 2007.

[13] R. Jin, Y. Liu, L. Si, J. Carbonell, and A. G. Hauptmann. A
new boosting algorithm using input-dependent regularizer. In
ICML, 2003.

[33] Z. E. Xu, K. Q. Weinberger, and O. Chapelle. The greedy
miser: Learning under test-time budgets. In ICML, 2012.

104

