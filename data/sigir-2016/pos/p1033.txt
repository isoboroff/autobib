Toward Estimating the Rank Correlation between the
Test Collection Results and the True System Performance
Julián Urbano

Mónica Marrero

Universitat Pompeu Fabra
Barcelona, Spain

National Supercomputing Center
Barcelona, Spain

urbano.julian@gmail.com

monica.marrero@bsc.es

ABSTRACT

difference required between two systems to ensure a maximum error rate in relative comparisons. Common practice
nowadays is to report the p-value of a statistical significance
test run either for each pair of systems (e.g. t-test) or for the
whole set (e.g. ANOVA and F -test). They provide a sense of
confidence about individual pairs of systems or about a swap
somewhere in the ranking, but they do not give a general
idea of how similar the observed ranking is to the truth.
We propose parametric and non-parametric approaches
to estimate the τ and AP correlations. Through large scale
simulation with TREC data, we show that they have very
low bias and small error even for mid-sized collections.

The Kendall τ and AP rank correlation coefficients have
become mainstream in Information Retrieval research for
comparing the rankings of systems produced by two different evaluation conditions, such as different effectiveness
measures or pool depths. However, in this paper we focus
on the expected rank correlation between the mean scores
observed with a test collection and the true, unobservable
means under the same conditions. In particular, we propose
statistical estimators of τ and AP correlations following both
parametric and non-parametric approaches, and with special
emphasis on small topic sets. Through large scale simulation
with TREC data, we study the error and bias of the estimators. In general, such estimates of expected correlation with
the true ranking may accompany the results reported from
an evaluation experiment, as an easy to understand figure of
reliability. All the results in this paper are fully reproducible
with data and code available online.

2.

Let A = ha1 , . . . , am i and B = hb1 , . . . , bm i be the mean
scores of the same set of m systems as observed under two
different evaluation conditions, such that ai and bi refer to
the i-th system. In many situations we are interested in the
distance between the two rankings. Considering systems
in pairs, a distance can be computed by counting how many
pairs are concordant or discordant between the two rankings:
a pair is concordant if their relative order is the same in
both rankings, and discordant if it is the opposite. Kendall
[3] followed this idea to define his τ correlation coefficient

Keywords
Evaluation; Test Collection; Correlation; Kendall; Average
Precision; Estimation

1.

CORRELATION BETWEEN
TWO RANKINGS

INTRODUCTION

The Kendall τ [3] and AP [8] rank correlation coefficients
are widely used in Information Retrieval to compare rankings of systems produced by different evaluation conditions,
such as different assessors [6], effectiveness measures [4] or
topic sets [1]. One reason for this success is their simplicity:
they provide a single score that is easy to understand.
In this paper we tackle the problem of estimating the correlation between the ranking of systems obtained with a test
collection and the true ranking under the same conditions.
Such estimates can make a nice companion to a set of evaluation results, as a single figure of the reliability of the experiment. Voorhees and Buckley [7] proposed to report a
similar figure in terms of sensitivity, that is, the minimum

τ=

#discordants
#concordants−#discordants
= 1−2
, (1)
total
total

which evaluates to −1 when the rankings are reversed, +1
when they are the same, and 0 when there are as many
concordant pairs as there are discordant. Note that the term
#discordants/total can be interpreted as the expected value
of a random experiment: pick two arbitrary systems and
return 1 if they are discordant, or 0 if they are concordant.
The Kendall τ coefficient can thus be interpreted in terms
of the probability of discordance.
Yilmaz et al. [8] followed this idea to define a correlation
coefficient with the same rationale as Average Precision. It
is similar to Kendall τ , but it penalizes more if swaps occur
between systems at the top of the ranking, much like AP
penalizes more if the non-relevant documents appear at the
top of the results. In particular, they considered that one of
the rankings, say B, is the true ranking and the other one is
an estimate of it. The random experiment is now as follows:
pick one system at random from A and another one ranked
above it, and return 1 if they are discordant, or 0 if they
are concordant. Their AP correlation coefficient can then
be defined just as in (1) as follows:

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

SIGIR ’16, July 17 - 21, 2016, Pisa, Italy
c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2914752

1033

τAP = 1 −


m 
2 X #discordants above i
.
m − 1 i=2
i−1

1X
Xi ,
n
σ̂ = s · Cn ,
r
1 X
s=
(Xi − X)2 ,
n−1
r
n − 1 Γ((n − 1)/2)
Cn =
,
2
Γ(n/2)
µ̂ = X =

(2)

Note that τAP also ranges between −1 and +1.

3.

EXPECTED CORRELATION
WITH THE TRUE RANKING

The previous section contemplated the case where we compute the correlation between two given rankings A and B. In
this section we study the case where we are given a ranking
A obtained with the sample of topics in the test collection,
and want to estimate its correlation with the true ranking B
over the population of topics, which is of course unknown.
For simplicity, let us first assume that the systems are already sorted in descending order by their mean score. Let
us further define Dij as the random variable that equals 1
if systems i and j are discordant and 0 otherwise, that is,
whether they are swapped in the true ranking. Both τ and
τAP can be re-defined from (1) and (2) in terms of Dij alone:
τ =1−
τAP = 1 −

m−1
m
X X
4
Dij ,
m(m − 1) i=1 j=i+1

(3)

m−1
i−1
XX
2
Dij
.
m − 1 i=1 j=1 i − 1

(4)

3.1.2

Minimum Squared Quantile Deviation (MSQD)

The problem when estimating σ from a small sample is
that the observations are likely to be concentrated around
the mean and seldom occur near the tails. As a consequence,
(7) is likely to underestimate the true dispersion in the population. If the sample contains a few dozen observations this
is not expected to be a problem, but with very small samples
of, say, just 10 topics, it might be.
We propose a new and generic estimator to avoid this
problem. Let us consider a distribution function F with
parameter θ. A random sample from this distribution is
expected to uniformly cover the quantile space, that is, all
quantiles are equally likely to appear in the sample. Thus,
when we are given a sample we may force them to uniformly
cover the quantile space and then select the θ that minimizes
the observed deviations. For instance, if our sample contains
only one observation, we force it to correspond to the quantile 1/2; if we have two observations then we force them to
be the quantiles 1/3 and 2/3. In general, if Ri is the rank of
Xi within the sample, it will correspond to the Ri /(n + 1)
Ri
; θ . The squared quantile deviquantile, which is F −1 n+1
ation of an observation Xi is therefore



2
Ri
SQD(Xi ; θ) = F −1
; θ − Xi .
n+1

Estimating the Probability of Discordance

Since each pij is estimated independently from the other
systems, let us simplify notation here to just p. In addition,
let X1 , . . . , Xn be the differences in effectiveness between the
two systems and for each of the n topics in the collection.
The problem is therefore to estimate p = P (µ < 0) from
these n observations. Recall that systems are assumed to be
ranked by mean observed scores, so X > 0.
In the following we present two parametric estimators
based on the Central Limit Theorem (CLT) and then two
non-parametric estimators based on resampling.

3.1.1

(7)

where s is the sample standard deviation. The Cn factor [2]
ensures that E[σ̂] = σ. This bias correction is applied because, even though s2 is an unbiased estimator of σ 2 , by
Jensen’s inequality s is not an unbiased estimator of σ.

Since they are just a linear combination of random variables,
their expectations are as in (3) and (4) but replacing Dij
with E[Dij ]. Note that each Dij is a Bernoulli random variable, so its expectation is just the probability of discordance
E[Dij ] = P (µi − µj < 0) = pij . The problem of estimating
the correlation with the true ranking thus boils down to estimating the probability that any two systems are swapped.
The next subsection presents four ways of achieving this.

3.1

(6)

The Minimum Squared Quantile Deviation estimator is then
the one that minimizes the sum of squared deviations:
X
θ̂MSQD = arg min
SQD(Xi ; θ).
θ∈Θ

Let us assume again a normal distribution, so that


√
Ri
; µ, σ = µ + σ 2ei ,
F −1
n+1


Ri
ei = erf−1 2
−1 .
n+1

Maximum Likelihood (ML)

The CLT tells us that X is approximately normally distributed with mean µ and variance σ 2 /n as n → ∞. Using
the cdf of the normal distribution we can therefore estimate
the probability of discordance. However, our estimates are
likely off with small samples (see Section 3.1.2), so we assume Xi ∼ N (µ, σ 2 ) and employ the t distribution to account for the √
uncertainty in estimating σ 2 . Standardizing,
we have that n(X − µ)/σ ∼ t(n − 1), so


√ µ̂
p = P (µ < 0) ≈ Tn−1 − n
,
(5)
σ̂

The sum of squared deviations is thus
√ ✘
√
✿0
✘
✘✘2ei + 2σ 2 e2i − 2Xi µ − 2Xi σ 2ei + Xi2 ,
µ2 − ✘
2µσ
P
and the second term cancels out because
ei = 0. To find
the µ and σ that minimize this expression, we simply differentiate, equal to 0, and solve. The partial derivatives are
X
X
dSQD
=
2µ − 2Xi = 2nµ − 2
Xi and
dµ
X
√
dSQD
=
4σe2i − 2 2Xi ei ,
dσ
and therefore, the estimators are
X

where Tn−1 is the cdf of the t distribution with n − 1 degrees of freedom. The estimates µ̂ and σ̂ are computed via
Maximum Likelihood as

1034

1X
Xi = X,
n
√ P

Ri
−1
2 Xi · erf −1 2 n+1
σ̂ =
2 .
P
Ri
2 erf −1 2 n+1
−1
µ̂ =



error = E |τ̂ (A) − τ (A, µ)| .

(8)

Even if the error is small, it could tend to be in the same
direction, that is, over- or underestimating the correlation.
Bias refers to this tendency, measured as the expected difference between the estimated and the true correlation:


bias = E τ̂ (A) − τ (A, µ) .

(9)

As above, the probability of discordance is estimated with
the cdf of the t distribution as in (5), but using estimators (8) and (9) instead of (6) and (7).

3.1.3

If the bias is positive it means that the estimator tends to
overestimate the correlations. In general, we seek estimators
with small error and zero bias.

Resampling (RES)

In both the ML and MSQD estimators above we assumed
that scores are normally distributed, but this is clearly not
strictly true. A non-parametric alternative is the use of resampling to estimate the sampling distribution of the mean
and from there the probability of discordance.
Suppose we draw a random sample X1∗ , . . . , Xn∗ with replacement from our original observations, and compute their
∗
sample mean X . This experiment is replicated T = 1, 000
∗
∗
times, yielding sample means X 1 , . . . , X T . By the law of
large numbers, the distribution of these sample means converges to the sampling distribution of X as T → ∞. The
probability of discordance can thus be estimated as the frac∗
tion of times that X i is negative:

1 X  ∗
(10)
p = P (µ < 0) ≈
I Xi < 0 .
T

4.2

Methods, Data and Baselines

which involves the sum of n2 terms. In general, for n variables this requires the evaluation of nn terms, which is clearly
unfeasible even for small samples, so instead we resort to
Monte Carlo methods. As with the RES estimator, we generate a random sample X1∗ , . . . , Xn∗ from fˆ and compute the
∗
mean X . After T replications, the probability of discor∗
dance is estimated as the fraction of times that X i is negative. We set T = 1, 000 replications and use gaussian kernels.

From the above definitions it is evident that we need to
know the true ranking of systems µ, but this is of course
unknown. To solve this problem we resort to the simulation
method proposed by Urbano [5]. Given the topic-by-system
matrix of scores B from an existing collection, it generates
a new matrix A with the scores by the same set of systems
over a new and random set of topics. There are two important characteristics of this method that are appealing for us.
First, the simulated scores are realistic, as they maintain the
same distributions and correlations among systems as in the
original collection. Second, it is designed to ensure that the
expected mean score of a system is equal
 to the mean score
in the original collection, that is, E As = B s . For us, this
means that the true mean scores are fixed to be the mean
scores in the original collection, that is, µs := B s . This allows us to analyze the error and bias of the estimators with
a large number of simulated, yet realistic test collections.
We use the TREC 6, 7 and 8 ad hoc collections as evaluated with Average Precision. As is common practice, we
first drop the bottom 25% of results to avoid effects of possibly buggy systems. From each original collection, we simulate 1, 000 new collections of sizes n = 10, 20, . . . , 100 topics,
leading to a total of 30, 000 simulated collections. For each
of them, we estimate τ and τAP using each of the estimators
defined above, and also compute the true correlations (recall
that this is possible because the true system scores are fixed
upfront when simulating new collections). Finally, for each
correlation coefficient, original collection, topic set size and
estimator, we compute expected error and bias.
Two baselines are used to compare our estimators to.
They are based on a split-half method that randomly splits
the available topic set in two subsets, and then computes the
correlations as if one was the truth and the other one the
estimate. This is replicated a number of times for different
subset sizes, up to a maximum of n/2 topics. The observations are then used to fit a model and extrapolate the
expected correlation with n topics. This simple estimator is
found for instance in [7, 4]. Here we run 2, 000 replicates to
fit the model y = a·eb·x , and sample topics with and without
replacement, leading to baselines SH(w) and SH(w/o).

4.

4.3

3.1.4

Kernel Density (KD)

A potential problem with resampling from the original
observations is again that estimates from very small samples
are likely off. An alternative is to approximate the true pdf
via Kernel Density Estimation, and use it to estimate the
probability of discordance. The estimated pdf has the form


1 X
x − Xi
fˆ(x) =
k
,
nh
h
where k is the pdf of the kernel and h is the bandwidth.
Next, we need to estimate the sampling distribution of the
mean, which is basically the distribution of the sum of n
variables drawn from fˆ. For n = 2 this requires the evaluation of the self-convolution of fˆ as follows:
 

Z 
x−z −Xi
z −Xj
1 XX
k
k
dz,
fˆX+X (x) = 2 2
n h i j
h
h

4.1

EVALUATION

Results

Figure 1 shows that the error of the estimators is larger
with small collections. This is somewhat expected, because
collections with too few topics are unstable and the rankings
of systems vary too much to begin with. The error seems
to plateau at about 0.025 in all our estimators, though with
small collections of just 10 topics they are expected to be
off by about 0.065. With the usual 50 topics, the expected
error is 0.035. We can finally observe that the typical SH

Criteria

There are two properties of the correlation estimators that
we are interested in, namely error and bias. Error refers to
the expected difference between the estimate and the truth.
Here we measure absolute error, thus quantifying the expected magnitude of the error when estimating the correlation of a given collection:

1035

Bias
0.04

90

80

70

60

50

40

30

20

100

90

100

90

100

80

70

60

50

40

30

20

10

0.00

Bias
0.04

0.08

tauAP − adhoc7

topic set size

Bias
0.04

0.08

tauAP − adhoc8

topic set size

80

70

60

50

40

30

20

10

90

100

topic set size

80

70

60

50

40

30

20

10

0.00

Bias
0.04
0.00
100

90

80

70

60

50

40

30

20

10

topic set size

100

90

80

70

60

50

40

30

20

10

tau − adhoc8

0.08

0.10
Error
0.06 0.08

topic set size

10

0.00
90

100

80

70

60

50

40

30

20

10
Bias
0.04
0.00

90

80

70

60

50

40

30

20

10

100

topic set size

tauAP − adhoc8

tauAP − adhoc6

0.08

0.08

SH(w/o)
SH(w)

Bias
0.04

RES
KD

tau − adhoc7

0.08

0.10
Error
0.06 0.08
0.02

topic set size

0.02

ML
MSQD

topic set size

0.04
topic set size

100

90

80

70

60

50

40

30

20

10

0.04

Error
0.06 0.08

0.10

tau − adhoc8

tau − adhoc6

0.00
90

100

80

70

60

50

40

30

20

10

topic set size

tauAP − adhoc7

0.04
topic set size

100

90

80

70

60

50

40

30

20

10

0.02

0.04

Error
0.06 0.08

0.10

tau − adhoc7

0.02

90

100

80

70

60

50

40

30

20

10

0.02

topic set size

0.02

tauAP − adhoc6

0.10

SH(w/o)
SH(w)

0.04

RES
KD

0.04

Error
0.06 0.08

ML
MSQD

Error
0.06 0.08

0.10

tau − adhoc6

Figure 1: Error of the estimators of τ (left) and τAP
(right) for each of the three original collections.

Figure 2: Bias of the estimators of τ (left) and τAP
(right) for each of the three original collections.

estimators are clearly outperformed by all our proposed estimators. In general, with 30–40 topics they behave almost
the same, but with small samples MSQD is slightly better.
Figure 2 shows that the correlations tend to be overestimated, especially with small collections, but this time we see
clear differences among estimators. MSQD behaves much
better than the others, especially with very small collections.
With only 10 topics ML outperforms KD because there is
just too little data to properly approximate the pdf , but with
20 or more topics it does a very good job at approximating
the true distribution. ML, on the other hand, assumes a
normal distribution and can therefore be less faithful to the
data. Even at around 40–50 topics KD gets to slightly outperform MSQD for the same reason. Overall, they seem to
plateau at about 0.004, and RES always performs worse than
the others. Finally, the SH estimator with replacement has
a roughly constant bias of about 0.055. The SH estimator
without replacement shows a clearly biased behavior probably due to the choice of model.

of reliability in the results of an evaluation experiment.
In light of the expected error with individual collections,
our future work will mainly focus on the development of interval estimates. We also plan to study other estimators of
discordance as well as the application of a fully bayesian approach to estimate correlations. All the results in this paper
are fully reproducible with data and code available online at
http://github.com/julian-urbano/sigir2016-correlation.

5.

Acknowledgments. Work supported by the Spanish Government: JdC postdoctoral fellowship, and projects TIN201570816-R and MDM-2015-0502. Florentino dimisión.

References
[1] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and
J. Allan. If I Had a Million Queries. In ECIR, 2009.
[2] W. H. Holtzman. The Unbiased Estimate of the Population
Variance and Standard Deviation. Am. J. Psychology, 1950.
[3] M. G. Kendall. A New Measure of Rank Correlation.
Biometrika, 1938.
[4] T. Sakai. On the Reliability of Information Retrieval Metrics
Based on Graded Relevance. Inf. Proc. & Mngmnt, 2007.
[5] J. Urbano. Test Collection Reliability: A Study of Bias and
Robustness to Statistical Assumptions via Stochastic
Simulation. Information Retrieval, 2016.
[6] E. M. Voorhees. Variations in Relevance Judgments and the
Measurement of Retrieval Effectiveness. In SIGIR, 1998.
[7] E. M. Voorhees and C. Buckley. The Effect of Topic Set Size
on Retrieval Experiment Error. In SIGIR, 2002.
[8] E. Yilmaz, J. Aslam, and S. Robertson. A New Rank
Correlation Coefficient for Information Retrieval. In SIGIR,
2008.

CONCLUSION

In this paper we present two estimators of the Kendall
τ and AP rank correlation coefficients between the mean
system scores produced by a test collection and the true,
unobservable means. We proposed parametric and nonparametric alternatives, and through large scale simulation
with realistic collections we showed that even with small
topic sets the estimators have little bias and the errors are
generally small with collections of medium size. These estimators may prove useful as an easy to understand indicator

1036

