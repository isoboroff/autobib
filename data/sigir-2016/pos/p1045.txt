Two Sample T-tests for IR Evaluation: Student or Welch?
Tetsuya Sakai
Waseda University, Japan.

tetsuyasakai@acm.org

ABSTRACT
There are two well-known versions of the t-test for comparing means
from unpaired data: Student’s t-test and Welch’s t-test. While
Welch’s t-test does not assume homoscedasticity (i.e., equal variances), it involves approximations. A classical textbook recommendation would be to use Student’s t-test if either the two sample
sizes are similar or the two sample variances are similar, and to
use Welch’s t-test only when both of the above conditions are violated. However, a more recent recommendation seems to be to use
Welch’s t-test unconditionally. Using past data from both TREC
and NTCIR, the present study demonstrates that the latter advice
should not be followed blindly in the context of IR system evaluation. More specifically, our results suggest that if the sample
sizes differ substantially and if the larger sample has a substantially
larger variance, Welch’s t-test may not be reliable.

Keywords
statistical significance; test collections; topics; variances

1.

INTRODUCTION

The present study concerns IR evaluation where two means from
different samples need to be compared. The classical approach
for this would be to employ a two-sample (i.e., unpaired) t-test
to discuss whether, given the observed sample means, the population means may be different. There are two well-known versions
of the two-sample t-test: Student’s t-test and Welch’s t-test. While
Welch’s t-test does not assume homoscedasticity (i.e., equal variances), it involves approximations. A classical textbook recommendation would be to use Student’s t-test if either the two sample sizes n1 , n2 are similar or the two sample variances V1 , V2 are
similar, and to use Welch’s t-test only when both of the above conditions are violated [3]. However, a more recent recommendation
seems to be to use Welch’s t-test unconditionally. For example,
Daniel Laken’s blog posted on January 26, 2015 recommends researchers to “Always use Welch’s t-test instead of Student’s t-test,”
while demonstrating the superiority of Welch’s t-test using simu-

lated data1 . The t.test function provided in the stats library
of the R language uses Welch’s t-test by default2 ; hence researchers
who use this function as a black box may well be using Welch’s
t-test all the time. In fact, in as far back as 1981, Gans [1] recommended “the automatic use of the Welch test alone” as an option,
based on simulations. The present study seeks to obtain the right
recommendations for the purpose of IR system evaluation using
real data from TREC and NTCIR.

2. PRIOR ART
In the IR evaluation literature, researchers have focussed mainly
on the paired data setting, because the most basic method for comparing two IR systems is to use the same test collection with a single topic set to compare two systems. For example, Smucker et
al. [7] compared the sign test, the Wilcoxon signed rank test, the
paired t-test, the bootstrap test and the randomisation test from the
viewpoint of how similar the p-values of different test types are to
one another, and concluded that the use of the two nonparametric
tests (sign and Wilcoxon) should be discontinued. Urbano et al. [8]
conducted a follow-up study on the same set of paired significance
tests, using repeated topic set splitting experiments in a way similar to earlier studies (e.g. [10]) to quantify the discrepancies across
the pairs of topic sets for comparing two systems. Contrary to the
recommendation by Smucker et al., Urbano et al. report that “the
permutation test is not optimal under any criterion.”
In contrast to the aforementioned studies, the present study concerns two-sample t-tests for comparing two means, with sample
sizes n1 , n2 . Possible applications of two-sample tests in IR include comparing sets of clicks from two different search engines,
between-subject design user experiments, and comparing the “hardness” of two test collections using the same IR system. Sakai [4]
used the two-sample bootstrap test (in addition to the paired bootstrap test) for the purpose of comparing different evaluation measures in terms of “discriminative power.”

3. T-TEST: STUDENT’S AND WELCH’S
The common assumptions are as follows. We have scores
x11 , . . . , x1n1 that each obey N (μ1 , σ12 ), as well as x21 , . . . , x2n1
that each obey N (μ2 , σ22 ). The population means and variances
(μ• ’s and σ•2 ’s) are unknown,
and we want to test if 
μ1 = μ2 ,
n2
1
1
given sample means x̄1 = n11 n
x
and
x̄
=
1j
2
j=1
j=1 x2j .
n2
n1
n2
2
2
Let S1 =
(x
−
x̄
)
,
S
=
(x
−
x̄
)
for later
1j
1
2
2j
2
j=1
j=1
purposes.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.

SIGIR ’16, July 17 - 21, 2016, Pisa, Italy

1
http://daniellakens.blogspot.nl/2015/01/
always-use-welchs-t-test-instead-of.html
2
http://127.0.0.1:27533/library/stats/html/t.test.html

c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.

ISBN 978-1-4503-4069-4/16/07. . . $15.00
DOI: http://dx.doi.org/10.1145/2911451.2914684

1045

3.1 Student’s t-test

Table 1: Test collections and runs used in this study.

Student’s two-sample t-test further assumes homoscedasticity:
= σ22 . While this is a strong assumption, it is also known that
this test is quite robust to assumption violations. For this test, we
first define a pooled variance V = (S1 + S2 )/φ where φ is the
degree of freedom for V given by φ = φ1 +φ2 , φ1 = n1 −1, φ2 =
n2 − 1. The test statistic is:

σ12

x̄1 − x̄2
t0 = 
V (1/n1 + 1/n2 )

Topics (n)
Runs (m)
Qrels

Table 2: Sample size ratios experimented in this study.
Target ratio
50:50 (a = 1.0)
40:60 (a = 1.5)
30:70 (a = 2.3)
10:90 (a = 9.0)

(1)

which is compared against t(φ; α), the critical t value for φ =
n1 + n2 − 2 degrees of freedom with the significance level of α.

3.2 Welch’s t-test
The good news about Welch’s t-test is that it does not assume homoscedasticity; the bad news is that it involves approximations [3],
as briefly discussed below. Let V1 = S1 /φ1 , V2 = S2 /φ2 . For this
test, the test statistic is
tw0 = 

x̄1 − x̄2
V1 /n1 + V2 /n2

(2)

which is compared against t(φ∗ ; α), where
φ∗ = (

V1
V2 2 (V1 /n1 )2
(V2 /n2 )2
+
) /{
+
}.
n1
n2
φ1
φ2

(3)

Welch’s t-test approximates the distribution of the following statistic by a χ2 distribution with φ0 degrees of freedom:
W = φ0 (

V1
V2
σ2
σ2
+
)/( 1 + 2 ) .
n1
n2
n1
n2

(4)

Furthermore, it estimates φ0 as φ̂0 = φ∗ using Eq. 3. Do these
approximations have any consequences for IR evaluation?

3.3 Analytical Relationships
Nagata [3] clarifies the relationship between the above two ttests analytically. Let a = n2 /n1 , b = V2 /V1 . Then it is easy to
derive that the ratio of the two test statictics tw0 /t0 is given by:

(a + 1){n1 (ab + 1) − (b + 1)}
tw0
= g(a, b) =
.
(5)
t0
(a + b)(n1 (a + 1) − 2)
Note that g(1, b) = g(a, 1) = 1. Hence, if either n1 = n2 or
V1 = V2 holds, then t0 = tw0 holds. In practice, if the larger
sample is no larger than 1.5 times the other, or if the larger variance
is no larger than 1.5 times the other, t0 and tw0 will differ by at most
20% or so [3]. As for the degrees of freedom, it can be shown that:
∗

Actual ratio (TREC99)
50:49
40:59
30:69
10:89

Actual ratio (NTCIR97)
50:47
40:57
30:67
10:87

randomly partition the topics into two sets of size n1 and n2 , respectively. For each of the m runs and a given evaluation measure,
conduct a two-sided, two-sample test to see if the two means for
the same run are statistically significant. The ground truth is that
they are not, since the scores actually come from the same system.
The random partitioning is done B = 1000 times, so a test collection with m runs will yield Bm = 1000m p-values for each
significance test type with a given evaluation measure.
Table 1 shows a summary of the two data sets used in this study.
We chose them because (a) we wanted about n = 100 topics
(or more if available) with graded relevance assessments; (b) we
wanted data (with many runs) from different evaluation conferences to ensure generalisability. “TREC99” comprises 110 runs
from the TREC 2004 robust track [9], with 99 robust track topics. While this data set has many runs, the 99 topics come from
two robust track rounds: 50 from TREC 2003 and 49 from 2004.
In contrast, “NTCIR97,” which comprises 40 Simplified Chinese
runs from the NTCIR-7 ACLIA IR4QA task [6], uses 97 topics
that originate from a single round of the task.
For each data set with a given evaluation measure, we experimented with four different sample size ratios a = n2 /n1 as shown
in Table 2 to obtain a total of 4000m p-values. Recall that, according to Nagata [3], Student’s and Welch’s t statistics should not be
vastly different for the a = 1 (n1 : n2 = 50 : 50) and a = 1.5
(n1 : n2 = 40 : 60) settings, regardless of how the two sample
variances differ. We experimented with four evaluation measures:
(binary) Average Precision (AP), Q-measure (Q), normalised Discounted Cumulative Gain (nDCG) and normalised Expected Reciprocal Rank (nERR). Unlike the other three measures, nERR is
known to be suitable for navigational information needs due to its
diminishing return property; for this very reason, it is known to be
statistically less stable than the other measures [5].

5. RESULTS AND DISCUSSIONS
Figure 1 plots Welch’s p-values against Student’s p-values for
the 110,000 two-sample t-tests conducted with the TREC99 data,
for nDCG ((a)-(d)) and nERR ((e)-(h)) with different target sample
size ratios. The graphs for TREC99 with AP and Q, as well all
all graphs for NTCIR97, are omitted due to lack of space. However, the overall picture is the same for all evaluation measures
and across the two data sets, and we believe that our findings are
general. In each graph, the blue rectangle represents the situation
where Student’s t-test obtains a p-value smaller than α = 0.05 (i.e.,
a false positive) while Welch’s t-test does not; the red rectangle represents the opposite situation; the bottom left square represents the
situation where both tests obtain a false alarm at α = 0.05. It
can be observed that when the target sample size ratio is a = 1
(n1 : n2 = 50 : 50), the two tests are indeed virtually identical and
false alarms are very rare; as we gradually increase the sample size
ratio until it reaches a = 9.0 (n1 : n2 = 10 : 90), we obtain more
and more false alarms on both sides of the diagonal.

2

φ
(a + b) (n1 − 1)(an1 − 1)
= h(a, b) = 2
.
φ
{a (an1 − 1) + b2 (n1 − 1)}{(a + 1)n1 − 2}
(6)
Since h(1, 1) = 1 holds, having n1 = n2 and V1 = V2 is a
sufficient condition for obtaining φ∗ = φ. Also, it can be verified
that h(a, b) = φ∗ /φ is much smaller than one if b = V2 /V1 is
close to one and a = n2 /n1 is far from one. That is, Welch’s t-test
has relatively low statistical power (due to its degree of freedom
φ∗ being much smaller than Student’s φ) when the variances are
roughly equal but the sample sizes are quite different.

4.

TREC99
NTCIR97
601-700 minus 672 (99)
T41-385 minus 86, 331, 362 (97)
TREC 2004 robust (110)
NTCIR-7 IR4QA Chinese (40)
L2: relevant; L1: partially relevant; L0: judged nonrelevant

EXPERIMENTS

In order to compare Student’s and Welch’s t-tests in terms of reliability, we adopt a method similar to that of Webber et al. [11]:
given a topic set of size n, with m runs that processed these topics,

1046

Figure 1: Welch’s vs. Student’s p-values: TREC99; 110 runs; nDCG (top) and nERR (bottom).

• When the two variances are similar (2/3 ≤ b ≤ 3/2), the
false positive rates of the two tests are very small (1.0-4.8%),
although, as discussed immediately below, Student’s t-test
yields more false positive than Welch’s when the sample size
ratio a = n2 /n1 is extreme (Column (d));

Tables 3 and 4 show the false positive rates for all of our TREC99
and NTCIR97 experiments, respectively. Based on the discussion
provided in Section 3.3, we categorised the observations into three
classes: the first is for those where the variance ratio satisfies 2/3 ≤
b ≤ 3/2: recall that Student’s and Welch’s t statistics are expected
to be similar to each other in this situation. The other two classes
(b < 2/3 and b > 3/2) represent the situations where the sample
variances are very different. For example, Table 3 Section (III)
Column (d) provides the following information about our TREC99
experiments for nDCG with the sample size ratio a = 9.0 (with the
actual sample sizes n1 = 10, n2 = 89 as shown in Table 1):

• As we increase the sample size ratio a = n2 /n1 , Welch’s ttest yields more and more false positives when b = V2 /V1 >
3/2. That is, if the sample sizes differ substantially and if the
larger sample has a substantially larger variance, Welch’s ttest may not be reliable for two-sample IR evaluation. In particular, when a = n2 /n1 = 9 (Column (d)), the false positive rates for Welch’s t-test are 14.1-24.6% whereas those
for Student’s t-test are only 4.4-13.5%. Whereas, as we increase the sample size ratio a, Student’s t-test yields more
and more false positives when b ≤ 3/2 (i.e., 2/3 ≤ b ≤ 3/2
or b < 2/3), but this tendency is not as marked as Welch’s
for the b > 3/2 range. As a result, in terms of the overall false positive rates, Welch’s t-test is actually slightly less
reliable than the Student’s t-test when a is very large.

• For 64,447 out of the 110,000 observations, the sample variance ratio b = V2 /V1 satisfied 2/3 ≤ b ≤ 3/2; for these
observations, the false positive rate was 4.0% for Student’s
t-test and 2.6% for Welch’s t-test;
• For 16,614 out of the 110,000 observations, b < 2/3; for
these observations, the false positive rate was 4.9% for Student’s t-test and 0.7% for Welch’s t-test;
• For 28,939 out of the 110,000 observations, b > 3/2; for
these observations, the false positive rate was 5.6% for Student’s t-test and 14.1% for Welch’s t-test;

The above results, based on real IR system runs from both TREC
and NTCIR, suggest that the advice “Always use Welch’s t-test instead of Student’s t-test” should not be followed blindly in IR system evaluation.

• Overall, for the 110,000 observations, the false positive rate
was 4.6% for Student’s t-test and 5.3% for Welch’s t-test.
Thus, Welch’s t-test slightly underperforms Student’s, due
to its very high false positive rate for the b > 3/2 setting.

6. CONCLUSIONS
For the purpose of reliable IR system evaluation, we compared
two versions of two-sample t-tests: Student’s t-test (which assumes
homoscedasticity) and Welch’s t-test (which relies on approximated
distributions). Our topic set splitting experiments with runs from

More generally, we can observe the following consistent trends
from Tables 3 and 4:
• When the two sample sizes are equal (Column (a)), Student’s
and Welch’s t-tests perform equally well, regardless of the
range of b3 . The overall false positive rates are 4.9-5.2%.
3

and b > 3/2 are equivalent: it is just a matter of swapping the
two samples, since the two tests are symmetric with respect to the
two samples. So we should expect similar results in Column (a) for
these two ranges of b. The slight caveat is that we actually have
n1  n2 rather than n1 = n2 , as n1 + n2 = 99 or 97.

When n1 = n2 , note that the variance ratio conditions b < 2/3

1047

Table 3: TREC99 false positives at α = 0.05: Student/Welch.
The higher false positive rate in each condition is shown in
bold. The total number of observations for each variance ratio is shown in parentheses.
(a) 50:50
a = 1.0
2/3 ≤ b
≤ 3/2
b < 2/3
b > 3/2
All
2/3 ≤ b
≤ 3/2
b < 2/3
b > 3/2
All
2/3 ≤ b
≤ 3/2
b < 2/3
b > 3/2
All
2/3 ≤ b
≤ 3/2
b < 2/3
b > 3/2
All

3.5%/3.5%
(91,286)
11.9%/12.0%
(9,192)
15.0%/14.7%
(9,522)
5.2%/5.2%
(110,000)
3.7%/3.7%
(90,299)
11.1%/11.1%
(9,620)
13.3%/13.2%
(10,081)
5.2%/5.2%
(110,000)
4.4%/4.4%
(94,936)
7.7%/7.7%
(7,640)
8.4%/8.3%
(7,424)
4.9%/4.9%
(110,000)
4.8%/4.7%
(107,590)
10.5%/10.8%
(1,165)
13.9%/13.5%
(1,245)
4.9%/4.9%
(110,000)

(b) 40:60
a = 1.5
(I) AP
3.4%/3.4%
(90,405)
10.9%/8.5%
(8,887)
13.0%/15.7%
(10,708)
4.9%/5.0%
(110,000)
(II) Q
3.5%/3.5%
(89,563)
10.5%/7.9%
(9,252)
11.6%/14.3%
(11,185)
4.9%/5.0%
(110,000)
(III) nDCG
4.4%/4.4%
(94,480)
7.0%/5.4%
(7,076)
7.9%/9.5%
(8,444)
4.9%/4.9%
(110,000)
(IV) nERR
4.4%/4.4%
(107,161)
10.4%/8.3%
(1,126)
14.4%/16.8%
(1,713)
4.6%/4.7%
(110,000)

Table 4: NTCIR97 false positives at α = 0.05: Student/Welch.
The higher false positive rate in each condition is shown in
bold. The total number of observations for each variance ratio is shown in parentheses.

(c) 30:70
a = 2.3

(d) 10:90
a = 9.0

(a) 50:50
a = 1.0

3.2%/3.1%
(87,833)
9.7%/5.7%
(9,374)
9.9%/15.9%
(12,793)
4.5%/4.8%
(110,000)

2.8%/1.6%
(62,674)
7.7%/0.7%
(16,938)
6.0%/18.4%
(30,388)
4.5%/6.1%
(110,000)

2/3 ≤ b
≤ 3/2
b < 2/3

3.5%/3.4%
(86,872)
8.9%/4.9%
(9,763)
8.5%/14.2%
(13,365)
4.6%/4.8%
(110,000)

3.2%/1.9%
(61,402)
7.6%/0.7%
(17,285)
5.5%/16.9%
(31,313)
4.5%/6.0%
(110,000)

2/3 ≤ b
≤ 3/2
b < 2/3

4.2%/4.2%
(90,837)
6.6%/4.0%
(7,710)
7.6%/12.1%
(11,453)
4.7%/5.0%
(110,000)

4.0%/2.6%
(64,447)
4.9%/0.7%
(16,614)
5.6%/14.1%
(28,939)
4.6%/5.3%
(110,000)

2/3 ≤ b
≤ 3/2
b < 2/3

4.3%/4.3%
(105,953)
6.0%/3.1%
(1,196)
14.1%/20.6%
(2,851)
4.6%/4.7%
(110,000)

3.0%/2.1%
(87,017)
1.7%/0.1%
(7,485)
13.5%/24.2%
(15,498)
4.4%/5.1%
(110,000)

2/3 ≤ b
≤ 3/2
b < 2/3

b > 3/2
All

b > 3/2
All

b > 3/2
All

b > 3/2
All

3.9%/3.9%
(33,043)
9.0%/9.3%
(3,575)
10.4%/9.9%
(3,382)
4.9%/4.9%
(40,000)
1.8%/1.8%
(29,136)
12.9%/13.5%
(5,578)
13.7%/13.0%
(5,286)
4.9%/4.9%
(40,000)
2.0%/2.0%
(32,698)
17.9%/18.5%
(3,715)
20.1%/19.2%
(3,587)
5.1%/5.1%
(40,000)

(c) 30:70
a = 2.3

(d) 10:90
a = 9.0

4.2%/4.2%
(32,716)
10.7%/5.1%
(2,949)
7.5%/14.8%
(4,335)
5.0%/5.4%
(40,000)

3.6%/2.3%
(24,263)
7.2%/0.4%
(6,277)
6.8%/23.4%
(9,460)
4.9%/7.0%
(40,000)

4.1%/4.0%
(32,135)
10.7%/5.6%
(3,300)
8.1%/15.4%
(45,65)
5.1%/5.4%
(40,000)

3.5%/2.2%
(23,172)
7.6%/0.4%
(6,766)
6.4%/23.2%
(10,062)
4.9%/7.2%
(40,000)

1.8%/1.7%
(27,739)
14.9%/8.2%
(5,318)
9.9%/17.7%
(6,943)
4.9%/5.3%
(40,000)

2.2%/1.1%
(18,548)
11.7%/1.1%
(8,533)
4.4%/22.4%
(12,919)
4.9%/8.0%
(40,000)

1.5%/1.4%
(31,171)
18.2%/10.7%
(3,625)
15.1%/24.5%
(5,204)
4.8%/5.3%
(40,000)

2.2%/1.0%
(21,570)
12.4%/1.7%
(7,159)
5.0%/24.6%
(11,270)
4.9%/7.8%
(40,000)

[2] D. Hawking and N. Craswell. The very large collection and web
tracks. In E. M. Voorhees and D. K. Harman, editors, TREC:
Experiment and Evaluation in Information Retrieval, chapter 9. The
MIT Press, 2005.
[3] Y. Nagata. How to Understand Statistical Methods (in Japanese).
Nikkagiren, 1996.
[4] T. Sakai. Evaluating evaluation metrics based on the bootstrap. In
Proceedings of ACM SIGIR 2006, pages 525–532, 2006.
[5] T. Sakai. Metrics, statistics, tests. In PROMISE Winter School 2013:
Bridging between Information Retrieval and Databases (LNCS
8173), pages 116–163, 2014.
[6] T. Sakai, N. Kando, C.-J. Lin, T. Mitamura, H. Shima, D. Ji, K.-H.
Chen, and E. Nyberg. Overview of the NTCIR-7 ACLIA IR4QA
task. In Proceedings of NTCIR-7, pages 77–114, 2008.
[7] M. D. Smucker, J. Allan, and B. Carterette. A comparison of
statistical significance tests for information retrieval evaluation. In
Proceedings of ACM CIKM 2007, pages 623–632, 2007.
[8] J. Urbano, M. Marrero, and D. Martín. A comparison of the
optimality of statistical significance tests for information retrieval
evaluation. In Proceedings of ACM SIGIR 2013, pages 925–928,
2013.
[9] E. M. Voorhees. Overview of the TREC 2004 robust retrieval track.
In Proceedings of TREC 2004, 2005.
[10] E. M. Voorhees. Topic set size redux. In Proceedings of ACM SIGIR
2009, pages 806–807, 2009.
[11] W. Webber, A. Moffat, and J. Zobel. Score standardization for
inter-collection comparison of retrieval systems. In Proceedings of
ACM SIGIR 2008, pages 51–58, 2008.

both TREC and NTCIR are consistent across different evaluation
measures and across these two different IR venues. While neither
our equal variance settings nor our equal sample size settings do not
demonstrate any advantages of Student’s t-test over Welch’s t-test,
our results do suggest that if the sample sizes differ substantially
and if the larger sample has a substantially larger variance, Welch’s
t-test may be less reliable. Hence we argue that the advice “Always use Welch’s t-test instead of Student’s t-test” should not be
followed blindly in IR system evaluation.
In practice, we recommend IR researchers to examine the sample
sizes n1 , n2 and the sample variances V1 , V2 first and then make a
conscious decision, rather than (say) relying on a default setting
in the t.test function provided in R. We also recommend IR
researchers to report explicitly which version of the two-sample ttest was used in their experiments, even if we may be able to spot a
Welch’s t-test when the degree of freedom reported is not an integer
(Eq. 3).

Acknowledgement
I would like to thank Professor Yasushi Nagata for his helpful comments on my results.

7.

4.3%/4.3%
(33,683)
8.8%/9.2%
(3,233)
9.7%/8.9%
(3,084)
5.1%/5.0%
(40,000)

(b) 40:60
a = 1.5
(I) AP
4.3%/4.3%
(33,587)
9.3%/6.1%
(2,965)
9.0%/11.8%
(3,448)
5.0%/5.1%
(40,000)
(II) Q
4.1%/4.1%
(33,085)
10.3%/7.1%
(3,203)
9.5%/11.9%
(3,712)
5.1%/5.1%
(40,000)
(III) nDCG
1.8%/1.8%
(29,193)
14.2%/11.1%
(5,076)
12.6%/16.1%
(5,731)
4.9%/5.0%
(40,000)
(IV) nERR
1.7%/1.7%
(32,303)
18.0%/14.7%
(3,498)
18.7%/23.2%
(4,199)
4.9%/5.1%
(40,000)

REFERENCES

[1] D. J. Gans. Use of a preliminary test in comparing two sample
means. Communications in Statistics - Simuation and Computation,
10(2):163–174, 1981.

1048

